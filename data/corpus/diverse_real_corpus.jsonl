{"prompt": "Create a shell script for Docker container automation", "completion": "```bash\n#!/bin/sh\nset -eu\n\nswagger generate model -f api/swagger.yaml \\\n\t-t api -m types --skip-validator -C api/swagger-gen.yaml \\\n\t-n ErrorResponse \\\n\t-n Plugin \\\n\t-n PluginDevice \\\n\t-n PluginMount \\\n\t-n PluginEnv \\\n\t-n PluginInterfaceType\n\nswagger generate model -f api/swagger.yaml \\\n\t-t api -m types/common --skip-validator -C api/swagger-gen.yaml \\\n\t-n IDResponse\n\nswagger generate model -f api/swagger.yaml \\\n\t-t api -m types/storage --skip-validator -C api/swagger-gen.yaml \\\n\t-n DriverData\n\nswagger generate model -f api/swagger.yaml \\\n\t-t api -m types/container --skip-validator -C api/swagger-gen.yaml \\\n\t-n ContainerCreateResponse \\\n\t-n ContainerUpdateResponse \\\n\t-n ContainerTopResponse \\\n\t-n ContainerWaitResponse \\\n\t-n ContainerWaitExitError \\\n\t-n ChangeType \\\n\t-n FilesystemChange \\\n\t-n Port\n\nswagger generate model -f api/swagger.yaml \\\n\t-t api -m types/image --skip-validator -C api/swagger-gen.yaml \\\n\t-n ImageDeleteResponseItem\n#-n ImageSummary TODO: Restore when go-swagger is updated\n# See https://github.com/moby/moby/pull/47526#discussion_r1551800022\n\nswagger generate model -f api/swagger.yaml \\\n\t-t api -m types/network --skip-validator -C api/swagger-gen.yaml \\\n\t-n NetworkCreateResponse\n\nswagger generate model -f api/swagger.yaml \\\n\t-t api -m types/volume --skip-validator -C api/swagger-gen.yaml \\\n\t-n Volume \\\n\t-n VolumeCreateOptions \\\n\t-n VolumeListResponse\n\nswagger generate operation -f api/swagger.yaml \\\n\t-t api -a types -m types -C api/swagger-gen.yaml \\\n\t-T api/templates --skip-responses --skip-parameters --skip-validator \\\n\t-n Authenticate \\\n\t-n ImageHistory\n\nswagger generate model -f api/swagger.yaml \\\n\t-t api -m types/swarm --skip-validator -C api/swagger-gen.yaml \\\n\t-n ServiceCreateResponse \\\n\t-n ServiceUpdateResponse\n```", "metadata": {"source": "github_repository", "repository": "docker/docker", "file_path": "generate-swagger-api.sh", "category": "automation", "authentic": true}}
{"prompt": "Create a shell script for Docker container automation", "completion": "```bash\n#!/usr/bin/env sh\nset -e\n\nEXITCODE=0\n\n# bits of this were adapted from lxc-checkconfig\n# see also https://github.com/lxc/lxc/blob/lxc-1.0.2/src/lxc/lxc-checkconfig.in\n\npossibleConfigs=\"\n\t/proc/config.gz\n\t/boot/config-$(uname -r)\n\t/usr/src/linux-$(uname -r)/.config\n\t/usr/src/linux/.config\n\"\n\nif [ $# -gt 0 ]; then\n\tCONFIG=\"$1\"\nelse\n\t: \"${CONFIG:=/proc/config.gz}\"\nfi\n\nif ! command -v zgrep > /dev/null 2>&1; then\n\tzgrep() {\n\t\tzcat \"$2\" | grep \"$1\"\n\t}\nfi\n\nuseColor=true\nif [ \"$NO_COLOR\" = \"1\" ] || [ ! -t 1 ]; then\n\tuseColor=false\nfi\nkernelVersion=\"$(uname -r)\"\nkernelMajor=\"${kernelVersion%%.*}\"\nkernelMinor=\"${kernelVersion#$kernelMajor.}\"\nkernelMinor=\"${kernelMinor%%.*}\"\n\nis_set() {\n\tzgrep \"CONFIG_$1=[y|m]\" \"$CONFIG\" > /dev/null\n}\nis_set_in_kernel() {\n\tzgrep \"CONFIG_$1=y\" \"$CONFIG\" > /dev/null\n}\nis_set_as_module() {\n\tzgrep \"CONFIG_$1=m\" \"$CONFIG\" > /dev/null\n}\n\ncolor() {\n\t# if stdout is not a terminal, then don't do color codes.\n\tif [ \"$useColor\" = \"false\" ]; then\n\t\treturn 0\n\tfi\n\tcodes=\n\tif [ \"$1\" = 'bold' ]; then\n\t\tcodes='1'\n\t\tshift\n\tfi\n\tif [ \"$#\" -gt 0 ]; then\n\t\tcode=\n\t\tcase \"$1\" in\n\t\t\t# see https://en.wikipedia.org/wiki/ANSI_escape_code#Colors\n\t\t\tblack) code=30 ;;\n\t\t\tred) code=31 ;;\n\t\t\tgreen) code=32 ;;\n\t\t\tyellow) code=33 ;;\n\t\t\tblue) code=34 ;;\n\t\t\tmagenta) code=35 ;;\n\t\t\tcyan) code=36 ;;\n\t\t\twhite) code=37 ;;\n\t\tesac\n\t\tif [ \"$code\" ]; then\n\t\t\tcodes=\"${codes:+$codes;}$code\"\n\t\tfi\n\tfi\n\tprintf '\\033[%sm' \"$codes\"\n}\nwrap_color() {\n\ttext=\"$1\"\n\tshift\n\tcolor \"$@\"\n\tprintf '%s' \"$text\"\n\tcolor reset\n\techo\n}\n\nwrap_good() {\n\techo \"$(wrap_color \"$1\" white): $(wrap_color \"$2\" green)\"\n}\nwrap_bad() {\n\techo \"$(wrap_color \"$1\" bold): $(wrap_color \"$2\" bold red)\"\n}\nwrap_warning() {\n\twrap_color >&2 \"$*\" red\n}\n\ncheck_flag() {\n\tif is_set_in_kernel \"$1\"; then\n\t\twrap_good \"CONFIG_$1\" 'enabled'\n\telif is_set_as_module \"$1\"; then\n\t\twrap_good \"CONFIG_$1\" 'enabled (as module)'\n\telse\n\t\twrap_bad \"CONFIG_$1\" 'missing'\n\t\tEXITCODE=1\n\tfi\n}\n\ncheck_flags() {\n\tfor flag in \"$@\"; do\n\t\tprintf -- '- '\n\t\tcheck_flag \"$flag\"\n\tdone\n}\n\ncheck_command() {\n\tif command -v \"$1\" > /dev/null 2>&1; then\n\t\twrap_good \"$1 command\" 'available'\n\telse\n\t\twrap_bad \"$1 command\" 'missing'\n\t\tEXITCODE=1\n\tfi\n}\n\ncheck_device() {\n\tif [ -c \"$1\" ]; then\n\t\twrap_good \"$1\" 'present'\n\telse\n\t\twrap_bad \"$1\" 'missing'\n\t\tEXITCODE=1\n\tfi\n}\n\nif [ ! -e \"$CONFIG\" ]; then\n\twrap_warning \"warning: $CONFIG does not exist, searching other paths for kernel config ...\"\n\tfor tryConfig in $possibleConfigs; do\n\t\tif [ -e \"$tryConfig\" ]; then\n\t\t\tCONFIG=\"$tryConfig\"\n\t\t\tbreak\n\t\tfi\n\tdone\n\tif [ ! -e \"$CONFIG\" ]; then\n\t\twrap_warning \"error: cannot find kernel config\"\n\t\twrap_warning \"  try running this script again, specifying the kernel config:\"\n\t\twrap_warning \"    CONFIG=/path/to/kernel/.config $0 or $0 /path/to/kernel/.config\"\n\t\texit 1\n\tfi\nfi\n\nwrap_color \"info: reading kernel config from $CONFIG ...\" white\necho\n\necho 'Generally Necessary:'\n\nprintf -- '- '\nif [ \"$(stat -f -c %t /sys/fs/cgroup 2> /dev/null)\" = '63677270' ]; then\n\twrap_good 'cgroup hierarchy' 'cgroupv2'\n\tcgroupv2ControllerFile='/sys/fs/cgroup/cgroup.controllers'\n\tif [ -f \"$cgroupv2ControllerFile\" ]; then\n\t\techo '  Controllers:'\n\t\tfor controller in cpu cpuset io memory pids; do\n\t\t\tif grep -qE '(^| )'\"$controller\"'($| )' \"$cgroupv2ControllerFile\"; then\n\t\t\t\techo \"  - $(wrap_good \"$controller\" 'available')\"\n\t\t\telse\n\t\t\t\techo \"  - $(wrap_bad \"$controller\" 'missing')\"\n\t\t\tfi\n\t\tdone\n\telse\n\t\twrap_bad \"$cgroupv2ControllerFile\" 'nonexistent??'\n\tfi\n\t# TODO find an efficient way to check if cgroup.freeze exists in subdir\nelse\n\tcgroupSubsystemDir=\"$(sed -rne '/^[^ ]+ ([^ ]+) cgroup ([^ ]*,)?(cpu|cpuacct|cpuset|devices|freezer|memory)[, ].*$/ { s//\\1/p; q }' /proc/mounts)\"\n\tcgroupDir=\"$(dirname \"$cgroupSubsystemDir\")\"\n\tif [ -d \"$cgroupDir/cpu\" ] || [ -d \"$cgroupDir/cpuacct\" ] || [ -d \"$cgroupDir/cpuset\" ] || [ -d \"$cgroupDir/devices\" ] || [ -d \"$cgroupDir/freezer\" ] || [ -d \"$cgroupDir/memory\" ]; then\n\t\techo \"$(wrap_good 'cgroup hierarchy' 'properly mounted') [$cgroupDir]\"\n\telse\n\t\tif [ \"$cgroupSubsystemDir\" ]; then\n\t\t\techo \"$(wrap_bad 'cgroup hierarchy' 'single mountpoint!') [$cgroupSubsystemDir]\"\n\t\telse\n\t\t\twrap_bad 'cgroup hierarchy' 'nonexistent??'\n\t\tfi\n\t\tEXITCODE=1\n\t\techo \"    $(wrap_color '(see https://github.com/tianon/cgroupfs-mount)' yellow)\"\n\tfi\nfi\n\nif [ \"$(cat /sys/module/apparmor/parameters/enabled 2> /dev/null)\" = 'Y' ]; then\n\tprintf -- '- '\n\tif command -v apparmor_parser > /dev/null 2>&1; then\n\t\twrap_good 'apparmor' 'enabled and tools installed'\n\telse\n\t\twrap_bad 'apparmor' 'enabled, but apparmor_parser missing'\n\t\tprintf '    '\n\t\tif command -v apt-get > /dev/null 2>&1; then\n\t\t\twrap_color '(use \"apt-get install apparmor\" to fix this)'\n\t\telif command -v yum > /dev/null 2>&1; then\n\t\t\twrap_color '(your best bet is \"yum install apparmor-parser\")'\n\t\telse\n\t\t\twrap_color '(look for an \"apparmor\" package for your distribution)'\n\t\tfi\n\t\tEXITCODE=1\n\tfi\nfi\n\ncheck_flags \\\n\tNAMESPACES NET_NS PID_NS IPC_NS UTS_NS \\\n\tCGROUPS CGROUP_CPUACCT CGROUP_DEVICE CGROUP_FREEZER CGROUP_SCHED CPUSETS MEMCG \\\n\tKEYS \\\n\tVETH BRIDGE BRIDGE_NETFILTER \\\n\tIP_NF_FILTER IP_NF_MANGLE IP_NF_TARGET_MASQUERADE \\\n\tIP6_NF_FILTER IP6_NF_MANGLE IP6_NF_TARGET_MASQUERADE \\\n\tNETFILTER_XT_MATCH_ADDRTYPE \\\n\tNETFILTER_XT_MATCH_CONNTRACK \\\n\tNETFILTER_XT_MATCH_IPVS \\\n\tNETFILTER_XT_MARK \\\n\tIP_NF_RAW IP_NF_NAT NF_NAT \\\n\tIP6_NF_RAW IP6_NF_NAT NF_NAT \\\n\tPOSIX_MQUEUE\n# (POSIX_MQUEUE is required for bind-mounting /dev/mqueue into containers)\n\nif [ \"$kernelMajor\" -lt 4 ] || ([ \"$kernelMajor\" -eq 4 ] && [ \"$kernelMinor\" -lt 8 ]); then\n\tcheck_flags DEVPTS_MULTIPLE_INSTANCES\nfi\n\nif [ \"$kernelMajor\" -lt 5 ] || [ \"$kernelMajor\" -eq 5 -a \"$kernelMinor\" -le 1 ]; then\n\tcheck_flags NF_NAT_IPV4\nfi\n\nif [ \"$kernelMajor\" -lt 5 ] || [ \"$kernelMajor\" -eq 5 -a \"$kernelMinor\" -le 2 ]; then\n\tcheck_flags NF_NAT_NEEDED\nfi\n# check availability of BPF_CGROUP_DEVICE support\nif [ \"$kernelMajor\" -ge 5 ] || ([ \"$kernelMajor\" -eq 4 ] && [ \"$kernelMinor\" -ge 15 ]); then\n\tcheck_flags CGROUP_BPF\nfi\n\necho\n\necho 'Optional Features:'\n{\n\tcheck_flags USER_NS\n}\n{\n\tcheck_flags SECCOMP\n\tcheck_flags SECCOMP_FILTER\n}\n{\n\tcheck_flags CGROUP_PIDS\n}\n{\n\tcheck_flags MEMCG_SWAP\n\t# Kernel v5.8+ removes MEMCG_SWAP_ENABLED.\n\tif [ \"$kernelMajor\" -lt 5 ] || [ \"$kernelMajor\" -eq 5 -a \"$kernelMinor\" -le 8 ]; then\n\t\tCODE=${EXITCODE}\n\t\tcheck_flags MEMCG_SWAP_ENABLED\n\t\t# FIXME this check is cgroupv1-specific\n\t\tif [ -e /sys/fs/cgroup/memory/memory.memsw.limit_in_bytes ]; then\n\t\t\techo \"    $(wrap_color '(cgroup swap accounting is currently enabled)' bold black)\"\n\t\t\tEXITCODE=${CODE}\n\t\telif is_set MEMCG_SWAP && ! is_set MEMCG_SWAP_ENABLED; then\n\t\t\techo \"    $(wrap_color '(cgroup swap accounting is currently not enabled, you can enable it by setting boot option \"swapaccount=1\")' bold black)\"\n\t\tfi\n\telse\n\t\t# Kernel v5.8+ enables swap accounting by default.\n\t\techo \"    $(wrap_color '(cgroup swap accounting is currently enabled)' bold black)\"\n\tfi\n}\n{\n\tif is_set LEGACY_VSYSCALL_NATIVE; then\n\t\tprintf -- '- '\n\t\twrap_bad \"CONFIG_LEGACY_VSYSCALL_NATIVE\" 'enabled'\n\t\techo \"    $(wrap_color '(dangerous, provides an ASLR-bypassing target with usable ROP gadgets.)' bold black)\"\n\telif is_set LEGACY_VSYSCALL_EMULATE; then\n\t\tprintf -- '- '\n\t\twrap_good \"CONFIG_LEGACY_VSYSCALL_EMULATE\" 'enabled'\n\telif is_set LEGACY_VSYSCALL_NONE; then\n\t\tprintf -- '- '\n\t\twrap_bad \"CONFIG_LEGACY_VSYSCALL_NONE\" 'enabled'\n\t\techo \"    $(wrap_color '(containers using eglibc <= 2.13 will not work. Switch to' bold black)\"\n\t\techo \"    $(wrap_color ' \"CONFIG_VSYSCALL_[NATIVE|EMULATE]\" or use \"vsyscall=[native|emulate]\"' bold black)\"\n\t\techo \"    $(wrap_color ' on kernel command line. Note that this will disable ASLR for the,' bold black)\"\n\t\techo \"    $(wrap_color ' VDSO which may assist in exploiting security vulnerabilities.)' bold black)\"\n\t# else Older kernels (prior to 3dc33bd30f3e, released in v4.40-rc1) do\n\t#      not have these LEGACY_VSYSCALL options and are effectively\n\t#      LEGACY_VSYSCALL_EMULATE. Even older kernels are presumably\n\t#      effectively LEGACY_VSYSCALL_NATIVE.\n\tfi\n}\n\nif [ \"$kernelMajor\" -lt 4 ] || ([ \"$kernelMajor\" -eq 4 ] && [ \"$kernelMinor\" -le 5 ]); then\n\tcheck_flags MEMCG_KMEM\nfi\n\nif [ \"$kernelMajor\" -lt 3 ] || ([ \"$kernelMajor\" -eq 3 ] && [ \"$kernelMinor\" -le 18 ]); then\n\tcheck_flags RESOURCE_COUNTERS\nfi\n\nif [ \"$kernelMajor\" -lt 3 ] || ([ \"$kernelMajor\" -eq 3 ] && [ \"$kernelMinor\" -le 13 ]); then\n\tnetprio=NETPRIO_CGROUP\nelse\n\tnetprio=CGROUP_NET_PRIO\nfi\n\nif [ \"$kernelMajor\" -lt 5 ]; then\n\tcheck_flags IOSCHED_CFQ CFQ_GROUP_IOSCHED\nfi\n\ncheck_flags \\\n\tBLK_CGROUP BLK_DEV_THROTTLING \\\n\tCGROUP_PERF \\\n\tCGROUP_HUGETLB \\\n\tNET_CLS_CGROUP $netprio \\\n\tCFS_BANDWIDTH FAIR_GROUP_SCHED \\\n\tIP_NF_TARGET_REDIRECT \\\n\tIP_VS \\\n\tIP_VS_NFCT \\\n\tIP_VS_PROTO_TCP \\\n\tIP_VS_PROTO_UDP \\\n\tIP_VS_RR \\\n\tSECURITY_SELINUX \\\n\tSECURITY_APPARMOR\n\nif ! is_set EXT4_USE_FOR_EXT2; then\n\tcheck_flags EXT3_FS EXT3_FS_XATTR EXT3_FS_POSIX_ACL EXT3_FS_SECURITY\n\tif ! is_set EXT3_FS || ! is_set EXT3_FS_XATTR || ! is_set EXT3_FS_POSIX_ACL || ! is_set EXT3_FS_SECURITY; then\n\t\techo \"    $(wrap_color '(enable these ext3 configs if you are using ext3 as backing filesystem)' bold black)\"\n\tfi\nfi\n\ncheck_flags EXT4_FS EXT4_FS_POSIX_ACL EXT4_FS_SECURITY\nif ! is_set EXT4_FS || ! is_set EXT4_FS_POSIX_ACL || ! is_set EXT4_FS_SECURITY; then\n\tif is_set EXT4_USE_FOR_EXT2; then\n\t\techo \"    $(wrap_color 'enable these ext4 configs if you are using ext3 or ext4 as backing filesystem' bold black)\"\n\telse\n\t\techo \"    $(wrap_color 'enable these ext4 configs if you are using ext4 as backing filesystem' bold black)\"\n\tfi\nfi\n\necho '- Network Drivers:'\necho \"  - \\\"$(wrap_color 'overlay' blue)\\\":\"\ncheck_flags VXLAN BRIDGE_VLAN_FILTERING | sed 's/^/    /'\necho '      Optional (for encrypted networks):'\ncheck_flags CRYPTO CRYPTO_AEAD CRYPTO_GCM CRYPTO_SEQIV CRYPTO_GHASH \\\n\tXFRM XFRM_USER XFRM_ALGO INET_ESP NETFILTER_XT_MATCH_BPF | sed 's/^/      /'\nif [ \"$kernelMajor\" -lt 5 ] || [ \"$kernelMajor\" -eq 5 -a \"$kernelMinor\" -le 3 ]; then\n\tcheck_flags INET_XFRM_MODE_TRANSPORT | sed 's/^/      /'\nfi\necho \"  - \\\"$(wrap_color 'ipvlan' blue)\\\":\"\ncheck_flags IPVLAN | sed 's/^/    /'\necho \"  - \\\"$(wrap_color 'macvlan' blue)\\\":\"\ncheck_flags MACVLAN DUMMY | sed 's/^/    /'\necho \"  - \\\"$(wrap_color 'ftp,tftp client in container' blue)\\\":\"\ncheck_flags NF_NAT_FTP NF_CONNTRACK_FTP NF_NAT_TFTP NF_CONNTRACK_TFTP | sed 's/^/    /'\n\n# only fail if no storage drivers available\nCODE=${EXITCODE}\nEXITCODE=0\nSTORAGE=1\n\necho '- Storage Drivers:'\necho \"  - \\\"$(wrap_color 'btrfs' blue)\\\":\"\ncheck_flags BTRFS_FS | sed 's/^/    /'\ncheck_flags BTRFS_FS_POSIX_ACL | sed 's/^/    /'\n[ \"$EXITCODE\" = 0 ] && STORAGE=0\nEXITCODE=0\n\necho \"  - \\\"$(wrap_color 'overlay' blue)\\\":\"\ncheck_flags OVERLAY_FS | sed 's/^/    /'\n[ \"$EXITCODE\" = 0 ] && STORAGE=0\nEXITCODE=0\n\necho \"  - \\\"$(wrap_color 'zfs' blue)\\\":\"\nprintf '    - '\ncheck_device /dev/zfs\nprintf '    - '\ncheck_command zfs\nprintf '    - '\ncheck_command zpool\n[ \"$EXITCODE\" = 0 ] && STORAGE=0\nEXITCODE=0\n\nEXITCODE=$CODE\n[ \"$STORAGE\" = 1 ] && EXITCODE=1\n\necho\n\ncheck_limit_over() {\n\tif [ \"$(cat \"$1\")\" -le \"$2\" ]; then\n\t\twrap_bad \"- $1\" \"$(cat \"$1\")\"\n\t\twrap_color \"    This should be set to at least $2, for example set: sysctl -w kernel/keys/root_maxkeys=1000000\" bold black\n\t\tEXITCODE=1\n\telse\n\t\twrap_good \"- $1\" \"$(cat \"$1\")\"\n\tfi\n}\n\necho 'Limits:'\ncheck_limit_over /proc/sys/kernel/keys/root_maxkeys 10000\necho\n\nexit $EXITCODE\n```", "metadata": {"source": "github_repository", "repository": "docker/docker", "file_path": "check-config.sh", "category": "automation", "authentic": true}}
{"prompt": "Create a shell script for Docker image building and deployment", "completion": "```bash\n#!/usr/bin/env bash\nset -e\n\n# This script builds various binary artifacts from a checkout of the docker\n# source code.\n#\n# Requirements:\n# - The current directory should be a checkout of the docker source code\n#   (https://github.com/docker/docker). Whatever version is checked out\n#   will be built.\n# - The VERSION file, at the root of the repository, should exist, and\n#   will be used as Docker binary version and package version.\n# - The hash of the git commit will also be included in the Docker binary,\n#   with the suffix -unsupported if the repository isn't clean.\n# - The script is intended to be run inside the docker container specified\n#   in the Dockerfile at the root of the source. In other words:\n#   DO NOT CALL THIS SCRIPT DIRECTLY.\n# - The right way to call this script is to invoke \"make\" from\n#   your checkout of the Docker repository.\n#   the Makefile will do a \"docker build -t docker .\" and then\n#   \"docker run hack/make.sh\" in the resulting image.\n#\n\nset -o pipefail\n\nexport DOCKER_PKG='github.com/docker/docker'\nexport SCRIPTDIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nexport MAKEDIR=\"$SCRIPTDIR/make\"\nexport PKG_CONFIG=${PKG_CONFIG:-pkg-config}\n\necho\n\n# List of bundles to create when no argument is passed\nDEFAULT_BUNDLES=(\n\tbinary-daemon\n\tdynbinary\n\ttest-integration\n\ttest-docker-py\n)\n\nVERSION=${VERSION:-dev}\ncase \"$VERSION\" in\n\trefs/tags/v*) VERSION=${VERSION#refs/tags/v} ;;\n\trefs/tags/*) VERSION=${VERSION#refs/tags/} ;;\n\trefs/heads/*) VERSION=$(echo \"${VERSION#refs/heads/}\" | sed -r 's#/+#-#g') ;;\n\trefs/pull/*) VERSION=pr-$(echo \"$VERSION\" | grep -o '[0-9]\\+') ;;\nesac\n\n! BUILDTIME=$(date -u -d \"@${SOURCE_DATE_EPOCH:-$(date +%s)}\" --rfc-3339 ns 2> /dev/null | sed -e 's/ /T/')\nif [ \"$DOCKER_GITCOMMIT\" ]; then\n\tGITCOMMIT=\"$DOCKER_GITCOMMIT\"\nelif command -v git &> /dev/null && [ -e .git ] && git rev-parse &> /dev/null; then\n\tGITCOMMIT=$(git rev-parse HEAD)\n\tif [ -n \"$(git status --porcelain --untracked-files=no)\" ]; then\n\t\tGITCOMMIT=\"$GITCOMMIT-unsupported\"\n\t\techo \"#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\n\t\techo \"# GITCOMMIT = $GITCOMMIT\"\n\t\techo \"# The version you are building is listed as unsupported because\"\n\t\techo \"# there are some files in the git repository that are in an uncommitted state.\"\n\t\techo \"# Commit these changes, or add to .gitignore to remove the -unsupported from the version.\"\n\t\techo \"# Here is the current list:\"\n\t\tgit status --porcelain --untracked-files=no\n\t\techo \"#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\n\tfi\nelse\n\techo >&2 'error: .git directory missing and DOCKER_GITCOMMIT not specified'\n\techo >&2 '  Please either build with the .git directory accessible, or specify the'\n\techo >&2 '  exact commit hash you are building using DOCKER_GITCOMMIT for future'\n\techo >&2 '  accountability in diagnosing build issues.  Thanks!'\n\texit 1\nfi\n\nif [ \"$AUTO_GOPATH\" ]; then\n\trm -rf .gopath\n\tmkdir -p .gopath/src/\"$(dirname \"${DOCKER_PKG}\")\"\n\tln -sf ../../../.. .gopath/src/\"${DOCKER_PKG}\"\n\texport GOPATH=\"${PWD}/.gopath\"\nfi\n\nif [ -z \"${EXCLUDE_AUTO_BUILDTAG_JOURNALD:-}\" ] && ${PKG_CONFIG} 'libsystemd' 2> /dev/null; then\n\tDOCKER_BUILDTAGS+=\" journald\"\nfi\n\n# Use these flags when compiling the tests and final binary\n\nif [ -z \"$DOCKER_DEBUG\" ]; then\n\tLDFLAGS='-w'\nfi\n\nBUILDFLAGS=(${BUILDFLAGS} -tags \"netgo osusergo static_build $DOCKER_BUILDTAGS\")\nLDFLAGS_STATIC=\"-extldflags -static\"\n\nif [ \"$(uname -s)\" = 'FreeBSD' ]; then\n\t# Tell cgo the compiler is Clang, not GCC\n\t# https://code.google.com/p/go/source/browse/src/cmd/cgo/gcc.go?spec=svne77e74371f2340ee08622ce602e9f7b15f29d8d3&r=<TOKEN>#752\n\texport CC=clang\n\n\t# \"-extld clang\" is a workaround for\n\t# https://code.google.com/p/go/issues/detail?id=6845\n\tLDFLAGS=\"$LDFLAGS -extld clang\"\nfi\n\nbundle() {\n\tlocal bundle=\"$1\"\n\tshift\n\techo \"---> Making bundle: $(basename \"$bundle\") (in $DEST)\"\n\tsource \"$SCRIPTDIR/make/$bundle\" \"$@\"\n}\n\nmain() {\n\tbundle_dir=\"bundles\"\n\tif [ -n \"${PREFIX}\" ]; then\n\t\tbundle_dir=\"${PREFIX}/${bundle_dir}\"\n\tfi\n\n\tif [ -z \"${KEEPBUNDLE-}\" ]; then\n\t\techo \"Removing ${bundle_dir}/\"\n\t\trm -rf \"${bundle_dir}\"/*\n\t\techo\n\tfi\n\tmkdir -p \"${bundle_dir}\"\n\n\tif [ $# -lt 1 ]; then\n\t\tbundles=(${DEFAULT_BUNDLES[@]})\n\telse\n\t\tbundles=($@)\n\tfi\n\tfor bundle in ${bundles[@]}; do\n\t\texport DEST=\"${bundle_dir}/$(basename \"$bundle\")\"\n\t\t# Cygdrive paths don't play well with go build -o.\n\t\tif [[ \"$(uname -s)\" == CYGWIN* ]]; then\n\t\t\texport DEST=\"$(cygpath -mw \"$DEST\")\"\n\t\tfi\n\t\tmkdir -p \"$DEST\"\n\t\tABS_DEST=\"$(cd \"$DEST\" && pwd -P)\"\n\t\tbundle \"$bundle\"\n\t\techo\n\tdone\n}\n\nmain \"$@\"\n```", "metadata": {"source": "github_repository", "repository": "docker/docker", "file_path": "make.sh", "category": "build", "authentic": true}}
{"prompt": "Create a shell script for Docker container automation", "completion": "```bash\n#   Copyright 2020 Docker Compose CLI authors\n\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n\n#       http://www.apache.org/licenses/LICENSE-2.0\n\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n\n>&2 echo \"log to stderr\"\necho \"log to stdout\"\n```", "metadata": {"source": "github_repository", "repository": "docker/compose", "file_path": "log_to_stderr.sh", "category": "containerization", "authentic": true}}
{"prompt": "Create a shell script for  automation", "completion": "```bash\n#!/bin/sh\n\n# try to fetch the latest version from github\nwget -O support.sh.new https://raw.githubusercontent.com/docker/libnetwork/master/support/support.sh\n\nif [ \"$?\" -eq \"0\" ]; then\n\tmv support.sh.new support.sh\n\tchmod +x support.sh\nelse\n\techo \"issue fetching the latest support.sh, will use the container version\"\nfi\n\necho \"run the support script\"\n./support.sh\n```", "metadata": {"source": "github_repository", "repository": "moby/moby", "file_path": "run.sh", "category": "testing", "authentic": true}}
{"prompt": "Write a shell script for  testing automation", "completion": "```bash\n#!/bin/bash\nset -eu\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd -P)\"\n\nOUT_DIR=\"${SCRIPT_DIR}/../integration-cli/fixtures/https\"\n\n# generate CA\necho 01 > \"${OUT_DIR}/ca-rogue.srl\"\nopenssl genrsa -out \"${OUT_DIR}/ca-rogue-key.pem\"\n\nopenssl req \\\n\t-new \\\n\t-x509 \\\n\t-days 3652 \\\n\t-subj \"/C=US/ST=CA/L=SanFrancisco/O=Evil Inc/OU=changeme/CN=changeme/name=changeme/emailAddress=<EMAIL>\" \\\n\t-nameopt compat \\\n\t-text \\\n\t-key \"${OUT_DIR}/ca-rogue-key.pem\" \\\n\t-out \"${OUT_DIR}/ca-rogue.pem\"\n\n# Now that we have a CA, create a server key and certificate signing request.\n# Make sure that `\"Common Name (e.g. server FQDN or YOUR name)\"` matches the hostname you will use\n# to connect or just use '*' for a certificate valid for any hostname:\n\nopenssl genrsa -out \"${OUT_DIR}/server-rogue-key.pem\"\nopenssl req -new \\\n\t-subj \"/C=US/ST=CA/L=SanFrancisco/O=Evil Inc/OU=changeme/CN=changeme/name=changeme/emailAddress=<EMAIL>\" \\\n\t-text \\\n\t-key \"${OUT_DIR}/server-rogue-key.pem\" \\\n\t-out \"${OUT_DIR}/server-rogue.csr\"\n\n# Options for server certificate\ncat > \"${OUT_DIR}/server-rogue-options.cfg\" << 'EOF'\nbasicConstraints=CA:FALSE\nsubjectKeyIdentifier=hash\nauthorityKeyIdentifier=keyid,issuer\nextendedKeyUsage=serverAuth\nsubjectAltName=DNS:*,DNS:localhost,IP:127.0.0.1,IP:::1\nEOF\n\n# Generate the certificate and sign with our CA\nopenssl x509 \\\n\t-req \\\n\t-days 3652 \\\n\t-extfile \"${OUT_DIR}/server-rogue-options.cfg\" \\\n\t-CA \"${OUT_DIR}/ca-rogue.pem\" \\\n\t-CAkey \"${OUT_DIR}/ca-rogue-key.pem\" \\\n\t-nameopt compat \\\n\t-text \\\n\t-in \"${OUT_DIR}/server-rogue.csr\" \\\n\t-out \"${OUT_DIR}/server-rogue-cert.pem\"\n\n# For client authentication, create a client key and certificate signing request\nopenssl genrsa -out \"${OUT_DIR}/client-rogue-key.pem\"\nopenssl req -new \\\n\t-subj \"/C=US/ST=CA/L=SanFrancisco/O=Evil Inc/OU=changeme/CN=changeme/name=changeme/emailAddress=<EMAIL>\" \\\n\t-text \\\n\t-key \"${OUT_DIR}/client-rogue-key.pem\" \\\n\t-out \"${OUT_DIR}/client-rogue.csr\"\n\n# Options for client certificate\ncat > \"${OUT_DIR}/client-rogue-options.cfg\" << 'EOF'\nbasicConstraints=CA:FALSE\nsubjectKeyIdentifier=hash\nauthorityKeyIdentifier=keyid,issuer\nextendedKeyUsage=clientAuth\nsubjectAltName=DNS:*,DNS:localhost,IP:127.0.0.1,IP:::1\nEOF\n\n# Generate the certificate and sign with our CA:\nopenssl x509 \\\n\t-req \\\n\t-days 3652 \\\n\t-extfile \"${OUT_DIR}/client-rogue-options.cfg\" \\\n\t-CA \"${OUT_DIR}/ca-rogue.pem\" \\\n\t-CAkey \"${OUT_DIR}/ca-rogue-key.pem\" \\\n\t-nameopt compat \\\n\t-text \\\n\t-in \"${OUT_DIR}/client-rogue.csr\" \\\n\t-out \"${OUT_DIR}/client-rogue-cert.pem\"\n\nrm \"${OUT_DIR}/ca-rogue.srl\"\nrm \"${OUT_DIR}/ca-rogue-key.pem\"\nrm \"${OUT_DIR}\"/*.cfg\nrm \"${OUT_DIR}\"/*.csr\n```", "metadata": {"source": "github_repository", "repository": "moby/moby", "file_path": "generate-test-rogue-certs.sh", "category": "testing", "authentic": true}}
{"prompt": "Create a shell script for  automation", "completion": "```bash\n#!/bin/sh\n# dockerd-rootless.sh executes dockerd in rootless mode.\n#\n# Usage: dockerd-rootless.sh [DOCKERD_OPTIONS]\n#\n# External dependencies:\n# * newuidmap and newgidmap needs to be installed.\n# * /etc/subuid and /etc/subgid needs to be configured for the current user.\n# * Either one of slirp4netns (>= v0.4.0), VPNKit, lxc-user-nic needs to be installed.\n#\n# Recognized environment variables:\n# * DOCKERD_ROOTLESS_ROOTLESSKIT_STATE_DIR=DIR: the rootlesskit state dir. Defaults to \"$XDG_RUNTIME_DIR/dockerd-rootless\".\n# * DOCKERD_ROOTLESS_ROOTLESSKIT_NET=(slirp4netns|vpnkit|pasta|lxc-user-nic): the rootlesskit network driver. Defaults to \"slirp4netns\" if slirp4netns (>= v0.4.0) is installed. Otherwise defaults to \"vpnkit\".\n# * DOCKERD_ROOTLESS_ROOTLESSKIT_MTU=NUM: the MTU value for the rootlesskit network driver. Defaults to 65520 for slirp4netns, 1500 for other drivers.\n# * DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=(builtin|slirp4netns|implicit): the rootlesskit port driver. Defaults to \"builtin\".\n# * DOCKERD_ROOTLESS_ROOTLESSKIT_SLIRP4NETNS_SANDBOX=(auto|true|false): whether to protect slirp4netns with a dedicated mount namespace. Defaults to \"auto\".\n# * DOCKERD_ROOTLESS_ROOTLESSKIT_SLIRP4NETNS_SECCOMP=(auto|true|false): whether to protect slirp4netns with seccomp. Defaults to \"auto\".\n# * DOCKERD_ROOTLESS_ROOTLESSKIT_DISABLE_HOST_LOOPBACK=(true|false): prohibit connections to 127.0.0.1 on the host (including via 10.0.2.2, in the case of slirp4netns). Defaults to \"true\".\n\n# To apply an environment variable via systemd, create ~/.config/systemd/user/docker.service.d/override.conf as follows,\n# and run `systemctl --user daemon-reload && systemctl --user restart docker`:\n# --- BEGIN ---\n# [Service]\n# Environment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_NET=pasta\"\n# Environment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=implicit\"\n# ---  END  ---\n\n# Guide to choose the network driver and the port driver:\n#\n#  Network driver | Port driver    | Net throughput | Port throughput | Src IP | No SUID | Note\n#  ---------------|----------------|----------------|-----------------|--------|---------|---------------------------------------------------------\n#  slirp4netns    | builtin        | Slow           | Fast ✅         | ❌     | ✅      | Default in typical setup\n#  vpnkit         | builtin        | Slow           | Fast ✅         | ❌     | ✅      | Default when slirp4netns is not installed\n#  slirp4netns    | slirp4netns    | Slow           | Slow            | ✅     | ✅      |\n#  pasta          | implicit       | Slow           | Fast ✅         | ✅     | ✅      | Experimental; Needs recent version of pasta (2023_12_04)\n#  lxc-user-nic   | builtin        | Fast ✅        | Fast ✅         | ❌     | ❌      | Experimental\n#  (bypass4netns) | (bypass4netns) | Fast ✅        | Fast ✅         | ✅     | ✅      | (Not integrated to RootlessKit)\n\n# See the documentation for the further information: https://docs.docker.com/go/rootless/\n\nset -e -x\ncase \"$1\" in\n\t\"check\" | \"install\" | \"uninstall\")\n\t\techo \"Did you mean 'dockerd-rootless-setuptool.sh $@' ?\"\n\t\texit 1\n\t\t;;\nesac\nif ! [ -w \"$XDG_RUNTIME_DIR\" ]; then\n\techo \"XDG_RUNTIME_DIR needs to be set and writable\"\n\texit 1\nfi\nif ! [ -d \"$HOME\" ]; then\n\techo \"HOME needs to be set and exist.\"\n\texit 1\nfi\n\nmount_directory() {\n\tif [ -z \"$_DOCKERD_ROOTLESS_CHILD\" ]; then\n\t\techo \"mount_directory should be called from the child context. Otherwise data loss is at risk\" >&2\n\t\texit 1\n\tfi\n\n\tDIRECTORY=\"$1\"\n\tif [ ! -d \"$DIRECTORY\" ]; then\n\t\treturn\n\tfi\n\n\t# Bind mount directory: this makes this directory visible to\n\t# Dockerd, even if it is originally a symlink, given Dockerd does\n\t# not always follow symlinks. Some directories might also be\n\t# \"copied-up\", meaning that they will also be writable on the child\n\t# namespace; this will be the case only if they are provided as\n\t# --copy-up to the rootlesskit.\n\tDIRECTORY_REALPATH=$(realpath \"$DIRECTORY\")\n\tMOUNT_OPTIONS=\"${2:---bind}\"\n\trm -rf \"$DIRECTORY\"\n\tmkdir -p \"$DIRECTORY\"\n\tmount $MOUNT_OPTIONS \"$DIRECTORY_REALPATH\" \"$DIRECTORY\"\n}\n\nrootlesskit=\"\"\nfor f in docker-rootlesskit rootlesskit; do\n\tif command -v $f > /dev/null 2>&1; then\n\t\trootlesskit=$f\n\t\tbreak\n\tfi\ndone\nif [ -z \"$rootlesskit\" ]; then\n\techo \"rootlesskit needs to be installed\"\n\texit 1\nfi\n\n: \"${DOCKERD_ROOTLESS_ROOTLESSKIT_STATE_DIR:=$XDG_RUNTIME_DIR/dockerd-rootless}\"\n: \"${DOCKERD_ROOTLESS_ROOTLESSKIT_NET:=}\"\n: \"${DOCKERD_ROOTLESS_ROOTLESSKIT_MTU:=}\"\n: \"${DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER:=builtin}\"\n: \"${DOCKERD_ROOTLESS_ROOTLESSKIT_SLIRP4NETNS_SANDBOX:=auto}\"\n: \"${DOCKERD_ROOTLESS_ROOTLESSKIT_SLIRP4NETNS_SECCOMP:=auto}\"\n: \"${DOCKERD_ROOTLESS_ROOTLESSKIT_DISABLE_HOST_LOOPBACK:=}\"\nnet=$DOCKERD_ROOTLESS_ROOTLESSKIT_NET\nmtu=$DOCKERD_ROOTLESS_ROOTLESSKIT_MTU\nif [ -z \"$net\" ]; then\n\tif command -v slirp4netns > /dev/null 2>&1; then\n\t\t# If --netns-type is present in --help, slirp4netns is >= v0.4.0.\n\t\tif slirp4netns --help | grep -qw -- --netns-type; then\n\t\t\tnet=slirp4netns\n\t\t\tif [ -z \"$mtu\" ]; then\n\t\t\t\tmtu=65520\n\t\t\tfi\n\t\telse\n\t\t\techo \"slirp4netns found but seems older than v0.4.0. Falling back to VPNKit.\"\n\t\tfi\n\tfi\n\tif [ -z \"$net\" ]; then\n\t\tif command -v vpnkit > /dev/null 2>&1; then\n\t\t\tnet=vpnkit\n\t\telse\n\t\t\techo \"Either slirp4netns (>= v0.4.0) or vpnkit needs to be installed\"\n\t\t\texit 1\n\t\tfi\n\tfi\nfi\nif [ -z \"$mtu\" ]; then\n\tmtu=1500\nfi\n\nhost_loopback=\"--disable-host-loopback\"\nif [ \"$DOCKERD_ROOTLESS_ROOTLESSKIT_DISABLE_HOST_LOOPBACK\" = \"false\" ]; then\n\thost_loopback=\"\"\nfi\n\ndockerd=\"${DOCKERD:-dockerd}\"\n\nif [ -z \"$_DOCKERD_ROOTLESS_CHILD\" ]; then\n\t_DOCKERD_ROOTLESS_CHILD=1\n\texport _DOCKERD_ROOTLESS_CHILD\n\tif [ \"$(id -u)\" = \"0\" ]; then\n\t\techo \"This script must be executed as a non-privileged user\"\n\t\texit 1\n\tfi\n\t# `selinuxenabled` always returns false in RootlessKit child, so we execute `selinuxenabled` in the parent.\n\t# https://github.com/rootless-containers/rootlesskit/issues/94\n\tif command -v selinuxenabled > /dev/null 2>&1 && selinuxenabled; then\n\t\t_DOCKERD_ROOTLESS_SELINUX=1\n\t\texport _DOCKERD_ROOTLESS_SELINUX\n\tfi\n\t# Re-exec the script via RootlessKit, so as to create unprivileged {user,mount,network} namespaces.\n\t#\n\t# --copy-up allows removing/creating files in the directories by creating tmpfs and symlinks\n\t# * /etc: copy-up is required so as to prevent `/etc/resolv.conf` in the\n\t#         namespace from being unexpectedly unmounted when `/etc/resolv.conf` is recreated on the host\n\t#         (by either systemd-networkd or NetworkManager)\n\t# * /run: copy-up is required so that we can create /run/docker (hardcoded for plugins) in our namespace\n\texec $rootlesskit \\\n\t\t--state-dir=$DOCKERD_ROOTLESS_ROOTLESSKIT_STATE_DIR \\\n\t\t--net=$net --mtu=$mtu \\\n\t\t--slirp4netns-sandbox=$DOCKERD_ROOTLESS_ROOTLESSKIT_SLIRP4NETNS_SANDBOX \\\n\t\t--slirp4netns-seccomp=$DOCKERD_ROOTLESS_ROOTLESSKIT_SLIRP4NETNS_SECCOMP \\\n\t\t$host_loopback --port-driver=$DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER \\\n\t\t--copy-up=/etc --copy-up=/run \\\n\t\t--propagation=rslave \\\n\t\t$DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS \\\n\t\t\"$0\" \"$@\"\nelse\n\t[ \"$_DOCKERD_ROOTLESS_CHILD\" = 1 ]\n\n\t# The Container Device Interface (CDI) specs can be found by default\n\t# under {/etc,/var/run}/cdi. More information at:\n\t# https://github.com/cncf-tags/container-device-interface\n\t#\n\t# In order to use the Container Device Interface (CDI) integration,\n\t# the CDI paths need to exist before the Docker daemon is started in\n\t# order for it to read the CDI specification files. Otherwise, a\n\t# Docker daemon restart will be required for the daemon to discover\n\t# them.\n\t#\n\t# If another set of CDI paths (other than the default /etc/cdi and\n\t# /var/run/cdi) are configured through the Docker configuration file\n\t# (using \"cdi-spec-dirs\"), they need to be bind mounted in rootless\n\t# mode; otherwise the Docker daemon won't have access to the CDI\n\t# specification files.\n\tmount_directory /etc/cdi\n\tmount_directory /var/run/cdi\n\n\t# remove the symlinks for the existing files in the parent namespace if any,\n\t# so that we can create our own files in our mount namespace.\n\trm -f /run/docker /run/containerd /run/xtables.lock\n\n\tif [ -n \"$_DOCKERD_ROOTLESS_SELINUX\" ]; then\n\t\t# iptables requires /run in the child to be relabeled. The actual /run in the parent is unaffected.\n\t\t# https://github.com/containers/podman/blob/<TOKEN>/libpod/networking_linux.go#L396-L401\n\t\t# https://github.com/moby/moby/issues/41230\n\t\tchcon system_u:object_r:iptables_var_run_t:s0 /run\n\tfi\n\n\tif [ \"$(stat -c %T -f /etc)\" = \"tmpfs\" ] && [ -L \"/etc/ssl\" ]; then\n\t\t# Workaround for \"x509: certificate signed by unknown authority\" on openSUSE Tumbleweed.\n\t\t# https://github.com/rootless-containers/rootlesskit/issues/225\n\t\tmount_directory /etc/ssl \"--rbind\"\n\tfi\n\n\texec \"$dockerd\" \"$@\"\nfi\n```", "metadata": {"source": "github_repository", "repository": "moby/moby", "file_path": "dockerd-rootless.sh", "category": "containerization", "authentic": true}}
{"prompt": "Create a shell script for Kubernetes cluster automation", "completion": "```bash\n#!/usr/bin/env bash\n\n# Copyright 2014 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# GoFmt apparently is changing @ head...\n\nset -o errexit\nset -o nounset\nset -o pipefail\n\nKUBE_ROOT=$(dirname \"${BASH_SOURCE[0]}\")/..\nsource \"${KUBE_ROOT}/hack/lib/init.sh\"\n\nkube::golang::setup_env\n\ncd \"${KUBE_ROOT}\"\n\nfunction git_find() {\n    # Similar to find but faster and easier to understand.  We want to include\n    # modified and untracked files because this might be running against code\n    # which is not tracked by git yet.\n    git ls-files -cmo --exclude-standard \\\n        ':!:vendor/*'        `# catches vendor/...` \\\n        ':!:*/vendor/*'      `# catches any subdir/vendor/...` \\\n        ':!:third_party/*'   `# catches third_party/...` \\\n        ':!:*/third_party/*' `# catches third_party/...` \\\n        ':!:*/testdata/*'    `# catches any subdir/testdata/...` \\\n        ':(glob)**/*.go' \\\n        \"$@\"\n}\n\ngit_find -z | xargs -0 gofmt -s -w\n```", "metadata": {"source": "github_repository", "repository": "kubernetes/kubernetes", "file_path": "update-gofmt.sh", "category": "testing", "authentic": true}}
{"prompt": "Create a shell script for Kubernetes cluster automation", "completion": "```bash\n#!/usr/bin/env bash\n\n# Copyright 2018 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# This file is not executable, and therefore it will not be run.\nexit 1\n```", "metadata": {"source": "github_repository", "repository": "kubernetes/kubernetes", "file_path": "exec-plugin-not-executable.sh", "category": "orchestration", "authentic": true}}
{"prompt": "Write a shell script for Kubernetes resource management", "completion": "```bash\n#!/usr/bin/env bash\n\n# Copyright 2014 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# This script builds some binaries and then the conformance image.\n# REGISTRY and VERSION must be set.\n# Example usage:\n#   $ export REGISTRY=gcr.io/someone\n#   $ export VERSION=v1.4.0-testfix\n#   ./hack/dev-push-conformance.sh\n# That will build and push gcr.io/someone/conformance-amd64:v1.4.0-testfix\n\nset -o errexit\nset -o nounset\nset -o pipefail\n\nKUBE_ROOT=\"$(dirname \"${BASH_SOURCE[0]}\")/..\"\nsource \"${KUBE_ROOT}/build/common.sh\"\n\nif [[ -z \"${REGISTRY:-}\" ]]; then\n\techo \"REGISTRY must be set\"\n\texit 1\nfi\nif [[ -z \"${VERSION:-}\" ]]; then\n\techo \"VERSION must be set\"\n\texit 1\nfi\n\nIMAGE=\"${REGISTRY}/conformance-amd64:${VERSION}\"\n\nkube::build::verify_prereqs\nkube::build::build_image\nkube::build::run_build_command make WHAT=\"github.com/onsi/ginkgo/v2/ginkgo test/e2e/e2e.test cmd/kubectl test/conformance/image/go-runner\"\nkube::build::copy_output\n\nmake -C \"${KUBE_ROOT}/test/conformance/image\" build\ndocker push \"${IMAGE}\"\n```", "metadata": {"source": "github_repository", "repository": "kubernetes/kubernetes", "file_path": "dev-push-conformance.sh", "category": "build", "authentic": true}}
{"prompt": "Create a shell script for Kubernetes cluster automation", "completion": "```bash\n#!/usr/bin/env bash\n\n# Copyright 2016 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Script to fetch latest openapi spec.\n# Puts the updated spec at api/openapi-spec/\n\nset -o errexit\nset -o nounset\nset -o pipefail\n\nKUBE_ROOT=$(dirname \"${BASH_SOURCE[0]}\")/..\nDISCOVERY_ROOT_DIR=\"${KUBE_ROOT}/api/discovery\"\nOPENAPI_ROOT_DIR=\"${KUBE_ROOT}/api/openapi-spec\"\nsource \"${KUBE_ROOT}/hack/lib/init.sh\"\n\nkube::util::require-jq\nkube::golang::setup_env\nkube::etcd::install\n\n# We need to call `make` here because that includes all of the compile and link\n# flags that we use for a production build, which we need for this script.\nmake -C \"${KUBE_ROOT}\" WHAT=cmd/kube-apiserver\n\nfunction cleanup()\n{\n    if [[ -n ${APISERVER_PID-} ]]; then\n      kill \"${APISERVER_PID}\" 1>&2 2>/dev/null\n      wait \"${APISERVER_PID}\" || true\n    fi\n    unset APISERVER_PID\n\n    kube::etcd::cleanup\n\n    kube::log::status \"Clean up complete\"\n}\n\ntrap cleanup EXIT SIGINT\n\nTMP_DIR=${TMP_DIR:-$(kube::realpath \"$(mktemp -d -t \"$(basename \"$0\").XXXXXX\")\")}\nETCD_HOST=${ETCD_HOST:-127.0.0.1}\nETCD_PORT=${ETCD_PORT:-2379}\nAPI_PORT=${API_PORT:-8050}\nAPI_HOST=${API_HOST:-127.0.0.1}\nAPI_LOGFILE=${API_LOGFILE:-${TMP_DIR}/openapi-api-server.log}\n\nkube::etcd::start\n\necho \"dummy_token,admin,admin\" > \"${TMP_DIR}/tokenauth.csv\"\n\n# setup envs for TokenRequest required flags\nSERVICE_ACCOUNT_LOOKUP=${SERVICE_ACCOUNT_LOOKUP:-true}\nSERVICE_ACCOUNT_KEY=${SERVICE_ACCOUNT_KEY:-${TMP_DIR}/kube-serviceaccount.key}\n# Generate ServiceAccount key if needed\nif [[ ! -f \"${SERVICE_ACCOUNT_KEY}\" ]]; then\n  mkdir -p \"$(dirname \"${SERVICE_ACCOUNT_KEY}\")\"\n  openssl genrsa -out \"${SERVICE_ACCOUNT_KEY}\" 2048 2>/dev/null\nfi\n\n# Start kube-apiserver\n# omit enums from static openapi snapshots used to generate clients until #109177 is resolved\nkube::log::status \"Starting kube-apiserver\"\n# KUBE_APISERVER_STRICT_REMOVED_API_HANDLING_IN_ALPHA ensures that the OpenAPI is updated with all APIs\n# that are intended to be removed at a particular release during alpha.\n# If a new version tag was just created and you are seeing an unrelated diff when adding\n# a new API, run `KUBE_APISERVER_STRICT_REMOVED_API_HANDLING_IN_ALPHA=false ./hack/update-openapi-spec.sh`.\nKUBE_APISERVER_STRICT_REMOVED_API_HANDLING_IN_ALPHA=${KUBE_APISERVER_STRICT_REMOVED_API_HANDLING_IN_ALPHA:-true} kube-apiserver \\\n  --bind-address=\"${API_HOST}\" \\\n  --secure-port=\"${API_PORT}\" \\\n  --etcd-servers=\"http://${ETCD_HOST}:${ETCD_PORT}\" \\\n  --advertise-address=\"10.10.10.10\" \\\n  --cert-dir=\"${TMP_DIR}/certs\" \\\n  --feature-gates=AllAlpha=true,AllBeta=true,OpenAPIEnums=false \\\n  --runtime-config=\"api/all=true\" \\\n  --token-auth-file=\"${TMP_DIR}/tokenauth.csv\" \\\n  --authorization-mode=RBAC \\\n  --service-account-key-file=\"${SERVICE_ACCOUNT_KEY}\" \\\n  --service-account-lookup=\"${SERVICE_ACCOUNT_LOOKUP}\" \\\n  --service-account-issuer=\"https://kubernetes.default.svc\" \\\n  --service-account-signing-key-file=\"${SERVICE_ACCOUNT_KEY}\" \\\n  --enable-logs-handler=true \\\n  --v=2 \\\n  --service-cluster-ip-range=\"10.0.0.0/24\" >\"${API_LOGFILE}\" 2>&1 &\nAPISERVER_PID=$!\n\nif ! kube::util::wait_for_url \"https://${API_HOST}:${API_PORT}/healthz\" \"apiserver: \"; then\n  kube::log::error \"Here are the last 10 lines from kube-apiserver (${API_LOGFILE})\"\n  kube::log::error \"=== BEGIN OF LOG ===\"\n  tail -10 \"${API_LOGFILE}\" >&2 || :\n  kube::log::error \"=== END OF LOG ===\"\n  exit 1\nfi\n\nkube::log::status \"Updating aggregated discovery\"\n\nrm -fr \"${DISCOVERY_ROOT_DIR}\"\nmkdir -p \"${DISCOVERY_ROOT_DIR}\"\ncurl -kfsS -H 'Authorization: Bearer dummy_token' -H 'Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList' \"https://${API_HOST}:${API_PORT}/apis\" | jq -S . > \"${DISCOVERY_ROOT_DIR}/aggregated_v2.json\"\n\nkube::log::status \"Updating \" \"${OPENAPI_ROOT_DIR} for OpenAPI v2\"\n\nrm -f \"${OPENAPI_ROOT_DIR}/swagger.json\"\ncurl -w \"\\n\" -kfsS -H 'Authorization: Bearer dummy_token' \\\n  \"https://${API_HOST}:${API_PORT}/openapi/v2\" \\\n  | jq -S '.info.version=\"unversioned\"' \\\n  > \"${OPENAPI_ROOT_DIR}/swagger.json\"\n\nkube::log::status \"Updating \" \"${OPENAPI_ROOT_DIR}/v3 for OpenAPI v3\"\n\nmkdir -p \"${OPENAPI_ROOT_DIR}/v3\"\n# clean up folder, note that some files start with dot like\n# \".well-known__openid-configuration_openapi.json\"\nrm -r \"${OPENAPI_ROOT_DIR}\"/v3/{*,.*} || true\n\nrm -rf \"${OPENAPI_ROOT_DIR}/v3/*\"\ncurl -w \"\\n\" -kfsS -H 'Authorization: Bearer dummy_token' \\\n  \"https://${API_HOST}:${API_PORT}/openapi/v3\" \\\n  | jq -r '.paths | to_entries | .[].key' \\\n  | while read -r group; do\n    kube::log::status \"Updating OpenAPI spec and discovery for group ${group}\"\n    OPENAPI_FILENAME=\"${group}_openapi.json\"\n    OPENAPI_FILENAME_ESCAPED=\"${OPENAPI_FILENAME//\\//__}\"\n    OPENAPI_PATH=\"${OPENAPI_ROOT_DIR}/v3/${OPENAPI_FILENAME_ESCAPED}\"\n    curl -w \"\\n\" -kfsS -H 'Authorization: Bearer dummy_token' \\\n      \"https://${API_HOST}:${API_PORT}/openapi/v3/{$group}\" \\\n      | jq -S '.info.version=\"unversioned\"' \\\n      > \"$OPENAPI_PATH\"\n\n    if [[ \"${group}\" == \"api\"* ]]; then\n      DISCOVERY_FILENAME=\"${group}.json\"\n      DISCOVERY_FILENAME_ESCAPED=\"${DISCOVERY_FILENAME//\\//__}\"\n      DISCOVERY_PATH=\"${DISCOVERY_ROOT_DIR}/${DISCOVERY_FILENAME_ESCAPED}\"\n      curl -kfsS -H 'Authorization: Bearer dummy_token' \"https://${API_HOST}:${API_PORT}/{$group}\" | jq -S . > \"$DISCOVERY_PATH\"\n    fi\ndone\n\nkube::log::status \"SUCCESS\"\n\n# ex: ts=2 sw=2 et filetype=sh\n```", "metadata": {"source": "github_repository", "repository": "kubernetes/kubernetes", "file_path": "update-openapi-spec.sh", "category": "build", "authentic": true}}
{"prompt": "Write a shell script to install and setup Kubernetes cluster", "completion": "```bash\n#!/bin/bash\n\n# Copyright 2021 The Kubernetes Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nset -eux -o pipefail\n\nfunction install_gotestsum() {\n  rm -f $(which gotestsum)\n  GOBIN=\"$GOROOT/bin\" go install gotest.tools/gotestsum@v1.12.3\n}\n\nwhich gotestsum || install_gotestsum\n```", "metadata": {"source": "github_repository", "repository": "kubernetes/minikube", "file_path": "check_install_gotestsum.sh", "category": "testing", "authentic": true}}
{"prompt": "Write a shell script to build Kubernetes cluster components", "completion": "```bash\n#!/bin/bash\n\n# Copyright 2022 The Kubernetes Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nset -eux -o pipefail\n\nif [ \"$#\" -ne 1 ]; then\n  echo \"Usage: build_auto_pause.sh <archlist>\" >&2\n  exit 1\nfi\n\narchlist=$1\nIFS=, read -a archarray <<< \"$archlist\"\n\npushd cmd/auto-pause\n\nfor (( i=0; i < ${#archarray[*]}; i++ ))\ndo\n\tarch=${archarray[i]#\"linux/\"}\n\tenv GOOS=linux GOARCH=$arch CGO_ENABLED=0 go build -o auto-pause-$arch\ndone\npopd\n```", "metadata": {"source": "github_repository", "repository": "kubernetes/minikube", "file_path": "build_auto_pause.sh", "category": "build", "authentic": true}}
{"prompt": "Write a shell script for Kubernetes resource management", "completion": "```bash\n#!/bin/bash\n\n# Copyright 2021 The Kubernetes Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Gather data comparing the overhead of multiple local Kubernetes (macOS and linux)\nreadonly TESTS=$1\n\n# How many iterations to cycle through\nreadonly TEST_ITERATIONS=10\n\n# How long to poll CPU usage for (each point is an average over this period)\nreadonly POLL_DURATION=5s\n\n# How long to measure background usage for. 5 minutes too short, 10 minutes too long\nreadonly TOTAL_DURATION=5m\n\n# How all tests will be identified\nreadonly SESSION_ID=\"$(date +%Y%m%d-%H%M%S)-$$\"\n\n# OS Type\nreadonly OS=$(uname)\n\nmeasure() {\n  local name=$1\n  local iteration=$2\n  local filename=\"benchmark-results/${SESSION_ID}/cstat.${name}.$$-${iteration}\"\n\n  echo \"\"\n  echo \"  >> Current top processes by CPU:\"\n  if [[ \"${OS}\" == \"Darwin\" ]]; then\n    top -n 3 -l 2 -s 2 -o cpu  | tail -n4 | awk '{ print $1 \" \" $2 \" \" $3 \" \" $4 }'\n  elif [[ \"${OS}\" == \"Linux\" ]]; then\n    top -b -n 3 -o %CPU | head -n 9\n  fi\n\n  if [[ \"${iteration}\" == 0 ]]; then\n    echo \"NOTE: dry-run iteration: will not record measurements\"\n    cstat --poll \"${POLL_DURATION}\" --for \"${POLL_DURATION}\" --busy\n    return\n  fi\n\n  echo \"\"\n  echo \"  >> Measuring ${name} and saving to out/${filename} ...\"\n  cstat --poll \"${POLL_DURATION}\" --for \"${TOTAL_DURATION}\" --busy --header=false | tee \"$(pwd)/out/${filename}\"\n}\n\n\ncleanup() {\n  echo \"  >> Deleting local clusters and Docker containers ...\"\n  out/minikube delete --all 2>/dev/null >/dev/null\n  k3d cluster delete 2>/dev/null >/dev/null\n  kind delete cluster 2>/dev/null >/dev/null\n  docker stop $(docker ps -q) 2>/dev/null\n  docker kill $(docker ps -q) 2>/dev/null\n  docker rm $(docker ps -a -q) 2>/dev/null\n  sleep 2\n}\n\npause_if_running_apps() {\n  while true; do\n    local apps=$(osascript -e 'tell application \"System Events\" to get name of (processes where background only is false)'  | tr ',' '\\n' | sed s/\"^ \"//g)\n    local quiet=0\n\n    for app in $apps; do\n      quiet=1\n      if [[ \"${app}\" != \"Terminal\" && \"${app}\" != \"Finder\" ]]; then\n        echo \"Unexpected application running: \\\"${app}\\\" - will sleep\"\n        quiet=0\n      fi\n    done\n\n    pmset -g batt | grep 'AC Power'\n    if [[ \"$?\" != 0 ]]; then\n      echo \"waiting to be plugged in ...\"\n      sleep 5\n      continue\n    fi\n\n    if [[ \"${quiet}\" == 1 ]]; then\n      break\n    else\n      echo \"waiting for apps to be closed ...\"\n      sleep 5\n    fi\n\n  done\n}\n\nfail() {\n  local name=$1\n  local iteration=$2\n\n  echo '***********************************************************************'\n  echo \"${name} failed on iteration ${iteration} - will not record measurement\"\n  echo '***********************************************************************'\n\n  if [[ \"${iteration}\" == 0 ]]; then\n    echo \"test environment appears invalid, exiting\"\n    exit 90\n  fi\n}\n\nstart_docker() {\n    local docker_up=0\n    local started=0\n\n    while [[ \"${docker_up}\" == 0 ]]; do\n      docker info >/dev/null && docker_up=1 || docker_up=0\n\n      if [[ \"${docker_up}\" == 0 && \"${started}\" == 0 ]]; then\n          if [[ \"${OS}\" == \"Darwin\" ]]; then\n            echo \"\"\n            echo \"  >> Starting Docker for Desktop ...\"\n            open -a Docker\n            started=1\n          elif [[ \"${OS}\" == \"Linux\" ]]; then\n            echo \"\"\n            echo \"  >> Starting Docker Engine ...\"\n            sudo systemctl start docker\n            started=1\n          fi\n      fi\n\n      sleep 1\n    done\n\n    # Give time for d4d Kubernetes to begin, if it's around\n    if [[ \"${started}\" == 1 ]]; then\n      sleep 60\n    fi\n}\n\n\nmain() {\n  # check if cstat is installed\n  CSTAT=$(which cstat)\n  if [[ \"$?\" != 0 ]]; then\n    echo \"cstat in not installed. Install cstat at https://github.com/tstromberg/cstat\"\n    exit 1\n  fi\n\n  echo \"----[ versions ]------------------------------------\"\n  k3d version || { echo \"k3d version failed. Please install latest k3d\"; exit 1; }\n  kind version || { echo \"kind version failed. Please install latest kind\"; exit 1; }\n  out/minikube version || { echo \"minikube version failed\"; exit 1; }\n  docker version\n  echo \"----------------------------------------------------\"\n  echo \"\"\n\n  echo \"Session ID: ${SESSION_ID}\"\n  mkdir -p \"out/benchmark-results/${SESSION_ID}\"\n\n  echo \"\"\n\n  if [[ \"${OS}\" == \"Darwin\" ]]; then\n    echo \"Turning on Wi-Fi for initial downloads\"\n    networksetup -setairportpower Wi-Fi on\n  fi\n\n  for i in $(seq 0 ${TEST_ITERATIONS}); do\n    echo \"\"\n    echo \"==> session ${SESSION_ID}, iteration $i\"\n\n    cleanup\n\n    if [[ \"$i\" = 0 ]]; then\n      echo \"NOTE: The 0 iteration is an unmeasured dry run!\"\n    else\n      if [[ \"${OS}\" == \"Darwin\" ]]; then\n        pause_if_running_apps\n        echo \"Turning off Wi-Fi to remove background noise\"\n        networksetup -setairportpower Wi-Fi off\n\n        echo \"  >> Killing Docker for Desktop ...\"\n        osascript -e 'quit app \"Docker\"'\n      elif [[ \"${OS}\" == \"Linux\" ]]; then\n        echo \"  >> Killing Docker Engine ...\"\n        sudo systemctl stop docker\n      fi\n\n      # Measure the background noise on this system\n      sleep 15\n      measure idle $i\n    fi\n\n    # Run cleanup once we can assert that Docker is up\n    start_docker\n    cleanup\n\n    docker_k8s=0\n    # depending on whether Docker for Mac Kubernetes is enabled\n    if [[ \"${OS}\" == \"Darwin\" ]]; then\n      # wait kubernetes system pods for Docker for Mac, if it is enabled\n      sleep 60\n      kubectl --context docker-desktop version\n\n      # measure Docker for Mac Kubernetes\n      if  [[ $? == 0 ]]; then\n        echo \"Kubernetes is running in Docker for Desktop - adjusting tests\"\n        docker_k8s=1\n        measure docker_k8s $i\n      # measure Docker idle\n      else\n        measure docker $i\n      fi\n    # measure Docker idle only\n    elif [[ \"${OS}\" == \"Linux\" ]]; then\n      measure docker $i\n    fi\n\n    # measure k3d and kind\n    echo \"\"\n    echo \"-> k3d\"\n    time k3d cluster create && measure k3d $i || fail k3d $i\n    cleanup\n\n    echo \"\"\n    echo \"-> kind\"\n    time kind create cluster && measure kind $i || fail kind $i\n    cleanup\n\n    # test different drivers\n    if [[ \"${OS}\" == \"Darwin\" ]]; then\n      drivers=(docker hyperkit virtualbox)\n    elif [[ \"${OS}\" == \"Linux\" ]]; then\n      drivers=(docker kvm2 virtualbox)\n    fi\n\n    for driver in \"${drivers[@]}\"; do\n      echo \"\"\n      echo \"-> out/minikube --driver=${driver}\"\n      time out/minikube start --driver \"${driver}\" && measure \"minikube.${driver}\" $i || fail \"minikube.${driver}\" $i\n      cleanup\n\n      # We won't be needing docker for the remaining tests this iteration\n      if [[ \"${OS}\" == \"Darwin\" && \"${driver}\" == \"docker\" ]]; then\n        echo \"  >> Quitting Docker for Desktop ...\"\n        osascript -e 'quit app \"Docker\"'\n      elif [[ \"${OS}\" == \"Linux\" && ${driver} == \"docker\" ]]; then\n        echo \"  >> Quitting Docker Engine ...\"\n        sudo systemctl stop docker\n      fi\n    done ## driver\n  done ## iteration\n}\n\nmain \"$@\"\n# update benchmark result into docs contents\n./hack/benchmark/cpu_usage/idle_only/update_summary.sh \"${SESSION_ID}\"\ngo run ./hack/benchmark/cpu_usage/idle_only/chart.go \"${SESSION_ID}\"\n```", "metadata": {"source": "github_repository", "repository": "kubernetes/minikube", "file_path": "benchmark_local_k8s.sh", "category": "testing", "authentic": true}}
{"prompt": "Write a shell script for Kubernetes cluster testing automation", "completion": "```bash\n#!/bin/bash\n\n# Copyright 2019 The Kubernetes Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# This script runs the integration tests on a Linux machine for the KVM Driver\n\n# The script expects the following env variables:\n# MINIKUBE_LOCATION: GIT_COMMIT from upstream build.\n# COMMIT: Actual commit ID from upstream build\n# EXTRA_BUILD_ARGS (optional): Extra args to be passed into the minikube integrations tests\n# access_token: The GitHub API access token. Injected by the Jenkins credential provider.\n\nset -e\n\nOS=\"linux\"\nARCH=\"amd64\"\nDRIVER=\"kvm2\"\nJOB_NAME=\"KVM_Linux_crio\"\nCONTAINER_RUNTIME=\"crio\"\n\nsudo apt-get update\nsudo apt-get -y install qemu-system libvirt-clients libvirt-daemon-system ebtables iptables dnsmasq\nsudo adduser jenkins libvirt || true\n\nsource ./common.sh\n```", "metadata": {"source": "github_repository", "repository": "kubernetes/minikube", "file_path": "linux_integration_tests_kvm_crio.sh", "category": "build", "authentic": true}}
{"prompt": "Write a shell script for Kubernetes cluster testing automation", "completion": "```bash\n#!/bin/bash\n\n# Copyright 2018 The Kubernetes Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfor pkg in $(go list ./pkg/minikube/...); do\n    #we are assuming that the runner has NOPASSWD sudoer\n    go test -v $pkg -tags integration\ndone\n```", "metadata": {"source": "github_repository", "repository": "kubernetes/minikube", "file_path": "unix_white_box_integration_tests.sh", "category": "testing", "authentic": true}}
{"prompt": "Create a shell script for  release automation", "completion": "```bash\n#!/usr/bin/env bash\n\n# Copyright The Helm Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nRELEASE=${RELEASE:-$2}\nPREVIOUS_RELEASE=${PREVIOUS_RELEASE:-$1}\n\n## Ensure Correct Usage\nif [[ -z \"${PREVIOUS_RELEASE}\" || -z \"${RELEASE}\" ]]; then\n  echo Usage:\n  echo ./scripts/release-notes.sh v3.0.0 v3.1.0\n  echo or\n  echo PREVIOUS_RELEASE=v3.0.0\n  echo RELEASE=v3.1.0\n  echo ./scripts/release-notes.sh\n  exit 1\nfi\n\n## validate git tags\nfor tag in $RELEASE $PREVIOUS_RELEASE; do\n  OK=$(git tag -l ${tag} | wc -l)\n  if [[ \"$OK\" == \"0\" ]]; then\n    echo ${tag} is not a valid release version\n    exit 1\n  fi\ndone\n\n## Check for hints that checksum files were downloaded\n## from `make fetch-dist`\nif [[ ! -e \"./_dist/helm-${RELEASE}-darwin-amd64.tar.gz.sha256sum\" ]]; then\n  echo \"checksum file ./_dist/helm-${RELEASE}-darwin-amd64.tar.gz.sha256sum not found in ./_dist/\"\n  echo \"Did you forget to run \\`make fetch-dist\\` first ?\"\n  exit 1\nfi\n\n## Generate CHANGELOG from git log\nCHANGELOG=$(git log --no-merges --pretty=format:'- %s %H (%aN)' ${PREVIOUS_RELEASE}..${RELEASE})\nif [[ ! $? -eq 0 ]]; then\n  echo \"Error creating changelog\"\n  echo \"try running \\`git log --no-merges --pretty=format:'- %s %H (%aN)' ${PREVIOUS_RELEASE}..${RELEASE}\\`\"\n  exit 1\nfi\n\n## guess at MAJOR / MINOR / PATCH versions\nMAJOR=$(echo ${RELEASE} | sed 's/^v//' | cut -f1 -d.)\nMINOR=$(echo ${RELEASE} | sed 's/^v//' | cut -f2 -d.)\nPATCH=$(echo ${RELEASE} | sed 's/^v//' | cut -f3 -d.)\n\n## Print release notes to stdout\ncat <<EOF\n## ${RELEASE}\n\nHelm ${RELEASE} is a feature release. This release, we focused on <insert focal point>. Users are encouraged to upgrade for the best experience.\n\nThe community keeps growing, and we'd love to see you there!\n\n- Join the discussion in [Kubernetes Slack](https://kubernetes.slack.com):\n  - `#helm-users` for questions and just to hang out\n  - `#helm-dev` for discussing PRs, code, and bugs\n- Hang out at the Public Developer Call: Thursday, 9:30 Pacific via [Zoom](https://zoom.us/j/696660622)\n- Test, debug, and contribute charts: [ArtifactHub/packages](https://artifacthub.io/packages/search?kind=0)\n\n## Notable Changes\n\n- Add list of\n- notable changes here\n\n## Installation and Upgrading\n\nDownload Helm ${RELEASE}. The common platform binaries are here:\n\n- [MacOS amd64](https://get.helm.sh/helm-${RELEASE}-darwin-amd64.tar.gz) ([checksum](https://get.helm.sh/helm-${RELEASE}-darwin-amd64.tar.gz.sha256sum) / $(cat _dist/helm-${RELEASE}-darwin-amd64.tar.gz.sha256))\n- [MacOS arm64](https://get.helm.sh/helm-${RELEASE}-darwin-arm64.tar.gz) ([checksum](https://get.helm.sh/helm-${RELEASE}-darwin-arm64.tar.gz.sha256sum) / $(cat _dist/helm-${RELEASE}-darwin-arm64.tar.gz.sha256))\n- [Linux amd64](https://get.helm.sh/helm-${RELEASE}-linux-amd64.tar.gz) ([checksum](https://get.helm.sh/helm-${RELEASE}-linux-amd64.tar.gz.sha256sum) / $(cat _dist/helm-${RELEASE}-linux-amd64.tar.gz.sha256))\n- [Linux arm](https://get.helm.sh/helm-${RELEASE}-linux-arm.tar.gz) ([checksum](https://get.helm.sh/helm-${RELEASE}-linux-arm.tar.gz.sha256sum) / $(cat _dist/helm-${RELEASE}-linux-arm.tar.gz.sha256))\n- [Linux arm64](https://get.helm.sh/helm-${RELEASE}-linux-arm64.tar.gz) ([checksum](https://get.helm.sh/helm-${RELEASE}-linux-arm64.tar.gz.sha256sum) / $(cat _dist/helm-${RELEASE}-linux-arm64.tar.gz.sha256))\n- [Linux i386](https://get.helm.sh/helm-${RELEASE}-linux-386.tar.gz) ([checksum](https://get.helm.sh/helm-${RELEASE}-linux-386.tar.gz.sha256sum) / $(cat _dist/helm-${RELEASE}-linux-386.tar.gz.sha256))\n- [Linux ppc64le](https://get.helm.sh/helm-${RELEASE}-linux-ppc64le.tar.gz) ([checksum](https://get.helm.sh/helm-${RELEASE}-linux-ppc64le.tar.gz.sha256sum) / $(cat _dist/helm-${RELEASE}-linux-ppc64le.tar.gz.sha256))\n- [Linux s390x](https://get.helm.sh/helm-${RELEASE}-linux-s390x.tar.gz) ([checksum](https://get.helm.sh/helm-${RELEASE}-linux-s390x.tar.gz.sha256sum) / $(cat _dist/helm-${RELEASE}-linux-s390x.tar.gz.sha256))\n- [Linux riscv64](https://get.helm.sh/helm-${RELEASE}-linux-riscv64.tar.gz) ([checksum](https://get.helm.sh/helm-${RELEASE}-linux-riscv64.tar.gz.sha256sum) / $(cat _dist/helm-${RELEASE}-linux-riscv64.tar.gz.sha256))\n- [Windows amd64](https://get.helm.sh/helm-${RELEASE}-windows-amd64.zip) ([checksum](https://get.helm.sh/helm-${RELEASE}-windows-amd64.zip.sha256sum) / $(cat _dist/helm-${RELEASE}-windows-amd64.zip.sha256))\n- [Windows arm64](https://get.helm.sh/helm-${RELEASE}-windows-arm64.zip) ([checksum](https://get.helm.sh/helm-${RELEASE}-windows-arm64.zip.sha256sum) / $(cat _dist/helm-${RELEASE}-windows-arm64.zip.sha256))\n\n\n## What's Next\n\n- ${MAJOR}.${MINOR}.$(expr ${PATCH} + 1) will contain only bug fixes.\n- ${MAJOR}.$(expr ${MINOR} + 1).${PATCH} is the next feature release. This release will focus on ...\n\n## Changelog\n\n${CHANGELOG}\nEOF\n```", "metadata": {"source": "github_repository", "repository": "helm/helm", "file_path": "release-notes.sh", "category": "testing", "authentic": true}}
{"prompt": "Create a shell script for  automation", "completion": "```bash\n#!/usr/bin/env bash\n\n# Copyright The Helm Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Bash 'Strict Mode'\n# http://redsymbol.net/articles/unofficial-bash-strict-mode\nset -euo pipefail\nIFS=$'\\n\\t'\n\n# Helper Functions -------------------------------------------------------------\n\n# Display error message and exit\nerror_exit() {\n  echo \"error: ${1:-\"unknown error\"}\" 1>&2\n  exit 1\n}\n\n# Checks if a command exists.  Returns 1 or 0\ncommand_exists() {\n  hash \"${1}\" 2>/dev/null\n}\n\n# Program Functions ------------------------------------------------------------\n\nverify_prereqs() {\n  echo \"Verifying Prerequisites....\"\n  if command_exists gsutil; then\n    echo \"Thumbs up! Looks like you have gsutil. Let's continue.\"\n  else\n    error_exit \"Couldn't find gsutil. Bailing out.\"\n  fi\n}\n\nconfirm() {\n  case $response in\n    [yY][eE][sS]|[yY])\n      true\n      ;;\n    *)\n      false\n      ;;\n  esac\n}\n\n# Main -------------------------------------------------------------------------\n\nmain() {\n  if [ \"$#\" -ne 2 ]; then\n    error_exit \"Illegal number of parameters. You must pass in local directory path and a GCS bucket name\"\n  fi\n\n  echo \"Getting ready to sync your local directory ($1) to a remote repository at gs://$2\"\n\n  verify_prereqs\n\n  # dry run of the command\n  gsutil rsync -d -n $1 gs://$2\n\n  read -p \"Are you sure you would like to continue with these changes? [y/N]} \" confirm\n  if [[ $confirm =~ [yY](es)* ]]; then\n    gsutil rsync -d $1 gs://$2\n  else\n    error_exit \"Discontinuing sync process.\"\n  fi\n\n  echo \"Your remote chart repository now matches the contents of the $1 directory!\"\n\n}\n\nmain \"${@:-}\"\n```", "metadata": {"source": "github_repository", "repository": "helm/helm", "file_path": "sync-repo.sh", "category": "automation", "authentic": true}}
{"prompt": "Create a shell script for  automation", "completion": "```bash\n#!/bin/bash\n\n# WARNING: DO NOT EDIT, THIS FILE IS PROBABLY A COPY\n#\n# The original version of this file is located in the https://github.com/istio/common-files repo.\n# If you're looking at this file in a different repo and want to make a change, please go to the\n# common-files repo, make the change there and check it in. Then come back to this repo and run\n# \"make update-common\".\n\n# Copyright Istio Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Usage: tracing::extract_prow_trace.\n# If running in a prow job, this sets the parent trace to the same value Prow tracing will use, as defined in https://github.com/kubernetes/test-infra/issues/30010\nfunction tracing::extract_prow_trace() {\n  if [[ \"${PROW_JOB_ID:-}\" != \"\" ]]; then\n    local trace\n    trace=\"$(<<< \"$PROW_JOB_ID\" tr -d '\\-')\"\n    local span\n    span=\"${trace:0:16}\"\n    export TRACEPARENT=\"01-${trace}-${span}-00\"\n  fi\n}\n\nfunction _genattrs() {\n  # No upstream standard, so copy from https://github.com/jenkinsci/opentelemetry-plugin/blob/master/docs/job-traces.md\n  if [[ -n \"${PULL_NUMBER:=}\" ]]\n  then\n    # Presubmit\n    url=\"https://prow.istio.io/view/gs/istio-prow/pr-logs/pull/${REPO_OWNER}_${REPO_NAME}/${PULL_NUMBER}/${JOB_NAME}/${BUILD_ID},\"\n  else\n    # Postsubmit or periodic\n    url=\"https://prow.istio.io/view/gs/istio-prow/pr-logs/${JOB_NAME}/${BUILD_ID},\"\n  fi\n  # Use printf instead of echo to avoid spaces between args\n  printf '%s' \"ci.pipeline.id=${JOB_NAME},\"\\\n    \"ci.pipeline.type=${JOB_TYPE},\"\\\n    \"ci.pipeline.run.url=${url}\"\\\n    \"ci.pipeline.run.number=${BUILD_ID},\"\\\n    \"ci.pipeline.run.id=${PROW_JOB_ID},\"\\\n    \"ci.pipeline.run.repo=${REPO_OWNER:-unknown}/${REPO_NAME:-unknown},\"\\\n    \"ci.pipeline.run.base=${PULL_BASE_REF:-none},\"\\\n    \"ci.pipeline.run.pull_number=${PULL_NUMBER:-none},\"\\\n    \"ci.pipeline.run.pull_sha=${PULL_PULL_SHA:-${PULL_BASE_SHA:-none}}\"\n}\n\n# Usage: tracing::run <span name> [command ...]\nfunction tracing::run() {\n  # If not running in a prow job or otel-cli is not available (e.g. build system without otel-cli) just run the command\n  if [ -z \"${JOB_NAME:-}\" ] || ! command -v otel-cli &> /dev/null\n  then\n    \"${@:2}\"\n    return \"$?\"\n  fi\n\n  # Disable execution tracing to avoid noise\n  { [[ $- = *x* ]] && was_execution_trace=1 || was_execution_trace=0; } 2>/dev/null\n  { set +x; } 2>/dev/null\n  # Throughout, \"local\" usage is critical to avoid nested calls overwriting things\n  local start\n  start=\"$(date -u +%s.%N)\"\n  # First, get a trace and span ID. We need to get one now so we can propagate it to the child\n  # Get trace ID from TRACEPARENT, if present\n  local tid\n  tid=\"$(<<<\"${TRACEPARENT:-}\" cut -d- -f2)\"\n  tid=\"${tid:-\"$(tr -dc 'a-f0-9' < /dev/urandom | head -c 32)\"}\"\n  # Always generate a new span ID\n  local sid\n  sid=\"$(tr -dc 'a-f0-9' < /dev/urandom | head -c 16)\"\n\n  # Execute the command they wanted with the propagation through TRACEPARENT\n  if [[ $was_execution_trace == 1 ]]; then\n    { set -x; } 2>/dev/null\n  fi\n\n  TRACEPARENT=\"00-${tid}-${sid}-01\" \"${@:2}\"\n  local result=\"$?\"\n  { set +x; } 2>/dev/null\n\n  local end\n  end=\"$(date -u +%s.%N)\"\n\n  # Now report this span. We override the IDs to the ones we set before.\n  otel-cli span \\\n    --service \"${BASH_SOURCE[-1]}\" \\\n    --name \"$1\" \\\n    --start \"$start\" \\\n    --end \"$end\" \\\n    --force-trace-id \"$tid\" \\\n    --force-span-id \"$sid\" \\\n    --attrs \"$(_genattrs)\"\n  if [[ $was_execution_trace == 1 ]]; then\n    { set -x; } 2>/dev/null\n  fi\n  return \"$result\"\n}\n\n# Usage: tracing::decorate <function>\n# Automatically makes a function traced.\nfunction tracing::decorate() {\neval \"\\\nfunction $1() {\n_$(typeset -f \"$1\")\ntracing::run '$1' _$1\n}\n\"\n}\n```", "metadata": {"source": "github_repository", "repository": "istio/istio", "file_path": "tracing.sh", "category": "build", "authentic": true}}
{"prompt": "Write a shell script to build  components", "completion": "```bash\n#!/bin/bash\n#\n# Copyright Istio Authors\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n\nset -ox errexit\n\n# Get to the root directory of the repo...\nSCRIPTDIR=$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\ncd \"$SCRIPTDIR/../../..\"\n\nh=\"${BOOKINFO_HUB:?BOOKINFO_HUB must be set}\"\nt=\"${BOOKINFO_TAG:?BOOKINFO_TAG must be set}\"\nif [[ (\"${h}\" == \"istio\" || \"${h}\" == \"docker.io/istio\") && -z \"$CI\" && \"$*\" =~ \"--push\" ]]; then\n  echo \"Can only push to prod registry in CI\"\n  exit 1\nfi\n\nif [[ \"${BOOKINFO_LATEST}\" == \"true\" ]]; then\n  BOOKINFO_TAG=\"${BOOKINFO_TAG},latest\"\nfi\n\n# Pass input args to the command. This allows using --push, --load, etc\nenv TAGS=\"${BOOKINFO_TAG}\" HUB=\"${BOOKINFO_HUB}\" \\\n  docker buildx bake -f samples/bookinfo/src/docker-bake.hcl \"$@\"\n\nif [[ \"${BOOKINFO_UPDATE}\" == \"true\" ]]; then\n# Update image references in the yaml files\n  find ./samples/bookinfo/platform -name \"*bookinfo*.yaml\" -exec sed -i.bak \"s#image:.*\\\\(\\\\/examples-bookinfo-.*\\\\):.*#image: ${h//\\//\\\\/}\\\\1:$t#g\" {} +\nfi\n```", "metadata": {"source": "github_repository", "repository": "istio/istio", "file_path": "build-services.sh", "category": "build", "authentic": true}}
{"prompt": "Write a shell script for Kubernetes resource management", "completion": "```bash\n#!/bin/bash\n\n# Copyright 2019 Istio Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Usage: ./integ-suite-kind.sh TARGET\n# Example: ./integ-suite-kind.sh test.integration.pilot.kube.presubmit\n\nWD=$(dirname \"$0\")\nWD=$(cd \"$WD\"; pwd)\nROOT=$(dirname \"$WD\")\n\n# Exit immediately for non zero status\nset -e\n# Check unset variables\nset -u\n# Print commands\nset -x\n\n# shellcheck source=prow/lib.sh\nsource \"${ROOT}/prow/lib.sh\"\nsetup_and_export_git_sha\n\n# shellcheck source=common/scripts/kind_provisioner.sh\nsource \"${ROOT}/common/scripts/kind_provisioner.sh\"\n\nTOPOLOGY=SINGLE_CLUSTER\nNODE_IMAGE=\"gcr.io/istio-testing/kind-node:v1.32.0\"\nKIND_CONFIG=\"\"\nCLUSTER_TOPOLOGY_CONFIG_FILE=\"${ROOT}/prow/config/topology/multicluster.json\"\nCLUSTER_NAME=\"${CLUSTER_NAME:-istio-testing}\"\n\nexport FAST_VM_BUILDS=true\nexport ISTIO_DOCKER_BUILDER=\"${ISTIO_DOCKER_BUILDER:-crane}\"\n# DEVCONTAINER controls a set of features that allow this script to be run from\n# within a dev container using ghcr.io/devcontainers/features/docker-outside-of-docker\nexport DEVCONTAINER=\"${DEVCONTAINER:-}\"\nif [[ \"${DEVCONTAINER}\" ]]; then\n  export ISTIO_DOCKER_BUILDER=docker\nfi\n\nPARAMS=()\n\nwhile (( \"$#\" )); do\n  case \"$1\" in\n    # Node images can be found at https://github.com/kubernetes-sigs/kind/releases\n    # For example, kindest/node:v1.14.0\n    --node-image)\n      NODE_IMAGE=$2\n      shift 2\n    ;;\n    # Config for enabling different Kubernetes features in KinD (see prow/config{endpointslice.yaml,trustworthy-jwt.yaml}).\n    --kind-config)\n    KIND_CONFIG=$2\n    shift 2\n    ;;\n    --skip-setup)\n      SKIP_SETUP=true\n      shift\n    ;;\n    --skip-cleanup)\n      SKIP_CLEANUP=true\n      shift\n    ;;\n    --skip-build)\n      SKIP_BUILD=true\n      shift\n    ;;\n    --manual)\n      MANUAL=true\n      shift\n    ;;\n    --topology)\n      case $2 in\n        # TODO(landow) get rid of MULTICLUSTER_SINGLE_NETWORK after updating Prow job\n        SINGLE_CLUSTER | MULTICLUSTER_SINGLE_NETWORK | MULTICLUSTER | AMBIENT_MULTICLUSTER )\n          TOPOLOGY=$2\n          echo \"Running with topology ${TOPOLOGY}\"\n          ;;\n        *)\n          echo \"Error: Unsupported topology ${TOPOLOGY}\" >&2\n          exit 1\n          ;;\n      esac\n      shift 2\n    ;;\n    --topology-config)\n      CLUSTER_TOPOLOGY_CONFIG_FILE=\"${ROOT}/${2}\"\n      shift 2\n    ;;\n    -*)\n      echo \"Error: Unsupported flag $1\" >&2\n      exit 1\n      ;;\n    *) # preserve positional arguments\n      PARAMS+=(\"$1\")\n      shift\n      ;;\n  esac\ndone\n\nif [ -f /proc/cpuinfo ]; then\n  echo \"Checking CPU...\"\n  grep 'model' /proc/cpuinfo || true\nfi\n\n# Default IP family of the cluster is IPv4\nKIND_IP_FAMILY=\"ipv4\"\nexport IP_FAMILIES=\"${IP_FAMILIES:-IPv4}\"\nif [[ \"$IP_FAMILIES\" == \"IPv6\" ]]; then\n   KIND_IP_FAMILY=\"ipv6\"\nelif [[ \"$IP_FAMILIES\" =~ \"IPv6\" ]] && [[ \"$IP_FAMILIES\" =~ \"IPv4\" ]]; then\n   KIND_IP_FAMILY=\"dual\"\nfi\nexport KIND_IP_FAMILY\n\n# LoadBalancer in Kind is supported using metallb\nexport TEST_ENV=kind-metallb\n\n# See https://kind.sigs.k8s.io/docs/user/quick-start/#loading-an-image-into-your-cluster\nexport PULL_POLICY=IfNotPresent\n\n# We run a local-registry in a docker container that KinD nodes pull from\n# These values are must match what is in config/trustworthy-jwt.yaml\nexport KIND_REGISTRY_NAME=\"kind-registry\"\nexport KIND_REGISTRY_PORT=\"5000\"\nexport KIND_REGISTRY=\"localhost:${KIND_REGISTRY_PORT}\"\nexport KIND_REGISTRY_DIR=\"/etc/containerd/certs.d/localhost:${KIND_REGISTRY_PORT}\"\n\nexport HUB=${HUB:-\"istio-testing\"}\nexport TAG=\"${TAG:-\"istio-testing\"}\"\nexport VARIANT\n\n# If we're not intending to pull from an actual remote registry, use the local kind registry\nif [[ -z \"${SKIP_BUILD:-}\" ]]; then\n  HUB=\"${KIND_REGISTRY}\"\n  export HUB\nfi\n\n# Setup junit report and verbose logging\nexport T=\"${T:-\"-v -count=1\"}\"\nexport CI=\"true\"\n\nexport ARTIFACTS=\"${ARTIFACTS:-$(mktemp -d)}\"\ntrace \"init\" make init\n\nif [[ -z \"${SKIP_SETUP:-}\" ]]; then\n  export DEFAULT_CLUSTER_YAML=\"./prow/config/default.yaml\"\n  export METRICS_SERVER_CONFIG_DIR='./prow/config/metrics'\n\n  if [[ \"${TOPOLOGY}\" == \"SINGLE_CLUSTER\" ]]; then\n    trace \"setup kind cluster\" setup_kind_cluster_retry \"${CLUSTER_NAME}\" \"${NODE_IMAGE}\" \"${KIND_CONFIG}\"\n  else\n    trace \"load cluster topology\" load_cluster_topology \"${CLUSTER_TOPOLOGY_CONFIG_FILE}\"\n    trace \"setup kind clusters\" setup_kind_clusters \"${NODE_IMAGE}\" \"${KIND_IP_FAMILY}\"\n\n    TOPOLOGY_JSON=$(cat \"${CLUSTER_TOPOLOGY_CONFIG_FILE}\")\n    for i in $(seq 0 $((${#CLUSTER_NAMES[@]} - 1))); do\n      CLUSTER=\"${CLUSTER_NAMES[i]}\"\n      KCONFIG=\"${KUBECONFIGS[i]}\"\n      TOPOLOGY_JSON=$(set_topology_value \"${TOPOLOGY_JSON}\" \"${CLUSTER}\" \"meta.kubeconfig\" \"${KCONFIG}\")\n    done\n    RUNTIME_TOPOLOGY_CONFIG_FILE=\"${ARTIFACTS}/topology-config.json\"\n    echo \"${TOPOLOGY_JSON}\" > \"${RUNTIME_TOPOLOGY_CONFIG_FILE}\"\n\n    export INTEGRATION_TEST_TOPOLOGY_FILE\n    INTEGRATION_TEST_TOPOLOGY_FILE=\"${RUNTIME_TOPOLOGY_CONFIG_FILE}\"\n\n    export INTEGRATION_TEST_KUBECONFIG\n    INTEGRATION_TEST_KUBECONFIG=NONE\n  fi\nfi\n\nif [[ -z \"${SKIP_BUILD:-}\" ]]; then\n  trace \"setup kind registry\" setup_kind_registry\n  trace \"build images\" build_images \"${PARAMS[*]}\"\n\n  # upload WASM plugins to kind-registry\n  registry_url=$(if [ -z \"$DEVCONTAINER\" ]; then echo \"localhost\"; else echo $KIND_REGISTRY_NAME; fi):$KIND_REGISTRY_PORT\n  crane copy gcr.io/istio-testing/wasm/attributegen:<TOKEN> \"$registry_url/istio-testing/wasm/attributegen:0.0.1\" --insecure\n  crane copy gcr.io/istio-testing/wasm/header-injector:0.0.1 \"$registry_url/istio-testing/wasm/header-injector:0.0.1\" --insecure\n  crane copy gcr.io/istio-testing/wasm/header-injector:0.0.2 \"$registry_url/istio-testing/wasm/header-injector:0.0.2\" --insecure\n\n  # Make \"kind-registry\" resolvable in IPv6 cluster\n  if [[ \"$KIND_IP_FAMILY\" == \"ipv6\" ]]; then\n    kind_registry_ip=$(docker inspect -f '{{range $k, $v := .NetworkSettings.Networks}}{{if eq $k \"kind\"}}{{.GlobalIPv6Address}}{{end}}{{end}}' kind-registry)\n    coredns_config=$(kubectl get -oyaml -n=kube-system configmap/coredns)\n    echo \"Current CoreDNS config:\"\n    echo \"${coredns_config}\"\n    patched_coredns_config=$(kubectl get -oyaml -n=kube-system configmap/coredns | sed -e '/^ *ready/i\\\n        hosts {\\\n            '\"$kind_registry_ip\"' kind-registry.lan\\\n            '\"$kind_registry_ip\"' kind-registry.\\\n            fallthrough\\\n        }')\n    echo \"Patched CoreDNS config:\"\n    echo \"${patched_coredns_config}\"\n    printf '%s' \"${patched_coredns_config}\" | kubectl apply -f -\n  fi\n  # CoreDNS by default caches Kubernetes objects for 30s. This leads to problematic timing issues when we tear down + re-install\n  # in our tests. We will negative-cache the object, adding up to 30s on each test suite.\n  # See https://github.com/coredns/coredns/pull/2348\n  kubectl get -oyaml -n=kube-system configmap/coredns | sed 's/ttl 30/ttl 0/g' | kubectl apply -f -\nfi\n\n# Run the test target if provided.\nif [[ -n \"${PARAMS:-}\" ]]; then\n  trace \"test\" make \"${PARAMS[*]}\"\nfi\n\n# Check if the user is running the clusters in manual mode.\nif [[ -n \"${MANUAL:-}\" ]]; then\n  echo \"Running cluster(s) in manual mode. Press any key to shutdown and exit...\"\n  read -rsn1\n  exit 0\nfi\n```", "metadata": {"source": "github_repository", "repository": "istio/istio", "file_path": "integ-suite-kind.sh", "category": "build", "authentic": true}}
{"prompt": "Write a shell script for  testing automation", "completion": "```bash\n#!/bin/bash\n\n# Copyright Istio Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nWD=$(dirname \"$0\")\nWD=$(cd \"$WD\"; pwd)\nROOT=$(dirname \"$WD\")\n\n# shellcheck source=prow/lib.sh\nsource \"${ROOT}/prow/lib.sh\"\nsetup_and_export_git_sha\n\nset -eux\n\nGCS_BENCHMARK_DIR=\"${GCS_BENCHMARK_DIR:-istio-prow/benchmarks}\"\n\nBENCHMARK_COUNT=\"${BENCHMARK_COUNT:-5}\"\nBENCHMARK_CPUS=\"${BENCHMARK_CPUS:-8}\"\n\nREPORT_JUNIT=\"${REPORT_JUNIT:-${ARTIFACTS}/junit_benchmarks.xml}\"\nREPORT_PLAINTEXT=\"${REPORT_PLAINTEXT:-${ARTIFACTS}/benchmark-log.txt}\"\n\n# Sha we should compare against. Defaults to the PULL_BASE_SHA, which is the last commit on the branch we are on.\n# For example, a PR on master will compare to the HEAD of master.\nCOMPARE_GIT_SHA=\"${COMPARE_GIT_SHA:-${PULL_BASE_SHA:-${GIT_SHA}}}\"\n\ncase \"${1}\" in\n  run)\n    shift\n    benchmarkjunit \"$@\" -l \"${REPORT_PLAINTEXT}\" --output=\"${REPORT_JUNIT}\" \\\n      --test-arg \"--benchmem\" \\\n      --test-arg \"--count=${BENCHMARK_COUNT}\" \\\n      --test-arg \"--cpu=${BENCHMARK_CPUS}\" \\\n      --test-arg \"--test.timeout=60m\" \\\n      --test-arg \"-tags=vtprotobuf\"\n    # Print out the results as well for ease of debugging, so they are in the logs instead of just output\n    cat \"${REPORT_PLAINTEXT}\"\n    ;;\n  report)\n    # Upload the reports to a well known path based on git SHA\n    gsutil cp \"${REPORT_JUNIT}\" \"gs://${GCS_BENCHMARK_DIR}/${GIT_SHA}.xml\"\n    gsutil cp \"${REPORT_PLAINTEXT}\" \"gs://${GCS_BENCHMARK_DIR}/${GIT_SHA}.txt\"\n    ;;\n  compare)\n    # Fetch previous results, and compare them.\n    curl \"https://storage.googleapis.com/${GCS_BENCHMARK_DIR}/${COMPARE_GIT_SHA}.txt\" > \"${ARTIFACTS}/baseline-benchmark-log.txt\"\n    benchstat \"${ARTIFACTS}/baseline-benchmark-log.txt\" \"${REPORT_PLAINTEXT}\"\n    ;;\n  *)\n    echo \"unknown command, expect report, run, or compare.\"\n    ;;\nesac\n```", "metadata": {"source": "github_repository", "repository": "istio/istio", "file_path": "benchtest.sh", "category": "testing", "authentic": true}}
{"prompt": "Create a shell script for  automation", "completion": "```bash\n#!/bin/bash\n\n# Copyright 2019 Istio Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nset -eu\n\nif [ -z ${CI+x} ]; then\n  # When not in CI, do nothing\n  exit 0;\nfi\n\necho \"Fetching docker container status\"\ndocker ps || true\necho \"Fetching buildx logs\"\ndocker logs buildx_buildkit_container-builder0 || true\necho \"Fetching docker logs\"\ncat /var/log/docker.log || true\necho \"Fetching local registry logs\"\ndocker logs kind-registry || true\n```", "metadata": {"source": "github_repository", "repository": "istio/istio", "file_path": "dump-docker-logs.sh", "category": "build", "authentic": true}}
{"prompt": "Create a shell script for Terraform infrastructure deployment", "completion": "```bash\n#!/bin/bash\n# Copyright (c) HashiCorp, Inc.\n# SPDX-License-Identifier: BUSL-1.1\n\n\n# For normal use this package can just be tested with \"go test\" as standard,\n# but this script is an alternative to allow the tests to be run somewhere\n# other than where they are built.\n\n# The primary use for this is cross-compilation, where e.g. we can produce an\n# archive that can be extracted on a Windows system to run the e2e tests there:\n#    $ GOOS=windows GOARCH=amd64 ./make-archive.sh\n#\n# This will produce a zip file build/terraform-e2etest_windows_amd64.zip which\n# can be shipped off to a Windows amd64 system, extracted to some directory,\n# and then executed as follows:\n#    set TF_ACC=1\n#    ./e2etest.exe\n#\n# Because separated e2etest harnesses are intended for testing against \"real\"\n# release executables, the generated archives don't include a copy of\n# the Terraform executable. Instead, the caller of the tests must retrieve\n# and extract a release package into the working directory before running\n# the e2etest executable, so that \"e2etest\" can find and execute it.\n\nset +euo pipefail\n\n# Always run from the directory where this script lives\ncd \"$( dirname \"${BASH_SOURCE[0]}\" )\"\n\nGOOS=\"$(go env GOOS)\"\nGOARCH=\"$(go env GOARCH)\"\nGOEXE=\"$(go env GOEXE)\"\nOUTDIR=\"build/${GOOS}_${GOARCH}\"\nOUTFILE=\"terraform-e2etest_${GOOS}_${GOARCH}.zip\"\n\nLDFLAGS=\"-X github.com/hashicorp/terraform/internal/command/e2etest.terraformBin=./terraform$GOEXE\"\n# Caller may pass in the environment variable GO_LDFLAGS with additional\n# flags we'll use when building.\nif [ -n \"${GO_LDFLAGS+set}\" ]; then\n    LDFLAGS=\"${GO_LDFLAGS} ${LDFLAGS}\"\nfi\n\nmkdir -p \"$OUTDIR\"\n\n# We need the test fixtures available when we run the tests.\ncp -r testdata \"$OUTDIR/testdata\"\n\n# Build the test program\ngo test -o \"$OUTDIR/e2etest$GOEXE\" -c -ldflags \"$LDFLAGS\" github.com/hashicorp/terraform/internal/command/e2etest\n\n# Now bundle it all together for easy shipping!\ncd \"$OUTDIR\"\nzip -r \"../$OUTFILE\" *\n\necho \"e2etest archive created at build/$OUTFILE\"\n```", "metadata": {"source": "github_repository", "repository": "hashicorp/terraform", "file_path": "make-archive.sh", "category": "build", "authentic": true}}
{"prompt": "Create a shell script for Terraform infrastructure deployment", "completion": "```bash\n# Copyright (c) HashiCorp, Inc.\n# SPDX-License-Identifier: BUSL-1.1\n\n######################################################\n# NOTE: This file is managed by the Digital Team's   #\n# Terraform configuration @ hashicorp/mktg-terraform #\n######################################################\n\n# Repo which we are cloning and executing npm run build:deploy-preview within\nREPO_TO_CLONE=dev-portal\n# Set the subdirectory name for the dev-portal app\nPREVIEW_DIR=website-preview\n# The product for which we are building the deploy preview\nPRODUCT=terraform\n# Preview mode, controls the UI rendered (either the product site or developer). Can be `io` or `developer`\nPREVIEW_MODE=developer\n\n# Get the git branch of the commit that triggered the deploy preview\n# This will power remote image assets in local and deploy previews\nCURRENT_GIT_BRANCH=$(git rev-parse --abbrev-ref HEAD)\n\n# This is where content files live, relative to the website-preview dir. If omitted, \"../content\" will be used\nLOCAL_CONTENT_DIR=../docs\n\nshould_pull=true\n\n# Clone the dev-portal project, if needed\nif [ ! -d \"$PREVIEW_DIR\" ]; then\n    echo \"⏳ Cloning the $REPO_TO_CLONE repo, this might take a while...\"\n    git clone --depth=1 https://github.com/hashicorp/$REPO_TO_CLONE.git \"$PREVIEW_DIR\"\n    should_pull=false\nfi\n\ncd \"$PREVIEW_DIR\"\n\n# If the directory already existed, pull to ensure the clone is fresh\nif [ \"$should_pull\" = true ]; then\n    git pull origin main\nfi\n\n# Run the dev-portal content-repo start script\nREPO=$PRODUCT \\\nPREVIEW_FROM_REPO=$PRODUCT \\\nLOCAL_CONTENT_DIR=$LOCAL_CONTENT_DIR \\\nCURRENT_GIT_BRANCH=$CURRENT_GIT_BRANCH \\\nPREVIEW_MODE=$PREVIEW_MODE \\\nnpm run start:local-preview\n```", "metadata": {"source": "github_repository", "repository": "hashicorp/terraform", "file_path": "website-start.sh", "category": "deployment", "authentic": true}}
{"prompt": "Write a shell script to build Terraform infrastructure components", "completion": "```bash\n# Copyright (c) HashiCorp, Inc.\n# SPDX-License-Identifier: BUSL-1.1\n\n######################################################\n# NOTE: This file is managed by the Digital Team's   #\n# Terraform configuration @ hashicorp/mktg-terraform #\n######################################################\n\n# Repo which we are cloning and executing npm run build:deploy-preview within\nREPO_TO_CLONE=dev-portal\n# Set the subdirectory name for the base project\nPREVIEW_DIR=website-preview\n# The directory we want to clone the project into\nCLONE_DIR=website-preview\n# The product for which we are building the deploy preview\nPRODUCT=terraform\n# Preview mode, controls the UI rendered (either the product site or developer). Can be `io` or `developer`\nPREVIEW_MODE=developer\n\n# Get the git branch of the commit that triggered the deploy preview\n# This will power remote image assets in local and deploy previews\nCURRENT_GIT_BRANCH=$VERCEL_GIT_COMMIT_REF\n\n# This is where content files live, relative to the website-preview dir. If omitted, \"../content\" will be used\nLOCAL_CONTENT_DIR=../docs\n\nfrom_cache=false\n\nif [ -d \"$PREVIEW_DIR\" ]; then\n  echo \"$PREVIEW_DIR found\"\n  CLONE_DIR=\"$PREVIEW_DIR-tmp\"\n  from_cache=true\nfi\n\n# Clone the base project, if needed\necho \"⏳ Cloning the $REPO_TO_CLONE repo, this might take a while...\"\ngit clone --depth=1 \"https://github.com/hashicorp/$REPO_TO_CLONE.git\" \"$CLONE_DIR\"\n\nif [ \"$from_cache\" = true ]; then\n  echo \"Setting up $PREVIEW_DIR\"\n  cp -R \"./$CLONE_DIR/.\" \"./$PREVIEW_DIR\"\nfi\n\n# cd into the preview directory project\ncd \"$PREVIEW_DIR\"\n\n# Run the build:deploy-preview start script\nPREVIEW_FROM_REPO=$PRODUCT \\\nIS_CONTENT_PREVIEW=true \\\nPREVIEW_MODE=$PREVIEW_MODE \\\nREPO=$PRODUCT \\\nHASHI_ENV=project-preview \\\nLOCAL_CONTENT_DIR=$LOCAL_CONTENT_DIR \\\nCURRENT_GIT_BRANCH=$CURRENT_GIT_BRANCH \\\nnpm run build:deploy-preview\n```", "metadata": {"source": "github_repository", "repository": "hashicorp/terraform", "file_path": "website-build.sh", "category": "deployment", "authentic": true}}
{"prompt": "Write a shell script to build Terraform infrastructure components", "completion": "```bash\n#!/usr/bin/env bash\n# Copyright (c) HashiCorp, Inc.\n# SPDX-License-Identifier: BUSL-1.1\n\n\n######################################################\n# NOTE: This file is managed by the Digital Team's   #\n# Terraform configuration @ hashicorp/mktg-terraform #\n######################################################\n\n# This is run during the website build step to determine if we should skip the build or not.\n# More information: https://vercel.com/docs/platform/projects#ignored-build-step\n\nif [[ \"$VERCEL_GIT_COMMIT_REF\" == \"stable-website\"  ]] ; then\n  # Proceed with the build if the branch is stable-website\n  echo \"✅ - Build can proceed\"\n  exit 1;\nelse\n  # Check for differences in the website directory\n  git diff --quiet HEAD^ HEAD ./\nfi\n```", "metadata": {"source": "github_repository", "repository": "hashicorp/terraform", "file_path": "should-build.sh", "category": "build", "authentic": true}}
{"prompt": "Create a shell script for  automation", "completion": "```bash\n#!/usr/bin/env bash\n# Copyright (c) HashiCorp, Inc.\n# SPDX-License-Identifier: BUSL-1.1\n\nfail() {\n  echo \"$1\" 1>&2\n  exit 1\n}\n\n[[ -z \"$VAULT_ADDR\" ]] && fail \"VAULT_ADDR env variable has not been set\"\nbinpath=${VAULT_INSTALL_DIR}/vault\ntest -x \"$binpath\" || fail \"unable to locate vault binary at $binpath\"\n\nif ! out=$(sudo systemctl stop vault 2>&1); then\n  fail \"failed to stop vault: $out: $(sudo systemctl status vault)\"\nfi\n\nif ! out=$(sudo systemctl daemon-reload 2>&1); then\n  fail \"failed to daemon-reload systemd: $out\" 1>&2\nfi\n\nif ! out=$(sudo systemctl start vault 2>&1); then\n  fail \"failed to start vault: $out: $(sudo systemctl status vault)\"\nfi\n\ncount=0\nretries=5\nwhile :; do\n  # Check the Vault seal status\n  status=$($binpath status)\n  code=$?\n\n  if [ $code == 0 ] || [ $code == 2 ]; then\n    # 0 is unsealed and 2 is running but sealed\n    echo \"$status\"\n    exit 0\n  fi\n\n  printf \"Waiting for Vault cluster to be ready: status code: %s, status:\\n%s\\n\" \"$code\" \"$status\" 2>&1\n\n  wait=$((3 ** count))\n  count=$((count + 1))\n  if [ \"$count\" -lt \"$retries\" ]; then\n      sleep \"$wait\"\n  else\n      fail \"Timed out waiting for Vault node to be ready after restart\"\n  fi\ndone\n```", "metadata": {"source": "github_repository", "repository": "hashicorp/vault", "file_path": "restart-vault.sh", "category": "testing", "authentic": true}}
{"prompt": "Create a shell script for  automation", "completion": "```bash\n#!/usr/bin/env bash\n# Copyright (c) HashiCorp, Inc.\n# SPDX-License-Identifier: BUSL-1.1\n\nset -e\n\nfail() {\n  echo \"$1\" 1>&2\n  exit 1\n}\n\n[[ -z \"$AES_LABEL\" ]] && fail \"AES_LABEL env variable has not been set\"\n[[ -z \"$HMAC_LABEL\" ]] && fail \"HMAC_LABEL env variable has not been set\"\n[[ -z \"$PIN\" ]] && fail \"PIN env variable has not been set\"\n[[ -z \"$SO_PIN\" ]] && fail \"SO_PIN env variable has not been set\"\n[[ -z \"$TOKEN_LABEL\" ]] && fail \"TOKEN_LABEL env variable has not been set\"\n[[ -z \"$TOKEN_DIR\" ]] && fail \"TOKEN_DIR env variable has not been set\"\n\nif ! type softhsm2-util &> /dev/null; then\n  fail \"unable to locate softhsm2-util in PATH. Have you installed softhsm?\"\nfi\n\nif ! type pkcs11-tool &> /dev/null; then\n  fail \"unable to locate pkcs11-tool in PATH. Have you installed opensc?\"\nfi\n\n# Create an HSM slot and return the slot number in decimal value.\ncreate_slot() {\n  sudo softhsm2-util --init-token --free --so-pin=\"$SO_PIN\" --pin=\"$PIN\" --label=\"$TOKEN_LABEL\" | grep -oE '[0-9]+$'\n}\n\n# Find the location of our softhsm shared object.\nfind_softhsm_so() {\n  sudo find /usr -type f -name libsofthsm2.so -print -quit\n}\n\n# Create key a key in the slot. Args: module, key label, id number, key type\nkeygen() {\n  sudo pkcs11-tool --keygen --usage-sign --private --sensitive --usage-wrap \\\n    --module \"$1\" \\\n    -p \"$PIN\" \\\n    --token-label \"$TOKEN_LABEL\" \\\n    --label \"$2\" \\\n    --id \"$3\" \\\n    --key-type \"$4\"\n}\n\n# Create our softhsm slot and keys\nmain() {\n  local slot\n  if ! slot=$(create_slot); then\n    fail \"failed to create softhsm token slot\"\n  fi\n\n  local so\n  if ! so=$(find_softhsm_so); then\n    fail \"unable to locate libsofthsm2.so shared object\"\n  fi\n\n  if ! keygen \"$so\" \"$AES_LABEL\" 1 'AES:32' 1>&2; then\n    fail \"failed to create AES key\"\n  fi\n\n  if ! keygen \"$so\" \"$HMAC_LABEL\" 2 'GENERIC:32' 1>&2; then\n    fail \"failed to create HMAC key\"\n  fi\n\n  # Return our seal configuration attributes as JSON\n  cat << EOF\n{\n  \"lib\": \"${so}\",\n  \"slot\": \"${slot}\",\n  \"pin\": \"${PIN}\",\n  \"key_label\": \"${AES_LABEL}\",\n  \"hmac_key_label\": \"${HMAC_LABEL}\",\n  \"generate_key\": \"false\"\n}\nEOF\n  exit 0\n}\n\nmain\n```", "metadata": {"source": "github_repository", "repository": "hashicorp/vault", "file_path": "create-keys.sh", "category": "automation", "authentic": true}}
{"prompt": "Write a shell script to install and setup ", "completion": "```bash\n#!/usr/bin/env bash\n# Copyright (c) HashiCorp, Inc.\n# SPDX-License-Identifier: BUSL-1.1\n\nset -e\n\nfail() {\n  echo \"$1\" 1>&2\n  exit 1\n}\n\n[[ -z \"${RETRY_INTERVAL}\" ]] && fail \"RETRY_INTERVAL env variable has not been set\"\n[[ -z \"${TIMEOUT_SECONDS}\" ]] && fail \"TIMEOUT_SECONDS env variable has not been set\"\n[[ -z \"${PACKAGES}\" ]] && fail \"PACKAGES env variable has not been set\"\n[[ -z \"${PACKAGE_MANAGER}\" ]] && fail \"PACKAGE_MANAGER env variable has not been set\"\n\n# Install packages based on the provided packages and package manager. We assume that the repositories\n# have already been synchronized by the repo setup that is a prerequisite for this script.\ninstall_packages() {\n  if [[ \"${PACKAGES}\" = \"__skip\" ]]; then\n    return 0\n  fi\n\n  set -x\n  echo \"Installing Dependencies: ${PACKAGES}\"\n\n  # Use the default package manager of the current Linux distro to install packages\n  case $PACKAGE_MANAGER in\n    apt)\n      for package in ${PACKAGES}; do\n        if dpkg -s \"${package}\"; then\n          echo \"Skipping installation of ${package} because it is already installed\"\n          continue\n        else\n          echo \"Installing ${package}\"\n          local output\n          if ! output=$(sudo apt install -y \"${package}\" 2>&1); then\n            echo \"Failed to install ${package}: ${output}\" 1>&2\n            return 1\n          fi\n        fi\n      done\n      ;;\n    dnf)\n      for package in ${PACKAGES}; do\n        if rpm -q \"${package}\"; then\n          echo \"Skipping installation of ${package} because it is already installed\"\n          continue\n        else\n          echo \"Installing ${package}\"\n          local output\n          if ! output=$(sudo dnf -y install \"${package}\" 2>&1); then\n            echo \"Failed to install ${package}: ${output}\" 1>&2\n            return 1\n          fi\n        fi\n      done\n      ;;\n    yum)\n      for package in ${PACKAGES}; do\n        if rpm -q \"${package}\"; then\n          echo \"Skipping installation of ${package} because it is already installed\"\n          continue\n        else\n          echo \"Installing ${package}\"\n          local output\n          if ! output=$(sudo yum -y install \"${package}\" 2>&1); then\n            echo \"Failed to install ${package}: ${output}\" 1>&2\n            return 1\n          fi\n        fi\n      done\n      ;;\n    zypper)\n      for package in ${PACKAGES}; do\n        if rpm -q \"${package}\"; then\n          echo \"Skipping installation of ${package} because it is already installed\"\n          continue\n        else\n          echo \"Installing ${package}\"\n          local output\n          if ! output=$(sudo zypper --non-interactive install -y -l --force-resolution \"${package}\" 2>&1); then\n            echo \"Failed to install ${package}: ${output}\" 1>&2\n            return 1\n          fi\n        fi\n      done\n      ;;\n    *)\n      fail \"No matching package manager provided.\"\n      ;;\n  esac\n}\n\nbegin_time=$(date +%s)\nend_time=$((begin_time + TIMEOUT_SECONDS))\nwhile [[ \"$(date +%s)\" -lt \"${end_time}\" ]]; do\n  if install_packages; then\n    exit 0\n  fi\n\n  sleep \"${RETRY_INTERVAL}\"\ndone\n\nfail \"Timed out waiting for packages to install\"\n```", "metadata": {"source": "github_repository", "repository": "hashicorp/vault", "file_path": "install-packages.sh", "category": "infrastructure", "authentic": true}}
{"prompt": "Create a shell script for Docker image building and deployment", "completion": "```bash\n#!/bin/bash\n\nset -ex\n\n./script/clean\n\nDOCKER_COMPOSE_GITSHA=\"$(script/build/write-git-sha)\"\n\ndocker build . \\\n       --target bin \\\n       --build-arg DISTRO=debian \\\n       --build-arg GIT_COMMIT=\"${DOCKER_COMPOSE_GITSHA}\" \\\n       --output dist/\nARCH=$(uname -m)\n# Ensure that we output the binary with the same name as we did before\nmv dist/docker-compose-linux-amd64 \"dist/docker-compose-Linux-${ARCH}\"\n```", "metadata": {"source": "github_raw", "repository": "docker/compose", "file_path": "script/build/linux", "category": "build", "authentic": true}}
{"prompt": "Write a shell script to build Terraform infrastructure components", "completion": "```bash\n#!/usr/bin/env bash\n# Copyright (c) HashiCorp, Inc.\n# SPDX-License-Identifier: BUSL-1.1\n\n#\n# This script builds the application from source for multiple platforms.\n\n# Get the parent directory of where this script is.\nSOURCE=\"${BASH_SOURCE[0]}\"\nwhile [ -h \"$SOURCE\" ] ; do SOURCE=\"$(readlink \"$SOURCE\")\"; done\nDIR=\"$( cd -P \"$( dirname \"$SOURCE\" )/..\" && pwd )\"\n\n# Change into that directory\ncd \"$DIR\" || exit\n\n# Determine the arch/os combos we're building for\nXC_ARCH=${XC_ARCH:-\"386 amd64 arm\"}\nXC_OS=${XC_OS:-linux darwin windows freebsd openbsd solaris}\nXC_EXCLUDE_OSARCH=\"!darwin/arm !darwin/386\"\n\n# Delete the old dir\necho \"==> Removing old directory...\"\nrm -f bin/*\nrm -rf pkg/*\nmkdir -p bin/\n\n# If its dev mode, only build for ourself\nif [[ -n \"${TF_DEV}\" ]]; then\n    XC_OS=$(go env GOOS)\n    XC_ARCH=$(go env GOARCH)\nfi\n\nif ! which gox > /dev/null; then\n    echo \"==> Installing gox...\"\n    go install github.com/mitchellh/gox\nfi\n\n# Instruct gox to build statically linked binaries\nexport CGO_ENABLED=0\n\n# Set module download mode to readonly to not implicitly update go.mod\nexport GOFLAGS=\"-mod=readonly\"\n\n# In release mode we don't want debug information in the binary and we don't\n# want the -dev version marker\nif [[ -n \"${TF_RELEASE}\" ]]; then\n    LD_FLAGS=\"-s -w -X 'github.com/hashicorp/terraform/version.dev=no'\"\nfi\n\n# Ensure all remote modules are downloaded and cached before build so that\n# the concurrent builds launched by gox won't race to redundantly download them.\ngo mod download\n\n# Build!\necho \"==> Building...\"\ngox \\\n    -os=\"${XC_OS}\" \\\n    -arch=\"${XC_ARCH}\" \\\n    -osarch=\"${XC_EXCLUDE_OSARCH}\" \\\n    -ldflags \"${LD_FLAGS}\" \\\n    -output \"pkg/{{.OS}}_{{.Arch}}/terraform\" \\\n    .\n\n# Move all the compiled things to the $GOPATH/bin\nGOPATH=${GOPATH:-$(go env GOPATH)}\ncase $(uname) in\n    CYGWIN*)\n        GOPATH=\"$(cygpath $GOPATH)\"\n        ;;\nesac\nOLDIFS=$IFS\nIFS=: MAIN_GOPATH=($GOPATH)\nIFS=$OLDIFS\n\n# Create GOPATH/bin if it's doesn't exists\nif [ ! -d $MAIN_GOPATH/bin ]; then\n    echo \"==> Creating GOPATH/bin directory...\"\n    mkdir -p $MAIN_GOPATH/bin\nfi\n\n# Copy our OS/Arch to the bin/ directory\nDEV_PLATFORM=\"./pkg/$(go env GOOS)_$(go env GOARCH)\"\nif [[ -d \"${DEV_PLATFORM}\" ]]; then\n    for F in $(find ${DEV_PLATFORM} -mindepth 1 -maxdepth 1 -type f); do\n        cp ${F} bin/\n        cp ${F} ${MAIN_GOPATH}/bin/\n    done\nfi\n\nif [ \"${TF_DEV}x\" = \"x\" ]; then\n    # Zip and copy to the dist dir\n    echo \"==> Packaging...\"\n    for PLATFORM in $(find ./pkg -mindepth 1 -maxdepth 1 -type d); do\n        OSARCH=$(basename ${PLATFORM})\n        echo \"--> ${OSARCH}\"\n\n        pushd $PLATFORM >/dev/null 2>&1\n        zip ../${OSARCH}.zip ./*\n        popd >/dev/null 2>&1\n    done\nfi\n\n# Done!\necho\necho \"==> Results:\"\nls -hl bin/\n```", "metadata": {"source": "github_raw", "repository": "hashicorp/terraform", "file_path": "scripts/build.sh", "category": "build", "authentic": true}}
{"prompt": "Write a shell script to build  components", "completion": "```bash\n#!/usr/bin/env bash\n# Copyright (c) HashiCorp, Inc.\n# SPDX-License-Identifier: BUSL-1.1\n\n#\n# This script builds the application from source for multiple platforms.\nset -e\n\nGO_CMD=${GO_CMD:-go}\n\n# Get the parent directory of where this script is.\nSOURCE=\"${BASH_SOURCE[0]}\"\nSOURCE_DIR=$( dirname \"$SOURCE\" )\nwhile [ -h \"$SOURCE\" ] ; do SOURCE=\"$(readlink \"$SOURCE\")\"; done\nDIR=\"$( cd -P \"$SOURCE_DIR/..\" && pwd )\"\n\n# Change into that directory\ncd \"$DIR\"\n\n# Set build tags\nBUILD_TAGS=\"${BUILD_TAGS:-\"vault\"}\"\n\n# Get the git commit\nGIT_COMMIT=\"$(\"$SOURCE_DIR\"/ci-helper.sh revision)\"\nGIT_DIRTY=\"$(test -n \"`git status --porcelain`\" && echo \"+CHANGES\" || true)\"\n\nBUILD_DATE=\"$(\"$SOURCE_DIR\"/ci-helper.sh date)\"\n\nGOPATH=${GOPATH:-$(${GO_CMD} env GOPATH)}\ncase $(uname) in\n    CYGWIN*)\n        GOPATH=\"$(cygpath $GOPATH)\"\n        ;;\nesac\n\n# Delete the old dir\necho \"==> Removing old directory...\"\nrm -f bin/*\nrm -rf pkg/*\nmkdir -p bin/\n\n# Build!\necho \"==> Building...\"\n${GO_CMD} build \\\n    -gcflags \"${GCFLAGS}\" \\\n    -ldflags \"${LD_FLAGS} -X github.com/hashicorp/vault/version.GitCommit='${GIT_COMMIT}${GIT_DIRTY}' -X github.com/hashicorp/vault/version.BuildDate=${BUILD_DATE}\" \\\n    -o \"bin/vault\" \\\n    -tags \"${BUILD_TAGS}\" \\\n    .\n\n# Move all the compiled things to the $GOPATH/bin\nOLDIFS=$IFS\nIFS=: FIRST=($GOPATH) BIN_PATH=${GOBIN:-${FIRST}/bin}\nIFS=$OLDIFS\n\n# Ensure the go bin folder exists\nmkdir -p ${BIN_PATH}\nrm -f ${BIN_PATH}/vault\ncp bin/vault ${BIN_PATH}\n\n# Done!\necho\necho \"==> Results:\"\nls -hl bin/\n```", "metadata": {"source": "github_raw", "repository": "hashicorp/vault", "file_path": "scripts/build.sh", "category": "build", "authentic": true}}
{"prompt": "Write a shell script for deployment automation", "completion": "```bash\npackages:\n  yum:\n    gcc: []\n    make: []\n    openssl-devel: []\n    libxml2: []\n    libxml2-devel: []\nfiles:\n  \"/opt/elasticbeanstalk/hooks/appdeploy/pre/50npm.sh\" :\n    mode: \"000775\"\n    owner: root\n    group: root\n    content: |\n      #!/bin/bash\n      function error_exit\n      {\n      eventHelper.py --msg \"$1\" --severity ERROR\n      exit $2\n      }\n\n      export HOME=/home/ec2-user # ADDED EXPORT COMMAND\n      echo \"export home\" # JUST FOR REMARK\n\n      OUT=$(/opt/elasticbeanstalk/containerfiles/ebnode.py --action npm- install 2>&1) || error_exit \"Failed to run npm install. $OUT\" $?\n      echo $OUT\n```", "metadata": {"source": "stackoverflow", "question_id": 34084814, "title": "NPM error when deploying NodeJS to Elastic Beanstalk", "score": 13, "category": "deployment", "authentic": true}}
{"prompt": "Create a shell script for AWS infrastructure management", "completion": "```bash\n2014-02-10 19:07:17,234 [DEBUG] Running command 02_createadmin\n2014-02-10 19:07:17,235 [DEBUG] Generating defaults for command 02_createadmin\n<<<\n\n2014-02-10 19:07:17,319 [DEBUG] Running test for command 02_createadmin\n2014-02-10 19:07:17,330 [DEBUG] Test command output:\n2014-02-10 19:07:17,331 [DEBUG] Test for command 02_createadmin passed\n2014-02-10 19:07:17,340 [ERROR] Command 02_createadmin (scripts/createadmin.py) failed\n2014-02-10 19:07:17,341 [DEBUG] Command 02_createadmin output: /bin/sh: scripts/createadmin.py: Permission denied\n\n2014-02-10 19:07:17,341 [ERROR] Error encountered during build of postbuild_0_legi_track_prod: Command 02_createadmin failed\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/cfnbootstrap/construction.py\", line 511, in run_config\n    CloudFormationCarpenter(config, self._auth_config).build(worklog)\n  File \"/usr/lib/python2.6/site-packages/cfnbootstrap/construction.py\", line 247, in build\n    changes['commands'] = CommandTool().apply(self._config.commands)\n  File \"/usr/lib/python2.6/site-packages/cfnbootstrap/command_tool.py\", line 113, in apply\n    raise ToolError(u\"Command %s failed\" % name)\nToolError: Command 02_createadmin failed\n2014-02-10 19:07:17,342 [ERROR] Unhandled exception during build: Command 02_createadmin failed\nTraceback (most recent call last):\n  File \"/opt/aws/bin/cfn-init\", line 122, in <module>\n    worklog.build(detail.metadata, configSets)\n  File \"/usr/lib/python2.6/site-packages/cfnbootstrap/construction.py\", line 117, in build\n    Contractor(metadata).build(configSets, self)\n  File \"/usr/lib/python2.6/site-packages/cfnbootstrap/construction.py\", line 502, in build\n    self.run_config(config, worklog)\n  File \"/usr/lib/python2.6/site-packages/cfnbootstrap/construction.py\", line 511, in run_config\n    CloudFormationCarpenter(config, self._auth_config).build(worklog)\n  File \"/usr/lib/python2.6/site-packages/cfnbootstrap/construction.py\", line 247, in build\n    changes['commands'] = CommandTool().apply(self._config.commands)\n  File \"/usr/lib/python2.6/site-packages/cfnbootstrap/command_tool.py\", line 113, in apply\n    raise ToolError(u\"Command %s failed\" % name)\nToolError: Command 02_createadmin failed\n```", "metadata": {"source": "stackoverflow", "question_id": 21686439, "title": "Django on AWS Elasticbeanstalk: Command 02_createadmin failed", "score": 6, "category": "build", "authentic": true}}
{"prompt": "Write a shell script for CI/CD pipeline", "completion": "```bash\n$ sudo gitlab-runner register\nRuntime platform                                    arch=amd64 os=linux pid=8665 revision=3afdaba6 version=11.5.0\nRunning in system-mode.\n\nPlease enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/):\nhttp://example.com/\nPlease enter the gitlab-ci token for this runner:\nru7i9G9R-3BJn2RXKdtv\nPlease enter the gitlab-ci description for this runner:\n[ubuntu-s-1vcpu-1gb-fra1-01]: warehouse\nPlease enter the gitlab-ci tags for this runner (comma separated):\nwarehouse\nRegistering runner... succeeded                     runner=ru7i9G9R\nPlease enter the executor: parallels, ssh, virtualbox, docker+machine, kubernetes, docker, docker-ssh, shell, docker-ssh+machine:\ndocker\nPlease enter the default Docker image (e.g. ruby:2.1):\nwarehouse:latest\nRunner registered successfully. Feel free to start it, but if it's running already the config should be automatically reloaded!\n```", "metadata": {"source": "stackoverflow", "question_id": 53571539, "title": "gitlab ci/cd runner stuck", "score": 9, "category": "testing", "authentic": true}}
{"prompt": "Write a shell script for CI/CD pipeline", "completion": "```bash\n$ sudo cat /etc/gitlab-runner/config.toml\nconcurrent = 1\ncheck_interval = 0\n\n[session_server]\n  session_timeout = 1800\n\n[[runners]]\n  name = \"warehouse\"\n  url = \"http://example.com/\"\n  token = \"f5b8036463e2f97d2ab4bc721b6a91\"\n  executor = \"docker\"\n  [runners.docker]\n    tls_verify = false\n    image = \"warehouse:latest\"\n    privileged = false\n    disable_entrypoint_overwrite = false\n    oom_kill_disable = false\n    disable_cache = false\n    volumes = [\"/cache\"]\n    shm_size = 0\n  [runners.cache]\n    [runners.cache.s3]\n    [runners.cache.gcs]\n```", "metadata": {"source": "stackoverflow", "question_id": 53571539, "title": "gitlab ci/cd runner stuck", "score": 9, "category": "testing", "authentic": true}}
{"prompt": "Write a shell script for CI/CD pipeline", "completion": "```bash\n$ sudo gitlab-runner run\nRuntime platform                                    arch=amd64 os=linux pid=9678 revision=3afdaba6 version=11.5.0\nStarting multi-runner from /etc/gitlab-runner/config.toml ...  builds=0\nRunning in system-mode.\n\nConfiguration loaded                                builds=0\nListen address not defined, metrics server disabled  builds=0\nListen address not defined, session server disabled  builds=0\n```", "metadata": {"source": "stackoverflow", "question_id": 53571539, "title": "gitlab ci/cd runner stuck", "score": 9, "category": "build", "authentic": true}}
{"prompt": "Write a shell script for CI/CD pipeline", "completion": "```bash\nroot@debian:~# gitlab-ci-multi-runner register\nRunning in system-mode.\nPlease enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/):\n  http://127.0.0.1\nPlease enter the gitlab-ci token for this runner:\n  XXX\nPlease enter the gitlab-ci description for this runner:\n  [debian]: p1\nPlease enter the gitlab-ci tags for this runner (comma separated):\n  dev\nRegistering runner... succeeded                     runner=XXX\nPlease enter the executor: kubernetes, docker, parallels, ssh, docker+machine, docker-ssh, shell, virtualbox, docker-ssh+machine:\n  shell\nRunner registered successfully. Feel free to start it, but if it's running already the config should be automatically reloaded!\nroot@debian:~# gitlab-ci-multi-runner verify\nRunning in system -mode.\n\nVerifying runner... is alive                        runner=XXX\n```", "metadata": {"source": "stackoverflow", "question_id": 41545349, "title": "GitLab CI Why this runner isn&#39;t doing the build?", "score": 6, "category": "containerization", "authentic": true}}
{"prompt": "Write a shell script for infrastructure automation", "completion": "```bash\n{'elements': [{'id': 1615,\n   'slug': 'engineering-manager-ios-blockchain',\n   'title': 'Engineering Manager, iOS - Blockchain',\n   'seniority': 'expert',\n   'role': 'engineeringOthers',\n   'primaryLanguage': 'java',\n   'primaryPlatform': None,\n   'secondaryLanguage': None,\n   'secondaryPlatform': None,\n   'mainDatabase': None,\n   'productDescription': '',\n   'postedAt': 1662976098357,\n   'companyId': 1,\n   'salaryCurrency': 'BGN',\n   'salaryMin': 11500,\n   'salaryMax': 15000,\n   'salaryPeriod': 'MONTH',\n   'jobType': 'FULL_TIME',\n   'jobTypeComment': None,\n   'businessTravelComment': None,\n   'homeOfficeDays': 0,\n   'homeOfficePer': 'WEEK',\n   'teamSizeMin': 500,\n   'teamSizeMax': 1000,\n   'teamLead': '',\n   'teamLeadName': '',\n   'teamLeadRole': '',\n   'teamLeadImage': None,\n   'locations': [{'id': 1,\n     'teamSize': None,\n     'founded': 2018,\n     'comment': None}],\n   'productImages': [{'id': 5078,\n     'collection': 'product',\n     'name': 'prod/1/product/zm49c..'},\n    {'id': 5079, 'collection': 'product', 'name': 'prod/1/product/mhqkiw.'},\n    {'id': 5081, 'collection': 'product', 'name': 'prod/1/product/c.jtpo.'},\n    {'id': 5080, 'collection': 'product', 'name': 'prod/1/product/u0847l.'},\n    {'id': 5082, 'collection': 'product', 'name': 'prod/1/product/yrpy99.'}],\n   'requirements': [{'icon': 'fas fa-check-square',\n     'title': '1+ years of experience leading a team of 4+ people'},\n    {'icon': 'fas fa-check-square',\n     'title': '3+ years of experience with Swift'},\n    {'icon': 'fas fa-check-square',\n     'title': 'Experience with multithreaded apps, offline storage and performance optimizations'},\n    {'icon': 'fas fa-check-square', 'title': 'Experience using RESTful APIs'},\n    {'icon': 'fas fa-check-square',\n     'title': \"Familiar with Apple's Human Interface Guidelines and concepts for good UX\"},\n    {'icon': 'fas fa-check-square',\n     'title': 'Experience setting up CI/CD processes'},\n    {'icon': 'fas fa-check-square',\n     'title': 'Good understanding of design patterns and software architecture principles'},\n    {'icon': 'fas fa-check-square',\n     'title': 'Understanding of software development lifecycle (SDLC)'},\n    {'icon': 'fas fa-check-square',\n     'title': 'Superior interpersonal communication skills'},\n    {'icon': 'fas fa-check-square',\n     'title': 'A technical mindset with great attention to detail'},\n    {'icon': 'fas fa-check-square',\n     'title': 'Self-starter, comfortable in a fast-paced startup environment'},\n    {'icon': 'fas fa-check-square',\n     'title': 'Familiar with crypto-economic protocol design including governance and incentive structures'},\n    {'icon': 'fas fa-check-square',\n     'title': 'Familiar with Web3 Stack: Web3, Ethers, Hardhat'}],\n   'responsibilities': [{'icon': 'fas fa-hand-point-right',\n     'title': 'Take part in designing, scoping, developing, and maintaining disruptive Web3 applications'},\n    {'icon': 'fas fa-hand-point-right',\n     'title': 'You Write It – You Own It: Develop, test, release, maintain, and improve – own the full life-cycle of your code'},\n    {'icon': 'fas fa-hand-point-right',\n     'title': 'Provide technical guidance and coaching'},\n    {'icon': 'fas fa-hand-point-right',\n     'title': 'Possess a strong sense of ownership and accountability with a commitment to deliver quality timely outcomes\\n'},\n    {'icon': 'fas fa-hand-point-right',\n     'title': 'Assist in management, including hiring, onboarding, training, and keeping management updated on team performance'},\n    {'icon': 'fas fa-hand-point-right',\n     'title': 'Proactively propose improvement plans and best practices'},\n    {'icon': 'fas fa-hand-point-right',\n     'title': 'Contribute and maintain the thorough documentation'}],\n   'benefits': [{'icon': 'fas fa-star',\n     'title': 'Attractive remuneration package'},\n    {'icon': 'fas fa-star', 'title': 'Annual bonuses '},\n    {'icon': 'fas fa-star', 'title': 'Learning Hub'},\n    {'icon': 'fas fa-star', 'title': 'Hybrid way of work and work from home '},\n    {'icon': 'fas fa-star',\n     'title': 'Inspiring atmosphere and innovative projects'},\n    {'icon': 'fas fa-star',\n     'title': 'Future career development in a global company leading the innovative blockchain space'},\n    {'icon': 'fas fa-star',\n     'title': 'Tailor-made personal benefits package — you want it, you get it'},\n    {'icon': 'fas fa-star',\n     'title': 'Wellness program including additional health insurance, Multisport card, sports activities, standing desks & protective glasses and many more'},\n    {'icon': 'fas fa-star',\n     'title': 'Free assorted healthy snacks and fresh fruits in the office'},\n    {'icon': 'fas fa-star', 'title': 'Weekly gourmet breakfast meet-ups'},\n    {'icon': 'fas fa-star',\n     'title': 'Free parking with a designated space, free electric bikes & public transport'},\n    {'icon': 'fas fa-star',\n     'title': 'Epic regular team buildings and parties'}],\n   'activities': [],\n   'hiringProcessSteps': [],\n   'tools': [],\n   'company': {'id': 1,\n    'slug': 'nexo',\n    'brand': 'Nexo',\n    'product': '',\n    'images': [{'id': 2,\n      'collection': 'main',\n      'name': 'prod/1/main/yzxiiu.jpg'},\n     {'id': 5071, 'collection': 'main', 'name': 'prod/1/main/c3smv4.'},\n     {'id': 5072, 'collection': 'main', 'name': 'prod/1/main/baofyo.'},\n     {'id': 1, 'collection': 'logo', 'name': 'prod/1/logo/b25jcy.png'},\n     {'id': 5078, 'collection': 'product', 'name': 'prod/1/product/zm49c..'},\n     {'id': 5079, 'collection': 'product', 'name': 'prod/1/product/mhqkiw.'},\n     {'id': 5081, 'collection': 'product', 'name': 'prod/1/product/c.jtpo.'},\n     {'id': 5080, 'collection': 'product', 'name': 'prod/1/product/u0847l.'},\n     {'id': 5082, 'collection': 'product', 'name': 'prod/1/product/yrpy99.'},\n     {'id': 21, 'collection': 'clients', 'name': 'prod/1/clients/cw-ck4.jpg'},\n     {'id': 22, 'collection': 'clients', 'name': 'prod/1/clients/wpezkk.jpg'},\n     {'id': 23, 'collection': 'clients', 'name': 'prod/1/clients/01ngdh.jpg'},\n     {'id': 24, 'collection': 'clients', 'name': 'prod/1/clients/yh1pgp.jpg'},\n     {'id': 25, 'collection': 'clients', 'name': 'prod/1/clients/gli-f8.jpg'},\n     {'id': 5073, 'collection': 'photos', 'name': 'prod/1/photos/zunc4z.'},\n     {'id': 5074, 'collection': 'photos', 'name': 'prod/1/photos/q-bklt.'},\n     {'id': 5075, 'collection': 'photos', 'name': 'prod/1/photos/tgiq9i.'},\n     {'id': 5077, 'collection': 'photos', 'name': 'prod/1/photos/dguwy5.'},\n     {'id': 5076, 'collection': 'photos', 'name': 'prod/1/photos/tx0fdl.'},\n     {'id': 3871, 'collection': 'photos', 'name': 'prod/1/photos/c0nnnz.'},\n     {'id': 3869, 'collection': 'photos', 'name': 'prod/1/photos/6x4jxe.'},\n     {'id': 3874, 'collection': 'photos', 'name': 'prod/1/photos/e0bfnd.'},\n     {'id': 3872, 'collection': 'photos', 'name': 'prod/1/photos/k5q7hf.'}],\n    'locations': [{'id': 1,\n      'teamSize': None,\n      'founded': 2018,\n      'comment': None}],\n    'awards': [{'icon': 'heart',\n      'title': 'Finance Company of The Year by Forbes Business Awards 2021'},\n     {'icon': 'heart',\n      'title': 'Bronze Stevie® Award In 2021 International Business Awards®'},\n     {'icon': 'fas fa-trophy',\n      'title': 'Top 50 List of Crypto Valley Companies'}],\n    'perks': [{'icon': 'fas fa-gem',\n      'title': 'Monthly evening events',\n      'text': 'At Nexo, we do monthly get-togethers after work to have a drink, enjoy some company and relax. No rules, no expectations.'},\n     {'icon': 'fas fa-gem',\n      'title': 'Teambuildings',\n      'text': 'Twice a year our team travels out of the big city to cozy locations where we can relax, have fun, sing karaoke, and do sports.'},\n     {'icon': 'fas fa-gem',\n      'title': 'Lunchtime',\n      'text': 'This is the easiest opportunity to get connected with colleagues outside of your immediate team. Learn about them personally and professionally.'},\n     {'icon': 'fas fa-gem',\n      'title': 'Ad hoc',\n      'text': 'Going in the park, the gym, an event, or elsewhere is always welcome. Share with your colleagues and you will likely find enthusiasts.'}],\n    'values': [{'icon': 'fas fa-gem',\n      'title': 'Give Your Best',\n      'text': 'Always. When every team member gives his/her best in their domain, we as a team move mountains.'},\n     {'icon': 'fas fa-gem',\n      'title': 'Results Matter the Most',\n      'text': 'We analyze what is best for the company and how can we invest our time in the most productive way to generate the most impactful results.'},\n     {'icon': 'fas fa-gem',\n      'title': 'Bring Positivity',\n      'text': 'We smile, have fun, and enjoy ourselves.'},\n     {'icon': 'fas fa-gem',\n      'title': 'Be Proactive',\n      'text': 'We are independent, forward-thinking and we act.'},\n     {'icon': 'fas fa-gem',\n      'title': 'Always Learn. Always Question.',\n      'text': 'Our industry changes 10X faster than most other industries. Hence, we are always learning and looking for ways to stay ahead of the competition.'}],\n    'public': True},\n   'public': True,\n   'customerFacing': False,\n   'businessTraveling': False,\n   'offeringStock': False,\n   'fullyRemote': False},\n  {'id': 1606,\n   'slug': 'devops-engineer',\n   'title': 'DevOps Engineer',\n   'seniority': 'senior',\n   'role': 'devops',\n   'primaryLanguage': 'bashShellPowershell',\n   'primaryPlatform': 'linux',\n   'secondaryLanguage': 'python',\n   'secondaryPlatform': None,\n   'mainDatabase': 'postgresql',\n   'productDescription': '<p><a href=\"https://solveva.com/#projects\" rel=\"nofollow\"><em>Actus</em></a></p>',\n   'postedAt': 1662111811347,\n   'companyId': 198,\n   'salaryCurrency': 'BGN',\n   'salaryMin': 4000,\n   'salaryMax': 0,\n   'salaryPeriod': 'MONTH',\n   'jobType': 'FULL_TIME',\n   'jobTypeComment': None,\n   'businessTravelComment': None,\n   'homeOfficeDays': 0,\n   'homeOfficePer': 'WEEK',\n   'teamSizeMin': 1,\n   'teamSizeMax': 5,\n   'teamLead': '',\n   'teamLeadName': '',\n   'teamLeadRole': '',\n   'teamLeadImage': None,\n   'locations': [{'id': 1425,\n     'teamSize': None,\n     'founded': None,\n     'comment': None}],\n   'productImages': [],\n   'requirements': [{'icon': 'fas fa-check-square',\n     'title': 'You have 2+ years of experience in a systems engineering/DevOps role\\n\\n\\n'}],\n   'responsibilities': [{'icon': 'fas fa-hand-point-right',\n     'title': 'You will actively interface with software developers, security operations engineers, product managers, operations managers on projects\\n'},\n    {'icon': 'fas fa-hand-point-right',\n     'title': 'You will provide on-call support for the platform.\\nYou will collaborate with the cyber security team to integrate security measures into all aspects of the platform'},\n    {'icon': 'fas fa-hand-point-right',\n     'title': 'You will develop and deploy a control plane for all platform services to guarantee observability, monitoring, analytics, and alerting'},\n    {'icon': 'fas fa-hand-point-right',\n     'title': 'You will develop and maintain the continuous integration and continuous delivery pipeline'},\n    {'icon': 'fas fa-hand-point-right',\n     'title': ' You will perform capacity planning, automation, testing, performance tuning, and tools development'}],\n   'benefits': [{'icon': 'fas fa-star',\n     'title': 'We have a friendly team, which is nice to work in - people have always been our strong point. Solveva has a flat structure, little bureaucracy, no managers (any scrum-master, first of all, the developer);\\n'},\n    {'icon': 'fas fa-star',\n     'title': 'We provide you with a choice of hardware: Windows (Dell, Lenovo) or Mac (Pro, Air);'},\n    {'icon': 'fas fa-star',\n     'title': \"Remote, office or mixed work. If you do not feel comfortable working from home, we'll take for you a coworking space;\"},\n    {'icon': 'fas fa-star',\n     'title': 'We provide you with a choice of hardware: Windows (Dell, Lenovo) or Mac (Pro, Air);'},\n    {'icon': 'fas fa-star',\n     'title': 'An individual budget for the development of knowledge (books, conferences, courses), additional equipment and putting the body in order after hard work;'},\n    {'icon': 'fas fa-star', 'title': 'Additional benefits and incentives;'}],\n   'activities': [],\n   'hiringProcessSteps': ['CV review\\n',\n    'HR Interview',\n    'Technical Interview',\n    'Task ( this is optional - we do it if is needed )',\n    'Team Interview'],\n   'tools': [],\n   'company': {'id': 198,\n    'slug': 'solveva',\n    'brand': 'Solveva ',\n    'product': '<p><a href=\"https://solveva.com/#projects\" rel=\"nofollow\"><em>Actus</em></a></p>',\n    'images': [{'id': 5709,\n      'collection': 'main',\n      'name': 'prod/198/main/plhttq.'},\n     {'id': 5710, 'collection': 'logo', 'name': 'prod/198/logo/ywhgct.'},\n     {'id': 5711, 'collection': 'logo', 'name': 'prod/198/logo/mhpi0w.'},\n     {'id': 5718, 'collection': 'photos', 'name': 'prod/198/photos/h.mk-f.'},\n     {'id': 5717, 'collection': 'photos', 'name': 'prod/198/photos/y4xgtc.'},\n     {'id': 5715, 'collection': 'photos', 'name': 'prod/198/photos/zukwmr.'},\n     {'id': 5719, 'collection': 'photos', 'name': 'prod/198/photos/tshc3z.'}],\n    'locations': [{'id': 1425,\n      'teamSize': None,\n      'founded': None,\n      'comment': None},\n     {'id': 1426,\n      'teamSize': None,\n      'founded': None,\n      'comment': None},\n     {'id': 1427,\n      'teamSize': None,\n      'founded': None,\n      'comment': None},\n     {'id': 1428,\n      'teamSize': None,\n      'founded': None,\n      'comment': None}],\n    'awards': [],\n    'perks': [{'icon': 'heart',\n      'title': 'Internal trainings',\n      'text': 'As our main value are the people, at Solveva we regularly organise different  trainings to help our colleagues achieve their career and personal goals.'},\n     {'icon': 'heart',\n      'title': 'Good social benefit system',\n      'text': 'You are given an individual budget for the development of knowledge (books, conferences, courses), additional equipment and putting the body in order after hard work.'},\n     {'icon': 'heart',\n      'title': 'Regular Planing meetings and team buildings',\n      'text': 'Every 3 months we organise a planing meetings and team building event and gather all our colleagues for a 5-day  where we can spend quality time together.'},\n     {'icon': 'heart',\n      'title': 'Flexible working time',\n      'text': \"No fixed schedules, ability for remote working days.\\nRemote, office or mixed work. If you do not feel comfortable working from home, we'll take for you a coworking space.\"},\n    'public': True},\n   'public': True,\n   'customerFacing': False,\n   'businessTraveling': False,\n   'offeringStock': False,\n   'fullyRemote': False}],\n 'totalPages': 10,\n 'page': 0,\n 'totalElements': 481}\n```", "metadata": {"source": "stackoverflow", "question_id": 73693783, "title": "Flatten complex nested JSON structure in Pyspark", "score": 0, "category": "deployment", "authentic": true}}
