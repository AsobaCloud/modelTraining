{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\r\nname: Bug report\r\nabout: Create a report to help us improve\r\ntitle: BUG\r\nlabels: bug\r\nassignees: anmol098\r\n\r\n---\r\n\r\n**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Github repository link**\r\nIf applicable, please share the Github repository link.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": ".github/ISSUE_TEMPLATE/bug_report.md", "language": "markdown", "complexity": "simple", "star_count": 3648, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 28}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\r\nname: Feature request\r\nabout: Suggest an idea for this project\r\ntitle: FEAT\r\nlabels: Brainstorming\r\nassignees: anmol098\r\n\r\n---\r\n\r\n**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": ".github/ISSUE_TEMPLATE/feature_request.md", "language": "markdown", "complexity": "simple", "star_count": 3648, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\r\nname: Special mention\r\nabout: Add your name to the list if you are making your profile readme more awesome\r\n  with our GitHub Action\r\ntitle: Add||<!--- Your GitHub username here -->\r\nlabels: Name\r\nassignees: anmol098, ok-coder1\r\n\r\n---\r\n\r\n**Github Profile Link**\r\n<!--- A direct link to your profile -->\r\n\r\n**Your Name**\r\n<!--- Your name which should be displayed on the link -->\r\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": ".github/ISSUE_TEMPLATE/special-mention.md", "language": "markdown", "complexity": "simple", "star_count": 3648, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 16}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: 2\nupdates:\n  - package-ecosystem: \"docker\"\n    directory: \"/\"\n    schedule:\n      interval: \"daily\"\n    open-pull-requests-limit: 99\n  - package-ecosystem: \"github-actions\"\n    directory: \"/\"\n    schedule:\n      interval: \"daily\"\n    open-pull-requests-limit: 99\n  - package-ecosystem: \"pip\"\n    directory: \"/\"\n    schedule:\n      interval: \"daily\"\n    open-pull-requests-limit: 99\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": ".github/dependabot.yml", "language": "yaml", "complexity": "simple", "star_count": 3648, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "dependencies:\n  - head-branch: [\"^dep\", \"deps\"]\n  - changed-files:\n    - any-glob-to-any-file: \"requirements.txt\"\n\ndocker:\n  - changed-files:\n    - any-glob-to-any-file: \"Dockerfile\"\n\ndocumentation:\n  - changed-files:\n    - any-glob-to-any-file: \"**/**.md\"\n\ngithub_actions:\n  - changed-files:\n    - any-glob-to-any-file: \".github/workflows/**\"\n\npython:\n  - changed-files:\n    - any-glob-to-any-file: [\"sources/**\", \"!sources/translation.json\"]\n\ntranslation:\n  - changed-files:\n    - any-glob-to-any-file: \"sources/translation.json\"\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": ".github/labeler.yml", "language": "yaml", "complexity": "simple", "star_count": 3648, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 25}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: PUBLISH_IMAGE\n\non:\n  push:\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: ${{ github.ref != 'refs/heads/master' }}\n\njobs:\n  publish-server-image:\n    name: Publish 'waka-readme-stats' image\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout üõéÔ∏è\n        uses: actions/checkout@v3\n\n      - name: Set up Docker Buildx üêã\n        uses: docker/setup-buildx-action@v2\n\n      - name: Log in to the container registry üö™\n        uses: docker/login-action@v2\n        with:\n          username: wakareadmestats\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n      - name: Extract metadata (tags, labels) for Docker üèãÔ∏è\n        id: meta\n        uses: docker/metadata-action@v4\n        with:\n          images: wakareadmestats/waka-readme-stats\n          tags: |\n            type=ref,event=branch\n            type=ref,event=pr\n            type=semver,pattern={{version}}\n            type=semver,pattern={{major}}.{{minor}}\n\n      - name: Build and push Docker image üèóÔ∏è\n        uses: docker/build-push-action@v4\n        with:\n          push: ${{ github.ref == 'refs/heads/master' || startsWith(github.ref, 'refs/heads/releases') }}\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n          cache-from: type=gha\n          cache-to: type=gha,mode=max\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": ".github/workflows/build_image.yml", "language": "yaml", "complexity": "medium", "star_count": 3648, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 47}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CI\n\non:\n  - pull_request_target\n  - workflow_dispatch\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number }}\n  cancel-in-progress: true\n\njobs:\n  ci:\n    name: Run Test and Review PR\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout üõéÔ∏è\n        uses: actions/checkout@v4\n\n      - name: Setup Python 3.13 üêç\n        uses: actions/setup-python@v5\n        with:\n          python-version: 3.13\n          cache: 'pip'\n\n      - name: Install Dependencies üì•\n        run: pip install -r requirements.txt\n\n      - name: Create Assets Folder üì•\n        run: mkdir assets\n\n      - name: Create Previous Comments ü´£\n        uses: int128/hide-comment-action@v1\n        with:\n          starts-with: \"README stats current output:\"\n\n      - name: Run Action Preview on Current Code üß™\n        id: make-stats\n        env:\n          INPUT_GH_TOKEN: ${{ secrets.INPUT_GITHUB_TOKEN }}\n          INPUT_WAKATIME_API_KEY: ${{ secrets.WAKATIME_API_KEY }}\n          INPUT_SHOW_TIMEZONE: True\n          INPUT_SHOW_PROJECTS: True\n          INPUT_SHOW_EDITORS: True\n          INPUT_SHOW_OS: True\n          INPUT_SHOW_LANGUAGE: True\n          INPUT_SYMBOL_VERSION: 1\n          INPUT_SHOW_LINES_OF_CODE: True\n          INPUT_SHOW_LOC_CHART: True\n          INPUT_SHOW_PROFILE_VIEWS: True\n          INPUT_SHOW_TOTAL_CODE_TIME: True\n          INPUT_SHOW_SHORT_INFO: True\n          INPUT_SHOW_COMMIT: True\n          INPUT_SHOW_DAYS_OF_WEEK: True\n          INPUT_SHOW_LANGUAGE_PER_REPO: True\n          INPUT_SHOW_UPDATED_DATE: True\n          INPUT_COMMIT_BY_ME: True\n          INPUT_DEBUG_LOGGING: True # Not for prod\n          DEBUG_RUN: True # Not for prod\n        run: python3 sources/main.py\n\n      - name: Save Branch Name Without Slashes üìõ\n        if: ${{ github.ref != 'refs/heads/master' }}\n        env:\n          BRANCH_NAME: ${{ github.head_ref || github.ref_name }}\n        run: |\n          BRANCH_NAME=${{ env.BRANCH_NAME }}\n          BRANCH_NAME=${BRANCH_NAME////_}\n          echo BRANCH_NAME=${BRANCH_NAME} >> $GITHUB_ENV\n\n      - name: Upload Artifact üì¶\n        uses: actions/upload-artifact@v4\n        if: ${{ github.ref != 'refs/heads/master' }}\n        with:\n          name: ${{ format('github-pages-for-branch-{0}', env.BRANCH_NAME) }}\n          path: assets\n\n      - name: Create Comment üí¨\n        uses: jungwinter/comment@v1\n        with:\n          type: create\n          body: ${{ steps.make-stats.outputs.README_CONTENT }}\n          issue_number: ${{ github.event.number }}\n          token: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": ".github/workflows/ci.yml", "language": "yaml", "complexity": "medium", "star_count": 3648, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 85}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CodeQL\n\non:\n  push:\n    branches: master\n  pull_request:\n  workflow_dispatch:\n  schedule:\n    - cron: '30 13 * * 6'\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: ${{ github.ref != 'refs/heads/master' }}\n\njobs:\n  analyze:\n    name: Analyze\n    runs-on: ubuntu-latest\n\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language: [ 'python' ]\n\n    steps:\n    - name: Checkout üõéÔ∏è\n      uses: actions/checkout@v4\n    \n    - name: Setup Python 3.13 üêç\n      uses: actions/setup-python@v5\n      with:\n          python-version: 3.13\n          cache: 'pip'\n          \n    - name: Install dependencies üì•\n      run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n          echo \"CODEQL_PYTHON=$(which python)\" >> $GITHUB_ENV\n          \n    - name: Initialize CodeQL üßë‚Äçüíª\n      uses: github/codeql-action/init@v3\n      with:\n        languages: ${{ matrix.language }}\n        queries: security-and-quality\n        setup-python-dependencies: false\n\n    - name: Perform CodeQL Analysis üìà\n      uses: github/codeql-action/analyze@v3\n      with:\n        category: \"/language:${{matrix.language}}\"\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": ".github/workflows/codeql.yml", "language": "yaml", "complexity": "medium", "star_count": 3648, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 57}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CODESTYLE\n\non:\n  push:\n\njobs:\n  lint:\n    name: Run codestyle check\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout üõéÔ∏è\n        uses: actions/checkout@v4\n\n      - name: Setup Python 3.13 üêç\n        uses: actions/setup-python@v5\n        with:\n          python-version: 3.13\n\n      - name: Install Dependencies üì•\n        run: pip install -r requirements.txt\n\n      - name: Run Codestyle ‚úîÔ∏è\n        run: flake8 --max-line-length=160 --exclude venv,assets . && black --line-length=160 --check --exclude='/venv/|/assets/' .\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": ".github/workflows/codestyle.yml", "language": "yaml", "complexity": "simple", "star_count": 3648, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 25}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Dependency review\non: pull_request\n\npermissions:\n  contents: read\n  pull-requests: write\n\njobs:\n  dependency-review:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout repository üõéÔ∏è\n        uses: actions/checkout@v4\n\n      - name: Dependency review üëÄ\n        uses: actions/dependency-review-action@v4\n        with:\n          comment-summary-in-pr: true\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": ".github/workflows/dependency-review.yml", "language": "yaml", "complexity": "simple", "star_count": 3648, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: \"Pull Request Labeler\"\non:\n- pull_request_target\n\njobs:\n  labeler:\n    permissions:\n      contents: read\n      pull-requests: write\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n      with:\n        repository: ${{ github.repository }}\n    - uses: actions/labeler@v5\n      with:\n        sync-labels: true\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": ".github/workflows/labeler.yml", "language": "yaml", "complexity": "simple", "star_count": 3648, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributor Covenant Code of Conduct\r\n\r\n## Our Pledge\r\n\r\nIn the interest of fostering an open and welcoming environment, we as\r\ncontributors and maintainers pledge to making participation in our project and\r\nour community a harassment-free experience for everyone, regardless of age, body\r\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\r\nlevel of experience, education, socio-economic status, nationality, personal\r\nappearance, race, religion, or sexual identity and orientation.\r\n\r\n## Our Standards\r\n\r\nExamples of behavior that contributes to creating a positive environment\r\ninclude:\r\n\r\n* Using welcoming and inclusive language\r\n* Being respectful of differing viewpoints and experiences\r\n* Gracefully accepting constructive criticism\r\n* Focusing on what is best for the community\r\n* Showing empathy towards other community members\r\n\r\nExamples of unacceptable behavior by participants include:\r\n\r\n* The use of sexualized language or imagery and unwelcome sexual attention or\r\n advances\r\n* Trolling, insulting/derogatory comments, and personal or political attacks\r\n* Public or private harassment\r\n* Publishing others' private information, such as a physical or electronic\r\n address, without explicit permission\r\n* Other conduct which could reasonably be considered inappropriate in a\r\n professional setting\r\n\r\n## Our Responsibilities\r\n\r\nProject maintainers are responsible for clarifying the standards of acceptable\r\nbehavior and are expected to take appropriate and fair corrective action in\r\nresponse to any instances of unacceptable behavior.\r\n\r\nProject maintainers have the right and responsibility to remove, edit, or\r\nreject comments, commits, code, wiki edits, issues, and other contributions\r\nthat are not aligned to this Code of Conduct, or to ban temporarily or\r\npermanently any contributor for other behaviors that they deem inappropriate,\r\nthreatening, offensive, or harmful.\r\n\r\n## Scope\r\n\r\nThis Code of Conduct applies both within project spaces and in public spaces\r\nwhen an individual is representing the project or its community. Examples of\r\nrepresenting a project or community include using an official project e-mail\r\naddress, posting via an official social media account, or acting as an appointed\r\nrepresentative at an online or offline event. Representation of a project may be\r\nfurther defined and clarified by project maintainers.\r\n\r\n## Enforcement\r\n\r\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\r\nreported by contacting the project team at anmolpratapsingh1997@gmail.com. All\r\ncomplaints will be reviewed and investigated and will result in a response that\r\nis deemed necessary and appropriate to the circumstances. The project team is\r\nobligated to maintain confidentiality with regard to the reporter of an incident.\r\nFurther details of specific enforcement policies may be posted separately.\r\n\r\nProject maintainers who do not follow or enforce the Code of Conduct in good\r\nfaith may face temporary or permanent repercussions as determined by other\r\nmembers of the project's leadership.\r\n\r\n## Attribution\r\n\r\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\r\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\r\n\r\n[homepage]: https://www.contributor-covenant.org\r\n\r\nFor answers to common questions about this code of conduct, see\r\nhttps://www.contributor-covenant.org/faq\r\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": "CODE_OF_CONDUCT.md", "language": "markdown", "complexity": "medium", "star_count": 3648, "quality_score": 0.63, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 77}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributing\r\n\r\n\r\n**Working on your first Pull Request?** You can learn how from this *free* series: [How to Contribute to an Open Source Project on GitHub](https://egghead.io/courses/how-to-contribute-to-an-open-source-project-on-github)\r\n\r\n---\r\n\r\nThanks for taking the time to look at `CONTRIBUTING.md`.\r\n\r\nAll contributions to this project should follow the [Code of Conduct (CoC)](./CODE_OF_CONDUCT.md).\r\n\r\n### Reporting issues and providing feedback\r\n\r\nIf you found any issues or bugs, be sure to open up an issue so I can check it out!\r\n\r\n### Opening a pull request\r\n\r\nOnce you've worked on your feature/bugfix, you can open a pull request using the `master` branch as the base branch. Write a clear and concise PR title, and a detailed description of why you made the change, whether it is related to any issues etc. and we will review it as soon as I can. \\\r\n**If the code is not linted properly or not reformatted the PR _can_ and _will_ be rejected.**\r\n\r\n### Setting up a development environment\r\n\r\nThis project is written in Python. \\\r\nIt requires **Python 3.8 or higher**, and uses `pip`.\r\n\r\nTo set it up, just fork this repository and clone the newly opened fork. \\\r\nThen install all the dependencies using:\r\n```bash\r\n$ pip install -r requirements.txt\r\n```\r\nThis command will install all the requirements needed to run the project.\r\n\r\nCreate a `.env` file in the project directory. For information you can refer to the `.env.example` file for all the environment variables.\r\n\r\nIf you want to contribute to the chart module, then you have enable that flag in `.env` file and install Node.js and NPM the for charting library to work:\r\n```bash\r\nnpm install -g vega-lite vega-cli canvas\r\n```\r\n\r\nIf everything is installed, you are good to go üëç.\r\n\r\nUse\r\n```bash\r\npython main.py\r\n``` \r\nto run the program.\r\n\r\n**Linting checks**\r\n\r\nIt is recommended to use proper linting. if you're using JetBrains' IntelliJ IDEA, please reformat your code before making a pull request. \\\r\nFor more info see [how to reformat code in IntelliJ IDEA](https://www.jetbrains.com/help/idea/reformat-and-rearrange-code.html).\r\n\r\n\r\n**Commit Messages**\r\n\r\nIt is recommended to use proper [semantic commit messages](https://gist.github.com/joshbuchea/6f47e86d2510bce28f8e7f42ae84c716).\r\n\r\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "simple", "star_count": 3648, "quality_score": 0.63, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 58}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "> [!IMPORTANT]\n> We are looking for translators for this project. \\\n> Any kind of help would be greatly appreciated. \\\n> Please see issue https://github.com/anmol098/waka-readme-stats/issues/23 if you would like to help!\n\n# Dev Metrics in README with added feature flags üéå\n\n![Project Preview](https://user-images.githubusercontent.com/25841814/79395484-5081ae80-7fac-11ea-9e27-ac91472e31dd.png)\n\n<p align=\"center\">\n\n  ![Project Preview](https://user-images.githubusercontent.com/15426564/88030180-8e1c4780-cb58-11ea-8a8b-b3576dd73652.png)\n\n  <h3 align=\"center\">üìå‚ú®Awesome README Stats</h3>\n</p>\n\n----\n\n<p align=\"center\">\n   <img src=\"https://img.shields.io/badge/language-python-blue?style\"/>\n   <img src=\"https://img.shields.io/github/license/anmol098/waka-readme-stats\"/>\n   <img src=\"https://img.shields.io/github/stars/anmol098/waka-readme-stats\"/>\n   <img src=\"https://img.shields.io/github/forks/anmol098/waka-readme-stats\"/>\n   <img src=\"https://img.shields.io/static/v1?label=%F0%9F%8C%9F&message=If%20Useful&style=style=flat&color=BC4E99\" alt=\"Star Badge\"/>\n</p>\n<p align=\"center\">\n   Are you an early üê§ or a night ü¶â?\n   <br/>\n   When are you most productive during the day?\n   <br/>\n   What are the languages that you code in?\n   <br/>\n   Let's check it out in your profile's README!\n</p>\n\n<p align=\"center\">\n    <a href=\"https://github.com/anmol098/waka-readme-stats/issues\">Report Bug</a>\n    ¬∑\n    <a href=\"https://github.com/anmol098/waka-readme-stats/issues\">Request Feature</a>\n  </p>\n\n## Prep Work\n\n1. You need to update the markdown file(`.md`) with 2 comments. You can refer [here](#update-your-readme) for updating it.\n2. You'll need a WakaTime API Key. You can get that from your WakaTime Account Settings\n    - You can refer [here](#new-to-wakatime), if you're new to WakaTime.\n3. You'll need a GitHub API Token with `repo` and `user` scope from [here](https://github.com/settings/tokens) if you're running the action to get commit metrics.\n   - You can use [this](#profile-repository) example to work it out.\n> [!NOTE]\n> Enabling the `repo` scope seems **DANGEROUS**, \\\n> but this GitHub Action only accesses your commit timestamps and the number of lines of code added or deleted in repositories that you contributed to.\n4. You need to save the WakaTime API Key and the GitHub API Token in the repository secrets. You can find that in the Settings of your repository. \\\n  Be sure to save those as the following:\n    - WakaTime API Key as `WAKATIME_API_KEY=<your wakatime API Key>`\n    - GitHub Personal Access Token (PAT) as `GH_TOKEN=<your github access token>`\n5. You can enable and disable feature flags based on your requirements.\n\nThis GitHub Action can be set to run at any time you want using `cron`. See [Crontab.guru](https://crontab.guru/) and [this](https://crontab.cronhub.io/) website to generate `cron` expressions.\n\n## Update your Readme\n\nAdd a comment to your `README.md` like this:\n\n```md\n<!--START_SECTION:waka-->\n<!--END_SECTION:waka-->\n```\n\n`waka` can be replaced by any string specified in the `SECTION_NAME` flag as per [the available flags section](#flags-available).\n\nThese lines will be our entry-points for the dev metrics.\n\n## New to WakaTime\n\nWakaTime gives you an idea of the time you really spent on coding. This helps you boost your productivity and competitive edge.\n\n- Head over to <https://wakatime.com> and create an account.\n- Get your WakaTime API Key from your [Account Settings in WakaTime](https://wakatime.com/settings/account).\n- Install the [WakaTime plugin](https://wakatime.com/plugins) in your favourite editor / IDE.\n- Paste in your API key to start the analysis.\n\n### Profile Repository\n\nYou'll need to get a [GitHub Access Token](https://docs.github.com/en/actions/configuring-and-managing-workflows/authenticating-with-the-github_token) with a `repo` and `user` scope and save it in the Repo Secrets `GH_TOKEN = <Your GitHub Access Token>`\n\nHere is a sample workflow File for running it:\n\n```yml\nname: Waka Readme\n\non:\n  schedule:\n    # Runs at 12am IST\n    - cron: '30 18 * * *'\n  workflow_dispatch:\njobs:\n  update-readme:\n    name: Update Readme with Metrics\n    runs-on: ubuntu-latest\n    steps:\n      - uses: anmol098/waka-readme-stats@master\n        with:\n          WAKATIME_API_KEY: ${{ secrets.WAKATIME_API_KEY }}\n          GH_TOKEN: ${{ secrets.GH_TOKEN }}\n```\n- Now you can commit and wait for it to run automatically, or you can also trigger to run it to see the result now. Just go to the `Actions` in your repo, select your `Profile Readme Development Stats` workflow and click `Run workflow`. Now wait for a minute or two and you will see your changes.\n\n## Extras\n\nIf you want to add the other info to your stats, you can add multiple `FLAGS` in your workflow file. By default all flags are enabled (except the lines of code flag due to the heavy operation performed)\n\n```yml\n- uses: anmol098/waka-readme-stats@master\n  with:\n      WAKATIME_API_KEY: ${{ secrets.WAKATIME_API_KEY }}\n      GH_TOKEN: ${{ secrets.GH_TOKEN }}\n      SHOW_OS: \"False\"\n      SHOW_PROJECTS: \"False\"\n```\n\n### Flags Available\n\n---\n\n`LOCALE`  This flag can be used to show stats in your language. Default is English. Locale [Short Hand](https://saimana.com/list-of-country-locale-code/) to be passed in the flag variable. Example of the final result can be found [here](https://github.com/anmol098/anmol098/blob/master/Readme-fr.md)\n\nThe `SECTION_NAME` flag can be set to any string, and will be the name of the section to replace in the README.\n\nThe `COMMIT_BY_ME` flag can be set to `True` to commit the code using your name and email.\n\nThe `COMMIT_MESSAGE` flag can be set for the commit message. The default is \"Updated with Dev Metrics\"\n\nThe `COMMIT_USERNAME` flag can be set as a username to commit the code. The default is \"readme-bot\".\n\nThe `COMMIT_EMAIL` flag can be set to an email to commit the code. The default is \"41898282+github-actions[bot]@users.noreply.github.com\".\n\nThe `SHOW_UPDATED_DATE` flag can be set to `True` to show the updated date in end of paragraph.\n\nThe `UPDATED_DATE_FORMAT` flag can be set to put the updated date into a format. The default is `\"%d/%m/%Y %H:%M:%S\"`.\n\nThe `SHOW_LINES_OF_CODE` flag can be set to `True` to show the number of lines of code writen till date.\n\n![Lines of Code](https://img.shields.io/badge/From%20Hello%20World%20I've%20written-1.3%20million%20Lines%20of%20code-blue)\n\nThe `SHOW_TOTAL_CODE_TIME` flag can be set to `False` to hide *Code Time*.\n\n![Code Time](http://img.shields.io/badge/Code%20Time-1%2C438%20hrs%2054%20mins-blue)\n\nThe `SHOW_PROFILE_VIEWS` flag can be set to `False` to hide **Profile Views**\n\n![Profile Views](http://img.shields.io/badge/Profile%20Views-2189-blue)\n\nThe `SHOW_COMMIT` flag can be set to `False` to hide the commit stats.\n\n**I'm an early üê§** \n```text\nüåû Morning    95 commits     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   30.55% \nüåÜ Daytime    78 commits     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   25.08% \nüåÉ Evening    112 commits    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   36.01% \nüåô Night      26 commits     ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   8.36%\n\n```\n\nThe `SHOW_DAYS_OF_WEEK` flag can be set to `False` to hide the commits made on the different days of the week.\n\nüìÖ **I'm Most Productive on Sundays** \n\n```text\nMonday       50 commits     ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   13.19% \nTuesday      85 commits     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   22.43% \nWednesday    56 commits     ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   14.78% \nThursday     44 commits     ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   11.61% \nFriday       28 commits     ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   7.39% \nSaturday     30 commits     ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   7.92% \nSunday       86 commits     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   22.69%\n\n```\n\nThe `SHOW_LANGUAGE` flag can be set to `False` to hide the programming languages you use.\n\n```text\nüí¨ Languages:\nJavaScript               5 hrs 26 mins       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   61.97%\nPHP                      1 hr 35 mins        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   18.07%\nMarkdown                 1 hr 9 mins         ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   13.3%\nPython                   22 mins             ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   4.32%\nXML                      8 mins              ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   1.62%\n```\n\n\nThe `SHOW_OS` flag can be set to `False` to hide your OS details.\n\n```text\nüíª Operating Systems:\nWindows                  8 hrs 46 mins       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   100.0%\n```\n\nThe `SHOW_PROJECTS` flag can be set to `False` to hide the projects worked on.\n\n```text\nüê±‚Äçüíª Projects:\nctx_connector            4 hrs 3 mins        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   46.33%\nNetSuite-Connector       1 hr 31 mins        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   17.29%\nmango-web-master         1 hr 12 mins        ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   13.77%\ncable                    54 mins             ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   10.41%\ndenAPI                   40 mins             ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   7.66%\n```\n\nThe `SHOW_TIMEZONE` flag can be set to `False` to hide the time zone you are in.\n\n```text\n‚åöÔ∏é Timezone: Asia/Calcutta\n```\n\nThe `SHOW_EDITORS` flag can be set to `False` to hide the list of code editors/IDEs used.\n\n```text\nüî• Editors:\nWebStorm                 6 hrs 47 mins       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   77.43%\nPhpStorm                 1 hr 35 mins        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   18.07%\nPyCharm                  23 mins             ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   4.49%\n```\n\nThe `SHOW_LANGUAGE_PER_REPO` flag can be set to `False` to hide the number of repositories in different programming languages and frameworks.\n\n**I mostly code in Vue** \n\n```text\nVue          8 repos        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   25.0% \nJava         6 repos        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   18.75% \nJavaScript   6 repos        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   18.75% \nPHP          3 repos        ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   9.38% \nPython       2 repos        ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   6.25% \nDart         2 repos        ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   6.25% \nCSS          2 repos        ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   6.25%\n\n```\n\nThe `SHOW_SHORT_INFO` flag can be set to `False` to hide the short fun fact info of a user.\n> [!NOTE]\n> This section requires a Personal Access Token (PAT) with the `user` scope, otherwise the data shown here will be incorrect.\n\n**üê± My GitHub Data** \n\n> üèÜ 433 Contributions in year 2020\n > \n> üì¶ Used 292.3 kB in GitHub's Storage \n > \n> üíº Opted to Hire\n > \n> üìú 25 Public Repository \n > \n> üîë 15 Owned Private Repository \n\nThe `SHOW_LOC_CHART` flag can be set to `False` to hide the lines of code written in different quarters of different years.\n\nThe `IGNORED_REPOS` flag can be set to `\"waka-readme-stats, my-first-repo\"` (just an example) to ignore some repos you don‚Äôt want to be counted.\n\nThe `SYMBOL_VERSION` flag can be set for the symbol for the progress bar (default: `1`).\n| Version | Done block | Empty block |\n|-------- | ---------- | ----------- |\n|    1    |      ‚ñà     |       ‚ñë     |\n|    2    |      ‚£ø     |       ‚£Ä     |\n|    3    |      ‚¨õ    |       ‚¨ú    |\n\nThe `DEBUG_LOGGING` flag can be set to increase the GitHub Action's output verbosity, by default equals internal runner debug property\n\n**Timeline**\n\n![Chart not found](https://raw.githubusercontent.com/anmol098/anmol098/master/charts/bar_graph.png) \n\n## :sparkling_heart: Support the project\n\nI open-source almost everything I can, and I try to reply to everyone needing help using these projects. Obviously,\nthis takes time. You can use this service for free.\n\nHowever, if you are using this project and happy with it or just want to encourage me to continue creating stuff, there are few ways you can do it :-\n\n- Giving proper credit when you use this action on your readme, linking back to it :D\n- Starring and sharing the project :rocket:\n- [![paypal.me/aapreneur](https://ionicabizau.github.io/badges/paypal.svg)](https://www.paypal.me/aapreneur) - You can make one-time donations via PayPal. I'll probably buy some ~~beer~~ wine üç∑.\n\nThanks! :heart:\n\n---\n\n# Contributing\n\nContributions are welcome ‚ô•! Please share any features, and add unit tests! Use the pull request and issue systems to contribute.\n\n# Selected Contributors\n\n1. [Anmol Pratap Singh](https://github.com/anmol098): Maintainer\n2. [Alexander Sergeev](https://github.com/pseusys): Maintainer\n3. [Aravind V. Nair](https://github.com/aravindvnair99): Maintainer\n4. [Prabhat Singh](https://github.com/prabhatdev): For code timeline graph [#18](https://github.com/anmol098/waka-readme-stats/pull/18)\n5. [Hedy Li](https://github.com/hedythedev): For Pull Request [#34](https://github.com/anmol098/waka-readme-stats/pull/34) and [#23](https://github.com/anmol098/waka-readme-stats/pull/23)\n6. [Pedro Torres](https://github.com/Corfucinas): For Pull Request [#29](https://github.com/anmol098/waka-readme-stats/pull/29)\n7. [Aaron Meese](https://github.com/ajmeese7): For Pull Request [#45](https://github.com/anmol098/waka-readme-stats/pull/45)\n8. [Arnav Jindal](https://github.com/Daggy1234): For Pull Request [#48](https://github.com/anmol098/waka-readme-stats/pull/48)\n9. [Daniel Rowe](https://github.com/DanRowe): For Pull Request [#57](https://github.com/anmol098/waka-readme-stats/pull/57)\n10. [Ss5h](https://github.com/tlatkdgus1): For adding support for natural sentence writing for translation [#136](https://github.com/anmol098/waka-readme-stats/pull/136)\n\n<details>\n<summary>Special mention for those who are currently making their profile readme more awesome :smile: :tada:</summary>\n\n  - [Stanislas](https://github.com/angristan)\n  \n  - [Pratik Kumar](https://github.com/pr2tik1)\n  \n  - [Vladimir](https://github.com/sergeev-vn)\n\n  - [Pedro Torres](https://github.com/Corfucinas)\n  \n  - [leverglowh](https://github.com/leverglowh)\n  \n  - [patdc](https://github.com/patdc)\n  \n  - [ÊûÅÂÆ¢ÊåñÊéòÊú∫](https://github.com/meteor1993)\n  \n  - [Fan()](https://github.com/Fanduzi)\n  \n  - [Miller Camilo Vega](https://github.com/minoveaz)\n  \n  - [XLor](https://github.com/yjl9903)\n  \n  - [Jesse Okeya](https://github.com/jesseokeya)\n  \n  - [anaiel](https://github.com/anaiel)\n  \n  - [Dipto Mondal](https://github.com/diptomondal007)\n  \n  - [Jerry F. Zhang](https://github.com/JerryFZhang)\n  \n  - [Karan Singh](https://github.com/karan06126)\n  \n  - [Erwin Lejeune](https://github.com/guilyx)\n  \n  - [Manuel Cepeda](https://github.com/mecm1993)\n  \n  - [Jonathan S](https://github.com/TGTGamer)\n  \n  - [Tsotne Gvadzabia](https://github.com/RockiRider)\n  \n  - [Miray](https://github.com/MirayXS)\n  \n  - [Varad Patil](https://github.com/varadp2000)\n  \n  - [Prabhat Singh](https://github.com/prabhatdev)\n  \n  - [Nikhil](https://github.com/nikhilgorantla)\n  \n  - [Â§ßÁôΩ](https://github.com/2720851545)\n  \n  - [Du Yizhuo](https://github.com/dyzdyz010)\n  \n  - [Manas Talukdar](https://github.com/manastalukdar)\n  \n  - [Simranjeet Singh](https://github.com/smrnjeet222)\n  \n  - [Aaron Meese](https://github.com/ajmeese7)\n  \n  - [Prasad Narkhede](https://github.com/p014ri5)\n  \n  - [Manish Kushwaha](https://github.com/tzmanish)\n  \n  - [Hedy Li](https://github.com/hedythedev)\n  \n  - [SHIMIZU Taku](https://github.com/takuan-osho)\n  \n  - [Jude Wilson](https://github.com/mr-winson)\n  \n  - [Daniel Rowe](https://github.com/DanRowe)\n  \n  - [Muhammad Hassan Ahmed](https://github.com/hassan11196)\n  \n  - [Alessandro Maggio](https://github.com/Tkd-Alex)\n  \n  - [Siddharth Gupta](https://github.com/siddg97)\n  \n  - [Dev-Mehta](https://github.com/Dev-Mehta/)\n  \n  - [> EdgyCoder ‚úå](https://github.com/edgycoder)\n  \n  - [Korel Kashri](https://github.com/korelkashri)\n  \n  - [Gustavo Barbosa](https://github.com/gusbdev)\n\n  - [eagleanurag](https://github.com/eagleanurag)\n  \n  - [Aravind V. Nair](https://github.com/aravindvnair99)\n  \n  - [Raman Preet Singh](https://github.com/raman08)\n  \n  - [Hayat Tamboli](https://github.com/hayat-tamboli)\n  \n  - [Henry Boisdequin](https://github.com/henryboisdequin)\n   \n  - [Raman Preet Singh](https://github.com/raman08)\n  \n  - [Aadit Kamat](https://github.com/aaditkamat)\n    \n  - [Subhalingam D](https://github.com/subhalingamd)\n  \n  - [Adil Akhmetov](https://github.com/weeebdev)\n  \n  - [Isaac Maldonado](https://github.com/einjunge99)\n  \n  - [Syed Faateh Sultan Kazmi](https://github.com/faatehsultan)\n\n  - [Shreyam Maity](https://github.com/ShreyamMaity)\n\n  - [Sufiane](https://github.com/sufiane)\n\n  - [Muhammad Bilal](https://github.com/BilalJaved15)\n\n  - [Waterdev](https://github.com/UnrealValentin)\n  \n  - [Aditya Prasad S](https://github.com/adityaprasad502)\n  \n  - [C. Vinicius Santos](https://github.com/c-viniciussantos)\n  \n  - [James Tufarelli](https://github.com/Minituff)\n  \n  - [Muhammad Bilal](https://github.com/BilalJaved15)\n\n  - [Wyatt Walsh](https://www.github.com/wyattowalsh)\n\n  - [Nithin Balaji](https://github.com/thenithinbalaji)\n\n  - [John Cort√©s](https://github.com/johncortes117)\n\n  - [Taha Parker](https://github.com/tahayparker/)\n\n  - [Yang An Yi](https://github.com/yanganyi)\n\n  - [Mamdud Hasan](https://github.com/n8fury)\n\n  - [Satyam Vyas](https://github.com/SatyamVyas04)\n\n  - [Yash Naravade](https://github.com/yashnaravade)\n\n  - [Luicen Loua](https://github.com/lucien-loua)\n\n  \n\n</details>\n\n- And you! If ever you are using it right now and you are not on the list please tell us by sending a [Special Mention](https://github.com/anmol098/waka-readme-stats/issues/new/choose) issue! :blush: \\\n  We will be glad to add you in the list.\n\n\nMade with :heart: and Python üêç.\n\n# Inspired From\n\n> [Awesome Pinned Gists](https://github.com/matchai/awesome-pinned-gists) <br/>\n> [athul/waka-readme](https://github.com/athul/waka-readme)\n\n### This project needs a **star** ‚≠ê from you ‚ô•.\n\n\n## Stargazers over time\n\n[![Stargazers over time](https://starchart.cc/anmol098/waka-readme-stats.svg)](https://starchart.cc/anmol098/waka-readme-stats)\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 3648, "quality_score": 0.69, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 466}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "> Buscando por Tradu√ß√µes em diferentes linguas & locais  [#23](https://github.com/anmol098/waka-readme-stats/issues/23)\n\n# M√©tricas de Desenvolvimento no Readme com feature flags adicion√°veis üéå\n\n![Project Preview](https://user-images.githubusercontent.com/25841814/79395484-5081ae80-7fac-11ea-9e27-ac91472e31dd.png)\n\n<p align=\"center\">\n  \n  ![Project Preview](https://user-images.githubusercontent.com/15426564/88030180-8e1c4780-cb58-11ea-8a8b-b3576dd73652.png)\n  \n  <h3 align=\"center\">üìå‚ú®Incr√≠veis Estat√≠sticas no Readme</h3>\n</p>\n\n----\n\n<p align=\"center\">\n   <img src=\"https://img.shields.io/badge/language-python-blue?style\"/>\n   <img src=\"https://img.shields.io/github/license/anmol098/waka-readme-stats\"/>\n   <img src=\"https://img.shields.io/github/stars/anmol098/waka-readme-stats\"/>\n   <img src=\"https://img.shields.io/github/forks/anmol098/waka-readme-stats\"/>\n   <img src=\"https://img.shields.io/static/v1?label=%F0%9F%8C%9F&message=If%20Useful&style=style=flat&color=BC4E99\" alt=\"Star Badge\"/>\n</p>\n<p align=\"center\">\n   Voc√™ √© diurno üê§ ou noturno ü¶â?\n   <br/>\n   Quando voc√™ √© mais produtivo durante o dia?\n   <br/>\n   Quais s√£o as linguagens com que voc√™ programa?\n   <br/>\n   Vamos ver isso em seu perfil!\n</p>\n\n<p align=\"center\">\n    <a href=\"https://github.com/anmol098/waka-readme-stats/issues\">Reporte Bug</a>\n    ¬∑\n    <a href=\"https://github.com/anmol098/waka-readme-stats/issues\">Solicite Funcionalidades</a>\n  </p>\n\n## Configura√ß√£o Pr√©via\n\n1. Voc√™ precisa atualizar o arquivo markdown(.md) com 2 coment√°rios. Verifique [aqui](#atualize-seu-readme) como fazer isso.\n2. Voc√™ vai precisar de uma API Key do WakaTime. Voc√™ pode conseguir isso nas configura√ß√µes da conta do Wakatime\n    - Voc√™ pode verificar [aqui](#novo-no-wakatime), caso seja novo no WakaTime\n3. Voc√™ vai precisar de um API Token do GitHub com escopo de `repo` e `user` que poder√° ser encontrado [aqui](https://github.com/settings/tokens) caso esteja usando o Action para pegar as m√©tricas de commits\n   > habilitar o escopo de `repo` parece **PERIGOSO**<br/>\n   > mas esse GitHub Action apenas acessa a data e hora do seu commit e as linhas de c√≥digos adicionadas ou deletadas no reposit√≥rio que voc√™ contribuiu.\n   - Voc√™ pode usar [esse](#perfil-do-reposit√≥rio) exemplo como modelo\n4. Voc√™ precisa salvar a API Key do Wakatime e o API Token do GitHub no secrets do reposit√≥rio. Voc√™ pode encontrar isso nas configura√ß√µes do seu reposit√≥rio. Certifique-se de salva-los como mostra no exemplo abaixo.\n    - API Key do WakaTime como `WAKATIME_API_KEY=<your wakatime API Key>`\n    - Token de Acesso Pessoal do GitHub como `GH_TOKEN=<your github access token>`\n5. Voc√™ pode habilitar e desabilitar as feature flags baseando-se em seus requerimentos.\n\n\nEssa A√ß√£o ser√° executada todos os dias √†s 00.00 IST\n\n## Atualize seu Readme\n\nAdicione um coment√°rio igual a esse em seu `README.md`:\n\n```md\n<!--START_SECTION:waka-->\n<!--END_SECTION:waka-->\n```\n\nEstas linhas ser√£o nosso ponto de entrada para as m√©tricas de desenvolvimento.\n\n## Novo no WakaTime\n\nWakaTime te d√° uma ideia do tempo que voc√™ realmente gastou programando. Isso te ajuda a dar um boost na sua produtividade e no seu lado competitivo.\n\n- V√° para <https://wakatime.com> e crie uma conta.\n- Gere sua API Key do WakaTime em [Account Settings in WakaTime](https://wakatime.com/settings/account).\n- Instale o [WakaTime plugin](https://wakatime.com/plugins) no seu editor favorito / IDE.\n- Cole sua API key para iniciar suas al√°lises.\n\n### Perfil do Reposit√≥rio\n\nVoc√™ vai precisar de um [GitHub Access Token](https://docs.github.com/en/actions/configuring-and-managing-workflows/authenticating-with-the-github_token) com escopo de `repo` e `user` e salva-lo no Secrets do reposit√≥rio `GH_TOKEN = <Your GitHub Access Token>`\n\nAqui est√° um exemplo do arquivo com o Workflow para executa-lo:\n\n```yml\nname: Waka Readme\n\non:\n  schedule:\n    # Runs at 12am IST\n    - cron: '30 18 * * *'\n\njobs:\n  update-readme:\n    name: Update Readme with Metrics\n    runs-on: ubuntu-latest\n    steps:\n      - uses: anmol098/waka-readme-stats@master\n        with:\n          WAKATIME_API_KEY: ${{ secrets.WAKATIME_API_KEY }}\n          GH_TOKEN: ${{ secrets.GH_TOKEN }}\n```\n## Extras\n\n1. Caso queira adicionar outras informa√ß√µes em suas estat√≠sticas, voc√™ pode adicionar multiplas `FLAGS` no arquivo de workflow. Por padr√£o, todas as flags est√£o habilitadas \n>Exceto a flag de linhas de c√≥digos devido ao peso de seu processamento\n\n```yml\n- uses: anmol098/waka-readme-stats@master\n        with:\n          WAKATIME_API_KEY: ${{ secrets.WAKATIME_API_KEY }}\n          GH_TOKEN: ${{ secrets.GH_TOKEN }}\n          SHOW_OS: \"False\"\n          SHOW_PROJECTS: \"False\"\n```\n\n### Flags Dispon√≠veis\n\n---\n\n`LOCALE`  Essa Flag pode ser usada para mostrar as estat√≠sticas em sua l√≠ngua, o padr√£o √© Ingl√™s, use Locale [Forma Abreviada](https://saimana.com/list-of-country-locale-code/) para atribuir a vari√°vel na flag. Um exemplo do resultado final pode ser econtrado [aqui](https://github.com/anmol098/anmol098/blob/master/Readme-fr.md)\n\n\n`SHOW_LINES_OF_CODE`       essa flag pode ser configurada para `True` para mostrar as linhas de c√≥digos escritas at√© a data atual\n\n![Linhas de C√≥digos](https://img.shields.io/badge/Desde%20o%20Hello%20World%20Eu%20Escrevi-1.3%20milh√µes%20de%20Linhas%20de%20C√≥digo-blue)\n\n`SHOW_PROFILE_VIEWS`       essa flag pode ser configurada para `False` para ocultar as Vizualiza√ß√µes do Perfil\n\n![Vizualiza√ß√µes do Perfil](http://img.shields.io/badge/Vizualiza√ß√µes%20do%20Perfil-2189-blue)\n\n\n`SHOW_COMMIT`       essa flag pode ser configurada para `False` para ocultar as estat√≠sticas de commits\n\n**Eu sou Diurno üê§** \n```text\nüåû Manh√£    95 commits     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   30.55% \nüåÜ Tarde    78 commits     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   25.08% \nüåÉ Noite    112 commits    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   36.01% \nüåô Madrugada      26 commits     ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   8.36%\n\n```\n\n`SHOW_DAYS_OF_WEEK`       essa flag pode ser configurada para `False` para ocultar os commits feitos em diferentes dias da semana\n\nüìÖ **Eu Sou Mais Produtivo aos Domingos** \n\n```text\nSegunda-Feira       50 commits     ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   13.19% \nTer√ßa-Feira      85 commits     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   22.43% \nQuarta-Feira    56 commits     ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   14.78% \nQuinta-Feira     44 commits     ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   11.61% \nSexta-Feira       28 commits     ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   7.39% \nS√°bado     30 commits     ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   7.92% \nDomingo       86 commits     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   22.69%\n\n```\n\n`SHOW_LANGUAGE`       essa flag pode ser configurada para `False` para ocultar as linguagens de programa√ß√£o que voc√™ usa\n\n```text\nüí¨ Linguagens de Programa√ß√£o:\nJavaScript               5 hrs 26 mins       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   61.97%\nPHP                      1 hr 35 mins        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   18.07%\nMarkdown                 1 hr 9 mins         ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   13.3%\nPython                   22 mins             ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   4.32%\nXML                      8 mins              ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   1.62%\n```\n\n\n`SHOW_OS`       essa flag pode ser configurada para `False` para ocultar os detalhes do sistema operacional\n\n```text\nüíª Sistemas Operacionais:\nWindows                  8 hrs 46 mins       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   100.0%\n```\n\n`SHOW_PROJECTS` essa flag pode ser configurada para `False` para ocultar os Projetos que voc√™ trabalhou\n\n```text\nüê±‚Äçüíª Projetos:\nctx_connector            4 hrs 3 mins        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   46.33%\nNetSuite-Connector       1 hr 31 mins        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   17.29%\nmango-web-master         1 hr 12 mins        ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   13.77%\ncable                    54 mins             ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   10.41%\ndenAPI                   40 mins             ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   7.66%\n```\n\n`SHOW_TIMEZONE` essa flag pode ser configurada para `False` para ocultar o fuso hor√°rio em que voc√™ est√°\n\n```text\n‚åöÔ∏é Fuso hor√°rio: America/Sao_Paulo\n```\n\n`SHOW_EDITORS`  essa flag pode ser configurada para `False` para ocultar os editores de c√≥digos usados\n\n```text\nüî• Editores:\nWebStorm                 6 hrs 47 mins       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   77.43%\nPhpStorm                 1 hr 35 mins        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   18.07%\nPyCharm                  23 mins             ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   4.49%\n```\n\n`SHOW_LANGUAGE_PER_REPO`  essa flag pode ser configurada para `False` para ocultar o n√∫mero de reposit√≥rios com linguagens e frameworks diferentes\n\n**Eu geralmente programo em Vue** \n\n```text\nVue          8 repos        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   25.0% \nJava         6 repos        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   18.75% \nJavaScript   6 repos        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   18.75% \nPHP          3 repos        ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   9.38% \nPython       2 repos        ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   6.25% \nDart         2 repos        ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   6.25% \nCSS          2 repos        ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   6.25%\n\n```\n\n`SHOW_SHORT_INFO`  essa flag pode ser configurada para `False` para ocultar pequenas informa√ß√µes sobre o usu√°rio\n>Essa se√ß√£o requer um token pessoal de acesso com permiss√£o de usu√°rio, caso contr√°rio, os dados mostrados aqui estar√£o incorretos\n\n**üê± Meus Dados no GitHub** \n\n> üèÜ 433 Contribui√ß√µes no ano de 2020\n > \n> üì¶ Usado 292.3 kB no armazenamento do GitHub \n > \n> üíº Aberto para contrata√ß√£o\n > \n> üìú 25 Reposit√≥rios P√∫blicos \n > \n> üîë 15 Reposit√≥rios Privados \n\n`SHOW_LOC_CHART`  essa flag pode ser configurada para `False` para ocultar as linhas de c√≥digos escritas em diferentes trimestres do ano\n\n**Linha do Tempo**\n\n![Gr√°fico n√£o Encontrado](https://raw.githubusercontent.com/anmol098/anmol098/master/charts/bar_graph.png) \n\n## :sparkling_heart: Apoie o Projeto\n\nEu uso c√≥digo aberto em quase tudo que eu posso, e eu tento responder a todos que est√£o precisando de ajuda usando esse projeto. √ìbvio que isso demanda tempo. Voc√™ pode usar esse servi√ßo de gra√ßa.\n\nEntretanto, caso voc√™ esteja usando esse projeto e esteja feliz com isso ou apenas quer me incentivar a continuar criando solu√ß√µes, tem algumas maneiras em que voc√™ pode fazer isso:-\n\n- Dando cr√©ditos a mim quando usar essa a√ß√£o no seu readme, e linkando-o de volta para esse reposit√≥rio :D\n- Dando uma star e compartilhando o projeto :rocket:\n- [![paypal.me/aapreneur](https://ionicabizau.github.io/badges/paypal.svg)](https://www.paypal.me/aapreneur) - Voc√™ pode fazer uma doa√ß√£o via PayPal. Eu prov√°velmente irei comprar ~~cerveja~~ vinho üç∑\n\nObrigado! :heart:\n\n---\n\n# Contribui√ß√µes\n\nContribui√ß√µes s√£o bem vindas! ‚ô•! Por favor compartilhe qualquer funcionalidade e adicione testes unit√°rios! Use o sistema de pull request e issue para contribuir.\n\n# Contribuidores Selecionados\n\n1. [Anmol Pratap Singh](https://github.com/anmol098): Mantenedor\n2. [Prabhat Singh](https://github.com/prabhatdev): Pelo gr√°fico de linha do tempo de c√≥digo [#18](https://github.com/anmol098/waka-readme-stats/pull/18)\n3. [Hedy Li](https://github.com/hedythedev): Pelo Pull Request [#34](https://github.com/anmol098/waka-readme-stats/pull/34) e [#23](https://github.com/anmol098/waka-readme-stats/pull/23)\n4. [Pedro Torres](https://github.com/Corfucinas): Pelo Pull Request [#29](https://github.com/anmol098/waka-readme-stats/pull/29)\n5. [Aaron Meese](https://github.com/ajmeese7): Pelo Pull Request [#45](https://github.com/anmol098/waka-readme-stats/pull/45)\n6. [Arnav Jindal](https://github.com/Daggy1234): Pelo Pull Request [#48](https://github.com/anmol098/waka-readme-stats/pull/48)\n7. [Daniel Rowe](https://github.com/DanRowe): Pelo Pull Request [#57](https://github.com/anmol098/waka-readme-stats/pull/57)\n8. [Ss5h](https://github.com/tlatkdgus1): Por adicionar suporte de escrita de frase natural para tradu√ß√£o [#136](https://github.com/anmol098/waka-readme-stats/pull/136)\n\n<details>\n<summary>Men√ß√£o especial para aqueles que est√£o atualmente deixando seus readmes mais incr√≠veis :smile: :tada:</summary>\n\n  - [Stanislas](https://github.com/angristan)\n  \n  - [Pratik Kumar](https://github.com/pr2tik1)\n  \n  - [Vladimir](https://github.com/sergeev-vn)\n\n  - [Pedro Torres](https://github.com/Corfucinas)\n  \n  - [leverglowh](https://github.com/leverglowh)\n  \n  - [patdc](https://github.com/patdc)\n  \n  - [ÊûÅÂÆ¢ÊåñÊéòÊú∫](https://github.com/meteor1993)\n  \n  - [Fan()](https://github.com/Fanduzi)\n  \n  - [Miller Camilo Vega](https://github.com/minoveaz)\n  \n  - [XLor](https://github.com/yjl9903)\n  \n  - [Jesse Okeya](https://github.com/jesseokeya)\n  \n  - [anaiel](https://github.com/anaiel)\n  \n  - [Dipto Mondal](https://github.com/diptomondal007)\n  \n  - [Jerry F. Zhang](https://github.com/JerryFZhang)\n  \n  - [Karan Singh](https://github.com/karan06126)\n  \n  - [Erwin Lejeune](https://github.com/guilyx)\n  \n  - [Manuel Cepeda](https://github.com/mecm1993)\n  \n  - [Jonathan S](https://github.com/TGTGamer)\n  \n  - [Tsotne Gvadzabia](https://github.com/RockiRider)\n  \n  - [Miray](https://github.com/MirayXS)\n  \n  - [Varad Patil](https://github.com/varadp2000)\n  \n  - [Prabhat Singh](https://github.com/prabhatdev)\n  \n  - [Nikhil](https://github.com/nikhilgorantla)\n  \n  - [Â§ßÁôΩ](https://github.com/2720851545)\n  \n  - [Du Yizhuo](https://github.com/dyzdyz010)\n  \n  - [Manas Talukdar](https://github.com/manastalukdar)\n  \n  - [Simranjeet Singh](https://github.com/smrnjeet222)\n  \n  - [Aaron Meese](https://github.com/ajmeese7)\n  \n  - [Prasad Narkhede](https://github.com/p014ri5)\n  \n  - [Manish Kushwaha](https://github.com/tzmanish)\n  \n  - [Hedy Li](https://github.com/hedythedev)\n  \n  - [SHIMIZU Taku](https://github.com/takuan-osho)\n  \n  - [Jude Wilson](https://github.com/mr-winson)\n  \n  - [Daniel Rowe](https://github.com/DanRowe)\n  \n  - [Muhammad Hassan Ahmed](https://github.com/hassan11196)\n  \n  - [Alessandro Maggio](https://github.com/Tkd-Alex)\n  \n  - [Siddharth Gupta](https://github.com/siddg97)\n  \n  - [Dev-Mehta](https://github.com/Dev-Mehta/)\n  \n  - [> EdgyCoder ‚úå](https://github.com/edgycoder)\n  \n  - [> EdgyCoder ‚úå](https://github.com/edgycoder)\n  \n  - [Korel Kashri](https://github.com/korelkashri)\n  \n  - [Gustavo Barbosa](https://github.com/gusbdev)\n\n  - [eagleanurag](https://github.com/eagleanurag)\n  \n  - [Aravind V. Nair](https://github.com/aravindvnair99)\n  \n  - [Raman Preet Singh](https://github.com/raman08)\n  \n  - [Hayat Tamboli](https://github.com/hayat-tamboli)\n  \n  - [Henry Boisdequin](https://github.com/henryboisdequin)\n   \n  - [Raman Preet Singh](https://github.com/raman08)\n\n  \n\n</details>\n\n- E voc√™! Caso esteja usando isso agora e seu nome n√£o esteja na lista, por favor contacte-nos enviando um [Men√ß√£o Especial](https://github.com/anmol098/waka-readme-stats/issues/new/choose) issue :blush: n√≥s ficaremos gratos em adicionar voc√™ na lista.\n\n\nFeito com :heart: e Python üêç.\n\n# Inspirado por\n\n> [Awesome Pinned Gists](https://github.com/matchai/awesome-pinned-gists) <br/>\n> [athul/waka-readme](https://github.com/athul/waka-readme)\n\n### Esse projeto precisa de uma **star** ‚≠ê sua ‚ô•.\n", "metadata": {"source": "github.com/anmol098/waka-readme-stats", "file_path": "README_pt-BR.md", "language": "markdown", "complexity": "complex", "star_count": 3648, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 380}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Generate Stats Images\n\non:\n  push:\n    branches: [ master ]\n  schedule:\n    - cron: \"5 0 * * *\"\n  workflow_dispatch:\n\npermissions:\n  contents: write\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n    # Check out repository under $GITHUB_WORKSPACE, so the job can access it\n    - uses: actions/checkout@v3\n\n    # Run using Python 3.8 for consistency and aiohttp\n    - name: Set up Python 3.8\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.8'\n        architecture: 'x64'\n        cache: 'pip'\n\n    # Install dependencies with `pip`\n    - name: Install requirements\n      run: |\n        python3 -m pip install --upgrade pip setuptools wheel\n        python3 -m pip install -r requirements.txt\n\n    # Generate all statistics images\n    - name: Generate images\n      run: |\n        python3 --version\n        python3 generate_images.py\n      env:\n        ACCESS_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        EXCLUDED: ${{ secrets.EXCLUDED }}\n        EXCLUDED_LANGS: ${{ secrets.EXCLUDED_LANGS }}\n        EXCLUDE_FORKED_REPOS: true\n\n    # Commit all changed files to the repository\n    - name: Commit to the repo\n      run: |\n        git config --global user.name \"jstrieb/github-stats\"\n        git config --global user.email \"github-stats[bot]@jstrieb.github.io\"\n        git add .\n        # Force the build to succeed, even if no files were changed\n        git commit -m 'Update generated files' || true\n        git push\n", "metadata": {"source": "github.com/jstrieb/github-stats", "file_path": ".github/workflows/main.yml", "language": "yaml", "complexity": "medium", "star_count": 3211, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 56}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# [GitHub Stats Visualization](https://github.com/jstrieb/github-stats)\n\n<!--\nhttps://github.community/t/support-theme-context-for-images-in-light-vs-dark-mode/147981/84\n-->\n<a href=\"https://github.com/jstrieb/github-stats\">\n<img src=\"https://github.com/jstrieb/github-stats/blob/master/generated/overview.svg#gh-dark-mode-only\" />\n<img src=\"https://github.com/jstrieb/github-stats/blob/master/generated/languages.svg#gh-dark-mode-only\" />\n<img src=\"https://github.com/jstrieb/github-stats/blob/master/generated/overview.svg#gh-light-mode-only\" />\n<img src=\"https://github.com/jstrieb/github-stats/blob/master/generated/languages.svg#gh-light-mode-only\" />\n</a>\n\nGenerate visualizations of GitHub user and repository statistics with GitHub\nActions. Visualizations can include data for both private repositories, and for\nrepositories you have contributed to, but do not own.\n\nGenerated images automatically switch between GitHub light theme and GitHub\ndark theme.\n\n## Background\n\nWhen someone views a profile on GitHub, it is often because they are curious\nabout a user's open source projects and contributions. Unfortunately, that\nuser's stars, forks, and pinned repositories do not necessarily reflect the\ncontributions they make to private repositories. The data likewise does not\npresent a complete picture of the user's total contributions beyond the current\nyear.\n\nThis project aims to collect a variety of profile and repository statistics\nusing the GitHub API. It then generates images that can be displayed in\nrepository READMEs, or in a user's [Profile\nREADME](https://docs.github.com/en/github/setting-up-and-managing-your-github-profile/managing-your-profile-readme).\n\nSince the project runs on GitHub Actions, no server is required to regularly\nregenerate the images with updated statistics. Likewise, since the user runs\nthe analysis code themselves via GitHub Actions, they can use their GitHub\naccess token to collect statistics on private repositories that an external\nservice would be unable to access.\n\n## Disclaimer\n\nIf the project is used with an access token that has sufficient permissions to\nread private repositories, it may leak details about those repositories in\nerror messages. For example, the `aiohttp` library‚Äîused for asynchronous API\nrequests‚Äîmay include the requested URL in exceptions, which can leak the name\nof private repositories. If there is an exception caused by `aiohttp`, this\nexception will be viewable in the Actions tab of the repository fork, and\nanyone may be able to see the name of one or more private repositories.\n\nDue to some issues with the GitHub statistics API, there are some situations\nwhere it returns inaccurate results. Specifically, the repository view count\nstatistics and total lines of code modified are probably somewhat inaccurate.\nUnexpectedly, these values will become more accurate over time as GitHub\ncaches statistics for your repositories. Additionally, repositories that were\nlast contributed to more than a year ago may not be included in the statistics\ndue to limitations in the results returned by the API.\n\nFor more information on inaccuracies, see issue\n[#2](https://github.com/jstrieb/github-stats/issues/2),\n[#3](https://github.com/jstrieb/github-stats/issues/3), and\n[#13](https://github.com/jstrieb/github-stats/issues/13).\n\n# Installation\n\n<!-- TODO: Add details and screenshots -->\n\n1. Create a personal access token (not the default GitHub Actions token) using\n   the instructions\n   [here](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token).\n   Personal access token must have permissions: `read:user` and `repo`. Copy\n   the access token when it is generated ‚Äì if you lose it, you will have to\n   regenerate the token.\n   - Some users are reporting that it can take a few minutes for the personal\n     access token to work. For more, see \n     [#30](https://github.com/jstrieb/github-stats/issues/30).\n2. Create a copy of this repository by clicking\n   [here](https://github.com/jstrieb/github-stats/generate). Note: this is\n   **not** the same as forking a copy because it copies everything fresh,\n   without the huge commit history. \n3. Go to the \"Secrets\" page of your copy of the repository. If this is the\n   README of your copy, click [this link](../../settings/secrets/actions) to go\n   to the \"Secrets\" page. Otherwise, go to the \"Settings\" tab of the\n   newly-created repository and go to the \"Secrets\" page (bottom left).\n4. Create a new secret with the name `ACCESS_TOKEN` and paste the copied\n   personal access token as the value.\n5. It is possible to change the type of statistics reported by adding other\n   repository secrets. \n   - To ignore certain repos, add them (in owner/name format e.g.,\n     `jstrieb/github-stats`) separated by commas to a new secret‚Äîcreated as\n     before‚Äîcalled `EXCLUDED`.\n   - To ignore certain languages, add them (separated by commas) to a new\n     secret called `EXCLUDED_LANGS`. For example, to exclude HTML and TeX you\n     could set the value to `html,tex`.\n   - To show statistics only for \"owned\" repositories and not forks with\n     contributions, add an environment variable (under the `env` header in the\n     [main\n     workflow](https://github.com/jstrieb/github-stats/blob/master/.github/workflows/main.yml))\n     called `EXCLUDE_FORKED_REPOS` with a value of `true`.\n   - These other values are added as secrets by default to prevent leaking\n     information about private repositories. If you're not worried about that,\n     you can change the values directly [in the Actions workflow\n     itself](https://github.com/jstrieb/github-stats/blob/05de1314b870febd44d19ad2f55d5e59d83f5857/.github/workflows/main.yml#L48-L53).\n6. Go to the [Actions\n   Page](../../actions?query=workflow%3A\"Generate+Stats+Images\") and press \"Run\n   Workflow\" on the right side of the screen to generate images for the first\n   time. \n   - The images will be automatically regenerated every 24 hours, but they can\n     be regenerated manually by running the workflow this way.\n7. Take a look at the images that have been created in the\n   [`generated`](generated) folder.\n8. To add your statistics to your GitHub Profile README, copy and paste the\n   following lines of code into your markdown content. Change the `username`\n   value to your GitHub username.\n   ```md\n   ![](https://raw.githubusercontent.com/username/github-stats/master/generated/overview.svg#gh-dark-mode-only)\n   ![](https://raw.githubusercontent.com/username/github-stats/master/generated/overview.svg#gh-light-mode-only)\n   ```\n   ```md\n   ![](https://raw.githubusercontent.com/username/github-stats/master/generated/languages.svg#gh-dark-mode-only)\n   ![](https://raw.githubusercontent.com/username/github-stats/master/generated/languages.svg#gh-light-mode-only)\n   ```\n9. Link back to this repository so that others can generate their own\n   statistics images.\n10. Star this repo if you like it!\n\n\n# Support the Project\n\nThere are a few things you can do to support the project:\n\n- Star the repository (and follow me on GitHub for more)\n- Share and upvote on sites like Twitter, Reddit, and Hacker News\n- Report any bugs, glitches, or errors that you find\n\nThese things motivate me to keep sharing what I build, and they provide\nvalidation that my work is appreciated! They also help me improve the\nproject. Thanks in advance!\n\nIf you are insistent on spending money to show your support, I encourage you to\ninstead make a generous donation to one of the following organizations. By advocating\nfor Internet freedoms, organizations like these help me to feel comfortable\nreleasing work publicly on the Web.\n\n- [Electronic Frontier Foundation](https://supporters.eff.org/donate/)\n- [Signal Foundation](https://signal.org/donate/)\n- [Mozilla](https://donate.mozilla.org/en-US/)\n- [The Internet Archive](https://archive.org/donate/index.php)\n\n\n# Related Projects\n\n- Inspired by a desire to improve upon\n  [anuraghazra/github-readme-stats](https://github.com/anuraghazra/github-readme-stats)\n- Makes use of [GitHub Octicons](https://primer.style/octicons/) to precisely\n  match the GitHub UI\n", "metadata": {"source": "github.com/jstrieb/github-stats", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 3211, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 156}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/python3\r\n\r\nimport asyncio\r\nimport os\r\nimport re\r\n\r\nimport aiohttp\r\n\r\nfrom github_stats import Stats\r\n\r\n\r\n################################################################################\r\n# Helper Functions\r\n################################################################################\r\n\r\n\r\ndef generate_output_folder() -> None:\r\n    \"\"\"\r\n    Create the output folder if it does not already exist\r\n    \"\"\"\r\n    if not os.path.isdir(\"generated\"):\r\n        os.mkdir(\"generated\")\r\n\r\n\r\n################################################################################\r\n# Individual Image Generation Functions\r\n################################################################################\r\n\r\n\r\nasync def generate_overview(s: Stats) -> None:\r\n    \"\"\"\r\n    Generate an SVG badge with summary statistics\r\n    :param s: Represents user's GitHub statistics\r\n    \"\"\"\r\n    with open(\"templates/overview.svg\", \"r\") as f:\r\n        output = f.read()\r\n\r\n    output = re.sub(\"{{ name }}\", await s.name, output)\r\n    output = re.sub(\"{{ stars }}\", f\"{await s.stargazers:,}\", output)\r\n    output = re.sub(\"{{ forks }}\", f\"{await s.forks:,}\", output)\r\n    output = re.sub(\"{{ contributions }}\", f\"{await s.total_contributions:,}\", output)\r\n    changed = (await s.lines_changed)[0] + (await s.lines_changed)[1]\r\n    output = re.sub(\"{{ lines_changed }}\", f\"{changed:,}\", output)\r\n    output = re.sub(\"{{ views }}\", f\"{await s.views:,}\", output)\r\n    output = re.sub(\"{{ repos }}\", f\"{len(await s.repos):,}\", output)\r\n\r\n    generate_output_folder()\r\n    with open(\"generated/overview.svg\", \"w\") as f:\r\n        f.write(output)\r\n\r\n\r\nasync def generate_languages(s: Stats) -> None:\r\n    \"\"\"\r\n    Generate an SVG badge with summary languages used\r\n    :param s: Represents user's GitHub statistics\r\n    \"\"\"\r\n    with open(\"templates/languages.svg\", \"r\") as f:\r\n        output = f.read()\r\n\r\n    progress = \"\"\r\n    lang_list = \"\"\r\n    sorted_languages = sorted(\r\n        (await s.languages).items(), reverse=True, key=lambda t: t[1].get(\"size\")\r\n    )\r\n    delay_between = 150\r\n    for i, (lang, data) in enumerate(sorted_languages):\r\n        color = data.get(\"color\")\r\n        color = color if color is not None else \"#000000\"\r\n        progress += (\r\n            f'<span style=\"background-color: {color};'\r\n            f'width: {data.get(\"prop\", 0):0.3f}%;\" '\r\n            f'class=\"progress-item\"></span>'\r\n        )\r\n        lang_list += f\"\"\"\r\n<li style=\"animation-delay: {i * delay_between}ms;\">\r\n<svg xmlns=\"http://www.w3.org/2000/svg\" class=\"octicon\" style=\"fill:{color};\"\r\nviewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\"><path\r\nfill-rule=\"evenodd\" d=\"M8 4a4 4 0 100 8 4 4 0 000-8z\"></path></svg>\r\n<span class=\"lang\">{lang}</span>\r\n<span class=\"percent\">{data.get(\"prop\", 0):0.2f}%</span>\r\n</li>\r\n\r\n\"\"\"\r\n\r\n    output = re.sub(r\"{{ progress }}\", progress, output)\r\n    output = re.sub(r\"{{ lang_list }}\", lang_list, output)\r\n\r\n    generate_output_folder()\r\n    with open(\"generated/languages.svg\", \"w\") as f:\r\n        f.write(output)\r\n\r\n\r\n################################################################################\r\n# Main Function\r\n################################################################################\r\n\r\n\r\nasync def main() -> None:\r\n    \"\"\"\r\n    Generate all badges\r\n    \"\"\"\r\n    access_token = os.getenv(\"ACCESS_TOKEN\")\r\n    if not access_token:\r\n        # access_token = os.getenv(\"GITHUB_TOKEN\")\r\n        raise Exception(\"A personal access token is required to proceed!\")\r\n    user = os.getenv(\"GITHUB_ACTOR\")\r\n    if user is None:\r\n        raise RuntimeError(\"Environment variable GITHUB_ACTOR must be set.\")\r\n    exclude_repos = os.getenv(\"EXCLUDED\")\r\n    excluded_repos = (\r\n        {x.strip() for x in exclude_repos.split(\",\")} if exclude_repos else None\r\n    )\r\n    exclude_langs = os.getenv(\"EXCLUDED_LANGS\")\r\n    excluded_langs = (\r\n        {x.strip() for x in exclude_langs.split(\",\")} if exclude_langs else None\r\n    )\r\n    # Convert a truthy value to a Boolean\r\n    raw_ignore_forked_repos = os.getenv(\"EXCLUDE_FORKED_REPOS\")\r\n    ignore_forked_repos = (\r\n        not not raw_ignore_forked_repos\r\n        and raw_ignore_forked_repos.strip().lower() != \"false\"\r\n    )\r\n    async with aiohttp.ClientSession() as session:\r\n        s = Stats(\r\n            user,\r\n            access_token,\r\n            session,\r\n            exclude_repos=excluded_repos,\r\n            exclude_langs=excluded_langs,\r\n            ignore_forked_repos=ignore_forked_repos,\r\n        )\r\n        await asyncio.gather(generate_languages(s), generate_overview(s))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    asyncio.run(main())\r\n", "metadata": {"source": "github.com/jstrieb/github-stats", "file_path": "generate_images.py", "language": "python", "complexity": "medium", "star_count": 3211, "quality_score": 0.72, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 137}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/python3\n\nimport asyncio\nimport os\nfrom typing import Dict, List, Optional, Set, Tuple, Any, cast\n\nimport aiohttp\nimport requests\n\n\n###############################################################################\n# Main Classes\n###############################################################################\n\n\nclass Queries(object):\n    \"\"\"\n    Class with functions to query the GitHub GraphQL (v4) API and the REST (v3)\n    API. Also includes functions to dynamically generate GraphQL queries.\n    \"\"\"\n\n    def __init__(\n        self,\n        username: str,\n        access_token: str,\n        session: aiohttp.ClientSession,\n        max_connections: int = 10,\n    ):\n        self.username = username\n        self.access_token = access_token\n        self.session = session\n        self.semaphore = asyncio.Semaphore(max_connections)\n\n    async def query(self, generated_query: str) -> Dict:\n        \"\"\"\n        Make a request to the GraphQL API using the authentication token from\n        the environment\n        :param generated_query: string query to be sent to the API\n        :return: decoded GraphQL JSON output\n        \"\"\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.access_token}\",\n        }\n        try:\n            async with self.semaphore:\n                r_async = await self.session.post(\n                    \"https://api.github.com/graphql\",\n                    headers=headers,\n                    json={\"query\": generated_query},\n                )\n            result = await r_async.json()\n            if result is not None:\n                return result\n        except:\n            print(\"aiohttp failed for GraphQL query\")\n            # Fall back on non-async requests\n            async with self.semaphore:\n                r_requests = requests.post(\n                    \"https://api.github.com/graphql\",\n                    headers=headers,\n                    json={\"query\": generated_query},\n                )\n                result = r_requests.json()\n                if result is not None:\n                    return result\n        return dict()\n\n    async def query_rest(self, path: str, params: Optional[Dict] = None) -> Dict:\n        \"\"\"\n        Make a request to the REST API\n        :param path: API path to query\n        :param params: Query parameters to be passed to the API\n        :return: deserialized REST JSON output\n        \"\"\"\n\n        for _ in range(60):\n            headers = {\n                \"Authorization\": f\"token {self.access_token}\",\n            }\n            if params is None:\n                params = dict()\n            if path.startswith(\"/\"):\n                path = path[1:]\n            try:\n                async with self.semaphore:\n                    r_async = await self.session.get(\n                        f\"https://api.github.com/{path}\",\n                        headers=headers,\n                        params=tuple(params.items()),\n                    )\n                if r_async.status == 202:\n                    # print(f\"{path} returned 202. Retrying...\")\n                    print(f\"A path returned 202. Retrying...\")\n                    await asyncio.sleep(2)\n                    continue\n\n                result = await r_async.json()\n                if result is not None:\n                    return result\n            except:\n                print(\"aiohttp failed for rest query\")\n                # Fall back on non-async requests\n                async with self.semaphore:\n                    r_requests = requests.get(\n                        f\"https://api.github.com/{path}\",\n                        headers=headers,\n                        params=tuple(params.items()),\n                    )\n                    if r_requests.status_code == 202:\n                        print(f\"A path returned 202. Retrying...\")\n                        await asyncio.sleep(2)\n                        continue\n                    elif r_requests.status_code == 200:\n                        return r_requests.json()\n        # print(f\"There were too many 202s. Data for {path} will be incomplete.\")\n        print(\"There were too many 202s. Data for this repository will be incomplete.\")\n        return dict()\n\n    @staticmethod\n    def repos_overview(\n        contrib_cursor: Optional[str] = None, owned_cursor: Optional[str] = None\n    ) -> str:\n        \"\"\"\n        :return: GraphQL query with overview of user repositories\n        \"\"\"\n        return f\"\"\"{{\n  viewer {{\n    login,\n    name,\n    repositories(\n        first: 100,\n        orderBy: {{\n            field: UPDATED_AT,\n            direction: DESC\n        }},\n        isFork: false,\n        after: {\"null\" if owned_cursor is None else '\"'+ owned_cursor +'\"'}\n    ) {{\n      pageInfo {{\n        hasNextPage\n        endCursor\n      }}\n      nodes {{\n        nameWithOwner\n        stargazers {{\n          totalCount\n        }}\n        forkCount\n        languages(first: 10, orderBy: {{field: SIZE, direction: DESC}}) {{\n          edges {{\n            size\n            node {{\n              name\n              color\n            }}\n          }}\n        }}\n      }}\n    }}\n    repositoriesContributedTo(\n        first: 100,\n        includeUserRepositories: false,\n        orderBy: {{\n            field: UPDATED_AT,\n            direction: DESC\n        }},\n        contributionTypes: [\n            COMMIT,\n            PULL_REQUEST,\n            REPOSITORY,\n            PULL_REQUEST_REVIEW\n        ]\n        after: {\"null\" if contrib_cursor is None else '\"'+ contrib_cursor +'\"'}\n    ) {{\n      pageInfo {{\n        hasNextPage\n        endCursor\n      }}\n      nodes {{\n        nameWithOwner\n        stargazers {{\n          totalCount\n        }}\n        forkCount\n        languages(first: 10, orderBy: {{field: SIZE, direction: DESC}}) {{\n          edges {{\n            size\n            node {{\n              name\n              color\n            }}\n          }}\n        }}\n      }}\n    }}\n  }}\n}}\n\"\"\"\n\n    @staticmethod\n    def contrib_years() -> str:\n        \"\"\"\n        :return: GraphQL query to get all years the user has been a contributor\n        \"\"\"\n        return \"\"\"\nquery {\n  viewer {\n    contributionsCollection {\n      contributionYears\n    }\n  }\n}\n\"\"\"\n\n    @staticmethod\n    def contribs_by_year(year: str) -> str:\n        \"\"\"\n        :param year: year to query for\n        :return: portion of a GraphQL query with desired info for a given year\n        \"\"\"\n        return f\"\"\"\n    year{year}: contributionsCollection(\n        from: \"{year}-01-01T00:00:00Z\",\n        to: \"{int(year) + 1}-01-01T00:00:00Z\"\n    ) {{\n      contributionCalendar {{\n        totalContributions\n      }}\n    }}\n\"\"\"\n\n    @classmethod\n    def all_contribs(cls, years: List[str]) -> str:\n        \"\"\"\n        :param years: list of years to get contributions for\n        :return: query to retrieve contribution information for all user years\n        \"\"\"\n        by_years = \"\\n\".join(map(cls.contribs_by_year, years))\n        return f\"\"\"\nquery {{\n  viewer {{\n    {by_years}\n  }}\n}}\n\"\"\"\n\n\nclass Stats(object):\n    \"\"\"\n    Retrieve and store statistics about GitHub usage.\n    \"\"\"\n\n    def __init__(\n        self,\n        username: str,\n        access_token: str,\n        session: aiohttp.ClientSession,\n        exclude_repos: Optional[Set] = None,\n        exclude_langs: Optional[Set] = None,\n        ignore_forked_repos: bool = False,\n    ):\n        self.username = username\n        self._ignore_forked_repos = ignore_forked_repos\n        self._exclude_repos = set() if exclude_repos is None else exclude_repos\n        self._exclude_langs = set() if exclude_langs is None else exclude_langs\n        self.queries = Queries(username, access_token, session)\n\n        self._name: Optional[str] = None\n        self._stargazers: Optional[int] = None\n        self._forks: Optional[int] = None\n        self._total_contributions: Optional[int] = None\n        self._languages: Optional[Dict[str, Any]] = None\n        self._repos: Optional[Set[str]] = None\n        self._lines_changed: Optional[Tuple[int, int]] = None\n        self._views: Optional[int] = None\n\n    async def to_str(self) -> str:\n        \"\"\"\n        :return: summary of all available statistics\n        \"\"\"\n        languages = await self.languages_proportional\n        formatted_languages = \"\\n  - \".join(\n            [f\"{k}: {v:0.4f}%\" for k, v in languages.items()]\n        )\n        lines_changed = await self.lines_changed\n        return f\"\"\"Name: {await self.name}\nStargazers: {await self.stargazers:,}\nForks: {await self.forks:,}\nAll-time contributions: {await self.total_contributions:,}\nRepositories with contributions: {len(await self.repos)}\nLines of code added: {lines_changed[0]:,}\nLines of code deleted: {lines_changed[1]:,}\nLines of code changed: {lines_changed[0] + lines_changed[1]:,}\nProject page views: {await self.views:,}\nLanguages:\n  - {formatted_languages}\"\"\"\n\n    async def get_stats(self) -> None:\n        \"\"\"\n        Get lots of summary statistics using one big query. Sets many attributes\n        \"\"\"\n        self._stargazers = 0\n        self._forks = 0\n        self._languages = dict()\n        self._repos = set()\n\n        exclude_langs_lower = {x.lower() for x in self._exclude_langs}\n\n        next_owned = None\n        next_contrib = None\n        while True:\n            raw_results = await self.queries.query(\n                Queries.repos_overview(\n                    owned_cursor=next_owned, contrib_cursor=next_contrib\n                )\n            )\n            raw_results = raw_results if raw_results is not None else {}\n\n            self._name = raw_results.get(\"data\", {}).get(\"viewer\", {}).get(\"name\", None)\n            if self._name is None:\n                self._name = (\n                    raw_results.get(\"data\", {})\n                    .get(\"viewer\", {})\n                    .get(\"login\", \"No Name\")\n                )\n\n            contrib_repos = (\n                raw_results.get(\"data\", {})\n                .get(\"viewer\", {})\n                .get(\"repositoriesContributedTo\", {})\n            )\n            owned_repos = (\n                raw_results.get(\"data\", {}).get(\"viewer\", {}).get(\"repositories\", {})\n            )\n\n            repos = owned_repos.get(\"nodes\", [])\n            if not self._ignore_forked_repos:\n                repos += contrib_repos.get(\"nodes\", [])\n\n            for repo in repos:\n                if repo is None:\n                    continue\n                name = repo.get(\"nameWithOwner\")\n                if name in self._repos or name in self._exclude_repos:\n                    continue\n                self._repos.add(name)\n                self._stargazers += repo.get(\"stargazers\").get(\"totalCount\", 0)\n                self._forks += repo.get(\"forkCount\", 0)\n\n                for lang in repo.get(\"languages\", {}).get(\"edges\", []):\n                    name = lang.get(\"node\", {}).get(\"name\", \"Other\")\n                    languages = await self.languages\n                    if name.lower() in exclude_langs_lower:\n                        continue\n                    if name in languages:\n                        languages[name][\"size\"] += lang.get(\"size\", 0)\n                        languages[name][\"occurrences\"] += 1\n                    else:\n                        languages[name] = {\n                            \"size\": lang.get(\"size\", 0),\n                            \"occurrences\": 1,\n                            \"color\": lang.get(\"node\", {}).get(\"color\"),\n                        }\n\n            if owned_repos.get(\"pageInfo\", {}).get(\n                \"hasNextPage\", False\n            ) or contrib_repos.get(\"pageInfo\", {}).get(\"hasNextPage\", False):\n                next_owned = owned_repos.get(\"pageInfo\", {}).get(\n                    \"endCursor\", next_owned\n                )\n                next_contrib = contrib_repos.get(\"pageInfo\", {}).get(\n                    \"endCursor\", next_contrib\n                )\n            else:\n                break\n\n        # TODO: Improve languages to scale by number of contributions to\n        #       specific filetypes\n        langs_total = sum([v.get(\"size\", 0) for v in self._languages.values()])\n        for k, v in self._languages.items():\n            v[\"prop\"] = 100 * (v.get(\"size\", 0) / langs_total)\n\n    @property\n    async def name(self) -> str:\n        \"\"\"\n        :return: GitHub user's name (e.g., Jacob Strieb)\n        \"\"\"\n        if self._name is not None:\n            return self._name\n        await self.get_stats()\n        assert self._name is not None\n        return self._name\n\n    @property\n    async def stargazers(self) -> int:\n        \"\"\"\n        :return: total number of stargazers on user's repos\n        \"\"\"\n        if self._stargazers is not None:\n            return self._stargazers\n        await self.get_stats()\n        assert self._stargazers is not None\n        return self._stargazers\n\n    @property\n    async def forks(self) -> int:\n        \"\"\"\n        :return: total number of forks on user's repos\n        \"\"\"\n        if self._forks is not None:\n            return self._forks\n        await self.get_stats()\n        assert self._forks is not None\n        return self._forks\n\n    @property\n    async def languages(self) -> Dict:\n        \"\"\"\n        :return: summary of languages used by the user\n        \"\"\"\n        if self._languages is not None:\n            return self._languages\n        await self.get_stats()\n        assert self._languages is not None\n        return self._languages\n\n    @property\n    async def languages_proportional(self) -> Dict:\n        \"\"\"\n        :return: summary of languages used by the user, with proportional usage\n        \"\"\"\n        if self._languages is None:\n            await self.get_stats()\n            assert self._languages is not None\n\n        return {k: v.get(\"prop\", 0) for (k, v) in self._languages.items()}\n\n    @property\n    async def repos(self) -> Set[str]:\n        \"\"\"\n        :return: list of names of user's repos\n        \"\"\"\n        if self._repos is not None:\n            return self._repos\n        await self.get_stats()\n        assert self._repos is not None\n        return self._repos\n\n    @property\n    async def total_contributions(self) -> int:\n        \"\"\"\n        :return: count of user's total contributions as defined by GitHub\n        \"\"\"\n        if self._total_contributions is not None:\n            return self._total_contributions\n\n        self._total_contributions = 0\n        years = (\n            (await self.queries.query(Queries.contrib_years()))\n            .get(\"data\", {})\n            .get(\"viewer\", {})\n            .get(\"contributionsCollection\", {})\n            .get(\"contributionYears\", [])\n        )\n        by_year = (\n            (await self.queries.query(Queries.all_contribs(years)))\n            .get(\"data\", {})\n            .get(\"viewer\", {})\n            .values()\n        )\n        for year in by_year:\n            self._total_contributions += year.get(\"contributionCalendar\", {}).get(\n                \"totalContributions\", 0\n            )\n        return cast(int, self._total_contributions)\n\n    @property\n    async def lines_changed(self) -> Tuple[int, int]:\n        \"\"\"\n        :return: count of total lines added, removed, or modified by the user\n        \"\"\"\n        if self._lines_changed is not None:\n            return self._lines_changed\n        additions = 0\n        deletions = 0\n        for repo in await self.repos:\n            r = await self.queries.query_rest(f\"/repos/{repo}/stats/contributors\")\n            for author_obj in r:\n                # Handle malformed response from the API by skipping this repo\n                if not isinstance(author_obj, dict) or not isinstance(\n                    author_obj.get(\"author\", {}), dict\n                ):\n                    continue\n                author = author_obj.get(\"author\", {}).get(\"login\", \"\")\n                if author != self.username:\n                    continue\n\n                for week in author_obj.get(\"weeks\", []):\n                    additions += week.get(\"a\", 0)\n                    deletions += week.get(\"d\", 0)\n\n        self._lines_changed = (additions, deletions)\n        return self._lines_changed\n\n    @property\n    async def views(self) -> int:\n        \"\"\"\n        Note: only returns views for the last 14 days (as-per GitHub API)\n        :return: total number of page views the user's projects have received\n        \"\"\"\n        if self._views is not None:\n            return self._views\n\n        total = 0\n        for repo in await self.repos:\n            r = await self.queries.query_rest(f\"/repos/{repo}/traffic/views\")\n            for view in r.get(\"views\", []):\n                total += view.get(\"count\", 0)\n\n        self._views = total\n        return total\n\n\n###############################################################################\n# Main Function\n###############################################################################\n\n\nasync def main() -> None:\n    \"\"\"\n    Used mostly for testing; this module is not usually run standalone\n    \"\"\"\n    access_token = os.getenv(\"ACCESS_TOKEN\")\n    user = os.getenv(\"GITHUB_ACTOR\")\n    if access_token is None or user is None:\n        raise RuntimeError(\n            \"ACCESS_TOKEN and GITHUB_ACTOR environment variables cannot be None!\"\n        )\n    async with aiohttp.ClientSession() as session:\n        s = Stats(user, access_token, session)\n        print(await s.to_str())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n", "metadata": {"source": "github.com/jstrieb/github-stats", "file_path": "github_stats.py", "language": "python", "complexity": "complex", "star_count": 3211, "quality_score": 0.84, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 546}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Ask For Help\ntitle: '[Help] '\ndescription: ‰ΩøÁî®Ê±ÇÂä©\nlabels: [ 'help wanted' ]\n\nbody:\n  - type: checkboxes\n    attributes:\n      label: Ê≥®ÊÑè‰∫ãÈ°π\n      options:\n        - label: ÊèêISSUEÂâçÔºåËØ∑Á°ÆËÆ§‰Ω†Â∑≤ÁªèÂú®GitHub IssuesÂíåDiscussionsÈáåÊêúÁ¥¢ËøáÁ±ª‰ººÁöÑÈóÆÈ¢òÔºåÂπ∂‰∏îÊ≤°ÊâæÂà∞ÊÉ≥Ë¶ÅÁöÑÁ≠îÊ°à„ÄÇ\n          required: true\n\n  - type: dropdown\n    attributes:\n      label: ‰Ω†‰ΩøÁî®ÁöÑÊòØÂì™ÁßçÊñπÂºèÔºü\n      multiple: true\n      options:\n        - GitHub Actions\n        - Python‰ª£Á†Å\n        - jmcomicÂëΩ‰ª§Ë°å\n    validations:\n      required: true\n\n  - type: textarea\n    attributes:\n      label: ‰Ω†ÁöÑÁõÆÁöÑÔºü\n      placeholder: ËØ¶ÁªÜÊèèËø∞‰∏Ä‰∏ã‰Ω†ÊÉ≥Ë¶ÅÂπ≤‰ªÄ‰πà\n\n  - type: textarea\n    attributes:\n      label: ‰Ω†ÈÅáÂà∞ÁöÑÂõ∞ÈöæÔºü\n      placeholder: ËØ¶ÁªÜÊèèËø∞‰∏Ä‰∏ã‰Ω†ÊúâÂï•Âõ∞Èöæ\n\n  - type: textarea\n    attributes:\n      label: È¢ùÂ§ñ‰ø°ÊÅØÔºà‰ª£Á†Å/optionÈÖçÁΩÆ/Êà™ÂõæÁ≠âÔºâ\n      placeholder: |\n        - Â¶ÇÊûúÊòØËøêË°å‰ª£Á†ÅÔºåËØ∑Êèê‰æõÊ†∏ÂøÉ‰ª£Á†ÅÁâáÊÆµ\n        - Â¶ÇÊûúÊúâËá™ÂÆö‰πâoptionÈÖçÁΩÆÔºåËØ∑Êèê‰æõÈÖçÁΩÆÂÜÖÂÆπ\n        - Â¶ÇÊûúÊòØGitHub ActionsÔºåËØ∑Êèê‰æõËøêË°åËÆ∞ÂΩïÁöÑURL\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": ".github/ISSUE_TEMPLATE/ask-for-help.yml", "language": "yaml", "complexity": "simple", "star_count": 3197, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 42}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Bug\ntitle: '[Bug] '\ndescription: BugÂèçÈ¶à\nlabels: [ 'bug' ]\n\nbody:\n  - type: checkboxes\n    attributes:\n      label: ËØ∑Á°ÆËÆ§Ê≥®ÊÑè‰∫ãÈ°π\n      options:\n        - label: ‰Ω†Â∑≤ÁªèÂú®GitHub IssuesÂíåDiscussionsÈáåÊêúÁ¥¢ËøáÁ±ª‰ººÁöÑÈóÆÈ¢òÔºåÂπ∂‰∏îÊ≤°ÊâæÂà∞ÊÉ≥Ë¶ÅÁöÑÁ≠îÊ°à„ÄÇ\n          required: true\n        - label: ‰Ω†‰ΩøÁî®ÁöÑÊòØÊúÄÊñ∞ÁöÑjmcomicÁâàÊú¨\n          required: true\n        - label: ÈùûGitHub ActionsÁ±ªÈóÆÈ¢òÔºàÊ≠§Á±ªÈóÆÈ¢òËØ∑‰∏ìÈó®Âà∞ÁΩÆÈ°∂ÁöÑIssueÂèçÈ¶àÔºâ\n          required: true\n\n  - type: dropdown\n    attributes:\n      label: ÂèëÁîüBugÊó∂Ôºå‰Ω†ÁöÑ‰ΩøÁî®ÊñπÂºèÔºü\n      multiple: true\n      options:\n        - Python‰ª£Á†Å\n        - jmcomicÂëΩ‰ª§Ë°å\n    validations:\n      required: true\n\n  - type: dropdown\n    attributes:\n      label: ‰Ω†ÊòØÂê¶Âú®Á¶ÅÊº´ÂÆòÊñπÁΩëÈ°µÊàñAPPÔºåÈ™åËØÅËøáÁõ∏Â∫îÁöÑÂäüËÉΩÊòØÊ≠£Â∏∏ÁöÑÔºü\n      multiple: true\n      options:\n        - ÊµãËØïËøáÔºåÁ¶ÅÊº´ÂÆòÊñπÂäüËÉΩÊ≠£Â∏∏\n        - ÊàëË¶ÅÂèçÈ¶àÁöÑbugÊó†ÈúÄÈ™åËØÅ/‰∏çÊñπ‰æøÈ™åËØÅ\n    validations:\n      required: true\n\n  - type: textarea\n    attributes:\n      label: ‰ª£Á†Å/optionÈÖçÁΩÆ\n      placeholder: |\n        - Â¶ÇÊûúÊòØËøêË°å‰ª£Á†ÅÔºåËØ∑Êèê‰æõÊ†∏ÂøÉ‰ª£Á†ÅÁâáÊÆµ\n        - Â¶ÇÊûúÊúâËá™ÂÆö‰πâoptionÈÖçÁΩÆÔºåËØ∑Êèê‰æõÈÖçÁΩÆÂÜÖÂÆπ\n        - Â¶ÇÊûúÊòØGitHub ActionsÔºåËØ∑Êèê‰æõËøêË°åËÆ∞ÂΩïÁöÑURL\n\n  - type: textarea\n    attributes:\n      label: ÊèèËø∞bugÁé∞Ë±°Ôºà‰Ω†È¢ÑÊúüÁöÑÁªìÊûúÔºå‰∏éÂÆûÈôÖÁªìÊûúÁöÑÂ∑ÆÂà´Ôºâ\n      placeholder: |\n        1. È¢ÑÊúüÁªìÊûú...\n        2. Â∑ÆÂà´...\n\n  - type: textarea\n    attributes:\n      label: ÂÖ∂‰ªñÂèØÊèê‰æõÁöÑ‰ø°ÊÅØ\n      placeholder: Êà™Âõæ/Á®ãÂ∫èËæìÂá∫Êó•ÂøóÁ≠â\n\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": ".github/ISSUE_TEMPLATE/bug.yml", "language": "yaml", "complexity": "simple", "star_count": 3197, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 58}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Close Specific PR\n\non:\n  pull_request_target:\n    types: [opened, ]\n    # Note: If you use both the branches filter and the paths filter, the workflow will only run when both filters are satisfied.\n    branches:\n      - master\n    paths:\n      - 'usage/workflow_download.py'\njobs:\n  close_specific_pr:\n    env:\n      comment: |\n          To prevent beginners from mistakenly submitting PRs, \n          if your PR only modifies the usage/workflow_download.py file, \n          it will be automatically closed. \n          If you really want to submit a PR, please reopen it yourself. \n          Make sure you know what you are doing!\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Close PR\n        run: |\n          PR_TITLE=$(jq -r \".pull_request.title\" $GITHUB_EVENT_PATH)\n          echo \"PR_TITLE: $PR_TITLE\"\n          gh pr comment ${{ github.event.pull_request.number }} --repo ${{ github.repository }} --body '${{ env.comment }}'\n          gh pr close ${{ github.event.pull_request.number }} --repo ${{ github.repository }}\n        env:\n          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": ".github/workflows/close_specific_pr.yml", "language": "yaml", "complexity": "simple", "star_count": 3197, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 32}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: ‰∏ãËΩΩJMÊú¨Â≠ê (commit)\n\non:\n  workflow_dispatch:\n  push:\n    paths:\n      - '.github/workflows/download.yml' # Â∑•‰ΩúÊµÅÂÆö‰πâ\n      - 'usage/workflow_download.py' # ‰∏ãËΩΩËÑöÊú¨\n      - 'assets/option/option_workflow_download.yml' # ÈÖçÁΩÆÊñá‰ª∂\n\njobs:\n  crawler:\n    runs-on: ubuntu-latest\n    env:\n      # ÁôªÂΩïÁõ∏ÂÖ≥secrets\n      JM_USERNAME: ${{ secrets.JM_USERNAME }}\n      JM_PASSWORD: ${{ secrets.JM_PASSWORD }}\n\n      # ÈÇÆ‰ª∂Áõ∏ÂÖ≥secrets\n      EMAIL_FROM: ${{ secrets.EMAIL_FROM }}\n      EMAIL_TO: ${{ secrets.EMAIL_TO }}\n      EMAIL_PASS: ${{ secrets.EMAIL_PASS }}\n      EMAIL_TITLE: ${{ secrets.EMAIL_TITLE }}\n      EMAIL_CONTENT: ${{ secrets.EMAIL_CONTENT }}\n      \n      # Âõ∫ÂÆöÂÄº\n      JM_DOWNLOAD_DIR: /home/runner/work/jmcomic/download/\n      ZIP_NAME: 'Êú¨Â≠ê.tar.gz'\n      UPLOAD_NAME: 'Click me to download'\n\n    steps:\n      - uses: actions/checkout@v4\n      - name: Set up Python 3.11\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n\n      - name: Install Dependency\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements-dev.txt\n\n      - name: ÂÆâË£ÖjmcomicÔºàpipÔºâ\n        if: ${{ github.ref != 'refs/heads/dev' }}\n        run: |\n          pip install jmcomic -i https://pypi.org/project --upgrade\n\n      - name: ÂÆâË£ÖjmcomicÔºàlocalÔºâ\n        if: ${{ github.ref == 'refs/heads/dev' }}\n        run: |\n          pip install -e ./\n\n      - name: ËøêË°å‰∏ãËΩΩËÑöÊú¨\n        run: |\n          cd ./usage/\n          python workflow_download.py\n\n      - name: ÂéãÁº©Êñá‰ª∂\n        run: |\n          cd $JM_DOWNLOAD_DIR\n          tar -zcvf \"../$ZIP_NAME\" ./\n          mv \"../$ZIP_NAME\" .\n\n      - name: ‰∏ä‰º†ÁªìÊûú\n        uses: actions/upload-artifact@v4\n        with:\n          name: ${{ env.UPLOAD_NAME }}\n          path: ${{ env.JM_DOWNLOAD_DIR }}/${{ env.ZIP_NAME }}\n          if-no-files-found: warn\n          retention-days: 90\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": ".github/workflows/download.yml", "language": "yaml", "complexity": "medium", "star_count": 3197, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 71}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: ‰∏ãËΩΩJMÊú¨Â≠ê (dispatch)\n\non:\n  workflow_dispatch:\n    inputs:\n      JM_ALBUM_IDS:\n        type: string\n        description: Êú¨Â≠êidÔºàÂ§ö‰∏™idÁî®-ÈöîÂºÄÔºåÂ¶Ç '123-456-789'Ôºâ\n        required: false\n\n      JM_PHOTO_IDS:\n        type: string\n        description: Á´†ËäÇidÔºàÂçïÁã¨‰∏ãËΩΩÁ´†ËäÇÔºåÂ§ö‰∏™idÂêå‰∏äÔºâ\n        required: false\n\n      CLIENT_IMPL:\n        type: string\n        description: ÂÆ¢Êà∑Á´ØÁ±ªÂûãÔºàclient.implÔºâÔºå‰∏ãËΩΩÂ§±Ë¥•Êó∂Ôºå‰Ω†ÂèØ‰ª•Â∞ùËØïÂ°´ÂÖ•Ê≠§È°πÈáçËØï„ÄÇ'api' Ë°®Á§∫ÁßªÂä®Á´ØÔºå'html' Ë°®Á§∫ÁΩëÈ°µÁ´Ø„ÄÇ\n        default: ''\n        required: false\n\n      IMAGE_SUFFIX:\n        type: string\n        description: ÂõæÁâáÂêéÁºÄÔºàdownload.cache.suffixÔºâÔºåÈªòËÆ§‰∏∫Á©∫ÔºåË°®Á§∫‰∏çÂÅöÂõæÁâáÊ†ºÂºèËΩ¨Êç¢„ÄÇÂèØÂ°´ÂÖ•‰æãÂ¶Ç 'png' 'jpg'\n        default: ''\n        required: false\n\n      DIR_RULE:\n        type: string\n        description: ‰∏ãËΩΩÊñá‰ª∂Â§πËßÑÂàôÔºàdir_rule.ruleÔºâ„ÄÇÈªòËÆ§‰ΩøÁî®ÈÖçÁΩÆÊñá‰ª∂ÁöÑ 'Bd_Aauthor_Atitle_Pindex'„ÄÇ\n        default: ''\n        required: false\n\n      ZIP_NAME:\n        type: string\n        default: Êú¨Â≠ê.tar.gz\n        description: ÂéãÁº©Êñá‰ª∂ÂêçÁß∞\n        required: false\n\n      UPLOAD_NAME:\n        type: string\n        default: Click me to download\n        description: ‰∏ä‰º†Êñá‰ª∂ÂêçÁß∞\n        required: false\n\n#      JM_USERNAME:\n#        type: string\n#        default: ''\n#        description: 'Á¶ÅÊº´Â∏êÂè∑Ôºà‰∏çÂª∫ËÆÆ‰ΩøÁî®ÔºåÁî®Êà∑ÂêçÂíåÂØÜÁ†Å‰ºöÊ≥ÑÈú≤Âú®Êó•Âøó‰∏≠„ÄÇÊúÄÂ•ΩÁî®secretsÔºâ'\n#        required: false\n#\n#      JM_PASSWORD:\n#        type: string\n#        default: ''\n#        description: 'Á¶ÅÊº´ÂØÜÁ†ÅÔºà‰∏çÂª∫ËÆÆ‰ΩøÁî®ÔºåÁî®Êà∑ÂêçÂíåÂØÜÁ†Å‰ºöÊ≥ÑÈú≤Âú®Êó•Âøó‰∏≠„ÄÇÊúÄÂ•ΩÁî®secretsÔºâ'\n#        required: false\n\n\njobs:\n  crawler:\n    runs-on: ubuntu-latest\n    env:\n      # Â∑•‰ΩúÊµÅËæìÂÖ•\n      JM_ALBUM_IDS: ${{ github.event.inputs.JM_ALBUM_IDS }}\n      JM_PHOTO_IDS: ${{ github.event.inputs.JM_PHOTO_IDS }}\n      DIR_RULE: ${{ github.event.inputs.DIR_RULE }}\n      CLIENT_IMPL: ${{ github.event.inputs.CLIENT_IMPL }}\n      ZIP_NAME: ${{ github.event.inputs.ZIP_NAME }}\n      UPLOAD_NAME: ${{ github.event.inputs.UPLOAD_NAME }}\n      IMAGE_SUFFIX: ${{ github.event.inputs.IMAGE_SUFFIX }}\n\n      # ÁôªÂΩïÁõ∏ÂÖ≥secrets\n      JM_USERNAME: ${{ secrets.JM_USERNAME }}\n      JM_PASSWORD: ${{ secrets.JM_PASSWORD }}\n\n      # ÈÇÆ‰ª∂Áõ∏ÂÖ≥secrets\n      EMAIL_FROM: ${{ secrets.EMAIL_FROM }}\n      EMAIL_TO: ${{ secrets.EMAIL_TO }}\n      EMAIL_PASS: ${{ secrets.EMAIL_PASS }}\n      EMAIL_TITLE: ${{ secrets.EMAIL_TITLE }}\n      EMAIL_CONTENT: ${{ secrets.EMAIL_CONTENT }}\n\n      # Âõ∫ÂÆöÂÄº\n      JM_DOWNLOAD_DIR: /home/runner/work/jmcomic/download/\n\n    steps:\n      - uses: actions/checkout@v4\n      - name: Set up Python 3.11\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n\n      - name: Install Dependency\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements-dev.txt\n\n      - name: ÂÆâË£ÖjmcomicÔºàpipÔºâ\n        if: ${{ github.ref != 'refs/heads/dev' }}\n        run: |\n          pip install jmcomic -i https://pypi.org/project --upgrade\n\n      - name: ÂÆâË£ÖjmcomicÔºàlocalÔºâ\n        if: ${{ github.ref == 'refs/heads/dev' }}\n        run: |\n          pip install -e ./\n\n      - name: ËøêË°å‰∏ãËΩΩËÑöÊú¨\n        run: |\n          cd ./usage/\n          python workflow_download.py\n\n      - name: ÂéãÁº©Êñá‰ª∂\n        run: |\n          cd $JM_DOWNLOAD_DIR\n          tar -zcvf \"../$ZIP_NAME\" ./\n          mv \"../$ZIP_NAME\" .\n\n      - name: ‰∏ä‰º†ÁªìÊûú\n        uses: actions/upload-artifact@v4\n        with:\n          name: ${{ env.UPLOAD_NAME }}\n          path: ${{ env.JM_DOWNLOAD_DIR }}/${{ env.ZIP_NAME }}\n          if-no-files-found: error\n          retention-days: 90\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": ".github/workflows/download_dispatch.yml", "language": "yaml", "complexity": "medium", "star_count": 3197, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 126}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: ÂØºÂá∫Êî∂ËóèÂ§πÊï∞ÊçÆ\n\non:\n#  schedule:\n#    - cron: \"0 0 * * *\"\n  workflow_dispatch:\n    inputs:\n      IN_JM_USERNAME:\n        type: string\n        default: ''\n        description: 'Á¶ÅÊº´Â∏êÂè∑Ôºà‰∏çÂª∫ËÆÆ‰ΩøÁî®Ôºå‰ºöÊ≥ÑÈú≤Âú®Êó•Âøó‰∏≠„ÄÇÊúÄÂ•ΩÁî®secretsÔºâ'\n        required: false\n\n      IN_JM_PASSWORD:\n        type: string\n        default: ''\n        description: 'Á¶ÅÊº´ÂØÜÁ†ÅÔºà‰∏çÂª∫ËÆÆ‰ΩøÁî®Ôºå‰ºöÊ≥ÑÈú≤Âú®Êó•Âøó‰∏≠„ÄÇÊúÄÂ•ΩÁî®secretsÔºâ'\n        required: false\n\n      IN_ZIP_PASSWORD:\n        type: string\n        default: ''\n        description: 'ÂéãÁº©Êñá‰ª∂ÂØÜÁ†ÅÔºà‰∏çÂª∫ËÆÆ‰ΩøÁî®Ôºå‰ºöÊ≥ÑÈú≤Âú®Êó•Âøó‰∏≠„ÄÇÊúÄÂ•ΩÁî®secretsÔºâ'\n        required: false\n\njobs:\n  crawler:\n    runs-on: ubuntu-latest\n    env:\n      # Â∑•‰ΩúÊµÅËæìÂÖ•\n      IN_JM_USERNAME: ${{ github.event.inputs.IN_JM_USERNAME }}\n      IN_JM_PASSWORD: ${{ github.event.inputs.IN_JM_PASSWORD }}\n      IN_ZIP_PASSWORD: ${{ github.event.inputs.IN_ZIP_PASSWORD }}\n\n      # ÁôªÂΩïÁõ∏ÂÖ≥secrets\n      JM_USERNAME: ${{ secrets.JM_USERNAME }}\n      JM_PASSWORD: ${{ secrets.JM_PASSWORD }}\n      ZIP_PASSWORD: ${{ secrets.ZIP_PASSWORD }}\n\n      # ÈÇÆ‰ª∂Áõ∏ÂÖ≥secrets\n      EMAIL_FROM: ${{ secrets.EMAIL_FROM }}\n      EMAIL_TO: ${{ secrets.EMAIL_TO }}\n      EMAIL_PASS: ${{ secrets.EMAIL_PASS }}\n      EMAIL_TITLE: ${{ secrets.EMAIL_TITLE }}\n      EMAIL_CONTENT: ${{ secrets.EMAIL_CONTENT }}\n\n      # Âõ∫ÂÆöÂÄº\n      ZIP_FP: /home/runner/work/jmcomic/download/export.7z\n\n    steps:\n      - uses: actions/checkout@v4\n      - name: Set up Python 3.11\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n\n      - name: Install Dependency\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements-dev.txt\n          sudo apt update\n          sudo apt install p7zip-full\n\n      - name: ÂÆâË£ÖjmcomicÔºàlocalÔºâ\n        run: |\n          pip install -e ./\n\n      - name: ÊâßË°å‰ª£Á†Å\n        run: |\n          cd ./usage/\n          python workflow_export_favorites.py\n\n      - name: ‰∏ä‰º†ÁªìÊûú\n        uses: actions/upload-artifact@v4\n        with:\n          name: 'ÂØºÂá∫ÁöÑÊî∂ËóèÂ§π'\n          path: ${{ env.ZIP_FP }}\n          if-no-files-found: error\n          retention-days: 90\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": ".github/workflows/export_favorites.yml", "language": "yaml", "complexity": "medium", "star_count": 3197, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 80}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# publish when new release\nname: Publish If Release\n\non:\n  workflow_dispatch:\n  release:\n    types: [ published ]\n\njobs:\n  release:\n    name: Publish `jmcomic` to PYPI\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python 3.11\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n\n      - name: Build\n        run: |\n          python -m pip install build\n          python -m build\n\n      - name: Publish PYPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          password: ${{ secrets.PYPI_JMCOMIC }}\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": ".github/workflows/release.yml", "language": "yaml", "complexity": "medium", "star_count": 3197, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# auto release and publish\nname: Auto Release & Publish\n\non:\n  workflow_dispatch:\n  push:\n    branches:\n      - master\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: write\n    if: startsWith(github.event.head_commit.message, 'v')\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python 3.11\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n\n      - name: Parse Tag & Body\n        id: tb\n        run: |\n          commit_message=$(git log --format=%B -n 1 ${{ github.sha }})\n          python .github/release.py \"$commit_message\"\n\n      - name: Create Release\n        uses: softprops/action-gh-release@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          tag_name: ${{ steps.tb.outputs.tag }}\n          body_path: release_body.txt\n\n      - name: Build\n        run: |\n          python -m pip install build\n          python -m build\n\n      - name: Release PYPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n#        with:\n#          password: ${{ secrets.PYPI_JMCOMIC }}\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": ".github/workflows/release_auto.yml", "language": "yaml", "complexity": "medium", "star_count": 3197, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 48}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Run Test (API)\n\non:\n  schedule:\n    - cron: \"0 0 * * *\"\n  workflow_dispatch:\n  push:\n    branches: [ \"dev\" ]\n    paths:\n      - 'src/**/*.py'\n      - 'tests/**/*.py'\n      - '.github/workflows/test_api.yml'\n      - 'assets/option/option_test_api.yml'\n\njobs:\n  test: # This code is based on https://github.com/gaogaotiantian/viztracer/blob/master/.github/workflows/python-package.yml\n    strategy:\n      matrix:\n        python-version: ['3.9', '3.10', '3.11', '3.13']\n        os: [ ubuntu-latest ]\n    runs-on: ${{ matrix.os }}\n    timeout-minutes: 5\n    env:\n      # ÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ\n      JM_OPTION_PATH_TEST: ./assets/option/option_test_api.yml\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\n\n      - name: Install local\n        run: |\n          pip install -e ./\n\n      - name: Run Test\n        run: |\n          cd ./tests/\n          python -m unittest\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": ".github/workflows/test_api.yml", "language": "yaml", "complexity": "medium", "star_count": 3197, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 49}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Run Test (HTML)\n\non:\n  schedule:\n    - cron: \"0 0 * * *\"\n  workflow_dispatch:\n  push:\n    branches: [ \"dev\" ]\n    paths:\n      - 'src/**/*.py'\n      - 'tests/**/*.py'\n      - '.github/workflows/test_html.yml'\n      - 'assets/option/option_test_html.yml'\n\njobs:\n  test: # This code is based on https://github.com/gaogaotiantian/viztracer/blob/master/.github/workflows/python-package.yml\n    strategy:\n      matrix:\n        python-version: ['3.9', '3.10', '3.11', '3.13']\n        os: [ ubuntu-latest ]\n    runs-on: ${{ matrix.os }}\n    timeout-minutes: 5\n    env:\n      # ÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ\n      JM_OPTION_PATH_TEST: ./assets/option/option_test_html.yml\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\n\n      - name: Install local\n        run: |\n          pip install -e ./\n\n      - name: Run Test\n        run: |\n          cd ./tests/\n          python -m unittest\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": ".github/workflows/test_html.yml", "language": "yaml", "complexity": "medium", "star_count": 3197, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 49}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Required\nversion: 2\n\n# Set the OS, Python version and other tools you might need\nbuild:\n os: ubuntu-22.04\n tools:\n   python: \"3.12\"\nmkdocs:\n  configuration: assets/docs/mkdocs.yml\n\n# Optionally declare the Python requirements required to build your docs\npython:\n   install:\n   - requirements: assets/docs/requirements.txt\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": ".readthedocs.yaml", "language": "yaml", "complexity": "simple", "star_count": 3197, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 16}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Python API For JMComic (Á¶ÅÊº´Â§©Â†Ç)\n\nÊú¨È°πÁõÆÂ∞ÅË£Ö‰∫Ü‰∏ÄÂ•óÂèØÁî®‰∫éÁà¨ÂèñJMÁöÑPython API.\n\n‰Ω†ÂèØ‰ª•ÈÄöËøáÁÆÄÂçïÁöÑÂá†Ë°åPython‰ª£Á†ÅÔºåÂÆûÁé∞‰∏ãËΩΩJM‰∏äÁöÑÊú¨Â≠êÂà∞Êú¨Âú∞ÔºåÂπ∂‰∏îÊòØÂ§ÑÁêÜÂ•ΩÁöÑÂõæÁâá„ÄÇ\n\n[„ÄêÊåáË∑Ø„ÄëÊïôÁ®ãÔºö‰ΩøÁî®GitHub Actions‰∏ãËΩΩÁ¶ÅÊº´Êú¨Â≠ê](./assets/docs/sources/tutorial/1_github_actions.md)\n\n[„ÄêÊåáË∑Ø„ÄëÊïôÁ®ãÔºöÂØºÂá∫Âπ∂‰∏ãËΩΩ‰Ω†ÁöÑÁ¶ÅÊº´Êî∂ËóèÂ§πÊï∞ÊçÆ](./assets/docs/sources/tutorial/10_export_favorites.md)\n\n**ÂèãÊÉÖÊèêÁ§∫ÔºöÁèçÁà±JMÔºå‰∏∫‰∫ÜÂáèËΩªJMÁöÑÊúçÂä°Âô®ÂéãÂäõÔºåËØ∑‰∏çË¶Å‰∏ÄÊ¨°ÊÄßÁà¨ÂèñÂ§™Â§öÊú¨Â≠êÔºåË•øÈó®üôèüôèüôè**.\n\n## È°πÁõÆ‰ªãÁªç\n\nÊú¨È°πÁõÆÁöÑÊ†∏ÂøÉÂäüËÉΩÊòØ‰∏ãËΩΩÊú¨Â≠ê„ÄÇ\n\nÂü∫‰∫éÊ≠§ÔºåËÆæËÆ°‰∫Ü‰∏ÄÂ•óÊñπ‰æø‰ΩøÁî®„ÄÅ‰æø‰∫éÊâ©Â±ïÔºåËÉΩÊª°Ë∂≥‰∏Ä‰∫õÁâπÊÆä‰∏ãËΩΩÈúÄÊ±ÇÁöÑÊ°ÜÊû∂„ÄÇ\n\nÁõÆÂâçÊ†∏ÂøÉÂäüËÉΩÂÆûÁé∞ËæÉ‰∏∫Á®≥ÂÆöÔºåÈ°πÁõÆ‰πüÂ§Ñ‰∫éÁª¥Êä§Èò∂ÊÆµ„ÄÇ\n\nÈô§‰∫Ü‰∏ãËΩΩÂäüËÉΩ‰ª•Â§ñÔºå‰πüÂÆûÁé∞‰∫ÜÂÖ∂‰ªñÁöÑ‰∏Ä‰∫õÁ¶ÅÊº´Êé•Âè£ÔºåÊåâÈúÄÂÆûÁé∞„ÄÇÁõÆÂâçÂ∑≤ÊúâÂäüËÉΩÔºö\n\n- ÁôªÂΩï\n- ÊêúÁ¥¢Êú¨Â≠êÔºàÊîØÊåÅÊâÄÊúâÊêúÁ¥¢È°πÔºâ\n- ÂõæÁâá‰∏ãËΩΩËß£Á†Å\n- ÂàÜÁ±ª/ÊéíË°åÊ¶ú\n- Êú¨Â≠ê/Á´†ËäÇËØ¶ÊÉÖ\n- ‰∏™‰∫∫Êî∂ËóèÂ§π\n- Êé•Âè£Âä†Ëß£ÂØÜÔºàAPPÁöÑÊé•Âè£Ôºâ\n\n## ÂÆâË£ÖÊïôÁ®ã\n\n> ‚ö†Â¶ÇÊûú‰Ω†Ê≤°ÊúâÂÆâË£ÖËøáPythonÔºåÈúÄË¶ÅÂÖàÂÆâË£ÖPythonÂÜçÊâßË°å‰∏ãÈù¢ÁöÑÊ≠•È™§Ôºå‰∏îÁâàÊú¨ÈúÄË¶Å>=3.7Ôºà[ÁÇπÊàëÂéªpythonÂÆòÁΩë‰∏ãËΩΩ](https://www.python.org/downloads/)Ôºâ\n\n* ÈÄöËøápipÂÆòÊñπÊ∫êÂÆâË£ÖÔºàÊé®ËçêÔºåÂπ∂‰∏îÊõ¥Êñ∞‰πüÊòØËøô‰∏™ÂëΩ‰ª§Ôºâ\n\n  ```shell\n  pip install jmcomic -i https://pypi.org/project -U\n  ```\n* ÈÄöËøáÊ∫ê‰ª£Á†ÅÂÆâË£Ö\n\n  ```shell\n  pip install git+https://github.com/hect0x7/JMComic-Crawler-Python\n  ```\n\n## Âø´ÈÄü‰∏äÊâã\n\n### 1. ‰∏ãËΩΩÊú¨Â≠êÊñπÊ≥ï\n\nÂè™ÈúÄË¶Å‰ΩøÁî®Â¶Ç‰∏ã‰ª£Á†ÅÔºåÂ∞±ÂèØ‰ª•‰∏ãËΩΩÊú¨Â≠ê`JM422866`ÁöÑÊâÄÊúâÁ´†ËäÇÁöÑÂõæÁâáÔºö\n\n```python\nimport jmcomic  # ÂØºÂÖ•Ê≠§Ê®°ÂùóÔºåÈúÄË¶ÅÂÖàÂÆâË£Ö.\njmcomic.download_album('422866')  # ‰º†ÂÖ•Ë¶Å‰∏ãËΩΩÁöÑalbumÁöÑidÔºåÂç≥ÂèØ‰∏ãËΩΩÊï¥‰∏™albumÂà∞Êú¨Âú∞.\n```\n\n‰∏äÈù¢ÁöÑ `download_album`ÊñπÊ≥ïËøòÊúâ‰∏Ä‰∏™ÂèÇÊï∞`option`ÔºåÂèØÁî®‰∫éÊéßÂà∂‰∏ãËΩΩÈÖçÁΩÆÔºåÈÖçÁΩÆÂåÖÊã¨Á¶ÅÊº´ÂüüÂêç„ÄÅÁΩëÁªú‰ª£ÁêÜ„ÄÅÂõæÁâáÊ†ºÂºèËΩ¨Êç¢„ÄÅÊèí‰ª∂Á≠âÁ≠â„ÄÇ\n\n‰Ω†ÂèØËÉΩÈúÄË¶ÅËøô‰∫õÈÖçÁΩÆÈ°π„ÄÇÊé®Ëçê‰ΩøÁî®ÈÖçÁΩÆÊñá‰ª∂ÂàõÂª∫optionÔºåÁî®option‰∏ãËΩΩÊú¨Â≠êÔºåËßÅ‰∏ãÁ´†Ôºö\n\n### 2. ‰ΩøÁî®optionÈÖçÁΩÆÊù•‰∏ãËΩΩÊú¨Â≠ê\n\n1. È¶ñÂÖàÔºåÂàõÂª∫‰∏Ä‰∏™ÈÖçÁΩÆÊñá‰ª∂ÔºåÂÅáËÆæÊñá‰ª∂Âêç‰∏∫ `option.yml`\n\n   ËØ•Êñá‰ª∂ÊúâÁâπÂÆöÁöÑÂÜôÊ≥ïÔºå‰Ω†ÈúÄË¶ÅÂèÇËÄÉËøô‰∏™ÊñáÊ°£ ‚Üí [ÈÖçÁΩÆÊñá‰ª∂ÊåáÂçó](./assets/docs/sources/option_file_syntax.md)\n\n   ‰∏ãÈù¢ÂÅö‰∏Ä‰∏™ÊºîÁ§∫ÔºåÂÅáËÆæ‰Ω†ÈúÄË¶ÅÊää‰∏ãËΩΩÁöÑÂõæÁâáËΩ¨‰∏∫pngÊ†ºÂºèÔºå‰Ω†Â∫îËØ•Êää‰ª•‰∏ãÂÜÖÂÆπÂÜôËøõ`option.yml`\n\n```yml\ndownload:\n  image:\n    suffix: .png # ËØ•ÈÖçÁΩÆÁî®‰∫éÊää‰∏ãËΩΩÁöÑÂõæÁâáËΩ¨‰∏∫pngÊ†ºÂºè\n```\n\n2. Á¨¨‰∫åÊ≠•ÔºåËøêË°å‰∏ãÈù¢ÁöÑpython‰ª£Á†Å\n\n```python\nimport jmcomic\n\n# ÂàõÂª∫ÈÖçÁΩÆÂØπË±°\noption = jmcomic.create_option_by_file('‰Ω†ÁöÑÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑÔºå‰æãÂ¶Ç D:/option.yml')\n# ‰ΩøÁî®optionÂØπË±°Êù•‰∏ãËΩΩÊú¨Â≠ê\njmcomic.download_album(422866, option)\n# Á≠â‰ª∑ÂÜôÊ≥ï: option.download_album(422866)\n```\n\n### 3. ‰ΩøÁî®ÂëΩ‰ª§Ë°å\n> Â¶ÇÊûúÂè™ÊÉ≥‰∏ãËΩΩÊú¨Â≠êÔºå‰ΩøÁî®ÂëΩ‰ª§Ë°å‰ºöÊØî‰∏äËø∞ÊñπÂºèÊõ¥Âä†ÁÆÄÂçïÁõ¥Êé•\n> \n> ‰æãÂ¶ÇÔºåÂú®windows‰∏äÔºåÁõ¥Êé•Êåâ‰∏ãwin+rÈîÆÔºåËæìÂÖ•jmcomic xxxÂ∞±ÂèØ‰ª•‰∏ãËΩΩÊú¨Â≠ê„ÄÇ\n\nÁ§∫‰æãÔºö\n\n‰∏ãËΩΩÊú¨Â≠ê422866ÁöÑÂëΩ‰ª§\n\n```sh\njmcomic 422866\n```\nÂêåÊó∂‰∏ãËΩΩÊú¨Â≠ê123, Á´†ËäÇ456ÁöÑÂëΩ‰ª§\n```sh\njmcomic 123 p456\n```\n\nÂëΩ‰ª§Ë°åÊ®°Âºè‰πüÊîØÊåÅËá™ÂÆö‰πâoptionÔºå‰Ω†ÂèØ‰ª•‰ΩøÁî®ÁéØÂ¢ÉÂèòÈáèÊàñËÄÖÂëΩ‰ª§Ë°åÂèÇÊï∞Ôºö\n\na. ÈÄöËøáÂëΩ‰ª§Ë°å--optionÂèÇÊï∞ÊåáÂÆöoptionÊñá‰ª∂Ë∑ØÂæÑ\n\n```sh\njmcomic 123 --option=\"D:/a.yml\"\n```\n\nb. ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáè `JM_OPTION_PATH` ‰∏∫optionÊñá‰ª∂Ë∑ØÂæÑÔºàÊé®ËçêÔºâ\n\n> ËØ∑Ëá™Ë°ågoogleÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÁöÑÊñπÂºèÔºåÊàñ‰ΩøÁî®powershellÂëΩ‰ª§:  `setx JM_OPTION_PATH \"D:/a.yml\"` ÈáçÂêØÂêéÁîüÊïà\n\n```sh\njmcomic 123\n```\n\n\n\n## ËøõÈò∂‰ΩøÁî®\n\nËØ∑Êü•ÈòÖÊñáÊ°£È¶ñÈ°µ‚Üí[jmcomic.readthedocs.io](https://jmcomic.readthedocs.io/zh-cn/latest)\n\nÔºàÊèêÁ§∫ÔºöjmcomicÊèê‰æõ‰∫ÜÂæàÂ§ö‰∏ãËΩΩÈÖçÁΩÆÈ°πÔºåÂ§ßÈÉ®ÂàÜÁöÑ‰∏ãËΩΩÈúÄÊ±Ç‰Ω†ÈÉΩÂèØ‰ª•Â∞ùËØïÂØªÊâæÁõ∏ÂÖ≥ÈÖçÁΩÆÈ°πÊàñÊèí‰ª∂Êù•ÂÆûÁé∞„ÄÇÔºâ\n\n## È°πÁõÆÁâπÁÇπ\n\n- **ÁªïËøáCloudflareÁöÑÂèçÁà¨Ëô´**\n- **ÂÆûÁé∞Á¶ÅÊº´APPÊé•Âè£ÊúÄÊñ∞ÁöÑÂä†Ëß£ÂØÜÁÆóÊ≥ï (1.6.3)**\n- Áî®Ê≥ïÂ§öÊ†∑Ôºö\n\n  - GitHub\n    ActionsÔºöÁΩëÈ°µ‰∏äÁõ¥Êé•ËæìÂÖ•Êú¨Â≠êidÂ∞±ËÉΩ‰∏ãËΩΩÔºà[ÊïôÁ®ãÔºö‰ΩøÁî®GitHub Actions‰∏ãËΩΩÁ¶ÅÊº´Êú¨Â≠ê](./assets/docs/sources/tutorial/1_github_actions.md)Ôºâ\n  - ÂëΩ‰ª§Ë°åÔºöÊó†ÈúÄÂÜôPython‰ª£Á†ÅÔºåÁÆÄÂçïÊòìÁî®Ôºà[ÊïôÁ®ãÔºö‰ΩøÁî®ÂëΩ‰ª§Ë°å‰∏ãËΩΩÁ¶ÅÊº´Êú¨Â≠ê](./assets/docs/sources/tutorial/2_command_line.md)Ôºâ\n  - Python‰ª£Á†ÅÔºöÊúÄÊú¨Ë¥®„ÄÅÊúÄÂº∫Â§ßÁöÑ‰ΩøÁî®ÊñπÂºèÔºåÈúÄË¶Å‰Ω†Êúâ‰∏ÄÂÆöÁöÑpythonÁºñÁ®ãÂü∫Á°Ä\n- ÊîØÊåÅ**ÁΩëÈ°µÁ´Ø**Âíå**ÁßªÂä®Á´Ø**‰∏§ÁßçÂÆ¢Êà∑Á´ØÂÆûÁé∞ÔºåÂèØÈÄöËøáÈÖçÁΩÆÂàáÊç¢Ôºà**ÁßªÂä®Á´Ø‰∏çÈôêipÂÖºÂÆπÊÄßÂ•ΩÔºåÁΩëÈ°µÁ´ØÈôêÂà∂ipÂú∞Âå∫‰ΩÜÊïàÁéáÈ´ò**Ôºâ\n- ÊîØÊåÅ**Ëá™Âä®ÈáçËØïÂíåÂüüÂêçÂàáÊç¢**Êú∫Âà∂\n- **Â§öÁ∫øÁ®ã‰∏ãËΩΩ**ÔºàÂèØÁªÜÂåñÂà∞‰∏ÄÂõæ‰∏ÄÁ∫øÁ®ãÔºåÊïàÁéáÊûÅÈ´òÔºâ\n- **ÂèØÈÖçÁΩÆÊÄßÂº∫**\n\n  - ‰∏çÈÖçÁΩÆ‰πüËÉΩ‰ΩøÁî®ÔºåÂçÅÂàÜÊñπ‰æø\n  - ÈÖçÁΩÆÂèØ‰ª•‰ªéÈÖçÁΩÆÊñá‰ª∂ÁîüÊàêÔºåÊîØÊåÅÂ§öÁßçÊñá‰ª∂Ê†ºÂºè\n  - ÈÖçÁΩÆÁÇπÊúâÔºö`ËØ∑Ê±ÇÂüüÂêç` `ÂÆ¢Êà∑Á´ØÂÆûÁé∞` `ÊòØÂê¶‰ΩøÁî®Á£ÅÁõòÁºìÂ≠ò` `ÂêåÊó∂‰∏ãËΩΩÁöÑÁ´†ËäÇ/ÂõæÁâáÊï∞Èáè` `ÂõæÁâáÊ†ºÂºèËΩ¨Êç¢` `‰∏ãËΩΩË∑ØÂæÑËßÑÂàô` `ËØ∑Ê±ÇÂÖÉ‰ø°ÊÅØÔºàheaders,cookies,proxiesÔºâ`\n    Á≠â\n- **ÂèØÊâ©Â±ïÊÄßÂº∫**\n\n  - ÊîØÊåÅËá™ÂÆö‰πâÊú¨Â≠ê/Á´†ËäÇ/ÂõæÁâá‰∏ãËΩΩÂâçÂêéÁöÑÂõûË∞ÉÂáΩÊï∞\n  - ÊîØÊåÅËá™ÂÆö‰πâÁ±ªÔºö`DownloaderÔºàË¥üË¥£Ë∞ÉÂ∫¶Ôºâ` `OptionÔºàË¥üË¥£ÈÖçÁΩÆÔºâ` `ClientÔºàË¥üË¥£ËØ∑Ê±ÇÔºâ` `ÂÆû‰ΩìÁ±ª`Á≠â\n  - ÊîØÊåÅËá™ÂÆö‰πâÊó•Âøó„ÄÅÂºÇÂ∏∏ÁõëÂê¨Âô®\n  - **ÊîØÊåÅPluginÊèí‰ª∂ÔºåÂèØ‰ª•Êñπ‰æøÂú∞Êâ©Â±ïÂäüËÉΩÔºå‰ª•Âèä‰ΩøÁî®Âà´‰∫∫ÁöÑÊèí‰ª∂ÔºåÁõÆÂâçÂÜÖÁΩÆÊèí‰ª∂Êúâ**Ôºö\n    - `ÁôªÂΩïÊèí‰ª∂`\n    - `Á°¨‰ª∂Âç†Áî®ÁõëÊéßÊèí‰ª∂`\n    - `Âè™‰∏ãËΩΩÊñ∞Á´†Êèí‰ª∂`\n    - `ÂéãÁº©Êñá‰ª∂Êèí‰ª∂`\n    - `‰∏ãËΩΩÁâπÂÆöÂêéÁºÄÂõæÁâáÊèí‰ª∂`\n    - `ÂèëÈÄÅQQÈÇÆ‰ª∂Êèí‰ª∂`\n    - `Ëá™Âä®‰ΩøÁî®ÊµèËßàÂô®cookiesÊèí‰ª∂`\n    - `ÂØºÂá∫Êî∂ËóèÂ§π‰∏∫csvÊñá‰ª∂Êèí‰ª∂`\n    - `ÂêàÂπ∂ÊâÄÊúâÂõæÁâá‰∏∫pdfÊñá‰ª∂Êèí‰ª∂`\n    - `ÂêàÂπ∂ÊâÄÊúâÂõæÁâá‰∏∫ÈïøÂõæpngÊèí‰ª∂`\n    - `ÈáçÂ§çÊñá‰ª∂Ê£ÄÊµãÂà†Èô§Êèí‰ª∂`\n    - `ÁΩëÈ°µËßÇÁúãÊú¨Âú∞Á´†ËäÇÊèí‰ª∂`\n\n## ‰ΩøÁî®Â∞èËØ¥Êòé\n\n* Python >= 3.7ÔºåÂª∫ËÆÆ3.9‰ª•‰∏äÔºåÂõ†‰∏∫jmcomicÁöÑ‰æùËµñÂ∫ìÂèØËÉΩ‰ºö‰∏çÊîØÊåÅ3.9‰ª•‰∏ãÁöÑÁâàÊú¨„ÄÇ\n* ‰∏™‰∫∫È°πÁõÆÔºåÊñáÊ°£ÂíåÁ§∫‰æã‰ºöÊúâ‰∏çÂèäÊó∂‰πãÂ§ÑÔºåÂèØ‰ª•IssueÊèêÈóÆ\n\n## È°πÁõÆÊñá‰ª∂Â§π‰ªãÁªç\n\n* .githubÔºöGitHub ActionsÈÖçÁΩÆÊñá‰ª∂\n* assetsÔºöÂ≠òÊîæ‰∏Ä‰∫õÈùû‰ª£Á†ÅÁöÑËµÑÊ∫êÊñá‰ª∂\n\n  * docsÔºöÈ°πÁõÆÊñáÊ°£\n  * optionÔºöÂ≠òÊîæÈÖçÁΩÆÊñá‰ª∂\n* srcÔºöÂ≠òÊîæÊ∫ê‰ª£Á†Å\n\n  * jmcomicÔºö`jmcomic`Ê®°Âùó\n* testsÔºöÊµãËØïÁõÆÂΩïÔºåÂ≠òÊîæÊµãËØï‰ª£Á†ÅÔºå‰ΩøÁî®unittest\n* usageÔºöÁî®Ê≥ïÁõÆÂΩïÔºåÂ≠òÊîæÁ§∫‰æã/‰ΩøÁî®‰ª£Á†Å\n\n## ÊÑüË∞¢‰ª•‰∏ãÈ°πÁõÆ\n\n### ÂõæÁâáÂàÜÂâ≤ÁÆóÊ≥ï‰ª£Á†Å+Á¶ÅÊº´ÁßªÂä®Á´ØAPI\n\n<a href=\"https://github.com/tonquer/JMComic-qt\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github-readme-stats.vercel.app/api/pin/?username=tonquer&repo=JMComic-qt&theme=radical\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://github-readme-stats.vercel.app/api/pin/?username=tonquer&repo=JMComic-qt\" />\n    <img alt=\"Repo Card\" src=\"https://github-readme-stats.vercel.app/api/pin/?username=tonquer&repo=JMComic-qt\" />\n  </picture>\n</a>\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 3197, "quality_score": 0.63, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 195}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "È¶ñÈ°µ\n=====================================\n\n\n\n> jmcomicÂ∫ìÂ∞ÅË£Ö‰∫Ü‰∏ÄÂ•óÂèØÁî®‰∫éÁà¨ÂèñÁ¶ÅÊº´ÁöÑPython API.\n>\n> ‰Ω†ÂèØ‰ª•ÈÄöËøáÁÆÄÂçïÁöÑÂá†Ë°åPython‰ª£Á†ÅÔºåËÆøÈóÆÁ¶ÅÊº´ÁöÑÊé•Âè£Ôºå‰ª•Âèä‰∏ãËΩΩÁ¶ÅÊº´ÁöÑÊú¨Â≠ê„ÄÇ\n> \n> [Êü•ÁúãÈ°πÁõÆÊõ¥Êñ∞ËÆ°Âàí](TODO.md)\n\n\n\n## ÂÖ•Èó®\n\n- [Âø´ÈÄü‰∏äÊâã(GitHub README)](https://github.com/hect0x7/JMComic-Crawler-Python/tree/master?tab=readme-ov-file#%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B)\n- [Â∏∏Áî®Á±ªÂíåÊñπÊ≥ïÊºîÁ§∫](tutorial/0_common_usage.md)\n- [optionÈÖçÁΩÆ‰ª•ÂèäÊèí‰ª∂ÂÜôÊ≥ï](./option_file_syntax.md)\n\n\n\n## ÁâπÊÆäÁî®Ê≥ïÊïôÁ®ã\n- [‰ΩøÁî®GitHub Actions‰∏ãËΩΩÊú¨Â≠ê](./tutorial/1_github_actions.md)\n- [‰ΩøÁî®GitHub ActionsÂØºÂá∫Êî∂ËóèÂ§π](tutorial/10_export_favorites.md)\n- [ÂëΩ‰ª§Ë°åÁî®Ê≥ïÊïôÁ®ã](tutorial/2_command_line.md)\n- [ÊµãËØï‰Ω†ÁöÑipËÉΩËÆøÈóÆÂì™‰∫õÁ¶ÅÊº´ÂüüÂêç](tutorial/8_pick_domain.md)\n\n\n\n## Ê†∏ÂøÉÊú∫Âà∂\n- [‰∏ãËΩΩËøáÊª§Âô®Êú∫Âà∂](tutorial/5_filter.md)\n- [Êèí‰ª∂Êú∫Âà∂](tutorial/6_plugin.md)\n\n\n\n## Ëá™ÂÆö‰πâ\n- [‰∏ãËΩΩÊñá‰ª∂Â§πÂêç](tutorial/9_custom_download_dir_name.md)\n- [Êó•Âøó](tutorial/11_log_custom.md)\n- [Ê®°Âùó](tutorial/4_module_custom.md)\n\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": "assets/docs/sources/index.md", "language": "markdown", "complexity": "simple", "star_count": 3197, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 41}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# ÈÖçÁΩÆÊñá‰ª∂ÊåáÂçó\n\n## 1. ÈÖçÁΩÆÂâçÈúÄÁü•\n\n* optionÊúâ`ÈªòËÆ§ÂÄº`Ôºå‰Ω†ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑÈÖçÁΩÆÈ°π‰ºöË¶ÜÁõñ`ÈªòËÆ§ÂÄº`„ÄÇÂõ†Ê≠§‰Ω†Âè™ÈúÄË¶ÅÊ∑ªÂä†ÊÑüÂÖ¥Ë∂£ÁöÑÈÖçÁΩÆÈ°πÂç≥ÂèØ„ÄÇ\n\n* ‰Ω†‰πüÂèØ‰ª•‰ΩøÁî®‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÊù•ÂæóÂà∞optionÁöÑÈªòËÆ§ÂÄº„ÄÇ‰Ω†ÂèØ‰ª•Âà†Èô§ÂÖ∂‰∏≠ÁöÑÂ§ßÈÉ®ÂàÜÈÖçÁΩÆÈ°πÔºåÂè™‰øùÁïô‰Ω†Ë¶ÅË¶ÜÁõñÁöÑÈÖçÁΩÆÈ°π„ÄÇ\n\n```python\nfrom jmcomic import JmOption\nJmOption.default().to_file('./option.yml') # ÂàõÂª∫ÈªòËÆ§optionÔºåÂØºÂá∫‰∏∫option.ymlÊñá‰ª∂\n```\n\n## 2. optionÂ∏∏ËßÑÈÖçÁΩÆÈ°π\n\n```yaml\n# ÂºÄÂêØjmcomicÁöÑÊó•ÂøóËæìÂá∫ÔºåÈªòËÆ§‰∏∫true\n# ÂØπÊó•ÂøóÊúâÈúÄÊ±ÇÁöÑÂèØËøõ‰∏ÄÊ≠•ÂèÇËÄÉÊñáÊ°£ ‚Üí https://jmcomic.readthedocs.io/en/latest/tutorial/11_log_custom/\nlog: true\n\n# ÈÖçÁΩÆÂÆ¢Êà∑Á´ØÁõ∏ÂÖ≥\nclient:\n  # impl: ÂÆ¢Êà∑Á´ØÂÆûÁé∞Á±ªÔºå‰∏çÈÖçÁΩÆÈªòËÆ§‰ºö‰ΩøÁî®JmModuleConfig.DEFAULT_CLIENT_IMPL\n  # ÂèØÈÖçÁΩÆ:\n  #  html - Ë°®Á§∫ÁΩëÈ°µÁ´Ø\n  #  api - Ë°®Á§∫APPÁ´Ø\n  # APPÁ´Ø‰∏çÈôêipÂÖºÂÆπÊÄßÂ•ΩÔºåÁΩëÈ°µÁ´ØÈôêÂà∂ipÂú∞Âå∫‰ΩÜÊïàÁéáÈ´ò\n  impl: html\n\n  # domain: Á¶ÅÊº´ÂüüÂêçÈÖçÁΩÆÔºå‰∏ÄËà¨Êó†ÈúÄÈÖçÁΩÆÔºåjmcomic‰ºöÊ†πÊçÆ‰∏äÈù¢ÁöÑimplËá™Âä®ËÆæÁΩÆÁõ∏Â∫îÂüüÂêç\n  # ËØ•ÈÖçÁΩÆÈ°πÈúÄË¶ÅÂíå‰∏äÈù¢ÁöÑimplÁªìÂêà‰ΩøÁî®ÔºåÂõ†‰∏∫Á¶ÅÊº´ÁΩëÈ°µÁ´ØÂíåAPPÁ´Ø‰ΩøÁî®ÁöÑÊòØ‰∏çÂêåÂüüÂêçÔºå\n  # ÊâÄ‰ª•ÈÖçÁΩÆÊòØ‰∏Ä‰∏™dictÁªìÊûÑÔºåkeyÊòØimplÁöÑÂÄºÔºåvalueÊòØÂüüÂêçÂàóË°®ÔºåË°®Á§∫Ëøô‰∏™implËµ∞Ëøô‰∫õÂüüÂêç„ÄÇ\n  # ÂüüÂêçÂàóË°®ÁöÑ‰ΩøÁî®È°∫Â∫èÊòØÔºöÂÖàÁî®Á¨¨‰∏Ä‰∏™ÂüüÂêçÔºåÂ¶ÇÊûúÁ¨¨‰∏Ä‰∏™ÂüüÂêçÈáçËØïnÊ¨°Â§±Ë¥•ÔºåÂàôÊç¢‰∏ã‰∏Ä‰∏™ÂüüÂêçÈáçËØïÔºå‰ª•Ê≠§Á±ªÊé®„ÄÇ\n  # ‰∏ãÈù¢ÊòØÁ§∫‰æãÔºöÔºàÊ≥®ÊÑè‰∏ãÈù¢Ëøô‰∫õÂüüÂêçÂèØËÉΩ‰ºöËøáÊó∂Ôºå‰∏ç‰∏ÄÂÆöËÉΩÁî®Ôºâ\n  domain:\n    html:\n      - 18comic.vip\n      - 18comic.org\n    api:\n      - www.jmapiproxyxxx.vip\n\n  # retry_times: ËØ∑Ê±ÇÂ§±Ë¥•ÈáçËØïÊ¨°Êï∞ÔºåÈªòËÆ§‰∏∫5\n  retry_times: 5\n\n  # postman: ËØ∑Ê±ÇÈÖçÁΩÆ\n  postman:\n    meta_data:\n      # proxies: ‰ª£ÁêÜÈÖçÁΩÆÔºåÈªòËÆ§ÊòØ systemÔºåË°®Á§∫‰ΩøÁî®Á≥ªÁªü‰ª£ÁêÜ„ÄÇ\n      # ‰ª•‰∏ãÁöÑÂÜôÊ≥ïÈÉΩÂèØ‰ª•:\n      # proxies: null # ‰∏ç‰ΩøÁî®‰ª£ÁêÜ\n      # proxies: clash\n      # proxies: v2ray\n      # proxies: 127.0.0.1:7890\n      # proxies:\n      #   http: 127.0.0.1:7890\n      #   https: 127.0.0.1:7890\n      proxies: system\n\n      # cookies: Â∏êÂè∑ÈÖçÁΩÆÔºåÈªòËÆ§ÊòØ nullÔºåË°®Á§∫Êú™ÁôªÂΩïÁä∂ÊÄÅËÆøÈóÆJM„ÄÇ\n      # Á¶ÅÊº´ÁöÑÂ§ßÈÉ®ÂàÜÊú¨Â≠êÔºå‰∏ãËΩΩÊòØ‰∏çÈúÄË¶ÅÁôªÂΩïÁöÑÔºõÂ∞ëÈÉ®ÂàÜÊïèÊÑüÈ¢òÊùêÈúÄË¶ÅÁôªÂΩïÊâçËÉΩÁúã„ÄÇ\n      # Â¶ÇÊûú‰Ω†Â∏åÊúõ‰ª•ÁôªÂΩïÁä∂ÊÄÅ‰∏ãËΩΩÊú¨Â≠êÔºåÊúÄÁÆÄÂçïÁöÑÊñπÂºèÊòØÈÖçÁΩÆ‰∏Ä‰∏ãÊµèËßàÂô®ÁöÑcookiesÔºå\n      # ‰∏çÁî®ÂÖ®ÈÉ®cookiesÔºåÂè™Ë¶ÅÈÇ£‰∏™Âè´ AVS Â∞±Ë°å„ÄÇ\n      # ÁâπÂà´Ê≥®ÊÑèÔºÅÔºÅÔºÅ(https://github.com/hect0x7/JMComic-Crawler-Python/issues/104)\n      # cookiesÊòØÂå∫ÂàÜÂüüÂêçÁöÑÔºö\n      # ÂÅáÂ¶Ç‰Ω†Ë¶ÅËÆøÈóÆÁöÑÊòØ `18comic.vip`ÔºåÈÇ£‰πà‰Ω†ÈÖçÁΩÆÁöÑcookies‰πüË¶ÅÊù•Ëá™‰∫é `18comic.vip`Ôºå‰∏çËÉΩÈÖçÁΩÆÊù•Ëá™‰∫é `jm-comic.club` ÁöÑcookies„ÄÇ\n      # Â¶ÇÊûú‰Ω†ÂèëÁé∞ÈÖçÁΩÆ‰∫ÜcookiesËøòÊòØÊ≤°ÊúâÊïàÊûúÔºåÂ§ßÊ¶ÇÁéáÂ∞±ÊòØ‰Ω†ÈÖçÁΩÆÁöÑcookiesÂíå‰ª£Á†ÅËÆøÈóÆÁöÑÂüüÂêç‰∏ç‰∏ÄËá¥„ÄÇ\n      cookies:\n        AVS: qkwehjjasdowqeq # Ëøô‰∏™ÂÄºÊòØ‰π±ÊâìÁöÑÔºå‰∏çËÉΩÁî®\n\n# ‰∏ãËΩΩÈÖçÁΩÆ\ndownload:\n  cache: true # Â¶ÇÊûúË¶Å‰∏ãËΩΩÁöÑÊñá‰ª∂Âú®Á£ÅÁõò‰∏äÂ∑≤Â≠òÂú®Ôºå‰∏çÁî®ÂÜç‰∏ã‰∏ÄÈÅç‰∫ÜÂêßÔºüÈªòËÆ§‰∏∫true\n  image:\n    decode: true # JMÁöÑÂéüÂõæÊòØÊ∑∑Ê∑ÜËøáÁöÑÔºåË¶Å‰∏çË¶ÅËøòÂéüÔºüÈªòËÆ§‰∏∫true\n    suffix: .jpg # ÊääÂõæÁâáÈÉΩËΩ¨‰∏∫.jpgÊ†ºÂºèÔºåÈªòËÆ§‰∏∫nullÔºåË°®Á§∫‰∏çËΩ¨Êç¢„ÄÇ\n  threading:\n    # image: ÂêåÊó∂‰∏ãËΩΩÁöÑÂõæÁâáÊï∞ÔºåÈªòËÆ§ÊòØ30Âº†Âõæ\n    # Êï∞ÂÄºÂ§ßÔºå‰∏ãÂæóÂø´ÔºåÈÖçÁΩÆË¶ÅÊ±ÇÈ´òÔºåÂØπÁ¶ÅÊº´ÂéãÂäõÂ§ß\n    # Êï∞ÂÄºÂ∞èÔºå‰∏ãÂæóÊÖ¢ÔºåÈÖçÁΩÆË¶ÅÊ±Ç‰ΩéÔºåÂØπÁ¶ÅÊº´ÂéãÂäõÂ∞è\n    # PS: Á¶ÅÊº´ÁΩëÈ°µ‰∏ÄÊ¨°ÊúÄÂ§öËØ∑Ê±Ç50Âº†Âõæ\n    image: 30\n    # photo: ÂêåÊó∂‰∏ãËΩΩÁöÑÁ´†ËäÇÊï∞Ôºå‰∏çÈÖçÁΩÆÈªòËÆ§ÊòØcpuÁöÑÁ∫øÁ®ãÊï∞„ÄÇ‰æãÂ¶Ç8Ê†∏16Á∫øÁ®ãÁöÑcpu ‚Üí 16.\n    photo: 16\n\n\n\n# Êñá‰ª∂Â§πËßÑÂàôÈÖçÁΩÆÔºåÂÜ≥ÂÆöÂõæÁâáÊñá‰ª∂Â≠òÊîæÂú®‰Ω†ÁöÑÁîµËÑë‰∏äÁöÑÂì™‰∏™Êñá‰ª∂Â§π\ndir_rule:\n  # base_dir: Ê†πÁõÆÂΩï„ÄÇ\n  # Ê≠§ÈÖçÁΩÆ‰πüÊîØÊåÅÂºïÁî®ÁéØÂ¢ÉÂèòÈáèÔºå‰æãÂ¶Ç\n  # base_dir: ${JM_DIR}/‰∏ãËΩΩÊñá‰ª∂Â§π/\n  base_dir: D:/a/b/c/\n\n  # rule: ËßÑÂàôdsl„ÄÇ\n  # Êú¨È°πÂè™Âª∫ËÆÆ‰∫ÜËß£ÁºñÁ®ãÁöÑÊúãÂèãÂÆöÂà∂ÔºåÂÆûÁé∞Âú®Ëøô‰∏™Á±ª: jmcomic.jm_option.DirRule\n  # ÂÜôÊ≥ï:\n  # 1. ‰ª•'Bd'ÂºÄÂ§¥ÔºåË°®Á§∫Ê†πÁõÆÂΩï\n  # 2. Êñá‰ª∂Â§πÊØèÂ¢ûÂä†‰∏ÄÂ±ÇÔºå‰ΩøÁî® '_' ÊàñËÄÖ '/' Âå∫Èöî\n  # 3. Áî®PxxxÊàñËÄÖAyyyÊåá‰ª£Êñá‰ª∂Â§πÂêçÔºåÊÑèÊÄùÊòØ JmPhotoDetail.xxx / JmAlbumDetailÁöÑ.yyy„ÄÇ\n  # xxxÂíåyyyÂèØ‰ª•ÂÜô‰ªÄ‰πàÈúÄË¶ÅÁúãÊ∫êÁ†ÅÔºåÊàñÈÄöËøá‰∏ãÈù¢‰ª£Á†ÅÊâìÂç∞Âá∫ÊâÄÊúâÂèØÁî®ÁöÑÂÄº\n  # \n  # ```python\n  # import jmcomic\n  # properties: dict = jmcomic.JmOption.default().new_jm_client().get_album_detail(Êú¨Â≠êid).get_properties_dict()\n  # print(properties)\n  # ```\n  # \n  # ‰∏ãÈù¢ÊºîÁ§∫Â¶ÇÊûúË¶Å‰ΩøÁî®Á¶ÅÊº´ÁΩëÁ´ôÁöÑÈªòËÆ§‰∏ãËΩΩÊñπÂºèÔºåËØ•ÊÄé‰πàÂÜô:\n  # ËßÑÂàô: Ê†πÁõÆÂΩï / Êú¨Â≠êid / Á´†ËäÇÂ∫èÂè∑ / ÂõæÁâáÊñá‰ª∂\n  # rule: 'Bd  / Aid   / Pindex'\n  # rule: 'Bd_Aid_Pindex'\n\n  # ÈªòËÆ§ËßÑÂàôÊòØ: Ê†πÁõÆÂΩï / Á´†ËäÇÊ†áÈ¢ò / ÂõæÁâáÊñá‰ª∂\n  rule: Bd_Ptitle\n  # jmcomic v2.5.36 ‰ª•ÂêéÔºåÊîØÊåÅ‰ΩøÁî®pythonÁöÑf-stringÁöÑËØ≠Ê≥ïÁªÑÂêàÊñá‰ª∂Â§πÂêçÔºå‰∏ã‰∏∫Á§∫‰æã\n  # rule: Bd / Aauthor / (JM{Aid}-{Pindex})-{Pname}\n  # {}Â§ßÊã¨Âè∑ÈáåÁöÑÂÜÖÂÆπÂêåÊ†∑ÊòØÂÜô Axxx Êàñ PxxxÔºåÂÖ∂‰ªñËØ≠Ê≥ïËá™Ë°åÂèÇËÄÉpython f-stringÁöÑËØ≠Ê≥ï\n```\n\n## 3. optionÊèí‰ª∂ÈÖçÁΩÆÈ°π\n\n* **Êèí‰ª∂ÈÖçÁΩÆ‰∏≠ÁöÑkwargsÂèÇÊï∞ÊîØÊåÅÂºïÁî®ÁéØÂ¢ÉÂèòÈáèÔºåËØ≠Ê≥ï‰∏∫ ${ÁéØÂ¢ÉÂèòÈáèÂêç}**\n\n```yaml\n# Êèí‰ª∂ÁöÑÈÖçÁΩÆÁ§∫‰æã\nplugins:\n  after_init:\n    - plugin: usage_log # ÂÆûÊó∂ÊâìÂç∞Á°¨‰ª∂Âç†Áî®ÁéáÁöÑÊèí‰ª∂\n      kwargs:\n        interval: 0.5 # Èó¥ÈöîÊó∂Èó¥\n        enable_warning: true # Âç†Áî®ËøáÂ§ßÊó∂ÂèëÂá∫È¢ÑË≠¶\n\n    - plugin: login # ÁôªÂΩïÊèí‰ª∂\n      kwargs:\n        username: un # Áî®Êà∑Âêç\n        password: pw # ÂØÜÁ†Å\n\n    - plugin: find_update # Âè™‰∏ãËΩΩÊñ∞Á´†Êèí‰ª∂\n      kwargs:\n        145504: 290266 # ‰∏ãËΩΩÊú¨Â≠ê145504ÁöÑÁ´†ËäÇ290266‰ª•ÂêéÁöÑÊñ∞Á´†\n\n    - plugin: image_suffix_filter # ÂõæÁâáÂêéÁºÄËøáÊª§Âô®Êèí‰ª∂ÔºåÂèØ‰ª•ÊéßÂà∂Âè™‰∏ãËΩΩÂì™‰∫õÂêéÁºÄÁöÑÂõæÁâá\n      kwargs:\n        allowed_orig_suffix: # ÂêéÁºÄÂàóË°®ÔºåË°®Á§∫Âè™ÊÉ≥‰∏ãËΩΩ‰ª•.gifÁªìÂ∞æÁöÑÂõæÁâá\n          - .gif\n    - plugin: replace_path_string # Â≠óÁ¨¶‰∏≤ÊõøÊç¢Êèí‰ª∂ÔºåÁõ¥Êé•ÂØπ‰∏ãËΩΩÊñá‰ª∂Â§πÁöÑË∑ØÂæÑËøõË°åÊñáÊú¨ÊõøÊç¢\n      kwargs:\n        replace: \n          # {Â∑¶ËæπÂÜô‰Ω†Ë¶ÅÊõøÊç¢ÁöÑÂéüÊñá}: {Âè≥ËæπÂÜôÊõøÊç¢Êàê‰ªÄ‰πàÊñáÊú¨}\n          aaa: bbb\n          kyockcho: „Åç„Çá„Åè„Å°„Çá\n          \n    - plugin: client_proxy # ÂÆ¢Êà∑Á´ØÂÆûÁé∞Á±ª‰ª£ÁêÜÊèí‰ª∂Ôºå‰∏çÂª∫ËÆÆÈùûÂºÄÂèë‰∫∫Âëò‰ΩøÁî®\n      kwargs:\n        proxy_client_key: photo_concurrent_fetcher_proxy # ‰ª£ÁêÜÁ±ªÁöÑclient_key\n        whitelist: [ api, ] # ÁôΩÂêçÂçïÔºåÂΩìclient.implÂåπÈÖçÁôΩÂêçÂçïÊó∂Êâç‰ª£ÁêÜ\n\n    - plugin: auto_set_browser_cookies # Ëá™Âä®Ëé∑ÂèñÊµèËßàÂô®cookiesÔºåËØ¶ËßÅÊèí‰ª∂Á±ª‰ª£Á†Å‚ÜíAutoSetBrowserCookiesPlugin\n      kwargs:\n        browser: chrome\n        domain: 18comic.vip\n    \n    # v2.5.0 ÂºïÂÖ•ÁöÑÊèí‰ª∂\n    # ÂèØ‰ª•ÂêØÂä®‰∏Ä‰∏™ÊúçÂä°Âô®ÔºåÂèØ‰ª•Âú®ÊµèËßàÂô®‰∏äÊü•ÁúãÊú¨Â≠ê\n    # Âü∫‰∫éflaskÊ°ÜÊû∂ÔºåÈúÄË¶ÅÂÆâË£ÖÈ¢ùÂ§ñÂ∫ì: [pip install plugin_jm_server]\n    # Ê∫êÁ†ÅÔºöhttps://github.com/hect0x7/plugin-jm-server\n    - plugin: jm_server \n      kwargs:\n        password: '3333' # ÊúçÂä°Âô®ËÆøÈóÆÂØÜÁ†Å\n        base_dir: D:/a/b/c/ # Ê†πÁõÆÂΩïÔºåÈªòËÆ§‰ΩøÁî®dir_rule.base_dir\n        \n        # ‰∏ãÈù¢ÊòØÈ´òÁ∫ßÈÖçÁΩÆÔºå‰∏çÈÖçÁΩÆ‰πüÂèØ‰ª•\n        \n        # run‰∏ãÁöÑÂèÇÊï∞ÊòØflaskÊ°ÜÊû∂ÁöÑappÂØπË±°ÁöÑrunÊñπÊ≥ïÂèÇÊï∞ÔºåËØ¶ËßÅflaskÊñáÊ°£\n        run:\n          host: 0.0.0.0 # ÈªòËÆ§Êé•Êî∂ÊâÄÊúâipÁöÑËØ∑Ê±Ç\n          port: 80 # ÊúçÂä°Âô®Á´ØÂè£ÔºåÈªòËÆ§‰∏∫80\n          debug: false # ÊòØÂê¶ÂºÄÂêØdebugÊ®°ÂºèÔºåÈªòËÆ§‰∏∫false\n          \n        # ÊîØÊåÅÈáçÂÜôËÉåÊôØÂõæÁâáÔºåÂèØ‰ª•‰ΩøÁî®‰Ω†ÂñúÊ¨¢ÁöÑËÉåÊôØÂõæÁâá‰Ωú‰∏∫ËÉåÊôØ\n        img_overwrite:\n          bg.jpg: D:/ÊµèËßàÂô®ÁöÑËÉåÊôØÂõæ\n          m_bg.jpeg: D:/ÁßªÂä®ËÆæÂ§áÊµèËßàÂô®ÁöÑËÉåÊôØÂõæ\n\n    - plugin: subscribe_album_update # Ëá™Âä®ËÆ¢ÈòÖÊú¨Â≠êÂπ∂‰∏ãËΩΩ„ÄÅÂèëÈÄÅÈÇÆ‰ª∂ÈÄöÁü•ÁöÑÊèí‰ª∂\n      kwargs:\n        download_if_has_update: true\n        email_notify: # ÂèÇÊï∞ËØ¥ÊòéËßÅ‰∏ã„ÄêÂèëÈÄÅqqÈÇÆ‰ª∂Êèí‰ª∂„Äë\n          msg_from: aaa@qq.com\n          msg_to: aaa@qq.com\n          password: dkjlakdjlkas\n          title: album update !!!\n          content: album update !!!\n        album_photo_dict:\n          324930: 424507\n\n  after_album:\n    - plugin: zip # ÂéãÁº©Êñá‰ª∂Êèí‰ª∂\n      kwargs:\n        level: photo # ÊåâÁÖßÁ´†ËäÇÔºå‰∏Ä‰∏™Á´†ËäÇ‰∏Ä‰∏™ÂéãÁº©Êñá‰ª∂\n        # level ‰πüÂèØ‰ª•ÈÖçÊàê albumÔºåË°®Á§∫‰∏Ä‰∏™Êú¨Â≠êÂØπÂ∫î‰∏Ä‰∏™ÂéãÁº©Êñá‰ª∂ÔºåËØ•ÂéãÁº©Êñá‰ª∂‰ºöÂåÖÂê´Ëøô‰∏™Êú¨Â≠êÁöÑÊâÄÊúâÁ´†ËäÇ\n\n        filename_rule: Ptitle # ÂéãÁº©Êñá‰ª∂ÁöÑÂëΩÂêçËßÑÂàô\n        # ËØ∑Ê≥®ÊÑè‚ö† [https://github.com/hect0x7/JMComic-Crawler-Python/issues/223#issuecomment-2045227527]\n        # filename_ruleÂíålevelÊúâÂØπÂ∫îÂÖ≥Á≥ª\n        # Â¶ÇÊûúlevel=[photo], filename_ruleÂè™ËÉΩÂÜôPxxx\n        # Â¶ÇÊûúlevel=[album], filename_ruleÂè™ËÉΩÂÜôAxxx\n\n        zip_dir: D:/jmcomic/zip/ # ÂéãÁº©Êñá‰ª∂Â≠òÊîæÁöÑÊñá‰ª∂Â§π\n\n        suffix: zip #ÂéãÁº©ÂåÖÂêéÁºÄÂêçÔºåÈªòËÆ§ÂÄº‰∏∫zipÔºåÂèØ‰ª•ÊåáÂÆö‰∏∫zipÊàñËÄÖ7z\n\n        # v2.6.0 ‰ª•ÂêéÔºåzipÊèí‰ª∂‰πüÊîØÊåÅdir_ruleÈÖçÁΩÆÈ°πÔºåÂèØ‰ª•Êõø‰ª£ÊóßÁâàÊú¨ÁöÑzip_dirÂíåfilename_rule\n        # ËØ∑Ê≥®ÊÑè‚ö† ‰ΩøÁî®Ê≠§ÈÖçÁΩÆÈ°π‰ºö‰Ωøfilename_ruleÔºåzip_dirÔºåsuffix‰∏â‰∏™ÈÖçÁΩÆÈ°πÊó†ÊïàÔºå‰∏éËøô‰∏â‰∏™ÈÖçÁΩÆÈ°πÂêåÊó∂Â≠òÂú®Êó∂‰ªÖ‰ºö‰ΩøÁî®dir_rule\n        # Á§∫‰æãÂ¶Ç‰∏ã:\n        # dir_rule: # Êñ∞ÈÖçÁΩÆÈ°πÔºåÂèØÂèñ‰ª£ÊóßÁöÑzip_dirÂíåfilename_rule\n        #   base_dir: D:/jmcomic-zip\n        #   rule: 'Bd / {Atitle} / [{Pid}]-{Ptitle}.zip'  # ËÆæÁΩÆÂéãÁº©Êñá‰ª∂Â§πËßÑÂàôÔºå‰∏≠Èó¥AtitleË°®Á§∫ÂàõÂª∫‰∏ÄÂ±ÇÊñá‰ª∂Â§πÔºåÂêçÁß∞ÊòØÊú¨Â≠êÊ†áÈ¢ò„ÄÇ[{Pid}]-{Ptitle}.zip Ë°®Á§∫ÂéãÁº©Êñá‰ª∂ÁöÑÂëΩÂêçËßÑÂàô(ÈúÄÊòæÂºèÂÜôÂá∫ÂêéÁºÄÂêç)\n        # ‰ΩøÁî®Ê≠§ÊñπÊ≥ïÊåáÂÆöÂéãÁº©ÂåÖÂ≠òÂÇ®Ë∑ØÂæÑÂàôÊó†ÈúÄÂíålevelÂØπÂ∫î\n\n        delete_original_file: true # ÂéãÁº©ÊàêÂäüÂêéÔºåÂà†Èô§ÊâÄÊúâÂéüÊñá‰ª∂ÂíåÊñá‰ª∂Â§π\n        \n        # Âú®v2.6.0Âèä‰ª•ÂêéÁâàÊú¨ÔºåzipÊèí‰ª∂ËøòÊîØÊåÅËÆæÁΩÆÂØÜÁ†ÅÂíåÂä†ÂØÜÊñπÂºèÔºå‰ΩøÁî®encryptÈÖçÁΩÆÈ°πÔºåËØ•ÈÖçÁΩÆÊòØÂèØÈÄâÁöÑÔºåÁ§∫‰æãÂ¶Ç‰∏ãÔºö\n        # 1. ÁªôÂéãÁº©ÂåÖËÆæÁΩÆ‰∏Ä‰∏™ÊåáÂÆöÂØÜÁ†Å\n        # encrypt:\n        #   password: 123456\n        # 2. ËÆæÁΩÆÈöèÊú∫ÁîüÊàêÁöÑÂØÜÁ†Å„ÄÇËØ•ÂØÜÁ†Å‰ºöÂú®Êó•Âøó‰∏≠ÊâìÂç∞Âá∫Êù•ÔºåÂπ∂ÈôÑÁùÄÂà∞zipÁöÑÂéãÁº©Êñá‰ª∂Ê≥®ÈáäÈáå\n        # encrypt:\n        #   type: random\n        # ÈÖçÁΩÆÂØÜÁ†ÅÊó∂ÔºåtypeÂíåpassword‰∫åÈÄâ‰∏ÄÂøÖÂ°´\n        \n        # Êèí‰ª∂ËøòÊîØÊåÅ‰ΩøÁî®7zÂä†ÂØÜÔºåËøôÁßçÊñπÂºè‰ºöÂä†ÂØÜÂéãÁº©ÂåÖÊñá‰ª∂Â§¥ÔºåÂè™ÊúâËæìÂÖ•‰∫ÜÂØÜÁ†ÅÊâçËÉΩÊü•ÁúãÂéãÁº©ÂåÖÊñá‰ª∂ÂàóË°®ÔºåÈöêÁßÅÊÄßÊúÄÂ•Ω„ÄÇ\n        # ‰ΩøÁî®encrypt.implÈÖçÁΩÆÈ°πÂºÄÂêØ7zÊ†ºÂºèÂä†ÂØÜÔºåÂ¶ÇÊûú‰∏çÈÖçÁΩÆÔºåÈªòËÆ§‰ªç‰ΩøÁî®zipÊ†ºÂºè„ÄÇ\n        # ‰ΩøÁî®7zÊ†ºÂºèÊó∂ËÆ∞ÂæóÊääÂéãÁº©ÂåÖÂêéÁºÄÂêçÊåáÂÆö‰∏∫7z„ÄÇ\n        # Á§∫‰æãÂ¶Ç‰∏ã:\n        # suffix: 7z\n        # encrypt:\n        #   impl: 7z\n        #   type: random # typeÂíåpassword‰∫åÈÄâ‰∏ÄÂøÖÂ°´ÔºåÂíå‰∏äÈù¢‰∏ÄÊ†∑\n        # ÈúÄË¶ÅÊèêÈÜíÁöÑÊòØÔºå7zÊ≤°ÊúâÂéãÁº©Êñá‰ª∂Ê≥®ÈáäÔºåÂõ†Ê≠§Â¶ÇÊûúËÆæÁΩÆÈöèÊú∫ÂØÜÁ†ÅÔºåÂØÜÁ†ÅÂ∞±Âè™‰ºöÂ≠òÂú®‰∫éÊó•Âøó‰∏≠ÔºåËØ∑Ê≥®ÊÑèÂèäÊó∂‰øùÂ≠òÂØÜÁ†Å„ÄÇ\n         \n    \n    # Âà†Èô§ÈáçÂ§çÊñá‰ª∂Êèí‰ª∂\n    # ÂèÇËÄÉ ‚Üí [https://github.com/hect0x7/JMComic-Crawler-Python/issues/244]\n    - plugin: delete_duplicated_files\n      kwargs:\n        # limit: ÂøÖÂ°´ÔºåË°®Á§∫ÂØπmd5Âá∫Áé∞Ê¨°Êï∞ÁöÑÈôêÂà∂\n        limit: 3\n        # Â¶ÇÊûúÊñá‰ª∂ÁöÑmd5ÁöÑÂá∫Áé∞Ê¨°Êï∞ >= limitÔºåÊòØÂê¶Ë¶ÅÂà†Èô§\n        # Â¶ÇÊûúdelete_original_file‰∏çÈÖçÁΩÆÔºåÊ≠§Êèí‰ª∂Âè™‰ºöÊâìÂç∞‰ø°ÊÅØÔºå‰∏ç‰ºöÊâßË°åÂÖ∂‰ªñÊìç‰Ωú\n        # Â¶ÇÊûúlimit=1, delete_original_file=true ÊïàÊûú‰ºöÊòØÂà†Èô§ÊâÄÊúâÊñá‰ª∂ \n        delete_original_file: true\n\n    - plugin: send_qq_email # ÂèëÈÄÅqqÈÇÆ‰ª∂Êèí‰ª∂\n      kwargs:\n        msg_from: ${EMAIL} # Âèë‰ª∂‰∫∫\n        msg_to: aaa@qq.com # Êî∂‰ª∂‰∫∫\n        password: dkjlakdjlkas # Âèë‰ª∂‰∫∫ÁöÑÊéàÊùÉÁ†Å\n        title: jmcomic # Ê†áÈ¢ò\n        content: jmcomic finished !!! # ÂÜÖÂÆπ\n\n  main:\n    - plugin: favorite_folder_export # ÂØºÂá∫Êî∂ËóèÂ§πÊèí‰ª∂\n      log: false\n      kwargs:\n        zip_enable: true # ÂØπÂØºÂá∫Êñá‰ª∂ËøõË°åÂéãÁº©\n        zip_filepath: ${JM_DOWNLOAD_DIR}/export.zip # ÂéãÁº©Êñá‰ª∂Ë∑ØÂæÑ\n        zip_password: ${ZIP_PASSWORD} # ÂéãÁº©ÂØÜÁ†Å\n  \n  before_photo:\n    - plugin: skip_photo_with_few_images # Ë∑≥Ëøá‰∏ãËΩΩÁ´†ËäÇÂõæÁâáÊï∞ÈáèËøáÂ∞ëÁöÑÁ´†ËäÇ„ÄÇ‰∏Ä‰∫õÈü©Êº´ÁöÑÁ´†ËäÇÊòØÂÖ¨ÂëäÔºåÊ≤°ÊúâÂÆûÈôÖÂÜÖÂÆπÔºåÂ∞±ÂèØ‰ª•Áî®ËØ•Êèí‰ª∂Êù•Ë∑≥Ëøá‰∏ãËΩΩËøô‰∫õÁ´†ËäÇ„ÄÇ\n      kwargs:\n        at_least_image_count: 3 # Ëá≥Â∞ëË¶ÅÊúâÂ§öÂ∞ëÂº†ÂõæÔºåÊâç‰∏ãËΩΩÊ≠§Á´†ËäÇ\n\n  after_photo:\n    # ÊääÁ´†ËäÇÁöÑÊâÄÊúâÂõæÁâáÂêàÂπ∂‰∏∫‰∏Ä‰∏™pdfÁöÑÊèí‰ª∂\n    # ‰ΩøÁî®ÂâçÈúÄË¶ÅÂÆâË£Ö‰æùËµñÂ∫ì: [pip install img2pdf]\n    - plugin: img2pdf\n      kwargs:\n        pdf_dir: D:/pdf/ # pdfÂ≠òÊîæÊñá‰ª∂Â§π\n        filename_rule: Pid # pdfÂëΩÂêçËßÑÂàôÔºåP‰ª£Ë°®photo, id‰ª£Ë°®‰ΩøÁî®photo.id‰πüÂ∞±ÊòØÁ´†ËäÇid\n        encrypt: # pdfÂØÜÁ†ÅÔºåÂèØÈÄâÈÖçÁΩÆÈ°π\n          password: 123 # ÂØÜÁ†Å\n  \n    # img2pdf‰πüÊîØÊåÅÂêàÂπ∂Êï¥‰∏™Êú¨Â≠êÔºåÊää‰∏äÊñπÁöÑafter_photoÊîπ‰∏∫after_albumÂç≥ÂèØ„ÄÇ\n    # https://github.com/hect0x7/JMComic-Crawler-Python/discussions/258\n    # ÈÖçÁΩÆÂà∞after_albumÊó∂ÔºåÈúÄË¶Å‰øÆÊîπfilename_ruleÂèÇÊï∞Ôºå‰∏çËÉΩÂÜôPxxÂè™ËÉΩÂÜôAxxÁ§∫‰æãÂ¶Ç‰∏ã\n    - plugin: img2pdf\n      kwargs:\n        pdf_dir: D:/pdf/ # pdfÂ≠òÊîæÊñá‰ª∂Â§π\n        filename_rule: Aname # pdfÂëΩÂêçËßÑÂàôÔºåA‰ª£Ë°®album, name‰ª£Ë°®‰ΩøÁî®album.name‰πüÂ∞±ÊòØÊú¨Â≠êÂêçÁß∞\n  \n    # Êèí‰ª∂Êù•Ê∫êÔºöhttps://github.com/hect0x7/JMComic-Crawler-Python/pull/294\n    # long_imgÊèí‰ª∂ÊòØÊääÊâÄÊúâÂõæÁâáÂêàÂπ∂‰∏∫‰∏Ä‰∏™pngÈïøÂõæÔºåÊïàÊûúÂíåimg2pdfÁ±ª‰ºº\n    - plugin: long_img\n      kwargs:\n        img_dir: D:/pdf/ # ÈïøÂõæÂ≠òÊîæÊñá‰ª∂Â§π\n        filename_rule: Aname # ÈïøÂõæÂëΩÂêçËßÑÂàôÔºåÂêå‰∏ä\n  \n```\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": "assets/docs/sources/option_file_syntax.md", "language": "markdown", "complexity": "complex", "star_count": 3197, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 301}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Â∏∏Áî®Á±ªÂíåÊñπÊ≥ï\n\n## ‰∏ãËΩΩÊú¨Â≠ê/Á´†ËäÇ\n\n```python\nfrom jmcomic import *\n\n# ‰∏ãËΩΩid‰∏∫438696ÁöÑÊú¨Â≠ê (https://18comic.vip/album/438696)\ndownload_album(438696)\n\n# ‰∏ãËΩΩÁ´†ËäÇ (https://18comic.vip/photo/438696)\ndownload_photo(438696)\n\n# ÂêåÊó∂‰∏ãËΩΩÂ§ö‰∏™Êú¨Â≠ê\ndownload_album([123, 456, 789])\n```\n\n## ‰ΩøÁî®optionÂÆöÂà∂Âåñ‰∏ãËΩΩÊú¨Â≠ê\n\nÂ¶ÇÊûú‰Ω†Âú®‰∏ãËΩΩÊú¨Â≠êÊó∂Êúâ‰∏Ä‰∫õÂÆöÂà∂ÂåñÈúÄÊ±ÇÔºå\n\n‰æãÂ¶ÇÊåáÂÆöÁ¶ÅÊº´ÂüüÂêçÔºå‰ΩøÁî®‰ª£ÁêÜÔºåÁôªÂΩïÁ¶ÅÊº´ÔºåÂõæÁâáÊ†ºÂºèËΩ¨Êç¢Á≠âÁ≠âÔºå\n\nÈÇ£‰πàÔºå‰Ω†ÂèØ‰ª•ËØïËØïÁúãjmcomicÊèê‰æõÁöÑoptionÊú∫Âà∂\n\n```python\nfrom jmcomic import *\n\n# 1. Âú®Ë∞ÉÁî®‰∏ãËΩΩapiÂâçÔºåÈÄöËøáÂàõÂª∫Âíå‰ΩøÁî®optionÂØπË±°ÔºåÂèØ‰ª•ÂÆöÂà∂Âåñ‰∏ãËΩΩË°å‰∏∫„ÄÇ\n# Êé®Ëçê‰ΩøÁî®ÈÖçÁΩÆÊñá‰ª∂ÁöÑÊñπÂºèÊù•ÂàõÂª∫optionÂØπË±°Ôºå\n# ‰Ω†ÂèØ‰ª•ÈÖçÁΩÆÂæàÂ§ö‰∏úË•øÔºåÊØîÂ¶Ç‰ª£ÁêÜ„ÄÅcookies„ÄÅ‰∏ãËΩΩËßÑÂàôÁ≠âÁ≠â„ÄÇ\n# ÈÖçÁΩÆÊñá‰ª∂ÁöÑËØ≠Ê≥ïÂèÇËÄÉ: https://jmcomic.readthedocs.io/en/latest/option_file_syntax/\noption = create_option_by_file('op.yml')  # ÈÄöËøáÈÖçÁΩÆÊñá‰ª∂Êù•ÂàõÂª∫optionÂØπË±°\n\n# 2. Ë∞ÉÁî®‰∏ãËΩΩapiÔºåÊääoption‰Ωú‰∏∫ÂèÇÊï∞‰º†ÈÄí \ndownload_album(123, option)\n# ‰πüÂèØ‰ª•‰ΩøÁî®‰∏ãÈù¢ËøôÁßçÈù¢ÂêëÂØπË±°ÁöÑÊñπÂºèÔºåÊòØ‰∏ÄÊ†∑ÁöÑ\noption.download_album(123)\n```\n\n## Ëé∑ÂèñÊú¨Â≠ê/Á´†ËäÇ/ÂõæÁâáÁöÑÂÆû‰ΩìÁ±ªÔºå‰∏ãËΩΩÂõæÁâá\n\n```python\nfrom jmcomic import *\n\n# ÂÆ¢Êà∑Á´Ø\nclient = JmOption.default().new_jm_client()\n\n# Êú¨Â≠êÂÆû‰ΩìÁ±ª\nalbum: JmAlbumDetail = client.get_album_detail('427413')\n\n\ndef fetch(photo: JmPhotoDetail):\n    # Á´†ËäÇÂÆû‰ΩìÁ±ª\n    photo = client.get_photo_detail(photo.photo_id, False)\n    print(f'Á´†ËäÇid: {photo.photo_id}')\n    \n    # ÂõæÁâáÂÆû‰ΩìÁ±ª\n    image: JmImageDetail\n    for image in photo:\n        print(f'ÂõæÁâáurl: {image.img_url}')\n        \n    # ‰∏ãËΩΩÂçï‰∏™ÂõæÁâá\n    client.download_by_image_detail(image, './a.jpg')\n    # Â¶ÇÊûúÊòØÂ∑≤Áü•Êú™Ê∑∑Ê∑ÜÁöÑÂõæÁâáÔºå‰πüÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®urlÊù•‰∏ãËΩΩ\n    random_image_domain = JmModuleConfig.DOMAIN_IMAGE_LIST[0]\n    client.download_image(f'https://{random_image_domain}/media/albums/416130.jpg', './a.jpg')\n    \n\n# Â§öÁ∫øÁ®ãÂèëËµ∑ËØ∑Ê±Ç\nmulti_thread_launcher(\n    iter_objs=album,\n    apply_each_obj_func=fetch\n)\n```\n\n## jmcomicÂºÇÂ∏∏Â§ÑÁêÜÁ§∫‰æã\n\n```python\nfrom jmcomic import *\n\n# ÂÆ¢Êà∑Á´Ø\nclient = JmOption.default().new_jm_client()\n\n# ÊçïËé∑Ëé∑ÂèñÊú¨Â≠ê/Á´†ËäÇËØ¶ÊÉÖÊó∂ÂèØËÉΩÂá∫Áé∞ÁöÑÂºÇÂ∏∏\ntry:\n    # ËØ∑Ê±ÇÊú¨Â≠êÂÆû‰ΩìÁ±ª\n    album: JmAlbumDetail = client.get_album_detail('427413')\nexcept MissingAlbumPhotoException as e:\n    print(f'id={e.error_jmid}ÁöÑÊú¨Â≠ê‰∏çÂ≠òÂú®')\n\nexcept JsonResolveFailException as e:\n    print(f'Ëß£ÊûêjsonÂ§±Ë¥•')\n    # ÂìçÂ∫îÂØπË±°\n    resp = e.resp\n    print(f'resp.text: {resp.text}, resp.status_code: {resp.status_code}')\n\nexcept RequestRetryAllFailException as e:\n    print(f'ËØ∑Ê±ÇÂ§±Ë¥•ÔºåÈáçËØïÊ¨°Êï∞ËÄóÂ∞Ω')\n\nexcept JmcomicException as e:\n    # ÊçïËé∑ÊâÄÊúâÂºÇÂ∏∏ÔºåÁî®‰ΩúÂÖúÂ∫ï\n    print(f'jmcomicÈÅáÂà∞ÂºÇÂ∏∏: {e}')\n\n# Â§öÁ∫øÁ®ã‰∏ãËΩΩÊó∂ÔºåÂèØËÉΩÂá∫Áé∞ÈùûÂΩìÂâçÁ∫øÁ®ã‰∏ãËΩΩÂ§±Ë¥•ÔºåÊäõÂá∫ÂºÇÂ∏∏Ôºå\n# ËÄåJmDownloaderÊúâÂØπÂ∫îÂ≠óÊÆµËÆ∞ÂΩï‰∫ÜËøô‰∫õÁ∫øÁ®ãÂèëÁîüÁöÑÂºÇÂ∏∏\n# ‰ΩøÁî®check_exception=TrueÂèÇÊï∞ÂèØ‰ª•‰Ωødownloader‰∏ªÂä®Ê£ÄÊü•ÊòØÂê¶Â≠òÂú®‰∏ãËΩΩÂºÇÂ∏∏\n# Â¶ÇÊûúÊúâÔºåÂàôÂΩìÂâçÁ∫øÁ®ã‰ºö‰∏ªÂä®‰∏äÊäõ‰∏Ä‰∏™PartialDownloadFailedExceptionÂºÇÂ∏∏\n# ËØ•ÂèÇÊï∞‰∏ªË¶ÅÁî®‰∫é‰∏ªÂä®Ê£ÄÊü•ÈÉ®ÂàÜ‰∏ãËΩΩÂ§±Ë¥•ÁöÑÊÉÖÂÜµÔºå\n# Âõ†‰∏∫ÈùûÂΩìÂâçÁ∫øÁ®ãÊäõÂá∫ÁöÑÂºÇÂ∏∏ÔºàÊØîÂ¶Ç‰∏ãËΩΩÁ´†ËäÇÁöÑÁ∫øÁ®ãÂíå‰∏ãËΩΩÂõæÁâáÁöÑÁ∫øÁ®ãÔºâÔºåËøô‰∫õÁ∫øÁ®ãÂ¶ÇÊûúÊäõÂá∫ÂºÇÂ∏∏Ôºå\n# ÂΩìÂâçÁ∫øÁ®ãÊòØÊÑüÁü•‰∏çÂà∞ÁöÑÔºåtry-catch‰∏ãËΩΩÊñπÊ≥ïdownload_album‰∏çËÉΩÊçïËé∑Âà∞ÂÖ∂‰ªñÁ∫øÁ®ãÂèëÁîüÁöÑÂºÇÂ∏∏„ÄÇ\ntry:\n    album, downloader = download_album(123, check_exception=True)\nexcept PartialDownloadFailedException as e:\n    downloader: JmDownloader = e.downloader\n    print(f'‰∏ãËΩΩÂá∫Áé∞ÈÉ®ÂàÜÂ§±Ë¥•, ‰∏ãËΩΩÂ§±Ë¥•ÁöÑÁ´†ËäÇ: {downloader.download_failed_photo}, ‰∏ãËΩΩÂ§±Ë¥•ÁöÑÂõæÁâá: {downloader.download_failed_image}')\n```\n\n\n## ÊêúÁ¥¢Êú¨Â≠ê\n\n```python\nfrom jmcomic import *\n\nclient = JmOption.default().new_jm_client()\n\n# ÂàÜÈ°µÊü•ËØ¢Ôºåsearch_siteÂ∞±ÊòØÁ¶ÅÊº´ÁΩëÈ°µ‰∏äÁöÑ„ÄêÁ´ôÂÜÖÊêúÁ¥¢„Äë\npage: JmSearchPage = client.search_site(search_query='+MANA +Êó†‰øÆÊ≠£', page=1)\nprint(f'ÁªìÊûúÊÄªÊï∞: {page.total}, ÂàÜÈ°µÂ§ßÂ∞è: {page.page_size}ÔºåÈ°µÊï∞: {page.page_count}')\n\n# pageÈªòËÆ§ÁöÑËø≠‰ª£ÊñπÂºèÊòØpage.iter_id_title()ÔºåÊØèÊ¨°Ëø≠‰ª£ËøîÂõû albun_id, title\nfor album_id, title in page:\n    print(f'[{album_id}]: {title}')\n\n# Áõ¥Êé•ÊêúÁ¥¢Á¶ÅÊº´ËΩ¶Âè∑\npage = client.search_site(search_query='427413')\nalbum: JmAlbumDetail = page.single_album\nprint(album.tags)\n```\n\n## ÊêúÁ¥¢Âπ∂‰∏ãËΩΩÊú¨Â≠ê\n\n```python\nfrom jmcomic import *\n\noption = JmOption.default()\nclient = option.new_jm_client()\n\ntag = 'ÁÑ°‰øÆÊ≠£'\n# ÊêúÁ¥¢Ê†áÁ≠æÔºåÂèØ‰ª•‰ΩøÁî®search_tag„ÄÇ\n# ÊêúÁ¥¢Á¨¨‰∏ÄÈ°µ„ÄÇ\npage: JmSearchPage = client.search_tag(tag, page=1)\n\naid_list = []\n\nfor aid, atitle, tag_list in page.iter_id_title_tag():  # ‰ΩøÁî®pageÁöÑiter_id_title_tagËø≠‰ª£Âô®\n    if tag in tag_list:\n        print(f'[Ê†áÁ≠æ/{tag}] ÂèëÁé∞ÁõÆÊ†á: [{aid}]: [{atitle}]')\n        aid_list.append(aid)\n\ndownload_album(aid_list, option)\n```\n\n## Ëé∑ÂèñÊî∂ËóèÂ§π\n\nÂèØÂèÇËÄÉdiscussions: https://github.com/hect0x7/JMComic-Crawler-Python/discussions/235\n\n```python\nfrom jmcomic import *\n\noption = JmOption.default()\nclient = option.new_jm_client()\nclient.login('Áî®Êà∑Âêç', 'ÂØÜÁ†Å')  # ‰πüÂèØ‰ª•‰ΩøÁî®loginÊèí‰ª∂/ÈÖçÁΩÆcookies\n\n# ÈÅçÂéÜÂÖ®ÈÉ®Êî∂ËóèÁöÑÊâÄÊúâÈ°µ\nfor page in client.favorite_folder_gen():  # Â¶ÇÊûú‰Ω†Âè™ÊÉ≥Ëé∑ÂèñÁâπÂÆöÊî∂ËóèÂ§πÔºåÈúÄË¶ÅÊ∑ªÂä†folder_idÂèÇÊï∞\n    # ÈÅçÂéÜÊØèÈ°µÁªìÊûú\n    for aid, atitle in page.iter_id_title():\n        # aid: Êú¨Â≠êÁöÑalbum_id\n        # atitle: Êú¨Â≠êÁöÑÂêçÁß∞\n        print(aid)\n    # ÊâìÂç∞ÂΩìÂâçÂ∏êÂè∑ÁöÑÊâÄÊúâÊî∂ËóèÂ§π‰ø°ÊÅØ\n    for folder_id, folder_name in page.iter_folder_id_name():\n        print(f'Êî∂ËóèÂ§πid: {folder_id}, Êî∂ËóèÂ§πÂêçÁß∞: {folder_name}')\n\n# Ëé∑ÂèñÁâπÂÆöÊî∂ËóèÂ§πÁöÑÂçïÈ°µÔºå‰ΩøÁî®favorite_folderÊñπÊ≥ï\npage = client.favorite_folder(page=1,\n                              order_by=JmMagicConstants.ORDER_BY_LATEST,\n                              folder_id='0'  # Êî∂ËóèÂ§πid\n                              )\n```\n\n## ÂàÜÁ±ª / ÊéíË°åÊ¶ú\n\nÁ¶ÅÊº´ÁöÑÂàÜÁ±ªÊòØ‰∏Ä‰∏™ÂíåÊêúÁ¥¢Êúâ‰∫õÁ±ª‰ººÁöÑÂäüËÉΩ„ÄÇ\n\nÊêúÁ¥¢ÊòØÊåâÊüê‰∏ÄÊù°‰ª∂ËøõË°åËøáÊª§„ÄÇ\n\nÂàÜÁ±ªÊ≤°ÊúâËøáÊª§ÔºåÂ∞±ÊòØÊääÊüê‰∏ÄÁ±ªÂà´ÔºàcategoryÔºâ‰∏ãÁöÑÊú¨Â≠êÂÖ®ÈÉΩË∞ÉÂá∫Êù•„ÄÇ\n\nÁ¶ÅÊº´ÁöÑÊéíË°åÊ¶úÂ∞±ÊòØÂàÜÁ±ªÁöÑ‰∏ÄÁßçÂΩ¢Âºè\n\n‰∏ãÈù¢ÊºîÁ§∫Ë∞ÉÁî®ÂàÜÁ±ªapi\n\n```python\nfrom jmcomic import *\n\n# ÂàõÂª∫ÂÆ¢Êà∑Á´Ø\nop = JmOption.default()\ncl = op.new_jm_client()\n\n# Ë∞ÉÁî®ÂàÜÁ±ªÊé•Âè£\n# Ê†πÊçÆ‰∏ãÈù¢ÁöÑÂèÇÊï∞ÔºåËøô‰∏™Ë∞ÉÁî®ÁöÑÊÑè‰πâÂ∞±ÊòØÔºö\n# Âú®ÂÖ®ÈÉ®ÂàÜÁ±ª‰∏ãÔºåÈÄâÊã©ÊâÄÊúâÊó∂Èó¥ËåÉÂõ¥ÔºåÊåâËßÇÁúãÊï∞ÊéíÂ∫èÂêéÔºåËé∑ÂèñÁ¨¨‰∏ÄÈ°µÁöÑÊú¨Â≠ê\npage: JmCategoryPage = cl.categories_filter(\n    page=1,\n    time=JmMagicConstants.TIME_ALL,  # Êó∂Èó¥ÈÄâÊã©ÂÖ®ÈÉ®ÔºåÂÖ∑‰ΩìÂèØ‰ª•ÂÜô‰ªÄ‰πàËØ∑ËßÅJmMagicConstants\n    category=JmMagicConstants.CATEGORY_ALL,  # ÂàÜÁ±ªÈÄâÊã©ÂÖ®ÈÉ®ÔºåÂÖ∑‰ΩìÂèØ‰ª•ÂÜô‰ªÄ‰πàËØ∑ËßÅJmMagicConstants\n    order_by=JmMagicConstants.ORDER_BY_LATEST,  # ÊåâÁÖßËßÇÁúãÊï∞ÊéíÂ∫èÔºåÂÖ∑‰ΩìÂèØ‰ª•ÂÜô‰ªÄ‰πàËØ∑ËßÅJmMagicConstants\n)\n\n# ÊúàÊéíË°åÔºåÂ∫ïÂ±ÇÂÆûÁé∞‰πüÊòØË∞ÉÁöÑcategories_filter\npage: JmCategoryPage = cl.month_ranking(1)\n# Âë®ÊéíË°å\npage: JmCategoryPage = cl.week_ranking(1)\n\n# Âæ™ÁéØËé∑ÂèñÂàÜÈ°µÔºå‰ΩøÁî® cl.categories_filter_gen\nfor page in cl.categories_filter_gen(page=1, # Ëµ∑ÂßãÈ°µÁ†Å\n                                     # ‰∏ãÈù¢ÊòØÂàÜÁ±ªÂèÇÊï∞\n                                     time=JmMagicConstants.TIME_WEEK,\n                                     category=JmMagicConstants.CATEGORY_ALL,\n                                     order_by=JmMagicConstants.ORDER_BY_VIEW,\n                                     ):\n    for aid, atitle in page:\n        print(aid, atitle)\n\n```\n\n## È´òÁ∫ßÊêúÁ¥¢ÔºàÂàÜÁ±ª/ÂâØÂàÜÁ±ªÔºâ\n\nÁ¶ÅÊº´ÁΩëÈ°µÁ´ØÁöÑÊêúÁ¥¢Èô§‰∫ÜÂ∏∏ËßÑÊù°‰ª∂ÔºåËøòÊîØÊåÅ„ÄêÂàÜÁ±ª„ÄëÂíå„ÄêÂâØÂàÜÁ±ª„ÄëÁöÑÊêúÁ¥¢„ÄÇ\n\nÂú®‰ªª‰∏ÄÊêúÁ¥¢È°µÈù¢Ôºå‰Ω†‰ºöÁúãÂà∞Êú¨Â≠êÂõæÁöÑÂè≥‰∏äÊñπÊúâ‰∏§‰∏™Ê†áÁ≠æ„ÄÇÂ∑¶ËæπÁöÑÊòØ„ÄêÂàÜÁ±ª„ÄëÔºåÂè≥ËæπÁöÑÊòØ„ÄêÂâØÂàÜÁ±ª„Äë„ÄÇ\n\n‰∏ãÈù¢ÊºîÁ§∫‰ª£Á†ÅÂ¶Ç‰ΩïÁºñÂÜô„ÄÇ\n\n* **Ê≥®ÊÑèÔºÅÔºÅÁ¶ÅÊº´ÁßªÂä®Á´ØÊ≤°ÊúâÊèê‰æõÂ¶Ç‰∏ãÂäüËÉΩÔºå‰ª•‰∏ã‰ª£Á†Å‰ªÖÂØπÁΩëÈ°µÁ´ØÁîüÊïà„ÄÇ**\n\n```python\n# Âú®ÁºñÂÜô‰ª£Á†ÅÂâçÔºåÂª∫ËÆÆÂÖàÁÜüÊÇâÁ¶ÅÊº´ÁΩëÈ°µÁöÑÊêúÊú¨ÂäüËÉΩÔºå‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÈÉΩÊòØÂØπÁÖßÁΩëÈ°µÁºñÂÜôÁöÑ„ÄÇ\n# ÁΩëÈ°µÊêúÁ¥¢Á§∫‰æãÔºöhttps://18comic.vip/search/photos/doujin/sub/CG?main_tag=0&search_query=mana&page=1&o=mr&t=a\n\nfrom jmcomic import *\n\nop = create_option_by_file('op.yml')\n# ÂàõÂª∫ÁΩëÈ°µÁ´Øclient\nhtml_cl = op.new_jm_client(impl='html')\n\n# ‰ΩøÁî®Á´ôÂÜÖÊêúÁ¥¢ÔºåÊåáÂÆö„ÄêÂàÜÁ±ª„ÄëÂíå„ÄêÂâØÂàÜÁ±ª„Äë\n# ÂàÜÁ±ª = JmMagicConstants.CATEGORY_DOUJIN = Âêå‰∫∫Êú¨\n# ÂâØÂàÜÁ±ª = JmMagicConstants.SUB_DOUJIN_CG = CGÊú¨\n# ÂÆûÈôÖURLÔºöhttps://18comic.vip/search/photos/doujin/sub/CG?main_tag=0&search_query=mana&page=1&o=mr&t=a\npage = html_cl.search_site(search_query='mana',\n                           category=JmMagicConstants.CATEGORY_DOUJIN,\n                           sub_category=JmMagicConstants.SUB_DOUJIN_CG,\n                           page=1,\n                           )\n# ÊâìÂç∞pageÂÜÖÂÆπ\nfor aid, atitle in page.iter_id_title():\n    print(aid, atitle)\n\n# Âæ™ÁéØËé∑ÂèñÂàÜÈ°µ\nfor page in html_cl.search_gen(search_query='mana',\n                               category=JmMagicConstants.CATEGORY_DOUJIN,\n                               sub_category=JmMagicConstants.SUB_DOUJIN_CG,\n                               page=1,  # Ëµ∑ÂßãÈ°µÁ†Å\n                               ):\n    # ÊâìÂç∞pageÂÜÖÂÆπ\n    for aid, atitle in page.iter_id_title():\n        print(aid, atitle)\n```\n\n\n## ÊâãÂä®ÂàõÂª∫Client\n\n```python\n# ÈªòËÆ§ÁöÑ‰ΩøÁî®ÊñπÂºèÊòØÂÖàÂàõÂª∫optionÔºåoptionÂ∞ÅË£Ö‰∫ÜÊâÄÊúâÈÖçÁΩÆÔºåÁÑ∂ÂêéÁî±option.new_jm_client() ÂàõÂª∫ÂÆ¢Êà∑Á´ØclientÔºå‰ΩøÁî®clientÂèØ‰ª•ËÆøÈóÆÁ¶ÅÊº´Êé•Âè£\n\n# ‰∏ãÈù¢ÊºîÁ§∫Áõ¥Êé•ÊûÑÈÄ†clientÁöÑÊñπÂºè\nfrom jmcomic import *\n\n\"\"\"\nÂàõÂª∫JMÂÆ¢Êà∑Á´Ø\n\n:param postman: Ë¥üË¥£ÂÆûÁé∞HTTPËØ∑Ê±ÇÁöÑÂØπË±°ÔºåÊåÅÊúâcookies„ÄÅheaders„ÄÅproxiesÁ≠â‰ø°ÊÅØ\n:param domain_list: Á¶ÅÊº´ÂüüÂêç\n:param retry_times: ÈáçËØïÊ¨°Êï∞\n\"\"\"\n\n# ÁΩëÈ°µÁ´Ø\ncl = JmHtmlClient(\n    postman=JmModuleConfig.new_postman(),\n    domain_list=['18comic.vip'],\n    retry_times=1\n)\n\n# APIÁ´ØÔºàAPPÔºâ\ncl = JmApiClient(\n    postman=JmModuleConfig.new_postman(),\n    domain_list=JmModuleConfig.DOMAIN_API_LIST,\n    retry_times=1\n)\n```\n", "metadata": {"source": "github.com/hect0x7/JMComic-Crawler-Python", "file_path": "assets/docs/sources/tutorial/0_common_usage.md", "language": "markdown", "complexity": "complex", "star_count": 3197, "quality_score": 0.75, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 314}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: 2\n\nupdates:\n  - package-ecosystem: pip\n    directory: \"/\"\n    schedule:\n      interval: monthly\n      time: \"12:00\"\n    pull-request-branch-name:\n      separator: \"-\"\n    open-pull-requests-limit: 10\n\n  - package-ecosystem: \"github-actions\"\n    directory: \"/\"\n    schedule:\n      interval: monthly\n      time: \"12:00\"\n    pull-request-branch-name:\n      separator: \"-\"\n    open-pull-requests-limit: 10\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": ".github/dependabot.yml", "language": "yaml", "complexity": "simple", "star_count": 2917, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: API spec\n\non:\n  push:\n    branches:\n      - \"master\"\n\n  pull_request:\n    branches:\n      - \"*\"\n\njobs:\n  api-spec:\n    name: API spec tests\n\n    runs-on: ubuntu-18.04\n\n    strategy:\n      matrix:\n        python-version: [3.9]\n\n    services:\n      postgres:\n        image: postgres:11.5-alpine\n        env:\n          POSTGRES_USER: postgres\n          POSTGRES_PASSWORD: postgres\n          POSTGRES_DB: postgres\n        ports:\n          - 5432:5432\n        options: --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5\n\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v4.2.0\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install Poetry\n        uses: snok/install-poetry@v1\n        with:\n          version: \"1.1.12\"\n          virtualenvs-in-project: true\n\n      - name: Set up cache\n        uses: actions/cache@v3\n        id: cache\n        with:\n          path: .venv\n          key: venv-${{ runner.os }}-py-${{ matrix.python-version }}-poetry-${{ hashFiles('poetry.lock') }}\n\n      - name: Ensure cache is healthy\n        if: steps.cache.outputs.cache-hit == 'true'\n        run: poetry run pip --version >/dev/null 2>&1 || rm -rf .venv\n\n      - name: Install dependencies\n        run: poetry install --no-interaction\n\n      - name: Run newman and test service\n        env:\n          SECRET_KEY: secret_key\n          DATABASE_URL: postgresql://postgres:postgres@localhost/postgres\n        run: |\n          poetry run alembic upgrade head\n          poetry run uvicorn app.main:app &\n          APIURL=http://localhost:8000/api ./postman/run-api-tests.sh\n          poetry run alembic downgrade base\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": ".github/workflows/conduit.yml", "language": "yaml", "complexity": "medium", "star_count": 2917, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 69}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Deploy\n\non:\n  push:\n    branches:\n      - master\n\nenv:\n  IMAGE_NAME: nsidnev/fastapi-realworld-example-app\n  DOCKER_USER: ${{ secrets.DOCKER_USER }}\n  DOCKER_PASSWORD: ${{ secrets.DOCKER_PASSWORD }}\n\njobs:\n  build:\n    name: Build Container\n\n    runs-on: ubuntu-18.04\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Build image and publish to registry\n        run: |\n          docker build -t $IMAGE_NAME:latest .\n          echo $DOCKER_PASSWORD | docker login -u $DOCKER_USER --password-stdin\n          docker push $IMAGE_NAME:latest\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": ".github/workflows/deploy.yml", "language": "yaml", "complexity": "medium", "star_count": 2917, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 27}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Styles\n\non:\n  push:\n    branches:\n      - \"master\"\n\n  pull_request:\n    branches:\n      - \"*\"\n\njobs:\n  lint:\n    name: Lint code\n\n    runs-on: ubuntu-18.04\n\n    strategy:\n      matrix:\n        python-version: [3.9]\n\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v4.2.0\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install Poetry\n        uses: snok/install-poetry@v1\n        with:\n          version: \"1.1.12\"\n          virtualenvs-in-project: true\n\n      - name: Set up cache\n        uses: actions/cache@v3\n        id: cache\n        with:\n          path: .venv\n          key: venv-${{ runner.os }}-py-${{ matrix.python-version }}-poetry-${{ hashFiles('poetry.lock') }}\n\n      - name: Ensure cache is healthy\n        if: steps.cache.outputs.cache-hit == 'true'\n        run: poetry run pip --version >/dev/null 2>&1 || rm -rf .venv\n\n      - name: Install dependencies\n        run: poetry install --no-interaction\n\n      - name: Run linters\n        run: poetry run ./scripts/lint\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": ".github/workflows/styles.yml", "language": "yaml", "complexity": "simple", "star_count": 2917, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 51}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Tests\n\non:\n  push:\n    branches:\n      - \"master\"\n\n  pull_request:\n    branches:\n      - \"*\"\n\njobs:\n  lint:\n    name: Run tests\n\n    runs-on: ubuntu-18.04\n\n    strategy:\n      matrix:\n        python-version: [3.9]\n\n    services:\n      postgres:\n        image: postgres:11.5-alpine\n        env:\n          POSTGRES_USER: postgres\n          POSTGRES_PASSWORD: postgres\n          POSTGRES_DB: postgres\n        ports:\n          - 5432:5432\n        options: --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4.2.0\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install Poetry\n        uses: snok/install-poetry@v1\n        with:\n          version: \"1.1.12\"\n          virtualenvs-in-project: true\n\n      - name: Set up cache\n        uses: actions/cache@v3\n        id: cache\n        with:\n          path: .venv\n          key: venv-${{ runner.os }}-py-${{ matrix.python-version }}-poetry-${{ hashFiles('poetry.lock') }}\n\n      - name: Ensure cache is healthy\n        if: steps.cache.outputs.cache-hit == 'true'\n        run: poetry run pip --version >/dev/null 2>&1 || rm -rf .venv\n\n      - name: Install dependencies\n        run: poetry install --no-interaction\n\n      - name: Run tests\n        env:\n          SECRET_KEY: secret_key\n          DATABASE_URL: postgresql://postgres:postgres@localhost/postgres\n        run: |\n          poetry run alembic upgrade head\n          poetry run ./scripts/test\n\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v3.1.0\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": ".github/workflows/tests.yml", "language": "yaml", "complexity": "medium", "star_count": 2917, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 71}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Optional\n\nfrom fastapi import Depends, HTTPException, Path, Query\nfrom starlette import status\n\nfrom app.api.dependencies.authentication import get_current_user_authorizer\nfrom app.api.dependencies.database import get_repository\nfrom app.db.errors import EntityDoesNotExist\nfrom app.db.repositories.articles import ArticlesRepository\nfrom app.models.domain.articles import Article\nfrom app.models.domain.users import User\nfrom app.models.schemas.articles import (\n    DEFAULT_ARTICLES_LIMIT,\n    DEFAULT_ARTICLES_OFFSET,\n    ArticlesFilters,\n)\nfrom app.resources import strings\nfrom app.services.articles import check_user_can_modify_article\n\n\ndef get_articles_filters(\n    tag: Optional[str] = None,\n    author: Optional[str] = None,\n    favorited: Optional[str] = None,\n    limit: int = Query(DEFAULT_ARTICLES_LIMIT, ge=1),\n    offset: int = Query(DEFAULT_ARTICLES_OFFSET, ge=0),\n) -> ArticlesFilters:\n    return ArticlesFilters(\n        tag=tag,\n        author=author,\n        favorited=favorited,\n        limit=limit,\n        offset=offset,\n    )\n\n\nasync def get_article_by_slug_from_path(\n    slug: str = Path(..., min_length=1),\n    user: Optional[User] = Depends(get_current_user_authorizer(required=False)),\n    articles_repo: ArticlesRepository = Depends(get_repository(ArticlesRepository)),\n) -> Article:\n    try:\n        return await articles_repo.get_article_by_slug(slug=slug, requested_user=user)\n    except EntityDoesNotExist:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=strings.ARTICLE_DOES_NOT_EXIST_ERROR,\n        )\n\n\ndef check_article_modification_permissions(\n    current_article: Article = Depends(get_article_by_slug_from_path),\n    user: User = Depends(get_current_user_authorizer()),\n) -> None:\n    if not check_user_can_modify_article(current_article, user):\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=strings.USER_IS_NOT_AUTHOR_OF_ARTICLE,\n        )\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": "app/api/dependencies/articles.py", "language": "python", "complexity": "simple", "star_count": 2917, "quality_score": 0.72, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 60}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# noqa:WPS201\nfrom typing import Callable, Optional\n\nfrom fastapi import Depends, HTTPException, Security\nfrom fastapi.security import APIKeyHeader\nfrom starlette import requests, status\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\n\nfrom app.api.dependencies.database import get_repository\nfrom app.core.config import get_app_settings\nfrom app.core.settings.app import AppSettings\nfrom app.db.errors import EntityDoesNotExist\nfrom app.db.repositories.users import UsersRepository\nfrom app.models.domain.users import User\nfrom app.resources import strings\nfrom app.services import jwt\n\nHEADER_KEY = \"Authorization\"\n\n\nclass RWAPIKeyHeader(APIKeyHeader):\n    async def __call__(  # noqa: WPS610\n        self,\n        request: requests.Request,\n    ) -> Optional[str]:\n        try:\n            return await super().__call__(request)\n        except StarletteHTTPException as original_auth_exc:\n            raise HTTPException(\n                status_code=original_auth_exc.status_code,\n                detail=strings.AUTHENTICATION_REQUIRED,\n            )\n\n\ndef get_current_user_authorizer(*, required: bool = True) -> Callable:  # type: ignore\n    return _get_current_user if required else _get_current_user_optional\n\n\ndef _get_authorization_header_retriever(\n    *,\n    required: bool = True,\n) -> Callable:  # type: ignore\n    return _get_authorization_header if required else _get_authorization_header_optional\n\n\ndef _get_authorization_header(\n    api_key: str = Security(RWAPIKeyHeader(name=HEADER_KEY)),\n    settings: AppSettings = Depends(get_app_settings),\n) -> str:\n    try:\n        token_prefix, token = api_key.split(\" \")\n    except ValueError:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=strings.WRONG_TOKEN_PREFIX,\n        )\n    if token_prefix != settings.jwt_token_prefix:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=strings.WRONG_TOKEN_PREFIX,\n        )\n\n    return token\n\n\ndef _get_authorization_header_optional(\n    authorization: Optional[str] = Security(\n        RWAPIKeyHeader(name=HEADER_KEY, auto_error=False),\n    ),\n    settings: AppSettings = Depends(get_app_settings),\n) -> str:\n    if authorization:\n        return _get_authorization_header(authorization, settings)\n\n    return \"\"\n\n\nasync def _get_current_user(\n    users_repo: UsersRepository = Depends(get_repository(UsersRepository)),\n    token: str = Depends(_get_authorization_header_retriever()),\n    settings: AppSettings = Depends(get_app_settings),\n) -> User:\n    try:\n        username = jwt.get_username_from_token(\n            token,\n            str(settings.secret_key.get_secret_value()),\n        )\n    except ValueError:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=strings.MALFORMED_PAYLOAD,\n        )\n\n    try:\n        return await users_repo.get_user_by_username(username=username)\n    except EntityDoesNotExist:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=strings.MALFORMED_PAYLOAD,\n        )\n\n\nasync def _get_current_user_optional(\n    repo: UsersRepository = Depends(get_repository(UsersRepository)),\n    token: str = Depends(_get_authorization_header_retriever(required=False)),\n    settings: AppSettings = Depends(get_app_settings),\n) -> Optional[User]:\n    if token:\n        return await _get_current_user(repo, token, settings)\n\n    return None\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": "app/api/dependencies/authentication.py", "language": "python", "complexity": "medium", "star_count": 2917, "quality_score": 0.8099999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 112}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Optional\n\nfrom fastapi import Depends, HTTPException, Path\nfrom starlette import status\n\nfrom app.api.dependencies import articles, authentication, database\nfrom app.db.errors import EntityDoesNotExist\nfrom app.db.repositories.comments import CommentsRepository\nfrom app.models.domain.articles import Article\nfrom app.models.domain.comments import Comment\nfrom app.models.domain.users import User\nfrom app.resources import strings\nfrom app.services.comments import check_user_can_modify_comment\n\n\nasync def get_comment_by_id_from_path(\n    comment_id: int = Path(..., ge=1),\n    article: Article = Depends(articles.get_article_by_slug_from_path),\n    user: Optional[User] = Depends(\n        authentication.get_current_user_authorizer(required=False),\n    ),\n    comments_repo: CommentsRepository = Depends(\n        database.get_repository(CommentsRepository),\n    ),\n) -> Comment:\n    try:\n        return await comments_repo.get_comment_by_id(\n            comment_id=comment_id,\n            article=article,\n            user=user,\n        )\n    except EntityDoesNotExist:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=strings.COMMENT_DOES_NOT_EXIST,\n        )\n\n\ndef check_comment_modification_permissions(\n    comment: Comment = Depends(get_comment_by_id_from_path),\n    user: User = Depends(authentication.get_current_user_authorizer()),\n) -> None:\n    if not check_user_can_modify_comment(comment, user):\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=strings.USER_IS_NOT_AUTHOR_OF_ARTICLE,\n        )\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": "app/api/dependencies/comments.py", "language": "python", "complexity": "simple", "star_count": 2917, "quality_score": 0.72, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 48}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Optional\n\nfrom fastapi import Depends, HTTPException, Path\nfrom starlette.status import HTTP_404_NOT_FOUND\n\nfrom app.api.dependencies.authentication import get_current_user_authorizer\nfrom app.api.dependencies.database import get_repository\nfrom app.db.errors import EntityDoesNotExist\nfrom app.db.repositories.profiles import ProfilesRepository\nfrom app.models.domain.profiles import Profile\nfrom app.models.domain.users import User\nfrom app.resources import strings\n\n\nasync def get_profile_by_username_from_path(\n    username: str = Path(..., min_length=1),\n    user: Optional[User] = Depends(get_current_user_authorizer(required=False)),\n    profiles_repo: ProfilesRepository = Depends(get_repository(ProfilesRepository)),\n) -> Profile:\n    try:\n        return await profiles_repo.get_profile_by_username(\n            username=username,\n            requested_user=user,\n        )\n    except EntityDoesNotExist:\n        raise HTTPException(\n            status_code=HTTP_404_NOT_FOUND,\n            detail=strings.USER_DOES_NOT_EXIST_ERROR,\n        )\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": "app/api/dependencies/profiles.py", "language": "python", "complexity": "simple", "star_count": 2917, "quality_score": 0.72, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import APIRouter, Depends, HTTPException, Query\nfrom starlette import status\n\nfrom app.api.dependencies.articles import get_article_by_slug_from_path\nfrom app.api.dependencies.authentication import get_current_user_authorizer\nfrom app.api.dependencies.database import get_repository\nfrom app.db.repositories.articles import ArticlesRepository\nfrom app.models.domain.articles import Article\nfrom app.models.domain.users import User\nfrom app.models.schemas.articles import (\n    DEFAULT_ARTICLES_LIMIT,\n    DEFAULT_ARTICLES_OFFSET,\n    ArticleForResponse,\n    ArticleInResponse,\n    ListOfArticlesInResponse,\n)\nfrom app.resources import strings\n\nrouter = APIRouter()\n\n\n@router.get(\n    \"/feed\",\n    response_model=ListOfArticlesInResponse,\n    name=\"articles:get-user-feed-articles\",\n)\nasync def get_articles_for_user_feed(\n    limit: int = Query(DEFAULT_ARTICLES_LIMIT, ge=1),\n    offset: int = Query(DEFAULT_ARTICLES_OFFSET, ge=0),\n    user: User = Depends(get_current_user_authorizer()),\n    articles_repo: ArticlesRepository = Depends(get_repository(ArticlesRepository)),\n) -> ListOfArticlesInResponse:\n    articles = await articles_repo.get_articles_for_user_feed(\n        user=user,\n        limit=limit,\n        offset=offset,\n    )\n    articles_for_response = [\n        ArticleForResponse(**article.dict()) for article in articles\n    ]\n    return ListOfArticlesInResponse(\n        articles=articles_for_response,\n        articles_count=len(articles),\n    )\n\n\n@router.post(\n    \"/{slug}/favorite\",\n    response_model=ArticleInResponse,\n    name=\"articles:mark-article-favorite\",\n)\nasync def mark_article_as_favorite(\n    article: Article = Depends(get_article_by_slug_from_path),\n    user: User = Depends(get_current_user_authorizer()),\n    articles_repo: ArticlesRepository = Depends(get_repository(ArticlesRepository)),\n) -> ArticleInResponse:\n    if not article.favorited:\n        await articles_repo.add_article_into_favorites(article=article, user=user)\n\n        return ArticleInResponse(\n            article=ArticleForResponse.from_orm(\n                article.copy(\n                    update={\n                        \"favorited\": True,\n                        \"favorites_count\": article.favorites_count + 1,\n                    },\n                ),\n            ),\n        )\n\n    raise HTTPException(\n        status_code=status.HTTP_400_BAD_REQUEST,\n        detail=strings.ARTICLE_IS_ALREADY_FAVORITED,\n    )\n\n\n@router.delete(\n    \"/{slug}/favorite\",\n    response_model=ArticleInResponse,\n    name=\"articles:unmark-article-favorite\",\n)\nasync def remove_article_from_favorites(\n    article: Article = Depends(get_article_by_slug_from_path),\n    user: User = Depends(get_current_user_authorizer()),\n    articles_repo: ArticlesRepository = Depends(get_repository(ArticlesRepository)),\n) -> ArticleInResponse:\n    if article.favorited:\n        await articles_repo.remove_article_from_favorites(article=article, user=user)\n\n        return ArticleInResponse(\n            article=ArticleForResponse.from_orm(\n                article.copy(\n                    update={\n                        \"favorited\": False,\n                        \"favorites_count\": article.favorites_count - 1,\n                    },\n                ),\n            ),\n        )\n\n    raise HTTPException(\n        status_code=status.HTTP_400_BAD_REQUEST,\n        detail=strings.ARTICLE_IS_NOT_FAVORITED,\n    )\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": "app/api/routes/articles/articles_common.py", "language": "python", "complexity": "medium", "star_count": 2917, "quality_score": 0.69, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 105}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Optional\n\nfrom fastapi import APIRouter, Body, Depends, HTTPException, Response\nfrom starlette import status\n\nfrom app.api.dependencies.articles import (\n    check_article_modification_permissions,\n    get_article_by_slug_from_path,\n    get_articles_filters,\n)\nfrom app.api.dependencies.authentication import get_current_user_authorizer\nfrom app.api.dependencies.database import get_repository\nfrom app.db.repositories.articles import ArticlesRepository\nfrom app.models.domain.articles import Article\nfrom app.models.domain.users import User\nfrom app.models.schemas.articles import (\n    ArticleForResponse,\n    ArticleInCreate,\n    ArticleInResponse,\n    ArticleInUpdate,\n    ArticlesFilters,\n    ListOfArticlesInResponse,\n)\nfrom app.resources import strings\nfrom app.services.articles import check_article_exists, get_slug_for_article\n\nrouter = APIRouter()\n\n\n@router.get(\"\", response_model=ListOfArticlesInResponse, name=\"articles:list-articles\")\nasync def list_articles(\n    articles_filters: ArticlesFilters = Depends(get_articles_filters),\n    user: Optional[User] = Depends(get_current_user_authorizer(required=False)),\n    articles_repo: ArticlesRepository = Depends(get_repository(ArticlesRepository)),\n) -> ListOfArticlesInResponse:\n    articles = await articles_repo.filter_articles(\n        tag=articles_filters.tag,\n        author=articles_filters.author,\n        favorited=articles_filters.favorited,\n        limit=articles_filters.limit,\n        offset=articles_filters.offset,\n        requested_user=user,\n    )\n    articles_for_response = [\n        ArticleForResponse.from_orm(article) for article in articles\n    ]\n    return ListOfArticlesInResponse(\n        articles=articles_for_response,\n        articles_count=len(articles),\n    )\n\n\n@router.post(\n    \"\",\n    status_code=status.HTTP_201_CREATED,\n    response_model=ArticleInResponse,\n    name=\"articles:create-article\",\n)\nasync def create_new_article(\n    article_create: ArticleInCreate = Body(..., embed=True, alias=\"article\"),\n    user: User = Depends(get_current_user_authorizer()),\n    articles_repo: ArticlesRepository = Depends(get_repository(ArticlesRepository)),\n) -> ArticleInResponse:\n    slug = get_slug_for_article(article_create.title)\n    if await check_article_exists(articles_repo, slug):\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=strings.ARTICLE_ALREADY_EXISTS,\n        )\n\n    article = await articles_repo.create_article(\n        slug=slug,\n        title=article_create.title,\n        description=article_create.description,\n        body=article_create.body,\n        author=user,\n        tags=article_create.tags,\n    )\n    return ArticleInResponse(article=ArticleForResponse.from_orm(article))\n\n\n@router.get(\"/{slug}\", response_model=ArticleInResponse, name=\"articles:get-article\")\nasync def retrieve_article_by_slug(\n    article: Article = Depends(get_article_by_slug_from_path),\n) -> ArticleInResponse:\n    return ArticleInResponse(article=ArticleForResponse.from_orm(article))\n\n\n@router.put(\n    \"/{slug}\",\n    response_model=ArticleInResponse,\n    name=\"articles:update-article\",\n    dependencies=[Depends(check_article_modification_permissions)],\n)\nasync def update_article_by_slug(\n    article_update: ArticleInUpdate = Body(..., embed=True, alias=\"article\"),\n    current_article: Article = Depends(get_article_by_slug_from_path),\n    articles_repo: ArticlesRepository = Depends(get_repository(ArticlesRepository)),\n) -> ArticleInResponse:\n    slug = get_slug_for_article(article_update.title) if article_update.title else None\n    article = await articles_repo.update_article(\n        article=current_article,\n        slug=slug,\n        **article_update.dict(),\n    )\n    return ArticleInResponse(article=ArticleForResponse.from_orm(article))\n\n\n@router.delete(\n    \"/{slug}\",\n    status_code=status.HTTP_204_NO_CONTENT,\n    name=\"articles:delete-article\",\n    dependencies=[Depends(check_article_modification_permissions)],\n    response_class=Response,\n)\nasync def delete_article_by_slug(\n    article: Article = Depends(get_article_by_slug_from_path),\n    articles_repo: ArticlesRepository = Depends(get_repository(ArticlesRepository)),\n) -> None:\n    await articles_repo.delete_article(article=article)\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": "app/api/routes/articles/articles_resource.py", "language": "python", "complexity": "medium", "star_count": 2917, "quality_score": 0.69, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 121}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import APIRouter, Body, Depends, HTTPException\nfrom starlette.status import HTTP_201_CREATED, HTTP_400_BAD_REQUEST\n\nfrom app.api.dependencies.database import get_repository\nfrom app.core.config import get_app_settings\nfrom app.core.settings.app import AppSettings\nfrom app.db.errors import EntityDoesNotExist\nfrom app.db.repositories.users import UsersRepository\nfrom app.models.schemas.users import (\n    UserInCreate,\n    UserInLogin,\n    UserInResponse,\n    UserWithToken,\n)\nfrom app.resources import strings\nfrom app.services import jwt\nfrom app.services.authentication import check_email_is_taken, check_username_is_taken\n\nrouter = APIRouter()\n\n\n@router.post(\"/login\", response_model=UserInResponse, name=\"auth:login\")\nasync def login(\n    user_login: UserInLogin = Body(..., embed=True, alias=\"user\"),\n    users_repo: UsersRepository = Depends(get_repository(UsersRepository)),\n    settings: AppSettings = Depends(get_app_settings),\n) -> UserInResponse:\n    wrong_login_error = HTTPException(\n        status_code=HTTP_400_BAD_REQUEST,\n        detail=strings.INCORRECT_LOGIN_INPUT,\n    )\n\n    try:\n        user = await users_repo.get_user_by_email(email=user_login.email)\n    except EntityDoesNotExist as existence_error:\n        raise wrong_login_error from existence_error\n\n    if not user.check_password(user_login.password):\n        raise wrong_login_error\n\n    token = jwt.create_access_token_for_user(\n        user,\n        str(settings.secret_key.get_secret_value()),\n    )\n    return UserInResponse(\n        user=UserWithToken(\n            username=user.username,\n            email=user.email,\n            bio=user.bio,\n            image=user.image,\n            token=token,\n        ),\n    )\n\n\n@router.post(\n    \"\",\n    status_code=HTTP_201_CREATED,\n    response_model=UserInResponse,\n    name=\"auth:register\",\n)\nasync def register(\n    user_create: UserInCreate = Body(..., embed=True, alias=\"user\"),\n    users_repo: UsersRepository = Depends(get_repository(UsersRepository)),\n    settings: AppSettings = Depends(get_app_settings),\n) -> UserInResponse:\n    if await check_username_is_taken(users_repo, user_create.username):\n        raise HTTPException(\n            status_code=HTTP_400_BAD_REQUEST,\n            detail=strings.USERNAME_TAKEN,\n        )\n\n    if await check_email_is_taken(users_repo, user_create.email):\n        raise HTTPException(\n            status_code=HTTP_400_BAD_REQUEST,\n            detail=strings.EMAIL_TAKEN,\n        )\n\n    user = await users_repo.create_user(**user_create.dict())\n\n    token = jwt.create_access_token_for_user(\n        user,\n        str(settings.secret_key.get_secret_value()),\n    )\n    return UserInResponse(\n        user=UserWithToken(\n            username=user.username,\n            email=user.email,\n            bio=user.bio,\n            image=user.image,\n            token=token,\n        ),\n    )\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": "app/api/routes/authentication.py", "language": "python", "complexity": "medium", "star_count": 2917, "quality_score": 0.78, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 94}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Optional\n\nfrom fastapi import APIRouter, Body, Depends, Response\nfrom starlette import status\n\nfrom app.api.dependencies.articles import get_article_by_slug_from_path\nfrom app.api.dependencies.authentication import get_current_user_authorizer\nfrom app.api.dependencies.comments import (\n    check_comment_modification_permissions,\n    get_comment_by_id_from_path,\n)\nfrom app.api.dependencies.database import get_repository\nfrom app.db.repositories.comments import CommentsRepository\nfrom app.models.domain.articles import Article\nfrom app.models.domain.comments import Comment\nfrom app.models.domain.users import User\nfrom app.models.schemas.comments import (\n    CommentInCreate,\n    CommentInResponse,\n    ListOfCommentsInResponse,\n)\n\nrouter = APIRouter()\n\n\n@router.get(\n    \"\",\n    response_model=ListOfCommentsInResponse,\n    name=\"comments:get-comments-for-article\",\n)\nasync def list_comments_for_article(\n    article: Article = Depends(get_article_by_slug_from_path),\n    user: Optional[User] = Depends(get_current_user_authorizer(required=False)),\n    comments_repo: CommentsRepository = Depends(get_repository(CommentsRepository)),\n) -> ListOfCommentsInResponse:\n    comments = await comments_repo.get_comments_for_article(article=article, user=user)\n    return ListOfCommentsInResponse(comments=comments)\n\n\n@router.post(\n    \"\",\n    status_code=status.HTTP_201_CREATED,\n    response_model=CommentInResponse,\n    name=\"comments:create-comment-for-article\",\n)\nasync def create_comment_for_article(\n    comment_create: CommentInCreate = Body(..., embed=True, alias=\"comment\"),\n    article: Article = Depends(get_article_by_slug_from_path),\n    user: User = Depends(get_current_user_authorizer()),\n    comments_repo: CommentsRepository = Depends(get_repository(CommentsRepository)),\n) -> CommentInResponse:\n    comment = await comments_repo.create_comment_for_article(\n        body=comment_create.body,\n        article=article,\n        user=user,\n    )\n    return CommentInResponse(comment=comment)\n\n\n@router.delete(\n    \"/{comment_id}\",\n    status_code=status.HTTP_204_NO_CONTENT,\n    name=\"comments:delete-comment-from-article\",\n    dependencies=[Depends(check_comment_modification_permissions)],\n    response_class=Response,\n)\nasync def delete_comment_from_article(\n    comment: Comment = Depends(get_comment_by_id_from_path),\n    comments_repo: CommentsRepository = Depends(get_repository(CommentsRepository)),\n) -> None:\n    await comments_repo.delete_comment(comment=comment)\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": "app/api/routes/comments.py", "language": "python", "complexity": "simple", "star_count": 2917, "quality_score": 0.69, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 72}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import APIRouter, Depends, HTTPException\nfrom starlette.status import HTTP_400_BAD_REQUEST\n\nfrom app.api.dependencies.authentication import get_current_user_authorizer\nfrom app.api.dependencies.database import get_repository\nfrom app.api.dependencies.profiles import get_profile_by_username_from_path\nfrom app.db.repositories.profiles import ProfilesRepository\nfrom app.models.domain.profiles import Profile\nfrom app.models.domain.users import User\nfrom app.models.schemas.profiles import ProfileInResponse\nfrom app.resources import strings\n\nrouter = APIRouter()\n\n\n@router.get(\n    \"/{username}\",\n    response_model=ProfileInResponse,\n    name=\"profiles:get-profile\",\n)\nasync def retrieve_profile_by_username(\n    profile: Profile = Depends(get_profile_by_username_from_path),\n) -> ProfileInResponse:\n    return ProfileInResponse(profile=profile)\n\n\n@router.post(\n    \"/{username}/follow\",\n    response_model=ProfileInResponse,\n    name=\"profiles:follow-user\",\n)\nasync def follow_for_user(\n    profile: Profile = Depends(get_profile_by_username_from_path),\n    user: User = Depends(get_current_user_authorizer()),\n    profiles_repo: ProfilesRepository = Depends(get_repository(ProfilesRepository)),\n) -> ProfileInResponse:\n    if user.username == profile.username:\n        raise HTTPException(\n            status_code=HTTP_400_BAD_REQUEST,\n            detail=strings.UNABLE_TO_FOLLOW_YOURSELF,\n        )\n\n    if profile.following:\n        raise HTTPException(\n            status_code=HTTP_400_BAD_REQUEST,\n            detail=strings.USER_IS_ALREADY_FOLLOWED,\n        )\n\n    await profiles_repo.add_user_into_followers(\n        target_user=profile,\n        requested_user=user,\n    )\n\n    return ProfileInResponse(profile=profile.copy(update={\"following\": True}))\n\n\n@router.delete(\n    \"/{username}/follow\",\n    response_model=ProfileInResponse,\n    name=\"profiles:unsubscribe-from-user\",\n)\nasync def unsubscribe_from_user(\n    profile: Profile = Depends(get_profile_by_username_from_path),\n    user: User = Depends(get_current_user_authorizer()),\n    profiles_repo: ProfilesRepository = Depends(get_repository(ProfilesRepository)),\n) -> ProfileInResponse:\n    if user.username == profile.username:\n        raise HTTPException(\n            status_code=HTTP_400_BAD_REQUEST,\n            detail=strings.UNABLE_TO_UNSUBSCRIBE_FROM_YOURSELF,\n        )\n\n    if not profile.following:\n        raise HTTPException(\n            status_code=HTTP_400_BAD_REQUEST,\n            detail=strings.USER_IS_NOT_FOLLOWED,\n        )\n\n    await profiles_repo.remove_user_from_followers(\n        target_user=profile,\n        requested_user=user,\n    )\n\n    return ProfileInResponse(profile=profile.copy(update={\"following\": False}))\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": "app/api/routes/profiles.py", "language": "python", "complexity": "medium", "star_count": 2917, "quality_score": 0.69, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 85}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import APIRouter, Depends\n\nfrom app.api.dependencies.database import get_repository\nfrom app.db.repositories.tags import TagsRepository\nfrom app.models.schemas.tags import TagsInList\n\nrouter = APIRouter()\n\n\n@router.get(\"\", response_model=TagsInList, name=\"tags:get-all\")\nasync def get_all_tags(\n    tags_repo: TagsRepository = Depends(get_repository(TagsRepository)),\n) -> TagsInList:\n    tags = await tags_repo.get_all_tags()\n    return TagsInList(tags=tags)\n", "metadata": {"source": "github.com/nsidnev/fastapi-realworld-example-app", "file_path": "app/api/routes/tags.py", "language": "python", "complexity": "simple", "star_count": 2917, "quality_score": 0.69, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 16}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This is a basic workflow to help you get started with Actions\n\nname: Check JMComic Android APK\n\n# Controls when the workflow will run\non:\n  push:\n  workflow_dispatch:\n  schedule:\n    - cron: '0 */2 * * *'\n\npermissions:\n  contents: write\n\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n  check:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python 3.11\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.11\"\n\n      - name: Install Dependency\n        run: |\n          python -m pip install --upgrade pip\n          pip install jmcomic\n\n      - name: Run Script\n        id: script\n        run: |\n          python main.py\n\n      - name: Release\n        if: ${{ steps.script.outputs.found_new == 'true'}}\n        uses: softprops/action-gh-release@v1\n        with:\n          name: v${{ steps.script.outputs.new_ver }}\n          tag_name: ${{ steps.script.outputs.new_ver }}\n          draft: false\n          files: |\n            ${{ steps.script.outputs.new_ver }}.apk\n          body: |\n            Á¶ÅÊº´APP v${{ steps.script.outputs.new_ver }}\n            ‰Ω†ÂèØ‰ª•Áõ¥Êé•‰∏ãËΩΩAssetsÁöÑ`${{ steps.script.outputs.new_ver }}.apk`‰ΩøÁî®ÔºåÊàñËÄÖ‰ªéÁ¶ÅÊº´ÂÆòÁΩë‰∏ãËΩΩ:\n            \n            Á¶ÅÊº´ÂÆòÁΩëÁöÑ`${{ steps.script.outputs.new_ver }}.apk`‰∏ãËΩΩÈìæÊé•Ôºö\n            1. [${{ steps.script.outputs.download_path }}](${{ steps.script.outputs.download_path }})\n            2. [${{ steps.script.outputs.download_path }}](${{ steps.script.outputs.download_path }})\n            3. [https://18comic.vip/stray](https://18comic.vip/stray/)ÔºàÂèëÂ∏ÉÈ°µÔºâ\n            \n            `${{ steps.script.outputs.new_ver }}.apk`ÁöÑÊõ¥Êñ∞‰ø°ÊÅØÔºö\n            \n            > ${{ steps.script.outputs.desc }}\n\n      - name: Commit\n        if: ${{ steps.script.outputs.found_new == 'true'}}\n        run: |\n          # Extract the version number from the previous step\n          version=${{ steps.script.outputs.new_ver }}\n          git config --global user.email \"93357912+hect0x7@users.noreply.github.com\"\n          git config --global user.name \"hect0x7\"\n          git tag -a \"v$version\" -m \"Version $version\"\n          git add ./APK_VERSION.txt\n          git commit -m \"update APK_VERSION.txt\"\n          git push\n          echo \"finished\"\n", "metadata": {"source": "github.com/hect0x7/JMComic-APK", "file_path": ".github/workflows/auto_release.yml", "language": "yaml", "complexity": "simple", "star_count": 2703, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 70}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# JMComic-APK\n\n## [‚ú®‚ú®‚ú®ÁÇπÊàëÂéªAPK‰∏ãËΩΩÁïåÈù¢‚ú®‚ú®‚ú®](https://github.com/hect0x7/JMComic-APK/releases/latest)\n\n## È°πÁõÆ‰ªãÁªçÔºö\n\n‰ΩøÁî®GitHub ActionsÂÆöÊó∂Ê£ÄÊü•Á¶ÅÊº´APKÊòØÂê¶ÊúâÊõ¥Êñ∞ÔºåÂ¶ÇÊûúÊúâÔºåÂ∞±‰∏ãËΩΩ‰∏Ä‰ªΩ‰øùÂ≠òÂà∞Release‰∏≠„ÄÇ\n\n\n## Áõ∏ÂÖ≥È°πÁõÆÔºö\n\nÊú¨È°πÁõÆÁöÑ‰ª£Á†ÅÂü∫‰∫é`jmcomic`Â∫ìÔºåËøôÊòØÊàëÂÜôÁöÑ‰∏Ä‰∏™pipÂ∫ìÔºåÊîØÊåÅÁî®python‰ª£Á†ÅËÆøÈóÆÁ¶ÅÊº´Â§©Â†Ç„ÄÇ\n\n`jmcomic`Â∫ìÁöÑÈìæÊé•Â¶Ç‰∏ãÔºà**ÂºïÊµÅÔºö‰∏ãÈù¢Ëøô‰∏™È°πÁõÆËøòÂèØ‰ª•Â∏Æ‰Ω†‰∏ÄÈîÆÊâπÈáè‰∏ãËΩΩÁ¶ÅÊº´Êú¨Â≠ê**Ôºâ\n\n<a href=\"https://github.com/hect0x7/JMComic-Crawler-Python\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github-readme-stats.vercel.app/api/pin/?username=hect0x7&repo=JMComic-Crawler-Python&theme=radical\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://github-readme-stats.vercel.app/api/pin/?username=hect0x7&repo=JMComic-Crawler-Python\" />\n    <img alt=\"Repo Card\" src=\"https://github-readme-stats.vercel.app/api/pin/?username=hect0x7&repo=JMComic-Crawler-Python\" />\n  </picture>\n</a>\n\n\nStarÂéÜÂè≤ÂõæÔºö\n<a href=\"https://github.com/hect0x7/JMComic-Crawler-Python\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=hect0x7/JMComic-Crawler-Python%2Chect0x7/JMComic-APK&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=hect0x7/JMComic-Crawler-Python%2Chect0x7/JMComic-APK&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=hect0x7/JMComic-Crawler-Python%2Chect0x7/JMComic-APK&type=Date\" />\n  </picture>\n</a>\n", "metadata": {"source": "github.com/hect0x7/JMComic-APK", "file_path": "README.md", "language": "markdown", "complexity": "simple", "star_count": 2703, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: zhaoolee\non:\n  schedule:\n    - cron:  '0 22 * * *'    # ÂõΩÈôÖÊ†áÂáÜÊó∂Èó¥22ÁÇπÔºàÂåó‰∫¨Êó∂Èó¥Êó©‰∏ä6ÁÇπÂèëÔºâ \n  push:\n    branches:    \n      - main\njobs:\n  push:\n      runs-on: ${{ matrix.operating-system }}\n      strategy:\n        matrix:\n          # in this example, there is a newer version already installed, 3.7.7, so the older version will be downloaded\n          operating-system: ['ubuntu-22.04']\n      steps:\n      - uses: actions/checkout@v3 # Checking out the repo\n      - name: Run with setup-python 3.9\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.9'\n          update-environment: false\n          cache: 'pipenv'\n      - name: Install pipenv\n        run: pip3 install --user pipenv\n      - name: Install dependecies\n        run:  pipenv --python python3 && pipenv install\n      - name: Build\n        run: pipenv run build\n      - name: Commit and push if changed   # Êõ¥Êñ∞README.md\n        run: |\n          git diff\n          git config --global user.email \"zhaoolee@gmail.com\"\n          git config --global user.name \"zhaoolee\"\n          git add README.md\n          git commit -m \"Github Action auto Updated\"\n          git push\n", "metadata": {"source": "github.com/zhaoolee/ins", "file_path": ".github/workflows/main.yml", "language": "yaml", "complexity": "medium", "star_count": 2240, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 37}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# INS üç≠ \n\nÂºÄÊ∫ê**ÁÅµÊÑü**Êï∞ÊçÆÂ∫ìÔºåÂÖçË¥πÊó†ÂπøÂëäÔºåGithub ActionsËá™Âä®Ê£ÄÊµãÁΩëÂùÄËÆøÈóÆÈÄüÂ∫¶~\n## ÁõÆÂΩï\n\n(ÁÇπÂáªÂèØÁõ¥Ëææ)\n\n--tagIndexInfoStart----tagIndexInfoEnd--\n\n[ÂàáÊç¢ÂÆΩÂ±èÊ®°Âºè](https://zhaoolee.com/ins/#ÁõÆÂΩï)\n\n![INSÁÅµÊÑü](https://raw.githubusercontent.com/zhaoolee/ins/master/media/ins.png)\n\n> Êú¨È°πÁõÆVIÁî±ÊñπÂúÜSTUÂàõÂßã‰∫∫[ËÄÅÁΩóÂ∑¥ÊâéÂòø](https://huaban.com/user/syy946795671)Êèê‰æõËÆæËÆ°\n\n## ‰Ωï‰∏∫ÁÅµÊÑü¬†Ôºà‚óêÀç‚óëÔºâ\n\n- **ÁÅµÊÑü**ÊòØÊñáËâ∫„ÄÅÁßëÊäÄÊ¥ªÂä®‰∏≠Áû¨Èó¥‰∫ßÁîüÁöÑÂØåÊúâÂàõÈÄ†ÊÄßÁöÑÁ™ÅÂèëÊÄùÁª¥Áä∂ÊÄÅÔºåÊó†ÊÑèËØÜ‰∏≠Á™ÅÁÑ∂ÂÖ¥Ëµ∑ÁöÑÁ•ûÂ¶ôËÉΩÂäõÔºàÊüêÂ∫¶Ôºâ\n- **ÁÅµÊÑü**ÊòØÊ†πÊçÆËá™Â∑±ÁöÑÁªèÂéÜËÄåËÅîÊÉ≥Âà∞ÁöÑ‰∏ÄÁßçÂàõÈÄ†ÊÄßÊÄùÁª¥Ê¥ªÂä® (Áª¥Âü∫)\n- **ÁÅµÊÑü**ÊòØ**ÂèëÁé∞Êñ∞‰∏ñÁïå**ÔºàÁúÅÊµÅÔºâ\n\n![kokomi_teruhashi](https://raw.githubusercontent.com/zhaoolee/ins/master/media/kokomi_teruhashi.png)\n\n> ÁéãÂãÉÔºö‰ª•Ê≥ïÂ∏àÊô∫ÈÅó‰∫∫ÊàëÔºåËØÜÊ¥ûÂπΩÊòéÔºåÊÄùÂÅáÂ¶ôÂõ†ÔºåÂÜÄÈÄö**ÁÅµÊÑü(ins)**„ÄÇ\n\n## Ëøô‰∏™INSÈ°πÁõÆÊúâ‰ªÄ‰πàÁî®Ôºü(Ôæü‚ñΩÔæü)/\n\nÊûÑÂª∫‰ºòÁßÄ‰∫ßÂìÅÔºåÈúÄË¶ÅÁÅµÊÑüÔºå**Ëâ∫ÊúØÁöÑÁ¨¨‰∏ÄÊ≠•ÈÉΩÊòØÊ®°‰ªø‰∏¥Êëπ, Â§©Êâç‰πüÈöæ‰ª•ÈÄæË∂ä‰ø°ÊÅØÂ∑ÆÁöÑÈ´òÂ¢ôÔºåÁ´ôÂú®Â∑®‰∫∫ÁöÑËÇ©ËÜÄ‰∏ä‰ºö‰∫ãÂçäÂäüÂÄç**ÔºåËøô‰∏™‰ªìÂ∫ìÁöÑÁõÆÊ†áÊòØÊî∂ÈõÜÁÅµÊÑüÔºåËøΩË∏™Â≠òÊ¥ªÁöÑÁÅµÊÑüÔºåÂ§çÊ¥ªÈô®ËêΩÁöÑÁÅµÊÑü„ÄÇ\n\nINSÈ°πÁõÆÁöÑÁÅµÊÑüÊù•Ëá™ [**ÈòÆ‰∏ÄÂ≥∞ÁöÑÁΩëÁªúÊó•Âøó ‰πã ÁßëÊäÄÁà±Â•ΩËÄÖÂë®Âàä**](https://www.ruanyifeng.com/blog/weekly/)ÔºåÂë®ÂàäÊØèÊúüÈÉΩ‰ºöÂ∏¶‰∏Ä‰∫õÁªôËØªËÄÖÂ∏¶Êù•ÁÅµÊÑüÁöÑÈ°πÁõÆÔºå‰ΩÜÊ≤°ÊúâÁªôÂá∫Êñπ‰æøÊü•ÈòÖÁöÑÈ°πÁõÆÂØºËà™ÂàóË°®Ôºå‰ª•ÂèäÊ¥ªË∑ÉÈ°πÁõÆÊåÅÁª≠ËøΩË∏™ÂäüËÉΩÔºå‰∫éÊòØÊú¨INSÈ°πÁõÆËØûÁîü‰∫Ü„ÄÇ\n\n## INSÈ°πÁõÆËøê‰ΩúÊú∫Âà∂ÔºàÔø£Ô∏∂Ôø£Ôºâ‚Üó\n\nzhaooleeÂæÄÈ°πÁõÆÊ†πÁõÆÂΩïÁöÑ [website_info.csv](https://github.com/zhaoolee/ins/blob/main/website_info.csv) Ê∑ªÂä†Êï∞ÊçÆÊ∫ê, Êï∞ÊçÆÊ∫êË¢´Êèê‰∫§Âà∞Github Âêé, Github Action Â∞ÜËøêË°åÁà¨Ëô´, ÂÆûÊó∂Ê£ÄÊµãUrlÁä∂ÊÄÅ, Â¶ÇÊûúÊî∂Âà∞ÂìçÂ∫î, ÂàôNameÂêéËøΩÂä†‰∏Ä‰∏™ÁªøÁÅØüü¢, Âê¶Âàô‰∏∫Á∫¢ÁÅØüî¥ ; Github ActionÊØèÂ§©6ÁÇπÂÆöÊó∂ËøêË°å, Ê£ÄÊµãUrlÁöÑÁä∂ÊÄÅ, ÁªøÁÅØÂêéÈù¢‰ºöÊòæÁ§∫ÂìçÂ∫îÁöÑÊØ´ÁßíÊï∞, ÂÄºË∂äÂ∞è, ËØ¥ÊòéÁΩëÁ´ôÂìçÂ∫îÈÄüÂ∫¶Ë∂äÂø´(ÁªèË¥πÂÖÖË∂≥); ÂìçÂ∫îÈÄüÂ∫¶ÊÖ¢ÁöÑÂ§ßÂ§öÊòØÂÖ¨ÁõäÈ°πÁõÆÔºåÂ¶ÇÊûú‰Ω†ÂæàÂñúÊ¨¢Êüê‰∏™ÂÖ¨ÁõäÈ°πÁõÆÔºåÂèØ‰ª•ËµûÂä©‰∏ÄÊ≥¢ÔºåËÆ©Á´ôÈïøÊèêÂçáÁΩëÁ´ôÊâìÂºÄÈÄüÂ∫¶„ÄÇ\n## INSÈ°πÁõÆÁöÑÂ¢ìÁ¢ëÂ§çÊ¥ªÊú∫Âà∂(‚ïØ-_-)‚ïØ~‚ï©‚ï©\n\nÊòæÁ§∫Á∫¢ÁÅØÁöÑÈ°πÁõÆÔºåÂèØËÉΩÊú¨Ë∫´ÊòØ‰∏™Â•ΩÈ°πÁõÆÔºå‰ΩÜÁî±‰∫éÁßçÁßçÂéüÂõ†Êêû‰∏ç‰∏ãÂéª‰∫ÜÔºåÂ¶ÇÊûú‰Ω†ËÉΩÊêûÂá∫Êõø‰ª£ÂìÅÔºåÊ¨¢ËøéÂú®[issues](https://github.com/zhaoolee/ins/issues)‰∏≠ÁïôË®ÄÔºåÊú¨È°πÁõÆ‰ºöÁßØÊûÅÊî∂ÂΩï„ÄÇ\n\n## ‰∏ãËΩΩ(‚äôÀç‚äô)\n\nÁ∫ØÂáÄÊ†∏ÂøÉcsvÊï∞ÊçÆÂÖ®Èáè‰∏ãËΩΩ: [https://raw.githubusercontent.com/zhaoolee/ins/main/website_info.csv](https://raw.githubusercontent.com/zhaoolee/ins/main/website_info.csv)\n\nÂú®Á∫øCDNÁâàÊú¨: [https://cdn.jsdelivr.net/gh/zhaoolee/ins/website_info.csv](https://cdn.jsdelivr.net/gh/zhaoolee/ins/website_info.csv)\n\nÂ¶ÇÊûú‰Ω†ÊÉ≥Â§çÂàªÊú¨È°πÁõÆ, Âª∫Á´ã**ÁÅµÊÑü‰πãÂ§ñ**ÁöÑ‰∏ªÈ¢ò‰ªìÂ∫ì, Ê¨¢ËøéÂÖãÈöÜÊú¨È°πÁõÆËøõË°åÈ≠îÊîπ„ÄÇ \n\n```shell\ngit clone --depth=1 https://github.com/zhaoolee/ins\n```\n\n## Êé®ËçêÊâìÂºÄÁöÑÊ≠£Á°ÆÂßøÂäø\n\nÂ∞Ü[https://github.com/zhaoolee/ins#%E7%9B%AE%E5%BD%95](https://github.com/zhaoolee/ins#%E7%9B%AE%E5%BD%95) Â≠òÂÖ•‰π¶Á≠æ, ËÆøÈóÆ‰π¶Á≠æÂç≥ÂèØÂø´ÈÄüËÆøÈóÆÁõÆÂΩï\n\nÂ¶ÇÊûúÊüê‰∫õÁΩëÁ´ôÊó†Ê≥ïËÆøÈóÆ, ÂèØ‰ª•ÂèÇËÄÉ[‰∏ÄÊ¨æÂø´Êç∑Á≠æÂà∞È¢ÜÈ≠îÊ≥ï‰∏äÁΩëÂ§©Êï∞ÁöÑÂ∞èÂ∑•ÂÖ∑](https://www.v2fy.com/p/109-glados-2021-06-09/)\n\n\n--insStart----insEnd--\n\n--tagStart----tagEnd--\n\n## Ê¨¢ËøéË¥°ÁåÆ‚ï∞(Ôø£‚ñΩÔø£)‚ï≠\n\nË¥°ÁåÆÊñπÊ≥ï: ForkÊú¨È°πÁõÆ, Âú®È°πÁõÆÊ†πÁõÆÂΩïÁöÑ website_info.csv **Êú´Â∞æ**Ê∑ªÂä†Êï∞ÊçÆ, Êèê‰∫§PrÂç≥ÂèØ!\n\n## ËÅîÁ≥ª‰ΩúËÄÖ (ÔºæÔºçÔºæ)V\n\n<a href=\"mailto:zhaoolee@gmail.com\">zhaoolee@gmail.com</a>\n\nÂ¶ÇÊûú‰Ω†Êúâ**È•±Âê´ÁÅµÊÑüÁöÑÊÉ≥Ê≥ï**, ‰πüÂèØ‰ª•Âú®[issues](https://github.com/zhaoolee/ins/issues) ‰∏≠ÁïôË®Ä", "metadata": {"source": "github.com/zhaoolee/ins", "file_path": "EditREADME.md", "language": "markdown", "complexity": "simple", "star_count": 2240, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 70}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pandas as pd\nimport os\nimport re\nimport datetime\nimport time\nimport pytz\nimport requests\nimport ssl\nimport urllib.parse\nimport OpenSSL\nfrom dateutil import parser\n\ndef get_host_info(url):\n    parsed_url = urllib.parse.urlparse(url)\n    host = parsed_url.netloc\n    return host\n\ndef get_certificate_expiration_date(host):\n    result = ''\n    hostname = host\n    port = 443\n    cert = ssl.get_server_certificate((hostname, port)).encode()\n    if(cert):\n        cert_obj = OpenSSL.crypto.load_certificate(OpenSSL.crypto.FILETYPE_PEM, cert)\n        cert_expire_time = parser.parse(cert_obj.get_notAfter().decode(\"UTF-8\")).strftime('%Y-%m-%d %H:%M:%S')\n        if(cert_obj.has_expired()):\n            result = ''\n        else:\n            current_date = datetime.datetime.now()\n            remaining_days = (datetime.datetime.strptime(cert_expire_time, \"%Y-%m-%d %H:%M:%S\") - current_date).days\n            yymmdd_expiration_date = str(cert_expire_time)[0:10]\n            result = str(yymmdd_expiration_date)+\"(Ââ©\"+str(remaining_days)+ \"Â§©Âà∞Êúü)\"\n    else:\n        result = ''\n    return result\n\n\n\ndef get_all_tag(website_info_data):\n    all_tag = []\n    all_tag_info_data = []\n    # ÈÅçÂéÜÊï∞ÊçÆ,Ëé∑ÂèñÊâÄÊúâÁöÑtag\n    for website_info_index, website_info_row in website_info_data.iterrows():\n        tag_list = website_info_row[\"Tag\"].split(\";\")\n        pure_tag_list = []\n        for tag in tag_list:\n            pure_tag = tag.strip()\n            if pure_tag != \"\":\n                pure_tag_list.append(pure_tag)\n                if pure_tag not in all_tag:\n                    all_tag.append(pure_tag)\n                    all_tag_info_data.append([])\n        print(\"pure_tag_list\", pure_tag_list)\n        print(\n            \"tag==>>\",\n            website_info_index,\n            website_info_row[\"Tag\"],\n            \"pure_tag_list==>>\",\n            pure_tag_list,\n        )\n\n    # ÈÅçÂéÜÊâÄÊúâÊï∞ÊçÆ,Â∞ÜÊï∞ÊçÆÊîæÂà∞all_tag_info_data ‰∏≠\n    for website_info_index, website_info_row in website_info_data.iterrows():\n        tag_list = website_info_row[\"Tag\"].split(\";\")\n        for tag in tag_list:\n            pure_tag = tag.strip()\n            if pure_tag != \"\":\n                all_tag_info_data[all_tag.index(pure_tag)].append(website_info_row)\n\n    print(\"all_tag\", all_tag, \"all_tag_info_data\", all_tag_info_data)\n    return {\"all_tag\": all_tag, \"all_tag_info_data\": all_tag_info_data}\n\n\ndef short_url(url):\n    result = \"\"\n    if(url.startswith(\"http://\")):\n        url = url[7:]\n    if(url.startswith(\"https://\")):\n        url = url[8:]\n    if(url.startswith(\"www.\")):\n        url = url[4:]\n    if(url.endswith(\"/\")):\n        url = url[:-1]\n\n    if len(url) > 30:\n        result = url[0:30] + \"...\"\n    else:\n        result = url\n    return result\n\n\ndef replaceTemplate(template, reInfo, data):\n\n    reResult = re.findall(reInfo, template)\n    new_read_me = template.replace(reResult[0], data)\n    return new_read_me\n\n\ndef create_tag_table_html(tag_name, tag_info_data):\n    print(\"==create_tag_table_html\", tag_name)\n    website_info_html =  \"<a href='#ÁõÆÂΩï'>üîôÁõÆÂΩï</a>\" + \"\\n\" + \"<table>\"\n    website_info_html = (\n        website_info_html\n        + \"<tr>\"\n        + \"<td width='400'>\"\n        + \"<span>(„Å•ÔΩ°‚óï‚Äø‚Äø‚óïÔΩ°)„Å•</span><br/><span>Name</span>\"\n        + \"</td>\"\n        + \"<td>\"\n        + \"<span> (‚óèÔæüœâÔæü‚óè)</span><br/><span>Description</span>\"\n        + \"</td>\"\n        + \"</tr>\"\n    )\n    for info_data in tag_info_data:\n        print(\n            \"==>>\",\n            {\n                \"Name\": info_data[\"Name\"],\n                \"Url\": info_data[\"Url\"],\n                \"Description\": info_data[\"Description\"],\n            },\n        )\n        website_info_html = (\n            website_info_html\n            + \"<tr>\"\n            + \"<td>\"\n            + info_data[\"Name\"]\n            + \"</td>\"\n            + \"<td>\"\n            + info_data[\"Description\"]\n            + \"</td>\"\n            + \"</tr>\"\n        )\n\n    website_info_html = website_info_html + \"</table>\" + \"\\n\" + \"<a href='#ÁõÆÂΩï'>üîôÁõÆÂΩï</a>\" + \"\\n\"\n    return website_info_html\n\n\ndef main():\n    print(\"ÂΩìÂâçÊúçÂä°Âô®Êó∂Èó¥\", datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n    # ËØªÂèñcsvÊñá‰ª∂\n    website_info_data = pd.read_csv(\"./website_info.csv\")\n    # ÂèçËΩ¨Êï∞ÊçÆ,‰øùËØÅÊúÄÊñ∞ÁöÑÊï∞ÊçÆÂú®ÊúÄÂâçÈù¢\n    website_info_data = website_info_data.reindex(index=website_info_data.index[::-1])\n    print(website_info_data)\n    # ÈÅçÂéÜÊï∞ÊçÆ\n    for website_info_index, website_info_row in website_info_data.iterrows():\n        print(\"=start=>>\", website_info_index, website_info_row[\"Url\"])\n        # Ê£ÄÊµãÁΩëÁ´ôÂèØÁî®ÊÄß,ËÆ∞ÂΩïËØ∑Ê±ÇÊó∂Èó¥,ÂÆåÊàêÊï∞ÊçÆÊãºÊé•\n        try:\n            headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n            # Ê£ÄÊµãÁΩëÁ´ôÊòØÂê¶Ê≠£Â∏∏\n            website_info_row_url_result = requests.get(\n                website_info_row[\"Url\"], timeout=5, headers=headers\n            )\n            \n            expiration_date=''\n            try:\n                tmp_host = get_host_info(website_info_row[\"Url\"])\n                expiration_date_result = get_certificate_expiration_date(tmp_host)\n                if(len(expiration_date_result) > 0):\n                    expiration_date = expiration_date_result\n            except Exception as e:\n                expiration_date = ''\n            print(\"!!expiration_date\", expiration_date)\n            total_ms = str(\n                int(website_info_row_url_result.elapsed.total_seconds() * 1000)\n            )\n            # ÂìçÂ∫îÁ†Å‰∏∫2ÂºÄÂ§¥,Ê†áÊ≥®ÁªøËâ≤,Âê¶ÂàôÊ†áÊ≥®Á∫¢Ëâ≤\n            if website_info_row_url_result.status_code:\n                website_info_row[\"Name\"] = (\n                    \"<span style='font-weight: 600'>\"\n                    + website_info_row[\"Name\"]\n                    + \"</span>\"\n                    + \"<span>\"\n                    + (\n                        \" üü¢ \" + total_ms + \"ms\"\n                        if (str(website_info_row_url_result.status_code).startswith(\"1\") or str(website_info_row_url_result.status_code).startswith(\"2\") or str(website_info_row_url_result.status_code).startswith(\"3\") or str(website_info_row_url_result.status_code).startswith(\"4\"))\n                        else \" üî¥\"\n                    )\n                    + \"</span><br/>\"\n\n                )\n        # Êó†Ê≥ïÂìçÂ∫îÔºåÊ†áÊ≥®Á∫¢Ëâ≤\n        except Exception as e:\n            print(\"error==\", e)\n            website_info_row[\"Name\"] = (\n                \"<span style='font-weight: 600'>\"\n                + website_info_row[\"Name\"]\n                + \" üî¥\"\n                + \"</span><br/>\"\n            )\n        finally:\n            website_info_row[\"Name\"] = (\n                \"<span>\"\n                + website_info_row[\"Name\"]\n                + \"</span>\"\n                + \"<a href='\"\n                + website_info_row[\"Url\"]\n                + \"'>\"\n                + (short_url(website_info_row[\"Url\"]))\n                + \"</a>\"\n                + (\n                    \"<br/><span>SSLËØÅ‰π¶Âà∞ÊúüÊó∂Èó¥:\" + expiration_date + \"</span>\"\n                    if (expiration_date and (\"üü¢\" in website_info_row[\"Name\"])) else \"\" \n                )\n            )\n            print(\"finish\", website_info_row[\"Url\"], website_info_row[\"Name\"])\n    # ÂÆåÊàêtableÊï∞ÊçÆÊãºÊé•\n    website_info_html = \"<table>\"\n    website_info_html = (\n        website_info_html\n        + \"<tr>\"\n        + \"<td width='400'>\"\n        + \"<span>(„Å•ÔΩ°‚óï‚Äø‚Äø‚óïÔΩ°)„Å•</span><br/><span>Name</span>\"\n        + \"</td>\"\n        + \"<td>\"\n        + \"<span> (‚óèÔæüœâÔæü‚óè)</span><br/><span>Description</span>\"\n        + \"</td>\"\n        + \"<td width='300'>\"\n        + \"<span> Ô∏ø(Ôø£Ô∏∂Ôø£)Ô∏ø</span><br/><span>Tag</span>\"\n        + \"</td>\"\n        + \"</tr>\"\n    )\n    for website_info_index, website_info_row in website_info_data.iterrows():\n        website_info_html = (\n            website_info_html\n            + \"<tr>\"\n            + \"<td>\"\n            + website_info_row[\"Name\"]\n            + \"</td>\"\n            + \"<td>\"\n            + website_info_row[\"Description\"]\n            + \"</td>\"\n            + \"<td>\"\n            + website_info_row[\"Tag\"]\n            + \"</td>\"\n            + \"</tr>\"\n        )\n    website_info_html = website_info_html + \"</table>\"\n    # Ê†πÊçÆEditREADME.mdÊ®°Êùø,ÊõøÊç¢Âç†‰ΩçÁ¨¶, ÁîüÊàêÊúÄÁªàÊï∞ÊçÆ\n    readme_md = \"\"\n    with open(os.path.join(os.getcwd(), \"EditREADME.md\"), \"r\") as load_f:\n        readme_md = load_f.read()\n    mail_re = r\"--insStart----insEnd--\"\n    in_datetime = datetime.datetime.fromtimestamp(\n        int(time.time()), pytz.timezone(\"Asia/Shanghai\")\n    ).strftime(\"%Y-%m-%d %H:%M:%S\")\n    all_info_content = (\n        \"\\n\\n\"\n        + \"## ÂºÄÊ∫êÁÅµÊÑüÂ∫ìÂ∑≤Êî∂ÂΩï\"\n        + str(len(website_info_data))\n        + \"ÊùüÁÅµÊÑüINS!\"\n        + \"(ÔΩûÔø£‚ñΩÔø£)ÔΩûÊõ¥Êñ∞Êó∂Èó¥(\"\n        + in_datetime\n        + \")\\n\\n\"\n        + website_info_html\n        + \"\\n\\n\"\n    )\n    new_read_me = replaceTemplate(readme_md, mail_re, all_info_content)\n    print(\"new_read_me\", new_read_me)\n\n    # ÁîüÊàêÁ±ªÂà´Êï∞ÊçÆ\n    tag_re = r\"--tagStart----tagEnd--\"\n    all_tag_result = get_all_tag(website_info_data)\n    all_tag = all_tag_result[\"all_tag\"]\n    all_tag_info_data = all_tag_result[\"all_tag_info_data\"]\n    print(\"==all_tag_info_data==\", all_tag_info_data)\n    print(\"==all_tag==\", all_tag)\n    all_tag_content = \"\"\n\n    for tag_content in all_tag:\n        tag_html = create_tag_table_html(\n            tag_content, all_tag_info_data[all_tag.index(tag_content)]\n        )\n        tag_whole_content = \"## \" + tag_content + \"\\n\\n\" + tag_html + \"\\n\\n\"\n        all_tag_content = all_tag_content + tag_whole_content\n\n    new_read_me = replaceTemplate(new_read_me, tag_re, all_tag_content)\n\n    # Ê∑ªÂä†Á¥¢ÂºïÈîöÁÇπ\n    tag_index_info = \"\"\n    for tag_index, tag_content in enumerate(all_tag):\n        if tag_index != (len(all_tag) - 1):\n            tag_index_info = (\n                tag_index_info\n                + \"<a href='#\"\n                + tag_content\n                + \"'>\"\n                + tag_content + \"(\" + str(len(all_tag_info_data[all_tag.index(tag_content)])) + \")\"\n                + \"</a>\"\n                + \", \"\n            )\n        else:\n            tag_index_info = (\n                tag_index_info\n                + \"<a href='#\"\n                + tag_content\n                + \"'>\"\n                + tag_content + \"(\" + str(len(all_tag_info_data[all_tag.index(tag_content)])) + \")\"\n                + \"</a>\"\n            )\n\n    tag_index_re = r\"--tagIndexInfoStart----tagIndexInfoEnd--\"\n\n    new_read_me = replaceTemplate(new_read_me, tag_index_re, tag_index_info)\n\n    # Â∞ÜÁîüÊàêÁöÑÊï∞ÊçÆÂÜôÂÖ•README.md\n    with open(os.path.join(os.getcwd(), \"README.md\"), \"w\") as load_f:\n        load_f.write(new_read_me)\n\nmain()\n", "metadata": {"source": "github.com/zhaoolee/ins", "file_path": "main.py", "language": "python", "complexity": "complex", "star_count": 2240, "quality_score": 0.75, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 312}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Issue report\nabout: Create a report to help us improve\ntitle: ''\nlabels: ''\nassignees: ''\n\n---\n\n## Description of the problem, including code/CLI snippet\n\n\n## Expected Behavior\n\n\n## Actual Behavior\n\n\n## Specifications\n\n  - python-gitlab version:\n  - Gitlab server version (or gitlab.com):\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".github/ISSUE_TEMPLATE/issue_template.md", "language": "markdown", "complexity": "simple", "star_count": 2375, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!-- Please make sure your commit messages follow Conventional Commits\n(https://www.conventionalcommits.org) with a commit type (e.g. feat, fix, refactor, chore):\n\nBad:        Added support for release links\nGood:     feat(api): add support for release links\n\nBad:        Update documentation for projects\nGood:     docs(projects): update example for saving project attributes-->\n\n## Changes\n\n<!-- Remove this comment and describe your changes here. -->\n\n### Documentation and testing\n\nPlease consider whether this PR needs documentation and tests. **This is not required**, but highly appreciated:\n\n- [ ] Documentation in the matching [docs section](https://github.com/python-gitlab/python-gitlab/tree/main/docs)\n- [ ] [Unit tests](https://github.com/python-gitlab/python-gitlab/tree/main/tests/unit) and/or [functional tests](https://github.com/python-gitlab/python-gitlab/tree/main/tests/functional)\n\n<!--\nNote: In some cases, basic functional tests may be easier to add, as they do not require adding mocked GitLab responses.\n-->\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".github/pull_request_template.md", "language": "markdown", "complexity": "simple", "star_count": 2375, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 24}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Docs\n\n# If a pull-request is pushed then cancel all previously running jobs related\n# to that pull-request\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.sha }}\n  cancel-in-progress: true \n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n      - master\n\nenv:\n  PY_COLORS: 1\n\njobs:\n  sphinx:\n    runs-on: ubuntu-24.04\n    steps:\n      - uses: actions/checkout@v4.2.2\n      - name: Set up Python\n        uses: actions/setup-python@v5.6.0\n        with:\n          python-version: \"3.13\"\n      - name: Install dependencies\n        run: pip install tox\n      - name: Build docs\n        env:\n          TOXENV: docs\n        run: tox\n\n  twine-check:\n    runs-on: ubuntu-24.04\n    steps:\n      - uses: actions/checkout@v4.2.2\n      - name: Set up Python\n        uses: actions/setup-python@v5.6.0\n        with:\n          python-version: \"3.13\"\n      - name: Install dependencies\n        run: pip install tox twine wheel\n      - name: Check twine readme rendering\n        env:\n          TOXENV: twine-check\n        run: tox\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".github/workflows/docs.yml", "language": "yaml", "complexity": "simple", "star_count": 2375, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 51}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Lint\n\n# If a pull-request is pushed then cancel all previously running jobs related\n# to that pull-request\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.sha }}\n  cancel-in-progress: true \n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n      - master\n\nenv:\n  PY_COLORS: 1\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4.2.2\n        with:\n          fetch-depth: 0\n      - uses: actions/setup-python@v5.6.0\n        with:\n          python-version: \"3.13\"\n      - run: pip install --upgrade tox\n      - name: Run commitizen (https://commitizen-tools.github.io/commitizen/)\n        run: tox -e cz\n      - name: Run black code formatter (https://black.readthedocs.io/en/stable/)\n        run: tox -e black -- --check\n      - name: Run flake8 (https://flake8.pycqa.org/en/latest/)\n        run: tox -e flake8\n      - name: Run mypy static typing checker (http://mypy-lang.org/)\n        run: tox -e mypy\n      - name: Run isort import order checker (https://pycqa.github.io/isort/)\n        run: tox -e isort -- --check\n      - name: Run pylint Python code static checker (https://github.com/PyCQA/pylint)\n        run: tox -e pylint\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".github/workflows/lint.yml", "language": "yaml", "complexity": "simple", "star_count": 2375, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 44}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: 'Lock Closed Issues'\n\non:\n  schedule:\n    - cron: '0 0 * * 1 '\n  workflow_dispatch: # For manual cleanup\n\npermissions:\n  issues: write\n\nconcurrency:\n  group: lock\n\njobs:\n  action:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: dessant/lock-threads@v5.0.1\n        with:\n          process-only: 'issues'\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".github/workflows/lock.yml", "language": "yaml", "complexity": "simple", "star_count": 2375, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: pre_commit\n\n# If a pull-request is pushed then cancel all previously running jobs related\n# to that pull-request\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.sha }}\n  cancel-in-progress: true \n\non:\n  push:\n    branches:\n      - main\n    paths:\n      .github/workflows/pre_commit.yml\n      .pre-commit-config.yaml\n  pull_request:\n    branches:\n      - main\n      - master\n    paths:\n      - .github/workflows/pre_commit.yml\n      - .pre-commit-config.yaml\n\nenv:\n  PY_COLORS: 1\n\njobs:\n\n  pre_commit:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4.2.2\n      - uses: actions/setup-python@v5.6.0\n        with:\n          python-version: \"3.13\"\n      - name: install tox\n        run: pip install tox==3.26.0\n      - name: pre-commit\n        run: tox -e pre-commit\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".github/workflows/pre_commit.yml", "language": "yaml", "complexity": "simple", "star_count": 2375, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 40}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Release\n\non:\n  schedule:\n    - cron: '0 0 28 * *'  # Monthly auto-release\n  workflow_dispatch:      # Manual trigger for quick fixes\n\njobs:\n  release:\n    if: github.repository == 'python-gitlab/python-gitlab'\n    runs-on: ubuntu-latest\n    concurrency: release\n    permissions:\n      id-token: write\n    environment: pypi.org\n    steps:\n    - uses: actions/checkout@v4.2.2\n      with:\n        fetch-depth: 0\n        token: ${{ secrets.RELEASE_GITHUB_TOKEN }}\n\n    - name: Python Semantic Release\n      id: release\n      uses: python-semantic-release/python-semantic-release@v10.2.0\n      with:\n        github_token: ${{ secrets.RELEASE_GITHUB_TOKEN }}\n\n    - name: Publish package distributions to PyPI\n      uses: pypa/gh-action-pypi-publish@release/v1\n      # NOTE: DO NOT wrap the conditional in ${{ }} as it will always evaluate to true.\n      # See https://github.com/actions/runner/issues/1173\n      if: steps.release.outputs.released == 'true'\n\n    - name: Publish package distributions to GitHub Releases\n      uses: python-semantic-release/publish-action@v10.2.0\n      if: steps.release.outputs.released == 'true'\n      with:\n        github_token: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".github/workflows/release.yml", "language": "yaml", "complexity": "medium", "star_count": 2375, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 39}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: 'Rerun failed workflows'\n\non:\n  issue_comment:\n    types: [created]\n\njobs:\n  rerun_pr_tests:\n    name: rerun_pr_tests\n    if: ${{ github.event.issue.pull_request }}\n    runs-on: ubuntu-24.04\n    steps:\n    - uses: estroz/rerun-actions@main\n      with:\n        repo_token: ${{ secrets.GITHUB_TOKEN }}\n        comment_id: ${{ github.event.comment.id }}\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".github/workflows/rerun-test.yml", "language": "yaml", "complexity": "simple", "star_count": 2375, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# https://github.com/actions/stale\nname: 'Close stale issues and PRs'\non:\n  schedule:\n    - cron: '30 1 * * *'\n\npermissions:\n  issues: write\n  pull-requests: write\n\nconcurrency:\n  group: lock\n\njobs:\n  stale:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/stale@v9.1.0\n        with:\n          operations-per-run: 500\n          stale-issue-label: \"stale\"\n          stale-pr-label: \"stale\"\n\n          # If an issue/PR has an assignee it won't be marked as stale\n          exempt-all-assignees: true\n          stale-issue-message: |\n            This issue was marked stale because it has been open 60 days with\n            no activity. Please remove the stale label or comment on this\n            issue. Otherwise, it will be closed in 15 days.\n\n            As an open-source project, we rely on community contributions to\n            address many of the reported issues. Without a proposed fix or\n            active work towards a solution it is our policy to close inactive\n            issues. This is documented in CONTRIBUTING.rst\n\n            **How to keep this issue open:**\n            * If you are still experiencing this issue and are willing to\n            investigate a fix, please comment and let us know.\n            * If you (or someone else) can propose a pull request with a\n            solution, that would be fantastic.\n            * Any significant update or active discussion indicating progress\n            will also prevent closure.\n\n            We value your input. If you can help provide a fix, we'd be happy\n            to keep this issue open and support your efforts.\n\n            This is documented in CONTRIBUTING.rst\n            https://github.com/python-gitlab/python-gitlab/blob/main/CONTRIBUTING.rst\n\n          days-before-issue-stale: 60\n          days-before-issue-close: 15\n          close-issue-message: |\n            This issue was closed because it has been marked stale for 15 days\n            with no activity.\n\n            This open-source project relies on community contributions, and\n            while we value all feedback, we have a limited capacity to address\n            every issue without a clear path forward.\n\n            Currently, this issue hasn't received a proposed fix, and there\n            hasn't been recent active discussion indicating someone is planning\n            to work on it. To maintain a manageable backlog and focus our\n            efforts, we will be closing this issue for now.\n\n            **This doesn't mean the issue isn't valid or important.** If you or\n            anyone else in the community is willing to investigate and propose\n            a solution (e.g., by submitting a pull request), please do.\n\n            We believe that those who feel a bug is important enough to fix\n            should ideally be part of the solution. Your contributions are\n            highly welcome.\n\n            Thank you for your understanding and potential future\n            contributions.\n\n            This is documented in CONTRIBUTING.rst\n            https://github.com/python-gitlab/python-gitlab/blob/main/CONTRIBUTING.rst\n\n          stale-pr-message: >\n            This Pull Request (PR) was marked stale because it has been open 90 days\n            with no activity.  Please remove the stale label or comment on this PR.\n            Otherwise, it will be closed in 15 days.\n          days-before-pr-stale: 90\n          days-before-pr-close: 15\n          close-pr-message: >\n            This PR was closed because it has been marked stale for 15 days with no\n            activity. If this PR is still valid, please re-open.\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".github/workflows/stale.yml", "language": "yaml", "complexity": "medium", "star_count": 2375, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 88}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Test\n\n# If a pull-request is pushed then cancel all previously running jobs related\n# to that pull-request\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.sha }}\n  cancel-in-progress: true \n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n      - master\n\nenv:\n  PY_COLORS: 1\n\njobs:\n  unit:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]\n        python:\n          - version: \"3.9\"\n            toxenv: py39,smoke\n          - version: \"3.10\"\n            toxenv: py310,smoke\n          - version: \"3.11\"\n            toxenv: py311,smoke\n          - version: \"3.12\"\n            toxenv: py312,smoke\n          - version: \"3.13\"\n            toxenv: py313,smoke\n          - version: \"3.14.0-alpha - 3.14\" # SemVer's version range syntax\n            toxenv: py314,smoke\n        include:\n          - os: macos-latest\n            python:\n              version: \"3.13\"\n              toxenv: py313,smoke\n          - os: windows-latest\n            python:\n              version: \"3.13\"\n              toxenv: py313,smoke\n    steps:\n      - uses: actions/checkout@v4.2.2\n      - name: Set up Python ${{ matrix.python.version }}\n        uses: actions/setup-python@v5.6.0\n        with:\n          python-version: ${{ matrix.python.version }}\n      - name: Install dependencies\n        run: pip install tox\n      - name: Run tests\n        env:\n          TOXENV: ${{ matrix.python.toxenv }}\n        run: tox --skip-missing-interpreters false\n\n  functional:\n    timeout-minutes: 30\n    runs-on: ubuntu-24.04\n    strategy:\n      matrix:\n        toxenv: [api_func_v4, cli_func_v4]\n    steps:\n      - uses: actions/checkout@v4.2.2\n      - name: Set up Python\n        uses: actions/setup-python@v5.6.0\n        with:\n          python-version: \"3.13\"\n      - name: Install dependencies\n        run: pip install tox\n      - name: Run tests\n        env:\n          TOXENV: ${{ matrix.toxenv }}\n        run: tox -- --override-ini='log_cli=True'\n      - name: Upload codecov coverage\n        uses: codecov/codecov-action@v5.4.3\n        with:\n          files: ./coverage.xml\n          flags: ${{ matrix.toxenv }}\n          fail_ci_if_error: false\n          token: ${{ secrets.CODECOV_TOKEN }}\n\n  coverage:\n    runs-on: ubuntu-24.04\n    steps:\n      - uses: actions/checkout@v4.2.2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v5.6.0\n        with:\n          python-version: \"3.13\"\n      - name: Install dependencies\n        run: pip install tox\n      - name: Run tests\n        env:\n          PY_COLORS: 1\n          TOXENV: cover\n        run: tox\n      - name: Upload codecov coverage\n        uses: codecov/codecov-action@v5.4.3\n        with:\n          files: ./coverage.xml\n          flags: unit\n          fail_ci_if_error: false\n          token: ${{ secrets.CODECOV_TOKEN }}\n\n  dist:\n    runs-on: ubuntu-latest\n    name: Python wheel\n    steps:\n    - uses: actions/checkout@v4.2.2\n    - uses: actions/setup-python@v5.6.0\n      with:\n        python-version: \"3.13\"\n    - name: Install dependencies\n      run: |\n        pip install -r requirements-test.txt\n    - name: Build package\n      run: python -m build -o dist/\n    - uses: actions/upload-artifact@v4.6.2\n      with:\n        name: dist\n        path: dist\n\n  test:\n    runs-on: ubuntu-latest\n    needs: [dist]\n    steps:\n    - uses: actions/checkout@v4.2.2\n    - name: Set up Python\n      uses: actions/setup-python@v5.6.0\n      with:\n        python-version: '3.13'\n    - uses: actions/download-artifact@v4.3.0\n      with:\n        name: dist\n        path: dist\n    - name: install dist/*.whl and requirements\n      run: pip install dist/*.whl -r requirements-test.txt tox\n    - name: Run tests\n      run: tox -e install\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".github/workflows/test.yml", "language": "yaml", "complexity": "complex", "star_count": 2375, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 147}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "image: python:3.13\n\nstages:\n  - build\n  - deploy\n  - promote\n\nbuild-images:\n  stage: build\n  image:\n    name: gcr.io/kaniko-project/executor:debug\n    entrypoint: [\"\"]\n  script:\n    - executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $CI_REGISTRY_IMAGE/$OS_ARCH:$CI_COMMIT_TAG-alpine\n    - executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $CI_REGISTRY_IMAGE/$OS_ARCH:$CI_COMMIT_TAG-slim-bullseye --build-arg PYTHON_FLAVOR=slim-bullseye\n  rules:\n    - if: $CI_COMMIT_TAG\n  tags:\n    - $RUNNER_TAG\n  parallel:\n    matrix:\n      # See tags in https://docs.gitlab.com/ee/ci/runners/hosted_runners/linux.html\n      - RUNNER_TAG: saas-linux-medium-amd64\n        OS_ARCH: linux/amd64\n      - RUNNER_TAG: saas-linux-medium-arm64\n        OS_ARCH: linux/arm64\n\ndeploy-images:\n  stage: deploy\n  image: \n    name: mplatform/manifest-tool:alpine-v2.0.4@sha256:38b399ff66f9df247af59facceb7b60e2cd01c2d649aae318da7587efb4bbf87\n    entrypoint: [\"\"]\n  script:\n    - manifest-tool --username $CI_REGISTRY_USER --password $CI_REGISTRY_PASSWORD push from-args\n      --platforms linux/amd64,linux/arm64\n      --template $CI_REGISTRY_IMAGE/OS/ARCH:$CI_COMMIT_TAG-alpine\n      --target $CI_REGISTRY_IMAGE:$CI_COMMIT_TAG-alpine\n    - manifest-tool --username $CI_REGISTRY_USER --password $CI_REGISTRY_PASSWORD push from-args\n      --platforms linux/amd64,linux/arm64\n      --template $CI_REGISTRY_IMAGE/OS/ARCH:$CI_COMMIT_TAG-slim-bullseye\n      --target $CI_REGISTRY_IMAGE:$CI_COMMIT_TAG-slim-bullseye\n  rules:\n    - if: $CI_COMMIT_TAG\n\ntag-latest:\n  stage: promote\n  image:\n    name: gcr.io/go-containerregistry/crane:debug\n    entrypoint: [\"\"]\n  script:\n    - crane auth login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n    - crane tag $CI_REGISTRY_IMAGE:$CI_COMMIT_TAG-alpine ${CI_COMMIT_TAG}       # /python-gitlab:v1.2.3\n    - crane tag $CI_REGISTRY_IMAGE:$CI_COMMIT_TAG-alpine latest                 # /python-gitlab:latest\n    - crane tag $CI_REGISTRY_IMAGE:$CI_COMMIT_TAG-alpine alpine                 # /python-gitlab:alpine\n    - crane tag $CI_REGISTRY_IMAGE:$CI_COMMIT_TAG-slim-bullseye slim-bullseye   # /python-gitlab:slim-bullseye\n  rules:\n    - if: $CI_COMMIT_TAG\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".gitlab-ci.yml", "language": "yaml", "complexity": "medium", "star_count": 2375, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 58}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "default_language_version:\n    python: python3\n\nrepos:\n  - repo: https://github.com/psf/black\n    rev: 25.1.0\n    hooks:\n      - id: black\n  - repo: https://github.com/commitizen-tools/commitizen\n    rev: v4.8.3\n    hooks:\n      - id: commitizen\n        stages: [commit-msg]\n  - repo: https://github.com/pycqa/flake8\n    rev: 7.3.0\n    hooks:\n      - id: flake8\n  - repo: https://github.com/pycqa/isort\n    rev: 6.0.1\n    hooks:\n      - id: isort\n  - repo: https://github.com/pycqa/pylint\n    rev: v3.3.7\n    hooks:\n      - id: pylint\n        additional_dependencies:\n          - argcomplete==2.0.0\n          - gql==3.5.0\n          - httpx==0.27.2\n          - pytest==7.4.2\n          - requests==2.28.1\n          - requests-toolbelt==1.0.0\n        files: 'gitlab/'\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.17.0\n    hooks:\n      - id: mypy\n        args: []\n        additional_dependencies:\n          - gql==3.5.0\n          - httpx==0.27.2\n          - jinja2==3.1.2\n          - pytest==7.4.2\n          - responses==0.23.3\n          - types-PyYAML==6.0.12\n          - types-requests==2.28.11.2\n  - repo: https://github.com/pre-commit/pygrep-hooks\n    rev: v1.10.0\n    hooks:\n      - id: rst-backticks\n      - id: rst-directive-colons\n      - id: rst-inline-touching-normal\n  - repo: https://github.com/maxbrunet/pre-commit-renovate\n    rev: 41.42.1\n    hooks:\n      - id: renovate-config-validator\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".pre-commit-config.yaml", "language": "yaml", "complexity": "simple", "star_count": 2375, "quality_score": 0.63, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 57}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: 2\n\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.11\"\n\nsphinx:\n  configuration: docs/conf.py\n\nformats:\n  - pdf\n  - epub\n\npython:\n  install:\n    - requirements: requirements-docs.txt\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".readthedocs.yml", "language": "yaml", "complexity": "simple", "star_count": 2375, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{\n  \"extends\": [\n    \"config:base\",\n    \":enablePreCommit\",\n    \"group:allNonMajor\",\n    \"schedule:weekly\"\n  ],\n  \"ignorePaths\": [\n    \"**/.venv/**\",\n    \"**/node_modules/**\"\n  ],\n  \"pip_requirements\": {\n    \"fileMatch\": [\"^requirements(-[\\\\w]*)?\\\\.txt$\"]\n  },\n  \"regexManagers\": [\n    {\n      \"fileMatch\": [\n        \"(^|/)tests\\\\/functional\\\\/fixtures\\\\/\\\\.env$\"\n      ],\n      \"matchStrings\": [\n        \"GITLAB_TAG=(?<currentValue>.*?)\\n\"\n      ],\n      \"depNameTemplate\": \"gitlab/gitlab-ee\",\n      \"datasourceTemplate\": \"docker\",\n      \"versioningTemplate\": \"loose\"\n    },\n    {\n      \"fileMatch\": [\n        \"(^|/)tests\\\\/functional\\\\/fixtures\\\\/\\\\.env$\"\n      ],\n      \"matchStrings\": [\n        \"GITLAB_RUNNER_TAG=(?<currentValue>.*?)\\n\"\n      ],\n      \"depNameTemplate\": \"gitlab/gitlab-runner\",\n      \"datasourceTemplate\": \"docker\",\n      \"versioningTemplate\": \"loose\"\n    }\n  ],\n  \"packageRules\": [\n    {\n      \"depTypeList\": [\n        \"action\"\n      ],\n      \"extractVersion\": \"^(?<version>v\\\\d+\\\\.\\\\d+\\\\.\\\\d+)$\",\n      \"versioning\": \"regex:^v(?<major>\\\\d+)(\\\\.(?<minor>\\\\d+)\\\\.(?<patch>\\\\d+))?$\"\n    },\n    {\n      \"packageName\": \"argcomplete\",\n      \"enabled\": false\n    },\n    {\n      \"packagePatterns\": [\n        \"^gitlab\\/gitlab-.+$\"\n      ],\n      \"automerge\": true,\n      \"groupName\": \"GitLab\"\n    },\n    {\n      \"matchPackageNames\": [\n        \"pre-commit/mirrors-mypy\"\n      ],\n      \"matchManagers\": [\n        \"pre-commit\"\n      ],\n      \"versioning\": \"pep440\"\n    }\n  ]\n}\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": ".renovaterc.json", "language": "json", "complexity": "simple", "star_count": 2375, "quality_score": 0.63, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 69}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, caste, color, religion, or sexual\nidentity and orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the overall\n  community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or advances of\n  any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email address,\n  without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement by using GitHub's\n[Report Content](https://docs.github.com/en/communities/maintaining-your-safety-on-github/reporting-abuse-or-spam)\nfunctionality or contacting the currently active maintainers listed in\n[AUTHORS](https://github.com/python-gitlab/python-gitlab/blob/main/AUTHORS).\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series of\nactions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or permanent\nban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within the\ncommunity.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.1, available at\n[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].\n\nCommunity Impact Guidelines were inspired by\n[Mozilla's code of conduct enforcement ladder][Mozilla CoC].\n\nFor answers to common questions about this code of conduct, see the FAQ at\n[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at\n[https://www.contributor-covenant.org/translations][translations].\n\n[homepage]: https://www.contributor-covenant.org\n[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html\n[Mozilla CoC]: https://github.com/mozilla/diversity\n[FAQ]: https://www.contributor-covenant.org/faq\n[translations]: https://www.contributor-covenant.org/translations\n", "metadata": {"source": "github.com/python-gitlab/python-gitlab", "file_path": "CODE_OF_CONDUCT.md", "language": "markdown", "complexity": "medium", "star_count": 2375, "quality_score": 0.63, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 135}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: 2\nupdates:\n- package-ecosystem: pip\n  directory: \"/{{cookiecutter.project_name}}\"\n  schedule:\n    interval: daily\n    time: \"02:00\"\n  open-pull-requests-limit: 10\n  ignore:\n    - dependency-name: \"django\"\n      # Ignore non-LTS Django releases\n      # https://www.djangoproject.com/download/#supported-versions\n      versions:\n        - \">=4.3,<5.2\"\n\n- package-ecosystem: pip\n  directory: \"/{{cookiecutter.project_name}}/docker/django\"\n  schedule:\n    interval: daily\n    time: \"02:00\"\n  open-pull-requests-limit: 10\n\n- package-ecosystem: pip\n  directory: \"/\"\n  schedule:\n    interval: daily\n    time: \"02:00\"\n  open-pull-requests-limit: 10\n\n- package-ecosystem: docker\n  directory: \"/{{cookiecutter.project_name}}/docker/django\"\n  schedule:\n    interval: daily\n    time: \"02:00\"\n  open-pull-requests-limit: 10\n\n- package-ecosystem: docker\n  directory: \"/{{cookiecutter.project_name}}/docker/caddy\"\n  schedule:\n    interval: daily\n    time: \"02:00\"\n  open-pull-requests-limit: 10\n\n- package-ecosystem: github-actions\n  directory: \"/\"\n  schedule:\n    interval: daily\n    time: \"02:00\"\n  open-pull-requests-limit: 10\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": ".github/dependabot.yml", "language": "yaml", "complexity": "simple", "star_count": 2112, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 50}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: docs\n\non:\n  push:\n    branches:\n      - master\n    paths:\n      - '.github/workflows/docs.yml'\n      - 'tests/build_docs.sh'\n      - '{{cookiecutter.project_name}}/docs/**/*'\n      - '{{cookiecutter.project_name}}/pyproject.toml'\n  pull_request:\n  workflow_dispatch:\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\n  cancel-in-progress: true\n\npermissions: {}\n\njobs:\n  docs:\n    runs-on: ubuntu-latest\n    timeout-minutes: 30\n    permissions:\n      contents: write\n\n    steps:\n    - uses: actions/checkout@v4\n      with:\n        persist-credentials: false\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: '3.12.11'\n\n    - name: Install poetry\n      run: |\n        curl -sSL 'https://install.python-poetry.org' | python -\n\n        # Adding `poetry` to `$PATH`:\n        echo \"$HOME/.local/bin\" >> \"$GITHUB_PATH\"\n\n    - name: Install dependencies\n      run: poetry install\n\n    - name: Build docs\n      run: poetry run bash tests/build_docs.sh\n\n    - name: Deploy to Pages\n      uses: JamesIves/github-pages-deploy-action@v4.7.3\n      if: github.event_name == 'push'\n      with:\n        token: ${{ secrets.GITHUB_TOKEN }}\n        branch: gh-pages  # The branch the action should deploy to.\n        folder: html  # The folder the action should deploy.\n        clean: true  # Automatically remove deleted files from the deploy branch\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": ".github/workflows/docs.yml", "language": "yaml", "complexity": "medium", "star_count": 2112, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 58}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: hadolint\n\non:\n  push:\n    branches:\n      - master\n    paths:\n      - '{{cookiecutter.project_name}}/docker/django/Dockerfile'\n      - '{{cookiecutter.project_name}}/docker/caddy/Dockerfile'\n      - '.github/workflows/hadolint.yml'\n  pull_request:\n    paths:\n      - '{{cookiecutter.project_name}}/docker/django/Dockerfile'\n      - '{{cookiecutter.project_name}}/docker/caddy/Dockerfile'\n      - '.github/workflows/hadolint.yml'\n  workflow_dispatch:\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\n  cancel-in-progress: true\n\npermissions:\n  contents: read\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          persist-credentials: false\n      - uses: hadolint/hadolint-action@v3.1.0\n        with:\n          dockerfile: '{{cookiecutter.project_name}}/docker/django/Dockerfile'\n          failure-threshold: style\n      - uses: hadolint/hadolint-action@v3.1.0\n        with:\n          dockerfile: '{{cookiecutter.project_name}}/docker/caddy/Dockerfile'\n          failure-threshold: style\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": ".github/workflows/hadolint.yml", "language": "yaml", "complexity": "simple", "star_count": 2112, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 40}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: test\n\non:\n  push:\n    branches:\n      - master\n  pull_request:\n  workflow_dispatch:\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\n  cancel-in-progress: true\n\npermissions:\n  contents: read\n\nenv:  # Keep in sync with `POETRY_VERSION` in `Dockerfile`\n  POETRY_VERSION: \"2.1.3\"\n  PYTHON_VERSION: \"3.12\"\n\njobs:\n  test-generated-project:\n    runs-on: ubuntu-latest\n    timeout-minutes: 30\n\n    steps:\n    - uses: actions/checkout@v4\n      with:\n        persist-credentials: false\n    - name: Set up Python ${{ env.PYTHON_VERSION }}\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n\n    - name: Install poetry\n      run: |\n        curl -sSL 'https://install.python-poetry.org' | python -\n\n        # Adding `poetry` to `$PATH`:\n        echo \"$HOME/.local/bin\" >> \"$GITHUB_PATH\"\n\n    - name: Install dependencies\n      run: poetry install\n\n    - name: Run template's tests\n      run: poetry run bash tests/test_github.sh\n\n  test-internal:\n    runs-on: ubuntu-latest\n    timeout-minutes: 30\n\n    steps:\n    - uses: actions/checkout@v4\n      with:\n        persist-credentials: false\n    - name: Set up Python ${{ env.PYTHON_VERSION }}\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n\n    - name: Install poetry\n      run: |\n        curl -sSL 'https://install.python-poetry.org' | python -\n\n        # Adding `poetry` to `$PATH`:\n        echo \"$HOME/.local/bin\" >> \"$GITHUB_PATH\"\n\n    - name: Install dependencies\n      run: |\n        poetry config virtualenvs.in-project true\n        poetry install\n\n    - name: Run internal project checks\n      run: |\n        poetry run ruff check --exit-non-zero-on-fix --diff\n        poetry run ruff format --check --diff\n        poetry run flake8 .\n        poetry run mypy .\n        poetry run pytest\n        poetry run poetry check\n        poetry run pip check\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": ".github/workflows/test.yml", "language": "yaml", "complexity": "medium", "star_count": 2112, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 82}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "rules:\n  unpinned-uses:\n    config:\n      policies:\n        \"*\": ref-pin\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": ".github/zizmor.yml", "language": "yaml", "complexity": "simple", "star_count": 2112, "quality_score": 0.43, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 6}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# We only run this pipeline when something gitlab-related in changed,\n# no need to execute it on every run, because basic testing is covered\n# by Github Actions.\n\n# TODO: reenable https://gitlab.com/sobolevn/wemake-django-template\n\nworkflow:\n  rules:\n    - if: \"$CI_PIPELINE_SOURCE\"\n      when: always\n      changes:\n        - .gitlab-ci.yml\n        - '{{cookiecutter.project_name}}/.gitlab-ci.yml'\n        - tests/build.sh\n        - tests/test_gitlab.sh\n\n# Reusing existing gitlab definition:\ninclude:\n  - local: '{{cookiecutter.project_name}}/.gitlab-ci.yml'\n\n\n# Building template project:\nmirror-github-build:\n  stage: build\n  image: python:3.12.11\n  variables:\n    POETRY_VERSION: '2.1.3'\n  before_script:\n    # Installing poetry:\n    - curl -sSL 'https://install.python-poetry.org' | python -\n    - export PATH=\"$HOME/.local/bin:$PATH\"\n    # Installing dependencies:\n    - poetry install --no-dev\n  script:\n    - poetry run bash tests/test_gitlab.sh\n  only:\n    - master\n    - external_pull_requests\n  artifacts:\n    name: 'template-build'\n    expire_in: 1 week\n    paths:\n      - '.test/wemake-django-template'\n\n\n# Redefining how tests are executed:\nvariables:\n  GROUP_NAME: 'sobolevn'\n  PROJECT_NAME: 'wemake-django-template'\n\n\ntest:\n  variables:\n    CI_PROJECT_DIR: '.test/wemake-django-template'\n  only:\n    - master\n    - external_pull_requests\n\n\nrelease-image:\n  variables:\n    CI_PROJECT_DIR: '.test/wemake-django-template'\n  only:\n    - master\n    - external_pull_requests\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": ".gitlab-ci.yml", "language": "yaml", "complexity": "medium", "star_count": 2112, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 66}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-toml\n      - id: check-xml\n      - id: check-merge-conflict\n      - id: check-symlinks\n      - id: check-illegal-windows-names\n      - id: mixed-line-ending\n        args: [\"--fix=lf\"]\n      - id: check-case-conflict\n  - repo: https://github.com/python-jsonschema/check-jsonschema\n    rev: 0.33.2\n    hooks:\n      - id: check-dependabot\n      - id: check-github-workflows\n      - id: check-github-actions\n      - id: check-gitlab-ci\n        args: [\"--format-regex\", \"python\"]\n      - id: check-renovate\n  - repo: https://github.com/rhysd/actionlint\n    rev: v1.7.7\n    hooks:\n      - id: actionlint\n        additional_dependencies:\n          - \"github.com/wasilibs/go-shellcheck/cmd/shellcheck@latest\"\n  - repo: https://github.com/woodruffw/zizmor-pre-commit\n    rev: v1.11.0\n    hooks:\n      - id: zizmor\n  - repo: https://github.com/shellcheck-py/shellcheck-py\n    rev: v0.10.0.1\n    hooks:\n      - id: shellcheck\n        args: [\"--severity=style\"]\n\n  # Should be the last:\n  - repo: meta\n    hooks:\n      - id: check-useless-excludes\n\nci:\n  autofix_prs: true\n  autoupdate_schedule: weekly\n  submodules: false\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": ".pre-commit-config.yaml", "language": "yaml", "complexity": "simple", "star_count": 2112, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 50}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# How to contribute\n\nThis is a how-to-contribute guide for the template itself.\nThis guide is not about contributing to the project that is created\nusing this template.\n\n\n## Dependencies\n\nWe use `poetry` to manage the [dependencies](https://github.com/python-poetry/poetry).\n\nTo install them you would need to run `install` command:\n\n```bash\npoetry install\n```\n\nTo activate your `virtualenv` run `poetry shell`.\n\n\n## Linting\n\nWe use `flake8` to run linting.\nWe use `wemake-python-styleguide` as the main code style rules.\nRun:\n\n```bash\nflake8 .\n```\n\n\n## Unit tests\n\nWe use `pytest` to run unit tests. Run:\n\n```bash\npytest\n```\n\n\n## Other help\n\nYou can contribute by spreading a word about this library.\nIt would also be a huge contribution to write\na short article on how you are using this project.\nYou can also share your best practices with us.\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "simple", "star_count": 2112, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 47}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# wemake-django-template\n\n[![wemake.services](https://img.shields.io/badge/%20-wemake.services-green.svg?label=%20&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAMAAAAoLQ9TAAAABGdBTUEAALGPC%2FxhBQAAAAFzUkdCAK7OHOkAAAAbUExURQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP%2F%2F%2F5TvxDIAAAAIdFJOUwAjRA8xXANAL%2Bv0SAAAADNJREFUGNNjYCAIOJjRBdBFWMkVQeGzcHAwksJnAPPZGOGAASzPzAEHEGVsLExQwE7YswCb7AFZSF3bbAAAAABJRU5ErkJggg%3D%3D)](https://wemake-services.github.io)\n![Awesome](https://awesome.re/badge-flat2.svg)\n[![test](https://github.com/wemake-services/wemake-django-template/actions/workflows/test.yml/badge.svg?branch=master&event=push)](https://github.com/wemake-services/wemake-django-template/actions/workflows/test.yml)\n[![Dependencies Status](https://img.shields.io/badge/dependencies-up%20to%20date-brightgreen.svg)](https://github.com/wemake-services/wemake-django-template/pulls?utf8=%E2%9C%93&q=is%3Apr%20author%3Aapp%2Fdependabot)\n[![wemake-python-styleguide](https://img.shields.io/badge/style-wemake-000000.svg)](https://github.com/wemake-services/wemake-python-styleguide)\n\n\n\nBleeding edge `django5.2` template focused on code quality and security.\n[Documentation](https://wemake-services.github.io/wemake-django-template).\n\n---\n\n\n## Purpose\n\nThis project is used to scaffold a `django` project structure.\nJust like `django-admin.py startproject` but better.\n\n\n## Features\n\n- Always [`up-to-date`](https://github.com/wemake-services/wemake-django-template/pulls?utf8=%E2%9C%93&q=is%3Apr%20author%3Aapp%2Fdependabot) with the help of [`@dependabot`](https://dependabot.com/)\n- Supports latest `python3.12`\n- [`poetry@2`](https://github.com/python-poetry/poetry) for managing dependencies\n- [`mypy`](https://mypy.readthedocs.io) and [`django-stubs`](https://github.com/typeddjango/django-stubs) for static typing\n- [`pytest`](https://pytest.org/) and [`hypothesis`](https://github.com/HypothesisWorks/hypothesis) for unit tests\n- [`ruff`](https://docs.astral.sh/ruff) and [`wemake-python-styleguide`](https://wemake-python-styleguide.readthedocs.io/en/latest/) for linting\n- [`docker`](https://www.docker.com/) for development, testing, and production\n- [`sphinx`](http://www.sphinx-doc.org/en/master/) for documentation\n- [`Gitlab CI`](https://about.gitlab.com/gitlab-ci/) with full `build`, `test`, and `deploy` [pipeline configured by default](https://gitlab.com/sobolevn/wemake-django-template/-/pipelines)\n- [`Caddy`](https://caddyserver.com/) with [`https`](https://caddyserver.com/docs/automatic-https) and `http/2` turned on by default\n\n\n## Installation\n\nFirstly, you will need to install [dependencies](https://cookiecutter.readthedocs.io/en/latest/).\n\nThe recommended way is via [`uvx`](https://docs.astral.sh/uv/guides/tools):\n\n```bash\nuvx --with jinja2-git cookiecutter gh:wemake-services/wemake-django-template\n```\n\n---\n\nOr via [`pipx`](https://github.com/pypa/pipx):\n\n```bash\npipx install cookiecutter\npipx inject cookiecutter jinja2-git\n```\n\nOr via global `pip`:\n\n```bash\npip install cookiecutter jinja2-git\n```\n\nThen, create a project itself:\n\n```bash\ncookiecutter gh:wemake-services/wemake-django-template\n```\n\n\n## Who is using this template?\n\nIf you use our template, please add yourself or your company [in the list](https://github.com/wemake-services/wemake-django-template/wiki/Who-is-using-this-template).\n\nHere's a [nice list of real-life open-source usages](https://github.com/search?q=wemake-django-template&type=Code)\nof this template.\n\n\n## License\n\nMIT. See [LICENSE](https://github.com/wemake-services/wemake-django-template/blob/master/LICENSE) for more details.\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 2112, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 80}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{\n  \"project_name\": \"test-project\",\n  \"project_verbose_name\": \"Test Project\",\n  \"project_domain\": \"myapp.com\",\n  \"organization\": \"wemake.services\",\n\n  \"_extensions\": [\"jinja2_git.GitExtension\"],\n  \"_copy_without_render\": [\n    \".venv\",\n    \"docs/_build\"\n  ]\n}\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "cookiecutter.json", "language": "json", "complexity": "simple", "star_count": 2112, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 13}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis module is called after project is created.\n\nIt does the following:\n1. Generates and saves random secret key\n2. Prints further instructions\n\nA portion of this code was adopted from Django's standard crypto functions and\nutilities, specifically:\nhttps://github.com/django/django/blob/master/django/utils/crypto.py\n\nAnd from pydanny's cookiecutter-django:\nhttps://github.com/pydanny/cookiecutter-django\n\n\"\"\"\n\nimport secrets\nimport shutil\nimport string\nfrom pathlib import Path\nfrom typing import Final\n\n# CHANGEME mark\nCHANGEME: Final = '__CHANGEME__'\n\n# Get the root project directory\nPROJECT_DIRECTORY: Final = Path.cwd().resolve(strict=True)\nPROJECT_NAME: Final = '{{ cookiecutter.project_name }}'\n\n# Messages\nPROJECT_SUCCESS: Final = \"\"\"\nYour project {0} is created.\nNow you can start working on it:\n\n    cd {0}\n\"\"\"\n\n\ndef _get_random_string(length: int = 50) -> str:\n    \"\"\"\n    Returns a securely generated random string.\n\n    >>> secret = _get_random_string()\n    >>> len(secret)\n    50\n\n    \"\"\"\n    punctuation = (\n        string.punctuation.replace(\n            '\"',\n            '',\n        )\n        .replace(\n            \"'\",\n            '',\n        )\n        .replace(\n            '\\\\',\n            '',\n        )\n        .replace(\n            '$',\n            '',  # see issue-271\n        )\n    )\n\n    chars = string.digits + string.ascii_letters + punctuation\n    return ''.join(secrets.choice(chars) for _ in range(length))\n\n\ndef _create_secret_key(config_path: Path) -> None:\n    # Generate a SECRET_KEY that matches the Django standard\n    secret_key = _get_random_string()\n\n    with config_path.open(mode='r+', encoding='utf8') as config_file:\n        # Replace CHANGEME with SECRET_KEY\n        file_contents = config_file.read().replace(CHANGEME, secret_key, 1)\n\n        # Write the results to the file:\n        config_file.seek(0)\n        config_file.write(file_contents)\n        config_file.truncate()\n\n\ndef print_futher_instuctions() -> None:\n    \"\"\"Shows user what to do next after project creation.\"\"\"\n    print(PROJECT_SUCCESS.format(PROJECT_NAME))  # noqa: WPS421\n\n\ndef copy_local_configuration() -> None:\n    \"\"\"\n    Handler to copy local configuration.\n\n    It is copied from ``.template`` files to the actual files.\n    \"\"\"\n    secret_template = PROJECT_DIRECTORY / 'config' / '.env.template'\n    secret_config = PROJECT_DIRECTORY / 'config' / '.env'\n    shutil.copyfile(secret_template, secret_config)\n    _create_secret_key(secret_config)\n\n    # Local config:\n    local_template = (\n        PROJECT_DIRECTORY\n        / 'server'\n        / 'settings'\n        / 'environments'\n        / 'local.py.template'\n    )\n    local_config = (\n        PROJECT_DIRECTORY / 'server' / 'settings' / 'environments' / 'local.py'\n    )\n    shutil.copyfile(local_template, local_config)\n\n\ncopy_local_configuration()\nprint_futher_instuctions()\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "hooks/post_gen_project.py", "language": "python", "complexity": "medium", "star_count": 2112, "quality_score": 0.78, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 117}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\n# Used by both `test_github` and `test_gitlab`\n\nset -o errexit\nset -o nounset\nset -o pipefail\n\n# Creating a test directory:\nreadonly TEST_DIR='./.test'\n\n# Scaffold the project:\nreadonly PROJECT_NAME='wemake-django-template'\nreadonly PROJECT_ORGANIZATION='sobolevn'\n\nreadonly PROJECT_PATH=\"$TEST_DIR/$PROJECT_NAME\"\n\nrun_cookiecutter_build () {\n  mkdir -p \"$TEST_DIR\" && cd \"$TEST_DIR\"\n\n  cookiecutter \"$1\" \\\n    --no-input --overwrite-if-exists \\\n    project_name=\"$PROJECT_NAME\" \\\n    project_domain='myapp.com' \\\n    organization=\"$PROJECT_ORGANIZATION\"\n\n  cd '..'\n}\n\n# Exporting variables:\nexport PROJECT_PATH\nexport PROJECT_NAME\nexport PROJECT_ORGANIZATION\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "tests/build.sh", "language": "bash", "complexity": "simple", "star_count": 2112, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 34}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o errexit\nset -o nounset\nset -o pipefail\n\n# This file is used to setup fake project,\n# run tests inside it,\n# and remove this project completely.\n. './tests/build.sh'\n\n# Building the template:\nrun_cookiecutter_build \"$GITHUB_WORKSPACE\"\ncd \"$PROJECT_PATH\"\n\n# Install deps from the project:\npoetry sync --only=main --only=docs\n\n# Build docs:\nmake -C docs html\n\n# Move the result docs to the dest:\nls docs/_build/html\nmv docs/_build/html ../..\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "tests/build_docs.sh", "language": "bash", "complexity": "simple", "star_count": 2112, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 25}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\n\n\n@pytest.fixture\ndef context() -> dict[str, str]:\n    \"\"\"Creates default prompt values.\"\"\"\n    return {\n        'project_name': 'test-project',\n        'project_verbose_name': 'Test Project',\n        'project_domain': 'myapp.com',\n        'organization': 'wemake.services',\n    }\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "tests/conftest.py", "language": "python", "complexity": "simple", "star_count": 2112, "quality_score": 0.69, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 13}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o errexit\nset -o nounset\nset -o pipefail\n\n# This file is used to setup fake project,\n# run tests inside it,\n# and remove this project completely.\n. './tests/build.sh'\n\n# Building the template:\nrun_cookiecutter_build \"$GITHUB_WORKSPACE\"\ncd \"$PROJECT_PATH\"\n\n# enable docker buildkit\nexport DOCKER_BUILDKIT=1 COMPOSE_DOCKER_CLI_BUILD=1\n\n# Run tests that are located inside the generate project:\ndocker compose -f docker-compose.yml \\\n  -f docker/docker-compose.prod.yml config --quiet\n\n# Building and testing dev image:\ndocker compose build --build-arg BUILDKIT_INLINE_CACHE=1\ndocker compose run --user=root --rm web ./docker/django/ci.sh\n\n# Building and testing prod image:\ndocker compose -f docker-compose.yml \\\n  -f docker/docker-compose.prod.yml build\ndocker compose -f docker-compose.yml \\\n  -f docker/docker-compose.prod.yml run \\\n  --user=root --rm web \\\n  python manage.py check --deploy --fail-level WARNING\n\n# Testing caddy configuration:\ndocker compose -f docker-compose.yml \\\n  -f docker/docker-compose.prod.yml run \\\n  --rm caddy sh /etc/ci.sh\n\n# Checking the size of final images:\ndisl --current-size --max-layers=13 \"${PROJECT_NAME}:dev\" 950MiB\ndisl --current-size --max-layers=14 \\\n  \"registry.gitlab.com/${PROJECT_ORGANIZATION}/${PROJECT_NAME}:latest\" 700MiB\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "tests/test_github.sh", "language": "bash", "complexity": "medium", "star_count": 2112, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 44}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: \"Build and Test virtual routers\"\n\non:\n  pull_request:\n  push:\n    branches:\n      - main\n      - master\n\nenv:\n  PNS: ci-${GITHUB_RUN_ID}\n  DOCKER_REGISTRY: ghcr.io/vrnetlab/vrnetlab\n\njobs:\n  test-vr-xcon-and-vr-bgp:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Build vr-xcon\n        run: |\n          make vr-xcon\n\n      - name: Test vr-xcon\n        run: |\n          make vr-xcon-test\n\n      - name: Save vr-xcon logs\n        if: always()\n        run: |\n          make -C vr-xcon docker-test-save-logs\n\n      - name: Clean test vr-xcon containers\n        if: always()\n        run: make -C vr-xcon docker-test-clean\n\n      # vr-bgp depends on the vr-xcon container image, so let's build it right\n      # now on the same runner ...\n      - name: Build vr-bgp\n        run: |\n          make vr-bgp\n\n      - name: Test vr-bgp\n        run: |\n          make vr-bgp-test\n\n      - name: Save vr-bgp logs\n        if: always()\n        run: |\n          make -C vr-bgp docker-test-save-logs\n\n      - name: Clean test vr-bgp containers\n        if: always()\n        run: make -C vr-bgp docker-test-clean\n\n  test-topomachine:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Build topomachine\n        run: |\n          make topology-machine\n\n      - name: Test topomachine\n        run: |\n          make topology-machine-test\n\n      - name: Save topomachine logs\n        if: always()\n        run: |\n          make -C topology-machine docker-test-save-logs\n\n      - name: Clean test topomachine containers\n        if: always()\n        run: make -C topology-machine docker-test-clean\n\n  test-vr:\n    runs-on: [\"self-hosted\"]\n    container:\n      image: vrnetlab/ci-builder\n      volumes:\n        - vrnetlab-images:/vrnetlab-images\n    strategy:\n      fail-fast: false\n      matrix:\n        platform: ['csr', 'c8000v', 'nxos', 'nxos9kv', 'routeros', 'sros', 'ucpe-oneos', 'veos', 'vmx', 'vqfx', 'vrp', 'vsr1000', 'xrv', 'xrv9k']\n\n    steps:\n      - uses: actions/checkout@v4\n      - name: Fixup dubious ownership\n        run: git config --global --add safe.directory ${GITHUB_WORKSPACE}\n\n      - name: Use git to check if the source files (platform or shared) have changed\n        uses: dorny/paths-filter@v3\n        id: source_changes\n        with:\n          filters: |\n            platform:\n              - '${{ matrix.platform }}/**/*'\n              - 'common/*'\n\n      - name: Compute platform image hashes in bind-mounted volume\n        run: |\n          shasum -a 512 /vrnetlab-images/${{ matrix.platform }}/* > hash-${{ matrix.platform }}.txt || touch hash-${{ matrix.platform }}.txt\n          cat hash-${{ matrix.platform }}.txt\n\n      - name: Cache image hashes\n        id: cache-hash\n        uses: actions/cache@v4\n        with:\n          path: cache-hash-${{ matrix.platform }}.txt\n          key: hash-${{ matrix.platform }}\n\n      - name: Compare computed image hashes with cached hashes\n        id: image_changes\n        run: |\n          if diff hash-${{ matrix.platform }}.txt cache-hash-${{ matrix.platform}}.txt; then\n            echo platform=false >> ${GITHUB_OUTPUT};\n            echo \"No image changes detected for ${{ matrix.platform }}\";\n          else\n            echo platform=true >> ${GITHUB_OUTPUT};\n            echo \"Image changes detected for ${{ matrix.platform }}\";\n          fi\n\n      - name: Build ${{ matrix.platform }}\n        if: ${{ steps.source_changes.outputs.platform == 'true' || steps.image_changes.outputs.platform == 'true' }}\n        run: |\n          cp /vrnetlab-images/${{ matrix.platform }}/* ${{ matrix.platform }} || true\n          ls -al ${{ matrix.platform }}\n          make ${{ matrix.platform }}\n\n      - name: Test ${{ matrix.platform }}\n        if: ${{ steps.source_changes.outputs.platform == 'true' || steps.image_changes.outputs.platform == 'true' }}\n        run: |\n          make ${{ matrix.platform }}-test\n\n      - name: Save ${{ matrix.platform }} logs\n        if: ${{ always() && (steps.source_changes.outputs.platform == 'true' || steps.image_changes.outputs.platform == 'true') }}\n        run: |\n          make -C ${{ matrix.platform }} docker-test-save-logs\n\n      - uses: actions/upload-artifact@v4\n        if: ${{ always() && (steps.source_changes.outputs.platform == 'true' || steps.image_changes.outputs.platform == 'true') }}\n        with:\n          name: vr-logs-${{ matrix.platform }}\n          path: |\n            ${{ matrix.platform }}/*.log\n\n      - name: Clean test ${{ matrix.platform }} containers\n        if: ${{ always() && (steps.source_changes.outputs.platform == 'true' || steps.image_changes.outputs.platform == 'true') }}\n        run: make -C ${{ matrix.platform }} docker-test-clean\n\n      - name: Persist image hashes\n        run: mv hash-${{ matrix.platform }}.txt cache-hash-${{ matrix.platform }}.txt\n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": ".github/workflows/test.yml", "language": "yaml", "complexity": "complex", "star_count": 1285, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 156}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "image: vrnetlab/ci-builder\n\nvariables:\n  PNS: ci-${CI_PIPELINE_ID}\n\nstages:\n  - build\n\n.build: &build-template\n  stage: build\n  tags:\n    - vrnetlab\n  script:\n    # make sure we pulled LFS files\n    - git lfs fetch -I ${CI_JOB_NAME}\n    - git lfs checkout ${CI_JOB_NAME}\n    - ls -l ${CI_JOB_NAME}\n    # We allow the user to control which Docker registry is used through the\n    # env var DOCKER_REGISTRY. If it is not set then we assume we should use\n    # the GitLab built-in Docker registry so we check if it is enabled.\n    # CI_REGISTRY is only set when the GitLab Docker registry is enabled\n    - if [ -z \"${DOCKER_REGISTRY}\" ]; then if [ -n \"${CI_REGISTRY}\" ]; then export DOCKER_USER=gitlab-ci-token; export DOCKER_PASSWORD=${CI_JOB_TOKEN}; export DOCKER_REGISTRY=${CI_REGISTRY_IMAGE}; fi; fi\n    - 'echo \"DOCKER_REGISTRY: ${DOCKER_REGISTRY}\"'\n    # if DOCKER_REGISTRY set, either explicitly by user or implicitly by GitLab\n    # (see above) we login to repo, build images and push them\n    - if [ -n \"${DOCKER_REGISTRY}\" ]; then docker login -u ${DOCKER_USER} -p=${DOCKER_PASSWORD} ${DOCKER_REGISTRY}; fi\n    - if [ -n \"${DOCKER_REGISTRY}\" ]; then\n    -   make ${CI_JOB_NAME}\n    -   make ${CI_JOB_NAME}-test\n    -   if [ \"${CI_COMMIT_REF_NAME}\" = \"${CI_DEFAULT_BRANCH}\" ]; then\n    -     echo \"Pushing images\"\n    -     make ${CI_JOB_NAME}-push\n    -   fi\n    - fi\n  interruptible: true\n  after_script:\n    # save logs for artifacts\n    - make -C ${CI_JOB_NAME} docker-test-save-logs\n    # clean up leftover (failed) test containers\n    - make -C ${CI_JOB_NAME} docker-test-clean\n  artifacts:\n    when: always\n    paths:\n      - ${CI_JOB_NAME}/*.log\n\nvr-xcon:\n  <<: *build-template\n\nvr-bgp:\n  <<: *build-template\n\ntopology-machine:\n  extends: .build\n\ncsr:\n  <<: *build-template\n\nc8000v:\n  <<: *build-template\n\nnxos:\n  <<: *build-template\n\nnxos9kv:\n  <<: *build-template\n\nrouteros:\n  <<: *build-template\n\nsros:\n  <<: *build-template\n\nucpe-oneos:\n  <<: *build-template\n\nveos:\n  <<: *build-template\n\nvmx:\n  <<: *build-template\n\nvsr1000:\n  <<: *build-template\n\nvqfx:\n  <<: *build-template\n\nvrp:\n  <<: *build-template\n\nxrv:\n  <<: *build-template\n\nxrv9k:\n  <<: *build-template\n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": ".gitlab-ci.yml", "language": "yaml", "complexity": "medium", "star_count": 1285, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 96}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Code of conduct\nPlease be civilised and nice to each other. We're all humans. Act like one.\n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": "CODE_OF_CONDUCT.md", "language": "markdown", "complexity": "simple", "star_count": 1285, "quality_score": 0.39999999999999997, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 3}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributing to vrnetlab\n\nThank you for (considering) contributing to vrnetlab!\n\n## Bugs?\nPlease report bugs as issues here on GitLab! Also feel free to use issues for asking questions on how to use vrnetlab etc too.\n\n## New feature?\nDo you want to build something new? Please open an issue describing your idea so that we can align how it is best implemented. Discussing before you start coding significantly increases the chance that your code will be merged smoothly and without lots of refactoring etc.\n\nIf you don't want to code but have an idea for a new feature, please open an issue.\n\n## Code submission\nSubmit your code as a Pull Request (PR). Make sure it applies cleanly to the master branch!\n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "simple", "star_count": 1285, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "vrnetlab - VR Network Lab\n-------------------------\nRun your favourite virtual routers in docker for convenient labbing,\ndevelopment and testing.\n\nvrnetlab is being developed for the TeraStream project at Deutsche Telekom as\npart of an automated CI environment for testing our network provisioning\nsystem.\n\nIt supports:\n\n * Arista vEOS\n * Cisco CSR1000v\n * Cisco Nexus NX-OS (using Titanium emulator)\n * Cisco XRv\n * Cisco XRv 9000\n * Juniper vMX\n * Juniper vQFX\n * Nokia VSR\n \nI talk a little about it during my presentation about TeraStream testing at\nthe NetNod autumn meeting 2016 - https://youtu.be/R_vCdGkGeSk?t=9m25s\n\nBrian Linkletter has written a good introduction too https://www.brianlinkletter.com/vrnetlab-emulate-networks-using-kvm-and-docker/\n\n\nUsage\n-----\nYou have to build the virtual router docker images yourself since the license\nagreements of commercial virtual routers do not allow me to distribute the\nimages. See the README files of the respective virtual router types for more\ndetails.\n\nYou need KVM enabled in your kernel for hardware assisted virtualization. While\nit may be possible to run without it, it has not been tested. Make sure you\nload the kvm kernel module: `modprobe kvm`.\n\nLet's assume you've built the `xrv` router.\n\nStart two virtual routers:\n```\ndocker run -d --name vr1 --privileged vr-xrv:5.3.3.51U\ndocker run -d --name vr2 --privileged vr-xrv:5.3.3.51U\n```\nI'm calling them vr1 and vr2. Note that I'm using XRv 5.3.3.51U - you should\nfill in your XRv version in the image tag as the \"latest\" tag is not added to\nany images.\n\nIt takes a few minutes for XRv to start but once up you should be able to SSH\ninto each virtual router. You can get the IP address using docker inspect:\n```\nroot@host# docker inspect --format '{{.NetworkSettings.IPAddress}}' vr1\n172.17.0.98\n```\nNow SSH to that address and login with the default credentials of\nvrnetlab/VR-netlab9:\n```\nroot@host# ssh -l vrnetlab $(docker inspect --format '{{.NetworkSettings.IPAddress}}' vr1)\nThe authenticity of host '172.17.0.98 (172.17.0.98)' can't be established.\nRSA key fingerprint is e0:61:28:ba:12:77:59:5e:96:cc:58:e2:36:55:00:fa.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added '172.17.0.98' (RSA) to the list of known hosts.\n\nIMPORTANT:  READ CAREFULLY\nWelcome to the Demo Version of Cisco IOS XRv (the \"Software\").\nThe Software is subject to and governed by the terms and conditions\nof the End User License Agreement and the Supplemental End User\nLicense Agreement accompanying the product, made available at the\ntime of your order, or posted on the Cisco website at\nwww.cisco.com/go/terms (collectively, the \"Agreement\").\nAs set forth more fully in the Agreement, use of the Software is\nstrictly limited to internal use in a non-production environment\nsolely for demonstration and evaluation purposes.  Downloading,\ninstalling, or using the Software constitutes acceptance of the\nAgreement, and you are binding yourself and the business entity\nthat you represent to the Agreement.  If you do not agree to all\nof the terms of the Agreement, then Cisco is unwilling to license\nthe Software to you and (a) you may not download, install or use the\nSoftware, and (b) you may return the Software as more fully set forth\nin the Agreement.\n\n\nPlease login with any configured user/password, or cisco/cisco\n\n\nvrnetlab@172.17.0.98's password:\n\n\nRP/0/0/CPU0:ios#show version\nMon Jul 18 09:04:45.261 UTC\n\nCisco IOS XR Software, Version 5.3.3.51U[Default]\n...\n```\n\nYou can also login via NETCONF:\n```\nroot@host# ssh -l vrnetlab $(docker inspect --format '{{.NetworkSettings.IPAddress}}' vr1) -p 830 -s netconf\nvrnetlab@172.17.0.98's password:\n<hello xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\">\n <capabilities>\n  <capability>urn:ietf:params:netconf:base:1.1</capability>\n  <capability>urn:ietf:params:xml:ns:yang:ietf-netconf-monitoring</capability>\n  <capability>urn:ietf:params:netconf:capability:candidate:1.0</capability>\n  <capability>urn:ietf:params:netconf:capability:rollback-on-error:1.0</capability>\n  <capability>urn:ietf:params:netconf:capability:validate:1.1</capability>\n  <capability>urn:ietf:params:netconf:capability:confirmed-commit:1.1</capability>\n  <capability>http://cisco.com/ns/yang/Cisco-IOS-XR-aaa-lib-cfg?module=Cisco-IOS-XR-aaa-lib-cfg&amp;revision=2015-08-27</capability>\n  <capability>http://cisco.com/ns/yang/Cisco-IOS-XR-aaa-locald-admin-cfg?module=Cisco-IOS-XR-aaa-locald-admin-cfg&amp;revision=2015-08-27</capability>\n  <capability>http://cisco.com/ns/yang/Cisco-IOS-XR-aaa-locald-cfg?module=Cisco-IOS-XR-aaa-locald-cfg&amp;revision=2015-08-27</capability>\n  <capability>http://cisco.com/ns/yang/Cisco-IOS-XR-aaa-locald-oper?module=Cisco-IOS-XR-aaa-locald-oper&amp;revision=2015-08-27</capability>\n  <capability>http://cisco.com/ns/yang/Cisco-IOS-XR-bundlemgr-cfg?module=Cisco-IOS-XR-bundlemgr-cfg&amp;revision=2015-08-27</capability>\n...\n```\n\nThe serial console of the devices are mapped to port 5000. Use telnet to connect:\n```\nroot@host# telnet $(docker inspect --format '{{.NetworkSettings.IPAddress}}' vr1) 5000\n```\nJust like with any serial port, you can only have one connection at a time and\nwhile the router is booting the launch script will connect to the serial port\nto do the initialization of the router. As soon as it is done the port will be\nreleased and made available to the next connection.\n\nTo connect two virtual routers with each other we can use the `vr-xcon`\ncontainer. Let's say we want to connect Gi0/0/0/0 of vr1 and vr2 with each\nother, we would do:\n```\ndocker run -d --name vr-xcon --link vr1 --link vr2 vr-xcon --p2p vr1/1--vr2/1\n```\n\nConfigure a link network on vr1 and vr2 and you should be able to ping!\n```\nP/0/0/CPU0:ios(config)#inte GigabitEthernet 0/0/0/0\nRP/0/0/CPU0:ios(config-if)#no shutdown\nRP/0/0/CPU0:ios(config-if)#ipv4 address 192.168.1.2/24\nRP/0/0/CPU0:ios(config-if)#commit\nMon Jul 18 09:13:24.196 UTC\nRP/0/0/CPU0:Jul 18 09:13:24.216 : ifmgr[227]: %PKT_INFRA-LINK-3-UPDOWN : Interface GigabitEthernet0/0/0/0, changed state to Down\nRP/0/0/CPU0:ios(config-if)#dRP/0/0/CPU0:Jul 18 09:13:24.256 : ifmgr[227]: %PKT_INFRA-LINK-3-UPDOWN : Interface GigabitEthernet0/0/0/0, changed state to Up\no ping 192.168.1.1\nMon Jul 18 09:13:26.896 UTC\nType escape sequence to abort.\nSending 5, 100-byte ICMP Echos to 192.168.1.1, timeout is 2 seconds:\n!!!!!\nSuccess rate is 100 percent (5/5), round-trip min/avg/max = 1/1/1 ms\n```\n(obviously I configured the other end too!)\n\nAll of the NICs of the virtual routers are exposed via TCP ports by KVM. TCP\nport 10001 maps to the first NIC of the virtual router, which in the case of an\nXR router is GigabitEthernet 0/0/0/0. By simply connecting two of these TCP\nsockets together we can bridge the traffic between those two NICs and this is\nexactly what vr-xcon is for. Use the `--p2p` argument to specify the links.\nThe format is X/Y--Z/N where X is the name of the first router and Y is the\nport on that router. Z is the second router and N is the port on the second\nrouter.\n\nTo set up more than one p2p link, simply add more mappings separated by space\nand don't forget to link the virtual routers:\n```\ndocker run -d --name vr-xcon --link vr1 --link vr2 --link vr3 vr-xcon --p2p vr1/1--vr2/1 vr1/2--vr3/1\n```\nSee topology-machine/README.md for details on topology machine which can help\nyou with managing more complex topologies.\n\nThe containers expose port 22 for SSH, port 161 for SNMP, port 830 for NETCONF\nand port 5000 is mapped to the virtual serial device (use telnet). All the NICs\nof the virtual routers are exposed via TCP ports in the range 10001-10099.\n\nUse `docker rm -f vr1` to stop and remote a virtual router.\n\nHandy shell functions\n---------------------\nThere are some handy shell functions in vrnetlab.sh that provides shorthands\nfor connecting to ssh and console.\n\n1. Load the functions into your shell\n```\n. vrnetlab.sh\n```\n2. Login via ssh to router vr1, you can optionally specify a username. If no\nusername is provided, the default of vrnetlab will be used. If sshpass is\ninstalled, you will not be promted for password when you login with the default\nusername.\n```\nvrssh vr1 myuser \n```\n3. Connect console to router vr1\n```\nvrcons vr1\n```\n4. Create a bridge between two router interfaces, the below command bridges\ninterface 1 of router vr1 with interface 1 of router 2.\n```\nvrbridge vr1 1 vr2 1\n```\n\nTo load these aliases on login, copy it to ~/.vrnetlab_bashrc and add the\nfollowing to your .bashrc\n```\ntest -f ~/.vrnetlab_bashrc && . ~/.vrnetlab_bashrc\n``` \n\n\nVirtual routers\n---------------\nThere are a number of virtual routers available on the market:\n\n * Cisco XRv\n * Juniper VRR\n * Juniper vMX\n * Nokia VSR\n\nAll of the above are released as a qcow2 or vmdk file (which can easily be\nconverted into qcow2) making them easy to spin up on a Linux machine. Once spun\nup there are a few tasks one normally wants to perform:\n\n * set an IP address on a management interface\n * start SSH / NETCONF daemon (and generate crypto keys)\n * create initial user so we can login\n\nThere might be more things to the list but this is the bare minimum which makes\nthe router remotely reachable and thus we can configure the rest from the\nnormal provisioning system.\n\nvrnetlab aims to make this process as simple and convenient as possible so that\nit may be used both by humans and automated systems to spin up virtual routers.\nIn addition, there are scripts to help you generate topologies.\n\nThe virtual machines are packaged up in docker container. Since we need to\nstart KVM the docker containers have to be run with `--privileged` which\neffectively defeats the security features of docker. Our use of docker is\nessentially reduced to being a packaging format but a rather good one at that.\nAlso note that since we still rely on KVM the same amount of resources, if not\nsightly more, will be consumed by vrnetlab. A container is no thinner than a VM\nif the container contains a VM!\n\nThe assignment of a management IP address is handed over to docker, so you can\nuse whatever docker IPAM plugin you want. Overall the network setup of the\nvirtual routers are kind of shoe-horned into the world of docker networking.\nI'm not sure this is a good idea but it seems to work for now and it was fun\nputting it together ;)\n\nIt's possible to remotely control a docker engine and tell it to start/stop\ncontainers. It's not entirely uncommon to run the CI system in a VM and letting\nit remotely control another docker engine can give us some flexibility in where\nthe CI runner is executed vs where the virtual routers are running.\n\nlibvirt can also be remotely controlled so it could potentially be used to the\nsame effect. However, unlike libvirt, docker also has a registry concept which\ngreatly simplifies the distribution of the virtual routers. It's already neatly\npackaged up into a container image and now we can pull that image through a\nsingle command. With libvirt we would need to distribute the VM image and\nlaunch scripts as individual files.\n\nThe launch script differ from router to router. For example, it's possible to\nfeed a Cisco XR router a bootup config via a virtual CD-ROM drive so we can use\nthat to enable SSH/NETCONF and create a user. Nokia VSR however does not, so we\nneed to tell KVM to emulate a serial device and then have the launch script\naccess that virtual serial port via telnet to do the initial config.\n\nThe intention is to keep the arguments to each virtual router type as similar\nas possible so that a test orchestrator or similar need minimal knowledge about\nthe different router types.\n\n\nSystem requirements\n-------------------\nYou need to run these docker images on a machine that has a docker engine and\nthat supports KVM, i.e. you need a Linux kernel.\n\nDocker is available for OS X and it works by spinning up a Linux VM on top of\nthe xhyve hypervisor. While this means that we do have a docker engine and a\nLinux kernel, we are unable to use this for vrnetlab as xhyve does not offer\nnested virtualization and thus we cannot run KVM in the VM running in xhyve.\n\nVirtualBox does not offer nested virtualization either. Parallels and VMWare\nsupposedely do but I don't have access to those and can't test with.\n\nSee the README file of each virtual router type for CPU, RAM and disk\nrequirements.\n\n\nLow performance / virtual routers not starting properly\n-------------------------------------------------------\nIf you are having problems with performance, like routers not starting or being\nvery slow, there are a few knobs to tweak in order to improve the situation.\n\nThe basic problem is an unfortunate combination of CPU throttling and process\nscheduling causing cache thrasing which in turn leads to terrible performance.\nNo detailed measurements have been done to confirm this exact behaviour but the\nrecommended remedy has been confirmed working in multiple cases.\n\nvrnetlab runs virtual machines using qemu/KVM, which appear just as normal\nprocesses in Linux and are thus subject to the Linux process scheduler. If a\nprocess wants to do work it will be scheduled to run on a core. Now, if not all\ncores are used, APM will throttle down some of the cores such that the workload\ncan run on the remaining, say 3 out of 12 cores.  The Linux scheduler will try\nto schedule processes on the cores with the higher clock speed but if you have\nmore VMs than cores with high clock speed than it will start moving VMs around.\nL1/L2 caches are not shared by CPU cores, only L3.  Moving a process from one\ncore to another inevitably means that the cache is evicted. When processes are\nmoved around continuously we get cache thrasing and this appears to lower\nperformance for the VMs significantly. For some virtual routers it is to the\npoint where we hit various watchdog timeouts and the VMs will restart.\n\nThe very first step is to make sure you aren't trying to run too many virtual\nrouters on the same physical host. Some virtual routers, like Nokia SROS, has a\nrather low idle CPU usage of a few percent typically. Others, like Cisco XRV9k\nand Juniper vMX have a forwarding plane that is busy-looping over multiple CPU\ncores, thus consuming the entire CPU core. Trying to schedule multiple such\nvirtual machines over the same CPU cores can lead to failure.\n\nTo improve performance, we can start by changing the CPU governor in Linux to\n`performance`, for example using `cpupower frequency-set -g performance`. It\nlikely won't help much but try it first since it's considerably easier than the\nfollowing steps.\n\nDisable Advanced Power Management (APM) or similar in BIOS. This will\ncompletely prevent the CPU cores from throttling down and they will run at\ntheir designed maximum clock frequency. This probably means turbo boost\n(increasing clock frequency on a smaller subset of cores while decreasing the\nfrequency on remaining cores to remain at the same power and temperature\nenvelope) will be disabled too. Performance across all cores will however be\nmuch more deterministic. This alone usually means that the Linux process\nscheduler will now keep processes on the same cores instead of moving them\naround. Before only some of the cores would run at a higher frequency and so\nwould be more attractive to schedule work on. With all cores at the same\nfrequency, there is no reason for the process scheduler to move processes\naround. This removes the main cause of cache thrashing. At least that's the\nsimplified view of it but it appears to be working rather well in reality.\n\nIf performance is still not adequate the next step would be to disable\nhyperthreading. Hyperthreading is a technology to expose two logical cores that\nare executed by the same physical core. It's a strategy to avoid pipeline\nstalls, essentially where the CPU waits for memory. By having two logical\nthreads, the CPU core can switch to the other thread whenever it needs to wait\nfor memory lookups. It increases total concurrent throughput, however, each\nlogical thread will run slower than if it had run directly on a physical CPU\ncore.\n\nYou can avoid the effects of hyperthreading by only scheduling your qemu\nprocesses on half of the cores. You would need to inspect /proc/cpuinfo to\ndetermine the exact logical core layout and make sure you only schedule\nprocesses on one logical thread of each physical core. However, since you would\nthen only use half of the threads, it is easier to just disable hyperthreading\nin BIOS altogether.\n\nApplying the mentioned mitigations has so far resolved performance issues in\nall cases. Report if it doesn't for you.\n\n\nDocker healthcheck\n-----------------\nvrnetlab containers use the Docker healthcheck mechanism to report whether\nthey've started up properly or not.\n\n\nFUAQ - Frequently or Unfrequently Asked Questions\n-------------------------------------------------\n##### Q: Why don't you ship pre-built docker images?\nA: I don't think Cisco, Juniper or Nokia would allow me to distribute their virtual\n   router images and since one of the main points of vrnetlab is to have a self\n   contained docker image I don't see any other way than for you to build your\n   own image based on vrnetlab but where you get to download the router image\n   yourself.\n\n##### Q: Why don't you ship docker images where I can provide the image through a volume?\nA: I don't like the concept as it means you have to ship around an extra file.\n   If it's a self-contained image then all you have to do is push it to your\n   docker registry and then ask a box in your swarm cluster to spin it up!\n\n##### Q: Using docker typically means no persistent storage. How is configuration persisted across restarts?\nA: It is not persisted. The state of the virtual routers is lost once they are\nstopped/removed. It's not possible to restart vrnetlab or at least it's not at\nall tested and I don't see how it would work really. Since the primary use case\nis lab / CI you should embrace the statelessness :)\n\n##### Q: Will this consume less resources than the normal way of running XRv, vmX etc?\nA: No. vrnetlab still runs KVM (in docker) to start the virtual router which\nmeans that we will consume just as much CPU and memory, if not slightly more,\nthan running the router in KVM.\n\n##### Q: If it doesn't consume less resources than KVM, why use Docker?\nA: It's used primarily as a packaging format. All vrnetlab containers can be\nrun with similar arguments. The differences between different platforms are\neffectively hidden to present a clean uniform interface. That's certainly not\ntrue for trying to run XRv or vMX directly with qemu / virsh.\n\n##### Q: Do you plan to support classic IOS?\nA: IOS XE is available through the CSR1000v image which should satisfy all\nyour oldskool needs.\n\n##### Q: How do I connect a vrnetlab router with a normal docker container?\nA: I'm not entirely sure. For now you have to live with only communicating\nbetween vrnetlab routers. There's https://github.com/TOGoS/TUN2UDP and I\nsuppose the same idea could be used to bridge the TCP-socket NICs used by\nvrnetlab to a tun device, but if all this should happen inside a docker\ncontainer or if we should rely on setting this up on the docker host (using\nsomething similar to pipework) is not entirely clear to me. I'll probably work\non it.\n\n##### Q: How does this relate to GNS3, UNetLab and VIRL?\nA: It was a long time since I used GNS3 and I have only briefly looked at\nUNetLab and VIRL but from what I know or can see, these are all more targeted\ntowards interactive labbing. You get a pretty UI and similar whereas vrnetlab\nis controlled in a completely programmatic fashion which makes them good at\ndifferent things. vrnetlab is superb for CI and programmatic testing where the\nothers probably target labs run by humans.\n\n\nBuilding with GitLab CI\n-----------------------\nvrnetlab ships with a .gitlab-ci.yml config file so if you happen to be using\nGitLab CI you can use this file to let your CI infrastructure build the docker\nimages and push them to your registry. \n\nGitLab features a built-in Docker registry which will be used per default - all\nyou need to do is enable the registry for your vrnetlab project. The necessary\ninformation will be exposed as env vars in GitLab CI which is picked up by the\nbuild config.\n\nThe CI runner executing the jobs must have the tag 'vrnetlab'. Make sure this\nrunner supports running VMs (has KVM) and allows the execution of sibling\ndocker containers.\n\nIf you want, you can use an external docker registry by explicitly configuring\nthe following environment variables:\n\n * DOCKER_USER - the username to authenticate to the docker registry with\n * DOCKER_PASSWORD - the password to authenticate to the docker registry with\n * DOCKER_REGISTRY - the URL to the docker registry, like reg.example.com:5000\n\nNext you need to add the actual virtual router images to the git repository.\nYou can create a separate branch where you add the images as to avoid potential\ngit merge issues. I recommend using LFS:\n```\ngit checkout -b images\ngit lfs track \"*.vmdk\"\ngit add xrv/iosxrv-k9-demo-6.0.0.vmdk .gitattributes\ngit commit -a -m \"Added Cisco XRv 6.0.0 image\"\ngit push your-git-repo images\n```\nNow CI should build the images and push to wherever $DOCKER_REGISTRY points. If\nyou don't want to use LFS then just skip that command.\n\nWhen new changes are commited to the upstream repo/master you can just rebase\nyour branch on top of that:\n```\ngit checkout master\ngit pull origin master\ngit checkout images\ngit rebase master\ngit push --force your-git-repo images\n```\nNote that you have to force push since you've rewritten git history.\n\nLFS is a way to store large files with git but keeping them out of git. It's\ngreat for the virtual router images as they never change (version is in the\nfilename) and so we don't really need git's version tracking for them. LFS is\nconsiderably faster than plain git. For very large files it is possible to run\ninto LFS timeouts, try setting:\n```\ngit config lfs.dialtimeout 60\n```\n\nBuilding with GitHub Actions\n----------------------------\n\nvrnetlab ships with a .github/workflows/test.yml config file which is primarily\nused in the public upstream repository: https://github.com/vrnetlab/vrnetlab.\n\nThe GitHub Actions runner must support running VMs (has KVM) and allows the\nexecution of sibling docker containers. The first requirement is not met by the\nfree GitHub Actions runners, so we use a self-hosted runner for the VM test\n(thanks [GleSYS](https://glesys.com/) for sponsoring the runner ‚ù§Ô∏è).\n\nThis also solves the issue where to the public nature of the repository we\ncannot include the actual virtual router images in public repository via Git\nLFS. Instead, the images are provided to the build container via a local bind\nvolume mount. This means the router images must be available on the host machine\nwhere the build is running. The images are mounted into the build container at\n`/vrnetlab-images`.\n\nThe self-hosted GitHub Actions runner was set up on a machine using the\ninstructions here:\nhttps://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners/adding-self-hosted-runners\nWe then create a script that reads in the `VRNETLAB_IMAGES` environment variable\n(set up as a repository variable) and creates a named docker volume\n*vrnetlab-images* that bind mounts the host path provided in the variable:\n\n```\n$ cat create-vrnetlab-images-volume.sh\ndocker volume create --opt o=bind --opt type=none --opt device=${VRNETLAB_IMAGES} vrnetlab-images\ndocker volume inspect vrnetlab-images\n```\n\nThen in the runners `.env` file, we instruct the runner to run a script on job\nstartup to create the volume:\n\n```\nACTIONS_RUNNER_HOOK_JOB_STARTED=/path/to/runner/create-vrnetlab-images-volume.sh\n```\n\nThe job container can then mount this volume and access the images in\n`/vrnetlab-images` regardless of the host path.:\n\n```yaml\n  test-vr:\n    runs-on: [\"self-hosted\"]\n    container:\n      image: vrnetlab/ci-builder\n      volumes:\n        - vrnetlab-images:/vrnetlab-images\n```", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 1285, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 516}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "vrnetlab / Cisco Catalyst 8000V Edge Software\n===========================\nThis is the vrnetlab docker image for Cisco Catalyst 8000V Edge Software, or\n'c8000v' for short.\n\nThe Catalyst 8000v platform is a successor to the CSR 1000v. As such, this\nplatform directory 'c8000v' started off as a copy of the 'csr' directory. With\ntime we imagine the two platforms will diverge. One such change is already\nplanned to support using the Catalyst 8000v in one of the two modes:\n- regular,\n- SD-WAN Controller mode (managed by Viptela).\n\nRight now the SD-WAN flavor is still split off because to enable the Controller\nmode you have to effectively boot the router into a completely different mode.\nIn the near future these modifications will be merged back into the 'c8000v'\nplatform that will produce both the regular and sd-wan images.\n\nOn installation of Catalyst 8000v the user is presented with the choice of\noutput, which can be over serial console, a video console or through automatic\ndetection of one or the other. Empirical studies show that the automatic\ndetection is far from infallible and so we force the use of the serial console\nby feeding the VM an .iso image that contains a small bootstrap configuration\nthat sets the output to serial console. This means we have to boot up the VM\nonce to feed it this configuration and then restart it for the changes to take\neffect. Naturally we want to do this in the build process as to avoid having to\nrestart the router once for every time we run the docker image. Unfortunately\ndocker doesn't allow us to run docker build with `--privileged` so there is no\nKVM acceleration making this process excruciatingly slow were it to be performed\nin the docker build phase. Instead we build a basic image using docker build,\nwhich essentially just assembles the required files, then run it with\n`--privileged` to start up the VM and feed it the .iso image. After we are done\nwe shut down the VM and commit this new state into the final docker image. This\nis unorthodox but works and saves us a lot of time.\n\nBuilding the docker image\n-------------------------\nPut the .qcow2 file in this directory and run `make docker-image` and you should\nbe good to go. The resulting image is called `vr-c8000v`. You can tag it with\nsomething else if you want, like `my-repo.example.com/vr-c8000v` and then push\nit to your repo. The tag is the same as the version of the Catalyst 8000v image,\nso if you have c8000v-universalk9.16.04.01.qcow2 your final docker image will be\ncalled `vr-c8000v:16.04.01`\n\nPlease note that you will always need to specify version when starting your\nrouter as the \"latest\" tag is not added to any images since it has no meaning\nin this context.\n\nIt's been tested to boot and respond to SSH with:\n\n * 16.03.01a (c8000v-universalk9.16.03.01a.qcow2)\n * 16.04.01 (c8000v-universalk9.16.04.01.qcow2)\n\nUsage\n-----\n```\ndocker run -d --privileged --name my-c8000v-router vr-c8000v\n```\n\nInterface mapping\n-----------------\nIOS XE 16.03.01 and 16.04.01 does only support 10 interfaces, GigabitEthernet1 is always configured\nas a management interface and then we can only use 9 interfaces for traffic. If you configure vrnetlab\nto use more then 10 the interfaces will be mapped like the table below. \n\nThe following images have been verified to NOT exhibit this behavior\n- c8000v-universalk9.03.16.02.S.155-3.S2-ext.qcow2\n- c8000v-universalk9.03.17.02.S.156-1.S2-std.qcow2\n\n| vr-c8000v | vr-xcon |\n| :---:  |  :---:  |\n| Gi2    | 10      |\n| Gi3    | 1       |\n| Gi4    | 2       |\n| Gi5    | 3       |\n| Gi6    | 4       |\n| Gi7    | 5       |\n| Gi8    | 6       |\n| Gi9    | 7       |\n| Gi10   | 8       |\n| Gi11   | 9       |\n\nSystem requirements\n-------------------\nCPU: 1 core\n\nRAM: 4GB\n\nDisk: <500MB\n\nLicense handling\n----------------\nYou can feed a license file into c8000v by putting a text file containing the\nlicense in this directory next to your .qcow2 image. Name the license file the\nsame as your .qcow2 file but append \".license\", e.g. if you have\n\"c8000v-universalk9.16.04.01.qcow2\" you would name the license file\n\"c8000v-universalk9.16.04.01.qcow2.license\".\n\nThe license is bound to a specific UDI and usually expires within a given time.\nTo make sure that everything works out smoothly we configure the clock to\na specific date during the installation process. This is because the license\nonly has an expiration date not a start date.\n\nThe license unlocks feature and throughput. The default throughput for C8000v is\n20Mbit/s which is perfectly for basic management and testing.\n\nFUAQ - Frequently or Unfrequently Asked Questions\n-------------------------------------------------\n##### Q: Has this been extensively tested?\nA: Nope. \n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": "c8000v/README.md", "language": "markdown", "complexity": "medium", "star_count": 1285, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 110}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\n\nimport datetime\nimport logging\nimport os\nimport re\nimport signal\nimport subprocess\nimport sys\nimport telnetlib\nimport time\n\nimport vrnetlab\n\ndef handle_SIGCHLD(signal, frame):\n    os.waitpid(-1, os.WNOHANG)\n\ndef handle_SIGTERM(signal, frame):\n    sys.exit(0)\n\nsignal.signal(signal.SIGINT, handle_SIGTERM)\nsignal.signal(signal.SIGTERM, handle_SIGTERM)\nsignal.signal(signal.SIGCHLD, handle_SIGCHLD)\n\nTRACE_LEVEL_NUM = 9\nlogging.addLevelName(TRACE_LEVEL_NUM, \"TRACE\")\ndef trace(self, message, *args, **kws):\n    # Yes, logger takes its '*args' as 'args'.\n    if self.isEnabledFor(TRACE_LEVEL_NUM):\n        self._log(TRACE_LEVEL_NUM, message, args, **kws)\nlogging.Logger.trace = trace\n\n\n\nclass C8000v_vm(vrnetlab.VM):\n    def __init__(self, username, password, install_mode=False):\n        disk_image = None\n        for e in sorted(os.listdir(\"/\")):\n            if not disk_image and re.search(\".qcow2$\", e):\n                disk_image = \"/\" + e\n            if re.search(r\"\\.license$\", e):\n                os.rename(\"/\" + e, \"/tftpboot/license.lic\")\n\n        self.license = False\n        if os.path.isfile(\"/tftpboot/license.lic\"):\n            logger.info(\"License found\")\n            self.license = True\n\n        super(C8000v_vm, self).__init__(username, password, disk_image=disk_image)\n        self.nic_type = \"vmxnet3\"\n        self.install_mode = install_mode\n        self.num_nics = 9\n\n        if self.install_mode:\n            logger.trace(\"install mode\")\n            self.image_name = \"config.iso\"\n            self.create_boot_image()\n\n            self.qemu_args.extend([\"-cdrom\", \"/\" +self.image_name]) \n\n\n    def create_boot_image(self):\n        \"\"\" Creates a iso image with a bootstrap configuration\n        \"\"\"\n\n        cfg_file = open('/iosxe_config.txt', 'w')\n        if self.license:\n            cfg_file.write(\"do clock set 13:33:37 1 Jan 2010\\r\\n\")\n            cfg_file.write(\"interface GigabitEthernet1\\r\\n\")\n            cfg_file.write(\"ip address 10.0.0.15 255.255.255.0\\r\\n\")\n            cfg_file.write(\"no shut\\r\\n\")\n            cfg_file.write(\"exit\\r\\n\")\n            cfg_file.write(\"license accept end user agreement\\r\\n\")\n            cfg_file.write(\"yes\\r\\n\")\n            cfg_file.write(\"do license install tftp://10.0.0.2/license.lic\\r\\n\\r\\n\")\n        cfg_file.write(\"license boot level network-premier addon dna-premier\\r\\n\")\n        cfg_file.write(\"platform console serial\\r\\n\\r\\n\")\n        cfg_file.write(\"do clear platform software vnic-if nvtable\\r\\n\")\n        cfg_file.write(\"do wr\\r\\n\")\n        cfg_file.write(\"do reload\\r\\n\")\n        cfg_file.close()\n\n        genisoimage_args = [\"genisoimage\", \"-l\", \"-o\", \"/\" + self.image_name, \"/iosxe_config.txt\"]\n\n        subprocess.Popen(genisoimage_args)\n\n\n    def bootstrap_spin(self):\n        \"\"\" This function should be called periodically to do work.\n        \"\"\"\n\n        if self.spins > 300:\n            # too many spins with no result ->  give up\n            self.stop()\n            self.start()\n            return\n\n        (ridx, match, res) = self.tn.expect([b\"Press RETURN to get started!\", b\"IOSXEBOOT-4-FACTORY_RESET\"], 1)\n        if match: # got a match!\n            if ridx == 0: # login\n                self.logger.debug(\"matched, Press RETURN to get started.\")\n                if self.install_mode:\n                    self.logger.debug(\"Now we wait for the device to reload\")\n                else:\n                    self.wait_write(\"\", wait=None)\n\n                    # run main config!\n                    self.bootstrap_config()\n                    # close telnet connection\n                    self.tn.close()\n                    # startup time?\n                    startup_time = datetime.datetime.now() - self.start_time\n                    self.logger.info(\"Startup complete in: %s\" % startup_time)\n                    # mark as running\n                    self.running = True\n                    return\n            elif ridx == 1: # IOSXEBOOT-4-FACTORY_RESET\n                if self.install_mode:\n                    install_time = datetime.datetime.now() - self.start_time\n                    self.logger.info(\"Install complete in: %s\" % install_time)\n                    self.running = True\n                    return\n                else:\n                    self.log.warning(\"Unexpected reload while running\")\n\n        # no match, if we saw some output from the router it's probably\n        # booting, so let's give it some more time\n        if res != b'':\n            self.logger.trace(\"OUTPUT: %s\" % res.decode())\n            # reset spins if we saw some output\n            self.spins = 0\n\n        self.spins += 1\n\n        return\n\n\n    def bootstrap_config(self):\n        \"\"\" Do the actual bootstrap config\n        \"\"\"\n        self.logger.info(\"applying bootstrap configuration\")\n\n        self.wait_write(\"\", None)\n        self.wait_write(\"enable\", wait=\">\")\n        self.wait_write(\"configure terminal\", wait=\">\")\n\n        self.wait_write(\"hostname c8000v\")\n        self.wait_write(\"username %s privilege 15 password %s\" % (self.username, self.password))\n        if int(self.version.split('.')[0]) >= 16:\n           self.wait_write(\"ip domain name example.com\")\n        else:\n           self.wait_write(\"ip domain-name example.com\")\n        self.wait_write(\"crypto key generate rsa modulus 2048\")\n\n        self.wait_write(\"interface GigabitEthernet1\")\n        self.wait_write(\"ip address 10.0.0.15 255.255.255.0\")\n        self.wait_write(\"no shut\")\n        self.wait_write(\"exit\")\n        self.wait_write(\"restconf\")\n        self.wait_write(\"netconf-yang\")\n        self.wait_write(\"netconf max-sessions 16\")\n        # I did not find any documentation about this, but is seems like a good idea!?\n        self.wait_write(\"netconf detailed-error\")\n        self.wait_write(\"ip ssh server algorithm mac hmac-sha2-512\")\n        self.wait_write(\"ip ssh maxstartups 128\")\n\n        self.wait_write(\"line vty 0 4\")\n        self.wait_write(\"login local\")\n        self.wait_write(\"transport input all\")\n        self.wait_write(\"end\")\n        self.wait_write(\"copy running-config startup-config\")\n        self.wait_write(\"\\r\", \"Destination\")\n\n\nclass C8000v(vrnetlab.VR):\n    def __init__(self, username, password):\n        super(C8000v, self).__init__(username, password)\n        self.vms = [ C8000v_vm(username, password) ]\n\n\nclass C8000v_installer(C8000v):\n    \"\"\" C8000v installer\n        \n        Will start the C8000v with a mounted iso to make sure that we get\n        console output on serial, not vga.\n    \"\"\"\n    def __init__(self, username, password):\n        super(C8000v, self).__init__(username, password)\n        self.vms = [ C8000v_vm(username, password, install_mode=True)  ]\n\n    def install(self):\n        self.logger.info(\"Installing C8000v\")\n        csr = self.vms[0]\n        while not csr.running:\n            csr.work()\n        csr.stop()\n        self.logger.info(\"Installation complete\")\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='')\n    parser.add_argument('--trace', default=vrnetlab.bool_from_env('TRACE'), action='store_true', help='enable trace level logging')\n    parser.add_argument('--username', default=os.getenv('USERNAME', 'vrnetlab'), help='Username')\n    parser.add_argument('--password', default=os.getenv('PASSWORD', 'VR-netlab9'), help='Password')\n    parser.add_argument('--install', default=vrnetlab.bool_from_env('INSTALL'), action='store_true', help='Install C8000v')\n    args = parser.parse_args()\n\n    LOG_FORMAT = \"%(asctime)s: %(module)-10s %(levelname)-8s %(message)s\"\n    logging.basicConfig(format=LOG_FORMAT)\n    logger = logging.getLogger()\n\n    logger.setLevel(logging.DEBUG)\n    if args.trace:\n        logger.setLevel(1)\n\n    if args.install:\n        vr = C8000v_installer(args.username, args.password)\n        vr.install()\n    else:\n        vr = C8000v(args.username, args.password)\n        vr.start()\n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": "c8000v/docker/launch.py", "language": "python", "complexity": "complex", "star_count": 1285, "quality_score": 0.84, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 223}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\n\nimport datetime\nimport json\nimport logging\nimport math\nimport os\nimport random\nimport re\nimport subprocess\nimport telnetlib\nimport time\n\nMAX_RETRIES=60\n\n# Global list of ports that we want to set up forwarding from container IP ->\n# mgmt IP of router (usually 10.0.0.15). Each entry consists of the protocol,\n# the source port (on the outside of the container) and the destination port\n# (on the virtual router).\nHOST_FWDS = [\n    ('tcp', 22, 22),      # SSH\n    ('udp', 161, 161),    # SNMP\n    ('tcp', 830, 830),    # NETCONF\n    ('tcp', 80, 80),      # HTTP\n    ('tcp', 443, 443),    # HTTPS\n]\n\ndef gen_mac(last_octet=None):\n    \"\"\" Generate a random MAC address that is in the qemu OUI space and that\n        has the given last octet.\n    \"\"\"\n    return \"52:54:00:%02x:%02x:%02x\" % (\n            random.randint(0x00, 0xff),\n            random.randint(0x00, 0xff),\n            last_octet\n        )\n\n\n\ndef run_command(cmd, cwd=None, background=False):\n    res = None\n    try:\n        if background:\n            p = subprocess.Popen(cmd, cwd=cwd)\n        else:\n            p = subprocess.Popen(cmd, stdout=subprocess.PIPE, cwd=cwd)\n            res = p.communicate()\n    except:\n        pass\n    return res\n\n\ndef bool_from_env(env_var: str, default: bool=False):\n    \"\"\"Convert environment variable to boolean\n\n    For example, 'True', 'true', '1' and 'yes' are all considered True.\n    \"\"\"\n    return os.getenv(env_var, str(default)).lower() in ['true', '1', 'yes']\n\n\ndef list_from_env(env_var: str, default: list=[]):\n    \"\"\"Convert environment variable to list\n\n    The environment variable should be a space separate list of values.\n    \"\"\"\n    return os.getenv(env_var, ' '.join(default)).split()\n\n\nclass VM:\n    def __str__(self):\n        # TODO: use this in the logger?!\n        return f\"{self.__class__.__name__}[{self.num}]\"\n\n\n    def __init__(self, username, password, disk_image=None, num=0, ram=4096):\n        self.logger = logging.getLogger()\n        self.start_time = datetime.datetime.now()\n\n        # username / password to configure\n        self.username = username\n        self.password = password\n\n        self.num = num\n        self.image = disk_image\n\n        self.running = False\n        self.spins = 0\n        self.p = None\n        self.tn = None\n        self.qm = None\n\n        #  various settings\n        self.uuid = None\n        self.fake_start_date = None\n        self.nic_type = \"e1000\"\n        self.num_nics = 0\n        self.nics_per_pci_bus = 26 # tested to work with XRv\n        self.smbios = []\n\n        self.qemu_args = [\"qemu-system-x86_64\", \"-display\", \"none\", \"-machine\", \"pc\" ]\n        self.qemu_args.extend([\"-monitor\", \"tcp:0.0.0.0:40%02d,server,nowait\" % self.num])\n        self.qemu_args.extend([\"-m\", str(ram),\n                               \"-serial\", \"telnet:0.0.0.0:50%02d,server,nowait\" % self.num])\n        self.pre_start_cmds, overlay_qemu_args = self.create_overlay_image()\n        self.qemu_args.extend(overlay_qemu_args)\n        # enable hardware assist if KVM is available\n        if os.path.exists(\"/dev/kvm\"):\n            self.qemu_args.insert(1, '-enable-kvm')\n\n\n\n    def start(self):\n        self.logger.info(\"Starting %s\" % self)\n\n        cmd = list(self.qemu_args)\n\n        # uuid\n        if self.uuid:\n            cmd.extend([\"-uuid\", self.uuid])\n\n        # do we have a fake start date?\n        if self.fake_start_date:\n            cmd.extend([\"-rtc\", \"base=\" + self.fake_start_date])\n\n        # smbios\n        for e in self.smbios:\n            cmd.extend([\"-smbios\", e])\n\n        # setup PCI buses\n        for i in range(1, math.ceil(self.num_nics / self.nics_per_pci_bus) + 1):\n            cmd.extend([\"-device\", \"pci-bridge,chassis_nr={},id=pci.{}\".format(i, i)])\n\n        # generate mgmt NICs\n        cmd.extend(self.gen_mgmt())\n        # generate normal NICs\n        cmd.extend(self.gen_nics())\n\n        self.logger.debug(cmd)\n\n        # run pre-start-cmds before starting QEMU\n        if self.pre_start_cmds:\n            for pre_start_cmd in self.pre_start_cmds:\n                self.logger.info(f\"Running pre-start-cmd: {pre_start_cmd}\")\n                res = run_command(pre_start_cmd)\n                self.logger.debug(f\"Result: {res}\")\n\n        self.p = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE, universal_newlines=True)\n\n        try:\n            outs, errs = self.p.communicate(timeout=2)\n            self.logger.info(\"STDOUT: %s\" % outs)\n            self.logger.info(\"STDERR: %s\" % errs)\n        except:\n            pass\n\n        for i in range(1, MAX_RETRIES+1):\n            try:\n                self.qm = telnetlib.Telnet(\"127.0.0.1\", 4000 + self.num)\n                break\n            except:\n                self.logger.info(\"Unable to connect to qemu monitor (port {}), retrying in a second (attempt {})\".format(4000 + self.num, i))\n                time.sleep(1)\n            if i == MAX_RETRIES:\n                raise QemuBroken(\"Unable to connect to qemu monitor on port {}\".format(4000 + self.num))\n\n        for i in range(1, MAX_RETRIES+1):\n            try:\n                self.tn = telnetlib.Telnet(\"127.0.0.1\", 5000 + self.num)\n                break\n            except:\n                self.logger.info(\"Unable to connect to qemu monitor (port {}), retrying in a second (attempt {})\".format(5000 + self.num, i))\n                time.sleep(1)\n            if i == MAX_RETRIES:\n                raise QemuBroken(\"Unable to connect to qemu monitor on port {}\".format(5000 + self.num))\n        try:\n            outs, errs = self.p.communicate(timeout=2)\n            self.logger.info(\"STDOUT: %s\" % outs)\n            self.logger.info(\"STDERR: %s\" % errs)\n        except:\n            pass\n\n    def gen_host_forwards(self, mgmt_ip='10.0.0.15', offset=2000):\n        \"\"\"Generate the host forward argument for qemu\n        HOST_FWDS contain the ports we want to forward and allows mapping a\n        container (source) port to a different destination port on the VR/VM.\n        We do a straight mapping here and let socat do the port mapping. Since\n        multiple source ports can be mapped to the same destination port, we\n        first unique the set of ports.\n        \"\"\"\n        fwd_ports = {(proto, dst_port) for proto, src_port, dst_port in HOST_FWDS}\n        # hostfwd=tcp::2022-10.0.0.15:22,...\n        return \",\".join(\"hostfwd=%s::%d-%s:%d\" % (proto, port + offset, mgmt_ip, port) for proto, port in fwd_ports)\n\n\n    def gen_mgmt(self):\n        \"\"\" Generate qemu args for the mgmt interface(s)\n        \"\"\"\n        res = []\n        # mgmt interface is special - we use qemu user mode network\n        res.append(\"-device\")\n        # vEOS-lab requires its Ma1 interface to be the first in the bus, so let's hardcode it\n        if 'vEOS-lab' in self.image:\n            res.append(self.nic_type + \",netdev=p%(i)02d,mac=%(mac)s,bus=pci.1,addr=0x2\"\n                       % { 'i': 0, 'mac': gen_mac(0) })\n        else:\n            res.append(self.nic_type + \",netdev=p%(i)02d,mac=%(mac)s\"\n                       % { 'i': 0, 'mac': gen_mac(0) })\n        res.append(\"-netdev\")\n        res.append(\"user,id=p%(i)02d,net=10.0.0.0/24,tftp=/tftpboot,%(hostfwd)s\" % { 'i': 0, 'hostfwd': self.gen_host_forwards() })\n\n        return res\n\n\n    def gen_nics(self):\n        \"\"\" Generate qemu args for the normal traffic carrying interface(s)\n        \"\"\"\n        res = []\n        # vEOS-lab requires its Ma1 interface to be the first in the bus, so start normal nics at 2\n        if 'vEOS-lab' in self.image:\n            range_start = 2\n        else:\n            range_start = 1\n        for i in range(range_start, self.num_nics+1):\n            # calc which PCI bus we are on and the local add on that PCI bus\n            pci_bus = math.floor(i/self.nics_per_pci_bus) + 1\n            addr = (i % self.nics_per_pci_bus) + 1\n\n            res.append(\"-device\")\n            res.append(\"%(nic_type)s,netdev=p%(i)02d,mac=%(mac)s,bus=pci.%(pci_bus)s,addr=0x%(addr)x\" % {\n                       'nic_type': self.nic_type,\n                       'i': i,\n                       'pci_bus': pci_bus,\n                       'addr': addr,\n                       'mac': gen_mac(i)\n                    })\n            res.append(\"-netdev\")\n            res.append(\"socket,id=p%(i)02d,listen=:%(j)02d\"\n                       % { 'i': i, 'j': i + 10000 })\n        return res\n\n    @property\n    def overlay_disk_image(self) -> str:\n        \"\"\"Generate the overlay disk image name for VM instance\n\n        The overlay image name is derived from the base image name and the num\n        attribute (unique per VM instance).  For example the SROS_lc(slot=1)\n        first linecard uses the name sros-1-overlay.qcow2. This ensures that\n        each VM gets its own overlay.\n        \"\"\"\n        return re.sub(r'(\\.[^.]+$)', fr'-{self.num}-overlay\\1', self.image)\n\n    def _overlay_disk_image_format(self) -> str:\n        res = run_command([\"qemu-img\", \"info\", \"--output\", \"json\", self.image])\n        if res is not None:\n            image_info = json.loads(res[0])\n            if \"format\" in image_info:\n                return image_info[\"format\"]\n        raise ValueError(f\"Could not read image format for {self.image}\")\n\n    def create_overlay_image(self):\n        \"\"\"Creates an overlay disk image\n\n        *Always* create the overlay image. Return a tuple of pre-start-cmds and\n        an array of parameters to extend qemu_args. A subclass may want to\n        override this for using specific drive id. If the overlay image already\n        exists raise an exception.\n        \"\"\"\n        if os.path.exists(self.overlay_disk_image):\n            raise Exception(f\"Overlay image {self.overlay_disk_image} already exists for base {self.image}\")\n        self.logger.debug(f\"Adding creation of overlay disk image {self.overlay_disk_image} with base {self.image} to pre_start_cmds\")\n        format = self._overlay_disk_image_format()\n        pre_start_cmds = [\"qemu-img\", \"create\", \"-f\", \"qcow2\", \"-F\", format, \"-b\", self.image, self.overlay_disk_image]\n        return [pre_start_cmds], [\"-drive\", \"if=ide,file=%s\" % self.overlay_disk_image]\n\n    def stop(self):\n        \"\"\" Stop this VM\n        \"\"\"\n        self.running = False\n\n        try:\n            self.p.terminate()\n        except ProcessLookupError:\n            return\n\n        try:\n            self.p.communicate(timeout=10)\n        except:\n            try:\n                # this construct is included as an example at\n                # https://docs.python.org/3.6/library/subprocess.html but has\n                # failed on me so wrapping in another try block. It was this\n                # communicate() that failed with:\n                # ValueError: Invalid file object: <_io.TextIOWrapper name=3 encoding='ANSI_X3.4-1968'>\n                self.p.kill()\n                self.p.communicate(timeout=10)\n            except:\n                # just assume it's dead or will die?\n                self.p.wait(timeout=10)\n\n    def restart(self):\n        \"\"\" Restart this VM\n\n        Also removes the overlay disk image, effectively restoring the state of\n        the VM to initial image.\n        \"\"\"\n        self.stop()\n        if os.path.exists(self.overlay_disk_image):\n            os.remove(self.overlay_disk_image)\n        self.start()\n\n\n    def wait_write(self, cmd, wait='#', con=None):\n        \"\"\" Wait for something on the serial port and then send command\n\n            Defaults to using self.tn as connection but this can be overridden\n            by passing a telnetlib.Telnet object in the con argument.\n        \"\"\"\n        con_name = 'custom con'\n        if con is None:\n            con = self.tn\n\n        if con == self.tn:\n            con_name = 'serial console'\n        if con == self.qm:\n            con_name = 'qemu monitor'\n\n        if wait:\n            self.logger.trace(\"waiting for '%s' on %s\" % (wait, con_name))\n            res = con.read_until(wait.encode())\n            self.logger.trace(\"read from %s: %s\" % (con_name, res.decode()))\n        self.logger.debug(\"writing to %s: %s\" % (con_name, cmd))\n        con.write(\"{}\\r\".format(cmd).encode())\n\n\n    def work(self):\n        self.check_qemu()\n        if not self.running:\n            try:\n                self.bootstrap_spin()\n            except EOFError:\n                self.logger.error(\"Telnet session was disconnected, restarting\")\n                self.restart()\n\n    def bootstrap_spin(self):\n        raise NotImplementedError()\n\n    def check_qemu(self):\n        \"\"\" Check health of qemu. This is mostly just seeing if there's error\n            output on STDOUT from qemu which means we restart it.\n        \"\"\"\n        if self.p is None:\n            self.logger.debug(\"VM not started; starting!\")\n            self.start()\n\n        # check for output\n        try:\n            outs, errs = self.p.communicate(timeout=1)\n        except subprocess.TimeoutExpired:\n            return\n        self.logger.info(\"STDOUT: %s\" % outs)\n        self.logger.info(\"STDERR: %s\" % errs)\n\n        if errs != \"\":\n            self.logger.debug(\"KVM error, restarting\")\n            self.stop()\n            self.start()\n\n    def wait_config(self, show_cmd, expect, spins=90):\n        \"\"\" Some configuration takes some time to \"show up\".\n            To make sure the device is really ready, wait here.\n        \"\"\"\n        self.logger.debug('waiting for {} to appear in {}'.format(expect, show_cmd))\n        wait_spins = 0\n        # 10s * 90 = 900s = 15min timeout\n        while wait_spins < spins:\n            # On some devices (Huawei VRP), the command to disable paging\n            # only has a temporary effect?!\n            # To make sure we're not getting paged output, send the no_paging_command\n            # always, if the attribute exists on the extended VM class.\n            try:\n                self.wait_write(self.no_paging_command, wait=None)\n            except AttributeError:\n                pass\n            self.wait_write(show_cmd, wait=None)\n            _, match, data = self.tn.expect([expect.encode('UTF-8')], timeout=10)\n            self.logger.trace(data.decode('UTF-8'))\n            if match:\n                self.logger.debug('a wild {} has appeared!'.format(expect))\n                return True\n            wait_spins += 1\n        self.logger.error('{} not found in {}'.format(expect, show_cmd))\n        return False\n\n    @property\n    def version(self):\n        \"\"\"Read version number from VERSION environment variable\n\n        The VERSION environment variable is set at build time using the value\n        from the makefile. If the environment variable is not defined please add\n        the variables in the Dockerfile (see csr)\"\"\"\n        version = os.environ.get(\"VERSION\")\n        if version is not None:\n            return version\n        raise ValueError(\"The VERSION environment variable is not set\")\n\n\nclass VR:\n    def __init__(self, username, password):\n        self.logger = logging.getLogger()\n        self.vms = []\n\n        try:\n            os.mkdir(\"/tftpboot\")\n        except:\n            pass\n\n    def update_health(self, exit_status, message):\n        health_file = open(\"/health\", \"w\")\n        health_file.write(\"%d %s\" % (exit_status, message))\n        health_file.close()\n\n    def start_socat(self, src_offset=0, dst_offset=2000):\n        for proto, src_port, dst_port in HOST_FWDS:\n            # TCP6-LISTEN and UDP6-LISTEN are actually dual-stack and will work\n            # for an IPv4 only host too\n            run_command([\"socat\", \"%s6-LISTEN:%d,fork\" % (proto.upper(), src_port + src_offset),\n                         \"%s:127.0.0.1:%d\" % (proto.upper(), dst_port + dst_offset)],\n                         background=True)\n\n    def start(self):\n        \"\"\" Start the virtual router\n        \"\"\"\n        self.logger.debug(\"Starting vrnetlab %s\", self)\n        self.logger.debug(\"VMs: %s\", self.vms)\n        self.start_socat()\n\n        started = False\n        while True:\n            all_running = True\n            for vm in self.vms:\n                vm.work()\n                if not vm.running:\n                    all_running = False\n\n            if all_running:\n                self.update_health(0, \"running\")\n                started = True\n            else:\n                if started:\n                    self.update_health(1, \"VM failed - restarting\")\n                else:\n                    self.update_health(1, \"starting\")\n\nclass VR_Installer:\n    def __init__(self):\n        self.logger = logging.getLogger()\n        self.vm = None\n\n    def install(self):\n        vm =  self.vm\n        while not vm.running:\n            self.logger.trace(\"%s working\", self)\n            vm.work()\n        self.logger.debug(\"%s running, shutting down\", self)\n        vm.stop()\n        self.logger.info(\"Installation complete\")\n\nclass QemuBroken(Exception):\n    \"\"\" Our Qemu instance is somehow broken\n    \"\"\"\n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": "common/vrnetlab.py", "language": "python", "complexity": "complex", "star_count": 1285, "quality_score": 0.8999999999999999, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 472}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "vrnetlab Config Engine lite\n===========================\nConfig Engine lite is a small provisioning system shipped with vrnetlab,\nprimarily written for three use cases:\n\n * configure routers in a vrnetlab topology such that the functionality of\n   vrnetlab itself can be tested, for example, we want to make sure that\n   interfaces are correctly mapped\n * accelerate labing. If you want to do some specific iBGP testing you might\n   not be all too interested in setting IP addresses on the 7 routers required\n   for your test or configure an entire IGP - use config engine to quickly\n   provision the basics and do the rest by hand!\n * serve as inspiration for how you can write a provisioning system running\n\nIt's called 'lite' since it doesn't aspire to become a full blown provisioning\nsystem. While it might grow and gain new functionality it will always be\ntargeted for the requirements of the above, in particular the testing of\nvrnetlab itself.\n\nUsage\n-----\nAfter building the docker image, you run it like this. There are two modes of operation, topology mode and single-router-mode. \n\n\n### Topology mode\nUse config-engine-lite and jinja2 templates to configure your topomachine topology.\n```\ndocker run -v $(pwd)/templates:/templates -v $(pwd)/topology:/topology --link router1 --link router2 vr-configengine --topo /topology/lltopo.json --xr /templates/xr.j2 --junos /templates/junos.j2 --run\n```\n\n * -v $(pwd)/templates:/templates - Mount a directory containing your templates inside the container\n * -v $(pwd)/topology/topology - Mount a directory containing your topology files inside the container\n * --link router1 --link router2 - Link all routers specified in your topology, enabling config-engine-lite to configure them\n * --topo /topology/lltopo.json - The low level topology built by topology-machine, This references to the /topology mountpoint\n * --ios /templates/ios.j2 - Configuration template for IOS (CSR 1000v), this references to the /templates mountpoint\n * --xr /templates/xr.j2 - Configuration template for ios-xr, this references to the /templates mountpoint\n * --junos /templates/junos.j2 - Configuration template for JunOS, this refrences to the /templates mountpoint\n * --run - Actually deploy the configuration. If this is not specified, the configuration changes will not be committed and config diff will be printed.\n\n### Single-router mode\nApply a configuration template to a single router, useful for bootstrapping a router for use with vr-bgp for instance.\n```\ndocker run -v $(pwd)/templates:/templates --link router1 vr-configengine --type xrv --router router1 --config /templates/router1.j2 --attrs \"key1=value1,key2=value2\"\n```\n * -v $(pwd)/templates:/templates - Mount a directory containing your templates inside the container\n * --link router1 - Link the router you want to configure\n * --config /templates/router1.j2 - Your router configuration, references /templates moutpoint\n * --type vmx - Type of router to configure (valid values are vmx, xrv and csr)\n * --attr \"key=value\" - A key/value pair available in the template, can be specified multiple times.\n\n### Common parameters\nThese parameters are available in both modes\n * --wait-for-boot - Block until we can connect to the router via SSH. If neither --diff or --run is used this option will simply block until all your routers are started\n * --diff - Print configuration diff and discard the configuration\n * --run - Commit the configuration to the router\n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": "config-engine-lite/README.md", "language": "markdown", "complexity": "medium", "star_count": 1285, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 56}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "vrnetlab / Cisco CSR1000v\n===========================\nThis is the vrnetlab docker image for Cisco CSR1000v.\n\nOn installation of CSR1000v the user is presented with the choice of output,\nwhich can be over serial console, a video console or through automatic\ndetection of one or the other. Empirical studies show that the automatic\ndetection is far from infallible and so we force the use of the serial console\nby feeding the VM an .iso image that contains a small bootstrap configuration\nthat sets the output to serial console. This means we have to boot up the VM\nonce to feed it this configuration and then restart it for the changes to take\neffect. Naturally we want to do this in the build process as to avoid having to\nrestart the router once for every time we run the docker image. Unfortunately\ndocker doesn't allow us to run docker build with `--privileged` so there is no\nKVM acceleration making this process excruciatingly slow were it to be\nperformed in the docker build phase. Instead we build a basic image using\ndocker build, which essentially just assembles the required files, then run it\nwith `--privileged` to start up the VM and feed it the .iso image. After we are\ndone we shut down the VM and commit this new state into the final docker image.\nThis is unorthodox but works and saves us a lot of time.\n\nBuilding the docker image\n-------------------------\nPut the .qcow2 file in this directory and run `make docker-image` and\nyou should be good to go. The resulting image is called `vr-csr`. You can tag\nit with something else if you want, like `my-repo.example.com/vr-csr` and then\npush it to your repo. The tag is the same as the version of the CSR image, so\nif you have csr1000v-universalk9.16.04.01.qcow2 your final docker image will be called\nvr-csr:16.04.01\n\nPlease note that you will always need to specify version when starting your\nrouter as the \"latest\" tag is not added to any images since it has no meaning\nin this context.\n\nIt's been tested to boot and respond to SSH with:\n\n * 16.03.01a (csr1000v-universalk9.16.03.01a.qcow2)\n * 16.04.01 (csr1000v-universalk9.16.04.01.qcow2)\n\nUsage\n-----\n```\ndocker run -d --privileged --name my-csr-router vr-csr\n```\n\nInterface mapping\n-----------------\nIOS XE 16.03.01 and 16.04.01 does only support 10 interfaces, GigabitEthernet1 is always configured\nas a management interface and then we can only use 9 interfaces for traffic. If you configure vrnetlab\nto use more then 10 the interfaces will be mapped like the table below. \n\nThe following images have been verified to NOT exhibit this behavior\n- csr1000v-universalk9.03.16.02.S.155-3.S2-ext.qcow2\n- csr1000v-universalk9.03.17.02.S.156-1.S2-std.qcow2\n\n| vr-csr | vr-xcon |\n| :---:  |  :---:  |\n| Gi2    | 10      |\n| Gi3    | 1       |\n| Gi4    | 2       |\n| Gi5    | 3       |\n| Gi6    | 4       |\n| Gi7    | 5       |\n| Gi8    | 6       |\n| Gi9    | 7       |\n| Gi10   | 8       |\n| Gi11   | 9       |\n\nSystem requirements\n-------------------\nCPU: 1 core\n\nRAM: 4GB\n\nDisk: <500MB\n\nLicense handling\n----------------\nYou can feed a license file into CSR1000V by putting a text file containing the\nlicense in this directory next to your .qcow2 image. Name the license file the\nsame as your .qcow2 file but append \".license\", e.g. if you have\n\"csr1000v-universalk9.16.04.01.qcow2\" you would name the license file\n\"csr1000v-universalk9.16.04.01.qcow2.license\".\n\nThe license is bound to a specific UDI and usually expires within a given time.\nTo make sure that everything works out smoothly we configure the clock to\na specific date during the installation process. This is because the license\nonly has an expiration date not a start date.\n\nThe license unlocks feature and throughput, the default throughput\nfor CSR is 100Kbit/s and is totally useless if you want to configure the device\nwith a fairly large configuration.\n\nFUAQ - Frequently or Unfrequently Asked Questions\n-------------------------------------------------\n##### Q: Has this been extensively tested?\nA: Nope. \n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": "csr/README.md", "language": "markdown", "complexity": "medium", "star_count": 1285, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 98}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\n\nimport datetime\nimport logging\nimport os\nimport re\nimport signal\nimport subprocess\nimport sys\nimport telnetlib\nimport time\n\nimport vrnetlab\n\ndef handle_SIGCHLD(signal, frame):\n    os.waitpid(-1, os.WNOHANG)\n\ndef handle_SIGTERM(signal, frame):\n    sys.exit(0)\n\nsignal.signal(signal.SIGINT, handle_SIGTERM)\nsignal.signal(signal.SIGTERM, handle_SIGTERM)\nsignal.signal(signal.SIGCHLD, handle_SIGCHLD)\n\nTRACE_LEVEL_NUM = 9\nlogging.addLevelName(TRACE_LEVEL_NUM, \"TRACE\")\ndef trace(self, message, *args, **kws):\n    # Yes, logger takes its '*args' as 'args'.\n    if self.isEnabledFor(TRACE_LEVEL_NUM):\n        self._log(TRACE_LEVEL_NUM, message, args, **kws)\nlogging.Logger.trace = trace\n\n\n\nclass CSR_vm(vrnetlab.VM):\n    def __init__(self, username, password, install_mode=False):\n        disk_image = None\n        for e in sorted(os.listdir(\"/\")):\n            if not disk_image and re.search(\".qcow2$\", e):\n                disk_image = \"/\" + e\n            if re.search(r\"\\.license$\", e):\n                os.rename(\"/\" + e, \"/tftpboot/license.lic\")\n\n        self.license = False\n        if os.path.isfile(\"/tftpboot/license.lic\"):\n            logger.info(\"License found\")\n            self.license = True\n\n        super(CSR_vm, self).__init__(username, password, disk_image=disk_image)\n        self.nic_type = \"virtio-net-pci\"\n        self.install_mode = install_mode\n        self.num_nics = 9\n\n        if self.install_mode:\n            logger.trace(\"install mode\")\n            self.image_name = \"config.iso\"\n            self.create_boot_image()\n\n            self.qemu_args.extend([\"-cdrom\", \"/\" +self.image_name]) \n\n\n    def create_boot_image(self):\n        \"\"\" Creates a iso image with a bootstrap configuration\n        \"\"\"\n\n        cfg_file = open('/iosxe_config.txt', 'w')\n        if self.license:\n            cfg_file.write(\"do clock set 13:33:37 1 Jan 2010\\r\\n\")\n            cfg_file.write(\"interface GigabitEthernet1\\r\\n\")\n            cfg_file.write(\"ip address 10.0.0.15 255.255.255.0\\r\\n\")\n            cfg_file.write(\"no shut\\r\\n\")\n            cfg_file.write(\"exit\\r\\n\")\n            cfg_file.write(\"license accept end user agreement\\r\\n\")\n            cfg_file.write(\"yes\\r\\n\")\n            cfg_file.write(\"do license install tftp://10.0.0.2/license.lic\\r\\n\\r\\n\")\n\n        cfg_file.write(\"platform console serial\\r\\n\\r\\n\")\n        cfg_file.write(\"do wr\\r\\n\")\n        cfg_file.write(\"do reload\\r\\n\")\n        cfg_file.close()\n\n        genisoimage_args = [\"genisoimage\", \"-l\", \"-o\", \"/\" + self.image_name, \"/iosxe_config.txt\"]\n\n        subprocess.Popen(genisoimage_args)\n\n\n    def bootstrap_spin(self):\n        \"\"\" This function should be called periodically to do work.\n        \"\"\"\n\n        if self.spins > 300:\n            # too many spins with no result ->  give up\n            self.stop()\n            self.start()\n            return\n\n        (ridx, match, res) = self.tn.expect([b\"Press RETURN to get started!\"], 1)\n        if match: # got a match!\n            if ridx == 0: # login\n                if self.install_mode:\n                    self.wait_write(\"\", wait=None)\n                    self.wait_write(\"\", None)\n                    self.wait_write(\"enable\", wait=\">\")\n                    self.wait_write(\"clear platform software vnic-if nvtable\")\n                    self.wait_write(\"\")\n                    self.running = True\n                    return\n\n                self.logger.debug(\"matched, Press RETURN to get started.\")\n                self.wait_write(\"\", wait=None)\n\n                # run main config!\n                self.bootstrap_config()\n                # close telnet connection\n                self.tn.close()\n                # startup time?\n                startup_time = datetime.datetime.now() - self.start_time\n                self.logger.info(\"Startup complete in: %s\" % startup_time)\n                # mark as running\n                self.running = True\n                return\n\n        # no match, if we saw some output from the router it's probably\n        # booting, so let's give it some more time\n        if res != b'':\n            self.logger.trace(\"OUTPUT: %s\" % res.decode())\n            # reset spins if we saw some output\n            self.spins = 0\n\n        self.spins += 1\n\n        return\n\n\n    def bootstrap_config(self):\n        \"\"\" Do the actual bootstrap config\n        \"\"\"\n        self.logger.info(\"applying bootstrap configuration\")\n\n        self.wait_write(\"\", None)\n        self.wait_write(\"enable\", wait=\">\")\n        self.wait_write(\"configure terminal\", wait=\">\")\n\n        self.wait_write(\"hostname csr1000v\")\n        self.wait_write(\"username %s privilege 15 password %s\" % (self.username, self.password))\n        if int(self.version.split('.')[0]) >= 16:\n           self.wait_write(\"ip domain name example.com\")\n        else:\n           self.wait_write(\"ip domain-name example.com\")\n        self.wait_write(\"crypto key generate rsa modulus 2048\")\n\n        self.wait_write(\"interface GigabitEthernet1\")\n        self.wait_write(\"ip address 10.0.0.15 255.255.255.0\")\n        self.wait_write(\"no shut\")\n        self.wait_write(\"exit\")\n        self.wait_write(\"restconf\")\n        self.wait_write(\"netconf-yang\")\n\n        self.wait_write(\"line vty 0 4\")\n        self.wait_write(\"login local\")\n        self.wait_write(\"transport input all\")\n        self.wait_write(\"end\")\n        self.wait_write(\"copy running-config startup-config\")\n        self.wait_write(\"\\r\", None)\n\n\nclass CSR(vrnetlab.VR):\n    def __init__(self, username, password):\n        super(CSR, self).__init__(username, password)\n        self.vms = [ CSR_vm(username, password) ]\n\n\nclass CSR_installer(CSR):\n    \"\"\" CSR installer\n        \n        Will start the CSR with a mounted iso to make sure that we get\n        console output on serial, not vga.\n    \"\"\"\n    def __init__(self, username, password):\n        super(CSR, self).__init__(username, password)\n        self.vms = [ CSR_vm(username, password, install_mode=True)  ]\n\n    def install(self):\n        self.logger.info(\"Installing CSR\")\n        csr = self.vms[0]\n        while not csr.running:\n            csr.work()\n        time.sleep(30)\n        csr.stop()\n        self.logger.info(\"Installation complete\")\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='')\n    parser.add_argument('--trace', default=vrnetlab.bool_from_env('TRACE'), action='store_true', help='enable trace level logging')\n    parser.add_argument('--username', default=os.getenv('USERNAME', 'vrnetlab'), help='Username')\n    parser.add_argument('--password', default=os.getenv('PASSWORD', 'VR-netlab9'), help='Password')\n    parser.add_argument('--install', default=vrnetlab.bool_from_env('INSTALL'), action='store_true', help='Install CSR')\n    args = parser.parse_args()\n\n    LOG_FORMAT = \"%(asctime)s: %(module)-10s %(levelname)-8s %(message)s\"\n    logging.basicConfig(format=LOG_FORMAT)\n    logger = logging.getLogger()\n\n    logger.setLevel(logging.DEBUG)\n    if args.trace:\n        logger.setLevel(1)\n\n    if args.install:\n        vr = CSR_installer(args.username, args.password)\n        vr.install()\n    else:\n        vr = CSR(args.username, args.password)\n        vr.start()\n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": "csr/docker/launch.py", "language": "python", "complexity": "complex", "star_count": 1285, "quality_score": 0.84, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 216}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "vrnetlab / Cisco Nexus NXOS\n===========================\nThis is the vrnetlab docker image for Cisco Nexus NXOS Titanium emulator.\n\nBuilding the docker image\n-------------------------\nTitanium doesn't appear to be exactly official but you can get it from the\nInternet. VIRL is said to include it, so you may have luck in extracting it\nfrom there.\n\nAnyway, put the .qcow2 file in this directory and run `make docker-image` and\nyou should be good to go. The resulting image is called `vr-nxos`. You can tag\nit with something else if you want, like `my-repo.example.com/vr-nxos` and then\npush it to your repo. The tag is the same as the version of the NXOS image, so\nif you have nxosv-7.2.0.D1.1.qcow2 your final docker image will be called\nvr-nxos:7.2.0.D1.1\n\nUsage\n-----\n```\ndocker run -d --privileged --name my-nxos-router vr-nxos\n```\n\nSystem requirements\n-------------------\nCPU: 1 core\n\nRAM: 2GB\n\nDisk: <500MB\n\n\nFUAQ - Frequently or Unfrequently Asked Questions\n-------------------------------------------------\n##### Q: Has this been extensively tested?\nA: Nope. I don't use Nexus myself (yet) so not much testing at all really.\nPlease do try it out and let me know if it works.\n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": "nxos/README.md", "language": "markdown", "complexity": "simple", "star_count": 1285, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 38}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\n\nimport datetime\nimport logging\nimport os\nimport random\nimport re\nimport signal\nimport sys\nimport telnetlib\nimport time\n\nimport vrnetlab\n\ndef handle_SIGCHLD(signal, frame):\n    os.waitpid(-1, os.WNOHANG)\n\ndef handle_SIGTERM(signal, frame):\n    sys.exit(0)\n\nsignal.signal(signal.SIGINT, handle_SIGTERM)\nsignal.signal(signal.SIGTERM, handle_SIGTERM)\nsignal.signal(signal.SIGCHLD, handle_SIGCHLD)\n\nTRACE_LEVEL_NUM = 9\nlogging.addLevelName(TRACE_LEVEL_NUM, \"TRACE\")\ndef trace(self, message, *args, **kws):\n    # Yes, logger takes its '*args' as 'args'.\n    if self.isEnabledFor(TRACE_LEVEL_NUM):\n        self._log(TRACE_LEVEL_NUM, message, args, **kws)\nlogging.Logger.trace = trace\n\n\n\nclass NXOS_vm(vrnetlab.VM):\n    def __init__(self, username, password):\n        disk_image = None\n        for e in sorted(os.listdir(\"/\")):\n            if not disk_image and re.search(\".qcow2$\", e):\n                disk_image = \"/\" + e\n        super(NXOS_vm, self).__init__(username, password, disk_image=disk_image)\n        self.num_nics = 144\n        self.credentials = [\n                ['admin', 'admin']\n            ]\n\n\n    def bootstrap_spin(self):\n        \"\"\" This function should be called periodically to do work.\n        \"\"\"\n\n        if self.spins > 300:\n            # too many spins with no result ->  give up\n            self.stop()\n            self.start()\n            return\n\n        (ridx, match, res) = self.tn.expect([b\"login:\"], 1)\n        if match: # got a match!\n            if ridx == 0: # login\n                self.logger.debug(\"matched login prompt\")\n                try:\n                    username, password = self.credentials.pop(0)\n                except IndexError as exc:\n                    self.logger.error(\"no more credentials to try\")\n                    return\n                self.logger.debug(\"trying to log in with %s / %s\" % (username, password))\n                self.wait_write(username, wait=None)\n                self.wait_write(password, wait=\"Password:\")\n\n                # run main config!\n                self.bootstrap_config()\n                # close telnet connection\n                self.tn.close()\n                # startup time?\n                startup_time = datetime.datetime.now() - self.start_time\n                self.logger.info(\"Startup complete in: %s\" % startup_time)\n                # mark as running\n                self.running = True\n                return\n\n        # no match, if we saw some output from the router it's probably\n        # booting, so let's give it some more time\n        if res != b'':\n            self.logger.trace(\"OUTPUT: %s\" % res.decode())\n            # reset spins if we saw some output\n            self.spins = 0\n\n        self.spins += 1\n\n        return\n\n\n    def bootstrap_config(self):\n        \"\"\" Do the actual bootstrap config\n        \"\"\"\n        self.logger.info(\"applying bootstrap configuration\")\n        self.wait_write(\"\", None)\n        self.wait_write(\"configure\")\n        self.wait_write(\"username %s password 0 %s role network-admin\" % (self.username, self.password))\n\n        # configure mgmt interface\n        self.wait_write(\"interface mgmt0\")\n        self.wait_write(\"ip address 10.0.0.15/24\")\n        self.wait_write(\"exit\")\n        self.wait_write(\"exit\")\n        self.wait_write(\"copy running-config startup-config\")\n\n\nclass NXOS(vrnetlab.VR):\n    def __init__(self, username, password):\n        super(NXOS, self).__init__(username, password)\n        self.vms = [ NXOS_vm(username, password) ]\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='')\n    parser.add_argument('--trace', default=vrnetlab.bool_from_env('TRACE'), action='store_true', help='enable trace level logging')\n    parser.add_argument('--username', default=os.getenv('USERNAME', 'vrnetlab'), help='Username')\n    parser.add_argument('--password', default=os.getenv('PASSWORD', 'VR-netlab9'), help='Password')\n    args = parser.parse_args()\n\n    LOG_FORMAT = \"%(asctime)s: %(module)-10s %(levelname)-8s %(message)s\"\n    logging.basicConfig(format=LOG_FORMAT)\n    logger = logging.getLogger()\n\n    logger.setLevel(logging.DEBUG)\n    if args.trace:\n        logger.setLevel(1)\n\n    vr = NXOS(args.username, args.password)\n    vr.start()\n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": "nxos/docker/launch.py", "language": "python", "complexity": "medium", "star_count": 1285, "quality_score": 0.87, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 133}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "vrnetlab / Cisco NX-OSv 9000\n============================\nThis is the vrnetlab docker image for Cisco NX-OSv 9000 Virtual Switch.\n\nBuilding the docker image\n-------------------------\nThis is the officially supported image and is different from the Titanium\nemulator. The image can be downloaded directly from Cisco site.\n\nAdditional files needed\n-----------------------\nYou need to download the nexus9x00v image from Cisco. You will also need to\ndownload the EFI boot image, such as the one from \nhttps://www.kraxel.org/repos/jenkins/edk2. You need to extract the \nOVMF-pure-efi.fd file from the RPM package, this EFI boot image is used to\nboot up the nexus9x00v image.\n\nAnyway, put the .qcow2 and the fd files in this directory and run \n`make docker-image` and you should be good to go. The resulting image\nis called `vr-nxos9k`. You can tag it with something else if you want,\nlike `my-repo.example.com/vr-nxos9k` and then push it to your repo. The tag\nis the same as the version of the NXOS image, so if you have\nnexus9300v.9.3.7.qcow2 your final docker image will be called vr-nxos:9.3.7.\n\nUsage\n-----\n```\ndocker run -d --privileged --name my-nxos-router vr-nxos9k:9.3.7\n```\nYou may specify the number of NICs with --num-nics, the default is 24 with a\nmaximum of 65 for 9300v.\n\nInitial Configuration\n---------------------\nThe initial configuration file called nxos_config.txt is required. This file\nshould contain some minimal configuration. A sample configuration is included.\n\nSystem requirements\n-------------------\n\nCurrently there are two platforms, 9300v and 9500v. If you are using vrnetlab\nI assume you want the light weight 9300v. The resource requirement is based on\n9300v.\n\nCPU: 1 core, 2 prefered, the VR is not stable in my tests with 1 core.\n\nRAM: 8GB, Cisco recommends 4GB, but in my case 4GB is not enough.\n\nDisk: 8GB\n\nI only tested with 9300v.\n\nKnown issues\n------------\nIt is known that a previously booted image may not start properly. We have\nimplemented a check that once a container stops then restarts, all its\nconfigurations (stored in the overlay image) are wiped clean before running.\nYou will have to configure from scratch for each run.\n\nFUAQ - Frequently or Unfrequently Asked Questions\n-------------------------------------------------\n##### Q: Has this been extensively tested?\nA: Only basic configuration by the author on Ubuntu 20.04 server running Intel\nXeon chips, both CLI and Netconf work. Layer2/layer3 functions are not tested.\n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": "nxos9kv/README.md", "language": "markdown", "complexity": "simple", "star_count": 1285, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 65}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\n\nimport argparse\nimport datetime\nimport logging\nimport os\nimport re\nimport signal\nimport sys\n\nimport vrnetlab\n\ndef handle_SIGCHLD(signal, frame):\n    os.waitpid(-1, os.WNOHANG)\n\ndef handle_SIGTERM(signal, frame):\n    sys.exit(0)\n\nsignal.signal(signal.SIGINT, handle_SIGTERM)\nsignal.signal(signal.SIGTERM, handle_SIGTERM)\nsignal.signal(signal.SIGCHLD, handle_SIGCHLD)\n\nTRACE_LEVEL_NUM = 9\nlogging.addLevelName(TRACE_LEVEL_NUM, \"TRACE\")\n\ndef trace(self, message, *args, **kws):\n    # Yes, logger takes its '*args' as 'args'.\n    if self.isEnabledFor(TRACE_LEVEL_NUM):\n        self._log(TRACE_LEVEL_NUM, message, args, **kws)\n\nlogging.Logger.trace = trace\n\n\n\nclass NXOS9K_vm(vrnetlab.VM):\n    def __init__(self, bios, username, password, num_nics):\n        disk_image = None\n        for e in sorted(os.listdir(\"/\")):\n            if not disk_image and re.search(\".qcow2$\", e):\n                disk_image = \"/\" + e\n        # the parent constructor needs to call create_overlay_image,\n        # so must initialize the other parameters first\n        self.bios = bios\n        self.prompted = False\n        super().__init__(username, password, disk_image=disk_image, ram=8192)\n        self.num_nics = num_nics\n        self.credentials = [\n                ['admin', 'Cisco1234']\n            ]\n\n\n    def create_overlay_image(self):\n        pre_start_cmds, _ = super().create_overlay_image()\n        extended_args = ['-nographic', '-bios', self.bios, '-smp', '2']\n        # use SATA driver for disk and set to drive 0\n        extended_args.extend(['-device', 'ahci,id=ahci0,bus=pci.0',\n            '-drive', 'if=none,file=%s,id=drive-sata-disk0,format=qcow2' % self.overlay_disk_image,\n            '-device', 'ide-drive,bus=ahci0.0,drive=drive-sata-disk0,id=drive-sata-disk0'])\n        # create initial config and load it\n        pre_start_cmds.append(['genisoimage', '-o', '/cfg.iso', '-l', '--iso-level', '2', 'nxos_config.txt'])\n        extended_args.extend(['-drive', 'file=cfg.iso,media=cdrom'])\n\n        return pre_start_cmds, extended_args\n\n    def bootstrap_spin(self):\n        \"\"\" This function should be called periodically to do work.\n        \"\"\"\n\n        # press return to get prompt every 10 seconds\n        if not self.prompted and self.spins % 10 == 0:\n            self.wait_write('', wait=None)\n        if self.spins > 300:\n            # too many spins with no result ->  give up\n            self.prompted = False\n            self.restart()\n            return\n\n        username, password = self.credentials[0]\n        (ridx, match, res) = self.tn.expect([b'login:', b'Enter the password for \"admin\":', b'Confirm the password for \"admin\":'], 1)\n        if match: # got a match!\n            self.prompted = True\n            self.logger.info(\"match found: %s\", res.decode())\n            if ridx == 0: # login\n                self.logger.info(\"matched login prompt\")\n                self.logger.info(\"trying to log in with %s / %s\", username, password)\n                self.wait_write(username, wait=None)\n                self.wait_write(password, wait=\"Password:\")\n\n                # run main config!\n                self.bootstrap_config()\n                # close telnet connection\n                self.tn.close()\n                # startup time?\n                startup_time = datetime.datetime.now() - self.start_time\n                self.logger.info(\"Startup complete in: %s\", startup_time)\n                # mark as running\n                self.running = True\n            else:\n                self.logger.info(\"Trying to reset admin password to %s\", password)\n                self.wait_write(password, wait=None)\n                self.wait_write(password, wait='password')\n\n        # no match, if we saw some output from the router it's probably\n        # booting, so let's give it some more time\n        elif res != b'':\n            self.logger.trace(\"OUTPUT: %s\", res.decode())\n            # reset spins if we saw some output\n            self.spins = 0\n\n        self.spins += 1\n\n        return\n\n\n    def bootstrap_config(self):\n        \"\"\" Do the actual bootstrap config\n        \"\"\"\n        self.logger.info(\"applying bootstrap configuration\")\n        self.wait_write(\"\", None)\n        # figure out the running image\n        self.wait_write(\"\", None)\n        self.wait_write(\"configure\")\n        self.wait_write(\"username %s password 0 %s role network-admin\" % (self.username, self.password))\n\n        # configure mgmt interface\n        self.wait_write(\"interface mgmt0\")\n        self.wait_write(\"ip address 10.0.0.15/24\")\n        self.wait_write(\"exit\")\n        # enable netconf with 10 sessions (max allowed)\n        self.wait_write(\"feature netconf\")\n        self.wait_write(\"netconf sessions 10\")\n        self.wait_write(\"exit\")\n        self.wait_write(\"copy running-config startup-config\")\n\n\nclass NXOS9K(vrnetlab.VR):\n    def __init__(self, bios, username, password, num_nics):\n        super().__init__(username, password)\n        self.vms = [ NXOS9K_vm(bios, username, password, num_nics) ]\n\ndef main():\n    \"\"\"Main method\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--trace', default=vrnetlab.bool_from_env('TRACE'), action='store_true', help='enable trace level logging')\n    parser.add_argument('--username', default=os.getenv('USERNAME', 'vrnetlab'), help='Username')\n    parser.add_argument('--password', default=os.getenv('PASSWORD', 'VR-netlab9'), help='Password')\n    parser.add_argument('--bios', default=os.getenv('BIOS', 'OVMF-pure-efi.fd'), help='EFI bios image')\n    parser.add_argument('--num-nics', type=int, default=int(os.getenv('NUM_NICS', 24)), help='Number of NICs')\n    args = parser.parse_args()\n\n    # check if the bios file exists\n    if not os.path.exists(args.bios):\n        print('Bios file %s does not exit' % args.bios)\n        sys.exit(1)\n\n    LOG_FORMAT = \"%(asctime)s: %(module)-10s %(levelname)-8s %(message)s\"\n    logging.basicConfig(format=LOG_FORMAT)\n    logger = logging.getLogger()\n\n    logger.setLevel(logging.DEBUG)\n    if args.trace:\n        logger.setLevel(1)\n\n    vr = NXOS9K(args.bios, args.username, args.password, args.num_nics)\n    vr.start()\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/vrnetlab/vrnetlab", "file_path": "nxos9kv/docker/launch.py", "language": "python", "complexity": "complex", "star_count": 1285, "quality_score": 0.84, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 169}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{\n  \"image\": \"ghcr.io/cicada-software/codespaces:latest\",\n  \"forwardPorts\": [80],\n  \"portsAttributes\": {\n    \"80\": {\n      \"label\": \"Cicada\",\n      \"onAutoForward\": \"openBrowserOnce\"\n    },\n    \"8000\": {\n      \"onAutoForward\": \"ignore\"\n    }\n  },\n  \"customizations\": {\n    \"codespaces\": {\n      \"openFiles\": [\n        \"docs/github-codespaces.md\"\n      ]\n    },\n    \"vscode\": {\n      \"extensions\": [\n        \"bierner.markdown-preview-github-styles\",\n        \"ms-python.python\"\n      ]\n    }\n  },\n  \"capAdd\": [\"NET_ADMIN\", \"NET_RAW\"],\n  \"securityOpt\": [\"seccomp=unconfined\"],\n  \"privileged\": true\n}\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": ".devcontainer/devcontainer.json", "language": "json", "complexity": "simple", "star_count": 571, "quality_score": 0.5, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 30}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Cicada\n\nA FOSS, cross-platform version of GitHub Actions and Gitlab CI.\n\n![Workflow Run Example](/frontend/static/img/screenshot.png)\n\n> Example workflow run is from the [demo](https://github.com/Cicada-Software/demo) repo.\n\n## Demo\n\nYou can spin up a new Cicada demo server using [GitHub Codespaces](https://github.com/features/codespaces) in minutes using the button below:\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/Cicada-Software/cicada?quickstart=1)\n\n## Installing\n\nTo install Cicada in a more permanent environment (ie, a server), see the [installation docs](./docs/installing.md) for more info.\n\n## Syntax Highlighting\n\nSyntax highlighting support (for Neovim) is available [here](https://github.com/Cicada-Software/tree-sitter-cicada).\nBuilt using [Tree-sitter](https://tree-sitter.github.io/tree-sitter/).\n\n## Support\n\nIf you would like to support the development of Cicada, feel free to support\nme on [GitHub Sponsors](https://github.com/sponsors/dosisod).\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "README.md", "language": "markdown", "complexity": "simple", "star_count": 571, "quality_score": 0.53, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 28}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# pragma: no cover\n\nfrom pathlib import Path\n\nimport uvicorn\nfrom uvicorn.config import LOGGING_CONFIG\n\nfrom cicada.api.settings import verify_env_vars\nfrom cicada.logging import setup as setup_logging\n\nsetup_logging()\n\n\ndef run_setup_wizard() -> None:\n    from cicada.api.setup import app\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)  # noqa: S104\n\n\ndef run_live_site() -> None:\n    from cicada.api.main import app\n    from cicada.api.settings import DNSSettings\n\n    settings = DNSSettings()\n\n    # include timestamps in uvicorn logs\n    LOGGING_CONFIG[\"formatters\"][\"default\"][\"fmt\"] = \"%(asctime)s %(levelprefix)s %(message)s\"\n    LOGGING_CONFIG[\"formatters\"][\"access\"][\n        \"fmt\"\n    ] = '%(asctime)s %(levelprefix)s %(client_addr)s - \"%(request_line)s\" %(status_code)s'\n\n    uvicorn.run(\n        app,\n        host=settings.host,\n        port=settings.port,\n    )\n\n\nif Path(\".env\").exists():\n    verify_env_vars()\n\n    run_live_site()\n\nelse:\n    run_setup_wizard()\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "cicada/api/__main__.py", "language": "python", "complexity": "simple", "star_count": 571, "quality_score": 0.6799999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 46}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\n\nfrom cicada.api.endpoints.oauth_util import GitlabOAuthTokenStore\nfrom cicada.api.infra.environment_repo import EnvironmentRepo\nfrom cicada.api.infra.github.stop_session import github_session_terminator\nfrom cicada.api.infra.gitlab.webhook_repo import GitlabWebhookRepo\nfrom cicada.api.infra.installation_repo import InstallationRepo\nfrom cicada.api.infra.repository_repo import RepositoryRepo\nfrom cicada.api.infra.runner_repo import RunnerRepo\nfrom cicada.api.infra.secret_repo import SecretRepo\nfrom cicada.api.infra.secret_repo_shim import SecretRepoShim\nfrom cicada.api.infra.session_repo import SessionRepo\nfrom cicada.api.infra.terminal_session_repo import TerminalSessionRepo\nfrom cicada.api.infra.user_repo import UserRepo\nfrom cicada.api.infra.waitlist_repo import WaitlistRepo\nfrom cicada.api.settings import VaultSettings\nfrom cicada.application.session.stop_session import SessionTerminator\nfrom cicada.domain.repo.environment_repo import IEnvironmentRepo\nfrom cicada.domain.repo.gitlab_webhook_repo import IGitlabWebhookRepo\nfrom cicada.domain.repo.installation_repo import IInstallationRepo\nfrom cicada.domain.repo.repository_repo import IRepositoryRepo\nfrom cicada.domain.repo.runner_repo import IRunnerRepo\nfrom cicada.domain.repo.secret_repo import ISecretRepo\nfrom cicada.domain.repo.session_repo import ISessionRepo\nfrom cicada.domain.repo.terminal_session_repo import ITerminalSessionRepo\nfrom cicada.domain.repo.user_repo import IUserRepo\nfrom cicada.domain.repo.waitlist_repo import IWaitlistRepo\n\nlogger = logging.getLogger(\"cicada\")\n\n\nclass DiContainer:  # pragma: no cover\n    # TODO: move settings classes here so they can be injected without patching\n\n    @classmethod\n    def user_repo(cls) -> IUserRepo:\n        return UserRepo()\n\n    @classmethod\n    def session_repo(cls) -> ISessionRepo:\n        return SessionRepo()\n\n    @classmethod\n    def terminal_session_repo(cls) -> ITerminalSessionRepo:\n        return TerminalSessionRepo()\n\n    @classmethod\n    def waitlist_repo(cls) -> IWaitlistRepo:\n        return WaitlistRepo()\n\n    @classmethod\n    def repository_repo(cls) -> IRepositoryRepo:\n        return RepositoryRepo()\n\n    @classmethod\n    def environment_repo(cls) -> IEnvironmentRepo:\n        return EnvironmentRepo()\n\n    @classmethod\n    def installation_repo(cls) -> IInstallationRepo:\n        return InstallationRepo()\n\n    @classmethod\n    def runner_repo(cls) -> IRunnerRepo:\n        return RunnerRepo()\n\n    @classmethod\n    def secret_repo(cls) -> ISecretRepo:\n        try:\n            VaultSettings()\n            return SecretRepo()\n\n        except ValueError:\n            logger.warning(\"Vault is not installed, secret support will be disabled\")\n            return SecretRepoShim()\n\n    @classmethod\n    def session_terminators(cls) -> dict[str, SessionTerminator]:\n        return {\"github\": github_session_terminator}\n\n    token_store: GitlabOAuthTokenStore | None = None\n\n    @classmethod\n    def gitlab_token_store(cls) -> GitlabOAuthTokenStore:\n        if cls.token_store is None:\n            cls.token_store = GitlabOAuthTokenStore()\n\n        return cls.token_store\n\n    @classmethod\n    def gitlab_webhook_repo(cls) -> IGitlabWebhookRepo:\n        return GitlabWebhookRepo()\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "cicada/api/di.py", "language": "python", "complexity": "medium", "star_count": 571, "quality_score": 0.74, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 93}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nimport time\nfrom secrets import token_hex\n\nfrom fastapi import HTTPException\n\n\nclass CsrfTokenCache:\n    \"\"\"\n    An in-memory CSRF token cache. This class has 2 operations: generate and validate. When you\n    generate a token it will be stored along with the timestamp when it was created. When you\n    validate, the token will be popped, and the timestamp will be compared. If the token does not\n    exist or has expired, an HTTPException is thrown. In addition, if the token did not exist, a\n    logger message is emitted since a nonexistent CSRF token is either a bug or an attacker trying\n    to forge a token, which should be investigated.\n    \"\"\"\n\n    tokens: dict[str, float]\n\n    DEFAULT_EXPIRATION_IN_SECONDS = 5 * 60\n\n    def __init__(self) -> None:\n        self.tokens = {}\n\n    def generate(self) -> str:\n        token = token_hex(16)\n        self.tokens[token] = time.time()\n\n        return token\n\n    def validate(self, token: str) -> None:\n        expiration = self.tokens.pop(token, None)\n\n        if expiration is None:\n            msg = \"CSRF token validation failure\"\n\n            logger = logging.getLogger(\"cicada\")\n            logger.warning(msg)\n\n            raise HTTPException(401, msg) from None\n\n        if time.time() > (expiration + self.DEFAULT_EXPIRATION_IN_SECONDS):\n            raise HTTPException(401, \"CSRF token expired\")\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "cicada/api/endpoints/csrf.py", "language": "python", "complexity": "simple", "star_count": 571, "quality_score": 0.71, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 44}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Annotated\n\nfrom fastapi import Depends\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\n\nfrom cicada.api.di import DiContainer\n\nDi = Annotated[DiContainer, Depends()]\n\noauth2_schema = OAuth2PasswordBearer(tokenUrl=\"login\")\nJWTToken = Annotated[str, Depends(oauth2_schema)]\n\nPasswordForm = Annotated[OAuth2PasswordRequestForm, Depends()]\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "cicada/api/endpoints/di.py", "language": "python", "complexity": "simple", "star_count": 571, "quality_score": 0.56, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 14}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from dataclasses import dataclass\n\nfrom fastapi import APIRouter, HTTPException\nfrom fastapi.responses import JSONResponse\n\nfrom cicada.api.endpoints.di import Di\nfrom cicada.api.endpoints.login_util import CurrentUser\nfrom cicada.application.env.add_env_vars_to_repo import AddEnvironmentVariablesToRepository\nfrom cicada.common.json import asjson\nfrom cicada.domain.env_var import EnvironmentVariable\n\nrouter = APIRouter()\n\n\n@dataclass\nclass EnvVars:\n    repository_url: str\n    provider: str\n    env_vars: dict[str, str]\n\n\n@router.post(\"/api/env/repo_env_vars\")\ndef set_repo_env_vars(env_vars: EnvVars, user: CurrentUser, di: Di) -> None:\n    cmd = AddEnvironmentVariablesToRepository(\n        user_repo=di.user_repo(),\n        repository_repo=di.repository_repo(),\n        env_repo=di.environment_repo(),\n    )\n\n    cmd.handle(\n        user.username,\n        env_vars.repository_url,\n        env_vars.provider,\n        [EnvironmentVariable(k, v) for k, v in env_vars.env_vars.items()],\n    )\n\n\n@router.get(\"/api/env/repo_env_vars\")\ndef get_repo_env_vars(\n    user: CurrentUser,\n    di: Di,\n    provider: str,\n    url: str,\n) -> JSONResponse:\n    # TODO: turn into query command\n\n    repository_repo = di.repository_repo()\n\n    repo = repository_repo.get_repository_by_url_and_provider(url=url, provider=provider)\n\n    if not repo:\n        raise HTTPException(status_code=404)\n\n    if not repository_repo.can_user_access_repo(user, repo):\n        # TODO: test\n        raise HTTPException(status_code=401)\n\n    return JSONResponse([asjson(x) for x in di.environment_repo().get_env_vars_for_repo(repo.id)])\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "cicada/api/endpoints/env.py", "language": "python", "complexity": "simple", "star_count": 571, "quality_score": 0.62, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 59}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nfrom dataclasses import dataclass, field\nfrom hashlib import sha3_256\nfrom secrets import token_hex\nfrom uuid import uuid4\n\nfrom fastapi import APIRouter, HTTPException\nfrom gitlab import Gitlab, GitlabHttpError\n\nfrom cicada.api.endpoints.di import Di\nfrom cicada.api.endpoints.login_util import CurrentUser\nfrom cicada.api.infra.gitlab.common import (\n    GitlabProject,\n    get_authenticated_user_id,\n    get_projects_for_user,\n)\nfrom cicada.api.settings import GitlabSettings\nfrom cicada.domain.installation import Installation, InstallationScope\nfrom cicada.domain.repo.gitlab_webhook_repo import GitlabWebhook\nfrom cicada.domain.user import User\n\nrouter = APIRouter()\n\n\n@router.get(\"/api/gitlab/projects\")\nasync def get_gitlab_projects_for_user(di: Di, user: CurrentUser) -> list[GitlabProject]:\n    token = await di.gitlab_token_store().load_token(user)\n\n    if not token:\n        raise HTTPException(401)\n\n    gl = Gitlab(oauth_token=token.access_token)\n\n    user_id = get_authenticated_user_id(gl)\n\n    # TODO: add flag to show if hook already exists for project\n    return get_projects_for_user(gl, user_id)\n\n\n@dataclass\nclass UpdateProjectWebhooks:\n    all: bool = False\n    project_ids: list[int] = field(default_factory=list)\n\n\n@router.post(\"/api/gitlab/projects/add_webhooks\")\nasync def add_webhooks_to_projects(di: Di, user: CurrentUser, cmd: UpdateProjectWebhooks) -> None:\n    token = await di.gitlab_token_store().load_token(user)\n\n    if not token:\n        raise HTTPException(401)\n\n    gl = Gitlab(oauth_token=token.access_token)\n    user_id = get_authenticated_user_id(gl)\n\n    if cmd.all:\n        project_ids = [p.id for p in get_projects_for_user(gl, user_id)]\n\n    else:\n        project_ids = cmd.project_ids\n\n    for project_id in project_ids:\n        try:\n            update_webhook_for_project(di, gl, user, project_id)\n\n        except GitlabHttpError:\n            logger = logging.getLogger(\"cicada\")\n            logger.exception(\"Gitlab API call failed\")\n\n            continue\n\n\ndef update_webhook_for_project(di: Di, gl: Gitlab, user: User, project_id: int) -> None:\n    settings = GitlabSettings()\n\n    # TODO: make this lazy again\n    project = gl.projects.get(project_id)\n    hooks = project.hooks.list()\n\n    url = f\"https://{settings.domain}/api/gitlab_webhook\"\n\n    if url in [hook.url.split(\"?\")[0] for hook in hooks]:\n        # Webhook with our domain already exists, ignore\n        return\n\n    secret = token_hex(16)\n    webhook_id = uuid4()\n\n    hook = project.hooks.create(\n        {\n            \"id\": project_id,\n            \"url\": f\"https://{settings.domain}/api/gitlab_webhook?id={webhook_id}\",\n            \"enable_ssl_verification\": True,\n            \"issues_events\": True,\n            \"push_events\": True,\n            \"token\": secret,\n        }\n    )\n\n    webhook = GitlabWebhook(\n        id=webhook_id,\n        created_by_user_id=user.id,\n        project_id=project_id,\n        hook_id=hook.id,\n        hashed_secret=sha3_256(secret.encode()).hexdigest(),\n    )\n    di.gitlab_webhook_repo().add_webhook(webhook)\n\n    hook_url = f\"https://gitlab.com/{user.username}/{project.path}/-/hooks/{hook.id}/edit\"\n\n    installation = Installation(\n        id=uuid4(),\n        name=user.username,\n        provider=\"gitlab\",\n        scope=InstallationScope.USER,\n        admin_id=user.id,\n        provider_id=str(hook.id),\n        provider_url=hook_url,\n    )\n\n    di.installation_repo().create_or_update_installation(installation)\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "cicada/api/endpoints/gitlab/projects.py", "language": "python", "complexity": "medium", "star_count": 571, "quality_score": 0.74, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 122}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Any\n\nfrom fastapi import APIRouter\nfrom fastapi.responses import JSONResponse\n\nfrom cicada.api.endpoints.di import Di\nfrom cicada.api.endpoints.login_util import CurrentUser\nfrom cicada.application.exceptions import NotFound\nfrom cicada.common.json import asjson\nfrom cicada.domain.installation import Installation, InstallationId\n\nrouter = APIRouter()\n\n\ndef InstallationDto(  # type: ignore[misc]  # noqa: N802\n    installation: Installation,\n) -> dict[str, Any]:\n    data = asjson(installation)\n    del data[\"admin_id\"]\n\n    return data  # type: ignore[no-any-return]\n\n\n@router.get(\"/api/installations\")\ndef get_installations_for_current_user(\n    user: CurrentUser,\n    di: Di,\n) -> JSONResponse:\n    installations = di.installation_repo().get_installations_for_user(user)\n\n    return JSONResponse([InstallationDto(x) for x in installations])\n\n\n@router.get(\"/api/installation/{uuid}\")\ndef get_installation_by_id(\n    user: CurrentUser,\n    di: Di,\n    uuid: InstallationId,\n) -> JSONResponse:\n    installations = di.installation_repo().get_installations_for_user(user)\n\n    for installation in installations:\n        if installation.id == uuid:\n            return JSONResponse(InstallationDto(installation))\n\n    raise NotFound(\"Installation not found\")\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "cicada/api/endpoints/installation.py", "language": "python", "complexity": "simple", "star_count": 571, "quality_score": 0.59, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 47}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Annotated, Any\n\nfrom fastapi import APIRouter, Form, HTTPException\nfrom fastapi.responses import FileResponse\n\nfrom cicada.api.endpoints.di import Di, JWTToken, PasswordForm\nfrom cicada.application.user.change_password import ChangePassword\nfrom cicada.application.user.local_user_login import LocalUserLogin\n\nfrom .login_util import CurrentUser, create_access_token, get_user_and_payload_from_jwt\n\nrouter = APIRouter()\n\n\n@router.post(\"/api/login\", response_model=None)\nasync def login(  # type: ignore[misc]\n    di: Di,\n    form_data: PasswordForm,\n) -> dict[str, Any]:\n    cmd = LocalUserLogin(di.user_repo())\n    user = cmd.handle(form_data.username, form_data.password)\n\n    return create_access_token(user)\n\n\n@router.post(\"/api/change_password\")\nasync def change_password(\n    user: CurrentUser,\n    di: Di,\n    password: Annotated[str, Form()] = \"\",\n) -> None:\n    cmd = ChangePassword(di.user_repo())\n\n    cmd.handle(user, password)\n\n\n@router.post(\"/api/refresh_token\")\nasync def refresh_token(  # type: ignore[misc]\n    di: Di,\n    token: JWTToken,\n) -> dict[str, Any]:\n    if data := get_user_and_payload_from_jwt(di.user_repo(), token):\n        user, payload = data\n\n        return create_access_token(user, issuer=payload[\"iss\"])\n\n    raise HTTPException(\n        status_code=401,\n        detail=\"JWT Invalid\",\n        headers={\"WWW-Authenticate\": \"Bearer\"},\n    )\n\n\n@router.get(\"/login\")\ndef login_page() -> FileResponse:\n    return FileResponse(\"./frontend/login.html\")\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "cicada/api/endpoints/login.py", "language": "python", "complexity": "simple", "star_count": 571, "quality_score": 0.59, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 57}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copied from: https://fastapi.tiangolo.com/tutorial/security/oauth2-jwt\n\nfrom contextlib import suppress\nfrom datetime import timedelta\nfrom typing import Annotated, Any\n\nimport jwt\nfrom fastapi import Depends, HTTPException\n\nfrom cicada.api.endpoints.di import Di, JWTToken\nfrom cicada.api.settings import JWTSettings\nfrom cicada.domain.datetime import UtcDatetime\nfrom cicada.domain.repo.user_repo import IUserRepo\nfrom cicada.domain.user import User\n\n\ndef create_jwt(  # type: ignore[misc]\n    *,\n    subject: str,\n    issuer: str,\n    audience: str | None = None,\n    data: dict[str, Any] | None = None,\n) -> str:\n    settings = JWTSettings()\n\n    now = UtcDatetime.now()\n    delta = timedelta(seconds=settings.expiration_timeout)\n\n    audience = audience or settings.domain\n\n    payload = {\n        **(data or {}),\n        \"iat\": now,\n        \"exp\": now + delta,\n        \"sub\": subject,\n        \"iss\": issuer,\n        \"aud\": audience,\n    }\n\n    return jwt.encode(payload, settings.secret, algorithm=\"HS256\")\n\n\nIDENTITY_PROVIDERS = (\"github\", \"gitlab\", \"cicada\")\n\n\ndef get_user_and_payload_from_jwt(  # type: ignore[misc]\n    user_repo: IUserRepo, token: str\n) -> tuple[User, dict[str, Any]] | None:\n    settings = JWTSettings()\n\n    with suppress(jwt.PyJWTError):\n        payload = jwt.decode(\n            jwt=token,\n            key=settings.secret,\n            audience=settings.domain,\n            algorithms=[\"HS256\"],\n        )\n\n        username: str = payload.get(\"sub\", \"\")\n        provider: str = payload.get(\"iss\", \"\")\n\n        assert provider in IDENTITY_PROVIDERS\n\n        user = user_repo.get_user_by_username_and_provider(username, provider=provider)\n\n        if not user:\n            return None\n\n        user.password_hash = None\n\n        return user, payload\n\n    return None\n\n\ndef get_user_from_jwt(user_repo: IUserRepo, token: str) -> User | None:\n    if data := get_user_and_payload_from_jwt(user_repo, token):\n        return data[0]\n\n    return None\n\n\ndef get_current_user(di: Di, token: JWTToken) -> User:\n    if user := get_user_from_jwt(di.user_repo(), token):\n        return user\n\n    raise HTTPException(\n        status_code=401,\n        detail=\"JWT Invalid\",\n        headers={\"WWW-Authenticate\": \"Bearer\"},\n    )\n\n\nCurrentUser = Annotated[User, Depends(get_current_user)]\n\n\ndef create_access_token(  # type: ignore[misc]\n    user: User, issuer: str = \"cicada\"\n) -> dict[str, Any]:\n    jwt = create_jwt(subject=user.username, issuer=issuer)\n\n    return {\"access_token\": jwt}\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "cicada/api/endpoints/login_util.py", "language": "python", "complexity": "medium", "star_count": 571, "quality_score": 0.65, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 103}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nimport time\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport httpx\nfrom cryptography.fernet import Fernet\n\nfrom cicada.api.infra.db_connection import DbConnection\nfrom cicada.api.settings import GitlabSettings\nfrom cicada.domain.user import User\n\n\n@dataclass(kw_only=True)\nclass Token:\n    access_token: str\n    expires_at: int\n    refresh_token: str\n\n    @property\n    def expired(self) -> bool:\n        return time.time() >= self.expires_at\n\n\nclass GitlabOAuthTokenStore(DbConnection):\n    \"\"\"\n    Securely store Gitlab OAuth tokens. Because Gitlab does not allow authenticating via an OAuth\n    application you need to hold on to users' OAuth tokens. This API allows for securely storing,\n    refreshing, and accessing user tokens.\n    \"\"\"\n\n    tokens: dict[str, Token]\n    fernet: Fernet\n\n    def __init__(self) -> None:\n        super().__init__()\n\n        self.tokens = {}\n        self.settings = GitlabSettings()\n\n        self.fernet = Fernet(self.settings.token_store_secret.encode())\n\n    def save_token(self, user: User, token: Token) -> None:\n        if user.provider != \"gitlab\":\n            return\n\n        self.tokens[user.username] = token\n\n        data = json.dumps([token.access_token, token.refresh_token], separators=(\",\", \":\"))\n\n        cipher = self.fernet.encrypt(data.encode())\n\n        self.conn.execute(\n            \"\"\"\n            INSERT INTO gitlab_oauth_tokens (user_id, expires_at, data)\n            VALUES ((SELECT id FROM users WHERE uuid=?), ?, ?)\n            ON CONFLICT DO UPDATE SET expires_at=excluded.expires_at, data=excluded.data;\n            \"\"\",\n            [user.id, token.expires_at, cipher],\n        )\n\n        self.conn.commit()\n\n    async def load_token(self, user: User) -> Token | None:\n        if user.provider != \"gitlab\":\n            return None\n\n        token = self.tokens.get(user.username)\n\n        if not token:\n            row = self.conn.execute(\n                \"\"\"\n                SELECT data, expires_at\n                FROM gitlab_oauth_tokens\n                WHERE user_id=(SELECT id FROM users WHERE uuid=?);\n                \"\"\",\n                [user.id],\n            ).fetchone()\n\n            if not row:\n                return None\n\n            expires_at = row[\"expires_at\"]\n\n            plaintext = self.fernet.decrypt(row[\"data\"])\n            [access_token, refresh_token] = json.loads(plaintext)\n\n            token = Token(\n                access_token=access_token, expires_at=expires_at, refresh_token=refresh_token\n            )\n\n        token = await self._refresh_token(token) if token.expired else token\n\n        self.save_token(user, token)\n\n        return token\n\n    @staticmethod\n    def oauth_response_to_token(data: dict[str, Any]) -> Token:  # type: ignore\n        return Token(\n            access_token=data[\"access_token\"],\n            expires_at=data[\"created_at\"] + data[\"expires_in\"],\n            refresh_token=data[\"refresh_token\"],\n        )\n\n    async def _refresh_token(self, token: Token) -> Token:\n        params = {\n            \"client_id\": self.settings.client_id,\n            \"client_secret\": self.settings.client_secret,\n            \"refresh_token\": token.refresh_token,\n            \"grant_type\": \"refresh_token\",\n            \"redirect_uri\": self.settings.redirect_uri,\n        }\n\n        async with httpx.AsyncClient() as client:\n            resp = await client.post(\"https://gitlab.com/oauth/token\", params=params)\n\n        data = resp.json()\n\n        return self.oauth_response_to_token(data)\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "cicada/api/endpoints/oauth_util.py", "language": "python", "complexity": "medium", "star_count": 571, "quality_score": 0.6799999999999999, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 121}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, WebSocket, WebSocketDisconnect\nfrom websockets.exceptions import ConnectionClosed\n\nfrom cicada.api.endpoints.di import Di\nfrom cicada.common.json import asjson\nfrom cicada.domain.runner import RunnerId\nfrom cicada.domain.session import SessionStatus\nfrom cicada.domain.terminal_session import TerminalSession\n\nrouter = APIRouter()\n\nPROTOCOL_VERSION = 1\nMINIMUM_PROTOCOL_VERSION = 1\n\n\n@router.websocket(\"/ws/runner/connect/{runner_id}\")\nasync def runner(websocket: WebSocket, di: Di, runner_id: UUID) -> None:  # pragma: no cover\n    runner_repo = di.runner_repo()\n    session_repo = di.session_repo()\n\n    try:\n        await websocket.accept()\n\n        try:\n            runner_secret = websocket.headers[\"X-RunnerSecret\"]\n\n        except KeyError:\n            return await websocket.close(\n                code=1001,\n                reason=\"Runner secret not set\",\n            )\n\n        runner = runner_repo.get_runner_by_id(RunnerId(runner_id))\n\n        if not (runner and runner.verify(runner_secret)):\n            return await websocket.close(code=1001, reason=\"Unauthorized\")\n\n        # TODO: store in dataclass instead\n        await websocket.send_json(\n            {\n                \"type\": \"connect\",\n                \"protocol_version\": PROTOCOL_VERSION,\n                \"minimum_version\": MINIMUM_PROTOCOL_VERSION,\n            }\n        )\n\n        while True:\n            await websocket.send_json({\"type\": \"ping\"})\n\n            queue = runner_repo.get_queued_sessions_for_runner(runner.id)\n\n            for session, _, url in queue:\n                # TODO: store in dataclass instead\n                await websocket.send_json(\n                    {\n                        \"type\": \"new_workflow\",\n                        \"session\": {\n                            \"id\": str(session.id),\n                            \"run\": session.run,\n                            \"trigger\": asjson(session.trigger),\n                        },\n                        \"url\": url,\n                    }\n                )\n                # TODO: add timeout\n                accept = await websocket.receive_json()\n\n                # TODO: dont assume packets have these fields\n                if not (\n                    accept[\"session_id\"] == str(session.id)\n                    and accept[\"session_run\"] == session.run\n                ):\n                    # Malformed packet. At some point send this back to the\n                    # user, but for now just ignore it and move on\n\n                    continue\n\n                # Update status to pending\n                session.status = SessionStatus.PENDING\n                session_repo.update(session)\n\n                workflow_id = session_repo.get_workflow_id_from_session(session)\n                assert workflow_id\n\n                terminal_session_repo = di.terminal_session_repo()\n\n                terminal = terminal_session_repo.get_by_workflow_id(workflow_id)\n                assert terminal\n\n                session_running = True\n\n                async def session_stopper(terminal: TerminalSession) -> None:\n                    await terminal.should_stop.wait()\n\n                    await websocket.send_json(\n                        {\n                            \"type\": \"stop_workflow\",\n                            \"session\": {\n                                \"id\": str(session.id),  # noqa: B023\n                                \"run\": session.run,  # noqa: B023\n                            },\n                        }\n                    )\n\n                    nonlocal session_running\n                    session_running = False\n\n                task = asyncio.create_task(session_stopper(terminal))\n\n                while session_running:\n                    packet = await websocket.receive_json()\n\n                    if not (\n                        packet[\"session_id\"] == str(session.id)\n                        and packet[\"session_run\"] == session.run\n                    ):\n                        # Malformed packet. At some point send this back to the\n                        # user, but for now just ignore it and move on\n\n                        continue\n\n                    if packet[\"type\"] == \"update_workflow\":\n                        if stdout := packet.get(\"stdout\"):\n                            terminal.append(stdout.encode())\n\n                        if packet.get(\"status\"):\n                            session.finish()\n                            session_repo.update(session)\n\n                            break\n\n                task.cancel()\n\n            await asyncio.sleep(1)\n\n    # TODO: handle cancel error\n    except (WebSocketDisconnect, ConnectionClosed):\n        pass\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "cicada/api/endpoints/runner.py", "language": "python", "complexity": "medium", "star_count": 571, "quality_score": 0.6799999999999999, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 142}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from dataclasses import dataclass\nfrom typing import Literal\n\nfrom fastapi import APIRouter\n\nfrom cicada.api.endpoints.di import Di\nfrom cicada.api.endpoints.login_util import CurrentUser\nfrom cicada.application.exceptions import InvalidRequest, NotFound\nfrom cicada.application.secret.delete_installation_secret import DeleteInstallationSecret\nfrom cicada.application.secret.delete_repository_secret import DeleteRepositorySecret\nfrom cicada.application.secret.list_installation_secrets import ListInstallationSecrets\nfrom cicada.application.secret.list_repository_secrets import ListRepositorySecrets\nfrom cicada.application.secret.set_installation_secret import SetInstallationSecret\nfrom cicada.application.secret.set_repository_secret import SetRepositorySecret\nfrom cicada.domain.installation import InstallationId\nfrom cicada.domain.secret import Secret\n\nrouter = APIRouter()\n\n\n@dataclass\nclass UpdateSecret:\n    scope: Literal[\"r\", \"i\"]\n    key: str\n    value: str\n    installation_id: InstallationId | None = None\n\n    # TODO: use repository UUID on frontend\n    repository_url: str | None = None\n    repository_provider: str | None = None\n\n\n@dataclass\nclass DeleteSecret:\n    scope: Literal[\"r\", \"i\"]\n    key: str\n    installation_id: InstallationId | None = None\n\n    # TODO: use repository UUID on frontend\n    repository_url: str | None = None\n    repository_provider: str | None = None\n\n\n@router.put(\"/api/secret\")\nasync def update_secret(di: Di, user: CurrentUser, secret: UpdateSecret) -> None:\n    if secret.scope == \"r\":\n        # TODO: move validation to dataclass\n\n        if not secret.repository_url:\n            raise InvalidRequest(\"Expected repository url\")\n\n        if not secret.repository_provider:\n            raise InvalidRequest(\"Expected repository provider\")\n\n        # TODO: switch to repository uuid instead of url+provider combo\n        repository_repo = di.repository_repo()\n\n        repository = repository_repo.get_repository_by_url_and_provider(\n            secret.repository_url,\n            secret.repository_provider,\n        )\n\n        if not repository:\n            raise NotFound(\"Could not find repository\")\n\n        cmd = SetRepositorySecret(repository_repo, di.secret_repo())\n\n        cmd.handle(user, repository.id, Secret(secret.key, secret.value))\n\n    else:\n        if not secret.installation_id:\n            raise InvalidRequest(\"Expected installation id\")\n\n        cmd = SetInstallationSecret(di.installation_repo(), di.secret_repo())  # type: ignore\n\n        cmd.handle(\n            user,\n            secret.installation_id,  # type: ignore\n            Secret(secret.key, secret.value),\n        )\n\n\n@router.get(\"/api/secret/repository\")\nasync def list_secrets_for_repo(\n    di: Di,\n    user: CurrentUser,\n    url: str,\n    provider: str,\n) -> list[str]:\n    # TODO: use repository uuid\n\n    repository_repo = di.repository_repo()\n\n    repository = repository_repo.get_repository_by_url_and_provider(\n        url,\n        provider,\n    )\n\n    if not repository:\n        raise NotFound(\"Could not find repository\")\n\n    cmd = ListRepositorySecrets(repository_repo, di.secret_repo())\n\n    return cmd.handle(user, repository.id)\n\n\n@router.get(\"/api/secret/installation/{uuid}\")\nasync def list_secrets_for_installation(\n    di: Di,\n    user: CurrentUser,\n    uuid: InstallationId,\n) -> list[str]:\n    cmd = ListInstallationSecrets(di.installation_repo(), di.secret_repo())\n\n    return cmd.handle(user, uuid)\n\n\n@router.delete(\"/api/secret\")\nasync def delete_secret(\n    di: Di,\n    user: CurrentUser,\n    secret: DeleteSecret,\n) -> None:\n    if secret.scope == \"r\":\n        if not (secret.repository_provider and secret.repository_url):\n            raise InvalidRequest(\"repository provider and url must be passed\")\n\n        repository_repo = di.repository_repo()\n\n        repo = repository_repo.get_repository_by_url_and_provider(\n            secret.repository_url,\n            secret.repository_provider,\n        )\n\n        if not repo:\n            raise NotFound(\"Repository not found\")\n\n        cmd = DeleteRepositorySecret(repository_repo, di.secret_repo())\n\n        cmd.handle(user, repo.id, secret.key)\n\n    else:\n        if not secret.installation_id:\n            raise InvalidRequest(\"installation id must be set\")\n\n        cmd = DeleteInstallationSecret(di.installation_repo(), di.secret_repo())  # type: ignore\n\n        cmd.handle(user, secret.installation_id, secret.key)  # type: ignore\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "cicada/api/endpoints/secret.py", "language": "python", "complexity": "medium", "star_count": 571, "quality_score": 0.62, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 149}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create CI/CD pipeline for automated build, test, and deployment using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nimport logging\nfrom asyncio import CancelledError, InvalidStateError, Task, create_task\nfrom functools import partial\n\nfrom fastapi import APIRouter, HTTPException, WebSocket, WebSocketDisconnect\nfrom fastapi.responses import JSONResponse\nfrom websockets.exceptions import ConnectionClosed\n\nfrom cicada.api.endpoints.di import Di\nfrom cicada.api.endpoints.login_util import CurrentUser, get_user_from_jwt\nfrom cicada.api.endpoints.task_queue import TaskQueue\nfrom cicada.api.infra.github.common import (\n    gather_workflows_via_trigger as gather_github_git_push_workflows,\n)\nfrom cicada.api.infra.github.workflows import run_workflow as run_github_workflow\nfrom cicada.api.infra.gitlab.workflows import gather_workflows as gather_gitlab_workflows\nfrom cicada.api.infra.gitlab.workflows import run_workflow as run_gitlab_workflow\nfrom cicada.api.infra.notifications.send_email import send_failure_notifications\nfrom cicada.application.exceptions import CicadaException\nfrom cicada.application.session.rerun_session import RerunSession\nfrom cicada.application.session.stop_session import StopSession\nfrom cicada.application.session.stream_session import StreamSession\nfrom cicada.common.json import asjson\nfrom cicada.domain.session import Session, SessionId, SessionStatus\n\nrouter = APIRouter()\n\n\nlogger = logging.getLogger(\"cicada\")\n\n\n@router.post(\"/api/session/{session_id}/stop\")\nasync def stop_session(session_id: SessionId, di: Di, user: CurrentUser) -> None:\n    cmd = StopSession(di.session_repo(), di.session_terminators())\n\n    await cmd.handle(session_id, user)\n\n\nTASK_QUEUE = TaskQueue()\n\n\n@router.post(\"/api/session/{session_id}/rerun\")\nasync def rerun_session(session_id: SessionId, di: Di, user: CurrentUser) -> None:\n    # TODO: move to application\n    # TODO: test this\n\n    session_repo = di.session_repo()\n\n    # TODO: ensure this fails if user doesnt have write access\n    session = session_repo.get_session_by_session_id(session_id, user=user, permission=\"write\")\n\n    if not session:\n        return\n\n    provider = session.trigger.provider\n\n    if provider == \"github\":\n        gather = gather_github_git_push_workflows\n        workflow_wrapper = partial(run_github_workflow, di=di)\n\n    elif provider == \"gitlab\":\n        token = await di.gitlab_token_store().load_token(user)\n\n        if not token:\n            logger.error(\"Cannot rerun session %s\", session_id)\n            return\n\n        gather = partial(gather_gitlab_workflows, access_token=token.access_token)\n        workflow_wrapper = partial(run_gitlab_workflow, access_token=token.access_token, di=di)\n\n    else:\n        assert False\n\n    cmd = RerunSession(\n        session_repo,\n        di.terminal_session_repo(),\n        gather_workflows=gather,\n        workflow_runner=workflow_wrapper,\n        env_repo=di.environment_repo(),\n        repository_repo=di.repository_repo(),\n        installation_repo=di.installation_repo(),\n        secret_repo=di.secret_repo(),\n    )\n\n    async def run(old_session: Session) -> None:\n        session = await cmd.handle(old_session)\n\n        if session:\n            await send_failure_notifications(user, [session])\n\n    TASK_QUEUE.add(run(session))\n\n\n@router.get(\"/api/session/{uuid}/session_info\")\nasync def get_session_info(\n    uuid: SessionId,\n    di: Di,\n    user: CurrentUser,\n    # TODO: test query\n    run: int = -1,\n) -> JSONResponse:\n    # TODO: test failure if user cannot see session\n\n    session = di.session_repo().get_session_by_session_id(uuid, run, user, permission=\"read\")\n\n    if session:\n        return JSONResponse(asjson(session))\n\n    raise HTTPException(status_code=404, detail=\"Session not found\")\n\n\n@router.get(\"/api/session/recent\")\nasync def get_recent_sessions(\n    di: Di,\n    user: CurrentUser,\n    repo: str = \"\",\n    session: SessionId | None = None,\n) -> JSONResponse:\n    session_repo = di.session_repo()\n\n    if repo:\n        recent = session_repo.get_recent_sessions_for_repo(user, repository_url=repo)\n\n    elif session:\n        return JSONResponse(asjson(session_repo.get_runs_for_session(user, session)))\n\n    else:\n        recent = session_repo.get_recent_sessions(user)\n\n    return JSONResponse([asjson(session) for session in recent])\n\n\n@router.websocket(\"/ws/session/watch_status\")\nasync def watch_status(websocket: WebSocket, di: Di) -> None:  # pragma: no cover\n    \"\"\"\n    Open a websocket to listen for status updates for certain sessions. Once\n    connected, the user sends their JWT token to authenticate, then they send\n    a JSON array of string-encoded UUIDs for the sessions they want to listen\n    to. Then, as the sessions start to finish, the session data will be\n    returned to the user. Once all requested sessions have finished the\n    websocket will close.\n\n    This probably should be replaced with a pub-sub implementation that will\n    handle all of this for me.\n    \"\"\"\n\n    try:\n        await websocket.accept()\n\n        # TODO: add timeout here\n        # TODO: pull from header instead\n        jwt = await websocket.receive_text()\n\n        user = get_user_from_jwt(di.user_repo(), jwt)\n\n        if not user:\n            await websocket.send_json({\"error\": \"Websocket connection failed: Unauthorized\"})\n            return await websocket.close(code=1001)\n\n        # TODO: add timeout here\n        # TODO: limit size of JSON payload\n        data = await websocket.receive_json()\n\n        try:\n            session_ids = {SessionId(x) for x in data}\n\n        except (TypeError, ValueError):\n            await websocket.send_json({\"error\": \"Invalid JSON, expected array of UUIDs\"})\n            return await websocket.close()\n\n        session_repo = di.session_repo()\n\n        while session_ids:\n            for session_id in session_ids.copy():\n                session = session_repo.get_session_by_session_id(\n                    session_id, user=user, permission=\"read\"\n                )\n\n                if not session:\n                    session_ids.remove(session_id)\n\n                if session and session.status != SessionStatus.PENDING:\n                    session_ids.remove(session_id)\n\n                    await websocket.send_json(asjson(session))\n\n            await asyncio.sleep(1)\n\n        await websocket.close()\n\n    except (WebSocketDisconnect, ConnectionClosed):\n        pass\n\n\n@router.websocket(\"/ws/session/{uuid}\")\nasync def stream_session(\n    uuid: SessionId,\n    websocket: WebSocket,\n    di: Di,\n    run: int = -1,\n) -> None:  # pragma: no cover\n    task: Task[None] | None = None\n\n    try:\n        await websocket.accept()\n\n        # TODO: add timeout here\n        jwt = await websocket.receive_text()\n\n        session_repo = di.session_repo()\n\n        user = get_user_from_jwt(di.user_repo(), jwt)\n        session = session_repo.get_session_by_session_id(uuid)\n\n        if not (\n            user\n            and session\n            and session_repo.can_user_access_session(user, session, permission=\"read\")\n        ):\n            await websocket.send_json({\"error\": \"You do not have access to this session\"})\n            await websocket.close(code=1001)\n            return\n\n        async def stop_session() -> None:\n            cmd = StopSession(session_repo, di.session_terminators())\n\n            try:\n                await cmd.handle(uuid, user)\n\n            except CicadaException as exc:\n                await websocket.send_json({\"error\": str(exc)})\n\n        stream = StreamSession(\n            di.terminal_session_repo(),\n            session_repo,\n            stop_session,\n        )\n\n        async def command_sender() -> None:\n            async for command in websocket.iter_text():\n                stream.send_command(command)\n\n        task = create_task(command_sender())\n\n        async for data in stream.stream(uuid, run):\n            await websocket.send_json(data)\n\n    except (WebSocketDisconnect, ConnectionClosed):\n        return\n\n    except RuntimeError:\n        # A RuntimeError is thrown whenever trying to send data after the\n        # websocket is disconnected. There is no good way to detect this since\n        # uvicorn wraps the websocket connection, and the actual state of the\n        # websocket is not forwarded.\n\n        pass\n\n    finally:\n        if task:\n            task.cancel()\n\n            try:\n                task.result()\n\n            except (CancelledError, InvalidStateError):\n                pass\n\n            except (WebSocketDisconnect, ConnectionClosed):\n                return\n\n    await websocket.close()\n", "metadata": {"source": "github.com/Cicada-Software/cicada", "file_path": "cicada/api/endpoints/session.py", "language": "python", "complexity": "complex", "star_count": 571, "quality_score": 0.77, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 274}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CI\n\non:\n  pull_request:\n    branches: [ master ]\n\njobs:\n  ci:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Install flake8\n        run: pip install flake8\n      - name: Give executable permissions to run_ci.sh inside the scripts directory\n        run: chmod a+x scripts/run_ci.sh\n      - name: Run the ci script inside the scripts folder\n        run: bash scripts/run_ci.sh\n        shell: bash", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": ".github/workflows/ci_workflow.yml", "language": "yaml", "complexity": "simple", "star_count": 77723, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "language: \"python\"\npython:\n    - \"3.8\"\ninstall:\n    - pip install flake8\nscript:\n    - flake8 --max-line-length=100 .\n    - python tests/syntax_lint.py\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": ".travis.yml", "language": "yaml", "complexity": "simple", "star_count": 77723, "quality_score": 0.6000000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 9}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## How to contribute\n\nUse pull requests to contribute to the project.\n\nStick to the following format:\n\n\\<details>\n<summary>[Question]</summary><br><b>\n\n[Answer]\n\\</b></details>\n\n* If you added several questions and you would like to know how many questions are there you can use the script \"count_questions.sh\" in scripts directory.\n\n## What to avoid\n\n* Avoid adding installation questions. Those are the worst type of questions...\n* Don't copy questions and answers from other sources. They probably worked hard for adding them.\n* If you add new images, make sure they are free and can be used.\n\n## Before submitting the pull request\n\nYou can test your changes locally with the script `run_ci.sh` in scripts directory.\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "simple", "star_count": 77723, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 24}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS - Cloud Practitioner\n\nA summary of what you need to know for the exam can be found [here](https://aws.amazon.com/certification/certified-cloud-practitioner/)\n\n#### Cloud 101\n\n<details>\n<summary>What is cloud computing?</summary><br><b>\n\n[Wikipedia](https://en.wikipedia.org/wiki/Cloud_computing): \"Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user\"\n\nCloud computing also allows you to scale resources up or down as needed, paying only for what you use.\n</b></details>\n\n<details>\n<summary>What types of Cloud Computing services are there?</summary><br><b>\n\nIAAS  \nPAAS  \nSAAS\n</b></details>\n\n<details>\n<summary>Explain each of the following and give an example:\n\n  * IAAS\n  * PAAS\n  * SAAS</summary><br><b>\n\n- IAAS - Infrastructure As A Service is a cloud computing service where a cloud provider rents out IT infrastructure such as compute, networking resources and storage over the internet (e.g., AWS EC2).<br>\n\n- PAAS - Platform As A Service is a cloud hosting platform with an on-demand access to ready-to-use set of deployment, application management and DevOps tools (e.g., AWS Elastic Beanstalk).<br>\n\n- SAAS - Software As A Service is a software distribution model in which services are hosted by a cloud service provider (e.g., AWS WorkSpaces or any web-based email service).\n</b></details>\n\n<details>\n<summary>What types of clouds (or cloud deployments) are there?</summary><br><b>\n\n  * Public\n  * Hybrid\n  * Private\n</b></details>\n\n<details>\n<summary>Explain each of the following Cloud Computing Deployments:\n\n  * Public\n  * Hybrid\n  * Private</summary><br><b>\n\n- Public - Public cloud is when you leverage cloud services over the open internet on hardware owned by the cloud provider, but its usage is shared by other companies. It offers cost-effectiveness and ease of scaling.<br>\n\n- Hybrid - A hybrid cloud is a cloud computing environment that uses a mix of combining a public and private cloud environment, like an on-premises data center, and public CSPs. It provides greater flexibility and more deployment options.<br>\n\n- Private - Private cloud means that the cloud infrastructure is provisioned for exclusive use by a single organization. Resources are not shared with others, so it offers more control over security and data.\n  \n[Read more](https://aws.amazon.com/types-of-cloud-computing/)\n</b></details>\n\n#### AWS Global Infrastructure\n\n<details>\n<summary>Explain the following\n\n  * Availability zone\n  * Region\n  * Edge location</summary><br><b>\n\nAWS regions are data centers hosted across different geographical locations worldwide, each region is completely independent of one another.<br>\n\nWithin each region, there are multiple isolated locations known as Availability Zones. Multiple availability zones ensure high availability in case one of them goes down. Each Availability Zone is physically separated from others, with its own power, networking, and connectivity.<br>\n\nEdge locations are basically content delivery network endpoints which cache data and ensure lower latency and faster delivery to the users in any location. They are located in major cities around the world.\n</b></details>\n\n#### AWS Networking\n\n<details>\n<summary>What is VPC?</summary><br><b>\n\n\"A logically isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define\".\nRead more about it [here](https://aws.amazon.com/vpc).\n\nA VPC spans all the Availability Zones within a single region.\n</b></details>\n\n<details>\n<summary>True or False? VPC spans multiple regions</summary><br><b>\n\nFalse. A VPC is region-specific and cannot span multiple regions.\n</b></details>\n\n<details>\n<summary>True or False? Subnets belong to the same VPC, can be in different availability zones</summary><br><b>\n\nTrue. Just to clarify, a subnet must reside entirely in one AZ, but a single VPC can contain subnets across multiple AZs.\n</b></details>\n\n<details>\n<summary>What is an Internet Gateway?</summary><br><b>\n\n\"component that allows communication between instances in your VPC and the internet\" (AWS docs).\nRead more about it [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html)\n\nIt scales horizontally and is highly available, allowing inbound and outbound traffic to flow without imposing availability risks or bandwidth constraints.\n</b></details>\n\n<details>\n<summary>True or False? NACL allow or deny traffic on the subnet level</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? Multiple Internet Gateways can be attached to one VPC</summary><br><b>\n\nFalse. Only one internet gateway can be attached to a single VPC.\n</b></details>\n\n<details>\n<summary>True or False? Route Tables used to allow or deny traffic from the internet to AWS instances</summary><br><b>\n\nFalse. Route tables are used to direct traffic to the right destination (e.g., Internet Gateway, NAT Gateway, etc.), not to allow or deny traffic.\n</b></details>\n\n<details>\n<summary>Explain Security Groups and Network ACLs</summary><br><b>\n\n* NACL - security layer on the subnet level. They are stateless, meaning inbound and outbound rules are evaluated separately.<br>\n* Security Group - security layer on the instance level. They are stateful, meaning if you allow inbound traffic, outbound traffic is automatically allowed, and vice versa.\n\nRead more about it [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) and [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)\n</b></details>\n\n<details>\n<summary>What is AWS Direct Connect?</summary><br><b>\n\nAllows you to connect your corporate network to AWS network. It provides a dedicated network connection that can offer more consistent performance than internet-based connections.\n</b></details>\n\n#### AWS Compute\n\n<details>\n<summary>What is EC2?</summary><br><b>\n\n\"a web service that provides secure, resizable compute capacity in the cloud\".\nRead more [here](https://aws.amazon.com/ec2)\n\nEC2 allows you to quickly scale up or down to match resource needs, paying only for the compute time you consume.\n</b></details>\n\n<details>\n<summary>What is AMI?</summary><br><b>\n\nAmazon Machine Images is \"An Amazon Machine Image (AMI) provides the information required to launch an instance\".\nRead more [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html)\n\nAn AMI typically includes an operating system, application server, and applications, so you can quickly spin up new instances with the same configuration.\n</b></details>\n\n<details>\n<summary>What are the different source for AMIs?</summary><br><b>\n\n* Personal AMIs - AMIs you create\n* AWS Marketplace for AMIs - Paid AMIs usually bundled with licensed software\n* Community AMIs - Free\n\nYou can also share AMIs across accounts if needed.\n</b></details>\n\n<details>\n<summary>What is instance type?</summary><br><b>\n\n\"the instance type that you specify determines the hardware of the host computer used for your instance\"\nRead more about instance types [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html)\n\nInstance types vary by CPU, memory, storage, and networking capacity, e.g., t2.micro, c5.large, etc.\n</b></details>\n\n<details>\n<summary>True or False? The following are instance types available for a user in AWS:\n\n  * Compute optimized\n  * Network optimized\n  * Web optimized</summary><br><b>\n\nFalse. From the above list only compute optimized is available. There's no \"Web optimized\" or \"Network optimized\" instance type. You do have memory optimized, storage optimized, etc.\n</b></details>\n\n<details>\n<summary>What is EBS?</summary><br><b>\n\n\"provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices.\"\nMore on EBS [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html)\n\nEBS volumes are tied to an Availability Zone. They can be snapshotted to Amazon S3 for durability and can be detached/reattached between EC2 instances in the same AZ.\n</b></details>\n\n<details>\n<summary>What EC2 pricing models are there?</summary><br><b>\n\nOn Demand - pay a fixed rate by the hour/second with no commitment. You can provision and terminate at any time.<br>\nReserved - you get capacity reservation, basically purchase an instance for a fixed time period (1 or 3 years). The longer, the cheaper.<br>\nSpot - Enables you to bid whatever price you want for instances or pay the spot price. Ideal for workloads that can be interrupted.<br>\nDedicated Hosts - physical EC2 server dedicated for your use. Helps you address compliance requirements and use your own software licenses.\n</b></details>\n\n<details>\n<summary>What are Security Groups?</summary><br><b>\n\n\"A security group acts as a virtual firewall that controls the traffic for one or more instances\"\nMore on this subject [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html)\n\nThey are stateful, so any rule applied for inbound automatically applies to outbound, and vice versa (if the inbound rule is allowed).\n</b></details>\n\n<details>\n<summary>What can you attach to an EC2 instance in order to store data?</summary><br><b>\n\nEBS\n\nAdditionally, some instance types support Instance Store (ephemeral storage), and you can also mount EFS (file storage) if you need a shared filesystem across multiple instances.\n</b></details>\n\n<details>\n<summary>What EC2 RI types are there?</summary><br><b>\n\nStandard RI - most significant discount + suited for steady-state usage<br>\nConvertible RI - discount + change attribute of RI + suited for steady-state usage<br>\nScheduled RI - launch within time windows you reserve\n\nLearn more about EC2 RI [here](https://aws.amazon.com/ec2/pricing/reserved-instances)\n\nSome RIs also offer different payment options (no upfront, partial upfront, or all upfront) affecting the discount level.\n</b></details>\n\n#### AWS Containers\n\n<details>\n<summary>What is Amazon ECS?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. Customers such as Duolingo, Samsung, GE, and Cook Pad use ECS to run their most sensitive and mission critical applications because of its security, reliability, and scalability.\"\n\nLearn more [here](https://aws.amazon.com/ecs)\n</b></details>\n\n<details>\n<summary>What is Amazon ECR?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images.\"\n\nLearn more [here](https://aws.amazon.com/ecr)\n</b></details>\n\n<details>\n<summary>What is AWS Fargate?</summary><br><b>\n\nAmazon definition: \"AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\"\n\nLearn more [here](https://aws.amazon.com/fargate)\n</b></details>\n\n#### AWS Storage\n \n<details>\n<summary>Explain what is AWS S3?</summary><br><b>\n\nS3 stands for 3 S, Simple Storage Service.\nS3 is a object storage service which is fast, scalable and durable. S3 enables customers to upload, download or store any file or object that is up to 5 TB in size.\n\nMore on S3 [here](https://aws.amazon.com/s3) \n</b></details>\n\n<details>\n<summary>What is a bucket?</summary><br><b>\n\nAn S3 bucket is a resource which is similar to folders in a file system and allows storing objects, which consist of data.\n</b></details>\n\n<details>\n<summary>True or False? A bucket name must be globally unique</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>Explain folders and objects in regards to buckets</summary><br><b>\n\n* Folder - any sub folder in an s3 bucket\n* Object - The files which are stored in a bucket\n</b></details>\n\n<details>\n<summary>Explain the following:\n\n  * Object Lifecycles\n  * Object Sharing\n  * Object Versioning</summary><br><b>\n\n  * Object Lifecycles - Transfer objects between storage classes based on defined rules of time periods\n  * Object Sharing - Share objects via a URL link\n  * Object Versioning - Manage multiple versions of an object\n</b></details>\n\n<details>\n<summary>Explain Object Durability and Object Availability</summary><br><b>\n\nObject Durability: The percent over a one-year time period that a file will not be lost\nObject Availability: The percent over a one-year time period that a file will be accessible\n</b></details>\n\n<details>\n<summary>What is a storage class? What storage classes are there?</summary><br><b>\n\nEach object has a storage class assigned to, affecting its availability and durability. This also has effect on costs.\nStorage classes offered today:\n  * Standard:\n    * Used for general, all-purpose storage (mostly storage that needs to be accessed frequently)\n    * The most expensive storage class \n    * 11x9% durability\n    * 2x9% availability\n    * Default storage class\n\n  * Standard-IA (Infrequent Access)\n    * Long lived, infrequently accessed data but must be available the moment it's being accessed\n    * 11x9% durability\n    * 99.90% availability\n\n  * One Zone-IA (Infrequent Access):\n    * Long-lived, infrequently accessed, non-critical data\n    * Less expensive than Standard and Standard-IA storage classes\n    * 2x9% durability\n    * 99.50% availability\n    \n  * Intelligent-Tiering:\n    * Long-lived data with changing or unknown access patterns. Basically, In this class the data automatically moves to the class most suitable for you based on usage patterns\n    * Price depends on the used class\n    * 11x9% durability\n    * 99.90% availability\n\n  * Glacier: Archive data with retrieval time ranging from minutes to hours\n  * Glacier Deep Archive: Archive data that rarely, if ever, needs to be accessed with retrieval times in hours\n  * Both Glacier and Glacier Deep Archive are:\n    * The most cheap storage classes\n    * have 9x9% durability \n\nMore on storage classes [here](https://aws.amazon.com/s3/storage-classes)\n\n</b></details>\n\n<details>\n<summary>A customer would like to move data which is rarely accessed from standard storage class to the most cheapest class there is. Which storage class should be used?\n\n  * One Zone-IA\n  * Glacier Deep Archive\n  * Intelligent-Tiering</summary><br><b>\n\nGlacier Deep Archive\n</b></details>\n\n<details>\n<summary>What Glacier retrieval options are available for the user?</summary><br><b>\n\nExpedited, Standard and Bulk\n</b></details>\n\n<details>\n<summary>True or False? Each AWS account can store up to 500 PetaByte of data. Any additional storage will cost double</summary><br><b>\n\nFalse. Unlimited capacity.\n</b></details>\n\n<details>\n<summary>Explain what is Storage Gateway</summary><br><b>\n\n\"AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage\".\nMore on Storage Gateway [here](https://aws.amazon.com/storagegateway)\n</b></details>\n\n<details>\n<summary>Explain the following Storage Gateway deployments types\n\n  * File Gateway\n  * Volume Gateway\n  * Tape Gateway</summary><br><b>\n\nExplained in detail [here](https://aws.amazon.com/storagegateway/faqs)\n</b></details>\n\n<details>\n<summary>What is the difference between stored volumes and cached volumes?</summary><br><b>\n\nStored Volumes - Data is located at customer's data center and periodically backed up to AWS\nCached Volumes - Data is stored in AWS cloud and cached at customer's data center for quick access\n</b></details>\n\n<details>\n<summary>What is \"Amazon S3 Transfer Acceleration\"?</summary><br><b>\n\nAWS definition: \"Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket\"\n\nLearn more [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html)\n</b></details>\n\n<details>\n<summary>What is Amazon EFS?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources.\"\n\nLearn more [here](https://aws.amazon.com/efs)\n</b></details>\n\n<details>\n<summary>What is AWS Snowmobile?</summary><br><b>\n\n\"AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS.\"\n\nLearn more [here](https://aws.amazon.com/snowmobile)\n</b></details>\n\n#### AWS IAM\n\n<details>\n<summary>What is IAM? What are some of its features?</summary><br><b>\n\nIAM stands for Identity and Access Management, and is used for managing users, groups, access policies & roles\nFull explanation is [here](https://aws.amazon.com/iam)\n</b></details>\n\n<details>\n<summary>True or False? IAM configuration is defined globally and not per region</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? When creating an AWS account, root account is created by default. This is the recommended account to use and share in your organization</summary><br><b>\n\nFalse. Instead of using the root account, you should be creating users and use them.\n</b></details>\n\n<details>\n<summary>True or False? Groups in AWS IAM, can contain only users and not other groups</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? Users in AWS IAM, can belong only to a single group</summary><br><b>\n\nFalse. Users can belong to multiple groups.\n</b></details>\n\n<details>\n<summary>What are Roles?</summary><br><b>\n\nA way for allowing a service of AWS to use another service of AWS. You assign roles to AWS resources.\nFor example, you can make use of a role which allows EC2 service to accesses s3 buckets (read and write).\n</b></details>\n\n<details>\n<summary>What are Policies?</summary><br><b>\n\nPolicies documents used to give permissions as to what a user, group or role are able to do. Their format is JSON.\n</b></details>\n\n<details>\n<summary>A user is unable to access an s3 bucket. What might be the problem?</summary><br><b>\n\nThere can be several reasons for that. One of them is lack of policy. To solve that, the admin has to attach the user with a policy what allows him to access the s3 bucket.\n</b></details>\n\n<details>\n<summary>What should you use to:\n\n  * Grant access between two services/resources?\n  * Grant user access to resources/services?</summary><br><b>\n\n  * Role\n  * Policy\n</b></details>\n\n<details>\n<summary>What permissions does a new user have?</summary><br><b>\n\nOnly a login access.\n</b></details>\n\n##### AWS ELB\n\n<details>\n<summary>What is ELB (Elastic Load Balancing)?</summary><br><b>\n\nAWS definition: \"Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.\"\n\nMore on ELB [here](https://aws.amazon.com/elasticloadbalancing)\n</b></details>\n\n<details>\n<summary>What is auto scaling?</summary><br><b>\n\nAWS definition: \"AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost\"\n\nRead more about auto scaling [here](https://aws.amazon.com/autoscaling)\n</b></details>\n\n<details>\n<summary>True or False? Auto Scaling is about adding resources (such as instances) and not about removing resource</summary><br><b>\n\nFalse. Auto scaling adjusts capacity and this can mean removing some resources based on usage and performances.\n</b></details>\n\n<details>\n<summary>What types of load balancers are supported in EC2 and what are they used for?</summary><br><b>\n\n  * Application LB - layer 7 traffic\n  * Network LB - ultra-high performances or static IP address\n  * Classic LB - low costs, good for test or dev environments\n</b></details>\n\n#### AWS DNS\n\n<details>\n<summary>What is Route 53?</summary><br><b>\n\n\"Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service\"\nSome of Route 53 features:\n  * Register domain\n  * DNS service - domain name translations\n  * Health checks - verify your app is available\n\nMore on Route 53 [here](https://aws.amazon.com/route53)\n</b></details>\n\n#### AWS CloudFront\n\n<details>\n<summary>Explain what is CloudFront</summary><br><b>\n\nAWS definition: \"Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\"\n\nMore on CloudFront [here](https://aws.amazon.com/cloudfront)\n</b></details>\n\n<details>\n<summary>Explain the following\n\n  * Origin\n  * Edge location\n  * Distribution</summary><br><b>\n</b></details>\n\n#### AWS Monitoring & Logging\n\n<details>\n<summary>What is AWS CloudWatch?</summary><br><b>\n\nAWS definition: \"Amazon CloudWatch is a monitoring and observability service...\"\n\nMore on CloudWatch [here](https://aws.amazon.com/cloudwatch)\n</b></details>\n\n<details>\n<summary>What is AWS CloudTrail?</summary><br><b>\n\nAWS definition: \"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\"\n\nRead more on CloudTrail [here](https://aws.amazon.com/cloudtrail)\n</b></details>\n\n<details>\n<summary>What is Simply Notification Service?</summary><br><b>\n\nAWS definition: \"a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\"\n\nRead more about it [here](https://aws.amazon.com/sns)\n</b></details>\n\n<details>\n<summary>Explain the following in regards to SNS:\n\n  * Topics\n  * Subscribers\n  * Publishers</summary><br><b>\n\n  * Topics - used for grouping multiple endpoints\n  * Subscribers - the endpoints where topics send messages to \n  * Publishers - the provider of the message (event, person, ...)\n</b></details>\n\n#### AWS Security \n\n<details>\n<summary>What is the shared responsibility model? What AWS is responsible for and what the user is responsible for based on the shared responsibility model?</summary><br><b>\n\nThe shared responsibility model defines what the customer is responsible for and what AWS is responsible for. For example, AWS is responsible for security \"of\" the cloud, while the customer is responsible for security \"in\" the cloud.\n\nMore on the shared responsibility model [here](https://aws.amazon.com/compliance/shared-responsibility-model)\n</b></details>\n\n<details>\n<summary>True or False? Based on the shared responsibility model, Amazon is responsible for physical CPUs and security groups on instances</summary><br><b>\n\nFalse. It is responsible for Hardware in its sites but not for security groups which created and managed by the users.\n</b></details>\n\n<details>\n<summary>Explain \"Shared Controls\" in regards to the shared responsibility model</summary><br><b>\n\nAWS definition: \"apply to both the infrastructure layer and customer layers, but in completely separate contexts or perspectives. In a shared control, AWS provides the requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services\"\n\nLearn more about it [here](https://aws.amazon.com/compliance/shared-responsibility-model)\n</b></details>\n\n<details>\n<summary>What is the AWS compliance program?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS Artifact?</summary><br><b>\n\nAWS definition: \"AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS‚Äô security and compliance reports and select online agreements.\"\n\nRead more about it [here](https://aws.amazon.com/artifact)\n</b></details>\n\n<details>\n<summary>What is AWS Inspector?</summary><br><b>\n\nAWS definition: \"Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.\"\"\n\nLearn more [here](https://aws.amazon.com/inspector)\n</b></details>\n\n<details>\n<summary>What is AWS Guarduty?</summary><br><b>\n\nGuarduty is a threat detection service that monitors your AWS accounts to help detect and mitigate malicious activity\n</b></details>\n\n<details>\n<summary>What is AWS Shield?</summary><br><b>\n\nAWS definition: \"AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.\"\n</b></details>\n\n<details>\n<summary>What is AWS WAF? Give an example of how it can used and describe what resources or services you can use it with</summary><br><b>\n\nAn AWS Web Application Firewall (WAF) can filter out unwanted web traffic (bots), and protect against attacks like SQL injection and cross-site scripting. One service you could use it with would be Amazon CloudFront, a CDN service, to block attacks before they reach your origin servers\n</b></details>\n\n<details>\n<summary>What AWS VPN is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the difference between Site-to-Site VPN and Client VPN?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS CloudHSM?</summary><br><b>\n\nAmazon definition: \"AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud.\"\n\nLearn more [here](https://aws.amazon.com/cloudhsm)\n</b></details>\n\n<details>\n<summary>True or False? AWS Inspector can perform both network and host assessments</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>What is AWS Acceptable Use Policy?</summary><br><b>\n\nIt describes prohibited uses of the web services offered by AWS.\nMore on AWS Acceptable Use Policy [here](https://aws.amazon.com/aup)\n</b></details>\n\n<details>\n<summary>What is AWS Key Management Service (KMS)?</summary><br><b>\n\nAWS definition: \"KMS makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.\"\nMore on KMS [here](https://aws.amazon.com/kms)\n</b></details>\n\n<details>\n<summary>True or False? A user is not allowed to perform penetration testing on any of the AWS services</summary><br><b>\n\nFalse. On some services, like EC2, CloudFront and RDS, penetration testing is allowed.\n</b></details>\n\n<details>\n<summary>True or False? DDoS attack is an example of allowed penetration testing activity</summary><br><b>\n\nFalse.\n</b></details>\n\n<details>\n<summary>True or False? AWS Access Key is a type of MFA device used for AWS resources protection</summary><br><b>\n\nFalse. Security key is an example of an MFA device.\n</b></details>\n\n<details>\n<summary>What is Amazon Cognito?</summary><br><b>\n\nAmazon definition: \"Amazon Cognito handles user authentication and authorization for your web and mobile apps.\"\n\nLearn more [here](https://docs.aws.amazon.com/cognito/index.html)\n</b></details>\n\n<details>\n<summary>What is AWS ACM?</summary><br><b>\n\nAmazon definition: \"AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.\"\n\nLearn more [here](https://aws.amazon.com/certificate-manager)\n</b></details>\n\n#### AWS Databases\n\n<details>\n<summary>What is AWS RDS?</summary><br><b>\n\nAmazon Relational Database Service (RDS) is a service for setting up  and managing resizable, cost-efficient relational databases\nresource\n\nLearn more [here](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html)\n</b></details>\n\n<details>\n<summary>What is AWS DynamoDB?</summary><br><b>\n</b></details>\n\n<details>\n<summary>Explain \"Point-in-Time Recovery\" feature in DynamoDB</summary><br><b>\n\nAmazon definition: \"You can create on-demand backups of your Amazon DynamoDB tables, or you can enable continuous backups using point-in-time recovery. For more information about on-demand backups, see On-Demand Backup and Restore for DynamoDB.\"\n\nLearn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html)\n</b></details>\n\n<details>\n<summary>Explain \"Global Tables\" in DynamoDB</summary><br><b>\n\nAmazon definition: \"A global table is a collection of one or more replica tables, all owned by a single AWS account.\"\n\nLearn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html)\n</b></details>\n\n<details>\n<summary>What is DynamoDB Accelerator?</summary><br><b>\n\nAmazon definition: \"Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement ‚Äì from milliseconds to microseconds...\"\n\nLearn more [here](https://aws.amazon.com/dynamodb/dax)\n</b></details>\n\n<details>\n<summary>What is AWS Redshift and how is it different than RDS?</summary><br><b>\n\nAWS Redshift is a cloud data warehousing service that is geared towards handling massive amounts of data (think petabytes) and being able to execute complex queries. In contrast, Amazon RDS is best suited for things like web applications requiring simple queries with more frequent transactions, and on a smaller scale.\n</b></details>\n\n<details>\n<summary>What is AWS ElastiCache? For what cases is it used?</summary><br><b>\n\nAmazon Elasticache is a fully managed Redis or Memcached in-memory data store.                                                                       \nIt's great for use cases like two-tier web applications where the most frequently accesses data is stored in ElastiCache so response time is optimal.\n</b></details>\n\n<details>\n<summary>What is Amazon Aurora</summary><br><b>\n\nA MySQL & Postgresql based relational database. Also, the default database proposed for the user when using RDS for creating a database.\nGreat for use cases like two-tier web applications that has a MySQL or Postgresql database layer and you need automated backups for your application.\n</b></details>\n\n<details>\n<summary>What is Amazon DocumentDB?</summary><br><b>\n\nAmazon definition: \"Amazon DocumentDB (with MongoDB compatibility) is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data.\"\n\nLearn more [here](https://aws.amazon.com/documentdb)\n</b></details>\n\n<details>\n<summary>What \"AWS Database Migration Service\" is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What type of storage is used by Amazon RDS?</summary><br><b>\n\nEBS\n</b></details>\n\n<details>\n<summary>Explain Amazon RDS Read Replicas</summary><br><b>\n\nAWS definition: \"Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\"\nRead more about [here](https://aws.amazon.com/rds/features/read-replicas)\n</b></details>\n\n#### AWS Serverless Compute\n\n<details>\n<summary>Explain what is AWS Lambda</summary><br><b>\n\nAWS definition: \"AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\"\n\nRead more on it [here](https://aws.amazon.com/lambda)\n</b></details>\n\n<details>\n<summary>True or False? In AWS Lambda, you are charged as long as a function exists, regardless of whether it's running or not</summary><br><b>\n\nFalse. Charges are being made when the code is executed.\n</b></details>\n\n<details>\n<summary>Which of the following set of languages Lambda supports?\n\n  * R, Swift, Rust, Kotlin\n  * Python, Ruby, Go \n  * Python, Ruby, PHP</summary><br><b>\n\n  * Python, Ruby, Go \n</b></details>\n\n#### Identify the service or tool\n\n<details>\n<summary>What would you use for automating code/software deployments?</summary><br><b>\n\nAWS CodeDeploy\n</b></details>\n\n<details>\n<summary>What would you use for easily creating similar AWS environments/resources for different customers?</summary><br><b>\n\nCloudFormation\n</b></details>\n\n<details>\n<summary>Which service would you use for building a website or web application?</summary><br><b>\n\nLightsail or Elastic Beanstalk\n</b></details>\n\n<details>\n<summary>Which tool would you use for choosing between Reserved instances or On-Demand instances?</summary><br><b>\n\nCost Explorer\n</b></details>\n\n<details>\n<summary>What would you use to check how many unassociated Elastic IP address you have?</summary><br><b>\n\nTrusted Advisor\n</b></details>\n\n<details>\n<summary>What service allows you to transfer large amounts (Petabytes) of data in and out of the AWS cloud?</summary><br><b>\n\nAWS Snowball\n</b></details>\n\n<details>\n<summary>What provides a virtual network dedicated to your AWS account?</summary><br><b>\n\nVPC\n</b></details>\n\n<details>\n<summary>What you would use for having automated backups for an application that has MySQL database layer?</summary><br><b>\n\nAmazon Aurora\n</b></details>\n\n<details>\n<summary>What would you use to migrate on-premise database to AWS?</summary><br><b>\n\nAWS Database Migration Service (DMS)\n</b></details>\n\n<details>\n<summary>What would you use to check why certain EC2 instances were terminated?</summary><br><b>\n\nAWS CloudTrail\n</b></details>\n\n<details>\n<summary>What would you use for SQL database?</summary><br><b>\n\nAWS RDS\n</b></details>\n\n<details>\n<summary>What would you use for NoSQL database?</summary><br><b>\n\nAWS DynamoDB\n</b></details>\n\n<details>\n<summary>What would you use for running SQL queries interactively on S3?</summary><br><b>\n\nAWS Athena\n</b></details>\n\n<details>\n<summary>What would you use for adding image and video analysis to your application?</summary><br><b>\n\nAWS Rekognition\n</b></details>\n\n<details>\n<summary>Which service would you use for debugging and improving performances issues with your applications?</summary><br><b>\n\nAWS X-Ray\n</b></details>\n\n<details>\n<summary>Which service is used for sending notifications?</summary><br><b>\n\nSNS\n</b></details>\n\n<details>\n<summary>Which service would you use for monitoring malicious activity and unauthorized behavior in regards to AWS accounts and workloads?</summary><br><b>\n\nAmazon GuardDuty\n</b></details>\n\n<details>\n<summary>Which service would you use for centrally manage billing, control access, compliance, and security across multiple AWS accounts?</summary><br><b>\n\nAWS Organizations\n</b></details>\n\n<details>\n<summary>Which service would you use for web application protection?</summary><br><b>\n\nAWS WAF\n</b></details>\n\n<details>\n<summary>You would like to monitor some of your resources in the different services. Which service would you use for that?</summary><br><b>\n\nCloudWatch\n</b></details>\n\n<details>\n<summary>Which service would you use for performing security assessment?</summary><br><b>\n\nAWS Inspector\n</b></details>\n\n<details>\n<summary>Which service would you use for creating DNS record?</summary><br><b>\n\nRoute 53\n</b></details>\n\n<details>\n<summary>What would you use if you need a fully managed document database?</summary><br><b>\n\nAmazon DocumentDB\n</b></details>\n\n<details>\n<summary>Which service would you use to add access control (or sign-up, sign-in forms) to your web/mobile apps?</summary><br><b>\n\nAWS Cognito\n</b></details>\n\n<details>\n<summary>Which service would you use if you need messaging queue?</summary><br><b>\n\nSimple Queue Service (SQS)\n</b></details>\n\n<details>\n<summary>Which service would you use if you need managed DDOS protection?</summary><br><b>\n\nAWS Shield\n</b></details>\n\n<details>\n<summary>Which service would you use if you need store frequently used data for low latency access?</summary><br><b>\n\nElastiCache\n</b></details>\n\n<details>\n<summary>What would you use to transfer files over long distances between a client and an S3 bucket?</summary><br><b>\n\nAmazon S3 Transfer Acceleration\n</b></details>\n\n#### AWS Billing & Support\n\n<details>\n<summary>What is AWS Organizations?</summary><br><b>\n\nAWS definition: \"AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS.\"\nMore on Organizations [here](https://aws.amazon.com/organizations)\n</b></details>\n\n<details>\n<summary>Explain AWS pricing model</summary><br><b>\n\nIt mainly works on \"pay-as-you-go\" meaning you pay only for what are using and when you are using it.\nIn s3 you pay for 1. How much data you are storing 2. Making requests (PUT, POST, ...)\nIn EC2 it's based on the purchasing option (on-demand, spot, ...), instance type, AMI type and the region used.\n\nMore on AWS pricing model [here](https://aws.amazon.com/pricing)\n</b></details>\n\n<details>\n<summary>How one should estimate AWS costs when for example comparing to on-premise solutions?</summary><br><b>\n\n* TCO calculator\n* AWS simple calculator\n* Cost Explorer\n</b></details>\n\n<details>\n<summary>What basic support in AWS includes?</summary><br><b>\n\n* 24x7 customer service\n* Trusted Advisor\n* AWS personal Health Dashoard\n</b></details>\n\n<details>\n<summary>How are EC2 instances billed?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What AWS Pricing Calculator is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is Amazon Connect?</summary><br><b>\n\nAmazon definition: \"Amazon Connect is an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost.\"\n\nLearn more [here](https://aws.amazon.com/connect)\n</b></details>\n\n<details>\n<summary>What are \"APN Consulting Partners\"?</summary><br><b>\n\nAmazon definition: \"APN Consulting Partners are professional services firms that help customers of all types and sizes design, architect, build, migrate, and manage their workloads and applications on AWS, accelerating their journey to the cloud.\"\n\nLearn more [here](https://aws.amazon.com/partners/consulting)\n</b></details>\n\n<details>\n<summary>Which of the following are AWS accounts types (and are sorted by order)?\n\n  * Basic, Developer, Business, Enterprise\n  * Newbie, Intermediate, Pro, Enterprise\n  * Developer, Basic, Business, Enterprise\n  * Beginner, Pro, Intermediate Enterprise</summary><br><b>\n\n  * Basic, Developer, Business, Enterprise\n</b></details>\n\n<details>\n<summary>True or False? Region is a factor when it comes to EC2 costs/pricing</summary><br><b>\n\nTrue. You pay differently based on the chosen region.\n</b></details>\n\n<details>\n<summary>What is \"AWS Infrastructure Event Management\"?</summary><br><b>\n\nAWS Definition: \"AWS Infrastructure Event Management is a structured program available to Enterprise Support customers (and Business Support customers for an additional fee) that helps you plan for large-scale events such as product or application launches, infrastructure migrations, and marketing events.\"\n</b></details>\n\n#### AWS Automation\n\n<details>\n<summary>What is AWS CodeDeploy?</summary><br><b>\n\nAmazon definition: \"AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.\"\n\nLearn more [here](https://aws.amazon.com/codedeploy)\n</b></details>\n\n<details>\n<summary>Explain what is CloudFormation</summary><br><b>\n</b></details>\n\n#### AWS Misc\n\n<details>\n<summary>What is AWS Lightsail?</summary><br><b>\n\nAWS definition: \"Lightsail is an easy-to-use cloud platform that offers you everything needed to build an application or website, plus a cost-effective, monthly plan.\"\n</b></details>\n\n<details>\n<summary>What is AWS Rekognition?</summary><br><b>\n\nAWS definition: \"Amazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use.\"\n\nLearn more [here](https://aws.amazon.com/rekognition)\n</b></details>\n\n<details>\n<summary>What AWS Resource Groups used for?</summary><br><b>\n\nAmazon definition: \"You can use resource groups to organize your AWS resources. Resource groups make it easier to manage and automate tasks on large numbers of resources at one time. \"\n\nLearn more [here](https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html)\n</b></details>\n\n<details>\n<summary>What is AWS Global Accelerator?</summary><br><b>\n\nAmazon definition: \"AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users...\"\n\nLearn more [here](https://aws.amazon.com/global-accelerator)\n</b></details>\n\n<details>\n<summary>What is AWS Config?</summary><br><b>\n\nAmazon definition: \"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources.\"\n\nLearn more [here](https://aws.amazon.com/config)\n</b></details>\n\n<details>\n<summary>What is AWS X-Ray?</summary><br><b>\n\nAWS definition: \"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.\"\nLearn more [here](https://aws.amazon.com/xray)\n</b></details>\n\n<details>\n<summary>What is AWS OpsWorks?</summary><br><b>\n\nAmazon definition: \"AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.\"\n\nLearn more about it [here](https://aws.amazon.com/opsworks)\n</b></details>\n\n<details>\n<summary>What is AWS Service Catalog?</summary><br><b>\n\nAmazon definition: \"AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS.\"\n\nLearn more [here](https://aws.amazon.com/servicecatalog)\n</b></details>\n\n<details>\n<summary>What is AWS CAF?</summary><br><b>\n\nAmazon definition: \"AWS Professional Services created the AWS Cloud Adoption Framework (AWS CAF) to help organizations design and travel an accelerated path to successful cloud adoption. \"\n\nLearn more [here](https://aws.amazon.com/professional-services/CAF)\n</b></details>\n\n<details>\n<summary>What is AWS Cloud9?</summary><br><b>\n\nAWS definition: \"AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser\"\n</b></details>\n\n<details>\n<summary>What is AWS Application Discovery Service?</summary><br><b>\n\nAmazon definition: \"AWS Application Discovery Service helps enterprise customers plan migration projects by gathering information about their on-premises data centers.\"\n\nLearn more [here](https://aws.amazon.com/application-discovery)\n</b></details>\n\n<details>\n<summary>What is the Trusted Advisor?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the AWS well-architected framework and what pillars it's based on?</summary><br><b>\n\nAWS definition: \"The Well-Architected Framework has been developed to help cloud architects build secure, high-performing, resilient, and efficient infrastructure for their applications. Based on five pillars ‚Äî operational excellence, security, reliability, performance efficiency, and cost optimization\"\n\nLearn more [here](https://aws.amazon.com/architecture/well-architected)\n</b></details>\n\n<details>\n<summary>What AWS services are serverless (or have the option to be serverless)?</summary><br><b>\n\nAWS Lambda\nAWS Athena\n</b></details>\n\n<details>\n<summary>What is AWS EMR?</summary><br><b>\n\nAWS definition: \"big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto.\"\n\nLearn more [here](https://aws.amazon.com/emr)\n</b></details>\n\n<details>\n<summary>What is AWS Athena?</summary><br><b>\n\n\"Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.\"\n\nLearn more about AWS Athena [here](https://aws.amazon.com/athena)\n</b></details>\n\n<details>\n<summary>What is Amazon Cloud Directory?</summary><br><b>\n\nAmazon definition: \"Amazon Cloud Directory is a highly available multi-tenant directory-based store in AWS. These directories scale automatically to hundreds of millions of objects as needed for applications.\"\n\nLearn more [here](https://docs.aws.amazon.com/clouddirectory/latest/developerguide/what_is_cloud_directory.html)\n</b></details>\n\n<details>\n<summary>What is AWS Elastic Beanstalk?</summary><br><b>\n\nAWS definition: \"AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services...You can simply upload your code and Elastic Beanstalk automatically handles the deployment\"\n\nLearn more about it [here](https://aws.amazon.com/elasticbeanstalk)\n</b></details>\n\n<details>\n<summary>What is AWS SWF?</summary><br><b>\n\nAmazon definition: \"Amazon SWF helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud.\"\n\nLearn more on Amazon Simple Workflow Service [here](https://aws.amazon.com/swf)\n</b></details>\n\n<details>\n<summary>What is Simple Queue Service (SQS)?</summary><br><b>\n\nAWS definition: \"Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications\".\n\nLearn more about it [here](https://aws.amazon.com/sqs)\n</b></details>\n\n#### AWS Disaster Recovery\n\n<details>\n<summary>In regards to disaster recovery, what is RTO and RPO?</summary><br><b>\n\nRTO - The maximum acceptable length of time that your application can be offline.\n\nRPO - The maximum acceptable length of time during which data might be lost from your application due to an incident.\n</b></details>\n\n<details>\n<summary>What types of disaster recovery techniques AWS supports?</summary><br><b>\n\n* The Cold Method - Periodically backups and sending the backups off-site<br>\n* Pilot Light - Data is mirrored to an environment which is always running\n* Warm Standby - Running scaled down version of production environment\n* Multi-site - Duplicated environment that is always running\n</b></details>\n\n<details>\n<summary>Which disaster recovery option has the highest downtime and which has the lowest?</summary><br><b>\n\nLowest - Multi-site\nHighest - The cold method\n</b></details>\n\n### Final Note\n\nGood luck! You can do it :)\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "certificates/aws-cloud-practitioner.md", "language": "markdown", "complexity": "complex", "star_count": 77723, "quality_score": 0.79, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 1281}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS Cloud SysOps Administration Associate\n\nA summary of what you need to know for the exam can be found [here](https://aws.amazon.com/certification/certified-sysops-admin-associate)\n\n### <b>  Who should take this exam? </b>\n<br>\nAWS Certified SysOps Administrator - Associate is intended for system administrators in cloud operations roles to validate technical skills. \n\n<summary>Before you take this exam, we recommend you have :</summary><br><b>\n\n  * A minimum of one year of hands-on experience with AWS technology\n  * Experience deploying, managing, and operating workloads on AWS as well as implementing security controls and compliance requirements\n  * Familiarity with using both the AWS Management Console and the AWS Command Line Interface (CLI)\n  * Understanding of the AWS Well-Architected Framework as well as AWS networking and security services\n</b>\n\n### <b>  Prepare for your exam </b>\n<br>\nGet started with free resources or explore additional resources, including Official Practice Exams, with a subscription to AWS Skill Builder.\n\n * AWS Cloud SysOps Guide (SOA-C02) [here](https://d1.awsstatic.com/training-and-certification/docs-sysops-associate/AWS-Certified-SysOps-Administrator-Associate_Exam-Guide.pdf)\n * AWS Certified SysOps Administrator - Associate Official Practice Question Set (FREE) [here](https://explore.skillbuilder.aws/learn/course/external/view/elearning/12485/aws-certified-sysops-administrator-associate-practice-question-set-soa-c02-english?syops=sec&sec=prep)\n * Exam Prep: AWS Certified SysOps Administrator - Associate (FREE) [here](https://explore.skillbuilder.aws/learn/course/external/view/elearning/9313/exam-prep-aws-certified-sysops-administrator-associate)\n\n ### <b> Certification resource </b>\n <br>\n This is a resource for studying and preparing for the AWS Cloud SysOps Associate exam.\n\n * Architecting for the cloud [here](https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf)\n * AWS Well-Architected Framework [here](https://d0.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf)\n * Development and Test on Amazon Web Services [here](https://media.amazonwebservices.com/AWS_Development_Test_Environments.pdf)\n * Backup, Archive, and Restore Approaches Using AWS [here](https://d0.awsstatic.com/whitepapers/Backup_Archive_and_Restore_Approaches_Using_AWS.pdf)\n * How AWS Pricing Works - AWS Pricing Overview [here](https://d0.awsstatic.com/whitepapers/aws_pricing_overview.pdf)\n * Sample Question AWS SOA-C02 [here](https://d1.awsstatic.com/training-and-certification/docs-sysops-associate/AWS-Certified-SysOps-Administrator-Associate_Sample-Questions.pdf)", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "certificates/aws-cloud-sysops-associate.md", "language": "markdown", "complexity": "medium", "star_count": 77723, "quality_score": 0.8, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 34}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "ci_cd_pipelines", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create CI/CD pipeline for automated build, test, and deployment\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS - Solutions Architect Associate \n\nLast update: 2021\n\n#### AWS Global Infrastructure\n\n<details>\n<summary>Explain the following\n\n  * Availability zone\n  * Region\n  * Edge location</summary><br><b>\n\nAWS regions are data centers hosted across different geographical locations worldwide, each region is completely independent of one another.<br>\n\nWithin each region, there are multiple isolated locations known as Availability Zones. Multiple availability zones ensure high availability in case one of them goes down.<br>\n\nEdge locations are basically content delivery network which caches data and insures lower latency and faster delivery to the users in any location. They are located in major cities in the world.\n</b></details>\n\n#### AWS - IAM\n\n<details>\n<summary>What is IAM? What are some of its features?</summary><br><b>\n\nFull explanation is [here](https://aws.amazon.com/iam)\nIn short: it's used for managing users, groups, access policies & roles\n</b></details>\n\n<details>\n<summary>True or False? IAM configuration is defined globally and not per region</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? When creating an AWS account, root account is created by default. This is the recommended account to use and share in your organization</summary><br><b>\n\nFalse. Instead of using the root account, you should be creating users and use them.\n</b></details>\n\n<details>\n<summary>True or False? Groups in AWS IAM, can contain only users and not other groups</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? Users in AWS IAM, can belong only to a single group</summary><br><b>\n\nFalse. Users can belong to multiple groups.\n</b></details>\n\n<details>\n<summary>What are Roles?</summary><br><b>\n\nA way for allowing a service of AWS to use another service of AWS. You assign roles to AWS resources.\nFor example, you can make use of a role which allows EC2 service to accesses s3 buckets (read and write).\n</b></details>\n\n<details>\n<summary>What are Policies?</summary><br><b>\n\nPolicies documents used to give permissions as to what a user, group or role are able to do. Their format is JSON.\n</b></details>\n\n<details>\n<summary>A user is unable to access an s3 bucket. What might be the problem?</summary><br><b>\n\nThere can be several reasons for that. One of them is lack of policy. To solve that, the admin has to attach the user with a policy what allows him to access the s3 bucket.\n</b></details>\n\n<details>\n<summary>What should you use to:\n\n  * Grant access between two services/resources?\n  * Grant user access to resources/services?</summary><br><b>\n\n  * Role\n  * Policy\n</b></details>\n\n<details>\n<summary>What permissions does a new user have?</summary><br><b>\n\nOnly a login access.\n</b></details>\n\n\n#### AWS Networking\n\n<details>\n<summary>What is VPC?</summary><br><b>\n\n\"A logically isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define\"\nRead more about it [here](https://aws.amazon.com/vpc).\n</b></details>\n\n<details>\n<summary>True or False? VPC spans multiple regions</summary><br><b>\n\nFalse\n</b></details>\n\n<details>\n<summary>True or False? Subnets belong to the same VPC, can be in different availability zones</summary><br><b>\n\nTrue. Just to clarify, a subnet must reside entirely in one AZ.\n</b></details>\n\n<details>\n<summary>What is an Internet Gateway?</summary><br><b>\n\n\"component that allows communication between instances in your VPC and the internet\" (AWS docs).\nRead more about it [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html)\n</b></details>\n\n<details>\n<summary>True or False? NACL allow or deny traffic on the subnet level</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? Multiple Internet Gateways can be attached to one VPC</summary><br><b>\n\nFalse. Only one internet gateway can be attached to a single VPC.\n</b></details>\n\n<details>\n<summary>True or False? Route Tables used to allow or deny traffic from the internet to AWS instances</summary><br><b>\n\nFalse.\n</b></details>\n\n<details>\n<summary>Explain Security Groups and Network ACLs</summary><br><b>\n\n* NACL - security layer on the subnet level.\n* Security Group - security layer on the instance level.\n\nRead more about it [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) and [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)\n</b></details>\n\n<details>\n<summary>What is AWS Direct Connect?</summary><br><b>\n\nAllows you to connect your corporate network to AWS network.\n</b></details>\n\n#### AWS Compute\n\n<details>\n<summary>What is EC2?</summary><br><b>\n\n\"a web service that provides secure, resizable compute capacity in the cloud\".\nRead more [here](https://aws.amazon.com/ec2)\n</b></details>\n\n<details>\n<summary>True or False? EC2 is a regional service</summary><br><b>\n\nTrue. As opposed to IAM for example, which is a global service, EC2 is a regional service.\n</b></details>\n\n<details>\n<summary>What is AMI?</summary><br><b>\n\nAmazon Machine Images is \"An Amazon Machine Image (AMI) provides the information required to launch an instance\".\nRead more [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html)\n</b></details>\n\n<details>\n<summary>What are the different source for AMIs?</summary><br><b>\n\n* Personal AMIs - AMIs you create\n* AWS Marketplace for AMIs - Paid AMIs usually with bundled with licensed software\n* Community AMIs - Free\n</b></details>\n\n<details>\n<summary>What is instance type?</summary><br><b>\n\n\"the instance type that you specify determines the hardware of the host computer used for your instance\"\nRead more about instance types [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html)\n</b></details>\n\n<details>\n<summary>True or False? The following are instance types available for a user in AWS:\n\n  * Compute optimized\n  * Network optimized\n  * Web optimized</summary><br><b>\n\nFalse. From the above list only compute optimized is available.\n</b></details>\n\n<details>\n<summary>What is EBS?</summary><br><b>\n\n\"provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices.\"\nMore on EBS [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html)\n</b></details>\n\n<details>\n<summary>What EC2 pricing models are there?</summary><br><b>\n\nOn Demand - pay a fixed rate by the hour/second with no commitment. You can provision and terminate it at any given time.\nReserved - you get capacity reservation, basically purchase an instance for a fixed time of period. The longer, the cheaper.\nSpot - Enables you to bid whatever price you want for instances or pay the spot price.\nDedicated Hosts - physical EC2 server dedicated for your use.\n</b></details>\n\n<details>\n<summary>What are Security Groups?</summary><br><b>\n\n\"A security group acts as a virtual firewall that controls the traffic for one or more instances\"\nMore on this subject [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html)\n</b></details>\n\n<details>\n<summary>What can you attach to an EC2 instance in order to store data?</summary><br><b>\n\nEBS\n</b></details>\n\n<details>\n<summary>What EC2 RI types are there?</summary><br><b>\n\nStandard RI - most significant discount + suited for steady-state usage\nConvertible RI - discount + change attribute of RI + suited for steady-state usage\nScheduled RI - launch within time windows you reserve\n\nLearn more about EC2 RI [here](https://aws.amazon.com/ec2/pricing/reserved-instances)\n</b></details>\n\n#### AWS Containers\n\n<details>\n<summary>What is Amazon ECS?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. Customers such as Duolingo, Samsung, GE, and Cook Pad use ECS to run their most sensitive and mission critical applications because of its security, reliability, and scalability.\"\n\nLearn more [here](https://aws.amazon.com/ecs)\n</b></details>\n\n<details>\n<summary>What is Amazon ECR?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images.\"\n\nLearn more [here](https://aws.amazon.com/ecr)\n</b></details>\n\n<details>\n<summary>What is AWS Fargate?</summary><br><b>\n\nAmazon definition: \"AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\"\n\nLearn more [here](https://aws.amazon.com/fargate)\n</b></details>\n\n#### AWS Storage\n \n<details>\n<summary>Explain what is AWS S3?</summary><br><b>\n\nS3 stands for 3 S, Simple Storage Service.\nS3 is a object storage service which is fast, scalable and durable. S3 enables customers to upload, download or store any file or object that is up to 5 TB in size.\n\nMore on S3 [here](https://aws.amazon.com/s3) \n</b></details>\n\n<details>\n<summary>What is a bucket?</summary><br><b>\n\nAn S3 bucket is a resource which is similar to folders in a file system and allows storing objects, which consist of data.\n</b></details>\n\n<details>\n<summary>True or False? A bucket name must be globally unique</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>Explain folders and objects in regards to buckets</summary><br><b>\n\n* Folder - any sub folder in an s3 bucket\n* Object - The files which are stored in a bucket\n</b></details>\n\n<details>\n<summary>Explain the following:\n\n  * Object Lifecycles\n  * Object Sharing\n  * Object Versioning</summary><br><b>\n\n  * Object Lifecycles - Transfer objects between storage classes based on defined rules of time periods\n  * Object Sharing - Share objects via a URL link\n  * Object Versioning - Manage multiple versions of an object\n</b></details>\n\n<details>\n<summary>Explain Object Durability and Object Availability</summary><br><b>\n\nObject Durability: The percent over a one-year time period that a file will not be lost\nObject Availability: The percent over a one-year time period that a file will be accessible\n</b></details>\n\n<details>\n<summary>What is a storage class? What storage classes are there?</summary><br><b>\n\nEach object has a storage class assigned to, affecting its availability and durability. This also has effect on costs.\nStorage classes offered today:\n  * Standard:\n    * Used for general, all-purpose storage (mostly storage that needs to be accessed frequently)\n    * The most expensive storage class \n    * 11x9% durability\n    * 2x9% availability\n    * Default storage class\n\n  * Standard-IA (Infrequent Access)\n    * Long lived, infrequently accessed data but must be available the moment it's being accessed\n    * 11x9% durability\n    * 99.90% availability\n\n  * One Zone-IA (Infrequent Access):\n    * Long-lived, infrequently accessed, non-critical data\n    * Less expensive than Standard and Standard-IA storage classes\n    * 2x9% durability\n    * 99.50% availability\n    \n  * Intelligent-Tiering:\n    * Long-lived data with changing or unknown access patterns. Basically, In this class the data automatically moves to the class most suitable for you based on usage patterns\n    * Price depends on the used class\n    * 11x9% durability\n    * 99.90% availability\n\n  * Glacier: Archive data with retrieval time ranging from minutes to hours\n  * Glacier Deep Archive: Archive data that rarely, if ever, needs to be accessed with retrieval times in hours\n  * Both Glacier and Glacier Deep Archive are:\n    * The most cheap storage classes\n    * have 9x9% durability \n\nMore on storage classes [here](https://aws.amazon.com/s3/storage-classes)\n\n</b></details>\n\n<details>\n<summary>A customer would like to move data which is rarely accessed from standard storage class to the most cheapest class there is. Which storage class should be used?\n\n  * One Zone-IA\n  * Glacier Deep Archive\n  * Intelligent-Tiering</summary><br><b>\n\nGlacier Deep Archive\n</b></details>\n\n<details>\n<summary>What Glacier retrieval options are available for the user?</summary><br><b>\n\nExpedited, Standard and Bulk\n</b></details>\n\n<details>\n<summary>True or False? Each AWS account can store up to 500 PetaByte of data. Any additional storage will cost double</summary><br><b>\n\nFalse. Unlimited capacity.\n</b></details>\n\n<details>\n<summary>Explain what is Storage Gateway</summary><br><b>\n\n\"AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage\".\nMore on Storage Gateway [here](https://aws.amazon.com/storagegateway)\n</b></details>\n\n<details>\n<summary>Explain the following Storage Gateway deployments types\n\n  * File Gateway\n  * Volume Gateway\n  * Tape Gateway</summary><br><b>\n\nExplained in detail [here](https://aws.amazon.com/storagegateway/faqs)\n</b></details>\n\n<details>\n<summary>What is the difference between stored volumes and cached volumes?</summary><br><b>\n\nStored Volumes - Data is located at customer's data center and periodically backed up to AWS\nCached Volumes - Data is stored in AWS cloud and cached at customer's data center for quick access\n</b></details>\n\n<details>\n<summary>What is \"Amazon S3 Transfer Acceleration\"?</summary><br><b>\n\nAWS definition: \"Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket\"\n\nLearn more [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html)\n</b></details>\n\n<details>\n<summary>What is Amazon EFS?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources.\"\n\nLearn more [here](https://aws.amazon.com/efs)\n</b></details>\n\n<details>\n<summary>What is AWS Snowmobile?</summary><br><b>\n\n\"AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS.\"\n\nLearn more [here](https://aws.amazon.com/snowmobile)\n</b></details>\n\n##### AWS ELB\n\n<details>\n<summary>What is ELB (Elastic Load Balancing)?</summary><br><b>\n\nAWS definition: \"Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.\"\n\nMore on ELB [here](https://aws.amazon.com/elasticloadbalancing)\n</b></details>\n\n<details>\n<summary>What is auto scaling?</summary><br><b>\n\nAWS definition: \"AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost\"\n\nRead more about auto scaling [here](https://aws.amazon.com/autoscaling)\n</b></details>\n\n<details>\n<summary>True or False? Auto Scaling is about adding resources (such as instances) and not about removing resource</summary><br><b>\n\nFalse. Auto scaling adjusts capacity and this can mean removing some resources based on usage and performances.\n</b></details>\n\n<details>\n<summary>What types of load balancers are supported in EC2 and what are they used for?</summary><br><b>\n\n  * Application LB - layer 7 traffic\n  * Network LB - ultra-high performances or static IP address\n  * Classic LB - low costs, good for test or dev environments\n</b></details>\n\n#### AWS DNS\n\n<details>\n<summary>What is Route 53?</summary><br><b>\n\n\"Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service\"\nSome of Route 53 features:\n  * Register domain\n  * DNS service - domain name translations\n  * Health checks - verify your app is available\n\nMore on Route 53 [here](https://aws.amazon.com/route53)\n</b></details>\n\n#### AWS CloudFront\n\n<details>\n<summary>Explain what is CloudFront</summary><br><b>\n\nAWS definition: \"Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\"\n\nMore on CloudFront [here](https://aws.amazon.com/cloudfront)\n</b></details>\n\n<details>\n<summary>Explain the following\n\n  * Origin\n  * Edge location\n  * Distribution</summary><br><b>\n</b></details>\n\n#### AWS Monitoring & Logging\n\n<details>\n<summary>What is AWS CloudWatch?</summary><br><b>\n\nAWS definition: \"Amazon CloudWatch is a monitoring and observability service...\"\n\nMore on CloudWatch [here](https://aws.amazon.com/cloudwatch)\n</b></details>\n\n<details>\n<summary>What is AWS CloudTrail?</summary><br><b>\n\nAWS definition: \"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\"\n\nRead more on CloudTrail [here](https://aws.amazon.com/cloudtrail)\n</b></details>\n\n<details>\n<summary>What is Simply Notification Service?</summary><br><b>\n\nAWS definition: \"a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\"\n\nRead more about it [here](https://aws.amazon.com/sns)\n</b></details>\n\n<details>\n<summary>Explain the following in regards to SNS:\n\n  * Topics\n  * Subscribers\n  * Publishers</summary><br><b>\n\n  * Topics - used for grouping multiple endpoints\n  * Subscribers - the endpoints where topics send messages to \n  * Publishers - the provider of the message (event, person, ...)\n</b></details>\n\n#### AWS Security \n\n<details>\n<summary>What is the shared responsibility model? What AWS is responsible for and what the user is responsible for based on the shared responsibility model?</summary><br><b>\n\nThe shared responsibility model defines what the customer is responsible for and what AWS is responsible for.\n\nMore on the shared responsibility model [here](https://aws.amazon.com/compliance/shared-responsibility-model)\n</b></details>\n\n<details>\n<summary>True or False? Based on the shared responsibility model, Amazon is responsible for physical CPUs and security groups on instances</summary><br><b>\n\nFalse. It is responsible for Hardware in its sites but not for security groups which created and managed by the users.\n</b></details>\n\n<details>\n<summary>Explain \"Shared Controls\" in regards to the shared responsibility model</summary><br><b>\n\nAWS definition: \"apply to both the infrastructure layer and customer layers, but in completely separate contexts or perspectives. In a shared control, AWS provides the requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services\"\n\nLearn more about it [here](https://aws.amazon.com/compliance/shared-responsibility-model)\n</b></details>\n\n<details>\n<summary>What is the AWS compliance program?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS Artifact?</summary><br><b>\n\nAWS definition: \"AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS‚Äô security and compliance reports and select online agreements.\"\n\nRead more about it [here](https://aws.amazon.com/artifact)\n</b></details>\n\n<details>\n<summary>What is AWS Inspector?</summary><br><b>\n\nAWS definition: \"Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.\"\"\n\nLearn more [here](https://aws.amazon.com/inspector)\n</b></details>\n\n<details>\n<summary>What is AWS Guarduty?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS Shield?</summary><br><b>\n\nAWS definition: \"AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.\"\n</b></details>\n\n<details>\n<summary>What is AWS WAF? Give an example of how it can used and describe what resources or services you can use it with</summary><br><b>\n</b></details>\n\n<details>\n<summary>What AWS VPN is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the difference between Site-to-Site VPN and Client VPN?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS CloudHSM?</summary><br><b>\n\nAmazon definition: \"AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud.\"\n\nLearn more [here](https://aws.amazon.com/cloudhsm)\n</b></details>\n\n<details>\n<summary>True or False? AWS Inspector can perform both network and host assessments</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>What is AWS Acceptable Use Policy?</summary><br><b>\n\nIt describes prohibited uses of the web services offered by AWS.\nMore on AWS Acceptable Use Policy [here](https://aws.amazon.com/aup)\n</b></details>\n\n<details>\n<summary>What is AWS Key Management Service (KMS)?</summary><br><b>\n\nAWS definition: \"KMS makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.\"\nMore on KMS [here](https://aws.amazon.com/kms)\n</b></details>\n\n<details>\n<summary>True or False? A user is not allowed to perform penetration testing on any of the AWS services</summary><br><b>\n\nFalse. On some services, like EC2, CloudFront and RDS, penetration testing is allowed.\n</b></details>\n\n<details>\n<summary>True or False? DDoS attack is an example of allowed penetration testing activity</summary><br><b>\n\nFalse.\n</b></details>\n\n<details>\n<summary>True or False? AWS Access Key is a type of MFA device used for AWS resources protection</summary><br><b>\n\nFalse. Security key is an example of an MFA device.\n</b></details>\n\n<details>\n<summary>What is Amazon Cognito?</summary><br><b>\n\nAmazon definition: \"Amazon Cognito handles user authentication and authorization for your web and mobile apps.\"\n\nLearn more [here](https://docs.aws.amazon.com/cognito/index.html)\n</b></details>\n\n<details>\n<summary>What is AWS ACM?</summary><br><b>\n\nAmazon definition: \"AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.\"\n\nLearn more [here](https://aws.amazon.com/certificate-manager)\n</b></details>\n\n#### AWS Databases\n\n<details>\n<summary>What is AWS RDS?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS DynamoDB?</summary><br><b>\n</b></details>\n\n<details>\n<summary>Explain \"Point-in-Time Recovery\" feature in DynamoDB</summary><br><b>\n\nAmazon definition: \"You can create on-demand backups of your Amazon DynamoDB tables, or you can enable continuous backups using point-in-time recovery. For more information about on-demand backups, see On-Demand Backup and Restore for DynamoDB.\"\n\nLearn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html)\n</b></details>\n\n<details>\n<summary>Explain \"Global Tables\" in DynamoDB</summary><br><b>\n\nAmazon definition: \"A global table is a collection of one or more replica tables, all owned by a single AWS account.\"\n\nLearn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html)\n</b></details>\n\n<details>\n<summary>What is DynamoDB Accelerator?</summary><br><b>\n\nAmazon definition: \"Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement ‚Äì from milliseconds to microseconds...\"\n\nLearn more [here](https://aws.amazon.com/dynamodb/dax)\n</b></details>\n\n<details>\n<summary>What is AWS Redshift and how is it different than RDS?</summary><br><b>\n\ncloud data warehouse\n</b></details>\n\n<details>\n<summary>What is AWS ElastiCache? For what cases is it used?</summary><br><b>\n\nAmazon Elasticache is a fully managed Redis or Memcached in-memory data store.                                                                       \nIt's great for use cases like two-tier web applications where the most frequently accesses data is stored in ElastiCache so response time is optimal.\n</b></details>\n\n<details>\n<summary>What is Amazon Aurora</summary><br><b>\n\nA MySQL & Postgresql based relational database. Also, the default database proposed for the user when using RDS for creating a database.\nGreat for use cases like two-tier web applications that has a MySQL or Postgresql database layer and you need automated backups for your application.\n</b></details>\n\n<details>\n<summary>What is Amazon DocumentDB?</summary><br><b>\n\nAmazon definition: \"Amazon DocumentDB (with MongoDB compatibility) is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data.\"\n\nLearn more [here](https://aws.amazon.com/documentdb)\n</b></details>\n\n<details>\n<summary>What \"AWS Database Migration Service\" is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What type of storage is used by Amazon RDS?</summary><br><b>\n\nEBS\n</b></details>\n\n<details>\n<summary>Explain Amazon RDS Read Replicas</summary><br><b>\n\nAWS definition: \"Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\"\nRead more about [here](https://aws.amazon.com/rds/features/read-replicas)\n</b></details>\n\n#### AWS Serverless Compute\n\n<details>\n<summary>Explain what is AWS Lambda</summary><br><b>\n\nAWS definition: \"AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\"\n\nRead more on it [here](https://aws.amazon.com/lambda)\n</b></details>\n\n<details>\n<summary>True or False? In AWS Lambda, you are charged as long as a function exists, regardless of whether it's running or not</summary><br><b>\n\nFalse. Charges are being made when the code is executed.\n</b></details>\n\n<details>\n<summary>Which of the following set of languages Lambda supports?\n\n  * R, Swift, Rust, Kotlin\n  * Python, Ruby, Go \n  * Python, Ruby, PHP</summary><br><b>\n\n  * Python, Ruby, Go \n</b></details>\n\n#### Identify the service or tool\n\n<details>\n<summary>What would you use for automating code/software deployments?</summary><br><b>\n\nAWS CodeDeploy\n</b></details>\n\n<details>\n<summary>What would you use for easily creating similar AWS environments/resources for different customers?</summary><br><b>\n\nCloudFormation\n</b></details>\n\n<details>\n<summary>Which service would you use for building a website or web application?</summary><br><b>\n\nLightsail\n</b></details>\n\n<details>\n<summary>Which tool would you use for choosing between Reserved instances or On-Demand instances?</summary><br><b>\n\nCost Explorer\n</b></details>\n\n<details>\n<summary>What would you use to check how many unassociated Elastic IP address you have?</summary><br><b>\n\nTrusted Advisor\n</b></details>\n\n<details>\n<summary>What service allows you to transfer large amounts (Petabytes) of data in and out of the AWS cloud?</summary><br><b>\n\nAWS Snowball\n</b></details>\n\n<details>\n<summary>What provides a virtual network dedicated to your AWS account?</summary><br><b>\n\nVPC\n</b></details>\n\n<details>\n<summary>What you would use for having automated backups for an application that has MySQL database layer?</summary><br><b>\n\nAmazon Aurora\n</b></details>\n\n<details>\n<summary>What would you use to migrate on-premise database to AWS?</summary><br><b>\n\nAWS Database Migration Service (DMS)\n</b></details>\n\n<details>\n<summary>What would you use to check why certain EC2 instances were terminated?</summary><br><b>\n\nAWS CloudTrail\n</b></details>\n\n<details>\n<summary>What would you use for SQL database?</summary><br><b>\n\nAWS RDS\n</b></details>\n\n<details>\n<summary>What would you use for NoSQL database?</summary><br><b>\n\nAWS DynamoDB\n</b></details>\n\n<details>\n<summary>What would you use for running SQL queries interactively on S3?</summary><br><b>\n\nAWS Athena\n</b></details>\n\n<details>\n<summary>What would you use for adding image and video analysis to your application?</summary><br><b>\n\nAWS Rekognition\n</b></details>\n\n<details>\n<summary>Which service would you use for debugging and improving performances issues with your applications?</summary><br><b>\n\nAWS X-Ray\n</b></details>\n\n<details>\n<summary>Which service is used for sending notifications?</summary><br><b>\n\nSNS\n</b></details>\n\n<details>\n<summary>Which service would you use for monitoring malicious activity and unauthorized behavior in regards to AWS accounts and workloads?</summary><br><b>\n\nAmazon GuardDuty\n</b></details>\n\n<details>\n<summary>Which service would you use for centrally manage billing, control access, compliance, and security across multiple AWS accounts?</summary><br><b>\n\nAWS Organizations\n</b></details>\n\n<details>\n<summary>Which service would you use for web application protection?</summary><br><b>\n\nAWS WAF\n</b></details>\n\n<details>\n<summary>You would like to monitor some of your resources in the different services. Which service would you use for that?</summary><br><b>\n\nCloudWatch\n</b></details>\n\n<details>\n<summary>Which service would you use for performing security assessment?</summary><br><b>\n\nAWS Inspector\n</b></details>\n\n<details>\n<summary>Which service would you use for creating DNS record?</summary><br><b>\n\nRoute 53\n</b></details>\n\n<details>\n<summary>What would you use if you need a fully managed document database?</summary><br><b>\n\nAmazon DocumentDB\n</b></details>\n\n<details>\n<summary>Which service would you use to add access control (or sign-up, sign-in forms) to your web/mobile apps?</summary><br><b>\n\nAWS Cognito\n</b></details>\n\n<details>\n<summary>Which service would you use if you need messaging queue?</summary><br><b>\n\nSimple Queue Service (SQS)\n</b></details>\n\n<details>\n<summary>Which service would you use if you need managed DDOS protection?</summary><br><b>\n\nAWS Shield\n</b></details>\n\n<details>\n<summary>Which service would you use if you need store frequently used data for low latency access?</summary><br><b>\n\nElastiCache\n</b></details>\n\n<details>\n<summary>What would you use to transfer files over long distances between a client and an S3 bucket?</summary><br><b>\n\nAmazon S3 Transfer Acceleration\n</b></details>\n\n#### AWS Billing & Support\n\n<details>\n<summary>What is AWS Organizations?</summary><br><b>\n\nAWS definition: \"AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS.\"\nMore on Organizations [here](https://aws.amazon.com/organizations)\n</b></details>\n\n<details>\n<summary>Explain AWS pricing model</summary><br><b>\n\nIt mainly works on \"pay-as-you-go\" meaning you pay only for what are using and when you are using it.\nIn s3 you pay for 1. How much data you are storing 2. Making requests (PUT, POST, ...)\nIn EC2 it's based on the purchasing option (on-demand, spot, ...), instance type, AMI type and the region used.\n\nMore on AWS pricing model [here](https://aws.amazon.com/pricing)\n</b></details>\n\n<details>\n<summary>How one should estimate AWS costs when for example comparing to on-premise solutions?</summary><br><b>\n\n* TCO calculator\n* AWS simple calculator\n* Cost Explorer\n</b></details>\n\n<details>\n<summary>What basic support in AWS includes?</summary><br><b>\n\n* 24x7 customer service\n* Trusted Advisor\n* AWS personal Health Dashoard\n</b></details>\n\n<details>\n<summary>How are EC2 instances billed?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What AWS Pricing Calculator is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is Amazon Connect?</summary><br><b>\n\nAmazon definition: \"Amazon Connect is an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost.\"\n\nLearn more [here](https://aws.amazon.com/connect)\n</b></details>\n\n<details>\n<summary>What are \"APN Consulting Partners\"?</summary><br><b>\n\nAmazon definition: \"APN Consulting Partners are professional services firms that help customers of all types and sizes design, architect, build, migrate, and manage their workloads and applications on AWS, accelerating their journey to the cloud.\"\n\nLearn more [here](https://aws.amazon.com/partners/consulting)\n</b></details>\n\n<details>\n<summary>Which of the following are AWS accounts types (and are sorted by order)?\n\n  * Basic, Developer, Business, Enterprise\n  * Newbie, Intermediate, Pro, Enterprise\n  * Developer, Basic, Business, Enterprise\n  * Beginner, Pro, Intermediate Enterprise</summary><br><b>\n\n  * Basic, Developer, Business, Enterprise\n</b></details>\n\n<details>\n<summary>True or False? Region is a factor when it comes to EC2 costs/pricing</summary><br><b>\n\nTrue. You pay differently based on the chosen region.\n</b></details>\n\n<details>\n<summary>What is \"AWS Infrastructure Event Management\"?</summary><br><b>\n\nAWS Definition: \"AWS Infrastructure Event Management is a structured program available to Enterprise Support customers (and Business Support customers for an additional fee) that helps you plan for large-scale events such as product or application launches, infrastructure migrations, and marketing events.\"\n</b></details>\n\n#### AWS Automation\n\n<details>\n<summary>What is AWS CodeDeploy?</summary><br><b>\n\nAmazon definition: \"AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.\"\n\nLearn more [here](https://aws.amazon.com/codedeploy)\n</b></details>\n\n<details>\n<summary>Explain what is CloudFormation</summary><br><b>\n</b></details>\n\n#### AWS Misc\n\n<details>\n<summary>What is AWS Lightsail?</summary><br><b>\n\nAWS definition: \"Lightsail is an easy-to-use cloud platform that offers you everything needed to build an application or website, plus a cost-effective, monthly plan.\"\n</b></details>\n\n<details>\n<summary>What is AWS Rekognition?</summary><br><b>\n\nAWS definition: \"Amazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use.\"\n\nLearn more [here](https://aws.amazon.com/rekognition)\n</b></details>\n\n<details>\n<summary>What AWS Resource Groups used for?</summary><br><b>\n\nAmazon definition: \"You can use resource groups to organize your AWS resources. Resource groups make it easier to manage and automate tasks on large numbers of resources at one time. \"\n\nLearn more [here](https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html)\n</b></details>\n\n<details>\n<summary>What is AWS Global Accelerator?</summary><br><b>\n\nAmazon definition: \"AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users...\"\n\nLearn more [here](https://aws.amazon.com/global-accelerator)\n</b></details>\n\n<details>\n<summary>What is AWS Config?</summary><br><b>\n\nAmazon definition: \"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources.\"\n\nLearn more [here](https://aws.amazon.com/config)\n</b></details>\n\n<details>\n<summary>What is AWS X-Ray?</summary><br><b>\n\nAWS definition: \"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.\"\nLearn more [here](https://aws.amazon.com/xray)\n</b></details>\n\n<details>\n<summary>What is AWS OpsWorks?</summary><br><b>\n\nAmazon definition: \"AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.\"\n\nLearn more about it [here](https://aws.amazon.com/opsworks)\n</b></details>\n\n<details>\n<summary>What is AWS Service Catalog?</summary><br><b>\n\nAmazon definition: \"AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS.\"\n\nLearn more [here](https://aws.amazon.com/servicecatalog)\n</b></details>\n\n<details>\n<summary>What is AWS CAF?</summary><br><b>\n\nAmazon definition: \"AWS Professional Services created the AWS Cloud Adoption Framework (AWS CAF) to help organizations design and travel an accelerated path to successful cloud adoption. \"\n\nLearn more [here](https://aws.amazon.com/professional-services/CAF)\n</b></details>\n\n<details>\n<summary>What is AWS Cloud9?</summary><br><b>\n\nAWS definition: \"AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser\"\n</b></details>\n\n<details>\n<summary>What is AWS Application Discovery Service?</summary><br><b>\n\nAmazon definition: \"AWS Application Discovery Service helps enterprise customers plan migration projects by gathering information about their on-premises data centers.\"\n\nLearn more [here](https://aws.amazon.com/application-discovery)\n</b></details>\n\n<details>\n<summary>What is the Trusted Advisor?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the AWS well-architected framework and what pillars it's based on?</summary><br><b>\n\nAWS definition: \"The Well-Architected Framework has been developed to help cloud architects build secure, high-performing, resilient, and efficient infrastructure for their applications. Based on five pillars ‚Äî operational excellence, security, reliability, performance efficiency, and cost optimization\"\n\nLearn more [here](https://aws.amazon.com/architecture/well-architected)\n</b></details>\n\n<details>\n<summary>What AWS services are serverless (or have the option to be serverless)?</summary><br><b>\n\nAWS Lambda\nAWS Athena\n</b></details>\n\n<details>\n<summary>What is AWS EMR?</summary><br><b>\n\nAWS definition: \"big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto.\"\n\nLearn more [here](https://aws.amazon.com/emr)\n</b></details>\n\n<details>\n<summary>What is AWS Athena?</summary><br><b>\n\n\"Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.\"\n\nLearn more about AWS Athena [here](https://aws.amazon.com/athena)\n</b></details>\n\n<details>\n<summary>What is Amazon Cloud Directory?</summary><br><b>\n\nAmazon definition: \"Amazon Cloud Directory is a highly available multi-tenant directory-based store in AWS. These directories scale automatically to hundreds of millions of objects as needed for applications.\"\n\nLearn more [here](https://docs.aws.amazon.com/clouddirectory/latest/developerguide/what_is_cloud_directory.html)\n</b></details>\n\n<details>\n<summary>What is AWS Elastic Beanstalk?</summary><br><b>\n\nAWS definition: \"AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services...You can simply upload your code and Elastic Beanstalk automatically handles the deployment\"\n\nLearn more about it [here](https://aws.amazon.com/elasticbeanstalk)\n</b></details>\n\n<details>\n<summary>What is AWS SWF?</summary><br><b>\n\nAmazon definition: \"Amazon SWF helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud.\"\n\nLearn more on Amazon Simple Workflow Service [here](https://aws.amazon.com/swf)\n</b></details>\n\n<details>\n<summary>What is Simple Queue Service (SQS)?</summary><br><b>\n\nAWS definition: \"Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications\".\n\nLearn more about it [here](https://aws.amazon.com/sqs)\n</b></details>\n\n#### AWS Disaster Recovery\n\n<details>\n<summary>In regards to disaster recovery, what is RTO and RPO?</summary><br><b>\n\nRTO - The maximum acceptable length of time that your application can be offline.\n\nRPO - The maximum acceptable length of time during which data might be lost from your application due to an incident.\n</b></details>\n\n<details>\n<summary>What types of disaster recovery techniques AWS supports?</summary><br><b>\n\n* The Cold Method - Periodically backups and sending the backups off-site<br>\n* Pilot Light - Data is mirrored to an environment which is always running\n* Warm Standby - Running scaled down version of production environment\n* Multi-site - Duplicated environment that is always running\n</b></details>\n\n<details>\n<summary>Which disaster recovery option has the highest downtime and which has the lowest?</summary><br><b>\n\nLowest - Multi-site\nHighest - The cold method\n</b></details>\n\n### Final Note\n\nGood luck! You can do it :)\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "certificates/aws-solutions-architect-associate.md", "language": "markdown", "complexity": "complex", "star_count": 77723, "quality_score": 0.79, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 1203}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "labels: [question]\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Thanks for your interest in this project! üöÄ\n\n        Please follow these instructions, fill every question, and do every step. üôè\n\n        I'm asking this because answering questions and solving problems in GitHub is what consumes most of the time.\n\n        I end up not being able to add new features, fix bugs, review pull requests, etc. as fast as I wish because I have to spend too much time handling questions.\n\n        All that, on top of all the incredible help provided by a bunch of community members, that give a lot of their time to come here and help others.\n\n        That's a lot of work, but if more users came to help others like them just a little bit more, it would be much less effort for them (and you and me üòÖ).\n\n        By asking questions in a structured way (following this) it will be much easier to help you.\n\n        And there's a high chance that you will find the solution along the way and you won't even have to submit it and wait for an answer. üòé\n\n        As there are too many questions, I'll have to discard and close the incomplete ones. That will allow me (and others) to focus on helping people like you that follow the whole process and help us help you. ü§ì\n  - type: checkboxes\n    id: checks\n    attributes:\n      label: First Check\n      description: Please confirm and check all the following options.\n      options:\n        - label: I added a very descriptive title here.\n          required: true\n        - label: I used the GitHub search to find a similar question and didn't find it.\n          required: true\n        - label: I searched in the documentation/README.\n          required: true\n        - label: I already searched in Google \"How to do X\" and didn't find any information.\n          required: true\n        - label: I already read and followed all the tutorial in the docs/README and didn't find an answer.\n          required: true\n  - type: checkboxes\n    id: help\n    attributes:\n      label: Commit to Help\n      description: |\n        After submitting this, I commit to one of:\n\n          * Read open questions until I find 2 where I can help someone and add a comment to help there.\n          * I already hit the \"watch\" button in this repository to receive notifications and I commit to help at least 2 people that ask questions in the future.\n\n      options:\n        - label: I commit to help with one of those options üëÜ\n          required: true\n  - type: textarea\n    id: example\n    attributes:\n      label: Example Code\n      description: |\n        Please add a self-contained, [minimal, reproducible, example](https://stackoverflow.com/help/minimal-reproducible-example) with your use case.\n\n        If I (or someone) can copy it, run it, and see it right away, there's a much higher chance I (or someone) will be able to help you.\n\n      placeholder: |\n        Write your example code here.\n      render: Text\n    validations:\n      required: true\n  - type: textarea\n    id: description\n    attributes:\n      label: Description\n      description: |\n        What is the problem, question, or error?\n\n        Write a short description telling me what you are doing, what you expect to happen, and what is currently happening.\n      placeholder: |\n        * Open the browser and call the endpoint `/`.\n        * It returns a JSON with `{\"message\": \"Hello World\"}`.\n        * But I expected it to return `{\"message\": \"Hello Morty\"}`.\n    validations:\n      required: true\n  - type: dropdown\n    id: os\n    attributes:\n      label: Operating System\n      description: What operating system are you on?\n      multiple: true\n      options:\n        - Linux\n        - Windows\n        - macOS\n        - Other\n    validations:\n      required: true\n  - type: textarea\n    id: os-details\n    attributes:\n      label: Operating System Details\n      description: You can add more details about your operating system here, in particular if you chose \"Other\".\n    validations:\n      required: true\n  - type: input\n    id: python-version\n    attributes:\n      label: Python Version\n      description: |\n        What Python version are you using?\n\n        You can find the Python version with:\n\n        ```bash\n        python --version\n        ```\n    validations:\n      required: true\n  - type: textarea\n    id: context\n    attributes:\n      label: Additional Context\n      description: Add any additional context information or screenshots you think are useful.\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": ".github/DISCUSSION_TEMPLATE/questions.yml", "language": "yaml", "complexity": "medium", "star_count": 3007, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 119}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "blank_issues_enabled: false\ncontact_links:\n  - name: Security Contact\n    about: Please report security vulnerabilities to security@tiangolo.com\n  - name: Question or Problem\n    about: Ask a question or ask about a problem in GitHub Discussions.\n    url: https://github.com/tiangolo/uwsgi-nginx-flask-docker/discussions/categories/questions\n  - name: Feature Request\n    about: To suggest an idea or ask about a feature, please start with a question saying what you would like to achieve. There might be a way to do it already.\n    url: https://github.com/tiangolo/uwsgi-nginx-flask-docker/discussions/categories/questions\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": ".github/ISSUE_TEMPLATE/config.yml", "language": "yaml", "complexity": "simple", "star_count": 3007, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 11}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Privileged\ndescription: You are @tiangolo or he asked you directly to create an issue here. If not, check the other options. üëá\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Thanks for your interest in this project! üöÄ\n\n        If you are not @tiangolo or he didn't ask you directly to create an issue here, please start the conversation in a [Question in GitHub Discussions](https://github.com/tiangolo/uwsgi-nginx-flask-docker/discussions/categories/questions) instead.\n  - type: checkboxes\n    id: privileged\n    attributes:\n      label: Privileged issue\n      description: Confirm that you are allowed to create an issue here.\n      options:\n        - label: I'm @tiangolo or he asked me directly to create an issue here.\n          required: true\n  - type: textarea\n    id: content\n    attributes:\n      label: Issue Content\n      description: Add the content of the issue here.\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": ".github/ISSUE_TEMPLATE/privileged.yml", "language": "yaml", "complexity": "simple", "star_count": 3007, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "internal:\n  - all:\n    - changed-files:\n      - any-glob-to-any-file:\n        - .github/**\n        - .gitignore\n      - all-globs-to-all-files:\n        - '!docker-images/requirements.txt'\n\nupgrade:\n  - all:\n    - changed-files:\n      - any-glob-to-any-file:\n        - docker-images/requirements.txt\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": ".github/labeler.yml", "language": "yaml", "complexity": "simple", "star_count": 3007, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Deploy\n\non:\n  push:\n    branches:\n      - master\n  schedule:\n    # cron every week on monday\n    - cron: \"0 0 * * 1\"\n\njobs:\n  deploy:\n    strategy:\n      matrix:\n        image:\n          - name: latest\n            python_version: \"3.12\"\n          - name: python3.12\n            python_version: \"3.12\"\n          - name: python3.11\n            python_version: \"3.11\"\n          - name: python3.10\n            python_version: \"3.10\"\n          - name: python3.9\n            python_version: \"3.9\"\n      fail-fast: true\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Set Dockerfile name\n        if: matrix.image.name != 'latest'\n        run: echo \"DOCKERFILE_NAME=${{ matrix.image.name }}\" >> $GITHUB_ENV\n      - name: Set Dockerfile name latest\n        if: matrix.image.name == 'latest'\n        run: echo \"DOCKERFILE_NAME=python${{ matrix.image.python_version }}\" >> $GITHUB_ENV\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      - name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n      - name: Get date for tags\n        run: echo \"DATE_TAG=$(date -I)\" >> \"$GITHUB_ENV\"\n      - name: Build and push\n        uses: docker/build-push-action@v6\n        with:\n          push: true\n          platforms: linux/amd64,linux/arm64\n          tags: |\n            tiangolo/uwsgi-nginx-flask:${{ matrix.image.name }}\n            tiangolo/uwsgi-nginx-flask:${{ matrix.image.name }}-${{ env.DATE_TAG }} \n          context: ./docker-images/\n          file: ./docker-images/${{ env.DOCKERFILE_NAME }}.dockerfile\n      - name: Docker Hub Description\n        uses: peter-evans/dockerhub-description@v4\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n          repository: tiangolo/uwsgi-nginx-flask\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": ".github/workflows/deploy.yml", "language": "yaml", "complexity": "medium", "star_count": 3007, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 61}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Issue Manager\n\non:\n  schedule:\n    - cron: \"15 9 * * *\"\n  issue_comment:\n    types:\n      - created\n  issues:\n    types:\n      - labeled\n  pull_request_target:\n    types:\n      - labeled\n  workflow_dispatch:\n\npermissions:\n  issues: write\n  pull-requests: write\n\njobs:\n  issue-manager:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: tiangolo/issue-manager@0.5.1\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          config: >\n            {\n              \"answered\": {\n                \"delay\": 864000,\n                \"message\": \"Assuming the original need was handled, this will be automatically closed now. But feel free to add more comments or create new issues or PRs.\"\n              },\n              \"waiting\": {\n                \"delay\": 2628000,\n                \"message\": \"As this PR has been waiting for the original user for a while but seems to be inactive, it's now going to be closed. But if there's anyone interested, feel free to create a new PR.\"\n              }\n            }\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": ".github/workflows/issue-manager.yml", "language": "yaml", "complexity": "simple", "star_count": 3007, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 39}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Labels\non:\n  pull_request_target:\n    types:\n      - opened\n      - synchronize\n      - reopened\n      # For label-checker\n      - labeled\n      - unlabeled\n\njobs:\n  labeler:\n    permissions:\n      contents: read\n      pull-requests: write\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/labeler@v5\n      if: ${{ github.event.action != 'labeled' && github.event.action != 'unlabeled' }}\n    - run: echo \"Done adding labels\"\n  # Run this after labeler applied labels\n  check-labels:\n    needs:\n      - labeler\n    permissions:\n      pull-requests: read\n    runs-on: ubuntu-latest\n    steps:\n      - uses: docker://agilepathway/pull-request-label-checker:latest\n        with:\n          one_of: breaking,security,feature,bug,refactor,upgrade,docs,internal\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": ".github/workflows/labeler.yml", "language": "yaml", "complexity": "simple", "star_count": 3007, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 34}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Latest Changes\n\non:\n  pull_request_target:\n    branches:\n      - master\n    types:\n      - closed\n  workflow_dispatch:\n    inputs:\n      number:\n        description: PR number\n        required: true\n      debug_enabled:\n        description: 'Run the build with tmate debugging enabled (https://github.com/marketplace/actions/debugging-with-tmate)'\n        required: false\n        default: 'false'\n\njobs:\n  latest-changes:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          # To allow latest-changes to commit to master\n          token: ${{ secrets.UWSGI_NGINX_FLASK_DOCKER_LATEST_CHANGES }}\n      - uses: tiangolo/latest-changes@0.3.2\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": ".github/workflows/latest-changes.yml", "language": "yaml", "complexity": "medium", "star_count": 3007, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Test\n\non:\n  push:\n    branches:\n      - master\n  pull_request:\n    types:\n      - opened\n      - synchronize\n  schedule:\n    # cron every week on monday\n    - cron: \"0 0 * * 1\"\n\njobs:\n  test:\n    strategy:\n      matrix:\n        image:\n          - name: latest\n            python_version: \"3.12\"\n          - name: python3.12\n            python_version: \"3.12\"\n          - name: python3.11\n            python_version: \"3.11\"\n          - name: python3.10\n            python_version: \"3.10\"\n          - name: python3.9\n            python_version: \"3.9\"\n      fail-fast: true\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Set Dockerfile name\n        if: matrix.image.name != 'latest'\n        run: echo \"DOCKERFILE_NAME=${{ matrix.image.name }}\" >> $GITHUB_ENV\n      - name: Set Dockerfile name latest\n        if: matrix.image.name == 'latest'\n        run: echo \"DOCKERFILE_NAME=python${{ matrix.image.python_version }}\" >> $GITHUB_ENV\n      - name: Build\n        uses: docker/build-push-action@v6\n        with:\n          push: false\n          tags: tiangolo/uwsgi-nginx-flask:${{ matrix.image.name }}\n          context: ./docker-images/\n          file: ./docker-images/${{ env.DOCKERFILE_NAME }}.dockerfile\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.10\"\n      - name: Install Dependencies\n        run: python -m pip install docker pytest\n      - name: Test Image\n        run: pytest tests\n        env:\n          NAME: ${{ matrix.image.name }}\n          PYTHON_VERSION: ${{ matrix.image.python_version }}\n  check:\n    if: always()\n    needs:\n    - test\n    runs-on: ubuntu-latest\n    steps:\n    - name: Decide whether the needed jobs succeeded or failed\n      uses: re-actors/alls-green@release/v1\n      with:\n        jobs: ${{ toJSON(needs) }}\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": ".github/workflows/test.yml", "language": "yaml", "complexity": "simple", "star_count": 3007, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 68}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Security Policy\n\nSecurity is very important for this project and its community. üîí\n\nLearn more about it below. üëá\n\n## Versions\n\nThe latest version or release is supported.\n\nYou are encouraged to write tests for your application and update your versions frequently after ensuring that your tests are passing. This way you will benefit from the latest features, bug fixes, and **security fixes**.\n\n## Reporting a Vulnerability\n\nIf you think you found a vulnerability, and even if you are not sure about it, please report it right away by sending an email to: security@tiangolo.com. Please try to be as explicit as possible, describing all the steps and example code to reproduce the security issue.\n\nI (the author, [@tiangolo](https://twitter.com/tiangolo)) will review it thoroughly and get back to you.\n\n## Public Discussions\n\nPlease restrain from publicly discussing a potential security vulnerability. üôä\n\nIt's better to discuss privately and try to find a solution first, to limit the potential impact as much as possible.\n\n---\n\nThanks for your help!\n\nThe community and I thank you for that. üôá\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": "SECURITY.md", "language": "markdown", "complexity": "simple", "star_count": 3007, "quality_score": 0.63, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import sys\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n\n@app.route(\"/\")\ndef hello():\n    version = \"{}.{}\".format(sys.version_info.major, sys.version_info.minor)\n    message = \"Hello World from Flask in a uWSGI Nginx Docker container with Python {} (default)\".format(\n        version\n    )\n    return message\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", debug=True, port=80)\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": "docker-images/app/main.py", "language": "python", "complexity": "simple", "star_count": 3007, "quality_score": 0.69, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#! /usr/bin/env sh\n\necho \"Running inside /app/prestart.sh, you could add migrations to this file, e.g.:\"\n\necho \"\n#! /usr/bin/env sh\n\n# Let the DB start\nsleep 10;\n# Run migrations\nalembic upgrade head\n\"\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": "docker-images/app/prestart.sh", "language": "bash", "complexity": "simple", "star_count": 3007, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 13}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#! /usr/bin/env sh\nset -e\n\n/uwsgi-nginx-entrypoint.sh\n\n# Get the URL for static files from the environment variable\nUSE_STATIC_URL=${STATIC_URL:-'/static'}\n# Get the absolute path of the static files from the environment variable\nUSE_STATIC_PATH=${STATIC_PATH:-'/app/static'}\n# Get the listen port for Nginx, default to 80\nUSE_LISTEN_PORT=${LISTEN_PORT:-80}\n\nif [ -f /app/nginx.conf ]; then\n    cp /app/nginx.conf /etc/nginx/nginx.conf\nelse\n    content_server='server {\\n'\n    content_server=$content_server\"    listen ${USE_LISTEN_PORT};\\n\"\n    content_server=$content_server'    location / {\\n'\n    content_server=$content_server'        try_files $uri @app;\\n'\n    content_server=$content_server'    }\\n'\n    content_server=$content_server'    location @app {\\n'\n    content_server=$content_server'        include uwsgi_params;\\n'\n    content_server=$content_server'        uwsgi_pass unix:///tmp/uwsgi.sock;\\n'\n    content_server=$content_server'    }\\n'\n    content_server=$content_server\"    location $USE_STATIC_URL {\\n\"\n    content_server=$content_server\"        alias $USE_STATIC_PATH;\\n\"\n    content_server=$content_server'    }\\n'\n    # If STATIC_INDEX is 1, serve / with /static/index.html directly (or the static URL configured)\n    if [ \"$STATIC_INDEX\" = 1 ] ; then\n        content_server=$content_server'    location = / {\\n'\n        content_server=$content_server\"        index $USE_STATIC_URL/index.html;\\n\"\n        content_server=$content_server'    }\\n'\n    fi\n    content_server=$content_server'}\\n'\n    # Save generated server /etc/nginx/conf.d/nginx.conf\n    printf \"$content_server\" > /etc/nginx/conf.d/nginx.conf\nfi\n\nexec \"$@\"\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": "docker-images/entrypoint.sh", "language": "bash", "complexity": "simple", "star_count": 3007, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 40}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nimport time\n\nimport docker\nimport requests\nfrom docker.models.containers import Container\n\nfrom ..utils import (\n    CONTAINER_NAME,\n    get_logs,\n    get_nginx_config,\n    get_response_text1,\n    remove_previous_container,\n)\n\nclient = docker.from_env()\n\n\ndef verify_container(container: Container, response_text: str) -> None:\n    response = requests.get(\"http://127.0.0.1:8000\")\n    assert response.text == response_text\n    nginx_config = get_nginx_config(container)\n    assert \"client_max_body_size 0;\" in nginx_config\n    assert \"worker_processes 1;\" in nginx_config\n    assert \"listen 80;\" in nginx_config\n    assert \"worker_connections 1024;\" in nginx_config\n    assert \"worker_rlimit_nofile;\" not in nginx_config\n    assert \"daemon off;\" in nginx_config\n    assert \"include uwsgi_params;\" in nginx_config\n    assert \"uwsgi_pass unix:///tmp/uwsgi.sock;\" in nginx_config\n    assert \"try_files $uri @app;\" in nginx_config\n    assert \"location @app {\" in nginx_config\n    assert \"include uwsgi_params;\" in nginx_config\n    assert \"location /static {\" in nginx_config\n    assert \"alias /app/static;\" in nginx_config\n    # Nginx index.html specific\n    assert \"location = / {\" not in nginx_config\n    assert \"index /static/index.html;\" not in nginx_config\n    logs = get_logs(container)\n    assert \"getting INI configuration from /app/uwsgi.ini\" in logs\n    assert \"getting INI configuration from /etc/uwsgi/uwsgi.ini\" in logs\n    assert \"ini = /app/uwsgi.ini\" in logs\n    assert \"ini = /etc/uwsgi/uwsgi.ini\" in logs\n    assert \"socket = /tmp/uwsgi.sock\" in logs\n    assert \"chown-socket = nginx:nginx\" in logs\n    assert \"chmod-socket = 664\" in logs\n    assert \"hook-master-start = unix_signal:15 gracefully_kill_them_all\" in logs\n    assert \"need-app = true\" in logs\n    assert \"die-on-term = true\" in logs\n    assert \"show-config = true\" in logs\n    assert \"module = main\" in logs\n    assert \"callable = app\" in logs\n    assert \"processes = 16\" in logs\n    assert \"cheaper = 2\" in logs\n    assert \"Checking for script in /app/prestart.sh\" in logs\n    assert \"Running script /app/prestart.sh\" in logs\n    assert (\n        \"Running inside /app/prestart.sh, you could add migrations to this file\" in logs\n    )\n    assert \"spawned uWSGI master process\" in logs\n    assert \"spawned uWSGI worker 1\" in logs\n    assert \"spawned uWSGI worker 2\" in logs\n    assert \"spawned uWSGI worker 3\" not in logs\n    assert 'running \"unix_signal:15 gracefully_kill_them_all\" (master-start)' in logs\n    assert \"success: nginx entered RUNNING state, process has stayed up for\" in logs\n    assert \"success: uwsgi entered RUNNING state, process has stayed up for\" in logs\n\n\ndef test_defaults() -> None:\n    name = os.getenv(\"NAME\", \"\")\n    # It's an index postfix tag, skip it\n    image = f\"tiangolo/uwsgi-nginx-flask:{name}\"\n    response_text = get_response_text1()\n    sleep_time = int(os.getenv(\"SLEEP_TIME\", 3))\n    remove_previous_container(client)\n    container = client.containers.run(\n        image, name=CONTAINER_NAME, ports={\"80\": \"8000\"}, detach=True\n    )\n    time.sleep(sleep_time)\n    verify_container(container, response_text)\n    container.stop()\n    # Test that everything works after restarting too\n    container.start()\n    time.sleep(sleep_time)\n    verify_container(container, response_text)\n    container.stop()\n    container.remove()\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": "tests/test_01_main/test_defaults_01.py", "language": "python", "complexity": "medium", "star_count": 3007, "quality_score": 0.78, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 88}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nimport time\n\nimport docker\nimport requests\nfrom docker.models.containers import Container\nfrom requests import Response\n\nfrom ..utils import (\n    CONTAINER_NAME,\n    get_logs,\n    get_nginx_config,\n    get_response_text1,\n    remove_previous_container,\n)\n\nclient = docker.from_env()\n\n\ndef verify_container(container: Container, response_text: str) -> None:\n    nginx_config = get_nginx_config(container)\n    assert \"client_max_body_size 1m;\" in nginx_config\n    assert \"worker_processes 2;\" in nginx_config\n    assert \"listen 80;\" in nginx_config\n    assert \"worker_connections 2048;\" in nginx_config\n    assert \"worker_rlimit_nofile 2048;\" in nginx_config\n    assert \"daemon off;\" in nginx_config\n    assert \"listen 80;\" in nginx_config\n    assert \"include uwsgi_params;\" in nginx_config\n    assert \"uwsgi_pass unix:///tmp/uwsgi.sock;\" in nginx_config\n    assert \"try_files $uri @app;\" in nginx_config\n    assert \"location @app {\" in nginx_config\n    assert \"include uwsgi_params;\" in nginx_config\n    assert \"location /static {\" in nginx_config\n    assert \"alias /app/static;\" in nginx_config\n    # Nginx index.html specific\n    assert \"location = / {\" not in nginx_config\n    assert \"index /static/index.html;\" not in nginx_config\n    logs = get_logs(container)\n    assert \"getting INI configuration from /app/uwsgi.ini\" in logs\n    assert \"getting INI configuration from /etc/uwsgi/uwsgi.ini\" in logs\n    assert \"ini = /app/uwsgi.ini\" in logs\n    assert \"ini = /etc/uwsgi/uwsgi.ini\" in logs\n    assert \"socket = /tmp/uwsgi.sock\" in logs\n    assert \"chown-socket = nginx:nginx\" in logs\n    assert \"chmod-socket = 664\" in logs\n    assert \"hook-master-start = unix_signal:15 gracefully_kill_them_all\" in logs\n    assert \"need-app = true\" in logs\n    assert \"die-on-term = true\" in logs\n    assert \"show-config = true\" in logs\n    assert \"module = main\" in logs\n    assert \"callable = app\" in logs\n    assert \"processes = 8\" in logs\n    assert \"cheaper = 3\" in logs\n    assert \"spawned uWSGI master process\" in logs\n    assert \"spawned uWSGI worker 1\" in logs\n    assert \"spawned uWSGI worker 2\" in logs\n    assert \"spawned uWSGI worker 3\" in logs\n    assert \"spawned uWSGI worker 4\" not in logs\n    assert 'running \"unix_signal:15 gracefully_kill_them_all\" (master-start)' in logs\n    assert \"success: nginx entered RUNNING state, process has stayed up for\" in logs\n    assert \"success: uwsgi entered RUNNING state, process has stayed up for\" in logs\n    assert \"Checking for script in /app/prestart.sh\" in logs\n    assert \"Running script /app/prestart.sh\" in logs\n    assert (\n        \"Running inside /app/prestart.sh, you could add migrations to this file\" in logs\n    )\n    response: Response = requests.get(\"http://127.0.0.1:8000\")\n    assert response.status_code == 200\n    assert response.text == response_text\n\n\ndef test_defaults() -> None:\n\n    name = os.getenv(\"NAME\", \"\")\n    image = f\"tiangolo/uwsgi-nginx-flask:{name}\"\n    response_text = get_response_text1()\n    sleep_time = int(os.getenv(\"SLEEP_TIME\", 3))\n    remove_previous_container(client)\n    container = client.containers.run(\n        image,\n        name=CONTAINER_NAME,\n        environment={\n            \"UWSGI_CHEAPER\": 3,\n            \"UWSGI_PROCESSES\": 8,\n            \"NGINX_MAX_UPLOAD\": \"1m\",\n            \"NGINX_WORKER_PROCESSES\": 2,\n            \"NGINX_WORKER_CONNECTIONS\": 2048,\n            \"NGINX_WORKER_OPEN_FILES\": 2048,\n        },\n        ports={\"80\": \"8000\"},\n        detach=True,\n    )\n    time.sleep(sleep_time)\n    verify_container(container, response_text)\n    container.stop()\n    # Test that everything works after restarting too\n    container.start()\n    time.sleep(sleep_time)\n    verify_container(container, response_text)\n    container.stop()\n    container.remove()\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-flask-docker", "file_path": "tests/test_01_main/test_env_vars_03.py", "language": "python", "complexity": "medium", "star_count": 3007, "quality_score": 0.78, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 103}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\nIf this issue is intended to be a *bug report*, please fill out the following:\n\n**Expected behavior**\n\n**Actual behavior**\n\n**Steps to reproduce the behavior**\n\n---\n\nIf this issue is intended to be a *beginner issue*, please add all of the following:\n\n- [ ] Requirements are clearly specified\n- [ ] Links to relevant \"getting started\" documentation\n- [ ] Tagged as \"beginner\"\n- [ ] Tagged with language skill needed to complete\n- [ ] Level of difficulty:\n  * Trivial = Changing one-line of code somewhere.\n  * Standalone = Changes & Development do not affect other components of project. Tasks can be on a critical path, but probably should not be.\n  * Simple = from 2 hours work to maybe a week‚Äôs work (Thinking + Coding)", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": ".github/ISSUE_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 2879, "quality_score": 0.63, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Docker Image Build - Intel drivers\n\non:\n  push:\n    branches: [ master, dev*, test* ]\n  pull_request:\n    branches: [ master, dev*, test* ]\n\njobs:\n\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Build the Docker image\n      run: |\n        chmod a+w output/\n        ls -alth\n        ./build_intel.sh\n\n    - name: Info on Docker images\n      run: |\n        docker -v\n        docker images\n\n    - name: Run the Docker image\n      run: |\n        ./run.sh\n\n    - name: Info on generated files\n      run: |\n        ls -alth\n        ls -alth output\n        ls -alth output/*\n        more output/*/report.json\n", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": ".github/workflows/docker-image-intel.yml", "language": "yaml", "complexity": "simple", "star_count": 2879, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 39}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Docker Image Quick Run\n\non:\n  push:\n    branches: [ master, dev*, test* ]\n  pull_request:\n    branches: [ master, dev*, test* ]\n\njobs:\n\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v4\n    - name: Build the Docker image\n      run: |\n        chmod a+w output/\n        ls -alth\n        ./build.sh\n    - name: Info on Docker images\n      run: |\n        docker -v\n        docker images\n    - name: Run the Docker image\n      run: |\n        ./run-quick.sh\n    - name: Info on generated files\n      run: |\n        ls -alth\n        ls -alth output\n        ls -alth output/*\n", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": ".github/workflows/docker-image-quickrun.yml", "language": "yaml", "complexity": "simple", "star_count": 2879, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 34}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Docker Image Build and Run\n\non:\n  push:\n    branches: [ master, dev*, test* ]\n  pull_request:\n    branches: [ master, dev*, test* ]\n\njobs:\n\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v4\n    - name: Build the Docker image\n      run: |\n        chmod a+w output/\n        ls -alth\n        ./build.sh\n    - name: Info on Docker images\n      run: |\n        docker -v\n        docker images\n    - name: Run the Docker image\n      run: |\n        ./run.sh\n    - name: Info on generated files\n      run: |\n        ls -alth\n        ls -alth output\n        ls -alth output/*\n", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": ".github/workflows/docker-image.yml", "language": "yaml", "complexity": "simple", "star_count": 2879, "quality_score": 0.6, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 34}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Docker Pull Test\n\non:\n  push:\n    branches: [ master ]\n  pull_request:\n    branches: [ master ]\n\njobs:\n\n  build:\n\n    runs-on: ubuntu-22.04\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Run the Docker image\n      run: |\n        # Don't build. Will be pulled from Dockerhub when run below\n        ./run.sh\n\n    - name: Info on Docker images\n      run: |\n        docker -v\n        docker images\n\n    - name: Run the Docker image - no c302\n      run: |\n        # Shouldn't need to pull again.\n        ./stop.sh\n        ./run-quick.sh\n\n    - name: Info on Docker images\n      run: |\n        docker -v\n        docker images\n        \n    - name: Info on generated files\n      run: |\n        ls -alth\n        ls -alth output\n        ls -alth output/*\n        more output/*\n", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": ".github/workflows/docker-pull.yml", "language": "yaml", "complexity": "simple", "star_count": 2879, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 45}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "repo: openworm/OpenWorm\nshortDescription: \"Project Home repo for Central Dockerfile and Project-wide issues\"\ncontributor-guide: \"http://docs.openworm.org/community/#contribution-best-practices\"\ncoordinator: stephen@openworm.org\ndocumentation: \"http://docs.openworm.org\"\n", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": ".openworm.yml", "language": "yaml", "complexity": "simple", "star_count": 2879, "quality_score": 0.39999999999999997, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 6}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "![OpenWorm](http://www.openworm.org/img/OpenWormLogo.png)\n\n[![Docker Image CI](https://github.com/openworm/OpenWorm/actions/workflows/docker-image.yml/badge.svg)](https://github.com/openworm/OpenWorm/actions/workflows/docker-image.yml) [![Docker Image Test - quick](https://github.com/openworm/OpenWorm/actions/workflows/docker-image-quickrun.yml/badge.svg)](https://github.com/openworm/OpenWorm/actions/workflows/docker-image-quickrun.yml) [![Docker Image Test Build - Intel drivers](https://github.com/openworm/OpenWorm/actions/workflows/docker-image-intel.yml/badge.svg)](https://github.com/openworm/OpenWorm/actions/workflows/docker-image-intel.yml)\n\n## About **OpenWorm**\n\n[OpenWorm](http://openworm.org) aims to build the first comprehensive computational model of *Caenorhabditis elegans* (*C. elegans*), a microscopic roundworm. With only a thousand cells, it solves basic problems such as feeding, mate-finding and predator avoidance. Despite being extremely well-studied in biology, a deep, principled understanding of the biology of this organism remains elusive.\n\nWe are using a bottom-up approach, aimed at observing the worm behavior emerge from a simulation of data derived from scientific experiments carried out over the past decade. To do so, we are incorporating the data available from the scientific community into software models. We are also forging new collaborations with universities and research institutes to collect data that fill in the gaps.\n\nYou can earn a badge with us simply by trying out this package! Click on the image below to get started.\n[![OpenWorm Docker Badge](https://raw.githubusercontent.com/openworm/OpenWorm/master/img/ow-docker-badge.png)](https://www.badgelist.com/OpenWorm/OpenWorm-Docker-Apprentice)\n\n## Quickstart\n\nWe have put together a [Docker container](https://hub.docker.com/r/openworm/openworm) that pulls together the major components of our simulation and runs it on your machine. When you get it all running, it does the following:\n\n1. Run our nervous system model, known as [c302](https://github.com/openworm/c302), on your computer.\n2. In parallel, run our 3D worm body model, known as [Sibernetic](https://github.com/openworm/sibernetic), on your computer, using the output of the nervous system model.\n3. Produce graphs from the nervous system and body model that demonstrate its behavior on your computer for you to inspect.\n4. Produce a movie showing the output of the body model.\n\n**Example Output**\n\n![Worm Crawling](https://raw.githubusercontent.com/openworm/OpenWorm/master/img/worm-crawling.gif)\n\n<img src=\"https://raw.githubusercontent.com/openworm/OpenWorm/master/img/muscle-activity.png\" width=\"250\"><img src=\"https://raw.githubusercontent.com/openworm/OpenWorm/master/img/neuron-activity.png\" width=\"350\">\n\n**NOTE**: Running the simulation for the full amount of time would produce content like the above. However, in order to run in a reasonable amount of time, **the default run time for the simulation is limited**. As such, you will see only a partial output, equivalent to about 5% of run time, compared to the examples above. To extend the run time, use the `-d` argument as described below.\n\n## Installation\n\n### Pre-requisites\n\n1. At least 60 GB of free space on your machine and at least 2GB of RAM.\n2. Ability to clone git repositories on your machine. [Install git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git), or use [this GUI](https://desktop.github.com/).\n\n### To Install\n\n1. Install [Docker](http://docker.com) on your system.\n2. If your system does not have enough free space, you can use an external hard disk. On MacOS X, the location for image storage can be specified in the [Advanced Tab](https://forums.docker.com/t/change-docker-image-directory-for-mac/18891/15) in Preferences. See [this thread](https://forums.docker.com/t/how-do-i-change-the-docker-image-installation-directory/1169/18) for Linux instructions.\n\n## Running\n\n1. Ensure the Docker daemon is running in the background (on MacOS/Windows there should be an icon with the Docker whale logo showing in the menu bar/system tray). \n2. Open a terminal and run:\n   ```bash\n   git clone http://github.com/openworm/openworm\n   cd openworm\n   ```\n3. Optional: Run `./build.sh` (`build.cmd` on Windows or `.\\build.cmd` on [PowerShell](https://learn.microsoft.com/en-us/powershell/dsc/overview?view=dsc-2.0)). If you skip this step, it will download the latest released Docker image from the [OpenWorm Docker hub](https://hub.docker.com/r/openworm/openworm).\n4. Run `./run.sh` (`run.cmd` on Windows or `.\\run.cmd` in PowerShell).\n5. About 5-10 minutes of output will display on the screen as the steps run.\n6. The simulation will end. Run `stop.sh` (`stop.cmd` on Windows or `.\\stop.cmd` in PowerShell) on your system to clean up the running container.\n7. Inspect the output in the `output` directory on your local machine.\n\n## Advanced\n\n### Arguments\n\n- `-d [num]`: Use to modify the duration of the simulation in milliseconds. Default is 15. Use 5000 to run for time to make the full movie above (i.e. 5 seconds).\n\n### Other Things to Try\n\n- Open a terminal and run `./run-shell-only.sh` (`run-shell-only.cmd` on Windows or `.\\run-shell-only.cmd` in PowerShell). This will let you log into the container before it has run `master_openworm.py`. From here you can inspect the internals of the various checked out code bases and installed systems and modify things. Afterwards, you'll still need to run `./stop.sh` to clean up.\n- If you wish to modify what gets installed, you should modify `Dockerfile`. If you want to modify what runs, you should modify `master_openworm.py`. Either way you will need to run `build.sh` in order to rebuild the image locally. Afterwards, you can run normally.\n\n## FAQ\n\n### What is the Docker container?\n\nThe Docker container is a self-contained environment in which you can run OpenWorm simulations. It's fully set up to get you started by following the steps above. At the moment, it runs simulations and produces visualizations for you, but these visualizations must be viewed outside of the Docker container. While you do not need to know much about Docker to use OpenWorm, if you are planning on working extensively with the platform, you may benefit from understanding some basics. [Docker Curriculum](https://docker-curriculum.com) is an excellent tutorial for beginners that is straightforward to work through (Sections 1 - 2.5 are plenty sufficient).\n\n### Is it possible to modify the simulation without having to run `build.sh`?\n\nYes, but it is marginally more complex. The easiest way is to modify anything in the Docker container once you are inside of it - it will work just like a bash shell. If you want to modify any code in the container, you'll need to use an editor that runs in the terminal, like nano. Once you've modified something in the container, you don't need to re-build. However, if you run `stop.sh` once you exit, those changes will be gone.\n\n### How do I access more data than what is already output?\n\nThe simulation by default outputs only a few figures and movies to your home system (that is, outside of the Docker container). If you want to access the entire output of the simulation, you will need to copy it from the Docker container.\n\nFor example, say you want to extract the worm motion data. This is contained in the file `worm_motion_log.txt`, which is found in the `/home/ow/sibernetic/simulations/[SPECIFIC_TIMESTAMPED_DIRECTORY]/worm_motion_log.txt`. The directory `[SPECIFIC_TIMESTAMPED_DIRECTORY]` will have a name like `C2_FW_2018_02-12_18-36-32`, and its name can be found by checking the `output` directory. This is actually the main output directory for the simulation, and contains all output, including cell modelling and worm movement.\n\nOnce the simulation ends and you exit the container with `exit`, but before you run `stop.sh`, run the following command from the openworm-docker-master folder:\n\n```bash\ndocker cp openworm:/home/ow/sibernetic/simulations/[SPECIFIC_TIMESTAMPED_DIRECTORY]/worm_motion_log.txt ./worm_motion_log.txt\n```\n\nThis will copy the file from the Docker container, whose default name is `openworm`. **It is crucial that you do not run `stop.sh` before trying to get your data out (see below)**\n\n### What is the difference between `exit` and `stop.sh`?\n\nWhen you are in the Docker Container `openworm`, and are done interacting with it, you type `exit` to return to your system's shell. This stops execution of anything in the container, and that container's status is now `Exited`. If you try to re-start the process using `run-shell-only.sh`, you will get an error saying that the container already exists. You can choose, at this point, to run `stop.sh`. Doing so will remove the container and any files associated with it, allowing you to run a new simulation. However, if you don't want\n\n to remove that container, you will instead want to re-enter it.\n\n### How do I enter a container I just exited?\n\nIf you run `stop.sh` you'll delete your data and reset the container for a new run. If, however, you don't want to do that, you can re-enter the Docker container like this:\n\n```bash\ndocker start openworm                 # Restarts the container\ndocker exec -it openworm /bin/bash    # Runs bash inside the container\n```\n\nThis tells Docker to start the container, to *execute* commands (`exec`) with an *interactive, tty* (`-it`) bash (`bash`) shell in the container `openworm`.\n\nYou'll be able to interact with the container as before.\n\n## Documentation\n\nTo find out more about OpenWorm, please see the documentation at [http://docs.openworm.org](http://docs.openworm.org) or [get in contact](https://openworm.org/contacts.html).\n\nThis repository also contains project-wide tracking via high-level [issues](https://github.com/openworm/OpenWorm/issues) and [milestones](https://github.com/openworm/OpenWorm/milestones).", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 2879, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 115}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/bin/bash\n\nversion=$(<VERSION) # Read version of Dockerfile from file VERSION\n\n# Set the platform flag if we're on ARM\narch=$(uname -m)\nif [[ \"$arch\" == \"arm64\" || \"$arch\" == \"aarch64\" ]]; then\n    platform_flag=\"--platform linux/amd64\"\nelse\n    platform_flag=\"\"\nfi\n\n# Build the Docker image\ndocker build $platform_flag \"$@\" -t \"openworm/openworm:$version\" .\n", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": "build.sh", "language": "bash", "complexity": "simple", "star_count": 2879, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/bin/bash\n\n# For generating/running a Dockerfile image based on Python 2\n# Note: Python 2 is no longer officially supported and this Docker image will\n# probably stop working eventually...\n\nversion=$(<VERSION) # Read version of Dockerfile from file VERSION\ndocker build \"$@\" -t \"openworm/openworm:${version}_py2\"  -f Dockerfile2 .\n", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": "build2.sh", "language": "bash", "complexity": "simple", "star_count": 2879, "quality_score": 0.43, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 9}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/bin/bash\n\n# A version of the docker container using the Intel libraries for OpenCL\n\nversion=$(<VERSION) # Read version of Dockerfile from file VERSION\ndocker build --platform linux/amd64  \"$@\" -t \"openworm/openworm:$version\" -f Dockerfile_intel .\n", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": "build_intel.sh", "language": "bash", "complexity": "simple", "star_count": 2879, "quality_score": 0.43, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 7}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import errno\nimport matplotlib\nmatplotlib.use('Agg')\nimport shutil\nfrom subprocess import Popen, PIPE, check_output, STDOUT\nimport os\nimport pwd\nimport shlex\nimport sys\nimport time\nimport glob\nimport math\n\nprint(\"*****************************\")\nprint(\"OpenWorm Master Script\")\nprint(\"*****************************\")\nprint(\"\")\nprint(\"This script attempts to run a full pass through the OpenWorm scientific libraries.\")\nprint(\"This depends on several other repositories being loaded to work and presumes it is running in a preloaded Docker instance.\")\nprint(\"It will report out where steps are missing.\")\nprint(\"Eventually all the steps will be filled in.\")\nprint(\"\")\n\nprint(\"****************************\")\nprint(\"Step 1: Rebuild c302 from the latest owmeta\")\nprint(\"****************************\")\nprint(\"Not yet implemented. See https://github.com/openworm/c302/issues/10\")\n\n\nprint(\"****************************\")\nprint(\"Step 2: Execute unit tests via the c302 simulation framework\")\nprint(\"****************************\")\n\"\"\"\nfrom runAndPlot import run_c302\norig_display_var = None\nif os.environ.has_key('DISPLAY'):\n    orig_display_var = os.environ['DISPLAY']\n    del os.environ['DISPLAY'] # https://www.neuron.yale.edu/phpBB/viewtopic.php?f=6&t=1603\n\nrun_c302(DEFAULTS['reference'],\n         DEFAULTS['c302params'],\n         '',\n         DEFAULTS['duration'],\n         DEFAULTS['dt'],\n         'jNeuroML_NEURON',\n         data_reader=DEFAULTS['datareader'],\n         save=True,\n         show_plot_already=False,\n         target_directory=os.path.join(os.environ['C302_HOME'], 'examples'),\n         save_fig_to='tmp_images')\nprev_dir = os.getcwd()\nos.chdir(DEFAULTS['outDir'])\ntry:\n    os.mkdir('c302_out')\nexcept OSError as e:\n    if e.errno != errno.EEXIST:\n        raise\nsrc_files = os.listdir(os.path.join(os.environ['C302_HOME'], 'examples', 'tmp_images'))\nfor file_name in src_files:\n    full_file_name = os.path.join(os.environ['C302_HOME'], 'examples', 'tmp_images', file_name)\n    print(\"COPY %s\" % full_file_name)\n    if (os.path.isfile(full_file_name)):\n        shutil.copy2(full_file_name, 'c302_out')\nshutil.rmtree(os.path.join(os.environ['C302_HOME'], 'examples', 'tmp_images'))\nos.chdir(prev_dir)\nif orig_display_var:\n    os.environ['DISPLAY'] = orig_display_var\n\"\"\"\n\nprint(\"****************************\")\nprint(\"Step 3: Run c302 + Sibernetic in the same loop.\")\nprint(\"****************************\")\n\nOW_OUT_DIR = os.environ['OW_OUT_DIR']\n\ndef execute_with_realtime_output(command, directory, env=None):\n    p = None\n    try:\n\n        print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n        print('>> Executing command: %s'%command)\n        print('>> --------------------------------------------------------------')\n        p = Popen(shlex.split(command),\n                  stdout=PIPE,\n                  stderr=STDOUT,\n                  cwd=directory,\n                  env=env)\n        with p.stdout:\n            for line in iter(p.stdout.readline, b''):\n                 print('>>  %s'%line.decode('utf-8'), end='')\n        p.wait() # wait for the subprocess to exit\n    except KeyboardInterrupt as e:\n        print(\"Caught CTRL+C\")\n        if p:\n            p.kill()\n        raise e\n    print('>> --------------------------------------------------------------')\n    print('>> Command exited with %i: %s'%(p.returncode,command))\n    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n')\n\n    if p.returncode!=0:\n        print('Exiting as the last command failed')\n        exit(p.returncode)\n\n\n\nsys.path.append(os.environ['C302_HOME'])\n\ntry:\n    os.system('echo Granting permissions for xhost && xhost +')\nexcept:\n    print(\"Unexpected error: %s\" % sys.exc_info()[0])\n\nOW_OUT_DIR = os.environ['OW_OUT_DIR']\n\n\ntry:\n    if os.access(OW_OUT_DIR, os.W_OK) is not True:\n        os.system('sudo chown -R %s:%s %s' % (os.environ['USER'], os.environ['USER'], OW_OUT_DIR))\nexcept:\n    print(\"Unexpected error: %s\" % sys.exc_info()[0])\n    raise\n\n#Default is 15 ms of simulation time.\nsim_duration = 15.0\nif 'DURATION' in os.environ:\n    sim_duration = float(os.environ['DURATION'])\n\nnoc302 = False\nif 'NOC302' in os.environ:\n    noc302 = bool(os.environ['NOC302'])\n\nDEFAULTS = {'duration': sim_duration,\n            'dt': 0.005,\n            'dtNrn': 0.05,\n            'logstep': 100,\n            'reference': 'FW',\n            'c302params': 'C2',\n            'verbose': False,\n            'device': 'CPU',\n            'configuration': 'worm_crawl_half_resolution',\n            'noc302': noc302,\n            'datareader': 'UpdatedSpreadsheetDataReader2',\n            'outDir': OW_OUT_DIR}\n\nmy_env = os.environ.copy()\nDISPLAY = ':44'\nmy_env[\"DISPLAY\"] = DISPLAY\n\n# Xvfb or X virtual framebuffer is a display server implementing the X11 display server protocol.\n# In contrast to other display servers, Xvfb performs all graphical operations in virtual memory without showing any screen output.\nos.system('echo Starting xvfb && Xvfb %s -listen tcp -ac -screen 0 1920x1080x24+32 &'%DISPLAY) # TODO: terminate xvfb after recording\n\ntry:\n    command = \"\"\"python3 sibernetic_c302.py\n                -duration %s\n                -dt %s\n                -dtNrn %s\n                -logstep %s\n                -device=%s\n                -configuration %s\n                -reference %s\n                -c302params %s\n                -datareader %s\n                -outDir %s\"\"\" % \\\n                (DEFAULTS['duration'],\n                DEFAULTS['dt'],\n                DEFAULTS['dtNrn'],\n                DEFAULTS['logstep'],\n                DEFAULTS['device'],\n                DEFAULTS['configuration'],\n                DEFAULTS['reference'],\n                DEFAULTS['c302params'],\n                DEFAULTS['datareader'],\n                'simulations')\n                #DEFAULTS['outDir'])\n\n    if noc302: command += ' -noc302'\n\n    execute_with_realtime_output(command, os.environ['SIBERNETIC_HOME'], env=my_env)\nexcept KeyboardInterrupt as e:\n    pass\n\nsibernetic_sim_dir = '%s/simulations' % os.environ['SIBERNETIC_HOME']\n\nall_subdirs = []\nfor dirpath, dirnames, filenames in os.walk(sibernetic_sim_dir):\n    for directory in dirnames:\n        if directory.startswith('%s_%s' % (DEFAULTS['c302params'], DEFAULTS['reference'])):\n            all_subdirs.append(os.path.join(dirpath, directory))\n        if directory.startswith('Sibernetic'):\n            all_subdirs.append(os.path.join(dirpath, directory))\n\nlatest_subdir = max(all_subdirs, key=os.path.getmtime)\nprint('\\n========================================================================\\n')\nprint('Finished main simulation, data saved in: %s'%latest_subdir)\nexecute_with_realtime_output('ls -alt %s'%latest_subdir, os.environ['SIBERNETIC_HOME'], env=my_env)\n\ntry:\n    os.mkdir('%s/output' % OW_OUT_DIR)\nexcept OSError as e:\n    if e.errno != errno.EEXIST:\n        raise\n\nnew_sim_out = '%s/output/%s' % (OW_OUT_DIR, os.path.split(latest_subdir)[-1])\ntry:\n    os.mkdir(new_sim_out)\nexcept OSError as e:\n    if e.errno != errno.EEXIST:\n        raise\n\n\n# Copy PNGs, created during the Sibernetic simulation, in a separate child-directory to find them more easily\nfigures = glob.glob('%s/*.png' % latest_subdir)\nfor figure in figures:\n    print(\"Moving %s to %s\"%(figure, new_sim_out))\n    shutil.move(figure, new_sim_out)\n\n# Copy reports etc.\nreports = glob.glob('%s/report*' % latest_subdir)\nfor report in reports:\n    print(\"Moving %s to %s\"%(report, new_sim_out))\n    shutil.move(report, new_sim_out)\n\n# Copy WCON file(s)\nwcons = glob.glob('%s/*.wcon' % latest_subdir)\nfor wcon in wcons:\n    print(\"Moving %s to %s\"%(wcon, new_sim_out))\n    shutil.move(wcon, new_sim_out)\n\ntime.sleep(2)\n\n# Rerun and record simulation\nexecute_with_realtime_output('ls -alt /tmp/.X11-unix', os.environ['SIBERNETIC_HOME'], env=my_env)\nos.system('export DISPLAY=%s'%DISPLAY)\nexecute_with_realtime_output('ls -alt /tmp/.X11-unix', os.environ['SIBERNETIC_HOME'], env=my_env)\nsibernetic_movie_name = '%s.mp4' % os.path.split(latest_subdir)[-1]\ncommand = 'tmux new-session -d -P -s SiberneticRecording \"DISPLAY=%s ffmpeg -r 30 -f x11grab -draw_mouse 0 -s 1920x1080 -i %s -filter:v \"crop=1200:800:100:100\" -cpu-used 0 -b:v 384k -qmin 10 -qmax 42 -maxrate 384k -bufsize 1000k -an %s/%s\"' % (DISPLAY, DISPLAY, new_sim_out, sibernetic_movie_name)\nexecute_with_realtime_output(command, os.environ['SIBERNETIC_HOME'], env=my_env)\n\ntime.sleep(3)\n\nexecute_with_realtime_output('tmux list-sessions', os.environ['SIBERNETIC_HOME'], env=my_env)\n\ncommand = './Release/Sibernetic -f %s -l_from lpath=%s' % (DEFAULTS['configuration'], latest_subdir)\nexecute_with_realtime_output(command, os.environ['SIBERNETIC_HOME'], env=my_env)\n\nexecute_with_realtime_output('tmux send-keys -t SiberneticRecording q', os.environ['SIBERNETIC_HOME'], env=my_env)\nexecute_with_realtime_output('tmux send-keys -t SiberneticRecording \"exit\" C-m', os.environ['SIBERNETIC_HOME'], env=my_env)\n\ntime.sleep(3)\n\nexecute_with_realtime_output('ls -alt %s'%latest_subdir, os.environ['SIBERNETIC_HOME'], env=my_env)\n\n# Remove black frames at the beginning of the recorded video\ncommand = \"ffmpeg -i %s/%s -vf blackdetect=d=0:pic_th=0.70:pix_th=0.10 -an -f null - 2>&1 | grep blackdetect\" % (new_sim_out, sibernetic_movie_name)\noutstr = str(check_output(command, shell=True).decode('utf-8'))\noutstr = outstr.split('\\n')\n\nblack_start = 0.0\nblack_dur = None\n\n\nout = outstr[0]\n\nblack_start_pos = out.find('black_start:')\nblack_end_pos = out.find('black_end:')\nblack_dur_pos = out.find('black_duration:')\nif black_start_pos != -1:\n    black_start = float(out[black_start_pos + len('black_start:') : black_end_pos])\n    black_dur = float(out[black_dur_pos + len('black_duration:'):])\n\nif black_start == 0.0 and black_dur:\n    black_dur = int(math.ceil(black_dur))\n    command = 'ffmpeg -ss 00:00:0%s -i %s/%s -c copy -avoid_negative_ts 1 %s/cut_%s' % (black_dur, new_sim_out, sibernetic_movie_name, new_sim_out, sibernetic_movie_name)\n    if black_dur > 9:\n        command = 'ffmpeg -ss 00:00:%s -i %s/%s -c copy -avoid_negative_ts 1 %s/cut_%s' % (black_dur, new_sim_out, sibernetic_movie_name, new_sim_out, sibernetic_movie_name)\n    os.system(command)\n\n# SPEED-UP\ntry:\n    os.mkdir('tmp')\nexcept OSError as e:\n    if e.errno != errno.EEXIST:\n        raise\n\nos.system('ffmpeg -ss 1 -i %s/cut_%s -vf \"select=gt(scene\\,0.1)\" -vsync vfr -vf fps=fps=1/1 %s' % (new_sim_out, sibernetic_movie_name, 'tmp/out%06d.jpg'))\nos.system('ffmpeg -r 100 -i %s -r 100 -vb 60M %s/speeded_%s' % ('tmp/out%06d.jpg', new_sim_out, sibernetic_movie_name))\n\nos.system('rm -r tmp/*')\n\n\n\nprint(\"****************************\")\nprint(\"Step 4: Run movement analysis\")\nprint(\"****************************\")\nprint(\"Not yet implemented.\")\nprint(\"Note however the following WCON files have been generated into %s during the simulation: %s\"%(new_sim_out, [w.split('/')[-1] for w in wcons]))\n\n\nprint(\"****************************\")\nprint(\"Step 5: Report on movement analysis fit to real worm videos\")\nprint(\"****************************\")\nprint(\"Not yet implemented.\")\n", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": "master_openworm.py", "language": "python", "complexity": "complex", "star_count": 2879, "quality_score": 0.84, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 305}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "The output produced after running simulations in the Docker container will be saved here (in your native filesystem).\n", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": "output/README.md", "language": "markdown", "complexity": "simple", "star_count": 2879, "quality_score": 0.39999999999999997, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 2}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/bin/bash\n\nversion=$(<VERSION) # Read version of Dockerfile from file VERSION\n\n# Set the platform flag if we're on ARM\narch=$(uname -m)\nif [[ \"$arch\" == \"arm64\" || \"$arch\" == \"aarch64\" ]]; then\n    platform_flag=\"--platform linux/amd64\"\nelse\n    platform_flag=\"\"\nfi\n\n# Rebuild the Docker image\ndocker build $platform_flag \"$@\" -t \"openworm/openworm:$version\"  --no-cache .\n", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": "rebuild.sh", "language": "bash", "complexity": "simple", "star_count": 2879, "quality_score": 0.63, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/bin/bash\n\n#from: https://unix.stackexchange.com/a/129401\nwhile getopts \":d:p:\" opt; do\n  case $opt in\n    d) duration=\"$OPTARG\"\n    ;;\n    p) p_out=\"$OPTARG\"\n    ;;\n    \\?) echo \"Invalid option -$OPTARG\" >&2\n    ;;\n  esac\ndone\n\nOW_OUT_DIR=/home/ow/shared\nHOST_OUT_DIR=$PWD\nversion=$(<VERSION) # Read version of Dockerfile from file VERSION\n\nxhost +\n\n\nNOC302_PART=\"-e NOC302=1\"\nDURATION_PART=\"-e DURATION=10\"\n\ndocker run -d \\\n--name openworm_$version \\\n--device=/dev/dri:/dev/dri \\\n-e DISPLAY=$DISPLAY \\\n$NOC302_PART \\\n$DURATION_PART \\\n-e OW_OUT_DIR=$OW_OUT_DIR \\\n-v /tmp/.X11-unix:/tmp/.X11-unix:rw \\\n--privileged \\\n-v $HOST_OUT_DIR:$OW_OUT_DIR:rw \\\nopenworm/openworm:$version \\\nbash -c \"DISPLAY=:44 python master_openworm.py\"\n\ndocker logs -f openworm_$version\n", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": "run-quick.sh", "language": "bash", "complexity": "simple", "star_count": 2879, "quality_score": 0.63, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 39}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/bin/bash\n\nOW_OUT_DIR=/home/ow/shared\nHOST_OUT_DIR=$PWD\n\nversion=$(<VERSION) # Read version of Dockerfile from file VERSION\n\nxhost +\n\ndocker run -ti \\\n  --name openworm_$version \\\n  --device=/dev/dri:/dev/dri \\\n  -e DISPLAY=$DISPLAY \\\n  -e OW_OUT_DIR=$OW_OUT_DIR \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix:rw \\\n  --privileged \\\n  -v $HOST_OUT_DIR:$OW_OUT_DIR:rw \\\n  openworm/openworm:$version \\\n  bash\n", "metadata": {"source": "github.com/openworm/OpenWorm", "file_path": "run-shell-only.sh", "language": "bash", "complexity": "simple", "star_count": 2879, "quality_score": 0.63, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{\n  \"name\": \"Python 3\",\n  \"dockerFile\": \"Dockerfile\",\n  \"runArgs\": [\"--privileged\"],\n  \"mounts\": [\"source=/var/run/docker.sock,target=/var/run/docker.sock,type=bind\"],\n  \"extensions\": [\"ms-python.python\"],\n  \"settings\": {\n    \"terminal.integrated.shell.linux\": \"/bin/bash\",\n    \"python.pythonPath\": \"/usr/local/bin/python\",\n    \"python.linting.enabled\": true,\n    \"python.linting.pylintEnabled\": true,\n    \"python.linting.pylintPath\": \"/usr/local/py-utils/bin/pylint\"\n  },\n  \"postCreateCommand\": \"pip3 install --user -r requirements.txt\",\n}\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": ".devcontainer/devcontainer.json", "language": "json", "complexity": "simple", "star_count": 993, "quality_score": 0.53, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 16}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Bug Report\nabout: Report bugs you notice while using Tern\n\n---\n\n**Describe the bug**\nA clear and concise description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to '...'\n2. Click on '....'\n3. Scroll down to '....'\n4. See error\n\n**Error in terminal**\n```\nPaste your terminal output here\n```\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Environment you are running Tern on**\nEnter all that apply\n- Output of 'tern --version'\n- Operating System (Linux Distro and version or Mac or Windows)\n- Vagrant file\n- Container OS\n- Python version (3.6 or higher)\n- Cloud environment (AWS, Azure, GCP)\n\n**Please attach files if they exist**\n- tern.log\n- report.*\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": ".github/ISSUE_TEMPLATE/bug.md", "language": "markdown", "complexity": "simple", "star_count": 993, "quality_score": 0.5, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 37}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Pull Request Lint and Test\non: pull_request\n\njobs:\n  Prospector_Linting:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v1 #equivlent to running git fetch and git checkout latest\n      - uses: actions/setup-python@v1 # setup python3 environment\n        with:\n          python-version: '3.8'\n      - name: Setup\n        run: |\n          sudo apt-get install -y attr\n          pip install --upgrade pip\n          pip install .\n      - name: Prospector Linting\n        run: |\n          pip install prospector>=1.5.1\n          prospector --version\n          prospector\n  Commit_Message_Linting:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v1\n      - uses: actions/setup-python@v1\n        with:\n          python-version: '3.8'\n      - name: Setup \n        run: |\n          sudo apt-get install -y attr\n          pip install --upgrade pip\n          pip install .\n      - name: Commit Message Linting\n        run: python ci/test_commit_message.py\n  Security_Linting:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v1\n      - uses: actions/setup-python@v1\n        with:\n          python-version: '3.8'\n      - name: Setup \n        run: |\n          sudo apt-get install -y attr\n          pip install --upgrade pip\n          pip install .\n      - name: Security Linting\n        run: |\n          pip install bandit>=1.6\n          c=`python ci/evaluate_docs.py`; if [ -z $c ]; then echo \"No .py files to lint\"; else echo $c | xargs bandit; fi\n  Test_Changes:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v1\n      - uses: actions/setup-python@v1\n        with:\n          python-version: '3.8'\n      - name: Setup\n        run: |\n          sudo apt-get update && sudo apt-get install -y attr openjdk-8-jdk-headless maven\n          pip install --upgrade pip\n          pip install .\n          docker pull photon:3.0 && docker save photon:3.0 > photon.tar\n          # build SPDX validation tool from source\n          git clone https://github.com/spdx/tools-java.git && cd tools-java\n          export JAVA_HOME=$(readlink -f /usr/bin/javac | sed \"s:/bin/javac::\")\n          mvn clean install && cd ..\n      - name: Test Changes\n        run: python ci/test_files_touched.py\n  Test_Coverage:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v1\n      - uses: actions/setup-python@v1\n        with:\n          python-version: '3.8'\n      - name: Setup\n        run: |\n          sudo apt-get install -y attr\n          pip install --upgrade pip\n          pip install .\n      - name: Test Coverage\n        run: |\n          pip install coverage\n          pip install .\n          coverage run -m unittest discover -s tests\n          coverage report\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": ".github/workflows/pull_request.yml", "language": "yaml", "complexity": "medium", "star_count": 993, "quality_score": 0.56, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 89}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n# Copyright ¬© 2020, VMware, Inc.  All rights reserved.\n# SPDX-License-Identifier: BSD-2-Clause\n\nstrictness: medium\ntest-warnings: false\n\npycodestyle:\n  full: true\n  disable:\n    - N802 # pep8-naming: function name should be lower case\n\npydocstyle:\n  run: false\n\npylint:\n  disable:\n    - consider-using-f-string\n    - global-variable-not-assigned\n    - consider-iterating-dictionary\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": ".prospector.yaml", "language": "yaml", "complexity": "simple", "star_count": 993, "quality_score": 0.5, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 21}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"Tern\",\n      \"type\": \"python\",\n      \"preLaunchTask\": \"build\",\n      \"request\": \"launch\",\n      \"program\": \"tern\",\n      \"args\": [\n        \"--driver\",\n        \"fuse\",\n        \"--clear-cache\",\n        \"report\",\n        \"--docker-image\",\n        \"ubuntu:latest\"\n      ],\n      \"cwd\": \"${workspaceFolder}\",\n      \"env\": { \"PYTHONPATH\": \"${cwd}\" }\n    }\n  ]\n}\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": ".vscode/launch.json", "language": "json", "complexity": "simple", "star_count": 993, "quality_score": 0.53, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 23}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of experience,\neducation, socio-economic status, nationality, personal appearance, race,\nreligion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by sending an email to nishak@vmware.com and/or tpepper@vmware.com. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": "CODE_OF_CONDUCT.md", "language": "markdown", "complexity": "simple", "star_count": 993, "quality_score": 0.53, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 75}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributing to the Tern project\n\nOnce again, we hope you have read our [code of conduct](/CODE_OF_CONDUCT.md) before starting.\n\nWe are excited to work with anyone at any level contributing their time and effort towards the project.\n\nYou can contribute in the following ways:\n\n**Start a Discussion**\n\n- Ask questions about the project goals, features, a specific use case, or clarification on documentation. Asking questions lead to issues being filed, eventually making the project more usable.\n\n**File Issues**\n\n- Run the code against any Docker image you are using, or any of the tests and submit bug reports when you find them.\n\n**Contribute Documentation**\n\n- Improve the documentation, project structure and workflow by creating a proposal.\n- Contribute changes to documentation by [submitting pull requests](#submit-pr) to it.\n\n**Contribute Code**\n\n- [Resolve Issues](https://github.com/tern-tools/tern/issues)\n- Improve the robustness of the project by:\n  - [Adding to the Command Library](docs/adding-to-command-library.md)\n  - [Adding a Custom Report Format](docs/creating-custom-templates.md)\n  - [Adding an Extension](docs/creating-tool-extensions.md)\n\n## Am I Qualified to Contribute?\n\nThe short answer is 'yes'!\n\nThe long answer is that there are some basic requirements that will help you contribute to any project on GitHub and this one in particular. Leveling up requires some extra knowledge:\n\n**Beginner**\n\n- English (not necessarily fluent but workable)\n- The Python 3 Programming Language (Python is a very friendly community with plenty of tutorials you can search for)\n- The Git version control system. Follow GitHub's setup guide [here](https://help.github.com/articles/set-up-git/). You can also practice submitting a pull request with no code [here](https://github.com/nishakm/puns).\n\n**Intermediate**\n\n- YAML (Tern uses yaml files heavily for data lookup. An overview of YAML can be found [here](http://yaml.org/))\n- Object Oriented Programming in Python\n- Python virtual environments\n- Unix Command Line and shell scripts\n- Linux System Administration\n- [Docker Containers](https://docs.docker.com/)\n- [Vagrant](https://www.vagrantup.com/docs/index.html)\n\n**Advanced**\n\n- [Container Images](https://docs.docker.com/v17.09/engine/userguide/storagedriver/imagesandcontainers/)\n- [OverlayFS](https://wiki.archlinux.org/index.php/Overlay_filesystem)\n- Advanced Python knowledge\n- Linux Kernel knowledge would be nice\n\n## Communicating through GitHub Issues\n\nThe fastest way you can get our attention is by filing an issue. We have some templates that help with organizing the information but if you don't think any of them are suitable, just file a regular GitHub issue. All issue templates are located in the .github/ISSUE_TEMPLATE folder in your local clone if you want to take a look at it before filing an issue.\n\nYou can:\n\n- Ask a question\n- Submit a proposal\n- Submit a feature request\n- File a bug report\n- File a regular GitHub issue\n\nPlease do not submit a pull request without filing an issue. We will ask you to file one and refer to it in your commit message before we merge it.\n\n**A note about maintainer's time**\nMaintainers are people with lives of their own. The typical time to get a response to a question or a review on a PR is 3 business days but we take chunks of time off for other things as well so it could take longer.\n\n**Project Maintainers**\n\n- @nishakm\n- @rnjudge\n\n## Other Communication Channels\n\n### Public forum\n\nYou can post a topic on the [public forum](https://groups.google.com/forum/#!forum/tern-discussion). You will need to apply to join the group before posting.\n\n### Slack channel\n\nIf you would like to chat live, you can join the Slack channel. If you don't have an @vmware.com or @emc.com email, please [follow this link](https://code.vmware.com/join), enter your email address and click \"Request Invite\". If you are already a member of the slack channel, or if you have an @vmware.com or @emc.com email, you can simply [sign-in to the Slack channel](https://vmwarecode.slack.com/messages/tern).\n\n## How Do I Start Contributing?\n\nLook for issues without the label `assigned` on it. This indicates that someone is already working on it. Some labels that may be of interest are listed here:\n\n- docs: Documentation changes\n- good-first-issue: A good issue to start off with if this is your first time contributing to this project\n- first-timers-only: A good issue to start off with if this is your first time contributing to any GitHub project\n- tests: An issue regarding testing the project code\n- tools: An issue regarding tools for developers or contributors\n- bug: A bug in code functionality\n- arch: An issue requiring some architectural change\n\nTo start, comment on the issue asking if you can work on it. A maintainer will tag your username to the issue and apply the `assigned` label on it.\n\n## Setting up a Development Environment\n\nFor hassle-free code contribution, follow the steps below. NOTE: I mostly work in a Linux environment using a basic text editor (vim). Your set-up could be different. If it is, please feel free to submit an issue regarding your development environment and a pull request to add to this documentation.\n\n### Visual Studio Code Development Container\n\nDeveloping using Visual Studio Code development container lets you use a Docker container as a development environment. The Tern repository contains a definition for a pre-configured Docker container to simplify the setup process. To use it you need to download the following software on your host machine:\n\n- [Visual Studio Code](https://code.visualstudio.com/)\n- [Docker](https://www.docker.com/get-started)\n- [The Visual Studio Code Remote - Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)\n\nOnce you have all that installed you can setup your dev enviornment:\n\n1. Fork the main Tern repository using your GitHub account. Once forked, clone *your* fork (not the main Tern repo) on your local desktop.\n2. Using the terminal, `cd` into the forked Tern directory on your local desktop and setup your git credentials:\n```\n    git config --global user.name \"Your Name\"\n    git config --global user.email \"your.email@address.com\"\n```\n3. Open Visual Studio Code. Click the `><` icon in the lower lefthand corner. A search bar will come up at the top of the window. Type \"Remote-Containers: Open Folder in Container\" and click on it. In the window that pops up, navigate to the cloned `tern` folder on you desktop and click \"Open\". If you do not see the \"Remote-Containers: Open Folder in Container\" in your search bar, try clicking the `gear` icon in the lower lefthand corner and select \"Command Palette\", or use the `F1` key on your keyboard to go directly to it and search this way.\n4. On the left-most side panel, click on the icon that looks like four squares with one detaching. This is the \"Extensions\" menu. A search bar will appear. Use this to install the Docker Extension. It might be suggested, but if not, you can type \"Docker\" to search for it.\n5. If you are working on a Mac, there is a reported [bug](https://github.com/microsoft/vscode-remote-release/issues/3149) and [workaround](https://github.com/microsoft/vscode-remote-release/issues/3149#issuecomment-646588532) when using the Docker extension with a remote container. To change the Docker path once the Docker extension is installed, click on the `><` icon, type \"Settings\", click on \"Remote-Containers: Settings\" and update the `Remote > Containers: Docker Path` section to be `/usr/local/bin/docker`.\n6. In the top menu bar, click on \"Terminal\" --> \"New Terminal\". You should see a terminal open at the bottom and if you type `ls` you should see the files in the Tern repository.\n7. Run `pip3 install -e .[dev]` to install Tern and its dependencies.\n8. If you are running on Mac or Windows, test that you can run Tern in your dev container by running: `python -m tern --driver fuse report -i ubuntu:latest`. You should see an SBOM report printed to the console for the `debian:buster` container image.\n9. To setup your branch for long-term contributions, follow the commands in step five under the \"Setting up a Long Term Development Environment\" section.\n10. Create a new branch by running `git checkout -b <newbranch-name>`. You're now ready to start making changes to Tern, which you can do via the Terminal or using GUI editor provided by VS Code. Note that you will need to commit your changes with git using the Terminal command line.\n\n\n### Setting up a Long Term Development Environment\n\nYou may have already cloned the project and started working on it. If you're reading this after the fact, I would highly recommend you save your work and set up a new development environment in this way.\n\n1. Set up a Python virtual environment that has either the Python 3.6 or Python 3.7 executable. See [here](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments) for instructions on how to set this up using your host machine's Python3. There are also a guide to [managing virtual environments using pipenv](https://docs.python-guide.org/dev/virtualenvs/) but I haven't used it. Once done, you should have a folder created. Change to that folder.\n2. Clone *your fork* of Tern in the virtual environment folder.\n3. Activate your virtual environment `source bin/activate`. NOTE: This specific activate script only works for Bash and Mac ZSH shells. If you need to activate a Fish Shell or C Shell you should use `source/bin/activate.fish` or `source/bin/activate.csh`, respectively.\n4. Change directory into the Tern clone.\n5. Next, set up your project for long-term open source contribution, run these commands:\n\n    **Note**: the following command will only work if you have a functioning ssh-key setup with GitHub\n    ```\n    $ git remote add upstream git@github.com:tern-tools/tern.git\n    ```\n\n    If you are not working with ssh keys, use the HTTPS link instead of running the above command:\n    ```\n    $ git remote add upstream https://github.com/tern-tools/tern.git\n    ```\n\n    In either case, continue with the following commands:\n    ```\n    $ git fetch upstream\n    $ git checkout -b up upstream/main\n    $ git push origin up:refs/heads/main\n    ```\n\n    Your `up` branch can now be used to keep track of changes on Tern's `main` branch and your other dev branches can stay separate.\n\n6. Create a development branch. Assuming you are still on the `up` branch we just created (to verify, run `git branch` and make sure the `*` is next to `up`), run: `git checkout -b <new-dev-branch-name>`. If you are not currently on the `up` branch, first run `git checkout up` before creating your new dev branch.\n7. Run `pip install wheel`. This is needed because some dependencies for development fail to build python wheels.\n8. Run `pip install -e.[dev]`. This will install tern in development mode. This will install the project dependencies and [Prospector](https://github.com/PyCQA/prospector) which is a tool to check for style and linting errors. Every time you make functional changes to Tern, re-run this command to pick up the changes for local testing.\n9. For more git tutorials, follow [these examples](https://github.com/nishakm/puns) or ask one of the maintainers for help :) \n\n### Setting up a development environment on Mac and Windows\n\n1. [Install](https://github.com/tern-tools/tern#install) VirtualBox and Vagrant for Mac or Windows\n2. Run `git clone https://github.com/tern-tools/tern.git`\n3. Run `cd tern/vagrant`\n4. Run `vagrant up`\n5. Run `vagrant ssh`\n6. Run `python3 -m venv ternenv`\n7. Run `cd ternenv`\n8. Run `source bin/activate`\n9. Run `cp -r /tern .`\n10. Run `cd tern`\n11. Run `pip3 install wheel`\n12. Run `pip3 install -e.[dev]`\nThis will install tern at the tip of master on Windows and Mac.\n\n### After making changes\n\n1. Install your changes in the development virtual environment `pip3 install -e.[dev]`\n2. Run prospector from the project's root directory `prospector .`\n3. Fix any issues prospector brings up.\n4. Run bandit from the project's root directory `bandit -r .`\n5. Fix any issues bandit brings up.\n6. Test your changes.\n\n### Testing you changes\n\nAfter you make the changes just run `tox`. This will run a set tests and inform you if anything is wrong.\n\n### Setting up a development environment for use with extensions\n\nTo learn more about how Tern can be used with external tools as extensions, see [these instructions](https://github.com/tern-tools/tern#extensions).\n\nTern will be able to find the absolute path of the command line tool which will be used as an extension if it is available on the system. For instance, to make Tern work with Scancode, you can install Scancode in your development environment such that it is available when you run `which scancode`.\n\nNote that you do not need to do this if you are using a Python library as an extension.\n\n#### Development with Tern and Scancode\n\nIf you are developing on Tern and Scancode together, you will need to install both packages in your python virtual environment before running Tern. Eg:\n\n```sh\n$ python3 -m venv devenv\n$ cd devenv\n$ git clone git@github.com:tern-tools/tern.git\n$ git clone git@github.com:nexB/scancode-toolkit.git\n$ cd tern\n$ pip install -e.[dev] .\n$ cd ../scancode-toolkit\n$ pip install .\n$ cd ..\n$ tern report -x scancode -i debian:buster\n```\n\n## Coding Style\n\nTern follows general [PEP8](https://www.python.org/dev/peps/pep-0008/) style guidelines. Apart from that, these specific rules apply:\n\n- Indents for .py files are 4 spaces long\n- Indents for .yml files are 2 spaces long\n- Modules are listed as external dependencies, followed by a space, followed by internal dependencies, listed individually and in alphabetical order. For example:\n\n```python\n# external dependencies not part of Tern\n# possibly part of the python package or installed via pip\nimport os\nimport sys\n\n# internal dependencies that are part of Tern\nimport common\nfrom utils import rootfs\n```\n\n- Minimize [cyclomatic complexity](https://en.wikipedia.org/wiki/Cyclomatic_complexity). Most python style checkers also come with McCabe complexity checkers. A good rule of thumb is to limit the number of if-then-else statements to 3 and return once in a module.\n\n## Commit message format\n\nThe commit message of your PR should be able to communicate what problem/opportunity the PR addresses without any reference to forum messages or discussions on slack or IRL. You may make a reference to a github issue number using the github format (eg: Resolves: #1). Here is an overview of what is needed in a good commit message taken from [this blog](https://chris.beams.io/posts/git-commit/)\n\n- Separate subject from body with a blank line\n- Limit the subject line to 50 characters\n- Capitalize the subject line\n- Do not end the subject line with a period\n- Use the imperative mood in the subject line\n- Wrap the body at 72 characters\n- Use the body to explain what and why vs. how\n\nThe commit message should be done with git and for that run the following command in your terminal:\n\n```sh\n$ git commit\n```\n\nOnce this command is executed, you will be able to add the commit message (follow the guidelines given below for this). \nWhen you are done, save the message and push the changes.\n\nIn addition to this, we would like you to sign off on your commit. You can configure git to automatically do this for you when you run 'git commit -s'.\n\n```sh\n$ git config --global user.name \"Some Dev\"\n$ git config --global user.email somedev@example.com\n```\n\nYou should see a footer in your commit message like this:\n\n```sh\nSigned-off-by: Some Dev <somedev@example.com>\n```\n\nPlease use a name you would like to be identified with and an email address that you monitor.\n\nExample:\n\n```diff\nSummarize changes in around 50 characters or less\n\nMore detailed explanatory text, if necessary. Wrap it to about 72\ncharacters or so. In some contexts, the first line is treated as the\nsubject of the commit and the rest of the text as the body. The\nblank line separating the summary from the body is critical (unless\nyou omit the body entirely); various tools like `log`, `shortlog`\nand `rebase` can get confused if you run the two together.\n\nExplain the problem that this commit is solving. Focus on why you\nare making this change as opposed to how (the code explains that).\nAre there side effects or other unintuitive consequences of this\nchange? Here's the place to explain them.\n\nFurther paragraphs come after blank lines.\n\n - Bullet points are okay, too\n\n - Typically a hyphen or asterisk is used for the bullet, preceded\n   by a single space, with blank lines in between, but conventions\n   vary here\n\nIf you use an issue tracker, put references to them at the bottom,\nlike this:\n\nResolves: #123\nSee also: #456, #789\n\nSigned-off-by: Some Dev <somedev@example.com>\n```\n\nTo make any changes to your existing git commit message, make sure you are on your development branch and run the following command in your terminal:\n\n```sh\n$ git commit --amend\n```\n\nOnce you are done editing your commit message, save it like you did previously.\n\n\n## Learn about Tern\n\n- [FAQ](/docs/faq.md)\n- [Glossary](/docs/glossary.md)\n- [Architecture](/docs/architecture.md)\n- [Navigating the Code](/docs/navigating-the-code.md)\n\n## Troubleshooting\n\n- Unable to find module 'yaml': make sure to activate the python virtualenv first and then run `pip install .`\n- Cannot find bdist_wheel: This usually happens when there is a mismatch between the python version and the virtualenv version. Make sure the symlinks you are using point to the right versions in `/usr/bin/python`.\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "complex", "star_count": 993, "quality_score": 0.59, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 334}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Tern Governance\n\nThis document defines the project governance for Tern.\n\n## Overview\n\n**Tern**, an open source project, is committed to building an open, inclusive, productive and self-governing open source community focused on building a high-quality and high performance container inspection tool. The community is governed by this document with the goal of defining how community should work together to achieve this goal.\n\n## Community Roles\n\n* **Users:** Members that engage with the Tern community via any medium (Slack, GitHub, mailing lists, etc.).\n* **Contributors:** Regular contributions to projects (documentation, code reviews, responding to issues, participation in proposal discussions, contributing code, etc.). \n* **Maintainers**: The Tern project leaders. They are responsible for the overall health and direction of the project; final reviewers of PRs and responsible for releases. Maintainers are expected to contribute code and documentation, review PRs including ensuring quality of code, triage issues, proactively fix bugs, and perform maintenance tasks for Tern.\n\n### Maintainers\n\nNew maintainers must be nominated by an existing maintainer and must be elected by a supermajority of existing maintainers. Likewise, maintainers can be removed by a supermajority of the existing maintainers or can resign by notifying one of the other maintainers.\n\n### Decision Making\n\nIdeally, all project decisions are resolved by consensus. If impossible, any maintainer may call a vote. Unless otherwise specified in this document, any vote will be decided by a supermajority of maintainers.\n\n## Proposal Process\n\nOne of the most important aspects in any open source community is the concept of proposals. Large changes to the codebase and / or new features should be preceded by a proposal in the Tern repo via issue. This process allows for all members of the community to weigh in on the concept (including the technical details), share their comments and ideas, and offer to help. It also ensures that members are not duplicating work or inadvertently stepping on toes by making large conflicting changes.\n\nThe project roadmap is defined by accepted proposals.\n\nProposals should cover the high-level objectives, use cases, and technical recommendations on how to implement. In general, the community member(s) interested in implementing the proposal should be either deeply engaged in the proposal process or be an author of the proposal.\n\n\n## Lazy Consensus\n\nTo maintain velocity in the Tern project, the concept of [Lazy Consensus](http://en.osswiki.info/concepts/lazy_consensus) is practiced. Lazy Consensus is practiced for all projects and decisions in the `tern-tools` org.\n\nLazy consensus does _not_ apply to the process of:\n* Removal of maintainers from Tern\n* Governance changes\n* Adding new maintainers to Tern\n\nFor the above mentioned processes, consensus between @rnjudge and @nishakm is required until there are enough maintainers for a supermajority vote.\n\n## Updating Governance\n\nAll substantive changes in Governance require a supermajority agreement by all maintainers.\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": "GOVERNANCE.md", "language": "markdown", "complexity": "simple", "star_count": 993, "quality_score": 0.53, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 46}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "![Tern](/docs/img/tern_logo.png)\n\n[![Pull Request Lint and Test](https://github.com/tern-tools/tern/actions/workflows/pull_request.yml/badge.svg)](https://github.com/tern-tools/tern/actions/workflows/pull_request.yml)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/2689/badge)](https://bestpractices.coreinfrastructure.org/projects/2689)\n[![License](https://img.shields.io/badge/License-BSD%202--Clause-orange.svg)](https://opensource.org/licenses/BSD-2-Clause)\n\n# Welcome to the Tern Project\n\nTern is a software package inspection tool that can create a Software Bill of Materials (SBOM) for containers. It's written in Python3 with a smattering of shell scripts.\n\n# Table of Contents\n- [Introduction](#what-is-tern)\n  - [FAQ](/docs/faq.md)\n  - [Glossary of Terms](/docs/glossary.md)\n  - [Architecture](/docs/architecture.md)\n  - [Navigating the Code](/docs/navigating-the-code.md)\n  - [Data Model](/docs/data-model.md)\n- [Getting Started](#getting-started)\n  - [GitHub Action](#github-action)\n  - [Getting Started on Linux](#getting-started-on-linux)\n  - [Getting Started with Docker](#getting-started-with-docker)\n  - [Creating a Kubernetes Job](#k8s-job)\n  - [Getting Started with Vagrant](#getting-started-with-vagrant)\n- [Using Tern](#using-tern)\n  - [Generating an SBOM report for a Docker image](#sbom-for-docker-image)\n  - [Generating an SBOM report from a Dockerfile](#sbom-for-dockerfile)\n  - [Generating a locked Dockerfile](#dockerfile-lock)\n- [Report Formats](#report-formats)\n  - [Understanding the Reports](#understanding-the-reports)\n  - [Human Readable Format](#report-human-readable)\n  - [JSON Format](#report-json)\n  - [HTML Format](#report-html)\n  - [YAML Format](#report-yaml)\n  - [SPDX tag-value Format](#report-spdxtagvalue)\n  - [SPDX JSON Format](#report-spdxjson)\n  - [CycloneDX JSON Format](#report-cyclonedxjson)\n- [Extensions](#extensions)\n  - [Scancode](#scancode)\n  - [cve-bin-tool](#cve-bin-tool)\n- [Running tests](#running-tests)\n- [Project Status](#project-status)\n- [Contributing](/CONTRIBUTING.md)\n  - [Code of Conduct](/CODE_OF_CONDUCT.md)\n  - [Creating Report Formats](/docs/creating-custom-templates.md)\n  - [Creating Tool Extensions](/docs/creating-tool-extensions.md)\n  - [Adding to the Command Library](/docs/adding-to-command-library.md)\n\n# What is Tern?<a name=\"what-is-tern\">\nTern is an inspection tool to find the metadata of the packages installed in a container image. The overall operation looks like this:\n1. It analyzes the first layer of the container image to collect information like distro type, package format, and package managers.\n2. It then executes scripts from the \"command library\" in a chroot environment to collect information about packages installed in that layer.\n3. With that information as a starting point, it continues to analyze the subsequent layers in the container image.\n4. Once done, it generates a report of packages with their metadata. Several formats are available. The report, in its default format, provides a layer by layer, explanation of the various software components imported. If a Dockerfile is provided, the report indicates the Dockerfile lines corresponding to each of the file system layers.\n\nTern gives you a deeper understanding of your container's bill of materials so you can make better decisions about your container based infrastructure, integration and deployment strategies. It's also a good tool if you are curious about the contents of the container images you have built.\n\n![Tern quick demo](/docs/img/tern_demo_fast.gif)\n\n# Getting Started<a name=\"getting-started\"/>\n\n## GitHub Action<a name=\"github-action\"/>\nA [GitHub Action](https://github.com/marketplace/actions/tern-action) is available if you just want to scan Docker container images to find the Base OS and packages installed. Please contribute changes [here](https://github.com/philips-labs/tern-action). Thanks to Jeroen Knoops @JeroenKnoops for their work on this.\n\n## Getting Started on Linux<a name=\"getting-started-on-linux\">\nIf you have a Linux OS you will need a distro with a kernel version >= 4.0 (Ubuntu 16.04 or newer or Fedora 25 or newer are good selections) and will need to install the following requirements:\n\n- Git (Installation instructions can be found here: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\n- attr (sudo apt-get install attr or sudo dnf install attr)\n- Python 3.6 or newer (sudo apt-get install python3.6(3.7) or sudo dnf install python36(37))\n- Pip (sudo apt-get install python3-pip).\n- jq (sudo apt-get install jq or sudo dnf install jq)\n- skopeo (See [here](https://github.com/containers/skopeo/blob/main/install.md) for installation instructions or building from source)\n\nSome distro versions have all of these except `attr`, `jq`, and/or `skopeo` preinstalled. `attr` and `jq` are common utilities and are available via the package manager. `skopeo` has only recently been packaged for common Linux distros. If you don't see your distro in the list, your best bet is building from source, which is reasonably straightforward if you have Go installed.\n\nFor analyzing Dockerfiles and to use the \"lock\" function\n- Docker CE (Installation instructions can be found here: https://docs.docker.com/engine/installation/#server)\n\n*NOTE:* We do not provide advice on the usage of [Docker Desktop](https://www.docker.com/blog/updating-product-subscriptions/)\n\nOnce installed, make sure the docker daemon is running.\n\nCreate a python3 virtual environment:\n```\npython3 -m venv ternenv\ncd ternenv\n```\n\n*NOTE:* Your OS might distribute each Python version separately. For example, on Ubuntu LTS, Python 2.7 is linked to `python2` and Python 3.6 is linked to `python3`. I develop with Python 3.7 which is installed separately with no symlinks. In this case, I use the binary. The binaries are usually installed in `/usr/bin/python`.\n\nActivate the virtual environment:\n```\nsource bin/activate\n```\nNOTE: This specific activate script only works for Bash shells. If you need to activate a Fish Shell or C Shell you should use `source bin/activate.fish` or `source bin/activate.csh`, respectively.\n\nInstall tern:\n```\npip install tern\n```\n\nRun Tern:\n```\ntern report -o output.txt -i debian:buster\n```\n\n## Getting Started with Docker<a name=\"getting-started-with-docker\">\nDocker is the most widely used tool to build and run containers. If you already have Docker installed, you can run Tern by building a container with the Dockerfile provided.\n\nClone this repository:\n```\ngit clone https://github.com/tern-tools/tern.git\n```\n\nBuild the Docker image (called `ternd` here). You may need to use sudo:\n```\ndocker build -f docker/Dockerfile -t ternd .\n```\n\nThis will install the latest release of tern using pip.\n\nIf you want to build a Docker image containing the latest changes to tern, run:\n```\npython setup.py sdist\ndocker build -f ci/Dockerfile -t ternd .\n```\n\n**NOTE**: By default, Tern will run with logging turned on. If you would like to silent the terminal output when running the ternd container, make the following change to the Dockerfile ENTRYPOINT before building:\n\n```\n--- a/Dockerfile\n+++ b/Dockerfile\n-ENTRYPOINT [\"tern\"]\n+ENTRYPOINT [\"tern\", \"-q\"]\n```\n\nRun the ternd container image\n\n```\ndocker run --rm ternd report -i debian:buster\n```\n\nIf you are using this container to analyze Dockerfiles and to use the \"lock\" feature, then you must volume mount the docker socket. We have a convenience script which will do that for you. \n\n```\n./docker_run.sh ternd \"report -i debian:buster\" > output.txt\n```\n\nTo produce a json report run\n```\n./docker_run.sh ternd \"report -f json -i debian:buster\"\n```\n\nTern is not distributed as Docker images yet. This is coming soon. Watch the [Project Status](#project-status) for updates.\n\n**WARNING**: If using the `--driver fuse` or `--driver overlay2` storage driver options, then the docker image needs to run as privileged.\n\n```\ndocker run --privileged -v /var/run/docker.sock:/var/run/docker.sock ternd --driver fuse report -i debian:buster\n```\n\nYou can make this change to the `docker_run.sh` script to make it easier.\n\n## Creating a Kubernetes Job<a name=\"k8s-job\">\nA Tern container can be deployed on Kubernetes as a Job. However, a host mount is required to retrieve the reports. We will describe below how to create a Kubernetes Job within minikube.\n\nTo install minikube, follow [these instructions](https://minikube.sigs.k8s.io/docs/start/). If using a virtual machine manager, make sure it [supports volume mounts](https://minikube.sigs.k8s.io/docs/handbook/mount/#driver-mounts). We will be using VirtualBox in this example.\n\nDownload the existing Tern Dockerfile\n```\nwget https://raw.githubusercontent.com/tern-tools/tern/main/docker/Dockerfile\n```\n\nStart minikube\n```\nminikube start --driver=virtualbox\n```\n\nUse minikube to build the Tern container image\n```\nminikube image build -t tern:test -f Dockerfile .\n```\n\nOnce build has completed, you should see the image by running `minikube image ls`. It should look something like `docker.io/library/tern:test`.\n\nWe are now ready to create a Job. You can modify the following YAML according to your host's filesystem:\n\n```\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tern\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n      - image: docker.io/library/tern:test\n\t# in order run the job for other containers, replace the \"-i\" argument here\n        command: [\"tern\", \"report\", \"-i\", \"docker.io/library/debian:buster\", \"-o\", \"/host/report.txt\"]\n        name: tern-example\n        volumeMounts:\n          - name: host-mount\n            mountPath: /host # this path exists in the pod\n      volumes:\n      - name: host-mount # create a corresponding directory on the host\n        hostPath:\n          path: /path/to/tern/reports # this path must exist on the host\n```\n\nWe can now deploy Tern on Kubernetes\n```\nminikube kubectl -- apply -f tern-example.yaml\n```\n\nTo check the status of the Job, you can run `minikube kubectl -- describe job.batch/tern`. You should be able to see `report.txt` in `/path/to/tern/reports/`.\n\n## Getting Started with Vagrant<a name=\"getting-started-with-vagrant\">\nVagrant is a tool to setup an isolated virtual software development environment. If you are using Windows or Mac OSes and want to run Tern from the command line (not in a Docker container) this is the best way to get started as Tern does not run natively in a Mac OS or Windows environment at this time.\n\n### Install\nFollow the instructions on the [VirtualBox](https://www.virtualbox.org/wiki/Downloads) website to download VirtualBox on your OS.\n\nFollow the instructions on the website to install [Vagrant](https://www.vagrantup.com/downloads.html) for your OS. \n\n### Create a Vagrant environment\n**NOTE**: The following steps will install the latest [PyPI release](https://pypi.org/project/tern/#history) version of Tern. If you want to install Tern from the tip of master, please instead follow \"Setting up a development environment on Mac and Windows\" in the [contributing guide](/CONTRIBUTING.md).\n\nIn your terminal app, run the following commands. \n\nClone this repository:\n```\ngit clone https://github.com/tern-tools/tern.git\n```\n\nBring up the Vagrant box: \n```\ncd tern/vagrant\nvagrant up\n```\n\nSSH into the created VM: \n```\nvagrant ssh\n```\n\nRun:\n```\ntern report -i debian:buster -o output.txt\n```\n\n# Using Tern<a name=\"using-tern\">\n\n*WARNING*: The CLI has changed since the last release. Visit [Tern's PyPI project page](https://pypi.org/project/tern/) to find the correct CLI options or just run `tern -h`.\n\nTern creates a report containing the Software Bill of Materials (SBOM) of a container image, including notes about how it collects this information, and files for which it has no information about. Currently, Tern supports containers only built using Docker using image manifest version 2, schema 2. Docker image manifest version 2, schema 1 has been [deprecated](https://docs.docker.com/registry/spec/deprecated-schema-v1/) by Docker. Tern will support container images created using Docker version 19.03.0 or later. Docker is the most ubiquitous type of container image that exists so the project started with a focus on those. However, it is architected to support other images that closely follow the [OCI image spec](https://github.com/opencontainers/image-spec/blob/master/spec.md).\n\n## Generating an SBOM report for a Docker image<a name=\"sbom-for-docker-image\">\nIf you have a Docker image pulled locally and want to inspect it\n```\ntern report -i debian:jessie\n```\nThe SBOM of packages that are installed in the Docker image and how Tern got this information will be printed to the console. To direct this output to a file, use the `-o file_name` command line option. If you encounter any errors, please file an issue.\n\n## Generating an SBOM report from a Dockerfile<a name=\"sbom-for-dockerfile\">\nYou can provide a Dockerfile to Tern to figure out the Software Bill of Materials and other information. Tern will build the image, analyze it with respect to the Dockerfile and discard the image. This is useful to engineers who are developing a Dockerfile for their app or in a container build and release pipeline.\n```\ntern report -d samples/photon_git/Dockerfile\n```\nThe SBOM of packages you would be shipping if you were to use the given Dockerfile will print to the console. To direct the output to a file, use the `-o file_name` command line option. Feel free to try this out on the other sample Dockerfiles in the samples directory or on Dockerfiles you may be working with. If it doesn't work for you, please file an issue.\n\n## Generating a locked Dockerfile<a name=\"dockerfile-lock\">\nBecause of the way Docker builds containers, Dockerfiles are generally not declarative or reflective of what ultimately gets included in the container image that gets produced. Pinning information in your Dockerfile (base OS, packages, etc.) can help create more reproducible container images should your Dockerfile be distributed to other parties. If you have a Dockerfile that you would like to lock to a more reproducible version, Tern can help.\n```\ntern lock Dockerfile\n```\nThe locked Dockerfile will be created in `Dockerfile.lock` unless an output file is otherwise specified. To specify an output file\n```\ntern lock Dockerfile -o output.txt\n```\nIf the packages are not pinned in the resulting `Dockerfile.lock` or output file that gets produced, it is because 1) Tern does not know the version of the packages to pin (i.e. unable to get this information from the package manager) or 2) your Dockerfile failed to build. In the case of a failed Dockerfile build, Tern only builds the base image and tries to pin what it can. If you encounter any errors, please file an issue.\n\n# Report Formats<a name=\"report-formats\">\nTern creates SBOM reports suitable to read over or to provide to another tool for consumption. A collection of sample reports is available to view [here](./docs/examples/). \n\n## Understanding the Reports<a name=\"understanding-the-reports\">\nTern provides a handful of different reporting styles that may work better for different applications of distribution, interoperability and comprehension. Understanding these reports will vary slightly between formats, but the information in the different report formats will generally be the same with varying degrees of package metadata detail. In all report formats, information about the version of Tern that generated the report and any applicable extension information will be at the top of the report followed by information about the metadata found in the container, organized sequentially by layer. \n\nThe base layer (Layer 1) will provide operating system information on which the container is based, the Dockerfile command that created the layer, the package retrieval method and any packages found in the layer. Note that the operating system information may be different than the container that Tern is generating an SBOM for. For example, the `golang` container's base OS is actually `Debian GNU/Linux 10 (buster)`. For each subsequent layer in the container, information about the Dockerfile command that created the container layer, any warnings about unrecognized Dockerfile commands, the package retrieval method and package information is provided. If Tern doesn't find any package information in a layer, it will report packages found in the layer as \"None\". File licenses may also be available in the reports if Tern is run using scancode.\n\nMore information about specific reporting formats can be found below and in the `tern/classes` directory where the properties being reported on are explained in the .py files -- specifically, `image_layer.py`, `package.py`, and `file_data.py`.\n\n## Human Readable Format<a name=\"report-human-readable\">\nThe default report Tern produces is a human readable, high-level overview. The object of this report is to give the container developer a deeper understanding of what is installed in a container image during development. This allows a developer to glean basic information about the container such as what the true base operating system is, what the app dependencies are, if the container is using an official or personal repository for sources or binaries, whether the dependencies are at the correct versions, etc. \n\nWhile the packages found in each layer and their associated version and license are listed on a per layer basis, there is also a summary of licenses found in the container printed at the bottom of the report which is unique to the default human readable format.\n```\ntern report -i golang:1.12-alpine -o output.txt\n```\n\n## JSON Format<a name=\"report-json\">\nYou can get the results in a JSON file to pass around in a network. The JSON report contains the most amount of container metadata compared to the default report and because of this, is often a very large file. If you are planning to look for information in this file manually, we recommend using the `jq` utility to better display and understand the information in the report.\n\nIn terms of general container information, the JSON report provides detailed \"created by\" information including docker container config information, layer `created_by` information and layer creation time stamps. It also provides the `diff_id` and tar file information for each layer, including each layer's unique package set and the packages metadata. The JSON report will also provide more detailed package metadata (if found) including the project URL information, files found in each package when run with scancode and package licenses (`pkg_licenses`) for containers based on Debian OSes where license information is parsed from Copyright text instead of declared by the package manager (`pkg_license`).\n```\ntern report -f json -i golang:1.12-alpine\n```\n\n## HTML Format<a name=\"report-html\">\nYou can get an html rendering of the JSON results. An output file with `.html` suffix should be provided in order to properly view the report in your browser. The HTML report will include all of the same information found in a JSON report. See above for details about the JSON report.\n```\ntern report -f html -i golang:1.12-alpine -o report.html\n```\n\n## YAML Format<a name=\"report-yaml\">\nYou can get the results in a YAML file to be consumed by a downstream tool or script. The YAML information will be the same information found in the JSON report. See above for details about the JSON report.\n```\ntern report -f yaml -i golang:1.12-alpine -o output.yaml\n```\n\n## SPDX tag-value Format<a name=\"report-spdxtagvalue\">\n[SPDX](https://spdx.org/) is a format developed by the Linux Foundation to provide a standard way of reporting license information. It is now an [ISO standard](https://www.iso.org/standard/81870.html). The National Telecommunications and Information Administration (NTIA) [recognizes SPDX](https://www.ntia.gov/files/ntia/publications/sbom_options_and_decision_points_20210427-1.pdf) as one of three valid SBOM formats that satisfies the minimum viable requirements for an SBOM in accordance with President Biden's [Executive Order on Improving the Nation's Cybersecurity](https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/).\n\nMany compliance tools are compatible with SPDX. Tern follows the [SPDX specifications](https://spdx.org/specifications). The tag-value format is most compatible with the toolkit the organization provides. There are conversion tools available [here](https://github.com/spdx/tools) (some still in development). You can read an overview of the SPDX tag-value specification [here](./docs/spdx-tag-value-overview) and about how Tern maps its properties to the keys mandated by the spec [here](./docs/spdx-tag-value-mapping.md).\n```\ntern report -f spdxtagvalue -i golang:1.12-alpine -o spdx.txt\n```\n\n## SPDX JSON Format<a name=\"report-spdxjson\">\nThe SPDX JSON format contains the same information that an SPDX Tag-value document does. The only difference between these two formats is the way the information is represented. The 'spdxjson' format represents the container information as a collection of key-value pairs. In some cases, the SPDX JSON format may be more interoperable between cloud native compliance tools.\n```\ntern report -f spdxjson -i golang:1.12-alpine -o spdx.json\n```\n\n## CycloneDX JSON Format<a name=\"report-cyclonedxjson\">\n[OWASP CycloneDX](https://cyclonedx.org/) is a lightweight software bill of materials standard designed for use in application security contexts and supply chain component analysis. The National Telecommunications and Information Administration (NTIA) [recognizes CycloneDX](https://www.ntia.gov/files/ntia/publications/sbom_options_and_decision_points_20210427-1.pdf) as one of three valid SBOM formats that satisfies the minimum viable requirements for an SBOM in accordance with President Biden's [Executive Order on Improving the Nation's Cybersecurity](https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/).\n\nMany tools for producing and consuming CycloneDX SBOMs are listed in the [CycloneDX Tool Center](https://cyclonedx.org/tool-center/).\n```\ntern report -f cyclonedxjson -i golang:1.12-alpine -o bom.json\n```\n\n# Extensions<a name=\"extensions\">\nTern does not have its own file level license scanner. In order to fill in the gap, Tern allows you to extend container image analysis with an external file analysis CLI tool or Python3 module. In order to take advantage of the extensions, both the extention tool and Tern need to be installed.\n\nNOTE: Neither the Docker container nor the Vagrant image has any of the extensions installed. You are welcomed to modify `Dockerfile` and `vagrant/bootstrap.sh` to install the extensions if you wish to use them. Please see the instructions below on how to enable the extension of your choice.\n\n## Scancode<a name=\"scancode\">\n[scancode-toolkit](https://github.com/nexB/scancode-toolkit) is a license analysis tool that \"detects licenses, copyrights, package manifests and direct dependencies and more both in source code and binary files\". Note that Scancode currently works on Python 3.6 to 3.9. Be sure to check what python version you are using below.\n\n**NOTE** Installation issues have been [reported](https://github.com/nexB/scancode-toolkit/issues/3205) on macOS on M1 and Linux on ARM for Scancode>=31.0.0. If you are wanting to run Tern + Scancode in either of these environments, you will need to install `scancode-toolkit-mini`.\n\n1. Install system dependencies for Scancode (refer to the [Scancode GitHub repo](https://github.com/nexB/scancode-toolkit) for instructions)\n\n2. Setup a python virtual environment\n```\npython3 -m venv scanenv\ncd scanenv\nsource bin/activate\n```\n3. Install tern and scancode\n```\npip install tern scancode-toolkit\n```\n<br> If you are using macOS on M1 or Linux on ARM, run:</br>\n```\npip install tern scancode-toolkit-mini\n```\n4. Run tern with scancode\n```\ntern report -x scancode -i golang:1.12-alpine\n```\n\nIf you are running Scancode for the first time, depending on the size of the container image, it takes anywhere between 10 minutes to a few hours to run due to the number of files needed to be analyzed. Once completed, subsequent runs will be much faster as the data will be cached for future use.\n\n## cve-bin-tool<a name=\"cve-bin-tool\">\n[cve-bin-tool](https://github.com/intel/cve-bin-tool) is a command line tool which \"scans for a number of common, vulnerable components (openssl, libpng, libxml2, expat and a few others) to let you know if your system includes common libraries with known vulnerabilities\". Vulnerability scanning tools can also be extended to work on containers using Tern, although support for certain metadata pertaining to CVEs may not be available yet. As a result, you will not see any of the results in the generated reports.\n\n1. Install system dependencies for cve-bin-tool (refer to the [cve-bin-tool GitHub repo](https://github.com/intel/cve-bin-tool) for instructions)\n\n2. Setup a python virtual environment\n```\npython3 -m venv scanenv\ncd scanenv\nsource bin/activate\n```\n3. Install tern and cve-bin-tool\n```\npip install tern cve-bin-tool\n```\n4. Run tern with cve-bin-tool\n```\ntern report -x cve_bin_tool -i golang:1.12-alpine\n```\n \n# Running tests<a name=\"running-tests\">\nWARNING: The `test_util_*` tests are not up to date. We are working on it :). From the Tern repository root directory run:\n```\npython tests/<test file>.py\n```\n\n## Project Status<a name=\"project-status\"/>\nPatched release 2.12.1 is out. See the [release notes](docs/releases/v2_12_1.md) for more information. We try to keep the [project roadmap](./docs/project-roadmap.md) as up to date as possible.\n\n## Recent Past Releases\n* [v2.12.0](docs/releases/v2_12_0.md)\n  * Includes an important security update for GitPython to address [CVE-2022-24439](https://nvd.nist.gov/vuln/detail/CVE-2022-24439)\n* [v2.11.0](docs/releases/v2_11_0.md)\n* [v2.10.1](docs/releases/v2_10_1.md)\n* [v2.9.1](docs/releases/v2_9_1.md)\n* [v2.8.0](docs/releases/v2_8_0.md)\n\n## Documentation\nArchitecture, function blocks, code descriptions and the project roadmap are located in the docs folder. Contributions to the documentation are welcome! See the [contributing guide](/CONTRIBUTING.md) to find out how to submit changes.\n\n## Get Involved\n\nDo you have questions about Tern? Do you think it can do better? Would you like to make it better? You can get involved by giving your feedback and contributing to the code, documentation and conversation!\n\nPlease read our [code of conduct](/CODE_OF_CONDUCT.md) first.\n\nNext, take a look at the [contributing guide](/CONTRIBUTING.md) to find out how you can start.\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 993, "quality_score": 0.62, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 423}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019-2020 VMware, Inc. All Rights Reserved.\n# SPDX-License-Identifier: BSD-2-Clause\n\nfrom git import Repo\nfrom git import GitCommandError\nimport os\nimport sys\n\n# This is meant to run within CI Integration\n# Print out only .py files that have changed\n# Pipe to any linting tools\n# Note that some linting tools will lint everything if the output\n# of this script is nothing\n\nrepo = Repo(os.getcwd())\ntry:\n    repo.git.remote('add', 'upstream',\n                    'https://github.com/tern-tools/tern.git')\nexcept GitCommandError:\n    pass\nrepo.git.fetch('upstream')\n\nhcommit = repo.head.commit\ndiff = hcommit.diff('upstream/main')\n\nif not diff:\n    sys.exit(0)\n\nfor d in diff:\n    if os.path.exists(d.b_path) and (d.b_path)[-3:] == '.py':\n        print(d.b_path)\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": "ci/evaluate_docs.py", "language": "python", "complexity": "simple", "star_count": 993, "quality_score": 0.59, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 34}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-                                                         \n# SPDX-License-Identifier: BSD-2-Clause\n\ntests = [\n    'tern report -i photon:3.0',\n    'python3 setup.py sdist && ' \n    'docker build -t ternd -f ci/Dockerfile . && '\n    './docker_run.sh ternd \"report -i golang:alpine\"',\n    'tern report -i golang:alpine',\n    'python tests/test_class_command.py',\n    'python tests/test_class_file_data.py',\n    'python tests/test_class_image.py',\n    'python tests/test_class_image_layer.py',\n    'python tests/test_class_notice.py',\n    'python tests/test_class_notice_origin.py',\n    'python tests/test_class_origins.py',\n    'python tests/test_class_package.py',\n    'python tests/test_class_template.py',\n    'tern report -f spdxtagvalue -i photon:3.0',\n    'tern lock samples/single_stage_tern/Dockerfile',\n    'tern report -i debian:buster',\n    'tern report -i alpine:3.9',\n    'tern report -i archlinux:latest',\n    'tern report -i centos:7',\n    'tern report -i node:12.16-alpine',\n    'python tests/test_analyze_default_dockerfile_parse.py',\n    'python tests/test_analyze_common.py',\n    'tern report -i golang:alpine',\n    'tern report -d samples/alpine_python/Dockerfile',\n    'tern report -w photon.tar',\n    'python tests/test_load_docker_api.py',\n    'tern report -f yaml -i photon:3.0',\n    'tern report -f json -i photon:3.0',\n    'tern report -f spdxjson -i photon:3.0',\n    'tern report -f html -i photon:3.0',\n    'tern report -f spdxtagvalue -i photon:3.0 -o spdx.spdx && '\n    'java -jar tools-java/target/tools-java-*-jar-with-dependencies.jar '\n    'Verify spdx.spdx', \n    'tern report -f spdxjson -i photon:3.0 -o spdx.json && '\n    'java -jar tools-java/target/tools-java-*-jar-with-dependencies.jar '\n    'Verify spdx.json',\n    'python tests/test_util_general.py',\n    'python tests/test_analyze_default_filter.py',\n    'python tests/test_class_command.py',\n    'python tests/test_class_docker_image.py'\n]\n            ", "metadata": {"source": "github.com/tern-tools/tern", "file_path": "ci/release_tests.py", "language": "python", "complexity": "simple", "star_count": 993, "quality_score": 0.53, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 47}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019-2020 VMware, Inc. All Rights Reserved.\n# SPDX-License-Identifier: BSD-2-Clause\n\nfrom git import Repo\nfrom git import GitCommandError\nimport os\nimport re\nimport sys\n\n\n# This script is written to be used with CI Integration\n\ndef has_url(string):\n    # findall() has been used\n    # with valid conditions for urls in string\n    regex = r\"((?:https?:\\/\\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\\/)\" \\\n            r\"(?:[^\\s()<>]+|\\((?:[^\\s()<>]+|(?:\\([^\\s()<>]+\\)))*\\))+\" \\\n            r\"(?:\\((?:[^\\s()<>]+|(?:\\([^\\s()<>]+\\)))*\\)|\" \\\n            r\"[^\\s`!()\\[\\]{};:'\\\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))\"\n    urls = re.findall(regex, string)\n    return bool(len(urls) > 0)\n\n\ndef lint_commit(commit_id):\n    \"\"\"Given a commit ID, determine if the\n    commit message complies with the tern guidelines. If\n    it does not comply, output the offending commit and\n    specify the reason for failure.\"\"\"\n\n    r = Repo(os.getcwd())\n    check = True\n    message = r.commit(commit_id).message\n    sha_short = r.git.rev_parse(commit_id, short=7)\n\n    # Check 1: Subject, body and DCO exist\n    # Note that git does not allow for empty subjects\n    msg_list = re.split('\\n\\n|\\r', message)\n    commit_subject = msg_list[0]\n    if len(msg_list) <= 2:\n        print(\"Commit {} does not have a body.\".format(sha_short))\n        check = False\n    try:\n        # pop the subject and signature\n        msg_list.pop(0)\n        msg_list.pop()\n    except IndexError:\n        pass\n\n    # Check 2: Subject length is less than about 50\n    if len(commit_subject) > 54:\n        print(\n            \"The subject of commit {} should be 50 characters or less.\".format(\n                sha_short))\n        check = False\n\n    # Check 3: Each line of the body is less than 72\n    for msg in msg_list:\n        for line in msg.split('\\n'):\n            if has_url(line) or line.startswith(\"[CM]\") or line.startswith(\"[LINK]\"):\n                print(\"Line contains url(s)/compiler messages. Skipping . . .\\n\"\n                      \"Line: {0}\\n\"\n                      \"Commit: {1}\\n\\n\".format(line, sha_short))\n                continue\n            if len(line) > 72:\n                print(\"Line exceeds 72 characters.\\n\"\n                      \"Line: {0}\\n\"\n                      \"Commit: {1}\\n\\n\".format(line, sha_short))\n                check = False\n\n    if check:\n        print(\"Commit message checks pass\")\n        sys.exit(0)\n    else:\n        sys.exit(1)\n\n\nif __name__ == '__main__':\n    # Get the list of commits and loop through them,\n    # inputting each one into the linting function\n\n    # We are in the 'tern' directory\n    repo = Repo(os.getcwd())\n    try:\n        repo.git.remote(\n            'add', 'upstream', 'https://github.com/tern-tools/tern.git')\n    except GitCommandError:\n        pass\n    repo.git.fetch('upstream')\n    # Will return commit IDs differentiating HEAD and main\n    commitstr = repo.git.rev_list('HEAD', '^upstream/main', no_merges=True)\n    # If we are on the project's main branch then there will be no\n    # difference and the result will be an empty string\n    # So we will not proceed if there is no difference\n    if commitstr:\n        commits = commitstr.split('\\n')\n        for commit_id in commits:\n            lint_commit(commit_id)\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": "ci/test_commit_message.py", "language": "python", "complexity": "medium", "star_count": 993, "quality_score": 0.65, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 100}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019-2021 VMware, Inc. All Rights Reserved.\n# SPDX-License-Identifier: BSD-2-Clause\n\nfrom git import Repo\nfrom git import GitCommandError\nimport os\nimport re\nimport sys\nimport subprocess  # nosec\n\n\n# This script is written to be used with CI integration\n\nrepo = Repo(os.getcwd())\ntry:\n    repo.git.remote('add', 'upstream',\n                    'https://github.com/tern-tools/tern.git')\nexcept GitCommandError:\n    pass\nrepo.git.fetch('upstream')\n\nhcommit = repo.head.commit\ndiff = hcommit.diff('upstream/main')\n\nchanges = []\nfor d in diff:\n    # Get the list of strings for changed files\n    changes.append(d.b_path)\n\n# check that changes has entries\nif not changes:\n    print('No changes to run tests for.')\n    sys.exit(0)\n\ntest_suite = {\n    # requirements.txt\n    re.compile('requirements.txt'): ['tern report -i photon:3.0'],\n    # Dockerfile\n    re.compile('docker/Dockerfile'): [\n        'python3 setup.py sdist && '\n        'docker build -t ternd -f ci/Dockerfile . && '\n        './docker_run.sh ternd \"report -i golang:alpine\"'],\n    # Files under tern directory\n    re.compile('tern/__init__.py|tern/__main__.py'):\n    ['tern  report -i golang:alpine'],\n    # tern/classes\n    re.compile('tern/classes/command.py'):\n    ['python tests/test_class_command.py'],\n    re.compile('tern/classes/oci_image.py'):\n    ['tern report -i photon:3.0',\n     'python tests/test_class_oci_image.py'],\n    re.compile('tern/classes/docker_image.py'):\n    ['tern report -d samples/alpine_python/Dockerfile',\n     'python tests/test_class_docker_image.py'],\n    re.compile('tern/classes/file_data.py'):\n    ['python tests/test_class_file_data.py'],\n    re.compile('tern/classes/image.py'):\n    ['python tests/test_class_image.py'],\n    re.compile('tern/classes/image_layer.py'):\n    ['python tests/test_class_image_layer.py'],\n    re.compile('tern/classes/notice.py'):\n    ['python tests/test_class_notice.py'],\n    re.compile('tern/classes/notice_origin.py'):\n    ['python tests/test_class_notice_origin.py'],\n    re.compile('tern/classes/origins.py'):\n    ['python tests/test_class_origins.py'],\n    re.compile('tern/classes/package.py'):\n    ['python tests/test_class_package.py'],\n    re.compile('tern/classes/template.py'):\n    ['python tests/test_class_template.py',\n     'tern report -f spdxtagvalue -i photon:3.0',\n     'tern lock samples/single_stage_tern/Dockerfile'],\n    # tern/command_lib\n    re.compile('tern/analyze/default/command_lib'): [\n        'tern report -i photon:3.0',\n        'tern report -i debian:buster',\n        'tern report -i alpine:3.9',\n        'tern report -i archlinux:latest',\n        'tern report -i centos:7',\n        'tern report -i node:12.16-alpine'],\n    # tern/analyze/default/dockerfile\n    re.compile('tern/analyze/default/dockerfile'): [\n        'python tests/test_analyze_default_dockerfile_parse.py',\n        'python tests/test_analyze_common.py',\n        'tern report -i golang:alpine',\n        'tern report -d samples/alpine_python/Dockerfile',\n        'tern report -w photon.tar',\n        'tern lock samples/single_stage_tern/Dockerfile'],\n    # tern/load\n    re.compile('tern/load'): [\n        'python tests/test_load_docker_api.py'],\n    # tern/report\n    re.compile('tern/report'): [\n        'tern report -i golang:alpine',\n        'tern report -f yaml -i photon:3.0',\n        'tern report -f json -i photon:3.0',\n        'tern report -f spdxtagvalue -i photon:3.0',\n        'tern report -f spdxjson -i photon:3.0',\n        'tern report -d samples/alpine_python/Dockerfile',\n        'tern report -f html -i photon:3.0',\n        'tern report -f cyclonedxjson -i photon:3.0'],\n    # tern/formats/spdx\n    re.compile('tern/formats/spdx'): [\n        'tern report -f spdxtagvalue -i photon:3.0 -o spdx.spdx && ' \\\n        'java -jar tools-java/target/tools-java-*-jar-with-dependencies.jar '\\\n        'Verify spdx.spdx',\n        'tern report -f spdxjson -i photon:3.0 -o spdx.json && ' \\\n        'java -jar tools-java/target/tools-java-*-jar-with-dependencies.jar '\\\n        'Verify spdx.json'],\n    # tern/tools\n    re.compile('tern/tools'):\n    ['tern report -i golang:alpine'],\n    # tern/utils\n    re.compile('tern/utils'): [\n        'python tests/test_util_general.py',\n        'tern report -i golang:alpine',\n        'tern report -d samples/alpine_python/Dockerfile'],\n    # tests\n    re.compile('tests/test_analyze_common.py'):\n        ['python tests/test_analyze_common.py'],\n    re.compile('tests/test_analyze_default_filter.py'):\n        ['python tests/test_analyze_default_filter.py'],\n    re.compile('tests/test_analyze_default_dockerfile_parse.py'):\n        ['python tests/test_analyze_default_dockerfile_parse.py'],\n    re.compile('tests/test_class_command.py'):\n        ['python tests/test_class_command.py'],\n    re.compile('tests/test_class_oci_image.py'):\n        ['python tests/test_class_oci_image.py'],\n    re.compile('tests/test_class_docker_image.py'):\n        ['python tests/test_class_docker_image.py',\n         'tern report -w photon.tar'],\n    re.compile('tests/test_class_file_data.py'):\n        ['python tests/test_class_file_data.py'],\n    re.compile('tests/test_class_image.py'):\n        ['python tests/test_class_image.py'],\n    re.compile('tests/test_class_image_layer.py'):\n        ['python tests/test_class_image_layer.py'],\n    re.compile('tests/test_class_notice.py'):\n        ['python tests/test_class_notice.py'],\n    re.compile('tests/test_class_notice_origin.py'):\n        ['python tests/test_class_notice_origin.py'],\n    re.compile('tests/test_class_origins.py'):\n        ['python tests/test_class_origins.py'],\n    re.compile('tests/test_class_package.py'):\n        ['python tests/test_class_package.py'],\n    re.compile('tests/test_class_template.py'):\n        ['python tests/test_class_template.py']}\n\nalltests = []\nfor change in changes:\n    for check, _ in test_suite.items():\n        if check.match(change):\n            alltests.extend(_)\n\n# Remove duplicate tests\ntests = list(set(alltests))\n\nif not tests:\n    print('No tests to run.')\nelse:\n    print('Running the following tests:')\n    for t in tests:\n        print(t)\n    print('-------------------------OUTPUT-------------------------')\n\n# Run the tests -- Precaution will be mad!\nfor t in tests:\n    subprocess.check_output(t, shell=True)  # nosec\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": "ci/test_files_touched.py", "language": "python", "complexity": "complex", "star_count": 993, "quality_score": 0.62, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 171}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/bin/sh\n#\n# Copyright (c) 2019-2021 VMware, Inc. All Rights Reserved.\n# SPDX-License-Identifier: BSD-2-Clause\n#\n# Script to run Tern within a prebuilt Docker container\n# Assume the Tern Docker container exists on the host\n# The script will make a directory that you provide\n# It will then run a docker container in privileged mode and bind mount to the directory\n#\n# Usage: ./docker_run.sh <tern image> <tern command arguments in quotes> > output.txt\n# Example: ./docker_run.sh ternd \"report -i golang:alpine\" > output.txt\n\ndocker run -v /var/run/docker.sock:/var/run/docker.sock --rm $1 $2\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": "docker_run.sh", "language": "bash", "complexity": "simple", "star_count": 993, "quality_score": 0.5, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 15}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Adding a Listing to the Command Library\nThere is no special magic that Tern uses to get all the information about\npackages installed in a Docker image. It just invokes shell scripts within\nor outside a [chroot](https://wiki.archlinux.org/index.php/change_root)\nenvironment - similar to what is done when building root filesystems for linux\ndistributions.\n\nSince its accuracy is heavily dependent on these shell scripts, it is\nimportant to check whether the scripts are producing the the right information\nin the right format.\n\nThe shell scripts are listed in the [Command Library](glossary.md) which is\nsimply two yaml files: `command_lib/base.yml` and `command_lib/snippets.yml`.\n\nbase.yml lists files that are typically found in the filesystem of a known\nLinux distribution. They are typically package managers but they can be any\nunique file that identifies the type of base OS for the container image.\n\nAlternatively, snippets.yml lists shell scripts for command line instructions\nused to create the diff filesystems that make up a container image. These are\nfor imperative commands like `apt-get install` or `wget web_package && ...`.\n\nBased on how a package is installed, one can figure out scripts to retrieve\ninformation for the package such as the version that was installed and the\nlicense of the package.                                                                                                                                    \nFor example, for a debian based container image, one can figure out what\npackages are installed by running `dpkg --get-selections`. If you look at\n`command_lib/base.yml` you will find a listing that looks like this:\n\n```\n-----\ndpkg:\n  path:\n    - 'usr/bin/dpkg'\n  shell: '/bin/bash'\n  # either enter the package names as a list or a snippet to invoke either within or outside the container\n  names:\n    invoke:\n      1:\n        # Currently two types of environments are supported - host or container\n        container:\n          - \"dpkg --get-selections | cut -f1 -d':' | awk '{print $1}'\"\n    delimiter: \"\\n\"\n```\n\n`dpkg --get-selections | cut -f1 -d':' | awk '{print $1}'` is a command that\nyou can run in a Docker container based on debian to get all the names of the\npackages installed in your container (the stuff after `dpkg --get-selections`\nis to trim out the information about the versions, arch and other information\nto just get the names). So, nothing special, but still needs to be accurate.\n                                                                                                                       \n## Adding to the Base Image Command Library                                                                                                                 \nYou can recognize a base OS Docker image from the Dockerfile. Typically, the\nDockerfile starts with the line `FROM scratch` followed by `ADD <filesystem>.tar /`\nwhich adds an entire OS filesystem to the image. There are many base images\navailable to use on Dockerhub. Before considering adding any of them to the\nCommand Library:\n                                                                                                      \n* Check if there is a Dockerfile published\n* Check if the Dockerfile starts with 'FROM scratch' followed by 'ADD <filesystem.tar>'\n\nHere is an example of Debian's base OS [Dockerfile](https://github.com/debuerreotype/docker-debian-artifacts/blob/de09dd55b6328b37b89a33e76b698f9dbe611fab/jessie/Dockerfile):\n\n```\nFROM scratch\nADD rootfs.tar.xz /\nCMD [\"bash\"]                             \n```\nThe object of the Base Image Command Library is to retrieve a list of package\nnames, a corresponding list of versions, a corresponding list of licenses and\na corresponding list of project urls using shell scripts.\n\nSo the first step to submitting an addition to the Base Image Command Library\nis to find the set of shell commands that will retrieve this information for\nyour chosen base image.\n\nFor instance, to get the names of all the packages installed in a pure debian:jessie\nOS container, we can run:\n```\n$ dpkg --get-selections | cut -f1 -d':' | awk '{print $1}'\n```\nwhich is what we would normally do on a regular Debian OS (granted there is some\nfinagling of the output to just print out the names but hey, who hasn't done fancy\nshell-fu to get a desired output? No joke - I found this on stackoverflow).\n\nYou can try this yourself on a debian:jessie container:\n```\n$ docker pull debian:jessie\n$ docker run -it debian:jessie\n# dpkg --get-selections | cut -f1 -d':' | awk '{print $1}'\n```\nAwesome! You now have a list of package names. Now to add this to command_lib/base.yml.\n\nIn this file you would add the following listing:\n\n```\ndpkg: <-- the package manager for debian\n  path: <-- where on the filesystem the binary exists\n    - 'usr/bin/dpkg' <-- it can exist in many places, make sure to not add a prepending '/'; Tern will take care of that for you\n  shell: '/bin/bash' <-- what shell to use\n  names: <-- listing to retrieve names of the packages\n    invoke: <-- it's not a hard coded listing but a list of snippets to invoke\n      1: <-- the first one\n        container: <-- invoke it in a container (Tern will actually use a chroot environment but for testing purposes we will usea Docker container\n          - \"dpkg --get-selections | cut -f1 -d':' | awk '{print $1}'\" <-- the command in double quotes as you are using awk here\n    delimiter: \"\\n\" <-- each of the results gets returned as a list with \"\\n\" as a delimiter. Double quotes are needed due to python formatting\n```\n\nThis is the minimum that is required to run Tern successfully, however, your\nreport will be full of messages telling you to add the other information\nwhich will be *hugely* appreciated by your fellow users and developers\nlike the version, licenses and project urls for the packages.\n\n## Adding to the Snippet Command Library\n\nOnce Tern analyzes the base layer of the container image, it will try to figure\nout what packages were installed in the subsequent layers. It does that by\nlooking up `command_lib/snippets.yml` where typical installation commands are\nlisted. For example, in the Dockerfile located in `samples/debian_vim` the line\n`RUN apt-get update && apt-get install vim && apt-get clean` indicates that the\npackage 'vim' was installed using the command 'apt-get install'. Tern only knows\nthis because 'apt-get' is a listed command and 'install' is the sub-command\nlisted to install a package. Here is that listing:\n\n```\napt-get: <-- the command\n  install: 'install' # subcommand to install <-- subcommand to install\n  remove: 'purge' # subcommand to remove a package <-- subcommand to remove\n  ignore: # list of subcommands that don't add or remove packages <-- list of subcommands to ignore\n    - 'update'\n```\n\nUnder this command is a list of packages that may be installed. If this is a\npackage management system used to install a whole host of packages, 'default'\ncan be given as a package name and Tern will use it on the list of packages\ninstalled using apt-get unless there is a specific item in the list of\npackages with a unique name. In this case, there is only the default listing:\n\n```\n  packages:\n    - name: default\n      version:\n        # either a version number or a script snippet\n        invoke:\n          1:\n            container:\n              - \"dpkg -l {package} | awk 'NR>5 {print $3}'\"\n      proj_url:\n        invoke:\n          1:\n            container:\n              - \"apt-get download --print-uris {package}\"\n      license:\n        invoke:\n          1:\n            container:\n              - \"cat /usr/share/doc/{package}/copyright\"\n      deps:\n        invoke:\n          1:\n            container:\n              - \"apt-cache depends --installed --no-suggests {package} | awk 'NR>1 {print $2}'\"\n        delimiter: \"\\n\"\n```\n\nFor each item the listing follows similarly to the listings in base.yml, where\na shell script is used to obtain the package information.\n\nAgain, you can try this on the sample image:\n\n```\n$ cd samples/debian_vim/Dockerfile\n$ docker build -t vim:test .\n$ docker run -it vim:test\n# dpkg -l vim | awk 'NR>5 {print $3}'\n```\nYou get the version of the package vim.\n\nWhy use '{package}' instead of 'vim' in the snippets.yml file? Since you have\ngiven the name of the package to be 'default', Tern will run the shell scripts\non every package name it has parsed from the container manifest that was installed\nwith apt-get.\n\nAlternatively, you can point to the `base.yml` way of listing packages. For example,\nyou can point `apt-get` back to `dpkg` like this:\n\n```\n  packages: 'dpkg'\n```\n\nThis is currently the most ubiquitous way of enabling inventorying using a package\nmanager.\n\n\n## Using Tern's 'debug' option to check your script\n\nTo check if your script is producing accurate results that Tern can understand,\nyou can make use of Tern's 'debug' option for both base.yml and\nsnippets.yml listing.\n\nI assume here that you have already installed Docker and are familiar with\nhow to run a container. verify_invoke.py will not do this for you, but you\ncan use it to test your snippets. I also assume you have set up the Tern\nenvironment:\n\n```\n$ python3 -m venv ternenv\n$ cd ternenv\n$ git clone https://github.com/tern-tools/tern.git\n$ source bin/activate\n$ cd tern\n$ pip install -r requirements.txt\n\n```\n\nGenerally here is the workflow:\n\n(This workflow only works for base.yml)\n\nRun tern in the following way:\n\n```\n$ tern debug -i <image:tag> --keys base <package_manager> <names/versions/licenses/src_urls> --shell '/usr/bin/bash'\n```\n\nThis should give you a result that looks something like this:\n```\n$ tern debug -i photon:2.0 --keys base tdnf names --shell '/usr/bin/bash'\nOutput list: bash bzip2-libs ca-certificates ca-certificates-pki curl curl-libs e2fsprogs-libs elfutils-libelf expat-libs filesystem glibc hawkey krb5 libcap libdb libgcc libsolv libssh2 ncurses-libs nspr nss-libs openssl photon-release photon-repos popt readline rpm-libs sqlite-libs tdnf toybox xz-libs zlib\nError messages: \nNumber of elements: 32\n```\n\nThe --keys flag is a list of keys to look up in the command library dictionary\nuntil you get to the 'invoke' key and the attribute's scripts you want to test\nout. For base.yml the first key is 'base'. If you look up tdnf > names in\nbase.yml, you should see a listing with and 'invoke' key like this:\n\n```\n   invoke:\n      1:\n        container:\n          # make sure to use single quotes for these lines as\n          # docker exec doesn't parse double quotes contiguously\n          - 'tdnf check-update > /dev/null' # used to update the repo metadata\n          - 'tdnf list installed | cut -f1 -d\".\"'\n    delimiter: \"\\n\" # make sure the delimiter is in double quotes or python will interpret the \\ as literal\n```\n\nYou can now verify if this result is what you expect for the list of package\nnames, the number of elements in the list i.e. the number of installed packages\nand if there were any error messages.\n\nWhat if you encounter an error? Tern's debug option can also help you chroot into the filesystem to try running the script manually.\n\n**WARNING** This only works on Linux distros. Windows and MacOS are not supported.\n\n```\n$ tern debug -i <image:tag> --step\n```\n\nThis will drop you into an interactive mode where you will be asked to pick the layer you want to debug\n\n```\n*************************************************************\n          Container Image Interactive Debug Mode             \n*************************************************************\n\n[0] /bin/sh -c #(nop) ADD file:240dde03c4d9f0ad759f8d1291fb45ab2745b6a108c6164d746766239d3420ab in / \n[1] /bin/sh -c dnf -y update && dnf install -y make git golang golang-github-cpuguy83-md2man    btrfs-progs-devel       device-mapper-devel     libassuan-devel gpgme-devel     gnupg   httpd-tools     which tar wget hostname util-linux bsdtar socat ethtool device-mapper iptables tree findutils nmap-ncat e2fsprogs xfsprogs lsof docker iproute         bats jq podman runc      golint  openssl         && dnf clean all\n[2] /bin/sh -c set -x   && REGISTRY_COMMIT_SCHEMA1=ec87e9b6971d831f0eff752ddb54fb64693e51cd     && REGISTRY_COMMIT=47a064d4195a9b56133891bbb13620c3ac83a827     && export GOPATH=\"$(mktemp -d)\"         && git clone https://github.com/docker/distribution.git \"$GOPATH/src/github.com/docker/distribution\"    && (cd \"$GOPATH/src/github.com/docker/distribution\" && git checkout -q \"$REGISTRY_COMMIT\")      && GOPATH=\"$GOPATH/src/github.com/docker/distribution/Godeps/_workspace:$GOPATH\"                go build -o /usr/local/bin/registry-v2 github.com/docker/distribution/cmd/registry      && (cd \"$GOPATH/src/github.com/docker/distribution\" && git checkout -q \"$REGISTRY_COMMIT_SCHEMA1\")      && GOPATH=\"$GOPATH/src/github.com/docker/distribution/Godeps/_workspace:$GOPATH\"               go build -o /usr/local/bin/registry-v2-schema1 github.com/docker/distribution/cmd/registry       && rm -rf \"$GOPATH\"\n[3] /bin/sh -c set -x   && export GOPATH=$(mktemp -d)   && git clone --depth 1 -b v1.5.0-alpha.3 git://github.com/openshift/origin \"$GOPATH/src/github.com/openshift/origin\"    && sed -i -e 's/\\[\\[ \"\\${go_version\\[2]}\" < \"go1.5\" ]]/false/' \"$GOPATH/src/github.com/openshift/origin/hack/common.sh\"         && (cd \"$GOPATH/src/github.com/openshift/origin\" && make clean build && make all WHAT=cmd/dockerregistry)       && cp -a \"$GOPATH/src/github.com/openshift/origin/_output/local/bin/linux\"/*/* /usr/local/bin   && cp \"$GOPATH/src/github.com/openshift/origin/images/dockerregistry/config.yml\" /atomic-registry-config.yml    && rm -rf \"$GOPATH\"     && mkdir /registry\n[4] /bin/sh -c go version\n[5] /bin/sh -c #(nop) WORKDIR /go/src/github.com/containers/skopeo\n[6] /bin/sh -c #(nop) COPY dir:8e7a6d060051c80f562171cdc69069a21b60ed3b5e3355a9dabe31523a31d432 in /go/src/github.com/containers/skopeo \n\nPick a layer to debug: \n```\n\nAfter picking a layer, you should get some instructions on how to proceed with your debugging\n\n```\nRun 'cd /home/nisha/.tern/temp/mergedir && sudo chroot . /bin/sh' to look around\n\nAfter exiting from your session, run 'cd -' to go back and 'tern debug --recover' to clean up.\n```\n\nYou should get something like this if you run the first command\n\n```\nroot@ubuntu / $ ls\natomic-registry-config.yml  boot  etc   lib    lost+found  mnt  proc      root  sbin  sys  usr\nbin                         dev   home  lib64  media       opt  registry  run   srv   tmp  var\n```\n\nThis is what the container filesystem looks like at that specific layer. To exit out of this, run `exit`. At this point you should still be in the working directory. To get back, run `cd -`.\n\nTo recover the filesystem, run the following:\n\n```\n$ tern debug --recover\n```\n\nAs always, don't hesitate to ask questions by filing an issue with 'Question:'\nas the prefix of the subject, on the Slack channel or on the mailing list.\n\n[Back to the README](../README.md)\n", "metadata": {"source": "github.com/tern-tools/tern", "file_path": "docs/adding-to-command-library.md", "language": "markdown", "complexity": "complex", "star_count": 993, "quality_score": 0.56, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 308}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Remove old package versions\non:\n  workflow_dispatch:\njobs:\n  remove-package-versions:\n    runs-on: ubuntu-latest\n    steps:\n      - name: purge packages\n        uses: dylanratcliffe/delete-untagged-containers@main\n        with:\n          package_name: headscale-webui\n          token: ${{ secrets.ACTIONS_WORKFLOWS }}\n", "metadata": {"source": "github.com/iFargle/headscale-webui", "file_path": ".github/workflows/clean-untagged-builds.yml", "language": "yaml", "complexity": "medium", "star_count": 710, "quality_score": 0.5, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 13}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# For most projects, this workflow file will not need changing; you simply need\n# to commit it to your repository.\n#\n# You may wish to alter this file to override the set of languages analyzed,\n# or to provide custom queries or build logic.\n#\n# ******** NOTE ********\n# We have attempted to detect the languages in your repository. Please check\n# the `language` matrix defined below to confirm you have the correct set of\n# supported CodeQL languages.\n#\nname: \"CodeQL\"\n\non:\n  push:\n    branches: [ \"main\" ]\n  pull_request:\n    # The branches below must be a subset of the branches above\n    branches: [ \"main\" ]\n  schedule:\n    - cron: '31 20 * * 2'\n\njobs:\n  analyze:\n    name: Analyze\n    runs-on: ubuntu-latest\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language: [ 'javascript', 'python' ]\n        # CodeQL supports [ 'cpp', 'csharp', 'go', 'java', 'javascript', 'python', 'ruby' ]\n        # Use only 'java' to analyze code written in Java, Kotlin or both\n        # Use only 'javascript' to analyze code written in JavaScript, TypeScript or both\n        # Learn more about CodeQL language support at https://aka.ms/codeql-docs/language-support\n\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v3\n\n    # Initializes the CodeQL tools for scanning.\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v2\n      with:\n        languages: ${{ matrix.language }}\n        # If you wish to specify custom queries, you can do so here or in a config file.\n        # By default, queries listed here will override any specified in a config file.\n        # Prefix the list here with \"+\" to use these queries and those in the config file.\n\n        # Details on CodeQL's query packs refer to : https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs\n        # queries: security-extended,security-and-quality\n\n\n    # Autobuild attempts to build any compiled languages  (C/C++, C#, Go, or Java).\n    # If this step fails, then you should remove it and run the build manually (see below)\n    - name: Autobuild\n      uses: github/codeql-action/autobuild@v2\n\n    # ‚ÑπÔ∏è Command-line programs to run using the OS shell.\n    # üìö See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun\n\n    #   If the Autobuild fails above, remove it and uncomment the following three lines.\n    #   modify them (or add more) to build your code if your project, please refer to the EXAMPLE below for guidance.\n\n    # - run: |\n    #     echo \"Run, Build Application using script\"\n    #     ./location_of_script_within_repo/buildscript.sh\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v2\n      with:\n        category: \"/language:${{matrix.language}}\"\n", "metadata": {"source": "github.com/iFargle/headscale-webui", "file_path": ".github/workflows/codeql.yml", "language": "yaml", "complexity": "medium", "star_count": 710, "quality_score": 0.53, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 77}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "*PR's to help expand and improve documentation are always welcome!*\n\n# Installation and Setup\n* Use [docker-compose.yml](docker-compose.yml) as an example\n* Containers are published to [GHCR](https://github.com/users/iFargle/packages/container/package/headscale-webui) and [Docker Hub](https://hub.docker.com/r/ifargle/headscale-webui)\n\n# Contents\n  * [Bare Metal](#bare-metal)\n  * [Docker Compose](#docker-compose)\n  * [Reverse Proxies](#reverse-proxies)\n  * [Authentication](#authentication)\n\n---\n# Bare Metal\n\n1. Install dependencies:\n\n```bash\n# Debian/Ubuntu\napt install gcc python3-poetry --yes\npoetry install --only main\n```\n\n2. Configurations: rename `.env.sample` -> `.env` and edit `.env` as per your requirements.\n\n3. Run server\n\n```bash\npoetry run gunicorn -b 0.0.0.0:5000 server:app\n```\n\nThat's it. Cheers.\n\n# Docker Compose\n## Environment Settings\n  * `TZ` - Set this to your current timezone.  Example:  `Asia/Tokyo`\n  * `COLOR` Set this to your preferred color scheme.  See the [MaterializeCSS docs](https://materializecss.github.io/materialize/color.html#palette) for examples.  Only set the \"base\" color -- ie, instead of `blue-gray darken-1`, just use `blue-gray`.\n  * `HS_SERVER` is the URL for your Headscale control server.\n  * `SCRIPT_NAME` is your \"Base Path\" for hosting.  For example, if you want to host on http://localhost/admin, set this to `/admin`, otherwise remove this variable entirely.\n  * `DATA_DIRECTORY` is your \"Data Path\". This is there the application will create and store data. Only applicable for bare metal installations.\n  * `KEY` is your encryption key.  Set this to a random value generated from `openssl rand -base64 32`\n  * `AUTH_TYPE` can be set to `Basic` or `OIDC`.  See the [Authentication](#Authentication) section below for more information.\n  * `LOG_LEVEL` can be one of `Debug`, `Info`, `Warning`, `Error`, or `Critical` for decreasing verbosity.  Default is `Info` if removed from your Environment.\n---\n# Podman rootless container\n\nA rootless container can be a good choice when running headscale-webui with Podman.\n\nTo achieve this, the option `allow_host_loopback` for the slirp4netns network driver must be explicitly set. This will allow the container to contact sockets listening on the host (specifically, headscale).\n\nBy default, slirp4netns will present the host on the IP address `10.0.2.2` (adjust accordingly if you specify different addressing options), so this IP will be the address to set in the HS_SERVER environment variable (along with the port number) when spinning the container.\nFor the rest of the enviroment settings, the considerations done for the Docker example above still hold.\n\n* Example:\n```\npodman run -d --network slirp4netns:allow_host_loopback=true -v /etc/headscale:/etc/headscale:ro \\\n           -p 5000:5000 --name headscale-webui -e HS_SERVER=http://10.0.2.2:8080 -e KEY=YOUR_ENC_KEY \\\n           -e DOMAIN_NAME=http://headscale-webui:5000 -e SCRIPT_NAME=/admin ifargle/headscale-webui:latest\n```\n\n---\n# Reverse Proxies\n*If your reverse proxy isn't listed or doesn't work, please open up a [new issue](https://github.com/iFargle/headscale-webui/issues/new) and it will be worked on.*\n\n## Traefik with SSL\n1.  Use the following labels for your container.  You may need to adjust slightly to fit your setup.\n    * You will need to change `[DOMAIN]`, `[SCRIPT_NAME]`, and the `entrypoint` to fit your setup.\n    * If you are hosting on `SCRIPT_NAME` of `/`, you can remove `&& (PathPrefix(`/[SCRIPT_NAME]/`) || PathPrefix(`/[SCRIPT_NAME]`)`\n```\n    labels:\n      # Traefik Configs\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.headscale-webui.entrypoints=web-secure\"\n      - \"traefik.http.routers.headscale-webui.rule=Host(`[DOMAIN]`) && (PathPrefix(`/[SCRIPT_NAME]/`) || PathPrefix(`/[SCRIPT_NAME]`))\"\n      - \"traefik.http.services.headscale-webui.loadbalancer.server.port=5000\"\n      - \"traefik.http.routers.headscale-webui.tls.certresolver=letsencrypt\"\n```\n\n## nginx\n1.  Replace `[SCRIPT_NAME]` with the one you set above.\n```\nlocation /[SCRIPT_NAME] {\nproxy_pass http://127.0.0.1:5000/[SCRIPT_NAME];\nproxy_http_version 1.1;\nproxy_set_header Host $server_name;\nproxy_buffering off;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header X-Forwarded-Proto $http_x_forwarded_proto;\n    auth_basic \"Administrator's Area\";\n    auth_basic_user_file /etc/nginx/htpasswd;\n}\n```\n\n## Caddy\n1.  Replace `[DOMAIN]` with your domain, `[HEADSCALE-WEBUI ADDRESS]` with the internal endpoint of your deployment (by default, this will be `http://headscale-webui:5000`), and `[HS_SERVER]` with your Headscale server.\n  * This will set up your Headscale Web UI under `[SCRIPT_NAME] `on the same domain as your Headscale control server.\n  * Example:\n    * Headscale will be reachable at `https://[DOMAIN]`, Headscale-WebUI will be reachable at `https://[DOMAIN]/[SCRIPT_NAME]`\n```\nhttps://[DOMAIN] {\n        reverse_proxy [SCRIPT_NAME]* [HEADSCALE-WEBUI ADDRESS]\n        reverse_proxy * [HS_SERVER]\n}\n```\n* Example:\n```\nhttps://example.com {\n        reverse_proxy /admin* http://headscale-webui:5000\n\n        reverse_proxy * http://headscale:8080\n}\n```\n\n---\n# Authentication\n*If your OIDC provider isn't listed or doesn't work, please open up a [new issue](https://github.com/iFargle/headscale-webui/issues/new) and it will be worked on.*\n\n## No Authentication\n1.  If you use your reverse proxy for authentication, simply remove `AUTH_TYPE` from your environment variables.\n\n## Basic Auth\n1.  Basic Auth is a relatively simple setup. Set the following environment variables in `docker-compose.yml`:\n    * Set `AUTH_TYPE` to `basic`\n    * Set `BASIC_AUTH_USER` to a username, i.e. `admin`\n    * Set `BASIC_AUTH_PASS` to set your password.\n\n## OpenID Connect Integration\n### Authelia\n1.  In your Authelia `configuration.yml` file, add a new client:\n```\n      - id: headscale-webui\n        description: Headscale WebUI\n        secret: [SECRET]\n        public: false\n        authorization_policy: two_factor\n        redirect_uris:\n          - https://[DOMAIN]/[SCRIPT_NAME]/oidc_callback\n        scopes:\n          - openid\n          - profile\n          - email\n```\n2.  Set `AUTH_TYPE` environment variable in your docker-compose.yml to `oidc` and set the following:\n    * `OIDC_AUTH_URL` should be set to your providers well-known endpoint.  For example, Authelia is `https://[YourAuthDomain]/.well-known/openid-configuration`.\n    * `OIDC_CLIENT_ID` is the `id` in your Authelia configuration.yaml.  In this case, it would be `headscale-webui`.\n    * `OIDC_CLIENT_SECRET` is your client secret, in this case `[SECRET]`.  You can generate a secret using `openssl rand -hex 64`.\n\n### KeyCloak\n1.  In your Keycloak settings, add the following:\n    *  The keycloak endpoint can be found on the realm settings page as the \"OpenID Endpoint Configuration\" link.\n```\nClient ID = headscale-webui\nCallback URI = https://[DOMAIN]/[SCRIPT_NAME]/oidc_callback\nClient Authentication (Previously called confidential access or similar) = True\nClient Secret = [SECRET]\n```\n2.  Set `AUTH_TYPE` environment variable in your docker-compose.yml to `oidc` and set the following:\n    * `OIDC_AUTH_URL` should be set to your providers well-known endpoint.  For example, Keycloak is `https://[DOMAIN]/realms/[REALM]/.well-known/openid-configuration`.\n    * `OIDC_CLIENT_ID` is the `id` in your Authelia configuration.yaml.  In this case, it would be `headscale-webui`.\n    * `OIDC_CLIENT_SECRET` is your client secret, in this case `[SECRET]`.  You can generate a secret using `openssl rand -hex 64`.\n\n---\n", "metadata": {"source": "github.com/iFargle/headscale-webui", "file_path": "SETUP.md", "language": "markdown", "complexity": "complex", "star_count": 710, "quality_score": 0.59, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 164}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: \"3\"\nservices:\n  headscale-webui:\n    image: ghcr.io/ifargle/headscale-webui:latest\n    container_name: headscale-webui\n    environment:\n      - TZ=Asia/Tokyo\n      - COLOR=red                              # Use the base colors (ie, no darken-3, etc) - \n      - HS_SERVER=https://headscale.$DOMAIN    # Reachable endpoint for your Headscale server\n      - DOMAIN_NAME=https://headscale.$DOMAIN  # The base domain name for this container.\n      - SCRIPT_NAME=/admin                     # This is your applications base path (wsgi requires the name \"SCRIPT_NAME\").  Remove if you are hosing at the root /\n      - KEY=\"YourKeyBetweenQuotes\"             # Generate with \"openssl rand -base64 32\" - used to encrypt your key on disk.\n      - AUTH_TYPE=oidc                         # AUTH_TYPE is either Basic or OIDC.  Empty for no authentication\n      - LOG_LEVEL=info                         # Log level.  \"DEBUG\", \"ERROR\", \"WARNING\", or \"INFO\".  Default \"INFO\"\n      # ENV for Basic Auth (Used only if AUTH_TYPE is \"Basic\").  Can be omitted if you aren't using Basic Auth\n      - BASIC_AUTH_USER=user                   # Used for basic auth\n      - BASIC_AUTH_PASS=pass                   # Used for basic auth\n      # ENV for OIDC (Used only if AUTH_TYPE is \"OIDC\").  Can be omitted if you aren't using OIDC\n      - OIDC_AUTH_URL=https://auth.$DOMAIN/.well-known/openid-configuration # URL for your OIDC issuer's well-known endpoint\n      - OIDC_CLIENT_ID=headscale-webui         # Your OIDC Issuer's Client ID for Headscale-WebUI\n      - OIDC_CLIENT_SECRET=YourSecretHere      # Your OIDC Issuer's Secret Key for Headscale-WebUI\n    volumes:\n      - ./volume:/data                         # Headscale-WebUI's storage.  Make sure ./volume is readable by UID 1000 (chown 1000:1000 ./volume)\n      - ./headscale/config/:/etc/headscale/:ro # Headscale's config storage location.  Used to read your Headscale config.", "metadata": {"source": "github.com/iFargle/headscale-webui", "file_path": "docker-compose.yml", "language": "yaml", "complexity": "medium", "star_count": 710, "quality_score": 0.53, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 24}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# pylint: disable=wrong-import-order\n\nimport headscale, helper, json, os, pytz, renderer, secrets, requests, logging\nfrom functools                     import wraps\nfrom datetime                      import datetime\nfrom flask                         import Flask, escape, Markup, redirect, render_template, request, url_for\nfrom dateutil                      import parser\nfrom flask_executor                import Executor\nfrom werkzeug.middleware.proxy_fix import ProxyFix\n\n# Global vars\n# Colors:  https://materializecss.com/color.html\nCOLOR       = os.environ[\"COLOR\"].replace('\"', '').lower()\nCOLOR_NAV   = COLOR+\" darken-1\"\nCOLOR_BTN   = COLOR+\" darken-3\"\nAUTH_TYPE   = os.environ[\"AUTH_TYPE\"].replace('\"', '').lower()\nLOG_LEVEL   = os.environ[\"LOG_LEVEL\"].replace('\"', '').upper()\n# If LOG_LEVEL is DEBUG, enable Flask debugging:\nDEBUG_STATE = True if LOG_LEVEL == \"DEBUG\" else False\n\n# Initiate the Flask application and logging:\napp = Flask(__name__, static_url_path=\"/static\")\nmatch LOG_LEVEL:\n    case \"DEBUG\"   : app.logger.setLevel(logging.DEBUG)\n    case \"INFO\"    : app.logger.setLevel(logging.INFO)\n    case \"WARNING\" : app.logger.setLevel(logging.WARNING)\n    case \"ERROR\"   : app.logger.setLevel(logging.ERROR)\n    case \"CRITICAL\": app.logger.setLevel(logging.CRITICAL)\n\nexecutor = Executor(app)\napp.wsgi_app = ProxyFix(app.wsgi_app, x_for=1, x_proto=1, x_host=1, x_prefix=1)\napp.logger.info(\"Headscale-WebUI Version:  \"+os.environ[\"APP_VERSION\"]+\" / \"+os.environ[\"GIT_BRANCH\"])\napp.logger.info(\"LOG LEVEL SET TO %s\", str(LOG_LEVEL))\napp.logger.info(\"DEBUG STATE:  %s\", str(DEBUG_STATE))\n\n########################################################################################\n# Set Authentication type.  Currently \"OIDC\" and \"BASIC\"\n########################################################################################\nif AUTH_TYPE == \"oidc\":\n    # Currently using: flask-providers-oidc - https://pypi.org/project/flask-providers-oidc/ \n    #\n    # https://gist.github.com/thomasdarimont/145dc9aa857b831ff2eff221b79d179a/ \n    # https://www.authelia.com/integration/openid-connect/introduction/ \n    # https://github.com/steinarvk/flask_oidc_demo \n    app.logger.info(\"Loading OIDC libraries and configuring app...\")\n\n    DOMAIN_NAME    = os.environ[\"DOMAIN_NAME\"]\n    BASE_PATH      = os.environ[\"SCRIPT_NAME\"] if os.environ[\"SCRIPT_NAME\"] != \"/\" else \"\"\n    OIDC_SECRET    = os.environ[\"OIDC_CLIENT_SECRET\"]\n    OIDC_CLIENT_ID = os.environ[\"OIDC_CLIENT_ID\"]\n    OIDC_AUTH_URL  = os.environ[\"OIDC_AUTH_URL\"]\n\n    # Construct client_secrets.json:\n    response = requests.get(str(OIDC_AUTH_URL))\n    oidc_info = response.json()\n    app.logger.debug(\"JSON Dumps for OIDC_INFO:  \"+json.dumps(oidc_info))\n\n    client_secrets = json.dumps(\n        {\n            \"web\": {\n                \"issuer\": oidc_info[\"issuer\"],\n                \"auth_uri\": oidc_info[\"authorization_endpoint\"],\n                \"client_id\": OIDC_CLIENT_ID,\n                \"client_secret\": OIDC_SECRET,\n                \"redirect_uris\": [DOMAIN_NAME + BASE_PATH + \"/oidc_callback\"],\n                \"userinfo_uri\": oidc_info[\"userinfo_endpoint\"],\n                \"token_uri\": oidc_info[\"token_endpoint\"],\n            }\n        }\n    )\n\n    with open(\"/app/instance/secrets.json\", \"w+\") as secrets_json:\n        secrets_json.write(client_secrets)\n    app.logger.debug(\"Client Secrets:  \")\n    with open(\"/app/instance/secrets.json\", \"r+\") as secrets_json:\n        app.logger.debug(\"/app/instances/secrets.json:\")\n        app.logger.debug(secrets_json.read())\n    \n    app.config.update({\n        'SECRET_KEY': secrets.token_urlsafe(32),\n        'TESTING': DEBUG_STATE,\n        'DEBUG': DEBUG_STATE,\n        'OIDC_CLIENT_SECRETS': '/app/instance/secrets.json',\n        'OIDC_ID_TOKEN_COOKIE_SECURE': True,\n        'OIDC_REQUIRE_VERIFIED_EMAIL': False,\n        'OIDC_USER_INFO_ENABLED': True,\n        'OIDC_OPENID_REALM': 'Headscale-WebUI',\n        'OIDC_SCOPES': ['openid', 'profile', 'email'],\n        'OIDC_INTROSPECTION_AUTH_METHOD': 'client_secret_post'\n    })\n    from flask_oidc import OpenIDConnect\n    oidc = OpenIDConnect(app)\n\nelif AUTH_TYPE == \"basic\":\n    # https://flask-basicauth.readthedocs.io/en/latest/\n    app.logger.info(\"Loading basic auth libraries and configuring app...\")\n    from flask_basicauth import BasicAuth\n\n    app.config['BASIC_AUTH_USERNAME'] = os.environ[\"BASIC_AUTH_USER\"].replace('\"', '')\n    app.config['BASIC_AUTH_PASSWORD'] = os.environ[\"BASIC_AUTH_PASS\"]\n    app.config['BASIC_AUTH_FORCE']    = True\n\n    basic_auth = BasicAuth(app)\n    ########################################################################################\n    # Set Authentication type - Dynamically load function decorators\n    # https://stackoverflow.com/questions/17256602/assertionerror-view-function-mapping-is-overwriting-an-existing-endpoint-functi \n    ########################################################################################\n    # Make a fake decorator for oidc.require_login\n    # If anyone knows a better way of doing this, please let me know.\n    class OpenIDConnect():\n        def require_login(self, view_func):\n            @wraps(view_func)\n            def decorated(*args, **kwargs):\n                return view_func(*args, **kwargs)\n            return decorated\n    oidc = OpenIDConnect()\n\nelse:\n    ########################################################################################\n    # Set Authentication type - Dynamically load function decorators\n    # https://stackoverflow.com/questions/17256602/assertionerror-view-function-mapping-is-overwriting-an-existing-endpoint-functi \n    ########################################################################################\n    # Make a fake decorator for oidc.require_login\n    # If anyone knows a better way of doing this, please let me know.\n    class OpenIDConnect():\n        def require_login(self, view_func):\n            @wraps(view_func)\n            def decorated(*args, **kwargs):\n                return view_func(*args, **kwargs)\n            return decorated\n    oidc = OpenIDConnect()\n\n########################################################################################\n# / pages - User-facing pages\n########################################################################################\n@app.route('/')\n@app.route('/overview')\n@oidc.require_login\ndef overview_page():\n    # Some basic sanity checks:\n    pass_checks = str(helper.load_checks())\n    if pass_checks != \"Pass\": return redirect(url_for(pass_checks))\n\n    # Check if OIDC is enabled.  If it is, display the buttons:\n    OIDC_NAV_DROPDOWN = Markup(\"\")\n    OIDC_NAV_MOBILE = Markup(\"\")\n    if AUTH_TYPE == \"oidc\":\n        email_address = oidc.user_getfield(\"email\")\n        user_name     = oidc.user_getfield(\"preferred_username\")\n        name          = oidc.user_getfield(\"name\")\n        OIDC_NAV_DROPDOWN = renderer.oidc_nav_dropdown(user_name, email_address, name)\n        OIDC_NAV_MOBILE   = renderer.oidc_nav_mobile(user_name, email_address, name)\n\n    return render_template('overview.html',\n        render_page       = renderer.render_overview(),\n        COLOR_NAV         = COLOR_NAV,\n        COLOR_BTN         = COLOR_BTN,\n        OIDC_NAV_DROPDOWN = OIDC_NAV_DROPDOWN,\n        OIDC_NAV_MOBILE   = OIDC_NAV_MOBILE\n    )\n\n@app.route('/routes', methods=('GET', 'POST'))\n@oidc.require_login\ndef routes_page():\n    # Some basic sanity checks:\n    pass_checks = str(helper.load_checks())\n    if pass_checks != \"Pass\": return redirect(url_for(pass_checks))\n\n    # Check if OIDC is enabled.  If it is, display the buttons:\n    OIDC_NAV_DROPDOWN = Markup(\"\")\n    OIDC_NAV_MOBILE = Markup(\"\")\n    INPAGE_SEARCH = Markup(renderer.render_search())\n    if AUTH_TYPE == \"oidc\":\n        email_address = oidc.user_getfield(\"email\")\n        user_name     = oidc.user_getfield(\"preferred_username\")\n        name          = oidc.user_getfield(\"name\")\n        OIDC_NAV_DROPDOWN = renderer.oidc_nav_dropdown(user_name, email_address, name)\n        OIDC_NAV_MOBILE   = renderer.oidc_nav_mobile(user_name, email_address, name)\n    \n    return render_template('routes.html',\n        render_page       = renderer.render_routes(),\n        COLOR_NAV         = COLOR_NAV,\n        COLOR_BTN         = COLOR_BTN,\n        OIDC_NAV_DROPDOWN = OIDC_NAV_DROPDOWN,\n        OIDC_NAV_MOBILE   = OIDC_NAV_MOBILE\n    )\n\n\n@app.route('/machines', methods=('GET', 'POST'))\n@oidc.require_login\ndef machines_page():\n    # Some basic sanity checks:\n    pass_checks = str(helper.load_checks())\n    if pass_checks != \"Pass\": return redirect(url_for(pass_checks))\n\n    # Check if OIDC is enabled.  If it is, display the buttons:\n    OIDC_NAV_DROPDOWN = Markup(\"\")\n    OIDC_NAV_MOBILE = Markup(\"\")\n    INPAGE_SEARCH = Markup(renderer.render_search())\n    if AUTH_TYPE == \"oidc\":\n        email_address = oidc.user_getfield(\"email\")\n        user_name     = oidc.user_getfield(\"preferred_username\")\n        name          = oidc.user_getfield(\"name\")\n        OIDC_NAV_DROPDOWN = renderer.oidc_nav_dropdown(user_name, email_address, name)\n        OIDC_NAV_MOBILE   = renderer.oidc_nav_mobile(user_name, email_address, name)\n    \n    cards = renderer.render_machines_cards()\n    return render_template('machines.html',\n        cards             = cards,\n        headscale_server  = headscale.get_url(True),\n        COLOR_NAV         = COLOR_NAV,\n        COLOR_BTN         = COLOR_BTN,\n        OIDC_NAV_DROPDOWN = OIDC_NAV_DROPDOWN,\n        OIDC_NAV_MOBILE   = OIDC_NAV_MOBILE,\n        INPAGE_SEARCH     = INPAGE_SEARCH\n    )\n\n@app.route('/users', methods=('GET', 'POST'))\n@oidc.require_login\ndef users_page():\n    # Some basic sanity checks:\n    pass_checks = str(helper.load_checks())\n    if pass_checks != \"Pass\": return redirect(url_for(pass_checks))\n\n    # Check if OIDC is enabled.  If it is, display the buttons:\n    OIDC_NAV_DROPDOWN = Markup(\"\")\n    OIDC_NAV_MOBILE = Markup(\"\")\n    INPAGE_SEARCH = Markup(renderer.render_search())\n    if AUTH_TYPE == \"oidc\":\n        email_address = oidc.user_getfield(\"email\")\n        user_name     = oidc.user_getfield(\"preferred_username\")\n        name          = oidc.user_getfield(\"name\")\n        OIDC_NAV_DROPDOWN = renderer.oidc_nav_dropdown(user_name, email_address, name)\n        OIDC_NAV_MOBILE   = renderer.oidc_nav_mobile(user_name, email_address, name)\n\n    cards = renderer.render_users_cards()\n    return render_template('users.html',\n        cards             = cards,\n        COLOR_NAV         = COLOR_NAV,\n        COLOR_BTN         = COLOR_BTN,\n        OIDC_NAV_DROPDOWN = OIDC_NAV_DROPDOWN,\n        OIDC_NAV_MOBILE   = OIDC_NAV_MOBILE,\n        INPAGE_SEARCH     = INPAGE_SEARCH\n    )\n\n@app.route('/settings', methods=('GET', 'POST'))\n@oidc.require_login\ndef settings_page():\n    # Some basic sanity checks:\n    pass_checks = str(helper.load_checks())\n    if pass_checks != \"Pass\" and pass_checks != \"settings_page\": \n        return redirect(url_for(pass_checks))\n\n    # Check if OIDC is enabled.  If it is, display the buttons:\n    OIDC_NAV_DROPDOWN = Markup(\"\")\n    OIDC_NAV_MOBILE = Markup(\"\")\n    if AUTH_TYPE == \"oidc\":\n        email_address = oidc.user_getfield(\"email\")\n        user_name     = oidc.user_getfield(\"preferred_username\")\n        name          = oidc.user_getfield(\"name\")\n        OIDC_NAV_DROPDOWN = renderer.oidc_nav_dropdown(user_name, email_address, name)\n        OIDC_NAV_MOBILE   = renderer.oidc_nav_mobile(user_name, email_address, name)\n\n    GIT_COMMIT_LINK = Markup(\"<a href='https://github.com/iFargle/headscale-webui/commit/\"+os.environ[\"GIT_COMMIT\"]+\"'>\"+str(os.environ[\"GIT_COMMIT\"])[0:7]+\"</a>\")\n\n    return render_template('settings.html', \n        url          = headscale.get_url(),\n        COLOR_NAV    = COLOR_NAV,\n        COLOR_BTN    = COLOR_BTN,\n        OIDC_NAV_DROPDOWN = OIDC_NAV_DROPDOWN,\n        OIDC_NAV_MOBILE = OIDC_NAV_MOBILE,\n        BUILD_DATE   = os.environ[\"BUILD_DATE\"],\n        APP_VERSION  = os.environ[\"APP_VERSION\"],\n        GIT_COMMIT   = GIT_COMMIT_LINK,\n        GIT_BRANCH   = os.environ[\"GIT_BRANCH\"],\n        HS_VERSION   = os.environ[\"HS_VERSION\"]\n    )\n\n@app.route('/error')\n@oidc.require_login\ndef error_page():\n    if helper.access_checks() == \"Pass\": \n        return redirect(url_for('overview_page'))\n\n    return render_template('error.html', \n        ERROR_MESSAGE = Markup(helper.access_checks())\n    )\n\n@app.route('/logout')\ndef logout_page():\n    if AUTH_TYPE == \"oidc\":\n        oidc.logout()\n    return redirect(url_for('overview_page'))\n########################################################################################\n# /api pages\n########################################################################################\n\n########################################################################################\n# Headscale API Key Endpoints\n########################################################################################\n\n@app.route('/api/test_key', methods=('GET', 'POST'))\n@oidc.require_login\ndef test_key_page():\n    api_key    = headscale.get_api_key()\n    url        = headscale.get_url()\n\n    # Test the API key.  If the test fails, return a failure.  \n    status = headscale.test_api_key(url, api_key)\n    if status != 200: return \"Unauthenticated\"\n\n    renewed = headscale.renew_api_key(url, api_key)\n    app.logger.warning(\"The below statement will be TRUE if the key has been renewed, \")\n    app.logger.warning(\"or DOES NOT need renewal.  False in all other cases\")\n    app.logger.warning(\"Renewed:  \"+str(renewed))\n    # The key works, let's renew it if it needs it.  If it does, re-read the api_key from the file:\n    if renewed: api_key = headscale.get_api_key()\n\n    key_info   = headscale.get_api_key_info(url, api_key)\n\n    # Set the current timezone and local time\n    timezone   = pytz.timezone(os.environ[\"TZ\"] if os.environ[\"TZ\"] else \"UTC\")\n    local_time = timezone.localize(datetime.now())\n\n    # Format the dates for easy readability\n    creation_parse   = parser.parse(key_info['createdAt'])\n    creation_local   = creation_parse.astimezone(timezone)\n    creation_delta   = local_time - creation_local\n    creation_print   = helper.pretty_print_duration(creation_delta)\n    creation_time    = str(creation_local.strftime('%A %m/%d/%Y, %H:%M:%S'))+\" \"+str(timezone)+\" (\"+str(creation_print)+\")\"\n\n    expiration_parse = parser.parse(key_info['expiration'])\n    expiration_local = expiration_parse.astimezone(timezone)\n    expiration_delta = expiration_local - local_time\n    expiration_print = helper.pretty_print_duration(expiration_delta, \"expiry\")\n    expiration_time  = str(expiration_local.strftime('%A %m/%d/%Y, %H:%M:%S'))+\" \"+str(timezone)+\" (\"+str(expiration_print)+\")\"\n\n    key_info['expiration'] = expiration_time\n    key_info['createdAt']  = creation_time\n\n    message = json.dumps(key_info)\n    return message\n\n@app.route('/api/save_key', methods=['POST'])\n@oidc.require_login\ndef save_key_page():\n    json_response = request.get_json()\n    api_key       = json_response['api_key']\n    url           = headscale.get_url()\n    file_written  = headscale.set_api_key(api_key)\n    message       = ''\n\n    if file_written:\n        # Re-read the file and get the new API key and test it\n        api_key = headscale.get_api_key()\n        test_status = headscale.test_api_key(url, api_key)\n        if test_status == 200:\n            key_info   = headscale.get_api_key_info(url, api_key)\n            expiration = key_info['expiration']\n            message = \"Key:  '\"+api_key+\"', Expiration:  \"+expiration\n            # If the key was saved successfully, test it:\n            return \"Key saved and tested:  \"+message\n        else: return \"Key failed testing.  Check your key\"\n    else: return \"Key did not save properly.  Check logs\"\n\n########################################################################################\n# Machine API Endpoints\n########################################################################################\n@app.route('/api/update_route', methods=['POST'])\n@oidc.require_login\ndef update_route_page():\n    json_response = request.get_json()\n    route_id      = escape(json_response['route_id'])\n    url           = headscale.get_url()\n    api_key       = headscale.get_api_key()\n    current_state = json_response['current_state']\n\n    return headscale.update_route(url, api_key, route_id, current_state)\n\n@app.route('/api/machine_information', methods=['POST'])\n@oidc.require_login\ndef machine_information_page():\n    json_response = request.get_json()\n    machine_id    = escape(json_response['id'])\n    url           = headscale.get_url()\n    api_key       = headscale.get_api_key()\n\n    return headscale.get_machine_info(url, api_key, machine_id)\n\n@app.route('/api/delete_machine', methods=['POST'])\n@oidc.require_login\ndef delete_machine_page():\n    json_response = request.get_json()\n    machine_id    = escape(json_response['id'])\n    url           = headscale.get_url()\n    api_key       = headscale.get_api_key()\n\n    return headscale.delete_machine(url, api_key, machine_id)\n\n@app.route('/api/rename_machine', methods=['POST'])\n@oidc.require_login\ndef rename_machine_page():\n    json_response = request.get_json()\n    machine_id    = escape(json_response['id'])\n    new_name      = escape(json_response['new_name'])\n    url           = headscale.get_url()\n    api_key       = headscale.get_api_key()\n\n    return headscale.rename_machine(url, api_key, machine_id, new_name)\n\n@app.route('/api/move_user', methods=['POST'])\n@oidc.require_login\ndef move_user_page():\n    json_response = request.get_json()\n    machine_id    = escape(json_response['id'])\n    new_user      = escape(json_response['new_user'])\n    url           = headscale.get_url()\n    api_key       = headscale.get_api_key()\n\n    return headscale.move_user(url, api_key, machine_id, new_user)\n\n@app.route('/api/set_machine_tags', methods=['POST'])\n@oidc.require_login\ndef set_machine_tags():\n    json_response = request.get_json()\n    machine_id    = escape(json_response['id'])\n    machine_tags  = json_response['tags_list']\n    url           = headscale.get_url()\n    api_key       = headscale.get_api_key()\n\n    return headscale.set_machine_tags(url, api_key, machine_id, machine_tags)\n\n@app.route('/api/register_machine', methods=['POST'])\n@oidc.require_login\ndef register_machine():\n    json_response = request.get_json()\n    machine_key   = escape(json_response['key'])\n    user          = escape(json_response['user'])\n    url           = headscale.get_url()\n    api_key       = headscale.get_api_key()\n\n    return headscale.register_machine(url, api_key, machine_key, user)\n\n########################################################################################\n# User API Endpoints\n########################################################################################\n@app.route('/api/rename_user', methods=['POST'])\n@oidc.require_login\ndef rename_user_page():\n    json_response = request.get_json()\n    old_name      = escape(json_response['old_name'])\n    new_name      = escape(json_response['new_name'])\n    url           = headscale.get_url()\n    api_key       = headscale.get_api_key()\n\n    return headscale.rename_user(url, api_key, old_name, new_name)\n\n@app.route('/api/add_user', methods=['POST'])\n@oidc.require_login\ndef add_user():\n    json_response  = request.get_json()\n    user_name      = str(escape(json_response['name']))\n    url            = headscale.get_url()\n    api_key        = headscale.get_api_key()\n    json_string    = '{\"name\": \"'+user_name+'\"}'\n\n    return headscale.add_user(url, api_key, json_string)\n\n@app.route('/api/delete_user', methods=['POST'])\n@oidc.require_login\ndef delete_user():\n    json_response  = request.get_json()\n    user_name      = str(escape(json_response['name']))\n    url            = headscale.get_url()\n    api_key        = headscale.get_api_key()\n\n    return headscale.delete_user(url, api_key, user_name)\n\n@app.route('/api/get_users', methods=['POST'])\n@oidc.require_login\ndef get_users_page():\n    url           = headscale.get_url()\n    api_key       = headscale.get_api_key()\n    \n    return headscale.get_users(url, api_key)\n\n########################################################################################\n# Pre-Auth Key API Endpoints\n########################################################################################\n@app.route('/api/add_preauth_key', methods=['POST'])\n@oidc.require_login\ndef add_preauth_key():\n    json_response  = json.dumps(request.get_json())\n    url            = headscale.get_url()\n    api_key        = headscale.get_api_key()\n\n    return headscale.add_preauth_key(url, api_key, json_response)\n\n@app.route('/api/expire_preauth_key', methods=['POST'])\n@oidc.require_login\ndef expire_preauth_key():\n    json_response  = json.dumps(request.get_json())\n    url            = headscale.get_url()\n    api_key        = headscale.get_api_key()\n\n    return headscale.expire_preauth_key(url, api_key, json_response)\n\n@app.route('/api/build_preauthkey_table', methods=['POST'])\n@oidc.require_login\ndef build_preauth_key_table():\n    json_response  = request.get_json()\n    user_name      = str(escape(json_response['name']))\n\n    return renderer.build_preauth_key_table(user_name)\n\n########################################################################################\n# Route API Endpoints\n########################################################################################\n@app.route('/api/get_routes', methods=['POST'])\n@oidc.require_login\ndef get_route_info():\n    url     = headscale.get_url()\n    api_key = headscale.get_api_key()\n\n    return headscale.get_routes(url, api_key)\n\n\n########################################################################################\n# Main thread\n########################################################################################\nif __name__ == '__main__':\n    app.run(host=\"0.0.0.0\", debug=DEBUG_STATE)\n", "metadata": {"source": "github.com/iFargle/headscale-webui", "file_path": "server.py", "language": "python", "complexity": "complex", "star_count": 710, "quality_score": 0.8, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 533}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<p align=\"center\">\n  <a href=\"http://materializecss.com/\">\n    <img src=\"http://materializecss.com/res/materialize.svg\" width=\"150\">\n  </a>\n</p>\n\n<h3 align=\"center\">MaterializeCSS</h3>\n\n<p align=\"center\">\n  Materialize, a CSS Framework based on material design.\n  <br>\n  <a href=\"http://materializecss.com/\"><strong>-- Browse the docs --</strong></a>\n  <br>\n  <br>\n  <a href=\"https://travis-ci.org/Dogfalo/materialize\">\n    <img src=\"https://travis-ci.org/Dogfalo/materialize.svg?branch=master\" alt=\"Travis CI badge\">\n  </a>\n  <a href=\"https://badge.fury.io/js/materialize-css\">\n    <img src=\"https://badge.fury.io/js/materialize-css.svg\" alt=\"npm version badge\">\n  </a>\n  <a href=\"https://cdnjs.com/libraries/materialize\">\n    <img src=\"https://img.shields.io/cdnjs/v/materialize.svg\" alt=\"CDNJS version badge\">\n  </a>\n  <a href=\"https://david-dm.org/Dogfalo/materialize\">\n    <img src=\"https://david-dm.org/Dogfalo/materialize/status.svg\" alt=\"dependencies Status badge\">\n    </a>\n  <a href=\"https://david-dm.org/Dogfalo/materialize#info=devDependencies\">\n    <img src=\"https://david-dm.org/Dogfalo/materialize/dev-status.svg\" alt=\"devDependency Status badge\">\n  </a>\n  <a href=\"https://gitter.im/Dogfalo/materialize\">\n    <img src=\"https://badges.gitter.im/Join%20Chat.svg\" alt=\"Gitter badge\">\n  </a>\n</p>\n\n## Table of Contents\n- [Quickstart](#quickstart)\n- [Documentation](#documentation)\n- [Supported Browsers](#supported-browsers)\n- [Changelog](#changelog)\n- [Testing](#testing)\n- [Contributing](#contributing)\n- [Copyright and license](#copyright-and-license)\n\n## Quickstart:\nRead the [getting started guide](http://materializecss.com/getting-started.html) for more information on how to use materialize.\n\n- [Download the latest release](https://github.com/Dogfalo/materialize/releases/latest) of materialize directly from GitHub. ([Beta](https://github.com/Dogfalo/materialize/releases/))\n- Clone the repo: `git clone https://github.com/Dogfalo/materialize.git` (Beta: `git clone -b v1-dev https://github.com/Dogfalo/materialize.git`)\n- Include the files via [cdnjs](https://cdnjs.com/libraries/materialize). More [here](http://materializecss.com/getting-started.html). ([Beta](https://cdnjs.com/libraries/materialize/1.0.0-beta))\n- Install with [npm](https://www.npmjs.com): `npm install materialize-css` (Beta: `npm install materialize-css@next`)\n- Install with [Bower](https://bower.io): `bower install materialize` ([DEPRECATED](https://bower.io/blog/2017/how-to-migrate-away-from-bower/))\n- Install with [Atmosphere](https://atmospherejs.com): `meteor add materialize:materialize` (Beta: `meteor add materialize:materialize@=1.0.0-beta`)\n\n## Documentation\nThe documentation can be found at <http://materializecss.com>. To run the documentation locally on your machine, you need [Node.js](https://nodejs.org/en/) installed on your computer.\n\n### Running documentation locally\nRun these commands to set up the documentation:\n\n```bash\ngit clone https://github.com/Dogfalo/materialize\ncd materialize\nnpm install\n```\n\nThen run `grunt monitor` to compile the documentation. When it finishes, open a new browser window and navigate to `localhost:8000`. We use [BrowserSync](https://www.browsersync.io/) to display the documentation.\n\n### Documentation for previous releases\nPrevious releases and their documentation are available for [download](https://github.com/Dogfalo/materialize/releases).\n\n## Supported Browsers:\nMaterialize is compatible with:\n\n- Chrome 35+\n- Firefox 31+\n- Safari 9+\n- Opera\n- Edge\n- IE 11+\n\n## Changelog\nFor changelogs, check out [the Releases section of materialize](https://github.com/Dogfalo/materialize/releases) or the [CHANGELOG.md](CHANGELOG.md).\n\n## Testing\nWe use Jasmine as our testing framework and we're trying to write a robust test suite for our components. If you want to help, [here's a starting guide on how to write tests in Jasmine](CONTRIBUTING.md#jasmine-testing-guide).\n\n## Contributing\nCheck out the [CONTRIBUTING document](CONTRIBUTING.md) in the root of the repository to learn how you can contribute. You can also browse the [help-wanted](https://github.com/Dogfalo/materialize/labels/help-wanted) tag in our issue tracker to find things to do.\n\n## Copyright and license\nCode Copyright 2018 Materialize. Code released under the MIT license.\n", "metadata": {"source": "github.com/iFargle/headscale-webui", "file_path": "static/README.md", "language": "markdown", "complexity": "medium", "star_count": 710, "quality_score": 0.53, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 92}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "labels: [question]\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Thanks for your interest in this project! üöÄ\n\n        Please follow these instructions, fill every question, and do every step. üôè\n\n        I'm asking this because answering questions and solving problems in GitHub is what consumes most of the time.\n\n        I end up not being able to add new features, fix bugs, review pull requests, etc. as fast as I wish because I have to spend too much time handling questions.\n\n        All that, on top of all the incredible help provided by a bunch of community members, that give a lot of their time to come here and help others.\n\n        That's a lot of work, but if more users came to help others like them just a little bit more, it would be much less effort for them (and you and me üòÖ).\n\n        By asking questions in a structured way (following this) it will be much easier to help you.\n\n        And there's a high chance that you will find the solution along the way and you won't even have to submit it and wait for an answer. üòé\n\n        As there are too many questions, I'll have to discard and close the incomplete ones. That will allow me (and others) to focus on helping people like you that follow the whole process and help us help you. ü§ì\n  - type: checkboxes\n    id: checks\n    attributes:\n      label: First Check\n      description: Please confirm and check all the following options.\n      options:\n        - label: I added a very descriptive title here.\n          required: true\n        - label: I used the GitHub search to find a similar question and didn't find it.\n          required: true\n        - label: I searched in the documentation/README.\n          required: true\n        - label: I already searched in Google \"How to do X\" and didn't find any information.\n          required: true\n        - label: I already read and followed all the tutorial in the docs/README and didn't find an answer.\n          required: true\n  - type: checkboxes\n    id: help\n    attributes:\n      label: Commit to Help\n      description: |\n        After submitting this, I commit to one of:\n\n          * Read open questions until I find 2 where I can help someone and add a comment to help there.\n          * I already hit the \"watch\" button in this repository to receive notifications and I commit to help at least 2 people that ask questions in the future.\n\n      options:\n        - label: I commit to help with one of those options üëÜ\n          required: true\n  - type: textarea\n    id: example\n    attributes:\n      label: Example Code\n      description: |\n        Please add a self-contained, [minimal, reproducible, example](https://stackoverflow.com/help/minimal-reproducible-example) with your use case.\n\n        If I (or someone) can copy it, run it, and see it right away, there's a much higher chance I (or someone) will be able to help you.\n\n      placeholder: |\n        Write your example code here.\n      render: Text\n    validations:\n      required: true\n  - type: textarea\n    id: description\n    attributes:\n      label: Description\n      description: |\n        What is the problem, question, or error?\n\n        Write a short description telling me what you are doing, what you expect to happen, and what is currently happening.\n      placeholder: |\n        * Open the browser and call the endpoint `/`.\n        * It returns a JSON with `{\"message\": \"Hello World\"}`.\n        * But I expected it to return `{\"message\": \"Hello Morty\"}`.\n    validations:\n      required: true\n  - type: dropdown\n    id: os\n    attributes:\n      label: Operating System\n      description: What operating system are you on?\n      multiple: true\n      options:\n        - Linux\n        - Windows\n        - macOS\n        - Other\n    validations:\n      required: true\n  - type: textarea\n    id: os-details\n    attributes:\n      label: Operating System Details\n      description: You can add more details about your operating system here, in particular if you chose \"Other\".\n    validations:\n      required: true\n  - type: input\n    id: python-version\n    attributes:\n      label: Python Version\n      description: |\n        What Python version are you using?\n\n        You can find the Python version with:\n\n        ```bash\n        python --version\n        ```\n    validations:\n      required: true\n  - type: textarea\n    id: context\n    attributes:\n      label: Additional Context\n      description: Add any additional context information or screenshots you think are useful.\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": ".github/DISCUSSION_TEMPLATE/questions.yml", "language": "yaml", "complexity": "medium", "star_count": 658, "quality_score": 0.5, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 119}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "blank_issues_enabled: false\ncontact_links:\n  - name: Security Contact\n    about: Please report security vulnerabilities to security@tiangolo.com\n  - name: Question or Problem\n    about: Ask a question or ask about a problem in GitHub Discussions.\n    url: https://github.com/tiangolo/uwsgi-nginx-docker/discussions/categories/questions\n  - name: Feature Request\n    about: To suggest an idea or ask about a feature, please start with a question saying what you would like to achieve. There might be a way to do it already.\n    url: https://github.com/tiangolo/uwsgi-nginx-docker/discussions/categories/questions\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": ".github/ISSUE_TEMPLATE/config.yml", "language": "yaml", "complexity": "simple", "star_count": 658, "quality_score": 0.5, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 11}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Privileged\ndescription: You are @tiangolo or he asked you directly to create an issue here. If not, check the other options. üëá\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Thanks for your interest in this project! üöÄ\n\n        If you are not @tiangolo or he didn't ask you directly to create an issue here, please start the conversation in a [Question in GitHub Discussions](https://github.com/tiangolo/uwsgi-nginx-docker/discussions/categories/questions) instead.\n  - type: checkboxes\n    id: privileged\n    attributes:\n      label: Privileged issue\n      description: Confirm that you are allowed to create an issue here.\n      options:\n        - label: I'm @tiangolo or he asked me directly to create an issue here.\n          required: true\n  - type: textarea\n    id: content\n    attributes:\n      label: Issue Content\n      description: Add the content of the issue here.\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": ".github/ISSUE_TEMPLATE/privileged.yml", "language": "yaml", "complexity": "simple", "star_count": 658, "quality_score": 0.5, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 23}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "internal:\n  - all:\n    - changed-files:\n      - any-glob-to-any-file:\n        - .github/**\n        - .gitignore\n      - all-globs-to-all-files:\n        - '!docker-images/requirements.txt'\n\nupgrade:\n  - all:\n    - changed-files:\n      - any-glob-to-any-file:\n        - docker-images/requirements.txt\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": ".github/labeler.yml", "language": "yaml", "complexity": "simple", "star_count": 658, "quality_score": 0.5, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 15}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Deploy\n\non:\n  push:\n    branches:\n      - master\n  schedule:\n    # cron every week on monday\n    - cron: \"0 0 * * 1\"\n\njobs:\n  deploy:\n    strategy:\n      matrix:\n        image:\n          - name: latest\n            python_version: \"3.12\"\n          - name: python3.12\n            python_version: \"3.12\"\n          - name: python3.11\n            python_version: \"3.11\"\n          - name: python3.10\n            python_version: \"3.10\"\n          - name: python3.9\n            python_version: \"3.9\"\n      fail-fast: true\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Set Dockerfile name\n        if: matrix.image.name != 'latest'\n        run: echo \"DOCKERFILE_NAME=${{ matrix.image.name }}\" >> $GITHUB_ENV\n      - name: Set Dockerfile name latest\n        if: matrix.image.name == 'latest'\n        run: echo \"DOCKERFILE_NAME=python${{ matrix.image.python_version }}\" >> $GITHUB_ENV\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      - name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n      - name: Get date for tags\n        run: echo \"DATE_TAG=$(date -I)\" >> \"$GITHUB_ENV\"\n      - name: Build and push\n        uses: docker/build-push-action@v6\n        with:\n          push: true\n          platforms: linux/amd64,linux/arm64\n          tags: |\n            tiangolo/uwsgi-nginx:${{ matrix.image.name }}\n            tiangolo/uwsgi-nginx:${{ matrix.image.name }}-${{ env.DATE_TAG }} \n          context: ./docker-images/\n          file: ./docker-images/${{ env.DOCKERFILE_NAME }}.dockerfile\n      - name: Docker Hub Description\n        uses: peter-evans/dockerhub-description@v4\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n          repository: tiangolo/uwsgi-nginx\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": ".github/workflows/deploy.yml", "language": "yaml", "complexity": "medium", "star_count": 658, "quality_score": 0.5, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 61}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Issue Manager\n\non:\n  schedule:\n    - cron: \"15 11 * * *\"\n  issue_comment:\n    types:\n      - created\n  issues:\n    types:\n      - labeled\n  pull_request_target:\n    types:\n      - labeled\n  workflow_dispatch:\n\npermissions:\n  issues: write\n  pull-requests: write\n\njobs:\n  issue-manager:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: tiangolo/issue-manager@0.5.1\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          config: >\n            {\n              \"answered\": {\n                \"delay\": 864000,\n                \"message\": \"Assuming the original need was handled, this will be automatically closed now. But feel free to add more comments or create new issues or PRs.\"\n              },\n              \"waiting\": {\n                \"delay\": 2628000,\n                \"message\": \"As this PR has been waiting for the original user for a while but seems to be inactive, it's now going to be closed. But if there's anyone interested, feel free to create a new PR.\"\n              }\n            }\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": ".github/workflows/issue-manager.yml", "language": "yaml", "complexity": "simple", "star_count": 658, "quality_score": 0.53, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 39}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Labels\non:\n  pull_request_target:\n    types:\n      - opened\n      - synchronize\n      - reopened\n      # For label-checker\n      - labeled\n      - unlabeled\n\njobs:\n  labeler:\n    permissions:\n      contents: read\n      pull-requests: write\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/labeler@v5\n      if: ${{ github.event.action != 'labeled' && github.event.action != 'unlabeled' }}\n    - run: echo \"Done adding labels\"\n  # Run this after labeler applied labels\n  check-labels:\n    needs:\n      - labeler\n    permissions:\n      pull-requests: read\n    runs-on: ubuntu-latest\n    steps:\n      - uses: docker://agilepathway/pull-request-label-checker:latest\n        with:\n          one_of: breaking,security,feature,bug,refactor,upgrade,docs,internal\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": ".github/workflows/labeler.yml", "language": "yaml", "complexity": "simple", "star_count": 658, "quality_score": 0.5, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 34}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Latest Changes\n\non:\n  pull_request_target:\n    branches:\n      - master\n    types:\n      - closed\n  workflow_dispatch:\n    inputs:\n      number:\n        description: PR number\n        required: true\n      debug_enabled:\n        description: 'Run the build with tmate debugging enabled (https://github.com/marketplace/actions/debugging-with-tmate)'\n        required: false\n        default: 'false'\n\njobs:\n  latest-changes:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4.1.2\n        with:\n          # To allow latest-changes to commit to master\n          token: ${{ secrets.UWSGI_NGINX_DOCKER_LATEST_CHANGES }}\n      - uses: tiangolo/latest-changes@0.3.2\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": ".github/workflows/latest-changes.yml", "language": "yaml", "complexity": "medium", "star_count": 658, "quality_score": 0.5, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 30}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Test\n\non:\n  push:\n    branches:\n      - master\n  pull_request:\n    types:\n      - opened\n      - synchronize\n  schedule:\n    # cron every week on monday\n    - cron: \"0 0 * * 1\"\n\njobs:\n  test:\n    strategy:\n      matrix:\n        image:\n          - name: latest\n            python_version: \"3.12\"\n          - name: python3.12\n            python_version: \"3.12\"\n          - name: python3.11\n            python_version: \"3.11\"\n          - name: python3.10\n            python_version: \"3.10\"\n          - name: python3.9\n            python_version: \"3.9\"\n      fail-fast: true\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Set Dockerfile name\n        if: matrix.image.name != 'latest'\n        run: echo \"DOCKERFILE_NAME=${{ matrix.image.name }}\" >> $GITHUB_ENV\n      - name: Set Dockerfile name latest\n        if: matrix.image.name == 'latest'\n        run: echo \"DOCKERFILE_NAME=python${{ matrix.image.python_version }}\" >> $GITHUB_ENV\n      - name: Build\n        uses: docker/build-push-action@v6\n        with:\n          push: false\n          tags: tiangolo/uwsgi-nginx:${{ matrix.image.name }}\n          context: ./docker-images/\n          file: ./docker-images/${{ env.DOCKERFILE_NAME }}.dockerfile\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.10\"\n      - name: Install Dependencies\n        run: python -m pip install docker pytest\n      - name: Test Image\n        run: pytest tests\n        env:\n          NAME: ${{ matrix.image.name }}\n          PYTHON_VERSION: ${{ matrix.image.python_version }}\n          SLEEP_TIME: \"5\"\n  check:\n    if: always()\n    needs:\n    - test\n    runs-on: ubuntu-latest\n    steps:\n    - name: Decide whether the needed jobs succeeded or failed\n      uses: re-actors/alls-green@release/v1\n      with:\n        jobs: ${{ toJSON(needs) }}\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": ".github/workflows/test.yml", "language": "yaml", "complexity": "simple", "star_count": 658, "quality_score": 0.5, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 69}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "[![Test](https://github.com/tiangolo/uwsgi-nginx-docker/actions/workflows/test.yml/badge.svg)](https://github.com/tiangolo/uwsgi-nginx-docker/actions/workflows/test.yml) [![Deploy](https://github.com/tiangolo/uwsgi-nginx-docker/workflows/Deploy/badge.svg)](https://github.com/tiangolo/uwsgi-nginx-docker/actions?query=workflow%3ADeploy)\n\n## Supported tags and respective `Dockerfile` links\n\n* [`python3.12`, `latest` _(Dockerfile)_](https://github.com/tiangolo/uwsgi-nginx-docker/blob/master/docker-images/python3.12.dockerfile)\n* [`python3.11`, _(Dockerfile)_](https://github.com/tiangolo/uwsgi-nginx-docker/blob/master/docker-images/python3.11.dockerfile)\n* [`python3.10`, _(Dockerfile)_](https://github.com/tiangolo/uwsgi-nginx-docker/blob/master/docker-images/python3.10.dockerfile)\n* [`python3.9`, _(Dockerfile)_](https://github.com/tiangolo/uwsgi-nginx-docker/blob/master/docker-images/python3.9.dockerfile)\n\n## Deprecated tags\n\nüö® These tags are no longer supported or maintained, they are removed from the GitHub repository, but the last versions pushed might still be available in Docker Hub if anyone has been pulling them:\n\n* `python3.8`\n* `python3.8-alpine`\n* `python3.7`\n* `python3.6`\n* `python2.7`\n\nThe last date tags for these versions are:\n\n* `python3.8-2024-10-28`\n* `python3.8-alpine-2024-03-11`\n* `python3.7-2024-10-28`\n* `python3.6-2022-11-25`\n* `python2.7-2022-11-25`\n\n---\n\n**Note**: There are [tags for each build date](https://hub.docker.com/r/tiangolo/uwsgi-nginx/tags). If you need to \"pin\" the Docker image version you use, you can select one of those tags. E.g. `tiangolo/uwsgi-nginx:python3.12-2024-11-02`.\n\n# uwsgi-nginx\n\n**Docker** image with **uWSGI** and **Nginx** for web applications in **Python** (as **Flask**) in a single container.\n\n## Description\n\nThis [**Docker**](https://www.docker.com/) image allows you to create [**Python**](https://www.python.org/) web applications that run with [**uWSGI**](https://uwsgi-docs.readthedocs.org/en/latest/) and [**Nginx**](http://nginx.org/en/) in a single container.\n\nThe combination of uWSGI with Nginx is a [common way to deploy Python web applications like Flask and Django](http://flask.pocoo.org/docs/1.0/deploying/uwsgi/). It is widely used in the industry and would give you decent performance. (*)\n\nThis image was created to be the base image for [**tiangolo/uwsgi-nginx-flask**](https://hub.docker.com/r/tiangolo/uwsgi-nginx-flask/) but could be used as the base image for any other (WSGI-based) Python web application, like Django.\n\n### * Note on performance and features\n\nIf you are starting a new project, you might benefit from a newer and faster framework based on ASGI instead of WSGI (Flask and Django are WSGI-based).\n\nYou could use an ASGI framework like:\n\n* [**FastAPI**](https://github.com/tiangolo/fastapi) (which is based on Starlette) with this Docker image: [**tiangolo/uvicorn-gunicorn-fastapi**](https://github.com/tiangolo/uvicorn-gunicorn-fastapi-docker).\n* [**Starlette**](https://github.com/encode/starlette) directly, with this Docker image: [**tiangolo/uvicorn-gunicorn-starlette**](https://github.com/tiangolo/uvicorn-gunicorn-starlette-docker).\n* Or any other ASGI framework with this Docker image: [**tiangolo/uvicorn-gunicorn**](https://github.com/tiangolo/uvicorn-gunicorn-docker).\n\nFastAPI, or Starlette, would give you about 800% (8x) the performance achievable with this image (**tiangolo/uwsgi-nginx**). [You can see the third-party benchmarks here](https://www.techempower.com/benchmarks/#section=test&runid=a979de55-980d-4721-a46f-77298b3f3923&hw=ph&test=query&l=z8kflr-v&a=2).\n\nAlso, if you want to use new technologies like WebSockets it would be easier (and *possible*) with a newer framework based on ASGI, like FastAPI or Starlette. As the standard ASGI was designed to be able to handle asynchronous code like the one needed for WebSockets.\n\n#### If you need a WSGI-based application (like Flask or Django)\n\nIf you need to use an older WSGI-based framework like Flask or Django (instead of something based on ASGI) and you need to have the best performance possible, you can use the alternative image: [**tiangolo/meinheld-gunicorn**](https://github.com/tiangolo/meinheld-gunicorn-docker).\n\n**tiangolo/meinheld-gunicorn** will give you about 400% (4x) the performance of this image.\n\n---\n\n**GitHub repo**: [https://github.com/tiangolo/uwsgi-nginx-docker](https://github.com/tiangolo/uwsgi-nginx-docker)\n\n**Docker Hub image**: [https://hub.docker.com/r/tiangolo/uwsgi-nginx/](https://hub.docker.com/r/tiangolo/uwsgi-nginx/)\n\n## üö® WARNING: You Probably Don't Need this Docker Image\n\nYou are probably using **Kubernetes** or similar tools. In that case, you probably **don't need this image** (or any other **similar base image**). You are probably better off **building a Docker image from scratch**.\n\n---\n\nIf you have a cluster of machines with **Kubernetes**, Docker Swarm Mode, Nomad, or other similar complex system to manage distributed containers on multiple machines, then you will probably want to **handle replication** at the **cluster level** instead of using a **process manager** in each container that starts multiple **worker processes**, which is what this Docker image does.\n\nIn those cases (e.g. using Kubernetes) you would probably want to build a **Docker image from scratch**, installing your dependencies, and running **a single process** instead of this image.\n\nFor example, using [Gunicorn](https://gunicorn.org/) you could have a file `app/gunicorn_conf.py` with:\n\n```Python\n# Gunicorn config variables\nloglevel = \"info\"\nerrorlog = \"-\"  # stderr\naccesslog = \"-\"  # stdout\nworker_tmp_dir = \"/dev/shm\"\ngraceful_timeout = 120\ntimeout = 120\nkeepalive = 5\nthreads = 3\n```\n\nAnd then you could have a `Dockerfile` with:\n\n```Dockerfile\nFROM python:3.9\n\nWORKDIR /code\n\nCOPY ./requirements.txt /code/requirements.txt\n\nRUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\n\nCOPY ./app /code/app\n\nCMD [\"gunicorn\", \"--conf\", \"app/gunicorn_conf.py\", \"--bind\", \"0.0.0.0:80\", \"app.main:app\"]\n```\n\nYou can read more about these ideas in the [FastAPI documentation about: FastAPI in Containers - Docker](https://fastapi.tiangolo.com/deployment/docker/#replication-number-of-processes) as the same ideas would apply to other web applications in containers.\n\n## When to Use this Docker Image\n\n### A Simple App\n\nYou could want a process manager running multiple worker processes in the container if your application is **simple enough** that you don't need (at least not yet) to fine-tune the number of processes too much, and you can just use an automated default, and you are running it on a **single server**, not a cluster.\n\n### Docker Compose\n\nYou could be deploying to a **single server** (not a cluster) with **Docker Compose**, so you wouldn't have an easy way to manage replication of containers (with Docker Compose) while preserving the shared network and **load balancing**.\n\nThen you could want to have **a single container** with a **process manager** starting **several worker processes** inside, as this Docker image does.\n\n### Prometheus and Other Reasons\n\nYou could also have **other reasons** that would make it easier to have a **single container** with **multiple processes** instead of having **multiple containers** with **a single process** in each of them.\n\nFor example (depending on your setup) you could have some tool like a Prometheus exporter in the same container that should have access to **each of the requests** that come.\n\nIn this case, if you had **multiple containers**, by default, when Prometheus came to **read the metrics**, it would get the ones for **a single container each time** (for the container that handled that particular request), instead of getting the **accumulated metrics** for all the replicated containers.\n\nThen, in that case, it could be simpler to have **one container** with **multiple processes**, and a local tool (e.g. a Prometheus exporter) on the same container collecting Prometheus metrics for all the internal processes and exposing those metrics on that single container.\n\n---\n\nRead more about it all in the [FastAPI documentation about: FastAPI in Containers - Docker](https://fastapi.tiangolo.com/deployment/docker/), as the same concepts apply to other web applications in containers.\n\n## How to use\n\nYou don't have to clone this repo.\n\nYou can use this image as a base image for other images.\n\nAssuming you have a file `requirements.txt`, you could have a `Dockerfile` like this:\n\n```Dockerfile\nFROM tiangolo/uwsgi-nginx:python3.12\n\nCOPY ./requirements.txt /app/requirements.txt\n\nRUN pip install --no-cache-dir --upgrade -r /app/requirements.txt\n\nCOPY ./app /app\n\n# Your Dockerfile code...\n```\n\n* By default it will try to find a uWSGI config file in `/app/uwsgi.ini`.\n\n* That `uwsgi.ini` file will make it try to run a Python file in `/app/main.py`.\n\nIf you are building a **Flask** web application you should use instead [**tiangolo/uwsgi-nginx-flask**](https://hub.docker.com/r/tiangolo/uwsgi-nginx-flask/).\n\n## Advanced usage\n\n### Custom app directory\n\nIf you need to use a directory for your app different than `/app`, you can override the uWSGI config file path with an environment variable `UWSGI_INI`, and put your custom `uwsgi.ini` file there.\n\nFor example, if you needed to have your application directory in `/application` instead of `/app`, your `Dockerfile` would look like:\n\n```Dockerfile\nFROM tiangolo/uwsgi-nginx:python3.12\n\nENV UWSGI_INI /application/uwsgi.ini\n\nCOPY ./application /application\nWORKDIR /appapplication\n```\n\nAnd your `uwsgi.ini` file in `./application/uwsgi.ini` would contain:\n\n```ini\n[uwsgi]\nwsgi-file=/application/main.py\n```\n\n**Note**: it's important to include the `WORKDIR` option, otherwise uWSGI will start the application in `/app`.\n\n### Custom uWSGI process number\n\nBy default, the image starts with 2 uWSGI processes running. When the server is experiencing a high load, it creates up to 16 uWSGI processes to handle it on demand.\n\nIf you need to configure these numbers you can use environment variables.\n\nThe starting number of uWSGI processes is controlled by the variable `UWSGI_CHEAPER`, by default set to `2`.\n\nThe maximum number of uWSGI processes is controlled by the variable `UWSGI_PROCESSES`, by default set to `16`.\n\nHave in mind that `UWSGI_CHEAPER` must be lower than `UWSGI_PROCESSES`.\n\nSo, if, for example, you need to start with 4 processes and grow to a maximum of 64, your `Dockerfile` could look like:\n\n```Dockerfile\nFROM tiangolo/uwsgi-nginx:python3.12\n\nENV UWSGI_CHEAPER 4\nENV UWSGI_PROCESSES 64\n\nCOPY ./app /app\n```\n\n### Custom max upload size\n\nIn this image, Nginx is configured to allow unlimited upload file sizes. This is done because by default a simple Python server would allow that, so that's the simplest behavior a developer would expect.\n\nIf you need to restrict the maximum upload size in Nginx, you can add an environment variable `NGINX_MAX_UPLOAD` and assign a value corresponding to the [standard Nginx config `client_max_body_size`](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size).\n\nFor example, if you wanted to set the maximum upload file size to 1 MB (the default in a normal Nginx installation), you would need to set the `NGINX_MAX_UPLOAD` environment variable to the value `1m`. Then the image would take care of adding the corresponding configuration file (this is done by the `entrypoint.sh`).\n\nSo, your `Dockerfile` would look something like:\n\n```Dockerfile\nFROM tiangolo/uwsgi-nginx:python3.12\n\nENV NGINX_MAX_UPLOAD 1m\n\nCOPY ./app /app\n```\n\n### Custom listen port\n\nBy default, the container made from this image will listen on port 80.\n\nTo change this behavior, set the `LISTEN_PORT` environment variable.\n\nYou might also need to create the respective `EXPOSE` Docker instruction.\n\nYou can do that in your `Dockerfile`, it would look something like:\n\n```Dockerfile\nFROM tiangolo/uwsgi-nginx:python3.12\n\nENV LISTEN_PORT 8080\n\nEXPOSE 8080\n\nCOPY ./app /app\n```\n\n### Custom `/app/prestart.sh`\n\nIf you need to run anything before starting the app, you can add a file `prestart.sh` to the directory `/app`. The image will automatically detect and run it before starting everything.\n\nFor example, if you want to add database migrations that are run on startup (e.g. with Alembic, or Django migrations), before starting the app, you could create a `./app/prestart.sh` file in your code directory (that will be copied by your `Dockerfile`) with:\n\n```bash\n#! /usr/bin/env bash\n\n# Let the DB start\nsleep 10;\n# Run migrations\nalembic upgrade head\n```\n\nand it would wait 10 seconds to give the database some time to start and then run that `alembic` command (you could update that to run Django migrations or any other tool you need).\n\nIf you need to run a Python script before starting the app, you could make the `/app/prestart.sh` file run your Python script, with something like:\n\n```bash\n#! /usr/bin/env bash\n\n# Run custom Python script before starting\npython /app/my_custom_prestart_script.py\n```\n\n**Note**: The image uses `.` to run the script (as in `. /app/prestart.sh`), so for example, environment variables would persist. If you don't understand the previous sentence, you probably don't need it.\n\n### Custom Nginx processes number\n\nBy default, Nginx will start one \"worker process\".\n\nIf you want to set a different number of Nginx worker processes you can use the environment variable `NGINX_WORKER_PROCESSES`.\n\nYou can use a specific single number, e.g.:\n\n```Dockerfile\nENV NGINX_WORKER_PROCESSES 2\n```\n\nor you can set it to the keyword `auto` and it will try to autodetect the number of CPUs available and use that for the number of workers.\n\nFor example, using `auto`, your Dockerfile could look like:\n\n```Dockerfile\nFROM tiangolo/uwsgi-nginx:python3.12\n\nENV NGINX_WORKER_PROCESSES auto\n\nCOPY ./app /app\n```\n\n### Custom Nginx maximum connections per worker\n\nBy default, Nginx will start with a maximum limit of 1024 connections per worker.\n\nIf you want to set a different number you can use the environment variable `NGINX_WORKER_CONNECTIONS`, e.g:\n\n```Dockerfile\nENV NGINX_WORKER_CONNECTIONS 2048\n```\n\nIt cannot exceed the current limit on the maximum number of open files. See how to configure it in the next section.\n\n### Custom Nginx maximum open files\n\nThe number connections per Nginx worker cannot exceed the limit on the maximum number of open files.\n\nYou can change the limit of open files with the environment variable `NGINX_WORKER_OPEN_FILES`, e.g.:\n\n```Dockerfile\nENV NGINX_WORKER_OPEN_FILES 2048\n```\n\n### Customizing Nginx additional configurations\n\nIf you need to configure Nginx further, you can add `*.conf` files to `/etc/nginx/conf.d/` in your `Dockerfile`.\n\nJust have in mind that the default configurations are created during startup in a file at `/etc/nginx/conf.d/nginx.conf` and `/etc/nginx/conf.d/upload.conf`. So you shouldn't overwrite them. You should name your `*.conf` file with something different than `nginx.conf` or `upload.conf`, for example: `custom.conf`.\n\n**Note**: if you are customizing Nginx, maybe copying configurations from a blog or a StackOverflow answer, have in mind that you probably need to use the [configurations specific to uWSGI](http://nginx.org/en/docs/http/ngx_http_uwsgi_module.html), instead of those for other modules, like for example, `ngx_http_fastcgi_module`.\n\n### Overriding Nginx configuration completely\n\nIf you need to configure Nginx even further, completely overriding the defaults, you can add a custom Nginx configuration to `/app/nginx.conf`.\n\nIt will be copied to `/etc/nginx/nginx.conf` and used instead of the generated one.\n\nHave in mind that, in that case, this image won't generate any of the Nginx configurations, it will only copy and use your configuration file.\n\nThat means that all the environment variables described above that are specific to Nginx won't be used.\n\nIt also means that it won't use additional configurations from files in `/etc/nginx/conf.d/*.conf`, unless you explicitly have a section in your custom file `/app/nginx.conf` with:\n\n```conf\ninclude /etc/nginx/conf.d/*.conf;\n```\n\nIf you want to add a custom `/app/nginx.conf` file but don't know where to start from, you can use [the `nginx.conf` used for the tests](https://github.com/tiangolo/uwsgi-nginx-docker/blob/master/tests/test_02_app/custom_nginx_app/app/nginx.conf) and customize it or modify it further.\n\n## Technical details\n\nThe combination of uWSGI with Nginx is a [common way to deploy Python web applications](http://flask.pocoo.org/docs/1.0/deploying/uwsgi/).\n\nRoughly:\n\n* **Nginx** is a web server, it takes care of the HTTP connections and also can serve static files directly and more efficiently.\n\n* **uWSGI** is an application server, that's what runs your Python code and it talks with Nginx.\n\n* **Your Python code** has the actual web application, and is run by uWSGI.\n\nThis image takes advantage of already slim and optimized existing Docker images (based on Debian as [recommended by Docker](https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/)) and implements Docker best practices.\n\nIt uses the official Python Docker image, installs uWSGI and on top of that, with the least amount of modifications, adds the official Nginx image (as of 2016-02-14).\n\nAnd it controls all these processes with Supervisord.\n\n---\n\nThere's the rule of thumb that you should have \"one process per container\".\n\nThat helps, for example, isolating an app and its database in different containers.\n\nBut if you want to have a \"micro-services\" approach you may want to [have more than one process in one container](https://valdhaus.co/writings/docker-misconceptions/) if they are all related to the same \"service\", and you may want to include your Flask code, uWSGI and Nginx in the same container (and maybe run another container with your database).\n\nThat's the approach taken in this image.\n\n---\n\nThis image has a default sample \"Hello World\" app in the container's `/app` directory using the example in the [uWSGI documentation](http://uwsgi-docs.readthedocs.org/en/latest/WSGIquickstart.html).\n\nYou probably want to override it or delete it in your project.\n\nIt is there in case you run this image by itself and not as a base image for your own `Dockerfile`, so that you get a sample app without errors.\n\n## üö® Alpine Python Warning\n\nIn short: You probably shouldn't use Alpine for Python projects, instead use the `slim` Docker image versions.\n\n---\n\nDo you want more details? Continue reading üëá\n\nAlpine is more useful for other languages where you build a static binary in one Docker image stage (using multi-stage Docker building) and then copy it to a simple Alpine image, and then just execute that binary. For example, using Go.\n\nBut for Python, as Alpine doesn't use the standard tooling used for building Python extensions, when installing packages, in many cases Python (`pip`) won't find a precompiled installable package (a \"wheel\") for Alpine. And after debugging lots of strange errors you will realize that you have to install a lot of extra tooling and build a lot of dependencies just to use some of these common Python packages. üò©\n\nThis means that, although the original Alpine image might have been small, you end up with a an image with a size comparable to the size you would have gotten if you had just used a standard Python image (based on Debian), or in some cases even larger. ü§Ø\n\nAnd in all those cases, it will take much longer to build, consuming much more resources, building dependencies for longer, and also increasing its carbon footprint, as you are using more CPU time and energy for each build. üå≥\n\nIf you want slim Python images, you should instead try and use the `slim` versions that are still based on Debian, but are smaller. ü§ì\n\n## Tests\n\nAll the image tags, configurations, environment variables and application options are tested.\n\n## Updates\n\nUpdates are announced in the releases.\n\nYou can click the \"watch\" button at the top right and select \"Releases only\" to receive an email notification when there's a new release.\n\n## Release Notes\n\n### Latest Changes\n\n#### Features\n\n* ‚ôªÔ∏è Add support for Python 3.12. PR [#228](https://github.com/tiangolo/uwsgi-nginx-docker/pull/228) by [@tiangolo](https://github.com/tiangolo).\n* üîß Avoid creating unnecessary `*.pyc` files with `PYTHONDONTWRITEBYTECODE=1` and ensure logs are printed immediately with `PYTHONUNBUFFERED=1`. PR [#208](https://github.com/tiangolo/uwsgi-nginx-docker/pull/208) by [@estebanx64](https://github.com/estebanx64).\n\n#### Refactors\n\n* ‚ôªÔ∏è Do not `EXPOSE` ports `80` and `443` by default as they can be customized. PR [#227](https://github.com/tiangolo/uwsgi-nginx-docker/pull/227) by [@tiangolo](https://github.com/tiangolo).\n\n#### Upgrades\n\n* ‚¨Ü Bump uwsgi from 2.0.29 to 2.0.30. PR [#240](https://github.com/tiangolo/uwsgi-nginx-docker/pull/240) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨Ü Bump uwsgi from 2.0.28 to 2.0.29. PR [#237](https://github.com/tiangolo/uwsgi-nginx-docker/pull/237) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨ÜÔ∏è Upgrade Docker images to Debian Bookworm and latest Nginx. PR [#238](https://github.com/tiangolo/uwsgi-nginx-docker/pull/238) by [@tiangolo](https://github.com/tiangolo).\n* ‚¨Ü Bump uwsgi from 2.0.26 to 2.0.28. PR [#232](https://github.com/tiangolo/uwsgi-nginx-docker/pull/232) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* üî• Drop support for Python 3.7 and 3.8. PR [#233](https://github.com/tiangolo/uwsgi-nginx-docker/pull/233) by [@tiangolo](https://github.com/tiangolo).\n* ‚¨Ü Bump uwsgi from 2.0.22 to 2.0.26. PR [#210](https://github.com/tiangolo/uwsgi-nginx-docker/pull/210) by [@dependabot[bot]](https://github.com/apps/dependabot).\n\n#### Internal\n\n* üë∑ Add CI Labeler. PR [#236](https://github.com/tiangolo/uwsgi-nginx-docker/pull/236) by [@tiangolo](https://github.com/tiangolo).\n* ‚¨Ü Bump tiangolo/latest-changes from 0.3.1 to 0.3.2. PR [#235](https://github.com/tiangolo/uwsgi-nginx-docker/pull/235) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* üî• Remove old unused files. PR [#234](https://github.com/tiangolo/uwsgi-nginx-docker/pull/234) by [@tiangolo](https://github.com/tiangolo).\n* ‚¨Ü Bump tiangolo/issue-manager from 0.5.0 to 0.5.1. PR [#229](https://github.com/tiangolo/uwsgi-nginx-docker/pull/229) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨Ü Bump docker/build-push-action from 5 to 6. PR [#225](https://github.com/tiangolo/uwsgi-nginx-docker/pull/225) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* üë∑ Update `issue-manager.yml`. PR [#226](https://github.com/tiangolo/uwsgi-nginx-docker/pull/226) by [@tiangolo](https://github.com/tiangolo).\n* ‚¨Ü Bump docker/login-action from 1 to 3. PR [#204](https://github.com/tiangolo/uwsgi-nginx-docker/pull/204) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨Ü Bump peter-evans/dockerhub-description from 3 to 4. PR [#205](https://github.com/tiangolo/uwsgi-nginx-docker/pull/205) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨Ü Bump docker/setup-buildx-action from 1 to 3. PR [#206](https://github.com/tiangolo/uwsgi-nginx-docker/pull/206) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨Ü Bump docker/build-push-action from 2 to 5. PR [#207](https://github.com/tiangolo/uwsgi-nginx-docker/pull/207) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* üë∑ Update `latest-changes` GitHub Action. PR [#214](https://github.com/tiangolo/uwsgi-nginx-docker/pull/214) by [@tiangolo](https://github.com/tiangolo).\n* üë∑ Update issue-manager.yml GitHub Action permissions. PR [#213](https://github.com/tiangolo/uwsgi-nginx-docker/pull/213) by [@tiangolo](https://github.com/tiangolo).\n* üîß Add GitHub templates for discussions and issues, and security policy. PR [#203](https://github.com/tiangolo/uwsgi-nginx-docker/pull/203) by [@alejsdev](https://github.com/alejsdev).\n* üîß Update `latest-changes.yml`. PR [#201](https://github.com/tiangolo/uwsgi-nginx-docker/pull/201) by [@alejsdev](https://github.com/alejsdev).\n\n### 2.1.0\n\n#### Features\n\n* ‚ú® Add support for multiarch builds, including ARM (e.g. Mac M1). PR [#200](https://github.com/tiangolo/uwsgi-nginx-docker/pull/200) by [@tiangolo](https://github.com/tiangolo).\n\n#### Refactors\n\n* üî• Remove support for Alpine. PR [#198](https://github.com/tiangolo/uwsgi-nginx-docker/pull/198) by [@tiangolo](https://github.com/tiangolo).\n\n#### Upgrades\n\n* ‚¨ÜÔ∏è Bump uwsgi from 2.0.21 to 2.0.22 in /docker-images. PR [#188](https://github.com/tiangolo/uwsgi-nginx-docker/pull/188) by [@dependabot[bot]](https://github.com/apps/dependabot).\n\n#### Docs\n\n* üìù Update test badge in `README.md`. PR [#199](https://github.com/tiangolo/uwsgi-nginx-docker/pull/199) by [@alejsdev](https://github.com/alejsdev).\n\n#### Internal\n\n* üë∑ Update dependabot. PR [#183](https://github.com/tiangolo/uwsgi-nginx-docker/pull/183) by [@tiangolo](https://github.com/tiangolo).\n* üë∑ Update latest changes token. PR [#180](https://github.com/tiangolo/uwsgi-nginx-docker/pull/180) by [@tiangolo](https://github.com/tiangolo).\n* üë∑ Add GitHub Action for Docker Hub description. PR [#172](https://github.com/tiangolo/uwsgi-nginx-docker/pull/172) by [@tiangolo](https://github.com/tiangolo).\n* ‚¨Ü Bump actions/checkout from 3.1.0 to 4.1.2. PR [#197](https://github.com/tiangolo/uwsgi-nginx-docker/pull/197) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨Ü Bump tiangolo/issue-manager from 0.4.1 to 0.5.0. PR [#193](https://github.com/tiangolo/uwsgi-nginx-docker/pull/193) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨Ü Bump peter-evans/dockerhub-description from 3 to 4. PR [#192](https://github.com/tiangolo/uwsgi-nginx-docker/pull/192) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨Ü Bump actions/setup-python from 4 to 5. PR [#189](https://github.com/tiangolo/uwsgi-nginx-docker/pull/189) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨Ü Bump tiangolo/issue-manager from 0.4.0 to 0.4.1. PR [#191](https://github.com/tiangolo/uwsgi-nginx-docker/pull/191) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* üë∑ Update latest-changes GitHub Action. PR [#182](https://github.com/tiangolo/uwsgi-nginx-docker/pull/182) by [@tiangolo](https://github.com/tiangolo).\n\n### 2.0.0\n\nHighlights of this release:\n\n* Support for Python 3.10, 3.11, and 3.9.\n* Deprecation of Python 3.6 and 2.7.\n    * The last Python 3.6 and 2.7 images are available in Docker Hub, but they won't be updated or maintained anymore.\n    * The last images with a date tag are `python3.6-2022-11-25` and `python2.7-2022-11-25`.\n* Upgraded versions of all the dependencies.\n* Small improvements and fixes.\n\n#### Features\n\n* ‚ú® Add support for Python 3.11. PR [#171](https://github.com/tiangolo/uwsgi-nginx-docker/pull/171) by [@tiangolo](https://github.com/tiangolo).\n* ‚ú® Add support for Python 3.10 and upgrade uWSGI to `2.0.20`. PR [#127](https://github.com/tiangolo/uwsgi-nginx-docker/pull/127) by [@tiangolo](https://github.com/tiangolo).\n* ‚¨ÜÔ∏è Update pip install command with flag --no-cache-dir to reduce disk used. PR [#120](https://github.com/tiangolo/uwsgi-nginx-docker/pull/120) by [@tiangolo](https://github.com/tiangolo).\n* ‚ú® Quit Supervisor on errors, to allow orchestrators to handle it. PR [#110](https://github.com/tiangolo/uwsgi-nginx-docker/pull/110) by [@tiangolo](https://github.com/tiangolo).\n* ‚ú® Add Python 3.9. PR [#101](https://github.com/tiangolo/uwsgi-nginx-docker/pull/101) by [@sjadema](https://github.com/sjadema).\n\n#### Breaking Changes\n\n* üî• Deprecate and remove Python 3.6 and 2.7. PR [#164](https://github.com/tiangolo/uwsgi-nginx-docker/pull/164) by [@tiangolo](https://github.com/tiangolo).\n* üî• Remove support for Python 2.7. PR [#123](https://github.com/tiangolo/uwsgi-nginx-docker/pull/123) by [@tiangolo](https://github.com/tiangolo).\n\n#### Upgrades\n\n* ‚¨ÜÔ∏è Upgrade Nginx to the latest version 1.23.2, and Debian to bullseye. PR [#163](https://github.com/tiangolo/uwsgi-nginx-docker/pull/163) by [@tiangolo](https://github.com/tiangolo).\n* ‚¨ÜÔ∏è Bump uwsgi from 2.0.20 to 2.0.21. PR [#159](https://github.com/tiangolo/uwsgi-nginx-docker/pull/159) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨Ü Upgrade Nginx to version 1.21.6 and Alpine to version 3.13. PR [#148](https://github.com/tiangolo/uwsgi-nginx-docker/pull/148) by [@haley-comet](https://github.com/haley-comet).\n* ‚¨Ü Upgrade Nginx to the latest version of the official images. PR [#107](https://github.com/tiangolo/uwsgi-nginx-docker/pull/107) by [@tiangolo](https://github.com/tiangolo).\n\n#### Docs\n\n* üìù Add note to discourage Alpine with Python. PR [#124](https://github.com/tiangolo/uwsgi-nginx-docker/pull/124) by [@tiangolo](https://github.com/tiangolo).\n* üìù Add Kubernetes warning, when to use this image. PR [#122](https://github.com/tiangolo/uwsgi-nginx-docker/pull/122) by [@tiangolo](https://github.com/tiangolo).\n* ‚úèÔ∏è Fix typo duplicate \"Note\" in Readme. PR [#121](https://github.com/tiangolo/uwsgi-nginx-docker/pull/121) by [@tiangolo](https://github.com/tiangolo).\n* üêõ Fix broken link to TechEmpower benchmarks. PR [#96](https://github.com/tiangolo/uwsgi-nginx-docker/pull/96) by [@tiangolo](https://github.com/tiangolo).\n\n#### Internal\n\n* ‚¨ÜÔ∏è Update autoflake requirement from ^1.3.1 to ^2.0.0. PR [#166](https://github.com/tiangolo/uwsgi-nginx-docker/pull/166) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨ÜÔ∏è Update mypy requirement from ^0.971 to ^0.991. PR [#167](https://github.com/tiangolo/uwsgi-nginx-docker/pull/167) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨ÜÔ∏è Update docker requirement from ^5.0.3 to ^6.0.1. PR [#168](https://github.com/tiangolo/uwsgi-nginx-docker/pull/168) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨ÜÔ∏è Update black requirement from ^20.8b1 to ^22.10. PR [#169](https://github.com/tiangolo/uwsgi-nginx-docker/pull/169) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨ÜÔ∏è Upgrade CI OS. PR [#170](https://github.com/tiangolo/uwsgi-nginx-docker/pull/170) by [@tiangolo](https://github.com/tiangolo).\n* üîß Update Dependabot config. PR [#165](https://github.com/tiangolo/uwsgi-nginx-docker/pull/165) by [@tiangolo](https://github.com/tiangolo).\n* ‚¨ÜÔ∏è Bump tiangolo/issue-manager from 0.2.0 to 0.4.0. PR [#112](https://github.com/tiangolo/uwsgi-nginx-docker/pull/112) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* üë∑ Add scheduled CI. PR [#162](https://github.com/tiangolo/uwsgi-nginx-docker/pull/162) by [@tiangolo](https://github.com/tiangolo).\n* üë∑ Add alls-green GitHub Action. PR [#161](https://github.com/tiangolo/uwsgi-nginx-docker/pull/161) by [@tiangolo](https://github.com/tiangolo).\n* ‚¨ÜÔ∏è Update black requirement from ^19.10b0 to ^20.8b1. PR [#116](https://github.com/tiangolo/uwsgi-nginx-docker/pull/116) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨ÜÔ∏è Update isort requirement from ^4.3.21 to ^5.8.0. PR [#118](https://github.com/tiangolo/uwsgi-nginx-docker/pull/118) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨ÜÔ∏è Update docker requirement from ^4.2.0 to ^5.0.3. PR [#126](https://github.com/tiangolo/uwsgi-nginx-docker/pull/126) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨ÜÔ∏è Bump actions/setup-python from 1 to 4.1.0. PR [#155](https://github.com/tiangolo/uwsgi-nginx-docker/pull/155) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨ÜÔ∏è Update mypy requirement from ^0.770 to ^0.971. PR [#156](https://github.com/tiangolo/uwsgi-nginx-docker/pull/156) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨ÜÔ∏è Update pytest requirement from ^5.4.1 to ^7.0.1. PR [#138](https://github.com/tiangolo/uwsgi-nginx-docker/pull/138) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* ‚¨ÜÔ∏è Bump actions/checkout from 2 to 3.1.0. PR [#157](https://github.com/tiangolo/uwsgi-nginx-docker/pull/157) by [@dependabot[bot]](https://github.com/apps/dependabot).\n* üîß Run tests only on PRs or when pushing on master to avoid double CI. PR [#149](https://github.com/tiangolo/uwsgi-nginx-docker/pull/149) by [@tiangolo](https://github.com/tiangolo).\n* üë∑ Update Latest Changes GitHub Action. PR [#119](https://github.com/tiangolo/uwsgi-nginx-docker/pull/119) by [@tiangolo](https://github.com/tiangolo).\n* üë∑ Add Dependabot and external dependencies, to get automatic upgrade PRs. PR [#111](https://github.com/tiangolo/uwsgi-nginx-docker/pull/111) by [@tiangolo](https://github.com/tiangolo).\n* üë∑ Add GitHub Action latest-changes, update issue-manager. PR [#92](https://github.com/tiangolo/uwsgi-nginx-docker/pull/92) by [@tiangolo](https://github.com/tiangolo).\n* Fix Python 3.8 Alpine environment for installed packages. PR [#84](https://github.com/tiangolo/uwsgi-nginx-docker/pull/84).\n\n### 1.4.0\n\n* Add [GitHub Sponsors](https://github.com/sponsors/tiangolo) button.\n* Add new image for Python 3.8, and new image for Python 3.8 on Alpine. PR [#83](https://github.com/tiangolo/uwsgi-nginx-docker/pull/83).\n* Upgrade Nginx to latest version, `1.17.10`, based on latest Debian, Buster. PR [#82](https://github.com/tiangolo/uwsgi-nginx-docker/pull/82).\n* Remove support for Python 3.5. PR [#81](https://github.com/tiangolo/uwsgi-nginx-docker/pull/81).\n\n### 1.3.0\n\n* This is the last version to support:\n    * Debian Stretch (before upgrading to Buster).\n    * Python 3.5.\n    * Alpine 3.7, 3.8, 3.9 (before upgrading to Alpine 3.11).\n    * Alpine in older versions of Python, 2.7 and 3.6 (Before upgrading to Python 3.8).\n    * If you need any of those, make sure to use a tag for the build date `2020-05-04`.\n* Refactor build set up:\n    * Re-use code and configs.\n    * Migrate to GitHub Actions.\n    * Simplify tests.\n    * PR [#78](https://github.com/tiangolo/uwsgi-nginx-docker/pull/78).\n* Migrate Travis to .com, update badge. PR [#77](https://github.com/tiangolo/uwsgi-nginx-docker/pull/77).\n\n### 1.2.0\n\n* 2019-10-14:\n    * Refactor and simplify test scripts. PR [#66](https://github.com/tiangolo/uwsgi-nginx-docker/pull/66).\n* 2019-09-28:\n    * Refactor build scripts and add image tags for each build date, like `tiangolo/uwsgi-nginx:python3.7-2019-09-28`. PR [#65](https://github.com/tiangolo/uwsgi-nginx-docker/pull/65).\n\n* Upgrade Travis. PR [#60](https://github.com/tiangolo/uwsgi-nginx-docker/pull/60).\n\n### 1.1.0\n\n* Added support for `/app/prestart.sh` script to run arbitrary code before starting the app (for example, Alembic - SQLAlchemy migrations). The [documentation for the `/app/prestart.sh` is in the main README](https://github.com/tiangolo/uwsgi-nginx-docker#custom-appprestartsh). [PR #59](https://github.com/tiangolo/uwsgi-nginx-docker/pull/59).\n\n### 1.0.0\n\n* 2019-05-04:\n    * Add Alpine Linux 3.9. PR [#55](https://github.com/tiangolo/uwsgi-nginx-docker/pull/55) by [evilgoldfish](https://github.com/evilgoldfish).\n    * Build images using Travis matrix to improve development/testing speed. Needed for some recent PRs. [PR #58](https://github.com/tiangolo/uwsgi-nginx-docker/pull/58).\n\n* 2019-02-02:\n    * The Nginx configurations are generated dynamically from the entrypoint, instead of modifying pre-existing files. [PR #50](https://github.com/tiangolo/uwsgi-nginx-docker/pull/50).\n    * Support for a completely custom `/app/nginx.conf` file that overrides the generated one. [PR #51](https://github.com/tiangolo/uwsgi-nginx-docker/pull/51).\n\n* 2018-11-23: New Alpine 3.8 images for Python 2.7, Python 3.6 and Python 3.7 (Python 3.7 temporarily disabled). Thanks to [philippfreyer](https://github.com/philippfreyer) in [PR #45](https://github.com/tiangolo/uwsgi-nginx-docker/pull/45)\n\n* 2018-09-22: New Python 3.7 versions, standard and Alpine based. Thanks to [desaintmartin](https://github.com/desaintmartin) in [this PR](https://github.com/tiangolo/uwsgi-nginx-docker/pull/39).\n\n* 2018-06-22: You can now use `NGINX_WORKER_CONNECTIONS` to set the maximum number of Nginx worker connections and `NGINX_WORKER_OPEN_FILES` to set the maximum number of open files. Thanks to [ronlut](https://github.com/ronlut) in [this PR](https://github.com/tiangolo/uwsgi-nginx-docker/pull/26).\n\n* 2018-06-22: Make uWSGI require an app to run, instead of going in \"full dynamic mode\" while there was an error. Supervisord doesn't terminate itself but tries to restart uWSGI and shows the errors. Uses `need-app` as suggested by [luckydonald](https://github.com/luckydonald) in [this comment](https://github.com/tiangolo/uwsgi-nginx-flask-docker/issues/3#issuecomment-321991279).\n\n* 2018-06-22: Correctly handled graceful shutdown of uWSGI and Nginx. Thanks to [desaintmartin](https://github.com/desaintmartin) in [this PR](https://github.com/tiangolo/uwsgi-nginx-docker/pull/30).\n\n* 2018-02-04: It's now possible to set the number of Nginx worker processes with the environment variable `NGINX_WORKER_PROCESSES`. Thanks to [naktinis](https://github.com/naktinis) in [this PR](https://github.com/tiangolo/uwsgi-nginx-docker/pull/22).\n\n* 2018-01-14: There are now two Alpine based versions, `python2.7-alpine3.7` and `python3.6-alpine3.7`.\n\n* 2017-12-08: Now you can configure which port the container should listen on, using the environment variable `LISTEN_PORT` thanks to [tmshn](https://github.com/tmshn) in [this PR](https://github.com/tiangolo/uwsgi-nginx-docker/pull/16).\n\n* 2017-08-09: You can set a custom maximum upload file size using an environment variable `NGINX_MAX_UPLOAD`, by default it has a value of `0`, that allows unlimited upload file sizes. This differs from Nginx's default value of 1 MB. It's configured this way because that's the simplest experience a developer that is not expert in Nginx would expect.\n\n* 2017-08-09: Now you can override where to look for the `uwsgi.ini` file, and with that, change the default directory from `/app` to something else, using the envirnoment variable `UWSGI_INI`.\n\n* 2017-08-08: There's a new `latest` tag image, just to show a warning for those still using `latest` for Python 2.7 web applications. As of now, [everyone](https://www.python.org/dev/peps/pep-0373/) [should be](http://flask.pocoo.org/docs/0.12/python3/#python3-support) [using Python 3](https://docs.djangoproject.com/en/1.11/faq/install/#what-python-version-should-i-use-with-django).\n\n* 2017-08-08: Supervisord now terminates uWSGI on `SIGTERM`, so if you run `docker stop` or something similar, it will actually stop everything, instead of waiting for Docker's timeout to kill the container.\n\n* 2017-07-31: There's now an image tag for Python 3.6, based on the official image for Python 3.6 thanks to [jrd](https://github.com/jrd) in [this PR](https://github.com/tiangolo/uwsgi-nginx-docker/pull/6).\n\n* 2016-10-01: Now you can override default `uwsgi.ini` parameters from the file in `/app/uwsgi.ini`.\n\n* 2016-08-16: There's now an image tag for Python 3.5, based on the official image for Python 3.5. So now you can use this image for your projects in Python 2.7 and Python 3.5.\n\n* 2016-08-16: Use dynamic a number of worker processes for uWSGI, from 2 to 16 depending on load. This should work for most cases. This helps especially when there are some responses that are slow and take some time to be generated, this change allows all the other responses to keep fast (in a new process) without having to wait for the first (slow) one to finish.\n\n* Also, it now uses a base `uwsgi.ini` file under `/etc/uwsgi/` with most of the general configurations, so, the `uwsgi.ini` inside `/app` (the one you could need to modify) is now a lot simpler.\n\n* 2016-04-05: Nginx and uWSGI logs are now redirected to stdout, allowing to use `docker logs`.\n\n## License\n\nThis project is licensed under the terms of the Apache license.\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 658, "quality_score": 0.59, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 630}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Security Policy\n\nSecurity is very important for this project and its community. üîí\n\nLearn more about it below. üëá\n\n## Versions\n\nThe latest version or release is supported.\n\nYou are encouraged to write tests for your application and update your versions frequently after ensuring that your tests are passing. This way you will benefit from the latest features, bug fixes, and **security fixes**.\n\n## Reporting a Vulnerability\n\nIf you think you found a vulnerability, and even if you are not sure about it, please report it right away by sending an email to: security@tiangolo.com. Please try to be as explicit as possible, describing all the steps and example code to reproduce the security issue.\n\nI (the author, [@tiangolo](https://twitter.com/tiangolo)) will review it thoroughly and get back to you.\n\n## Public Discussions\n\nPlease restrain from publicly discussing a potential security vulnerability. üôä\n\nIt's better to discuss privately and try to find a solution first, to limit the potential impact as much as possible.\n\n---\n\nThanks for your help!\n\nThe community and I thank you for that. üôá\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": "SECURITY.md", "language": "markdown", "complexity": "simple", "star_count": 658, "quality_score": 0.53, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 30}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import sys\n\n\ndef application(env, start_response):\n    version = \"{}.{}\".format(sys.version_info.major, sys.version_info.minor)\n    start_response(\"200 OK\", [(\"Content-Type\", \"text/plain\")])\n    message = \"Hello World from a default Nginx uWSGI Python {} app in a Docker container (default)\".format(\n        version\n    )\n    return [message.encode(\"utf-8\")]\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": "docker-images/app/main.py", "language": "python", "complexity": "simple", "star_count": 658, "quality_score": 0.59, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 11}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#! /usr/bin/env sh\n\necho \"Running inside /app/prestart.sh, you could add migrations to this file, e.g.:\"\n\necho \"\n#! /usr/bin/env bash\n\n# Let the DB start\nsleep 10;\n# Run migrations\nalembic upgrade head\n\"\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": "docker-images/app/prestart.sh", "language": "bash", "complexity": "simple", "star_count": 658, "quality_score": 0.5, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 13}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env sh\nset -e\n\n# Get the maximum upload file size for Nginx, default to 0: unlimited\nUSE_NGINX_MAX_UPLOAD=${NGINX_MAX_UPLOAD:-0}\n\n# Get the number of workers for Nginx, default to 1\nUSE_NGINX_WORKER_PROCESSES=${NGINX_WORKER_PROCESSES:-1}\n\n# Set the max number of connections per worker for Nginx, if requested \n# Cannot exceed worker_rlimit_nofile, see NGINX_WORKER_OPEN_FILES below\nNGINX_WORKER_CONNECTIONS=${NGINX_WORKER_CONNECTIONS:-1024}\n\n# Get the listen port for Nginx, default to 80\nUSE_LISTEN_PORT=${LISTEN_PORT:-80}\n\nif [ -f /app/nginx.conf ]; then\n    cp /app/nginx.conf /etc/nginx/nginx.conf\nelse\n    content='user  nginx;\\n'\n    # Set the number of worker processes in Nginx\n    content=$content\"worker_processes ${USE_NGINX_WORKER_PROCESSES};\\n\"\n    content=$content'error_log  /var/log/nginx/error.log warn;\\n'\n    content=$content'pid        /var/run/nginx.pid;\\n'\n    content=$content'events {\\n'\n    content=$content\"    worker_connections ${NGINX_WORKER_CONNECTIONS};\\n\"\n    content=$content'}\\n'\n    content=$content'http {\\n'\n    content=$content'    include       /etc/nginx/mime.types;\\n'\n    content=$content'    default_type  application/octet-stream;\\n'\n    content=$content'    log_format  main  '\"'\\$remote_addr - \\$remote_user [\\$time_local] \\\"\\$request\\\" '\\n\"\n    content=$content'                      '\"'\\$status \\$body_bytes_sent \\\"\\$http_referer\\\" '\\n\"\n    content=$content'                      '\"'\\\"\\$http_user_agent\\\" \\\"\\$http_x_forwarded_for\\\"';\\n\"\n    content=$content'    access_log  /var/log/nginx/access.log  main;\\n'\n    content=$content'    sendfile        on;\\n'\n    content=$content'    keepalive_timeout  65;\\n'\n    content=$content'    include /etc/nginx/conf.d/*.conf;\\n'\n    content=$content'}\\n'\n    content=$content'daemon off;\\n'\n    # Set the max number of open file descriptors for Nginx workers, if requested\n    if [ -n \"${NGINX_WORKER_OPEN_FILES}\" ] ; then\n        content=$content\"worker_rlimit_nofile ${NGINX_WORKER_OPEN_FILES};\\n\"\n    fi\n    # Save generated /etc/nginx/nginx.conf\n    printf \"$content\" > /etc/nginx/nginx.conf\n\n    content_server='server {\\n'\n    content_server=$content_server\"    listen ${USE_LISTEN_PORT};\\n\"\n    content_server=$content_server'    location / {\\n'\n    content_server=$content_server'        include uwsgi_params;\\n'\n    content_server=$content_server'        uwsgi_pass unix:///tmp/uwsgi.sock;\\n'\n    content_server=$content_server'    }\\n'\n    content_server=$content_server'}\\n'\n    # Save generated server /etc/nginx/conf.d/nginx.conf\n    printf \"$content_server\" > /etc/nginx/conf.d/nginx.conf\n\n    # Generate Nginx config for maximum upload file size\n    printf \"client_max_body_size $USE_NGINX_MAX_UPLOAD;\\n\" > /etc/nginx/conf.d/upload.conf\nfi\n\nexec \"$@\"\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": "docker-images/entrypoint.sh", "language": "bash", "complexity": "simple", "star_count": 658, "quality_score": 0.53, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 62}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#! /usr/bin/env bash\n\n# From official Nginx Docker image, as a script to re-use it, removing internal comments\n# Ref: https://github.com/nginx/docker-nginx/blob/7f1d49f6f222f7e588a9066fd53a0ce43c3466a5/mainline/debian/Dockerfile\n# Override group id from 101 to 102 as the original clashes with build-deps _ssh gid 101\n\n\n# Standard set up Nginx\nexport NGINX_VERSION=1.27.5\nexport NJS_VERSION=0.8.10\nexport NJS_RELEASE=1~bookworm\nexport PKG_RELEASE=1~bookworm\nexport DYNPKG_RELEASE=1~bookworm\n\nset -x \\\n    && groupadd --system --gid 102 nginx \\\n    && useradd --system --gid nginx --no-create-home --home /nonexistent --comment \"nginx user\" --shell /bin/false --uid 102 nginx \\\n    && apt-get update \\\n    && apt-get install --no-install-recommends --no-install-suggests -y gnupg1 ca-certificates \\\n    && \\\n    NGINX_GPGKEYS=\"573BFD6B3D8FBC641079A6ABABF5BD827BD9BF62 8540A6F18833A80E9C1653A42FD21310B49F6B46 9E9BE90EACBCDE69FE9B204CBCDCD8A38D88A2B3\"; \\\n    NGINX_GPGKEY_PATH=/etc/apt/keyrings/nginx-archive-keyring.gpg; \\\n    export GNUPGHOME=\"$(mktemp -d)\"; \\\n    found=''; \\\n    for NGINX_GPGKEY in $NGINX_GPGKEYS; do \\\n    for server in \\\n        hkp://keyserver.ubuntu.com:80 \\\n        pgp.mit.edu \\\n    ; do \\\n        echo \"Fetching GPG key $NGINX_GPGKEY from $server\"; \\\n        gpg1 --keyserver \"$server\" --keyserver-options timeout=10 --recv-keys \"$NGINX_GPGKEY\" && found=yes && break; \\\n    done; \\\n    test -z \"$found\" && echo >&2 \"error: failed to fetch GPG key $NGINX_GPGKEY\" && exit 1; \\\n    done; \\\n    gpg1 --export \"$NGINX_GPGKEYS\" > \"$NGINX_GPGKEY_PATH\" ; \\\n    rm -rf \"$GNUPGHOME\"; \\\n    apt-get remove --purge --auto-remove -y gnupg1 && rm -rf /var/lib/apt/lists/* \\\n    && dpkgArch=\"$(dpkg --print-architecture)\" \\\n    && nginxPackages=\" \\\n        nginx=${NGINX_VERSION}-${PKG_RELEASE} \\\n        nginx-module-xslt=${NGINX_VERSION}-${DYNPKG_RELEASE} \\\n        nginx-module-geoip=${NGINX_VERSION}-${DYNPKG_RELEASE} \\\n        nginx-module-image-filter=${NGINX_VERSION}-${DYNPKG_RELEASE} \\\n        nginx-module-njs=${NGINX_VERSION}+${NJS_VERSION}-${NJS_RELEASE} \\\n    \" \\\n    && case \"$dpkgArch\" in \\\n        amd64|arm64) \\\n            echo \"deb [signed-by=$NGINX_GPGKEY_PATH] https://nginx.org/packages/mainline/debian/ bookworm nginx\" >> /etc/apt/sources.list.d/nginx.list \\\n            && apt-get update \\\n            ;; \\\n        *) \\\n            tempDir=\"$(mktemp -d)\" \\\n            && chmod 777 \"$tempDir\" \\\n            \\\n            && savedAptMark=\"$(apt-mark showmanual)\" \\\n            \\\n            && apt-get update \\\n            && apt-get install --no-install-recommends --no-install-suggests -y \\\n                curl \\\n                devscripts \\\n                equivs \\\n                git \\\n                libxml2-utils \\\n                lsb-release \\\n                xsltproc \\\n            && ( \\\n                cd \"$tempDir\" \\\n                && REVISION=\"${NGINX_VERSION}-${PKG_RELEASE}\" \\\n                && REVISION=${REVISION%~*} \\\n                && curl -f -L -O https://github.com/nginx/pkg-oss/archive/${REVISION}.tar.gz \\\n                && PKGOSSCHECKSUM=\"c773d98b567bd585c17f55702bf3e4c7d82b676bfbde395270e90a704dca3c758dfe0380b3f01770542b4fd9bed1f1149af4ce28bfc54a27a96df6b700ac1745 *${REVISION}.tar.gz\" \\\n                && if [ \"$(openssl sha512 -r ${REVISION}.tar.gz)\" = \"$PKGOSSCHECKSUM\" ]; then \\\n                    echo \"pkg-oss tarball checksum verification succeeded!\"; \\\n                else \\\n                    echo \"pkg-oss tarball checksum verification failed!\"; \\\n                    exit 1; \\\n                fi \\\n                && tar xzvf ${REVISION}.tar.gz \\\n                && cd pkg-oss-${REVISION} \\\n                && cd debian \\\n                && for target in base module-geoip module-image-filter module-njs module-xslt; do \\\n                    make rules-$target; \\\n                    mk-build-deps --install --tool=\"apt-get -o Debug::pkgProblemResolver=yes --no-install-recommends --yes\" \\\n                        debuild-$target/nginx-$NGINX_VERSION/debian/control; \\\n                done \\\n                && make base module-geoip module-image-filter module-njs module-xslt \\\n            ) \\\n            && apt-mark showmanual | xargs apt-mark auto > /dev/null \\\n            && { [ -z \"$savedAptMark\" ] || apt-mark manual $savedAptMark; } \\\n            \\\n            && ls -lAFh \"$tempDir\" \\\n            && ( cd \"$tempDir\" && dpkg-scanpackages . > Packages ) \\\n            && grep '^Package: ' \"$tempDir/Packages\" \\\n            && echo \"deb [ trusted=yes ] file://$tempDir ./\" > /etc/apt/sources.list.d/temp.list \\\n            && apt-get -o Acquire::GzipIndexes=false update \\\n            ;; \\\n    esac \\\n    \\\n    && apt-get install --no-install-recommends --no-install-suggests -y \\\n                        $nginxPackages \\\n                        gettext-base \\\n                        curl \\\n    && apt-get remove --purge --auto-remove -y && rm -rf /var/lib/apt/lists/* /etc/apt/sources.list.d/nginx.list \\\n    \\\n    && if [ -n \"$tempDir\" ]; then \\\n        apt-get purge -y --auto-remove \\\n        && rm -rf \"$tempDir\" /etc/apt/sources.list.d/temp.list; \\\n    fi \\\n    && ln -sf /dev/stdout /var/log/nginx/access.log \\\n    && ln -sf /dev/stderr /var/log/nginx/error.log \\\n    # && mkdir /docker-entrypoint.d\n# Standard set up Nginx finished\n", "metadata": {"source": "github.com/tiangolo/uwsgi-nginx-docker", "file_path": "docker-images/install-nginx-debian.sh", "language": "bash", "complexity": "medium", "star_count": 658, "quality_score": 0.53, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 113}, "authority_level": "medium", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Bug report (ÁÆÄ‰Ωì‰∏≠Êñá)\nabout: Create a report to help us improve\ntitle: \"[Bug] \"\nlabels: bug\nassignees: vvbbnn00\n\n---\n\n**‰∏∫‰∫ÜÊõ¥Âø´Âú∞ÂÆö‰ΩçÈóÆÈ¢òÂπ∂ÂáèÂ∞ë‰∏çÂøÖË¶ÅÁöÑÈ∫ªÁÉ¶ÔºåËØ∑Âú®ÂèëËµ∑issueÂâçÊ£ÄÊü•ÊòØÂê¶Â∑≤ÊúâÁõ∏ÂÖ≥issueÂ≠òÂú®Ôºå\nË∞¢Ë∞¢ÔºÅ**\n\n**üìú Ê£ÄÊü•Ë°®**\n\n- [ ] ÊàëÂ∑≤‰∫ÜËß£ÁõÆÂâçissue‰∏ªË¶ÅÂ§ÑÁêÜ`bug`ÂíåÊé•Êî∂`feature request`\n  ÔºåÂÖ∂‰ªñÁ±ªÂûãÁöÑÈóÆÈ¢òÔºàÂ¶ÇÊìç‰Ωú„ÄÅÈÖçÁΩÆÈóÆÈ¢òÔºåÂÆ¢Êà∑Á´ØÈóÆÈ¢òÁ≠âÔºâÂ∫îÂèëÂ∏ÉÂú®[discussions](https://github.com/vvbbnn00/WARP-Clash-API/discussions)\n  ‰∏≠ÔºåÂê¶ÂàôËØ•issueÂèØËÉΩË¢´ÂÖ≥Èó≠„ÄÇ\n- [ ] ÊàëÁ°ÆËÆ§ËøôÊòØÁî±Êú¨È°πÁõÆÂºïËµ∑ÁöÑbugÔºåËÄåÈùûCloudflare\n  WARPËá™Ë∫´ÊúçÂä°ÈôêÂà∂„ÄÅÂÆ¢Êà∑Á´Ø‰∏çÊîØÊåÅÁ≠âÂÖ∂‰ªñÂéüÂõ†ÈÄ†ÊàêÁöÑÔºàËã•‰∏çÁ°ÆÂÆöÔºåËØ∑ÂêåÊ†∑ÂèëÂ∏ÉÂú®[discussions](https://github.com/vvbbnn00/WARP-Clash-API/discussions)Ôºâ„ÄÇ\n- [ ] ÊàëÂ∑≤Ê†∏ÂÆûËØ•bugÂ∞öÊú™Âú®ÂÖ∂‰ªñissue‰∏≠Ë¢´ÊèêÂèä„ÄÇ\n\n**BugÊèèËø∞**\n\nËØ∑Ê∏ÖÊô∞ÂÖ∑‰ΩìÂú∞ÊèèËø∞bugÔºåÂåÖÊã¨ËßÇÂØüÂà∞ÁöÑÂºÇÂ∏∏Ë°å‰∏∫ÂíåÈ¢ÑÊúüË°å‰∏∫„ÄÇ\n\n**Â§çÁé∞Ê≠•È™§**\n\nËØ∑Êèê‰æõÂ§çÁé∞bugÊâÄÈúÄÁöÑÂÖ∑‰ΩìÊ≠•È™§Ôºå‰æãÂ¶ÇÔºö\n\n1. ÊâßË°åÂëΩ‰ª§ `docker-compose up -d`\n2. ÁÇπÂáªÊåâÈíÆ `Button1`\n3. ËÆøÈóÆURL `http://xxxx`\n\n**È¢ÑÊúüË°å‰∏∫**\n\nÁÆÄËø∞ÊâßË°å‰∏äËø∞Ê≠•È™§ÂêéÈ¢ÑÊúüÁöÑÊ≠£Â∏∏Ë°å‰∏∫„ÄÇ\n\n**Êà™Â±è**\n\nÂ¶ÇÊúâÔºåËØ∑ÈôÑ‰∏äÁõ∏ÂÖ≥Êà™Âõæ„ÄÇ\n\n**ÁéØÂ¢É‰ø°ÊÅØ**\n\nËØ∑ÂáÜÁ°ÆÂ°´ÂÜôÁõ∏ÂÖ≥‰ø°ÊÅØÔºåÂ¶ÇÊòØÊúçÂä°Âô®Á´ØÈóÆÈ¢òÔºåËØ∑Êèê‰æõÊúçÂä°Âô®ÁéØÂ¢É‰ø°ÊÅØÔºö\n\n- ËÆæÂ§áÂûãÂè∑Ôºö[e.g. iPhone13/‰∫ëÊúçÂä°Âô®]\n\n- Á≥ªÁªüÁâàÊú¨Ôºö[e.g. iOS15.1/Windows 10]\n\n- ËΩØ‰ª∂ÂêçÁß∞Ôºö[e.g. ShadowRocket]\n\n- ËΩØ‰ª∂ÁâàÊú¨Ôºö[e.g. v2.2.45 (2171)]\n\n- ËÆ¢ÈòÖÁ±ªÂûãÔºö[e.g. Ëá™Âä®Ê£ÄÊµã/Clash/ShadowRocket/Surge/...]\n\n- Âá∫Áé∞ÈóÆÈ¢òÊó∂È°πÁõÆÁöÑ `Commit Hash`Ôºö[e.g. dfdb97f1a61ac5deb0db8f012c59d8050c9587e6]\n\n**È¢ùÂ§ñ‰ø°ÊÅØ**\n\nÂ¶ÇÊúâÈ¢ùÂ§ñ‰ø°ÊÅØÔºàÂ¶ÇËøêË°åÊó•Âøó„ÄÅÂìçÂ∫îÊï∞ÊçÆÁ≠âÔºâÂèØÂä©‰∫éÊéíÊü•ÈóÆÈ¢òÔºåËØ∑Âú®Ê≠§Â§ÑÊ∑ªÂä†„ÄÇÊ≥®ÊÑè‰øùÊä§‰∏™‰∫∫ÈöêÁßÅÔºå‰∏çË¶ÅÂÖ¨ÂºÄ\nÊïèÊÑü‰ø°ÊÅØÔºàÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºöIPÂú∞ÂùÄ„ÄÅLicenseKey„ÄÅPrivateKey„ÄÅSECRET_KEYÔºâ„ÄÇ\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": ".github/ISSUE_TEMPLATE/bug-report--ÁÆÄ‰Ωì‰∏≠Êñá-.md", "language": "markdown", "complexity": "simple", "star_count": 8732, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 62}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Bug report\nabout: Create a report to help us improve\ntitle: \"[Bug]\"\nlabels: bug\nassignees: vvbbnn00\n\n---\n\n**To quickly locate the issue and reduce unnecessary trouble, please check if a\nrelated issue already exists before creating a new one. Thank you!**\n\n**üìú Checklist**\n\n- [ ] I understand that the current issue mainly handles `bugs` and\n  accepts `feature requests`. Other types of issues (such as operational,\n  configuration issues, client issues, etc.) should be posted\n  in [discussions](https://github.com/vvbbnn00/WARP-Clash-API/discussions),\n  otherwise, the issue may be closed.\n- [ ] I confirm that this is a bug caused by this project, and not due to\n  Cloudflare WARP's own service limitations, client incompatibility, or other\n  reasons (if unsure, please also post\n  in [discussions](https://github.com/vvbbnn00/WARP-Clash-API/discussions)).\n- [ ] I have verified that this bug has not been mentioned in other issues.\n\n**Bug Description**\n\nPlease describe the bug clearly and specifically, including the observed\nabnormal behavior and the expected behavior.\n\n**Steps to Reproduce**\n\nPlease provide specific steps to reproduce the bug, such as:\n\n1. Execute the command `docker-compose up -d`\n2. Click the button `Button1`\n3. Visit the URL `http://xxxx`\n\n**Expected Behavior**\n\nBriefly describe the expected normal behavior after performing the above steps.\n\n**Screenshots**\n\nIf applicable, please attach relevant screenshots.\n\n**Environment Information**\n\nPlease accurately fill in the relevant information. If it is a server-side\nissue, please provide server environment information:\n\n- Device Model: [e.g. iPhone13/Cloud Server]\n\n- System Version: [e.g. iOS15.1/Windows 10]\n\n- Software Name: [e.g. ShadowRocket]\n\n- Software Version: [e.g. v2.2.45 (2171)]\n\n- Subscription Type: [e.g. Auto Detect/Clash/ShadowRocket/Surge/...]\n\n- `Commit Hash` of the project when the issue\n  occurred: [e.g. dfdb97f1a61ac5deb0db8f012c59d8050c9587e6]\n\n**Additional Information**\n\nIf there is any additional information (such as runtime logs, response data,\netc.) that can help diagnose the issue, please add it here. Be mindful of\nprotecting personal privacy and do not disclose sensitive information (including\nbut not limited to: IP address, LicenseKey, PrivateKey, SECRET_KEY).\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": ".github/ISSUE_TEMPLATE/bug_report.md", "language": "markdown", "complexity": "simple", "star_count": 8732, "quality_score": 0.73, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 71}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Docker Build and Push (linux/amd64)\n\non:\n  push:\n    branches:\n      - master\n  workflow_dispatch:\n\njobs:\n  build-and-push:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out the repo\n        uses: actions/checkout@v4\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      - name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n      - name: Extract metadata (tags, labels)\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n          images: vvbbnn00/warp-clash-api\n          tags: |\n            type=sha,format=short\n\n      - name: Build and push (Dockerfile)\n        uses: docker/build-push-action@v5\n        with:\n          context: .\n          file: ./Dockerfile\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}-amd64\n          labels: ${{ steps.meta.outputs.labels }}\n          cache-from: type=gha\n          cache-to: type=gha,mode=max\n          build-args: GITHUB_ACTIONS=true\n          platforms: linux/amd64\n\n      - name: Image digest\n        run: echo ${{ steps.meta.outputs.digest }}\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": ".github/workflows/docker_build_amd64.yml", "language": "yaml", "complexity": "medium", "star_count": 8732, "quality_score": 0.7, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 48}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Docker Build and Push (linux/arm64)\n\non:\n  push:\n    branches:\n      - master\n  workflow_dispatch:\n\njobs:\n  build-and-push:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out the repo\n        uses: actions/checkout@v4\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      - name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n      - name: Extract metadata (tags, labels)\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n          images: vvbbnn00/warp-clash-api\n          tags: |\n            type=sha,format=short\n\n      - name: Build and push (Dockerfile)\n        uses: docker/build-push-action@v5\n        with:\n          context: .\n          file: ./Dockerfile\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}-arm64\n          labels: ${{ steps.meta.outputs.labels }}\n          cache-from: type=gha\n          cache-to: type=gha,mode=max\n          build-args: GITHUB_ACTIONS=true\n          platforms: linux/arm64\n\n      - name: Image digest\n        run: echo ${{ steps.meta.outputs.digest }}\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": ".github/workflows/docker_build_arm64.yml", "language": "yaml", "complexity": "medium", "star_count": 8732, "quality_score": 0.7, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 48}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Docker Build and Push (linux/arm/v7)\n\non:\n  push:\n    branches:\n      - master\n  workflow_dispatch:\n\njobs:\n  build-and-push:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out the repo\n        uses: actions/checkout@v4\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      - name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n      - name: Extract metadata (tags, labels)\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n          images: vvbbnn00/warp-clash-api\n          tags: |\n            type=sha,format=short\n\n      - name: Build and push (Dockerfile.armhf)\n        uses: docker/build-push-action@v5\n        with:\n          context: .\n          file: ./Dockerfile.armhf\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}-armv7\n          labels: ${{ steps.meta.outputs.labels }}\n          cache-from: type=gha\n          cache-to: type=gha,mode=max\n          build-args: GITHUB_ACTIONS=true\n          platforms: linux/arm/v7\n\n      - name: Image digest\n        run: echo ${{ steps.meta.outputs.digest }}\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": ".github/workflows/docker_build_armv7.yaml", "language": "yaml", "complexity": "medium", "star_count": 8732, "quality_score": 0.7, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 48}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# WARP Clash API\n\n![GitHub License](https://img.shields.io/github/license/vvbbnn00/WARP-Clash-API)\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/67ca8d105fb947eca6204230ba3ac09b)](https://app.codacy.com/gh/vvbbnn00/WARP-Clash-API/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade)\n![GitHub Repo stars](https://img.shields.io/github/stars/vvbbnn00/WARP-Clash-API?style=flat)\n\n‰∏≠Êñá | [English](./README_en.md)\n\n> **Warning**\n>\n> Êú¨È°πÁõÆÊòØÂÆåÂÖ®ÈùûÂïÜ‰∏öÈ°πÁõÆÔºå‰ªÖ‰æõÂ≠¶‰π†‰∫§ÊµÅ‰ΩøÁî®ÔºåËØ∑ÂãøÁî®‰∫éÈùûÊ≥ïÁî®ÈÄîÔºåÂê¶ÂàôÂêéÊûúËá™Ë¥ü„ÄÇ\n\n## ü§î ËøôÊòØ‰ªÄ‰πàÔºü\n\nËØ•È°πÁõÆÂèØ‰ª•ËÆ©‰Ω†ÈÄöËøáËÆ¢ÈòÖÁöÑÊñπÂºè‰ΩøÁî®`WARP+`ÔºåÊîØÊåÅ`Clash`„ÄÅ`Shadowrocket`Á≠âÂÆ¢Êà∑Á´Ø„ÄÇÈ°πÁõÆÂÜÖÁΩÆ‰∫Ü\nÂà∑Âèñ`WARP+`ÊµÅÈáèÁöÑÂäüËÉΩÔºåÂèØ‰ª•ËÆ©‰Ω†ÁöÑ`WARP+`ÊµÅÈáè‰∏çÂÜçÂèóÈôêÂà∂ÔºàÊØè`18`ÁßíÂèØËé∑Âæó`1GB`ÊµÅÈáèÔºâÔºåÂêåÊó∂Ôºå\nÈÖçÂ§á‰∫Ü`IP`ÈÄâ‰ºòÂäüËÉΩ„ÄÇÊîØÊåÅ`Docker compose`\n‰∏ÄÈîÆÈÉ®ÁΩ≤ÔºåÊó†ÈúÄÈ¢ùÂ§ñÊìç‰ΩúÔºåÂç≥ÂèØ‰∫´Âèó‰Ω†Ëá™Â∑±ÁöÑ`WARP+`ÁßÅ\nÊúâÈ´òÈÄüËäÇÁÇπÔºÅ\n\n## üí° ÁâπËâ≤ÂäüËÉΩ\n\n- üíª ÊîØÊåÅ`Clash`„ÄÅ`Surge`„ÄÅ`Shadowrocket`Á≠âÂÆ¢Êà∑Á´Ø\n- üîë ÊîØÊåÅËÆæÁΩÆÊÇ®Ëá™Â∑±ÁöÑ`LicenseKey`\n- üåè ÊîØÊåÅ`IP`ÈÄâ‰ºò\n- üêã ÊîØÊåÅ`Docker compose`‰∏ÄÈîÆÈÉ®ÁΩ≤\n- üìï ÂÖ®Ëá™Âä®Âà∑Âèñ`WARP+`ÊµÅÈáèÔºåËØ∑Ê±ÇÁªèËøá‰ª£ÁêÜÔºåÈò≤Â∞Å`IP`\n- ‚ùì ÊØèÊ¨°Êõ¥Êñ∞ËÆ¢ÈòÖÈöèÊú∫ËäÇÁÇπÔºåËÆ©‰Ω†‰ΩìÈ™åÊäΩÂç°ÁöÑ‰πêË∂£\n\n## üöÄ Âø´ÈÄü‰∏äÊâã\n\n### 1. ÂÆâË£Ö`Docker`Âíå`Docker compose`\n\n- `Docker`\n  ÂÆâË£ÖÊïôÁ®ãÔºö[https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)\n- `Docker compose`\n  ÂÆâË£ÖÊïôÁ®ãÔºö[https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/)\n\n### 2. ‰∏ãËΩΩÈ°πÁõÆ\n\n```bash\ngit clone https://github.com/vvbbnn00/WARP-Clash-API.git\n```\n\n### 3. [ÂèØÈÄâ] ÈÖçÁΩÆ`SECRET_KEY`\n\nËã•ÊÇ®ÈúÄË¶ÅÂú®ÂÖ¨ÁΩë‰∏äÈÉ®ÁΩ≤ËØ•È°πÁõÆÔºåÂª∫ËÆÆÊÇ®ÈÖçÁΩÆ`SECRET_KEY`‰∏é`PUBLIC_URL`„ÄÇÂú®È°πÁõÆÁõÆÂΩï‰∏ãÂàõÂª∫\n`.env.local`Êñá‰ª∂ÔºåÂÜôÂÖ•Â¶Ç‰∏ãÂÜÖÂÆπÔºö\n\n```bash\nSECRET_KEY=your_secret_key\n```\n\nÂÖ≥‰∫éÁéØÂ¢ÉÂèòÈáèÁöÑÊõ¥Â§ö‰ø°ÊÅØÔºåËØ∑ÂèÇËÄÉ[ÁéØÂ¢ÉÂèòÈáè](#-ÁéØÂ¢ÉÂèòÈáè)„ÄÇ\n\n### 4. ÁºñËØëÂπ∂ËøêË°å\n\n```bash\ndocker-compose up -d\n```\n\n### 5. Ëé∑ÂèñËÆ¢ÈòÖÈìæÊé•\n\nËÆøÈóÆ`http://‰Ω†ÁöÑIP:21001`ÔºåËæìÂÖ•`SECRET_KEY`ÔºàËã•Ê≤°ÊúâÈÖçÁΩÆÔºåÂàôÂèØ‰ª•ÁïôÁ©∫ÔºâÔºåÂç≥ÂèØËé∑ÂèñËÆ¢ÈòÖÈìæÊé•„ÄÇ\n\n**üéâ Â§ßÂäüÂëäÊàê**\n\n## üåè ÊâãÂä®IPÈÄâ‰ºò\n\nÈ°πÁõÆÊú¨Ë∫´ÂåÖÂê´‰∫Ü‰∏Ä‰∏™ÈÄâ‰ºòËøáÁöÑ`IP`ÂàóË°®Ôºå‰ΩÜÊòØÁî±‰∫é`WARP`ÁöÑ`IP`\nÊòØÂä®ÊÄÅÁöÑÔºåÊâÄ‰ª•ÂèØËÉΩ‰ºöÂá∫Áé∞`IP`‰∏çÂèØÁî®ÁöÑ\nÊÉÖÂÜµ„ÄÇËã•ÊÇ®ÈúÄË¶ÅÊâãÂä®ÈÄâ‰ºòÔºåÂèØ‰ª•ÈÅµÂæ™‰ª•‰∏ãÊ≠•È™§Ôºö\n\nËã•ÊÇ®ÈÄöËøá`docker-compose`ÈÉ®ÁΩ≤ÔºåÂèØ‰ª•Âú®È°πÁõÆÁõÆÂΩï‰∏ãÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§ÊâãÂä®ÊâßË°å`IP`ÈÄâ‰ºòÔºö\n\n```bash\ndocker-compose exec warp-clash python3 app.py optimize\n```\n\nÂê¶ÂàôÔºåÂèØ‰ª•Âú®È°πÁõÆÁõÆÂΩï‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö\n\n```bash\npython3 app.py optimize\n```\n\n## üîß ÁéØÂ¢ÉÂèòÈáè\n\nÊ≤°ÈîôÔºåÊÇ®ÂèØ‰ª•ÈÄöËøáÁéØÂ¢ÉÂèòÈáèÊù•ÈÖçÁΩÆËØ•È°πÁõÆÔºåÂú®ÈÖçÁΩÆÊó∂ÔºåÂè™ÈúÄÊñ∞Âª∫‰∏Ä‰∏™`.env.local`Êñá‰ª∂ÔºåÂÜôÂÖ•ÊÇ®ÈúÄË¶ÅÁöÑÁéØÂ¢É\nÂèòÈáèÂç≥ÂèØ„ÄÇ\n\n‰ª•‰∏ãÊòØÂèØÁî®ÁöÑÁéØÂ¢ÉÂèòÈáèÔºö\n\n| ÂèòÈáèÂêç                    | ÈªòËÆ§ÂÄº                               | ËØ¥Êòé                                                                                                         |\n|------------------------|-----------------------------------|------------------------------------------------------------------------------------------------------------|\n| DELAY_THRESHOLD        | `500`                             | Âª∂ËøüÈòàÂÄºÔºåË∂ÖËøáËØ•ÈòàÂÄºÁöÑ`IP`Â∞ÜË¢´ÂâîÈô§                                                                                        |\n| DO_GET_WARP_DATA       | `True`                            | ÊòØÂê¶Âà∑Âèñ`WARP+`ÊµÅÈáèÔºåËã•‰∏çÈúÄË¶ÅÂà∑ÂèñÊµÅÈáèÔºåÂàôËÆæÁΩÆ‰∏∫`False`Âç≥ÂèØ                                                                       |\n| GET_WARP_DATA_INTERVAL | `18`                              | Âà∑Âèñ`WARP+`ÊµÅÈáèÁöÑÊó∂Èó¥Èó¥ÈöîÔºåÂçï‰Ωç‰∏∫ÁßíÔºåÊØèÈöîËØ•Êó∂Èó¥Èó¥Èöî‰ºöÂà∑Âèñ‰∏ÄÊ¨°`WARP+`ÊµÅÈáèÔºå‰∏çÂª∫ËÆÆÈó¥ÈöîËÆæÁΩÆËøáÁü≠„ÄÇ                                                     |\n| LOSS_THRESHOLD         | `10`                              | ‰∏¢ÂåÖÁéáÈòàÂÄºÔºåË∂ÖËøáËØ•ÈòàÂÄºÁöÑ`IP`Â∞ÜË¢´ÂâîÈô§                                                                                       |\n| PROXY_POOL_URL         | `https://getproxy.bzpl.tech/get/` | IP‰ª£ÁêÜÊ±†Âú∞ÂùÄÔºåÁî®‰∫éÂà∑Âèñ`WARP+`ÊµÅÈáèÔºåÊÇ®ÂèØ‰ª•Ëá™Ë°åÊê≠Âª∫ÔºåÂèÇÁÖß[proxy_pool](https://github.com/jhao104/proxy_pool)                        |\n| PUBLIC_URL             | `Êó†`                               | ÈÉ®ÁΩ≤Âú®ÂÖ¨ÁΩë‰∏äÊó∂ÔºåÂ°´ÂÜôÂÖ¨ÁΩë`IP`ÊàñÂüüÂêçÔºåÁî®‰∫éÁîüÊàêËÆ¢ÈòÖÈìæÊé•ÔºåÊØîÂ¶Ç `https://subs.zeabur.app`                                                  |\n| RANDOM_COUNT           | `10`                              | ÊØèÊ¨°Êõ¥Êñ∞ËÆ¢ÈòÖÈöèÊú∫ËäÇÁÇπÁöÑÊï∞Èáè                                                                                              |\n| REOPTIMIZE_INTERVAL    | `-1`                              | ÈáçÊñ∞ÈÄâ‰ºòÁöÑÊó∂Èó¥Èó¥ÈöîÔºåÂçï‰Ωç‰∏∫ÁßíÔºåËã•Â∞è‰∫éÁ≠â‰∫é0ÔºåÂàô‰∏ç‰ºöÈáçÊñ∞ÈÄâ‰ºòÔºåÂê¶ÂàôÊØèÈöîËØ•Êó∂Èó¥Èó¥Èöî‰ºöÈáçÊñ∞ÈÄâ‰ºò‰∏ÄÊ¨°Ôºå‰∏çÂª∫ËÆÆÈó¥ÈöîËÆæÁΩÆËøáÁü≠„ÄÇ                                                  |                     \n| REQUEST_RATE_LIMIT     | `0`                               | ÈôêÂà∂XÁßí‰∏ÄÊ¨°ËØ∑Ê±ÇÔºåËØ•ÂäüËÉΩ‰∏çÂ§™Á®≥ÂÆöÔºåÂª∫ËÆÆ‰∏çË¶ÅÂºÄÂêØ                                                                                    |\n| SECRET_KEY             | `Êó†`                               | Áî®‰∫é‰øùÊä§ËÆ¢ÈòÖÈìæÊé•ÔºåËã•‰∏çÈÖçÁΩÆÔºåÂàô‰∏çÈúÄË¶ÅËæìÂÖ•`SECRET_KEY`Âç≥ÂèØËé∑ÂèñËÆ¢ÈòÖÈìæÊé•                                                                   |\n| SHARE_SUBSCRIPTION     | `False`                           | Ëã•ÊÇ®ÁöÑÁ´ôÁÇπÊÉ≥Ë¶ÅÂêëÁ§æÂå∫ÂàÜ‰∫´ËÆ¢ÈòÖÔºå‰ΩÜ‰∏çÊÉ≥ËÆ©Ëá™Â∑±ÁöÑË¥¶Êà∑‰ø°ÊÅØË¢´ÂÖ¨ÂºÄÊàñ‰øÆÊîπÔºåÂèØ‰ª•ËÆæÁΩÆ‰∏∫`True`ÔºåÊ≠§Êó∂ÔºåËÆøÈóÆËÆ¢ÈòÖÈìæÊé•Êó∂Ôºå‰∏çÈúÄË¶ÅËæìÂÖ•`SECRET_KEY`Âç≥ÂèØËé∑ÂèñÔºåËÄåÂØπ‰∫éÂÖ∂‰ªñÁöÑÊìç‰ΩúÔºå‰ªçÁÑ∂ÈúÄË¶ÅËæìÂÖ•`SECRET_KEY`„ÄÇ |\n\n### üìù ÈÖçÁΩÆÁ§∫‰æã\n\n‰æãÂ¶ÇÔºåÊÇ®ËÆæÁΩÆ`SECRET_KEY`‰∏∫`123456`ÔºåÂπ∂ÊâìÁÆóÂ∞ÜËÆ¢ÈòÖÂàÜ‰∫´ÁªôÁ§æÂå∫ÔºåÈÇ£‰πàÊÇ®ÁöÑ`.env.local`\nÊñá‰ª∂Â∫îËØ•\nÂ¶Ç‰∏ãÔºö\n\n```env\nSECRET_KEY=123456\nSHARE_SUBSCRIPTION=True\n```\n\n## üß∞ ËøõÈò∂Êìç‰Ωú\n\n**ËØ∑Ê≥®ÊÑèÔºåÂ¶ÇÊûúÊÇ®ËÆæÁΩÆ‰∫Ü`SECRET_KEY`ÔºåÈúÄË¶ÅÂú®URLÁöÑÊú´Â∞æÊ∑ªÂä†`key`ÂèÇÊï∞**Ôºå‰æãÂ¶ÇÔºö\n\n```text\nhttp://your_IP:21001/some/api/actions?key=your_secret_key\n```\n\n### ÈáçÁΩÆË¥¶Êà∑ÁöÑ`PublicKey`Âíå`PrivateKey`\n\nÈ°πÁõÆÊîØÊåÅÊÇ®ÈÄöËøáËØ∑Ê±Ç‰ª•‰∏ãÊé•Âè£Êù•ÈáçÁΩÆ`PublicKey`Âíå`PrivateKey`Ôºö\n\n```bash\ncurl -X POST http://host:port/api/account/reset_key\n```\n\nÈáçÁΩÆËøáÂêéÔºåÈúÄË¶ÅÈáçÊñ∞Ëé∑ÂèñËÆ¢ÈòÖÂÜÖÂÆπÔºåÂê¶ÂàôÂèØËÉΩÊó†Ê≥ï‰ΩøÁî®„ÄÇ\n\n### ËÆæÁΩÆËá™Â∑±ÁöÑ`LicenseKey`\n\nËã•ÊÇ®Â∑≤ÁªèÊã•Êúâ‰∫Ü`WARP+`ÁöÑ`LicenseKey`ÔºåÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÊé•Âè£Êù•ËÆæÁΩÆÔºö\n\n```bash\ncurl -X POST http://host:port/api/account/update_license -H \"Content-Type: application/json\" -d \"{\\\"license_key\\\": \\\"your_license_key\\\"}\"\n```\n\nËØ∑Ê≥®ÊÑèÔºåÂΩìÊÇ®ËÆæÁΩÆ‰∫Ü`LicenseKey`ÂêéÔºåÂÖ∂`PublicKey`Âíå`PrivateKey`Â∞Ü‰ºöË¢´ÈáçÁΩÆÔºåÈúÄË¶ÅÈáçÊñ∞Ëé∑ÂèñËÆ¢ÈòÖ\nÂÜÖÂÆπ„ÄÇ\n\n### ‰ΩøÁî® IPv6 ‰ºòÈÄâ\n\n‰ªéÂéüÁêÜ‰∏äÊù•ÁúãÔºåÊú¨ÊúçÂä°ÈÄöËøáËØªÂèñ`config/result_v6.csv`Êñá‰ª∂Ëé∑Âèñ`IPv6`Âú∞ÂùÄÔºåÂõ†Ê≠§ÊÇ®ÂèØ‰ª•Âú®ÊîØÊåÅ\n`IPv6`ÁöÑÊúçÂä°Âô®‰∏äËøêË°å`IP`ÈÄâ‰ºòÂäüËÉΩÊù•Ëé∑Âæó`IPv6`Âú∞ÂùÄ„ÄÇËé∑ÂèñÂà∞ÁöÑÊé•ÂÖ•Âú∞ÂùÄÂàóË°®Âè™ÈúÄÂÜôÂÖ•\n`config/result_v6.csv`Êñá‰ª∂‰∏≠ÔºåÁõ∏ÊØîÂú®`Docker`ÂÆπÂô®‰∏≠ËøêË°å`IP`ÈÄâ‰ºòÂäüËÉΩÔºåÊ≠§ÊñπÊ≥ïÊõ¥‰∏∫ÁÆÄ‰æø„ÄÇ\n\nÂ¶ÇÊûúÊÇ®ÈúÄË¶ÅÂú®`Docker`ÂÆπÂô®‰∏≠ËøêË°å`IP`ÈÄâ‰ºòÂäüËÉΩÔºåÂèØ‰ª•‰ΩøÁî®`docker-compose_ipv6.yaml`\nÊñá‰ª∂Êù•ËÆ©`Docker`ÈïúÂÉèÊîØÊåÅ`IPv6`„ÄÇÂú®ËøêË°åÂâçÔºåËØ∑Á°Æ‰øùÊÇ®ÁöÑÊúçÂä°Âô®ÊîØÊåÅ`IPv6`ÔºåÂπ∂Âú®`Docker`ÊúçÂä°ÁöÑ\n`/etc/docker/daemon.json`‰∏≠Ê∑ªÂä†‰ª•‰∏ãÂÜÖÂÆπÔºåÂπ∂ÈáçÂêØ`Docker`ÊúçÂä°Ôºö\n\n```json\n{\n  \"experimental\": true,\n  \"ip6tables\": true,\n  \"ipv6\": true,\n  \"fixed-cidr-v6\": \"2001:db8:1::/64\"\n}\n```\n\nËã•ÊÇ®Âú®ÂÖàÂâçÂ∑≤ÁªèËøêË°åËøá`Docker`ÊúçÂä°ÔºåÂú®ËøêË°å‰πãÂâçÔºåËØ∑ÂÖàÂÅúÊ≠¢‰πãÂâçÁöÑÊúçÂä°Ôºö\n\n```bash\ndocker-compose down\n```\n\nÁÑ∂ÂêéÔºåÊÇ®ÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§Êù•ËøêË°å`Docker`ÊúçÂä°Ôºö\n\n```bash\ndocker-compose -f docker-compose_ipv6.yaml up -d\n```\n\n## üóÇÔ∏è ÂºïÁî®È°πÁõÆ\n\nÊú¨È°πÁõÆÁöÑÂºÄÂèëÂèÇÁÖß‰∫Ü‰ª•‰∏ãÈ°πÁõÆÔºåÊÑüË∞¢Ëøô‰∫õÂºÄÊ∫êÈ°πÁõÆÁöÑ‰ΩúËÄÖÔºö\n\n- [warp-script](https://gitlab.com/Misaka-blog/warp-script)\n- [warp](https://replit.com/@aliilapro/warp)\n- [wgcf](https://github.com/ViRb3/wgcf)\n- [proxy_pool](https://github.com/jhao104/proxy_pool)\n- [geolite2](https://dev.maxmind.com/geoip/geolite2-free-geolocation-data)\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 8732, "quality_score": 0.73, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 187}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# WARP Clash API\n\n![GitHub License](https://img.shields.io/github/license/vvbbnn00/WARP-Clash-API)\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/67ca8d105fb947eca6204230ba3ac09b)](https://app.codacy.com/gh/vvbbnn00/WARP-Clash-API/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade)\n![GitHub Repo stars](https://img.shields.io/github/stars/vvbbnn00/WARP-Clash-API?style=flat)\n\n[‰∏≠Êñá](README.md) | English\n\n> **Warning**\n>\n> This project is entirely non-commercial and is intended solely for educational\n> and communicative purposes. Please do not use it for illegal activities, as\n> the consequences will be borne by the user.\n\n## ü§î What is This?\n\nThis project allows you to use `WARP+` through a subscription, supporting\nclients like `Clash`, `Shadowrocket`, etc. The project includes a feature to\nreplenish `WARP+` traffic, enabling your `WARP+` traffic to be unrestricted\n(1GB of traffic every 18 seconds). Additionally, it comes with IP optimization.\n\nIt supports one-click deployment through `Docker compose`, so you can enjoy\nyour private high-speed `WARP+` node without extra hassle!\n\n## üí° Key Features\n\n- üíª Supports clients such as `Clash`, `Surge`, `Shadowrocket`, etc.\n-\n- üîë Supports setting your own `LicenseKey`.\n- üåè Supports IP optimization.\n- üêã Supports one-click deployment using `Docker compose`.\n- üìï Automatically replenishes `WARP+` traffic, requests are proxied,\n  preserving your IP from getting blocked.\n- ‚ùì Randomly selects nodes each subscription update, adding a sense of\n  randomness to your experience.\n\n## üöÄ Quick Start\n\n### 1. Install `Docker` and `Docker compose`\n\n- Docker Installation\n  Guide: [https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)\n- Docker Compose Installation\n  Guide: [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/)\n\n### 2. Download the Project\n\n```bash\ngit clone https://github.com/vvbbnn00/WARP-Clash-API.git\n```\n\n### 3. [Optional] Configure `SECRET_KEY`\n\nIf you plan to deploy this project on the public internet, it's recommended to\nset up the `SECRET_KEY`. Create a `.env.local` file in the project directory\nand add the following:\n\n```bash\nSECRET_KEY=your_secret_key\n```\n\nFor more information on environment variables, refer to\n[Environment Variables](#-environment-variables).\n\n### 4. Build and Run\n\n```bash\ndocker-compose up -d\n```\n\n### 5. Obtain Subscription Link\n\nVisit `http://your_IP:21001`, enter the `SECRET_KEY` and `PUBLIC_URL` (if\nconfigured), and you can get the subscription link.\n\n**üéâ Congratulations, you're all set!**\n\n## üåè Manual IP Optimization\n\nThe project includes a pre-optimized list of IPs, but due to the dynamic\nnature of `WARP` IPs, there might be cases where IPs become unusable. If you\nwish to optimize manually, follow these steps:\n\nIf you deployed via `docker-compose`, you can manually execute IP optimization\nwith the following command in the project directory:\n\n```bash\ndocker-compose exec warp-clash python3 app.py optimize\n```\n\nOtherwise, you can execute the following command in the project directory:\n\n```bash\npython3 app.py optimize\n```\n\n## üîß Environment Variables\n\nYes, you can configure this project using environment variables. Simply create\na `.env.local` file and add the required environment variables.\n\nHere are the available environment variables:\n\n| Variable Name          | Default                           | Description                                                                                                                                                                                                                                                                                                                |\n|------------------------|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| DELAY_THRESHOLD        | `500`                             | Delay threshold; IPs exceeding this threshold will be removed.                                                                                                                                                                                                                                                             |\n| DO_GET_WARP_DATA       | `True`                            | Whether to get `WARP+` data. If set to `False`, the `WARP+` data will not be obtained.                                                                                                                                                                                                                                     |\n| GET_WARP_DATA_INTERVAL | `18`                              | Time interval for obtaining `WARP+` data, in seconds. `WARP+` data will be obtained every this interval, it is not recommended to set the interval too short.                                                                                                                                                              |\n| LOSS_THRESHOLD         | `10`                              | Packet loss threshold; IPs exceeding this threshold will be removed.                                                                                                                                                                                                                                                       |\n| PROXY_POOL_URL         | `https://getproxy.bzpl.tech/get/` | IP proxy pool address, used to get `WARP+` traffic. You can build it yourself, check [proxy_pool](https://github.com/jhao104/proxy_pool) for more information.                                                                                                                                                             |\n| PUBLIC_URL             | `None`                            | When deployed on the public network, fill in the public IP or domain name to generate subscription links. for example `https://subs.zeabur.app`                                                                                                                                                                            |\n| RANDOM_COUNT           | `10`                              | Number of randomly selected nodes during each subscription update.                                                                                                                                                                                                                                                         |\n| REOPTIMIZE_INTERVAL    | `-1`                              | Re-optimization interval in seconds, if less than or equal to 0, re-optimization will not occur, otherwise it will re-optimize every this interval, it is not recommended to set the interval too short.                                                                                                                   |\n| REQUEST_RATE_LIMIT     | `0`                               | Limits requests to once every X seconds. This feature is unstable; it's recommended not to enable it.                                                                                                                                                                                                                      |\n| SECRET_KEY             | `None`                            | Used to protect the subscription link. If not configured, no `SECRET_KEY` input is required to get the link.                                                                                                                                                                                                               |\n| SHARE_SUBSCRIPTION     | `False`                           | If you want to share subscriptions with the community but doesn't want your account information to be publicly accessible or modified, you can set this to `True`. In this case, accessing the subscription link does not require entering the `SECRET_KEY`, but for other operations, the `SECRET_KEY` is still required. |\n\n### üìù Configuration Example\n\nIf you set `SECRET_KEY` to `123456` and plan to share the subscription with\nthe community, your `.env.local` file should look like this:\n\n```env\nSECRET_KEY=123456\nSHARE_SUBSCRIPTION=True\n```\n\n## üß∞ Advanced Operations\n\n**Please note that if you set `SECRET_KEY`, you need to add the `key`\nparameter at the end of the URL**, for example:\n\n```text\nhttp://your_IP:21001/some/api/actions?key=your_secret_key\n```\n\n### Resetting the `PublicKey` and `PrivateKey` of an Account\n\nThe project supports resetting the `PublicKey` and `PrivateKey` by requesting\nthe following interface:\n\n```bash\ncurl -X POST http://host:port/api/account/reset_key\n```\n\nAfter resetting, it is necessary to re-acquire the subscription content;\notherwise, it may not be usable.\n\n### Setting Your Own `LicenseKey`\n\nIf you already have a `WARP+` `LicenseKey`, you can set it through the\nfollowing interface:\n\n```bash\ncurl -X POST http://host:port/api/account/update_license -H \"Content-Type: application/json\" -d \"{\\\"license_key\\\": \\\"your_license_key\\\"}\"\n```\n\nPlease note that when you set the `LicenseKey`, your `PublicKey` and\n`PrivateKey` will be reset, and you will need to re-acquire the subscription\ncontent.\n\n### Using IPv6 Preference\n\nIn principle, this service obtains IPv6 addresses by reading\nthe `config/result_v6.csv` file. Therefore, you can run the IP preference\nfunction on a server that supports IPv6 to obtain IPv6 addresses. The list of\nobtained access addresses only needs to be written into\nthe `config/result_v6.csv` file. Compared to running the IP preference function\nin a Docker container, this method is more straightforward.\n\nIf you need to run the IP preference function in a Docker container, you can use\nthe `docker-compose_ipv6.yaml` file to make the Docker image support IPv6.\nBefore running, please ensure that your server supports IPv6 and add the\nfollowing content to /etc/docker/daemon.json in the Docker service (remember to\nrestart the Docker service):\n\n```json\n{\n  \"experimental\": true,\n  \"ip6tables\": true,\n  \"ipv6\": true,\n  \"fixed-cidr-v6\": \"2001:db8:1::/64\"\n}\n```\n\nIf you have previously run the Docker service, please stop the previous service\nbefore running:\n\n```bash\ndocker-compose down\n```\n\nThen, you can run the Docker service with the following command:\n\n```bash\ndocker-compose -f docker-compose_ipv6.yaml up -d\n```\n\n## üóÇÔ∏è Attribution\n\nThis project's development was influenced by the following projects.\nThanks to the authors of these open-source projects:\n\n- [warp-script](https://gitlab.com/Misaka-blog/warp-script)\n- [warp](https://replit.com/@aliilapro/warp)\n- [wgcf](https://github.com/ViRb3/wgcf)\n- [proxy_pool](https://github.com/jhao104/proxy_pool)\n- [geolite2](https://dev.maxmind.com/geoip/geolite2-free-geolocation-data)\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": "README_en.md", "language": "markdown", "complexity": "complex", "star_count": 8732, "quality_score": 0.76, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 209}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\r\n\r\nThis program is free software; you can redistribute it and/or modify\r\nit under the terms of the GNU General Public License as published by\r\nthe Free Software Foundation; either version 3 of the License, or\r\n(at your option) any later version.\r\n\r\nThis program is distributed in the hope that it will be useful,\r\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\r\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\r\nGNU General Public License for more details.\r\n\r\nYou should have received a copy of the GNU General Public License\r\nalong with this program; if not, see <https://www.gnu.org/licenses>.\r\n\r\n\"\"\"\r\nimport argparse\r\nimport sys\r\n\r\nfrom config import PORT, HOST\r\nfrom utils.entrypoints import optimizeEntrypoints\r\nfrom utils.logger import createLogger\r\n\r\n\r\ndef linuxStartWeb():\r\n    \"\"\"\r\n    Start web service on linux\r\n    :return:\r\n    \"\"\"\r\n    from gunicorn.app.base import BaseApplication\r\n    from services.web_service import createApp\r\n\r\n    class FlaskGunicornApp(BaseApplication):\r\n        def __init__(self, flaskapp, options=None):\r\n            self.options = options or {}\r\n            self.application = flaskapp\r\n            super().__init__()\r\n\r\n        def load_config(self):\r\n            config = {key: value for key, value in self.options.items() if\r\n                      key in self.cfg.settings and value is not None}\r\n            for key, value in config.items():\r\n                self.cfg.set(key.lower(), value)\r\n\r\n        def load(self):\r\n            return self.application\r\n\r\n    app = createApp(\"web\")\r\n    FlaskGunicornApp(app, options={\r\n        \"bind\": f\"{HOST}:{PORT}\",\r\n        \"workers\": 4,\r\n        \"worker_connections\": 1000,\r\n        \"timeout\": 30,\r\n        \"keepalive\": 2\r\n    }).run()\r\n\r\n\r\ndef main():\r\n    \"\"\"\r\n    Main function\r\n    :return:\r\n    \"\"\"\r\n    parser = argparse.ArgumentParser(description=\"WARP Clash API\")\r\n    parser.add_argument(\"command\", choices=[\"web\", \"background\", \"optimize\"], help=\"Command to run\")\r\n\r\n    args = parser.parse_args()\r\n\r\n    if args.command == \"web\":\r\n        logger = createLogger(\"app_web\")\r\n        from services.web_service import createApp\r\n        app = createApp(\"web\", logger=logger)\r\n\r\n        # If windows, use app.run()\r\n        if sys.platform == \"win32\":\r\n            app.run(host=HOST, port=PORT)\r\n        # If linux, use gunicorn\r\n        else:\r\n            linuxStartWeb()\r\n\r\n    elif args.command == \"background\":\r\n        logger = createLogger(\"app_background\")\r\n        from services.scheduled_service import main\r\n        main(logger=logger)\r\n\r\n    elif args.command == \"optimize\":\r\n        # Windows is not supported for this command\r\n        if sys.platform == 'win32':\r\n            print('This command is not supported on Windows.')\r\n            return\r\n        optimizeEntrypoints()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": "app.py", "language": "python", "complexity": "medium", "star_count": 8732, "quality_score": 0.94, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 95}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\r\n\r\nThis program is free software; you can redistribute it and/or modify\r\nit under the terms of the GNU General Public License as published by\r\nthe Free Software Foundation; either version 3 of the License, or\r\n(at your option) any later version.\r\n\r\nThis program is distributed in the hope that it will be useful,\r\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\r\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\r\nGNU General Public License for more details.\r\n\r\nYou should have received a copy of the GNU General Public License\r\nalong with this program; if not, see <https://www.gnu.org/licenses>.\r\n\r\n\"\"\"\r\nimport os\r\n\r\nDELAY_THRESHOLD = int(os.environ.get('DELAY_THRESHOLD')) if os.environ.get('DELAY_THRESHOLD') else 500\r\nDO_GET_WARP_DATA = os.environ.get('DO_GET_WARP_DATA', 'true').lower() == 'true'\r\nGET_WARP_DATA_INTERVAL = int(os.environ.get('GET_WARP_DATA_INTERVAL', 18))\r\nHOST = os.environ.get('HOST') or '0.0.0.0'\r\nLOSS_THRESHOLD = int(os.environ.get('LOSS_THRESHOLD')) if os.environ.get('LOSS_THRESHOLD') else 10\r\nPORT = int(os.environ.get('PORT')) if os.environ.get('PORT') else 3000\r\nPROXY_POOL_URL = os.environ.get('PROXY_POOL_URL', 'https://getproxy.bzpl.tech/get/')\r\nPUBLIC_URL = os.environ.get('PUBLIC_URL') or None\r\nRANDOM_COUNT = int(os.environ.get('RANDOM_COUNT')) if os.environ.get('RANDOM_COUNT') else 10\r\nREOPTIMIZE_INTERVAL = int(os.environ.get('REOPTIMIZE_INTERVAL', -1))\r\nREQUEST_RATE_LIMIT = int(os.environ.get('REQUEST_RATE_LIMIT')) if os.environ.get('REQUEST_RATE_LIMIT') else 0\r\nSECRET_KEY = os.environ.get('SECRET_KEY') or None\r\nSHARE_SUBSCRIPTION = os.environ.get('SHARE_SUBSCRIPTION', 'false').lower() == 'true'\r\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": "config.py", "language": "python", "complexity": "simple", "star_count": 8732, "quality_score": 0.76, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 32}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\r\n\r\nThis program is free software; you can redistribute it and/or modify\r\nit under the terms of the GNU General Public License as published by\r\nthe Free Software Foundation; either version 3 of the License, or\r\n(at your option) any later version.\r\n\r\nThis program is distributed in the hope that it will be useful,\r\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\r\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\r\nGNU General Public License for more details.\r\n\r\nYou should have received a copy of the GNU General Public License\r\nalong with this program; if not, see <https://www.gnu.org/licenses>.\r\n\r\n\"\"\"\r\nfrom .account import Account\r\nfrom .entrypoint import Entrypoint\r\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": "models/__init__.py", "language": "python", "complexity": "simple", "star_count": 8732, "quality_score": 0.7899999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\r\n\r\nThis program is free software; you can redistribute it and/or modify\r\nit under the terms of the GNU General Public License as published by\r\nthe Free Software Foundation; either version 3 of the License, or\r\n(at your option) any later version.\r\n\r\nThis program is distributed in the hope that it will be useful,\r\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\r\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\r\nGNU General Public License for more details.\r\n\r\nYou should have received a copy of the GNU General Public License\r\nalong with this program; if not, see <https://www.gnu.org/licenses>.\r\n\r\n\"\"\"\r\nimport datetime\r\nimport json\r\nimport logging\r\n\r\nACCOUNT_PATH = \"account/account.json\"\r\n\r\n\r\nclass Account:\r\n    account_id: str = \"\"\r\n    account_type: str = \"\"\r\n    created_at: datetime.datetime = datetime.datetime.now()\r\n    updated_at: datetime.datetime = datetime.datetime.now()\r\n    model: str = \"\"\r\n    referrer: str = \"\"\r\n    private_key: str = \"\"\r\n    license_key: str = \"\"\r\n    token: str = \"\"\r\n    premium_data: int = 0\r\n    quota: int = 0\r\n    usage: int = 0\r\n\r\n    def __str__(self):\r\n        return f\"Account ID: {self.account_id}\\n\" \\\r\n               f\"Account Type: {self.account_type}\\n\" \\\r\n               f\"Created At: {self.created_at}\\n\" \\\r\n               f\"Updated At: {self.updated_at}\\n\" \\\r\n               f\"Model: {self.model}\\n\" \\\r\n               f\"Referrer: {self.referrer}\\n\" \\\r\n               f\"Private Key: {self.private_key}\\n\" \\\r\n               f\"License Key: {self.license_key}\\n\" \\\r\n               f\"Token: {self.token}\\n\" \\\r\n               f\"Premium Data: {self.premium_data}\\n\" \\\r\n               f\"Quota: {self.quota}\\n\" \\\r\n               f\"Used: {self.usage}\\n\"\r\n\r\n    def save(self, file=ACCOUNT_PATH):\r\n        with open(file, \"w\") as f:\r\n            json.dump(self.__dict__, f)\r\n\r\n    @staticmethod\r\n    def load(file=ACCOUNT_PATH):\r\n        try:\r\n            with open(file, \"r\") as f:\r\n                data = json.load(f)\r\n                account = Account()\r\n                account.__dict__ = data\r\n                return account\r\n        except Exception as e:\r\n            logging.error(f\"Failed to load account from file: {e}\")\r\n            return None\r\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": "models/account.py", "language": "python", "complexity": "simple", "star_count": 8732, "quality_score": 0.9099999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 67}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\r\n\r\nThis program is free software; you can redistribute it and/or modify\r\nit under the terms of the GNU General Public License as published by\r\nthe Free Software Foundation; either version 3 of the License, or\r\n(at your option) any later version.\r\n\r\nThis program is distributed in the hope that it will be useful,\r\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\r\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\r\nGNU General Public License for more details.\r\n\r\nYou should have received a copy of the GNU General Public License\r\nalong with this program; if not, see <https://www.gnu.org/licenses>.\r\n\r\n\"\"\"\r\nimport datetime\r\n\r\n\r\nclass Entrypoint:\r\n    ip_type: str = \"ipv4\"\r\n    ip: str = \"\"\r\n    port: int = 0\r\n    loss: float = 0\r\n    delay: int = 0\r\n    last_check: datetime.datetime = datetime.datetime.now()\r\n\r\n    def __str__(self):\r\n        return f\"IP Type: {self.ip_type}\\n\" \\\r\n               f\"IP: {self.ip}\\n\" \\\r\n               f\"Port: {self.port}\\n\" \\\r\n               f\"Loss: {self.loss}\\n\" \\\r\n               f\"Delay: {self.delay}\\n\" \\\r\n               f\"Last Check: {self.last_check}\\n\"\r\n\r\n    def __repr__(self):\r\n        return f\"{self.ip}:{self.port}\"\r\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": "models/entrypoint.py", "language": "python", "complexity": "simple", "star_count": 8732, "quality_score": 0.82, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 38}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/bin/bash\n# This script is modified from https://gitlab.com/Misaka-blog/warp-script/-/raw/main/files/get_entrypoints.sh\n\narchAffix() {\n    case \"$(uname -m)\" in\n        i386 | i686 ) echo '386' ;;\n        x86_64 | amd64 ) echo 'amd64' ;;\n        armv8 | arm64 | aarch64 ) echo 'arm64' ;;\n        s390x ) echo 's390x' ;;\n        * ) echo \"‰∏çÊîØÊåÅÁöÑCPUÊû∂ÊûÑ!\" && exit 1 ;;\n    esac\n}\n\nendpointyx() {\n    # Âà§Êñ≠ warp Êñá‰ª∂ÊòØÂê¶Â≠òÂú®Ôºå‰∏çÂ≠òÂú®Âàô‰∏ãËΩΩ\n    if ! [ -f warp ]; then\n      # ‰∏ãËΩΩ‰ºòÈÄâÂ∑•ÂÖ∑ËΩØ‰ª∂ÔºåÊÑüË∞¢ÊüêÂåøÂêçÁΩëÂèãÁöÑÂàÜ‰∫´ÁöÑ‰ºòÈÄâÂ∑•ÂÖ∑\n      wget https://gitlab.com/vvbbnn00/warp-script/-/raw/main/files/warp-yxip/warp-linux-\"$(archAffix)\" -O warp || { echo \"‰∏ãËΩΩ‰ºòÈÄâÂ∑•ÂÖ∑Â§±Ë¥•\"; exit 1; }\n    fi\n\n    # ÂèñÊ∂à Linux Ëá™Â∏¶ÁöÑÁ∫øÁ®ãÈôêÂà∂Ôºå‰ª•‰æøÁîüÊàê‰ºòÈÄâ Endpoint IP\n    ulimit -n 102400\n\n    # ÂêØÂä® WARP Endpoint IP ‰ºòÈÄâÂ∑•ÂÖ∑\n    chmod +x warp\n\n    if [ \"$suffix\" == \"_v6\" ]; then\n      ./warp -ipv6\n    else\n      ./warp\n    fi\n\n    # Â∞Ü‰ºòÈÄâÁªìÊûúÁßªÂä®Âà∞ÊåáÂÆöÁõÆÂΩï\n    if [ -n \"$RUN_IN_DOCKER\" ]; then\n      mv -f result.csv \"/app/config/result${suffix}.csv\"\n    else\n      mv -f result.csv \"./config/result${suffix}.csv\"\n    fi\n\n    # Âà†Èô§ÈôÑÂ±ûÊñá‰ª∂\n    rm -f ips-v4.txt ips-v6.txt\n}\n\ninit_ipv4() {\n    echo \"162.159.192.0/24\n162.159.193.0/24\n162.159.195.0/24\n162.159.204.0/24\n188.114.96.0/24\n188.114.97.0/24\n188.114.98.0/24\n188.114.99.0/24\" > ips-v4.txt\n}\n\nendpoint4() {\n    init_ipv4\n    endpointyx\n}\n\ninit_ipv6() {\n    echo \"2606:4700:d0::/48\n2606:4700:d1::/48\" > ips-v6.txt\n}\n\nendpoint6() {\n    init_ipv6\n    endpointyx\n}\n\nsuffix=\n\n# Ê£ÄÊü•‰º†ÂÖ•ÂèÇÊï∞ÔºåËã•Âê´Êúâ-4Êàñ-6Âàô‰ºòÈÄâÂØπÂ∫îÁöÑIPÊÆµ\nwhile getopts \"46\" OPT; do\n    case $OPT in\n        6)\n          suffix=_v6\n          endpoint6\n          ;;\n        4)\n          endpoint4\n          ;; # ÈªòËÆ§‰ºòÈÄâ IPv4\n        *)\n          echo \"Usage: $0 [-4|-6]\" && exit 1\n          ;;\n    esac\ndone\n\n# Ëã•Êú™‰º†ÂÖ•ÂèÇÊï∞Âàô‰ºòÈÄâ IPv4\n[ -z \"$1\" ] && endpoint4\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": "scripts/get_entrypoints.sh", "language": "bash", "complexity": "medium", "star_count": 8732, "quality_score": 0.76, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 90}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n\nThis program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation; either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program; if not, see <https://www.gnu.org/licenses>.\n\n\"\"\"\nimport logging\n\nfrom services.cloudflare import updatePublicKey, updateLicenseKey\nfrom services.common import getCurrentAccount\nfrom utils.wireguard import generateWireguardKeys\n\n\ndef resetAccountKey(logger=logging.Logger(__name__)):\n    \"\"\"\n    Reset account private key, and update public key to Cloudflare\n    :param logger:\n    :return:\n    \"\"\"\n    # Get current account\n    account = getCurrentAccount(logger)\n    logger.info(f\"Reset account key for account: {account.account_id}\")\n\n    # Generate new keys\n    privkey, pubkey = generateWireguardKeys()\n    updatePublicKey(account, pubkey)\n    logger.info(f\"New public key: {pubkey}\")\n    logger.info(f\"New private key: {privkey}\")\n\n    # Save new private key\n    account.private_key = privkey\n    # Save account to file\n    account.save()\n\n    logger.info(\"Account key reset done\")\n\n\ndef doUpdateLicenseKey(license_key: str, logger=logging.Logger(__name__)):\n    \"\"\"\n    Update license key, and reset account key\n    :param license_key:\n    :param logger:\n    :return:\n    \"\"\"\n    logger.info(f\"Update license key: {license_key}\")\n\n    # Get current account\n    account = getCurrentAccount(logger)\n\n    if account.license_key == license_key:\n        logger.warning(\"License key is the same, no need to update\")\n        return\n\n    # Update license key\n    updateLicenseKey(account, license_key, logger=logger)\n\n    # Save changes to account\n    account.license_key = license_key\n    account.save()\n\n    # Account needs to be reset after license key update\n    resetAccountKey(logger)\n\n    logger.info(\"License key updated\")\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": "services/account.py", "language": "python", "complexity": "simple", "star_count": 8732, "quality_score": 0.8799999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 75}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\r\n\r\nThis program is free software; you can redistribute it and/or modify\r\nit under the terms of the GNU General Public License as published by\r\nthe Free Software Foundation; either version 3 of the License, or\r\n(at your option) any later version.\r\n\r\nThis program is distributed in the hope that it will be useful,\r\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\r\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\r\nGNU General Public License for more details.\r\n\r\nYou should have received a copy of the GNU General Public License\r\nalong with this program; if not, see <https://www.gnu.org/licenses>.\r\n\r\n\"\"\"\r\nimport datetime\r\nimport random\r\n\r\nimport requests\r\nimport faker\r\nfrom models import Account\r\n\r\nAPI_URL = \"https://api.cloudflareclient.com\"\r\nAPI_VERSION = \"v0i2308311933\"\r\nDEFAULT_HEADERS = {\r\n    \"User-Agent\": \"1.1.1.1/6.23\",\r\n    \"CF-Client-Version\": \"i-6.23-2308311933.1\",\r\n    'Content-Type': 'application/json; charset=UTF-8',\r\n    'Host': 'api.cloudflareclient.com',\r\n    'Connection': 'Keep-Alive',\r\n    'Accept-Encoding': 'gzip',\r\n}\r\n\r\nSESSION = requests.Session()\r\nSESSION.headers.update(DEFAULT_HEADERS)\r\nFAKE = faker.Faker()\r\nLOCALE_LIST = [\"en_US\", \"zh_CN\", \"zh_TW\", \"ja_JP\", \"ko_KR\", \"fr_FR\", \"de_DE\", \"es_ES\"]\r\nDEVICE_MODEL_LIST = [\"iPhone16,2\", \"iPhone16,1\", \"iPhone14,2\", \"iPhone14,1\", \"iPhone13,4\", \"iPhone13,3\", \"iPhone13,2\",\r\n                     \"iPhone13,1\", \"iPhone12,8\", \"iPhone12,5\", \"iPhone12,3\", \"iPhone12,1\", \"iPhone11,8\", \"iPhone11,6\",\r\n                     \"iPhone11,4\", \"iPhone11,2\", \"iPhone10,6\", \"iPhone10,5\", \"iPhone10,4\", \"iPhone10,3\", \"iPhone10,2\",\r\n                     \"iPhone10,1\", \"iPad8,8\", \"iPad8,7\", \"iPad8,6\", \"iPad8,5\", \"iPad8,4\", \"iPad8,3\", \"iPad8,2\",\r\n                     \"iPad8,1\", \"iPad7,6\", \"iPad7,5\", \"iPad7,4\", \"iPad7,3\", \"iPad7,2\", \"iPad7,1\", \"iPad6,12\",\r\n                     \"iPad6,11\", \"iPad5,4\", \"iPad5,3\", \"iPad5,2\", \"iPad5,1\"]\r\n\r\n\r\ndef genAccountFromResponse(response, referrer=None, private_key=None) -> Account:\r\n    \"\"\"\r\n    Generate an account from a response\r\n    :param private_key:\r\n    :param response:\r\n    :param referrer:\r\n    :return:\r\n    \"\"\"\r\n    account = Account()\r\n    response = response[\"result\"]\r\n\r\n    account.model = response[\"model\"]\r\n    account.account_id = response[\"id\"]\r\n    account.account_type = response[\"account\"][\"account_type\"]\r\n    account.token = response[\"token\"]\r\n    account.private_key = private_key\r\n    account.license_key = response[\"account\"][\"license\"]\r\n    account.created_at = response[\"account\"][\"created\"]\r\n    account.updated_at = response[\"account\"][\"updated\"]\r\n    account.premium_data = response[\"account\"][\"premium_data\"]\r\n    account.quota = response[\"account\"][\"quota\"]\r\n    account.usage = response[\"account\"][\"usage\"]\r\n    account.referrer = referrer\r\n\r\n    return account\r\n\r\n\r\ndef register(public_key, private_key, device_model=None, referrer=\"\",\r\n             proxy=None) -> Account:\r\n    \"\"\"\r\n    Register a new device\r\n\r\n    :param private_key: base64 encoded private key\r\n    :param proxy: proxy dict\r\n    :param referrer: referrer ID, optional\r\n    :param public_key: base64 encoded public key\r\n    :param device_model: device model\r\n    :return:\r\n    \"\"\"\r\n    timestamp = datetime.datetime.now().isoformat()[:-3] + \"Z\"\r\n    # install_id is 43 characters long\r\n    install_id = FAKE.pystr(min_chars=43, max_chars=43)\r\n\r\n    # if device_model is not provided, choose a random one\r\n    if device_model is None:\r\n        device_model = random.choice(DEVICE_MODEL_LIST)\r\n\r\n    data = {\r\n        # 152 characters in total\r\n        \"fcm_token\": \"{}:APA91b{}\".format(install_id, FAKE.pystr(min_chars=134, max_chars=134)),\r\n        \"install_id\": install_id,\r\n        \"key\": public_key,\r\n        \"warp_enabled\": True,\r\n        \"locale\": random.choice(LOCALE_LIST),\r\n        \"model\": device_model,\r\n        \"tos\": timestamp,\r\n        \"type\": \"IOS\",\r\n    }\r\n\r\n    if referrer:\r\n        data[\"referrer\"] = referrer\r\n\r\n    # Send the request\r\n    response = SESSION.post(f\"{API_URL}/{API_VERSION}/reg\", json=data, proxies=proxy)\r\n    response.raise_for_status()\r\n\r\n    return genAccountFromResponse(response.json(), referrer=referrer, private_key=private_key)\r\n\r\n\r\ndef getAccount(account: Account, proxy=None) -> dict:\r\n    \"\"\"\r\n    Get account details and update the account object\r\n\r\n    :param account: account\r\n    :param proxy: proxy dict\r\n    :return:\r\n    \"\"\"\r\n\r\n    response = SESSION.get(f\"{API_URL}/{API_VERSION}/reg/{account.account_id}\",\r\n                           headers={\"Authorization\": f\"Bearer {account.token}\"},\r\n                           proxies=proxy)\r\n    response.raise_for_status()\r\n    data = response.json()\r\n    data = data[\"result\"]\r\n\r\n    account.license_key = data[\"account\"].get(\"license\")\r\n    account.premium_data = data[\"account\"].get(\"premium_data\")\r\n    account.quota = data[\"account\"].get(\"quota\")\r\n    account.usage = data[\"account\"].get(\"usage\")\r\n    account.updated_at = data[\"account\"].get(\"updated\")\r\n\r\n    return data[\"account\"]\r\n\r\n\r\ndef updateLicenseKey(account: Account, license_key: str, proxy=None, logger=None) -> dict:\r\n    \"\"\"\r\n    Update license key\r\n\r\n    :param account: account\r\n    :param license_key: license key\r\n    :param proxy: proxy dict\r\n    :param logger: logger\r\n    :return:\r\n    \"\"\"\r\n    timestamp = datetime.datetime.now().isoformat()[:-3] + \"Z\"\r\n\r\n    data = {\r\n        \"tos\": timestamp,\r\n        \"license\": license_key,\r\n    }\r\n\r\n    response = SESSION.put(f\"{API_URL}/{API_VERSION}/reg/{account.account_id}/account\",\r\n                           headers={\"Authorization\": f\"Bearer {account.token}\"},\r\n                           json=data, proxies=proxy)\r\n\r\n    if response.status_code >= 400:\r\n        if logger:\r\n            logger.error(f\"Failed to update license key, response: {response.text}\")\r\n\r\n    response.raise_for_status()\r\n\r\n    return response.json()[\"result\"]\r\n\r\n\r\ndef updatePublicKey(account: Account, public_key: str, proxy=None) -> dict:\r\n    \"\"\"\r\n    Update public key\r\n\r\n    :param account: account\r\n    :param public_key: public key\r\n    :param proxy: proxy dict\r\n    :return:\r\n    \"\"\"\r\n    data = {\r\n        \"key\": public_key\r\n    }\r\n\r\n    response = SESSION.put(f\"{API_URL}/{API_VERSION}/reg/{account.account_id}\",\r\n                           headers={\"Authorization\": f\"Bearer {account.token}\"},\r\n                           json=data, proxies=proxy)\r\n    response.raise_for_status()\r\n\r\n    return response.json()[\"result\"]\r\n\r\n\r\ndef getClientConfig(proxy=None) -> dict:\r\n    \"\"\"\r\n    Get client config\r\n\r\n    :param proxy: proxy dict\r\n    :return:\r\n    \"\"\"\r\n\r\n    response = SESSION.get(f\"{API_URL}/{API_VERSION}/client_config\",\r\n                           proxies=proxy)\r\n    response.raise_for_status()\r\n\r\n    return response.json()[\"result\"]\r\n", "metadata": {"source": "github.com/vvbbnn00/WARP-Clash-API", "file_path": "services/cloudflare.py", "language": "python", "complexity": "complex", "star_count": 8732, "quality_score": 0.8799999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 205}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Configuration for probot-stale - https://github.com/probot/stale\n\n# Number of days of inactivity before an Issue or Pull Request becomes stale\ndaysUntilStale: 21\n\n# Number of days of inactivity before an Issue or Pull Request with the stale label is closed.\n# Set to false to disable. If disabled, issues still need to be closed manually, but will remain marked as stale.\ndaysUntilClose: 14\n\n# Only issues or pull requests with all of these labels are check if stale. Defaults to `[]` (disabled)\nonlyLabels: []\n\n# Issues or Pull Requests with these labels will never be considered stale. Set to `[]` to disable\nexemptLabels:\n  - priority/p0\n  - priority/p1\n  - priority/p2\n  - backlog\n  - status/wip\n  - type/bug\n  - type/discussion\n  - type/enhancement\n  - type/feature\n  - type/security\n\n# Set to true to ignore issues in a project (defaults to false)\nexemptProjects: true\n\n# Set to true to ignore issues in a milestone (defaults to false)\nexemptMilestones: true\n\n# Set to true to ignore issues with an assignee (defaults to false)\nexemptAssignees: true\n\n# Label to use when marking as stale\nstaleLabel: status/response_needed\n\n# Comment to post when marking as stale. Set to `false` to disable\nmarkComment: |\n  Issues not for bugs, enhancement requests or discussion go stale after 21 days of inactivity. This issue will be automatically closed after 14 days.\n  For all metrics refer to the [stale.yml file](https://github.com/Mailu/Mailu/blob/master/.github/stale.yml).\n  Github issues are not meant for user support. For **user-support questions**, reach out  on the [matrix support channel](https://matrix.to/#/#mailu:tedomum.net).\n\n  Mark the issue as fresh by simply adding a comment to the issue.\n  If this issue is safe to close, please do so now.\n\n\n# Comment to post when removing the stale label.\n# unmarkComment: >\n#   Your comment here.\n\n# Comment to post when closing a stale Issue or Pull Request.\ncloseComment: >\n  This issue has not seen activity since as it has become stale.\n  Stale issues are automatically closed after 14 days.\n\n# Limit the number of actions per hour, from 1-30. Default is 30\nlimitPerRun: 30\n\n# Limit to only `issues` or `pulls`\nonly: issues\n\n# Optionally, specify configuration settings that are specific to just 'issues' or 'pulls':\n# pulls:\n#   <any of above config here>\n#\n# issues:\n#   <any of above config here>\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": ".github/stale.yml", "language": "yaml", "complexity": "simple", "star_count": 6665, "quality_score": 0.7899999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 69}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "###############################################\n# REQUIRED secrets\n# ${{ secrets.Docker_Login }}\n#    Username of docker login for logging in docker for pulling images (higher pull rate limit)\n# ${{ secrets.Docker_Password }}\n#    Password of docker login for logging in docker for pulling images (higher pull rate limit)\n# ${{ secrets.Docker_Login2 }}\n#    Second Username of docker login for logging in docker for pulling images (higher pull rate limit)\n# ${{ secrets.Docker_Password2 }}\n#    Second Password of docker login for logging in docker for pulling images (higher pull rate limit)\n################################################\n\nname: build-test-deploy\non:\n  workflow_call:\n    inputs:\n      architecture:\n        description: 'The architecture(s) of the images that will be build. linux/amd64 or linux/arm64/v8,linux/arm/v7 or linux/amd64,linux/arm64/v8,linux/arm/v7'\n        required: false\n        default: 'linux/amd64,linux/arm64/v8,linux/arm/v7'\n        type: string\n      mailu_version:\n        description: 'The main version that is build. E.g. master or x.y.'\n        required: true\n        type: string\n      pinned_mailu_version:\n        description: 'The specific version that is build. E.g. commit hash or x.y.z.'\n        required: true\n        type: string\n      docker_org:\n        description: 'The docker organisation where the images are pushed to. E.g. ghcr.io/mailu'\n        required: true\n        type: string\n      branch:\n        description: 'The branch that triggered this workflow.'\n        required: true\n        type: string\n      deploy:\n        description: Deploy to container registry. Happens for all branches but staging. Use string true or false.\n        default: true\n        required: false\n        type: string\n      release:\n        description: Tag and create the github release. Use string true or false.\n        default: false\n        required: false\n        type: string\n\n  workflow_dispatch:\n    inputs:\n      architecture:\n        description: 'The architecture(s) of the images that will be build. linux/amd64 or linux/arm64/v8,linux/arm/v7 or linux/amd64,linux/arm64/v8,linux/arm/v7'\n        required: false\n        default: 'linux/amd64,linux/arm64/v8,linux/arm/v7'\n        type: string\n      mailu_version:\n        description: 'The main version that is build. E.g. master or x.y.'\n        required: true\n        type: string\n      pinned_mailu_version:\n        description: 'The specific version that is build. E.g. commit hash or x.y.z.'\n        required: true\n        type: string\n      docker_org:\n        description: 'The docker organisation where the images are pushed to. E.g. ghcr.io/mailu'\n        required: true\n        type: string\n      branch:\n        description: 'The branch that triggered this workflow.'\n        required: true\n        type: string\n      deploy:\n        description: Deploy to container registry. Happens for all branches but staging. Use string true or false.\n        default: true\n        required: false\n        type: string\n      release:\n        description: Tag and create the github release. Use string true or false.\n        default: false\n        required: false\n        type: string\nenv:\n  HCL_FILE: ./tests/build-ci.hcl\n\njobs:\n# This job calculates what images must be build. It reads the build.hcl file and then outputs all targets (images) in it.\n# This is used by the next build job.\n  targets:\n    name: create targets\n    runs-on: ubuntu-latest\n    outputs:\n      matrix: ${{ steps.targets.outputs.matrix }}\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n      - name: Create matrix\n        id: targets\n        run: |\n          echo matrix=$(docker buildx bake -f ${{env.HCL_FILE}} --print | jq -cr '.group.default.targets') >> $GITHUB_OUTPUT\n      - name: Show matrix\n        run: |\n          echo ${{ steps.targets.outputs.matrix }}\n\n# This job builds the base and assets images. The build cache is stored in the ghcr.io registry.\n  build-base-assets-x64:\n    name: x64 - build intermediate\n    if: contains(inputs.architecture, 'linux/amd64')\n    strategy:\n      fail-fast: false\n      matrix:\n        target: [\"base\", \"assets\"]\n    runs-on: ubuntu-latest\n    permissions:\n     contents: read\n     packages: write\n    steps:\n      - uses: actions/checkout@v4\n      - name: Retrieve global variables\n        shell: bash\n        run: |\n          echo \"BRANCH=${{ inputs.branch }}\" >> $GITHUB_ENV\n          echo \"MAILU_VERSION=${{ inputs.mailu_version }}\" >> $GITHUB_ENV\n          echo \"PINNED_MAILU_VERSION=${{ inputs.pinned_mailu_version }}\" >> $GITHUB_ENV\n          echo \"DOCKER_ORG=${{ inputs.docker_org }}\" >> $GITHUB_ENV\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      - uses: crazy-max/ghaction-github-runtime@v3\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      - name: Helper to convert docker org to lowercase\n        id: string\n        uses: ASzc/change-string-case-action@v6\n        with:\n          string: ${{ github.repository_owner }}\n      - name: Get uuid\n        id: uuid\n        run: |\n            echo uuid=$RANDOM >> $GITHUB_OUTPUT\n      - name: Build docker image with retry\n        env:\n          DOCKER_ORG: ghcr.io/${{ steps.string.outputs.lowercase }}\n          MAILU_VERSION: ${{ env.MAILU_VERSION }}-build\n          PINNED_MAILU_VERSION: ${{ env.PINNED_MAILU_VERSION }}-build\n          LABEL_VERSION: ${{ env.MAILU_VERSION }}\n          PINNED_LABEL_VERSION: ${{ env.PINNED_MAILU_VERSION }}\n          ARCH: 'linux/amd64'\n          BUILDER: ${{ steps.uuid.outputs.uuid }}\n          DOCKER_LOGIN: ${{ secrets.Docker_Login }}\n          DOCKER_PASSW: ${{ secrets.Docker_Password }}\n          BUILDX_NO_DEFAULT_ATTESTATIONS: 1\n        uses: nick-fields/retry@v3\n        with:\n          timeout_minutes: 20\n          retry_wait_seconds: 30\n          max_attempts: 3\n          shell: bash\n          command: |\n            set -euxo pipefail \\\n              ; /usr/bin/docker info \\\n              ; echo \"${{ github.token }}\" |        docker login --username \"${{ github.repository_owner }}\" --password-stdin ghcr.io \\\n              ; echo \"$DOCKER_PASSW\" |   docker login --username \"$DOCKER_LOGIN\" --password-stdin \\\n              ; /usr/bin/docker buildx rm builder-${{ env.BUILDER }} \\\n              || echo \"builder does not exist\" \\\n              ; /usr/bin/docker buildx create --name builder-${{ env.BUILDER }} --driver docker-container --use \\\n              ; /usr/bin/docker buildx bake --push \\\n              --file ./tests/build-ci.hcl \\\n              --set *.cache-from=type=registry,ref=ghcr.io/${{ steps.string.outputs.lowercase }}/${{ matrix.target }}:buildcache \\\n              --set *.cache-to=type=registry,ref=ghcr.io/${{ steps.string.outputs.lowercase }}/${{ matrix.target }}:buildcache,mode=max \\\n              --set *.platform=${{ env.ARCH }} ${{ matrix.target }} \\\n              ; /usr/bin/docker buildx rm builder-${{ env.BUILDER }}\n      - name: cleanup docker buildx instance after failure of build step\n        if: ${{ failure() }}\n        shell: bash\n        env:\n          BUILDER: ${{ steps.uuid.outputs.uuid }}\n        run: |\n          /usr/bin/docker buildx rm builder-${{ env.BUILDER }}\n\n# This job builds the base and assets images. The build cache is stored in the ghcr.io registry.\n  build-base-assets-arm:\n    name: ARM - build intermediate\n    if: contains(inputs.architecture, 'linux/arm64/v8,linux/arm/v7')\n    strategy:\n      fail-fast: false\n      matrix:\n        target: [\"base\", \"assets\"]\n    runs-on: self-hosted\n    permissions:\n     contents: read\n     packages: write\n    steps:\n      - uses: actions/checkout@v4\n      - name: Retrieve global variables\n        shell: bash\n        run: |\n          echo \"BRANCH=${{ inputs.branch }}\" >> $GITHUB_ENV\n          echo \"MAILU_VERSION=${{ inputs.mailu_version }}\" >> $GITHUB_ENV\n          echo \"PINNED_MAILU_VERSION=${{ inputs.pinned_mailu_version }}\" >> $GITHUB_ENV\n          echo \"DOCKER_ORG=${{ inputs.docker_org }}\" >> $GITHUB_ENV\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      - uses: crazy-max/ghaction-github-runtime@v3\n      - name: Helper to convert docker org to lowercase\n        id: string\n        uses: ASzc/change-string-case-action@v6\n        with:\n          string: ${{ github.repository_owner }}\n      #This is to prevent to shared runners from generating the same uuid\n      - name: Get unique random number\n        id: uuid\n        run: |\n            echo uuid=$RANDOM >> $GITHUB_OUTPUT\n      - name: Build docker image with retry\n        env:\n          DOCKER_ORG: ghcr.io/${{ steps.string.outputs.lowercase }}\n          MAILU_VERSION: ${{ env.MAILU_VERSION }}-build-arm\n          PINNED_MAILU_VERSION: ${{ env.PINNED_MAILU_VERSION }}-build-arm\n          LABEL_VERSION: ${{ env.MAILU_VERSION }}\n          PINNED_LABEL_VERSION: ${{ env.PINNED_MAILU_VERSION }}\n          ARCH: linux/arm64/v8,linux/arm/v7\n          BUILDER: ${{ steps.uuid.outputs.uuid }}\n          DOCKER_LOGIN2: ${{ secrets.Docker_Login2 }}\n          DOCKER_PASSW2: ${{ secrets.Docker_Password2 }}\n          BUILDX_NO_DEFAULT_ATTESTATIONS: 1\n        uses: nick-fields/retry@v3\n        with:\n          timeout_minutes: 30\n          retry_wait_seconds: 30\n          max_attempts: 10\n          shell: bash\n          command: |\n            set -euxo pipefail \\\n              ; /usr/bin/docker info \\\n              ; echo \"${{ github.token }}\" |        docker login --username \"${{ github.repository_owner }}\" --password-stdin ghcr.io \\\n              ; echo \"$DOCKER_PASSW2\" |    docker login --username \"$DOCKER_LOGIN2\" --password-stdin \\\n              ; /usr/bin/docker buildx rm builder-${{ env.BUILDER }} \\\n              || echo \"builder does not exist\" \\\n              ; /usr/bin/docker buildx create --name builder-${{ env.BUILDER }} --driver docker-container --use \\\n              ; /usr/bin/docker buildx bake --push \\\n              --file ./tests/build-ci.hcl \\\n              --set *.cache-from=type=registry,ref=ghcr.io/${{ steps.string.outputs.lowercase }}/${{ matrix.target }}:buildcache-arm \\\n              --set *.cache-to=type=registry,ref=ghcr.io/${{ steps.string.outputs.lowercase }}/${{ matrix.target }}:buildcache-arm,mode=max \\\n              --set *.platform=${{ env.ARCH }} ${{ matrix.target }} \\\n              ; /usr/bin/docker buildx rm builder-${{ env.BUILDER }}\n      - name: cleanup docker buildx instance after failure of build step\n        if: ${{ failure() }}\n        shell: bash\n        env:\n          BUILDER: ${{ steps.uuid.outputs.uuid }}\n        run: |\n          /usr/bin/docker buildx rm builder-${{ env.BUILDER }}\n\n# This job builds the main images. The build cache is stored in the ghcr.io registry.\n  build:\n    name: x64 - build\n    if: contains(inputs.architecture, 'linux/amd64')\n    needs:\n      - targets\n      - build-base-assets-x64\n    strategy:\n      fail-fast: false\n      matrix:\n        target: ${{ fromJson(needs.targets.outputs.matrix) }}\n    runs-on: ubuntu-latest\n    permissions:\n     contents: read\n     packages: write\n    steps:\n      - uses: actions/checkout@v4\n      - name: Retrieve global variables\n        shell: bash\n        run: |\n          echo \"BRANCH=${{ inputs.branch }}\" >> $GITHUB_ENV\n          echo \"MAILU_VERSION=${{ inputs.mailu_version }}\" >> $GITHUB_ENV\n          echo \"PINNED_MAILU_VERSION=${{ inputs.pinned_mailu_version }}\" >> $GITHUB_ENV\n          echo \"DOCKER_ORG=${{ inputs.docker_org }}\" >> $GITHUB_ENV\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      - uses: crazy-max/ghaction-github-runtime@v3\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      - name: Helper to convert docker org to lowercase\n        id: string\n        uses: ASzc/change-string-case-action@v6\n        with:\n          string: ${{ github.repository_owner }}\n      - name: Get uuid\n        id: uuid\n        run: |\n            echo uuid=$RANDOM >> $GITHUB_OUTPUT\n      - name: Build docker image with retry\n        env:\n          DOCKER_ORG: ghcr.io/${{ steps.string.outputs.lowercase }}\n          MAILU_VERSION: ${{ env.MAILU_VERSION }}-build\n          PINNED_MAILU_VERSION: ${{ env.PINNED_MAILU_VERSION }}-build\n          LABEL_VERSION: ${{ env.MAILU_VERSION }}\n          PINNED_LABEL_VERSION: ${{ env.PINNED_MAILU_VERSION }}\n          ARCH: 'linux/amd64'\n          BUILDER: ${{ steps.uuid.outputs.uuid }}\n          DOCKER_LOGIN: ${{ secrets.Docker_Login }}\n          DOCKER_PASSW: ${{ secrets.Docker_Password }}\n          BUILDX_NO_DEFAULT_ATTESTATIONS: 1\n        uses: nick-fields/retry@v3\n        with:\n          timeout_minutes: 20\n          retry_wait_seconds: 30\n          max_attempts: 3\n          shell: bash\n          command: |\n            set -euxo pipefail \\\n              ; /usr/bin/docker info \\\n              ; echo \"${{ github.token }}\" |        docker login --username \"${{ github.repository_owner }}\" --password-stdin ghcr.io \\\n              ; echo \"$DOCKER_PASSW\" |   docker login --username \"$DOCKER_LOGIN\" --password-stdin \\\n              ; /usr/bin/docker buildx rm builder-${{ env.BUILDER }} \\\n              || echo \"builder does not exist\" \\\n              ; /usr/bin/docker buildx create --name builder-${{ env.BUILDER }} --driver docker-container --use \\\n              ; /usr/bin/docker buildx bake --push \\\n              --file ./tests/build-ci.hcl \\\n              --set *.cache-from=type=registry,ref=ghcr.io/${{ steps.string.outputs.lowercase }}/${{ matrix.target }}:buildcache \\\n              --set *.cache-to=type=registry,ref=ghcr.io/${{ steps.string.outputs.lowercase }}/${{ matrix.target }}:buildcache,mode=max \\\n              --set *.platform=${{ env.ARCH }} ${{ matrix.target }} \\\n              ; /usr/bin/docker buildx rm builder-${{ env.BUILDER }}\n      - name: cleanup docker buildx instance after failure of build step\n        if: ${{ failure() }}\n        shell: bash\n        env:\n          BUILDER: ${{ steps.uuid.outputs.uuid }}\n        run: |\n          /usr/bin/docker buildx rm builder-${{ env.BUILDER }}\n\n# This job builds the main images. The build cache is stored in the ghcr.io registry.\n  build-arm:\n    name: ARM - build\n    if: contains(inputs.architecture, 'linux/arm64/v8,linux/arm/v7')\n    needs:\n      - targets\n      - build-base-assets-arm\n    strategy:\n      fail-fast: false\n      matrix:\n        target: ${{ fromJson(needs.targets.outputs.matrix) }}\n    runs-on: self-hosted\n    permissions:\n     contents: read\n     packages: write\n    steps:\n      - uses: actions/checkout@v4\n      - name: Retrieve global variables\n        shell: bash\n        run: |\n          echo \"BRANCH=${{ inputs.branch }}\" >> $GITHUB_ENV\n          echo \"MAILU_VERSION=${{ inputs.mailu_version }}\" >> $GITHUB_ENV\n          echo \"PINNED_MAILU_VERSION=${{ inputs.pinned_mailu_version }}\" >> $GITHUB_ENV\n          echo \"DOCKER_ORG=${{ inputs.docker_org }}\" >> $GITHUB_ENV\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      - uses: crazy-max/ghaction-github-runtime@v3\n      - name: Helper to convert docker org to lowercase\n        id: string\n        uses: ASzc/change-string-case-action@v6\n        with:\n          string: ${{ github.repository_owner }}\n      #This is to prevent to shared runners from generating the same uuid\n      - name: Get unique random number\n        id: uuid\n        run: |\n            echo uuid=$RANDOM >> $GITHUB_OUTPUT\n      - name: Build docker image with retry\n        env:\n          DOCKER_ORG: ghcr.io/${{ steps.string.outputs.lowercase }}\n          MAILU_VERSION: ${{ env.MAILU_VERSION }}-build-arm\n          PINNED_MAILU_VERSION: ${{ env.PINNED_MAILU_VERSION }}-build-arm\n          LABEL_VERSION: ${{ env.MAILU_VERSION }}\n          PINNED_LABEL_VERSION: ${{ env.PINNED_MAILU_VERSION }}\n          ARCH: linux/arm64/v8,linux/arm/v7\n          BUILDER: ${{ steps.uuid.outputs.uuid }}\n          DOCKER_LOGIN2: ${{ secrets.Docker_Login2 }}\n          DOCKER_PASSW2: ${{ secrets.Docker_Password2 }}\n          BUILDX_NO_DEFAULT_ATTESTATIONS: 1\n        uses: nick-fields/retry@v3\n        with:\n          timeout_minutes: 30\n          retry_wait_seconds: 30\n          max_attempts: 10\n          shell: bash\n          command: |\n            set -euxo pipefail \\\n              ; /usr/bin/docker info \\\n              ; echo \"${{ github.token }}\" |        docker login --username \"${{ github.repository_owner }}\" --password-stdin ghcr.io \\\n              ; echo \"$DOCKER_PASSW2\" |    docker login --username \"$DOCKER_LOGIN2\" --password-stdin \\\n              ; /usr/bin/docker buildx rm builder-${{ env.BUILDER }} \\\n              || echo \"builder does not exist\" \\\n              ; /usr/bin/docker buildx create --name builder-${{ env.BUILDER }} --driver docker-container --use \\\n              ; /usr/bin/docker buildx bake --push \\\n              --file ./tests/build-ci.hcl \\\n              --set *.cache-from=type=registry,ref=ghcr.io/${{ steps.string.outputs.lowercase }}/${{ matrix.target }}:buildcache-arm \\\n              --set *.cache-to=type=registry,ref=ghcr.io/${{ steps.string.outputs.lowercase }}/${{ matrix.target }}:buildcache-arm,mode=max \\\n              --set *.platform=${{ env.ARCH }} ${{ matrix.target }} \\\n              ; /usr/bin/docker buildx rm builder-${{ env.BUILDER }}\n      - name: cleanup docker buildx instance after failure of build step\n        if: ${{ failure() }}\n        shell: bash\n        env:\n          BUILDER: ${{ steps.uuid.outputs.uuid }}\n        run: |\n          /usr/bin/docker buildx rm builder-${{ env.BUILDER }}\n\n# This job runs all the tests.\n  tests:\n    name: tests\n    if: contains(inputs.architecture, 'linux/amd64')\n    runs-on: ubuntu-latest\n    permissions:\n     contents: read\n     packages: read\n    needs:\n      - build\n    strategy:\n      fail-fast: false\n      matrix:\n        target: [\"api\", \"core\", \"fetchmail\", \"filters\", \"webmail\", \"webdav\"]\n        time: [\"2\"]\n        include:\n          - target: \"filters\"\n            time: \"3\"\n        exclude:\n          - target: \"filters\"\n            time: \"2\"\n    steps:\n      - uses: actions/checkout@v4\n      - name: Retrieve global variables\n        shell: bash\n        run: |\n          echo \"BRANCH=${{ inputs.branch }}\" >> $GITHUB_ENV\n          echo \"MAILU_VERSION=${{ inputs.mailu_version }}\" >> $GITHUB_ENV\n          echo \"PINNED_MAILU_VERSION=${{ inputs.pinned_mailu_version }}\" >> $GITHUB_ENV\n          echo \"DOCKER_ORG=${{ inputs.docker_org }}\" >> $GITHUB_ENV\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      - uses: crazy-max/ghaction-github-runtime@v3\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      - name: Login to GitHub Container Registry\n        uses: docker/login-action@v3\n        with:\n          registry: ghcr.io\n          username: ${{ github.repository_owner }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n      - name: Helper to convert docker org to lowercase\n        id: string\n        uses: ASzc/change-string-case-action@v6\n        with:\n          string: ${{ github.repository_owner }}\n      - name: Install python packages\n        run: python3 -m pip install -r tests/requirements.txt\n      - name: Copy all certs\n        run: sudo -- sh -c 'mkdir -p /mailu && cp -r tests/certs /mailu && chmod 600 /mailu/certs/*'\n      - name: Test ${{ matrix.target }}\n        run: python tests/compose/test.py ${{ matrix.target }} ${{ matrix.time }}\n        env:\n          DOCKER_ORG: ghcr.io/${{ steps.string.outputs.lowercase }}\n          MAILU_VERSION: ${{ env.MAILU_VERSION }}-build\n          PINNED_MAILU_VERSION: ${{ env.PINNED_MAILU_VERSION }}-build\n\n  deploy:\n    name: Deploy images\n    # Deploying is not required for staging\n    if: inputs.deploy == 'true'\n    runs-on: ubuntu-latest\n    needs:\n      - build\n      - build-arm\n      - tests\n    strategy:\n      fail-fast: false\n      matrix:\n        target: [\"setup\", \"docs\", \"fetchmail\", \"webmail\", \"admin\", \"traefik-certdumper\", \"radicale\", \"rspamd\", \"oletools\", \"postfix\", \"dovecot\", \"unbound\", \"nginx\"]\n    steps:\n      - uses: actions/checkout@v4\n      - name: Retrieve global variables\n        shell: bash\n        run: |\n          echo \"BRANCH=${{ inputs.branch }}\" >> $GITHUB_ENV\n          echo \"MAILU_VERSION=${{ inputs.mailu_version }}\" >> $GITHUB_ENV\n          echo \"PINNED_MAILU_VERSION=${{ inputs.pinned_mailu_version }}\" >> $GITHUB_ENV\n          echo \"DOCKER_ORG=${{ inputs.docker_org }}\" >> $GITHUB_ENV\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      - uses: crazy-max/ghaction-github-runtime@v3\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      - name: Login to GitHub Container Registry\n        uses: docker/login-action@v3\n        with:\n          registry: ghcr.io\n          username: ${{ github.repository_owner }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n      - name: Helper to convert docker org to lowercase\n        id: string\n        uses: ASzc/change-string-case-action@v6\n        with:\n          string: ${{ github.repository_owner }}\n      - name: Push multiarch image to Github (ghcr.io)\n        if: contains(inputs.architecture, 'linux/amd64') && contains(inputs.architecture, 'linux/arm64/v8,linux/arm/v7')\n        shell: bash\n        run: |\n          if [ '${{ env.MAILU_VERSION }}' == 'master' ]; then pinned_mailu_version='master'; else pinned_mailu_version=${{ env.PINNED_MAILU_VERSION}}; fi;\n          docker buildx imagetools create \\\n            --tag ${{ inputs.docker_org }}/${{ matrix.target }}:${{ env.MAILU_VERSION }} \\\n            --tag ${{ inputs.docker_org }}/${{ matrix.target }}:$pinned_mailu_version \\\n            --tag ${{ inputs.docker_org }}/${{ matrix.target }}:latest \\\n            ghcr.io/${{ steps.string.outputs.lowercase }}/${{ matrix.target }}:${{ env.MAILU_VERSION }}-build \\\n            ghcr.io/${{ steps.string.outputs.lowercase }}/${{ matrix.target }}:${{ env.MAILU_VERSION }}-build-arm\n      - name: Push x64 image to Github (ghcr.io)\n        if: contains(inputs.architecture, 'linux/amd64') && !contains(inputs.architecture, 'linux/arm64/v8,linux/arm/v7')\n        shell: bash\n        run: |\n          if [ '${{ env.MAILU_VERSION }}' == 'master' ]; then pinned_mailu_version='master'; else pinned_mailu_version=${{ env.PINNED_MAILU_VERSION}}; fi;\n          docker buildx imagetools create \\\n            --tag ${{ inputs.docker_org }}/${{ matrix.target }}:${{ env.MAILU_VERSION }} \\\n            --tag ${{ inputs.docker_org }}/${{ matrix.target }}:$pinned_mailu_version \\\n            --tag ${{ inputs.docker_org }}/${{ matrix.target }}:latest \\\n            ghcr.io/${{ steps.string.outputs.lowercase }}/${{ matrix.target }}:${{ env.MAILU_VERSION }}-build\n      - name: Push arm image to Github (ghcr.io)\n        if: contains(inputs.architecture, 'linux/arm64/v8,linux/arm/v7') && !contains(inputs.architecture, 'linux/amd64')\n        shell: bash\n        run: |\n          if [ '${{ env.MAILU_VERSION }}' == 'master' ]; then pinned_mailu_version='master'; else pinned_mailu_version=${{ env.PINNED_MAILU_VERSION}}; fi;\n          docker buildx imagetools create \\\n            --tag ${{ inputs.docker_org }}/${{ matrix.target }}:${{ env.MAILU_VERSION }} \\\n            --tag ${{ inputs.docker_org }}/${{ matrix.target }}:$pinned_mailu_version \\\n            --tag ${{ inputs.docker_org }}/${{ matrix.target }}:latest \\\n            ghcr.io/${{ steps.string.outputs.lowercase }}/${{ matrix.target }}:${{ env.MAILU_VERSION }}-build-arm\n\n#This job creates a tagged release. A tag is created for the pinned version x.y.z. The GH release refers to this tag.\n  tag-release:\n    if: inputs.release == 'true'\n    runs-on: ubuntu-latest\n    needs:\n      - deploy\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          # fetch-depth 0 is required to also retrieve all tags.\n          fetch-depth: 0\n        # A bug in actions/checkout@v4 results in all files having mtime of the job running.\n      - name: Restore Timestamps\n        uses: chetan/git-restore-mtime-action@v2\n      - name: Retrieve global variables\n        shell: bash\n        run: |\n          echo \"MAILU_VERSION=${{ inputs.mailu_version }}\" >> $GITHUB_ENV\n          echo \"PINNED_MAILU_VERSION=${{ inputs.pinned_mailu_version }}\" >> $GITHUB_ENV\n      - name: Create tag for branch x.y.\n        shell: bash\n        run: |\n          echo git tag ${{ env.PINNED_MAILU_VERSION }} $(/usr/bin/git rev-parse HEAD)\n          git tag ${{ env.PINNED_MAILU_VERSION }} $(/usr/bin/git rev-parse HEAD)\n          git push origin ${{ env.PINNED_MAILU_VERSION }}\n      - name: Show list of changelog files (we pick the newest)\n        shell: bash\n        run: |\n          ls -Artl towncrier/newsfragments\n      - name: Get latest changelog\n        id: changelog\n        shell: bash\n        run: |\n            delimiter=\"$RANDOM\"\n            pushd . && cd towncrier/newsfragments && ls -Art | tail -n 1 | cut -d. -f1 | xargs -0I % echo \"issue=%\" >> $GITHUB_OUTPUT && popd\n            echo \"content<<${delimiter}\" >> \"${GITHUB_OUTPUT}\"\n            pushd . && cd towncrier/newsfragments && ls -Art | tail -n 1 | xargs cat | xargs -0I % echo \"%\" >> \"${GITHUB_OUTPUT}\" && popd\n            echo \"${delimiter}\" >> \"${GITHUB_OUTPUT}\"\n      - name: Construct message for release\n        shell: bash\n        env:\n          issue: \"${{ steps.changelog.outputs.issue }}\"\n          changelog: \"${{ steps.changelog.outputs.content }}\"\n        run: |\n            EOT=$(dd if=/dev/urandom bs=15 count=1 status=none | base64)\n            cat << \"${EOT}\" >> release_note.md\n            Changelog :mailbox:\n            ---------\n            + ${{ env.changelog }}\n\n            + This release was triggered by PR/Issue [${{ env.issue }}](https://github.com/Mailu/Mailu/issues/${{ env.issue }}).\n\n            + The release notes of the original main release can be accessed via menu item 'Release notes' on [mailu.io](https://mailu.io/).\n\n            Update\n            ------\n            The main version X.Y (e.g. 1.9) will always reflect the latest version of the branch. To update your Mailu installation simply pull the latest images \\`docker compose pull && docker compose up -d\\`.\n\n            The pinned version X.Y.Z (e.g. 1.9.1) is not updated. It is pinned to the commit that was used for creating this release. You can use a pinned version to make sure your Mailu installation is not suddenly updated when recreating containers. The pinned version allows the user to manually update. It also allows to go back to a previous pinned version.\n            ${EOT}\n      - name: Show release note\n        shell: bash\n        run: |\n            cat release_note.md\n      - name: Create release for tag x.y.z.\n        uses: ncipollo/release-action@v1\n        with:\n          bodyFile: \"release_note.md\"\n          tag: ${{ env.PINNED_MAILU_VERSION }}\n          token: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": ".github/workflows/build_test_deploy.yml", "language": "yaml", "complexity": "complex", "star_count": 6665, "quality_score": 0.7899999999999999, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 605}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: 'Lock closed issues'\n# Workflow that automatically locks already closed issues,\n# pull requests and discussions.\n\non:\n  schedule:\n    - cron: '30 6 * * *'\n  workflow_dispatch:\n\npermissions:\n  issues: write\n  pull-requests: write\n  discussions: write\n\nconcurrency:\n  group: lock-threads\n\njobs:\n  action:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: dessant/lock-threads@v5\n        with:\n          issue-inactive-days: 21\n          pr-inactive-days: 28\n          discussion-inactive-days: 28\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": ".github/workflows/lock-closed-issues.yml", "language": "yaml", "complexity": "simple", "star_count": 6665, "quality_score": 0.7, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 27}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Used for creating mirror images of images currently on docker.io.\n# The main reason is to circumvent the docker pull rate-limit\n# Currently this is used for:\n# Alpine (used in Dockerfile for base image)\n\nname: mirror-images\non:\n  workflow_dispatch:\n    inputs:\n      tag:\n        description: 'The tag of the docker.io/alpine image that is mirrored as ghcr.io/mailu/alpine'\n        required: true\n        type: string\n\njobs:\n# mirror alpine image\n  mirror-image-alpine:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Helper to convert docker org to lowercase\n        id: string\n        uses: ASzc/change-string-case-action@v5\n        with:\n          string: ${{ github.repository_owner }}\n      - name: Mirror images\n        shell: bash\n        env:\n          DOCKER_ORG: ghcr.io/${{ steps.string.outputs.lowercase }}\n          TAG: ${{ inputs.tag }}\n        run: |\n          set -euxo pipefail \\\n          ; echo \"${{ github.token }}\" | docker login --username \"${{ github.repository_owner }}\" --password-stdin ghcr.io \\\n          ; curl -L https://github.com/regclient/regclient/releases/latest/download/regctl-linux-amd64 >regctl \\\n          ; chmod 755 regctl \\\n          ; ./regctl image copy docker.io/alpine:\"$TAG\" \"$DOCKER_ORG\"/alpine:\"$TAG\" \\\n          ; rm regctl \\\n          ; docker logout ghcr.io\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": ".github/workflows/mirror.yml", "language": "yaml", "complexity": "simple", "star_count": 6665, "quality_score": 0.7, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 38}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: start-linux-multiarch\non:\n  push:\n    branches:\n      - testing\n      - staging\n      - '2024.06'\n      - master\n      - test-*\n\nconcurrency: ci-multiarch-${{ github.ref }}\n\n# REQUIRED global variables\n# DOCKER_ORG, docker org used for pushing images.\nenv:\n  DOCKER_ORG: ghcr.io/mailu\n\njobs:\n# This job calculates all global job variables that are required by all the subsequent jobs.\n# All subsequent jobs will retrieve and use these variables. This way the variables only have to be derived once.\n  derive-variables:\n    name: derive variables\n    runs-on: ubuntu-latest\n    outputs:\n        MAILU_VERSION: ${{ env.MAILU_VERSION }}\n        PINNED_MAILU_VERSION: ${{ env.PINNED_MAILU_VERSION }}\n        DOCKER_ORG: ${{ env.DOCKER_ORG_DERIVED }}\n        BRANCH: ${{ env.BRANCH }}\n        DEPLOY: ${{ env.DEPLOY }}\n        RELEASE: ${{ env.RELEASE }}\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          # fetch-depth 0 is required to also retrieve all tags.\n          fetch-depth: 0\n      - name: Extract branch name\n        shell: bash\n        run: |\n          echo \"BRANCH=${GITHUB_REF#refs/heads/}\" >> $GITHUB_ENV\n        #For branch TESTING, we set the image tag to pr-xxxx\n      - name: Derive MAILU_VERSION and PINNED_MAILU_VERSION and DEPLOY/RELEASE for branch testing\n        if: env.BRANCH == 'testing'\n        shell: bash\n        env:\n          COMMIT_MESSAGE: ${{ github.event.head_commit.message }}\n        run: |\n          echo \"MAILU_VERSION=pr-${COMMIT_MESSAGE//[!0-9]/}\" >> $GITHUB_ENV\n          echo \"PINNED_MAILU_VERSION=pr-${COMMIT_MESSAGE//[!0-9]/}\" >> $GITHUB_ENV\n          echo \"DOCKER_ORG_DERIVED=${{ env.DOCKER_ORG }}\" >> $GITHUB_ENV\n          echo \"DEPLOY=true\" >> $GITHUB_ENV\n          echo \"RELEASE=false\" >> $GITHUB_ENV\n      - name: Derive MAILU_VERSION and DEPLOY/RELEASE for other branches than testing\n        if: env.BRANCH != 'testing'\n        shell: bash\n        run: |\n          echo \"MAILU_VERSION=${{ env.BRANCH }}\" >> $GITHUB_ENV\n          echo \"DOCKER_ORG_DERIVED=${{ env.DOCKER_ORG }}\" >> $GITHUB_ENV\n          echo \"DEPLOY=true\" >> $GITHUB_ENV\n          echo \"RELEASE=false\" >> $GITHUB_ENV\n      - name: Derive PINNED_MAILU_VERSION and DEPLOY/RELEASE for normal release x.y\n        if: env.BRANCH != 'testing' && env.BRANCH != 'staging' && env.BRANCH != 'master' && !(contains(env.BRANCH, 'test-'))\n        shell: bash\n        run: |\n          version=$( git tag --sort=version:refname --list \"${{ env.MAILU_VERSION }}.*\" | tail -1  );root_version=${version%.*};patch_version=${version##*.};if [ \"$patch_version\" == \"\" ]; then pinned_version=${{ env.MAILU_VERSION }}.0; else pinned_version=$root_version.$(expr $patch_version + 1); fi;echo \"PINNED_MAILU_VERSION=$pinned_version\" >> $GITHUB_ENV\n          echo \"DEPLOY=true\" >> $GITHUB_ENV\n          echo \"RELEASE=true\" >> $GITHUB_ENV\n      - name: Derive PINNED_MAILU_VERSION and DEPLOY/RELEASE for staging\n        id: deploy\n        if: env.BRANCH == 'staging'\n        shell: bash\n        run: |\n          echo \"PINNED_MAILU_VERSION=staging\" >> $GITHUB_ENV\n          echo \"DEPLOY=false\" >> $GITHUB_ENV\n          echo \"RELEASE=false\" >> $GITHUB_ENV\n      - name: Derive PINNED_MAILU_VERSION for master\n        if: env.BRANCH == 'master'\n        shell: bash\n        env:\n          GITHUB_SHA: ${{ env.GITHUB_SHA }}\n        run: |\n          echo \"PINNED_MAILU_VERSION=$GITHUB_SHA\" >> $GITHUB_ENV\n          echo \"DEPLOY=true\" >> $GITHUB_ENV\n          echo \"RELEASE=false\" >> $GITHUB_ENV\n      - name: Derive for branch test-*\n        if: contains(env.BRANCH, 'test-')\n        run: |\n          echo \"MAILU_VERSION=${{ env.BRANCH }}\" >> $GITHUB_ENV\n          echo \"PINNED_MAILU_VERSION=${{ env.BRANCH }}\" >> $GITHUB_ENV\n          echo \"DOCKER_ORG_DERIVED=${{ env.DOCKER_ORG }}\" >> $GITHUB_ENV\n          echo \"DEPLOY=true\" >> $GITHUB_ENV\n          echo \"RELEASE=false\" >> $GITHUB_ENV\n\n  build-test-deploy:\n    needs:\n      - derive-variables\n    uses: ./.github/workflows/build_test_deploy.yml\n    with:\n      architecture: 'linux/amd64,linux/arm64/v8,linux/arm/v7'\n      mailu_version: ${{needs.derive-variables.outputs.MAILU_VERSION}}\n      pinned_mailu_version: ${{needs.derive-variables.outputs.PINNED_MAILU_VERSION}}\n      docker_org: ${{needs.derive-variables.outputs.DOCKER_ORG}}\n      branch: ${{needs.derive-variables.outputs.BRANCH}}\n      deploy: ${{needs.derive-variables.outputs.DEPLOY}}\n      release: ${{needs.derive-variables.outputs.RELEASE}}\n    secrets: inherit\n\n # This job is watched by bors. It only complets if building,testing and deploy worked.\n  ci-success:\n    name: CI-Done\n    #Returns true when none of the **previous** steps have failed or have been canceled.\n    if: success()\n    needs:\n      - build-test-deploy\n    runs-on: ubuntu-latest\n    steps:\n      - name: CI/CD succeeded.\n        run: exit 0\n\n################################################\n# Code block that is used as one liner for the step:\n# Derive PINNED_MAILU_VERSION and DEPLOY/RELEASE for normal release x.y\n##!/bin/bash\n#version=$( git tag --sort=version:refname --list \"{{ env.MAILU_VERSION }}.*\" | tail -1  )\n#root_version=${version%.*}\n#patch_version=${version##*.}\n#if [ \"$patch_version\" == \"\" ]\n#then\n#  pinned_version={{ env.MAILU_VERSION }}.0\n#else\n#  pinned_version=$root_version.$(expr $patch_version + 1)\n#fi\n#echo \"PINNED_MAILU_VERSION=$pinned_version\" >> $GITHUB_ENV\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": ".github/workflows/multiarch.yml", "language": "yaml", "complexity": "medium", "star_count": 6665, "quality_score": 0.73, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 133}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Most contributors are listed on Github at:\n\n https://github.com/Mailu/Mailu/graphs/contributors\n\nIf you contribute time, code or resources to the project, feel free to add\nyour name, pseudonym, and any contact information you feel is relevant to\nthis file.\n\nAs it is almost impossible to distinguish code contributions from various\nauthors, all are considered equal contributors and all must agree with\nany change in the software license.\n\nOther contributors:\n\n - \"Angedestenebres\" - Tests on development version & Current version\n - Stefan Auditor - German translation on POEditor.com\n - [Carlos Bern√°rdez](https://github.com/jkarlosb) - [[Contributions in Mailu]](https://github.com/Mailu/Mailu/commits?author=jkarlosb)\n - Felipe Lubra - Portugese translation\n - M√©lanie Henry - German translation\n - Maxime Saddok - French translation\n - \"ofthesun9\" - French translation\n - \"SunMar\" - Dutch translation\n - \"Marty Hou\" - Chinese Simple translation\n - \"Spoooyders\" - Belarusian translation\n - [Thomas S√§nger](https://github.com/HorayNarea) - German translation\n - [Danylo Sydorenko](https://github.com/Prosta4okua) - Ukrainian translation\n - [Hossein Hosni](https://github.com/hosni) - [Contributions](https://github.com/Mailu/Mailu/commits?author=hosni)\n - [Tim Mohlmann](https://github.com/muhlemmer) - [Contributions](https://github.com/Mailu/Mailu/commits?author=muhlemmer)\n - [Ionut Filip](https://github.com/ionutfilip) - [Contributions](https://github.com/Mailu/Mailu/commits?author=ionutfilip)\n - [Ichikawa Yuriko](https://github.com/IchikawaYukko) - [Contributions](https://github.com/Mailu/Mailu/commits?author=IchikawaYukko) Japanese translation\n - [Dimitri Huisman](https://github.com/Diman0) - [Contributions](https://github.com/Mailu/Mailu/commits?author=Diman0)\n - [lub](https://github.com/lub) - [Contributions](https://github.com/Mailu/Mailu/commits?author=lub)\n - [Dario Ernst](https://github.com/Nebukadneza) - [Contributions](https://github.com/Mailu/Mailu/commits?author=Nebukadneza)\n - [Florent Daigniere](https://github.com/nextgens) - [Contributions](https://github.com/Mailu/Mailu/commits?author=nextgens)\n - [Alexander Graf](https://github.com/ghostwheel42) - [Contributions](https://github.com/Mailu/Mailu/commits?author=ghostwheel42)\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": "AUTHORS.md", "language": "markdown", "complexity": "simple", "star_count": 6665, "quality_score": 0.73, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 36}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment include:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n* Accepting that not all community members share our view of the project\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at team@mailu.io. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at [http://contributor-covenant.org/version/1/4][version]\n\n[homepage]: http://contributor-covenant.org\n[version]: http://contributor-covenant.org/version/1/4/\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": "CODE_OF_CONDUCT.md", "language": "markdown", "complexity": "simple", "star_count": 6665, "quality_score": 0.73, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 47}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!--\n\nThank you for opening an issue with Mailu. Please understand that issues are meant for bugs only. The bug report should follow the issue template and provide clear replication steps and logs.\nFor **user-support questions**, reach out to us  on [matrix](https://matrix.to/#/#mailu:tedomum.net) or [disussions](https://github.com/Mailu/Mailu/discussions/categories/user-support).\n\nFor anything but bug reports use the [matrix channel](https://matrix.to/#/#mailu:tedomum.net) or [disussions](https://github.com/Mailu/Mailu/discussions).\nSo use discussions for topics such as\n\n* Checking announcements.\n* General discussion about Mailu usage or using Mail software in general.\n* Feature requests\n* User support.\n\nTo be able to help you best, we need some more information.\n\nBefore you open your issue\n- Check if no issue or pull-request for this already exists.\n- Check [documentation](https://mailu.io/master/) and [FAQ](https://mailu.io/master/faq.html). (Tip, use the search function on the documentation page)\n- You understand `Mailu` is made by volunteers in their **free time** ‚Äî be concise, civil and accept that delays can occur.\n- The title of the issue should be short and simple. It should contain specific terms related to the actual issue. Be specific while writing the title.\n- You understand issues are only meant for bug reports that follow the issue template. Non bug reports or bug reports that do not follow the template will be moved to [disussions](https://github.com/Mailu/Mailu/discussions)\n\nPlease put your text outside of the comment blocks to be visible. You can use the button \"Preview\" above to check.\n\nIf you do not follow the issue template suggested below your issue may be summarily closed.\n\n-->\n\n## Environment & Version\n\n- `docker compose version`\n- Version: `master`\n\n<!--\nTo find your version, get the image name of a mailu container and read  the version from the tag (example for version 1.7).\n\n$> docker ps -a | grep mailu\n140b09d4b09c    mailu/roundcube:1.7    \"docker-php-entrypoi‚Ä¶\"    2 weeks ago    Up 2 days (healthy)    80/tcp\n$> grep MAILU_VERSION docker-compose.yml mailu.env\n-->\n\nIf you are not using docker compose do not file any new issue here.\nKubernetes related issues belong to https://github.com/Mailu/helm-charts/issues\nIf you are not using docker compose or kubernetes, create a new thread on user support in [disussions](https://github.com/Mailu/Mailu/discussions/categories/user-support).\nNon-bug reports (or bug reports that do not follow the template) are moved to [disussions](https://github.com/Mailu/Mailu/discussions).\n\n## Description\n<!--\nFurther explain the bug in a few words. It should be clear what the unexpected behaviour is.  Share it in an easy-to-understand language.\n-->\n\n## Replication Steps\n<!--\nSteps for replicating your issue\n-->\n\n## Observed behaviour\n<!--\nExplain or paste the result you received.\n-->\n\n## Expected behaviour\n<!--\nExplain what results you expected - be as specific as possible.\nJust saying \"it doesn‚Äôt work as expected\" is not useful. It's also helpful to describe what you actually experienced.\n-->\n\n## Logs\n<!--\nOften it is very useful to include log fragments of the involved component.\nYou can get the logs via `docker logs <container name> --tail 1000`.\nFor example for the admin container: `docker logs mailu_admin_1 --tail 1000`\nor using docker compose `docker compose -f /mailu/docker-compose.yml logs --tail 1000 admin`\n\nIf you can find the relevant section, please share only the parts that seem relevant. If you have any logs, please enclose them in code tags and in a section, like so:\n\n```\nYour logs here!\n```\n\n-->\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": "ISSUE_TEMPLATE.md", "language": "markdown", "complexity": "medium", "star_count": 6665, "quality_score": 0.73, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 82}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "MIT License\n\nCopyright (c) 2016 All Mailu contributors at the date\n\nThis software consists of voluntary contributions made by multiple individuals.\nFor exact contribution history, see the revision history available at\nhttps://github.com/Mailu/Mailu\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": "LICENSE.md", "language": "markdown", "complexity": "simple", "star_count": 6665, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## What type of PR?\n\n(Feature, enhancement, bug-fix, documentation)\n\n## What does this PR do?\n\n### Related issue(s)\n- Mention an issue like: #001\n- Auto close an issue like: closes #001\n\n## Prerequisites\nBefore we can consider review and merge, please make sure the following list is done and checked.\nIf an entry in not applicable, you can check it or remove it from the list.\n\n- [ ] In case of feature or enhancement: documentation updated accordingly\n- [ ] Unless it's docs or a minor change: add [changelog](https://mailu.io/master/contributors/workflow.html#changelog) entry file.\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": "PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 6665, "quality_score": 0.73, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<p align=\"leftr\"><img src=\"docs/assets/logomark.png\" alt=\"Mailu\" height=\"200px\"></p>\n\n\nMailu is a simple yet full-featured mail server as a set of Docker images.\nIt is free software (both as in free beer and as in free speech), open to\nsuggestions and external contributions. The project aims at providing people\nwith an easily setup, easily maintained and full-featured mail server while\nnot shipping proprietary software nor unrelated features often found in\npopular groupware.\n\nMost of the documentation is available on our [Website](https://mailu.io),\nyou can also [try our demo server](https://mailu.io/master/demo.html)\nbefore setting up your own, and come [talk to us on Matrix](https://matrix.to/#/#mailu:tedomum.net).\n\nFeatures\n========\n\nMain features include:\n\n- **Standard email server**, IMAP and IMAP+, SMTP and Submission with auto-configuration profiles for clients\n- **Advanced email features**, aliases, domain aliases, custom routing, full-text search of email attachments\n- **Web access**, multiple Webmails and administration interface\n- **User features**, aliases, auto-reply, auto-forward, fetched accounts, managesieve\n- **Admin features**, global admins, announcements, per-domain delegation, quotas\n- **Security**, enforced TLS, DANE, MTA-STS, Letsencrypt!, outgoing DKIM, anti-virus scanner, [Snuffleupagus](https://github.com/jvoisin/snuffleupagus/), block malicious attachments\n- **Antispam**, auto-learn, greylisting, DMARC and SPF, anti-spoofing\n- **Freedom**, all FOSS components, no tracker included\n\n![Domains](docs/assets/screenshots/domains.png)\n\nContributing\n============\n\nMailu is free software, open to suggestions and contributions. All\ncomponents are free software and compatible with the MIT license. All\nspecific configuration files, Dockerfiles and code are placed under the\nMIT license.\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": "README.md", "language": "markdown", "complexity": "simple", "star_count": 6665, "quality_score": 0.73, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 38}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "This is a new automatic release of Mailu. The new version can be seen in the tag name.\nThe main version X.Y (e.g. 2.1) will always reflect the latest version of the branch. To update your Mailu installation simply pull the latest images `docker compose pull && docker compose up -d`.\nThe pinned version X.Y.Z (e.g. 2.1.1) is not updated. It is pinned to the commit that was used for creating this release. You can use a pinned version to make sure your Mailu installation is not suddenly updated when recreating containers. The pinned version allows the user to manually update. It also allows to go back to a previous pinned version.\n\nTo check what was changed:\n- Go to https://github.com/Mailu/Mailu/tree/master/towncrier/newsfragments\n- Change the branch to the tag of this release.\n- Read the news fragment files to check what was changed.\n\nThe release notes of the original release can be accessed via menu item 'Release notes' on [mailu.io](https://mailu.io/).\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": "RELEASE_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 6665, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 11}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Mailu Admin container\n=====================\n\nThis is the web-based administrative interface for the Mailu containerized\nmail server.\n\nResources\n---------\n\n * [Report issues](https://github.com/Mailu/Mailu/issues) and\n    [send Pull Requests](https://github.com/Mailu/Mailu/pulls)\n    in the [main Mailu repository](https://github.com/Mailu/Mailu)\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": "core/admin/README.md", "language": "markdown", "complexity": "simple", "star_count": 6665, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 13}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\" Mailu admin app\n\"\"\"\n\nimport flask\nimport flask_bootstrap\n\nfrom mailu import utils, debug, models, manage, configuration\nfrom gunicorn import glogging\nimport logging\n\nimport hmac\n\nclass NoPingFilter(logging.Filter):\n    skipAccessLogs = False\n\n    def __init__(self, filterAccessLogs=False):\n        self.skipAccessLogs = filterAccessLogs\n\n    def filter(self, record):\n        if self.skipAccessLogs and record.args['r'].endswith(' HTTP/1.1'):\n            return False\n        if record.args['r'].endswith(' /ping HTTP/1.1'):\n            return False\n        if record.args['r'].endswith(' /internal/rspamd/local_domains HTTP/1.1'):\n            return False\n        return True\n\nclass Logger(glogging.Logger):\n    def setup(self, cfg):\n        super().setup(cfg)\n\n        # Add filters to Gunicorn logger\n        logger = logging.getLogger(\"gunicorn.access\")\n        logger.addFilter(NoPingFilter(logger.getEffectiveLevel()>logging.DEBUG))\n\ndef create_app_from_config(config):\n    \"\"\" Create a new application based on the given configuration\n    \"\"\"\n    app = flask.Flask(__name__, static_folder='static', static_url_path='/static')\n    app.cli.add_command(manage.mailu)\n\n    # Bootstrap is used for error display and flash messages\n    app.bootstrap = flask_bootstrap.Bootstrap(app)\n\n    # Initialize application extensions\n    config.init_app(app)\n    models.db.init_app(app)\n    utils.session.init_app(app)\n    utils.limiter.init_app(app)\n    utils.babel.init_app(app, locale_selector=utils.get_locale)\n    utils.login.init_app(app)\n    utils.login.user_loader(models.User.get)\n    utils.proxy.init_app(app)\n    utils.migrate.init_app(app, models.db)\n\n    app.device_cookie_key = hmac.new(bytearray(app.secret_key, 'utf-8'), bytearray('DEVICE_COOKIE_KEY', 'utf-8'), 'sha256').digest()\n    app.temp_token_key = hmac.new(bytearray(app.secret_key, 'utf-8'), bytearray('WEBMAIL_TEMP_TOKEN_KEY', 'utf-8'), 'sha256').digest()\n    app.srs_key = hmac.new(bytearray(app.secret_key, 'utf-8'), bytearray('SRS_KEY', 'utf-8'), 'sha256').digest()\n    app.truncated_pw_key = hmac.new(bytearray(app.secret_key, 'utf-8'), bytearray('TRUNCATED_PW_KEY', 'utf-8'), 'sha256').digest()\n\n    # Initialize list of translations\n    with app.app_context():\n        app.config.translations = {\n            str(locale): locale\n            for locale in sorted(\n                utils.babel.list_translations(),\n                key=lambda l: l.get_language_name().title()\n            )\n        }\n\n    # Initialize debugging tools\n    if app.config.get(\"DEBUG\"):\n        debug.toolbar.init_app(app)\n    if app.config.get(\"DEBUG_PROFILER\"):\n        debug.profiler.init_app(app)\n    if assets := app.config.get('DEBUG_ASSETS'):\n        app.static_folder = assets\n    app.logger.setLevel(app.config.get('LOG_LEVEL'))\n\n    # Inject the default variables in the Jinja parser\n    # TODO: move this to blueprints when needed\n    @app.context_processor\n    def inject_defaults():\n        signup_domains = models.Domain.query.filter_by(signup_enabled=True).all()\n        return dict(\n            signup_domains= signup_domains,\n            config        = app.config,\n            get_locale    = utils.get_locale,\n        )\n\n    # Jinja filters\n    @app.template_filter()\n    def format_date(value):\n        return utils.flask_babel.format_date(value) if value else ''\n\n    @app.template_filter()\n    def format_datetime(value):\n        return utils.flask_babel.format_datetime(value) if value else ''\n\n    def ping():\n        return ''\n    app.route('/ping')(ping)\n\n    # Import views\n    from mailu import ui, internal, sso, api\n    app.register_blueprint(ui.ui, url_prefix=app.config['WEB_ADMIN'])\n    app.register_blueprint(internal.internal, url_prefix='/internal')\n    app.register_blueprint(sso.sso, url_prefix='/sso')\n    api.register(app, web_api_root=app.config.get('WEB_API'))\n    return app\n\n\ndef create_app():\n    \"\"\" Create a new application based on the config module\n    \"\"\"\n    config = configuration.ConfigManager()\n    return create_app_from_config(config)\n\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": "core/admin/mailu/__init__.py", "language": "python", "complexity": "medium", "star_count": 6665, "quality_score": 0.94, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 119}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from flask import redirect, url_for, Blueprint\nfrom flask_restx import apidoc\nfrom . import v1 as APIv1\n\ndef register(app, web_api_root):\n\n    APIv1.app = app\n    # register api bluprint(s)\n    apidoc.apidoc.url_prefix = f'{web_api_root}/v{int(APIv1.VERSION)}'\n    APIv1.api_token = app.config['API_TOKEN']\n    if app.config['API_TOKEN'] != '':\n        app.register_blueprint(APIv1.blueprint, url_prefix=f'{web_api_root}/v{int(APIv1.VERSION)}')\n\n        # add redirect to current api version\n        redirect_api = Blueprint('redirect_api', __name__)\n        @redirect_api.route('/')\n        def redir():\n            return redirect(url_for(f'{APIv1.blueprint.name}.root'))\n        app.register_blueprint(redirect_api, url_prefix=f'{web_api_root}')\n\n        # swagger ui config\n        app.config.SWAGGER_UI_DOC_EXPANSION = 'list'\n        app.config.SWAGGER_UI_OPERATION_ID = True\n        app.config.SWAGGER_UI_REQUEST_DURATION = True\n        app.config.RESTX_MASK_SWAGGER = False\n    else:\n        api = Blueprint('api', __name__)\n        @api.route('/', defaults={'path': ''})\n        @api.route('/<path:path>')\n        def api_token_missing(path):\n            return \"<p>Error: API_TOKEN is not configured</p>\", 500\n        app.register_blueprint(api, url_prefix=f'{web_api_root}')\n", "metadata": {"source": "github.com/Mailu/Mailu", "file_path": "core/admin/mailu/api/__init__.py", "language": "python", "complexity": "simple", "star_count": 6665, "quality_score": 0.82, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Bug report\nabout: Create a report to help us improve\ntitle: ''\nlabels: bug\nassignees: ''\n\n---\n\n**Describe the bug**\nA clear and concise description of what the bug is.\n\nPlease make sure it's not a bug in podman (in that case report it to podman)\nor your understanding of docker-compose or how rootless containers work (for example, it's normal for rootless container not to be able to listen for port less than 1024 like 80)\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. what is the content of the current working directory (ex. `docker-compose.yml`, `.env`, `Dockerfile`, ...etc.)\n2. what is the sequence of commands you typed\n\nplease use [minimal reproducible example](https://stackoverflow.com/help/minimal-reproducible-example) for example give me a small busybox-based compose yaml\n\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Actual behavior**\nWhat is the behavior you actually got and that should not happen.\n\n\n**Output**\n\n```\n$ podman-compose version\nusing podman version: 3.4.0\npodman-compose version  0.1.7dev\npodman --version \npodman version 3.4.0\n\n$ podman-compose up\n...\n\n```\n\n**Environment:**\n - OS: Linux / WSL / Mac\n - podman version: \n - podman compose version: (git hex)\n\n**Additional context**\n\nAdd any other context about the problem here.\n", "metadata": {"source": "github.com/containers/podman-compose", "file_path": ".github/ISSUE_TEMPLATE/bug_report.md", "language": "markdown", "complexity": "simple", "star_count": 5612, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 53}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\n## Contributor Checklist:\n\nIf this PR adds a new feature that improves compatibility with docker-compose, please add a link\nto the exact part of compose spec that the PR touches.\n\nFor any user-visible change please add a release note to newsfragments directory, e.g.\nnewsfragments/my_feature.feature. See newsfragments/README.md for more details.\n\nAll changes require additional unit tests.\n", "metadata": {"source": "github.com/containers/podman-compose", "file_path": ".github/PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 5612, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 11}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Codespell\n\non:\n  push:\n  pull_request:\n\npermissions:\n  contents: read\n\njobs:\n  codespell:\n    name: Check for spelling errors\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n      - name: Codespell\n        uses: codespell-project/actions-codespell@v2\n        with:\n            ignore_words_file: .codespellignore\n", "metadata": {"source": "github.com/containers/podman-compose", "file_path": ".github/workflows/codespell.yml", "language": "yaml", "complexity": "simple", "star_count": 5612, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "containerization", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create containerization setup with Docker and orchestration\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Build and Release\n\non:\n  push:\n    tags:\n      - 'v*.*.*'\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Build binary\n        run: |\n          mkdir -p release/\n          docker build -t podman-compose-bin -v \"$PWD/release:/result\" .\n          mv \"$PWD/release/podman-compose\" \"$PWD/release/podman-compose-linux-x86\"\n\n      - name: Upload release asset\n        uses: softprops/action-gh-release@v2\n        with:\n          files: ./release/podman-compose-linux-x86\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/containers/podman-compose", "file_path": ".github/workflows/release.yml", "language": "yaml", "complexity": "medium", "star_count": 5612, "quality_score": 0.7, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 28}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS Certification Paths\r\n\r\n[AWS Certification Paths based on Cloud Roles and Responsibilities](https://d1.awsstatic.com/training-and-certification/docs/AWS_certification_paths.pdf)", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "certificates/aws-certification-paths.md", "language": "markdown", "complexity": "simple", "star_count": 77725, "quality_score": 0.6000000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 3}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS - Cloud Practitioner\n\nA summary of what you need to know for the exam can be found [here](https://aws.amazon.com/certification/certified-cloud-practitioner/)\n\n#### Cloud 101\n\n<details>\n<summary>What is cloud computing?</summary><br><b>\n\n[Wikipedia](https://en.wikipedia.org/wiki/Cloud_computing): \"Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user\"\n\nCloud computing also allows you to scale resources up or down as needed, paying only for what you use.\n</b></details>\n\n<details>\n<summary>What types of Cloud Computing services are there?</summary><br><b>\n\nIAAS  \nPAAS  \nSAAS\n</b></details>\n\n<details>\n<summary>Explain each of the following and give an example:\n\n  * IAAS\n  * PAAS\n  * SAAS</summary><br><b>\n\n- IAAS - Infrastructure As A Service is a cloud computing service where a cloud provider rents out IT infrastructure such as compute, networking resources and storage over the internet (e.g., AWS EC2).<br>\n\n- PAAS - Platform As A Service is a cloud hosting platform with an on-demand access to ready-to-use set of deployment, application management and DevOps tools (e.g., AWS Elastic Beanstalk).<br>\n\n- SAAS - Software As A Service is a software distribution model in which services are hosted by a cloud service provider (e.g., AWS WorkSpaces or any web-based email service).\n</b></details>\n\n<details>\n<summary>What types of clouds (or cloud deployments) are there?</summary><br><b>\n\n  * Public\n  * Hybrid\n  * Private\n</b></details>\n\n<details>\n<summary>Explain each of the following Cloud Computing Deployments:\n\n  * Public\n  * Hybrid\n  * Private</summary><br><b>\n\n- Public - Public cloud is when you leverage cloud services over the open internet on hardware owned by the cloud provider, but its usage is shared by other companies. It offers cost-effectiveness and ease of scaling.<br>\n\n- Hybrid - A hybrid cloud is a cloud computing environment that uses a mix of combining a public and private cloud environment, like an on-premises data center, and public CSPs. It provides greater flexibility and more deployment options.<br>\n\n- Private - Private cloud means that the cloud infrastructure is provisioned for exclusive use by a single organization. Resources are not shared with others, so it offers more control over security and data.\n  \n[Read more](https://aws.amazon.com/types-of-cloud-computing/)\n</b></details>\n\n#### AWS Global Infrastructure\n\n<details>\n<summary>Explain the following\n\n  * Availability zone\n  * Region\n  * Edge location</summary><br><b>\n\nAWS regions are data centers hosted across different geographical locations worldwide, each region is completely independent of one another.<br>\n\nWithin each region, there are multiple isolated locations known as Availability Zones. Multiple availability zones ensure high availability in case one of them goes down. Each Availability Zone is physically separated from others, with its own power, networking, and connectivity.<br>\n\nEdge locations are basically content delivery network endpoints which cache data and ensure lower latency and faster delivery to the users in any location. They are located in major cities around the world.\n</b></details>\n\n#### AWS Networking\n\n<details>\n<summary>What is VPC?</summary><br><b>\n\n\"A logically isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define\".\nRead more about it [here](https://aws.amazon.com/vpc).\n\nA VPC spans all the Availability Zones within a single region.\n</b></details>\n\n<details>\n<summary>True or False? VPC spans multiple regions</summary><br><b>\n\nFalse. A VPC is region-specific and cannot span multiple regions.\n</b></details>\n\n<details>\n<summary>True or False? Subnets belong to the same VPC, can be in different availability zones</summary><br><b>\n\nTrue. Just to clarify, a subnet must reside entirely in one AZ, but a single VPC can contain subnets across multiple AZs.\n</b></details>\n\n<details>\n<summary>What is an Internet Gateway?</summary><br><b>\n\n\"component that allows communication between instances in your VPC and the internet\" (AWS docs).\nRead more about it [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html)\n\nIt scales horizontally and is highly available, allowing inbound and outbound traffic to flow without imposing availability risks or bandwidth constraints.\n</b></details>\n\n<details>\n<summary>True or False? NACL allow or deny traffic on the subnet level</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? Multiple Internet Gateways can be attached to one VPC</summary><br><b>\n\nFalse. Only one internet gateway can be attached to a single VPC.\n</b></details>\n\n<details>\n<summary>True or False? Route Tables used to allow or deny traffic from the internet to AWS instances</summary><br><b>\n\nFalse. Route tables are used to direct traffic to the right destination (e.g., Internet Gateway, NAT Gateway, etc.), not to allow or deny traffic.\n</b></details>\n\n<details>\n<summary>Explain Security Groups and Network ACLs</summary><br><b>\n\n* NACL - security layer on the subnet level. They are stateless, meaning inbound and outbound rules are evaluated separately.<br>\n* Security Group - security layer on the instance level. They are stateful, meaning if you allow inbound traffic, outbound traffic is automatically allowed, and vice versa.\n\nRead more about it [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) and [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)\n</b></details>\n\n<details>\n<summary>What is AWS Direct Connect?</summary><br><b>\n\nAllows you to connect your corporate network to AWS network. It provides a dedicated network connection that can offer more consistent performance than internet-based connections.\n</b></details>\n\n#### AWS Compute\n\n<details>\n<summary>What is EC2?</summary><br><b>\n\n\"a web service that provides secure, resizable compute capacity in the cloud\".\nRead more [here](https://aws.amazon.com/ec2)\n\nEC2 allows you to quickly scale up or down to match resource needs, paying only for the compute time you consume.\n</b></details>\n\n<details>\n<summary>What is AMI?</summary><br><b>\n\nAmazon Machine Images is \"An Amazon Machine Image (AMI) provides the information required to launch an instance\".\nRead more [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html)\n\nAn AMI typically includes an operating system, application server, and applications, so you can quickly spin up new instances with the same configuration.\n</b></details>\n\n<details>\n<summary>What are the different source for AMIs?</summary><br><b>\n\n* Personal AMIs - AMIs you create\n* AWS Marketplace for AMIs - Paid AMIs usually bundled with licensed software\n* Community AMIs - Free\n\nYou can also share AMIs across accounts if needed.\n</b></details>\n\n<details>\n<summary>What is instance type?</summary><br><b>\n\n\"the instance type that you specify determines the hardware of the host computer used for your instance\"\nRead more about instance types [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html)\n\nInstance types vary by CPU, memory, storage, and networking capacity, e.g., t2.micro, c5.large, etc.\n</b></details>\n\n<details>\n<summary>True or False? The following are instance types available for a user in AWS:\n\n  * Compute optimized\n  * Network optimized\n  * Web optimized</summary><br><b>\n\nFalse. From the above list only compute optimized is available. There's no \"Web optimized\" or \"Network optimized\" instance type. You do have memory optimized, storage optimized, etc.\n</b></details>\n\n<details>\n<summary>What is EBS?</summary><br><b>\n\n\"provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices.\"\nMore on EBS [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html)\n\nEBS volumes are tied to an Availability Zone. They can be snapshotted to Amazon S3 for durability and can be detached/reattached between EC2 instances in the same AZ.\n</b></details>\n\n<details>\n<summary>What EC2 pricing models are there?</summary><br><b>\n\nOn Demand - pay a fixed rate by the hour/second with no commitment. You can provision and terminate at any time.<br>\nReserved - you get capacity reservation, basically purchase an instance for a fixed time period (1 or 3 years). The longer, the cheaper.<br>\nSpot - Enables you to bid whatever price you want for instances or pay the spot price. Ideal for workloads that can be interrupted.<br>\nDedicated Hosts - physical EC2 server dedicated for your use. Helps you address compliance requirements and use your own software licenses.\n</b></details>\n\n<details>\n<summary>What are Security Groups?</summary><br><b>\n\n\"A security group acts as a virtual firewall that controls the traffic for one or more instances\"\nMore on this subject [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html)\n\nThey are stateful, so any rule applied for inbound automatically applies to outbound, and vice versa (if the inbound rule is allowed).\n</b></details>\n\n<details>\n<summary>What can you attach to an EC2 instance in order to store data?</summary><br><b>\n\nEBS\n\nAdditionally, some instance types support Instance Store (ephemeral storage), and you can also mount EFS (file storage) if you need a shared filesystem across multiple instances.\n</b></details>\n\n<details>\n<summary>What EC2 RI types are there?</summary><br><b>\n\nStandard RI - most significant discount + suited for steady-state usage<br>\nConvertible RI - discount + change attribute of RI + suited for steady-state usage<br>\nScheduled RI - launch within time windows you reserve\n\nLearn more about EC2 RI [here](https://aws.amazon.com/ec2/pricing/reserved-instances)\n\nSome RIs also offer different payment options (no upfront, partial upfront, or all upfront) affecting the discount level.\n</b></details>\n\n#### AWS Containers\n\n<details>\n<summary>What is Amazon ECS?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. Customers such as Duolingo, Samsung, GE, and Cook Pad use ECS to run their most sensitive and mission critical applications because of its security, reliability, and scalability.\"\n\nLearn more [here](https://aws.amazon.com/ecs)\n</b></details>\n\n<details>\n<summary>What is Amazon ECR?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images.\"\n\nLearn more [here](https://aws.amazon.com/ecr)\n</b></details>\n\n<details>\n<summary>What is AWS Fargate?</summary><br><b>\n\nAmazon definition: \"AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\"\n\nLearn more [here](https://aws.amazon.com/fargate)\n</b></details>\n\n#### AWS Storage\n \n<details>\n<summary>Explain what is AWS S3?</summary><br><b>\n\nS3 stands for 3 S, Simple Storage Service.\nS3 is a object storage service which is fast, scalable and durable. S3 enables customers to upload, download or store any file or object that is up to 5 TB in size.\n\nMore on S3 [here](https://aws.amazon.com/s3) \n</b></details>\n\n<details>\n<summary>What is a bucket?</summary><br><b>\n\nAn S3 bucket is a resource which is similar to folders in a file system and allows storing objects, which consist of data.\n</b></details>\n\n<details>\n<summary>True or False? A bucket name must be globally unique</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>Explain folders and objects in regards to buckets</summary><br><b>\n\n* Folder - any sub folder in an s3 bucket\n* Object - The files which are stored in a bucket\n</b></details>\n\n<details>\n<summary>Explain the following:\n\n  * Object Lifecycles\n  * Object Sharing\n  * Object Versioning</summary><br><b>\n\n  * Object Lifecycles - Transfer objects between storage classes based on defined rules of time periods\n  * Object Sharing - Share objects via a URL link\n  * Object Versioning - Manage multiple versions of an object\n</b></details>\n\n<details>\n<summary>Explain Object Durability and Object Availability</summary><br><b>\n\nObject Durability: The percent over a one-year time period that a file will not be lost\nObject Availability: The percent over a one-year time period that a file will be accessible\n</b></details>\n\n<details>\n<summary>What is a storage class? What storage classes are there?</summary><br><b>\n\nEach object has a storage class assigned to, affecting its availability and durability. This also has effect on costs.\nStorage classes offered today:\n  * Standard:\n    * Used for general, all-purpose storage (mostly storage that needs to be accessed frequently)\n    * The most expensive storage class \n    * 11x9% durability\n    * 2x9% availability\n    * Default storage class\n\n  * Standard-IA (Infrequent Access)\n    * Long lived, infrequently accessed data but must be available the moment it's being accessed\n    * 11x9% durability\n    * 99.90% availability\n\n  * One Zone-IA (Infrequent Access):\n    * Long-lived, infrequently accessed, non-critical data\n    * Less expensive than Standard and Standard-IA storage classes\n    * 2x9% durability\n    * 99.50% availability\n    \n  * Intelligent-Tiering:\n    * Long-lived data with changing or unknown access patterns. Basically, In this class the data automatically moves to the class most suitable for you based on usage patterns\n    * Price depends on the used class\n    * 11x9% durability\n    * 99.90% availability\n\n  * Glacier: Archive data with retrieval time ranging from minutes to hours\n  * Glacier Deep Archive: Archive data that rarely, if ever, needs to be accessed with retrieval times in hours\n  * Both Glacier and Glacier Deep Archive are:\n    * The most cheap storage classes\n    * have 9x9% durability \n\nMore on storage classes [here](https://aws.amazon.com/s3/storage-classes)\n\n</b></details>\n\n<details>\n<summary>A customer would like to move data which is rarely accessed from standard storage class to the most cheapest class there is. Which storage class should be used?\n\n  * One Zone-IA\n  * Glacier Deep Archive\n  * Intelligent-Tiering</summary><br><b>\n\nGlacier Deep Archive\n</b></details>\n\n<details>\n<summary>What Glacier retrieval options are available for the user?</summary><br><b>\n\nExpedited, Standard and Bulk\n</b></details>\n\n<details>\n<summary>True or False? Each AWS account can store up to 500 PetaByte of data. Any additional storage will cost double</summary><br><b>\n\nFalse. Unlimited capacity.\n</b></details>\n\n<details>\n<summary>Explain what is Storage Gateway</summary><br><b>\n\n\"AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage\".\nMore on Storage Gateway [here](https://aws.amazon.com/storagegateway)\n</b></details>\n\n<details>\n<summary>Explain the following Storage Gateway deployments types\n\n  * File Gateway\n  * Volume Gateway\n  * Tape Gateway</summary><br><b>\n\nExplained in detail [here](https://aws.amazon.com/storagegateway/faqs)\n</b></details>\n\n<details>\n<summary>What is the difference between stored volumes and cached volumes?</summary><br><b>\n\nStored Volumes - Data is located at customer's data center and periodically backed up to AWS\nCached Volumes - Data is stored in AWS cloud and cached at customer's data center for quick access\n</b></details>\n\n<details>\n<summary>What is \"Amazon S3 Transfer Acceleration\"?</summary><br><b>\n\nAWS definition: \"Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket\"\n\nLearn more [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html)\n</b></details>\n\n<details>\n<summary>What is Amazon EFS?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources.\"\n\nLearn more [here](https://aws.amazon.com/efs)\n</b></details>\n\n<details>\n<summary>What is AWS Snowmobile?</summary><br><b>\n\n\"AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS.\"\n\nLearn more [here](https://aws.amazon.com/snowmobile)\n</b></details>\n\n#### AWS IAM\n\n<details>\n<summary>What is IAM? What are some of its features?</summary><br><b>\n\nIAM stands for Identity and Access Management, and is used for managing users, groups, access policies & roles\nFull explanation is [here](https://aws.amazon.com/iam)\n</b></details>\n\n<details>\n<summary>True or False? IAM configuration is defined globally and not per region</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? When creating an AWS account, root account is created by default. This is the recommended account to use and share in your organization</summary><br><b>\n\nFalse. Instead of using the root account, you should be creating users and use them.\n</b></details>\n\n<details>\n<summary>True or False? Groups in AWS IAM, can contain only users and not other groups</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? Users in AWS IAM, can belong only to a single group</summary><br><b>\n\nFalse. Users can belong to multiple groups.\n</b></details>\n\n<details>\n<summary>What are Roles?</summary><br><b>\n\nA way for allowing a service of AWS to use another service of AWS. You assign roles to AWS resources.\nFor example, you can make use of a role which allows EC2 service to accesses s3 buckets (read and write).\n</b></details>\n\n<details>\n<summary>What are Policies?</summary><br><b>\n\nPolicies documents used to give permissions as to what a user, group or role are able to do. Their format is JSON.\n</b></details>\n\n<details>\n<summary>A user is unable to access an s3 bucket. What might be the problem?</summary><br><b>\n\nThere can be several reasons for that. One of them is lack of policy. To solve that, the admin has to attach the user with a policy what allows him to access the s3 bucket.\n</b></details>\n\n<details>\n<summary>What should you use to:\n\n  * Grant access between two services/resources?\n  * Grant user access to resources/services?</summary><br><b>\n\n  * Role\n  * Policy\n</b></details>\n\n<details>\n<summary>What permissions does a new user have?</summary><br><b>\n\nOnly a login access.\n</b></details>\n\n##### AWS ELB\n\n<details>\n<summary>What is ELB (Elastic Load Balancing)?</summary><br><b>\n\nAWS definition: \"Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.\"\n\nMore on ELB [here](https://aws.amazon.com/elasticloadbalancing)\n</b></details>\n\n<details>\n<summary>What is auto scaling?</summary><br><b>\n\nAWS definition: \"AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost\"\n\nRead more about auto scaling [here](https://aws.amazon.com/autoscaling)\n</b></details>\n\n<details>\n<summary>True or False? Auto Scaling is about adding resources (such as instances) and not about removing resource</summary><br><b>\n\nFalse. Auto scaling adjusts capacity and this can mean removing some resources based on usage and performances.\n</b></details>\n\n<details>\n<summary>What types of load balancers are supported in EC2 and what are they used for?</summary><br><b>\n\n  * Application LB - layer 7 traffic\n  * Network LB - ultra-high performances or static IP address\n  * Classic LB - low costs, good for test or dev environments\n</b></details>\n\n#### AWS DNS\n\n<details>\n<summary>What is Route 53?</summary><br><b>\n\n\"Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service\"\nSome of Route 53 features:\n  * Register domain\n  * DNS service - domain name translations\n  * Health checks - verify your app is available\n\nMore on Route 53 [here](https://aws.amazon.com/route53)\n</b></details>\n\n#### AWS CloudFront\n\n<details>\n<summary>Explain what is CloudFront</summary><br><b>\n\nAWS definition: \"Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\"\n\nMore on CloudFront [here](https://aws.amazon.com/cloudfront)\n</b></details>\n\n<details>\n<summary>Explain the following\n\n  * Origin\n  * Edge location\n  * Distribution</summary><br><b>\n</b></details>\n\n#### AWS Monitoring & Logging\n\n<details>\n<summary>What is AWS CloudWatch?</summary><br><b>\n\nAWS definition: \"Amazon CloudWatch is a monitoring and observability service...\"\n\nMore on CloudWatch [here](https://aws.amazon.com/cloudwatch)\n</b></details>\n\n<details>\n<summary>What is AWS CloudTrail?</summary><br><b>\n\nAWS definition: \"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\"\n\nRead more on CloudTrail [here](https://aws.amazon.com/cloudtrail)\n</b></details>\n\n<details>\n<summary>What is Simply Notification Service?</summary><br><b>\n\nAWS definition: \"a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\"\n\nRead more about it [here](https://aws.amazon.com/sns)\n</b></details>\n\n<details>\n<summary>Explain the following in regards to SNS:\n\n  * Topics\n  * Subscribers\n  * Publishers</summary><br><b>\n\n  * Topics - used for grouping multiple endpoints\n  * Subscribers - the endpoints where topics send messages to \n  * Publishers - the provider of the message (event, person, ...)\n</b></details>\n\n#### AWS Security \n\n<details>\n<summary>What is the shared responsibility model? What AWS is responsible for and what the user is responsible for based on the shared responsibility model?</summary><br><b>\n\nThe shared responsibility model defines what the customer is responsible for and what AWS is responsible for. For example, AWS is responsible for security \"of\" the cloud, while the customer is responsible for security \"in\" the cloud.\n\nMore on the shared responsibility model [here](https://aws.amazon.com/compliance/shared-responsibility-model)\n</b></details>\n\n<details>\n<summary>True or False? Based on the shared responsibility model, Amazon is responsible for physical CPUs and security groups on instances</summary><br><b>\n\nFalse. It is responsible for Hardware in its sites but not for security groups which created and managed by the users.\n</b></details>\n\n<details>\n<summary>Explain \"Shared Controls\" in regards to the shared responsibility model</summary><br><b>\n\nAWS definition: \"apply to both the infrastructure layer and customer layers, but in completely separate contexts or perspectives. In a shared control, AWS provides the requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services\"\n\nLearn more about it [here](https://aws.amazon.com/compliance/shared-responsibility-model)\n</b></details>\n\n<details>\n<summary>What is the AWS compliance program?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS Artifact?</summary><br><b>\n\nAWS definition: \"AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS‚Äô security and compliance reports and select online agreements.\"\n\nRead more about it [here](https://aws.amazon.com/artifact)\n</b></details>\n\n<details>\n<summary>What is AWS Inspector?</summary><br><b>\n\nAWS definition: \"Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.\"\"\n\nLearn more [here](https://aws.amazon.com/inspector)\n</b></details>\n\n<details>\n<summary>What is AWS Guarduty?</summary><br><b>\n\nGuarduty is a threat detection service that monitors your AWS accounts to help detect and mitigate malicious activity\n</b></details>\n\n<details>\n<summary>What is AWS Shield?</summary><br><b>\n\nAWS definition: \"AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.\"\n</b></details>\n\n<details>\n<summary>What is AWS WAF? Give an example of how it can used and describe what resources or services you can use it with</summary><br><b>\n\nAn AWS Web Application Firewall (WAF) can filter out unwanted web traffic (bots), and protect against attacks like SQL injection and cross-site scripting. One service you could use it with would be Amazon CloudFront, a CDN service, to block attacks before they reach your origin servers\n</b></details>\n\n<details>\n<summary>What AWS VPN is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the difference between Site-to-Site VPN and Client VPN?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS CloudHSM?</summary><br><b>\n\nAmazon definition: \"AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud.\"\n\nLearn more [here](https://aws.amazon.com/cloudhsm)\n</b></details>\n\n<details>\n<summary>True or False? AWS Inspector can perform both network and host assessments</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>What is AWS Acceptable Use Policy?</summary><br><b>\n\nIt describes prohibited uses of the web services offered by AWS.\nMore on AWS Acceptable Use Policy [here](https://aws.amazon.com/aup)\n</b></details>\n\n<details>\n<summary>What is AWS Key Management Service (KMS)?</summary><br><b>\n\nAWS definition: \"KMS makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.\"\nMore on KMS [here](https://aws.amazon.com/kms)\n</b></details>\n\n<details>\n<summary>True or False? A user is not allowed to perform penetration testing on any of the AWS services</summary><br><b>\n\nFalse. On some services, like EC2, CloudFront and RDS, penetration testing is allowed.\n</b></details>\n\n<details>\n<summary>True or False? DDoS attack is an example of allowed penetration testing activity</summary><br><b>\n\nFalse.\n</b></details>\n\n<details>\n<summary>True or False? AWS Access Key is a type of MFA device used for AWS resources protection</summary><br><b>\n\nFalse. Security key is an example of an MFA device.\n</b></details>\n\n<details>\n<summary>What is Amazon Cognito?</summary><br><b>\n\nAmazon definition: \"Amazon Cognito handles user authentication and authorization for your web and mobile apps.\"\n\nLearn more [here](https://docs.aws.amazon.com/cognito/index.html)\n</b></details>\n\n<details>\n<summary>What is AWS ACM?</summary><br><b>\n\nAmazon definition: \"AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.\"\n\nLearn more [here](https://aws.amazon.com/certificate-manager)\n</b></details>\n\n#### AWS Databases\n\n<details>\n<summary>What is AWS RDS?</summary><br><b>\n\nAmazon Relational Database Service (RDS) is a service for setting up  and managing resizable, cost-efficient relational databases\nresource\n\nLearn more [here](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html)\n</b></details>\n\n<details>\n<summary>What is AWS DynamoDB?</summary><br><b>\n</b></details>\n\n<details>\n<summary>Explain \"Point-in-Time Recovery\" feature in DynamoDB</summary><br><b>\n\nAmazon definition: \"You can create on-demand backups of your Amazon DynamoDB tables, or you can enable continuous backups using point-in-time recovery. For more information about on-demand backups, see On-Demand Backup and Restore for DynamoDB.\"\n\nLearn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html)\n</b></details>\n\n<details>\n<summary>Explain \"Global Tables\" in DynamoDB</summary><br><b>\n\nAmazon definition: \"A global table is a collection of one or more replica tables, all owned by a single AWS account.\"\n\nLearn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html)\n</b></details>\n\n<details>\n<summary>What is DynamoDB Accelerator?</summary><br><b>\n\nAmazon definition: \"Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement ‚Äì from milliseconds to microseconds...\"\n\nLearn more [here](https://aws.amazon.com/dynamodb/dax)\n</b></details>\n\n<details>\n<summary>What is AWS Redshift and how is it different than RDS?</summary><br><b>\n\nAWS Redshift is a cloud data warehousing service that is geared towards handling massive amounts of data (think petabytes) and being able to execute complex queries. In contrast, Amazon RDS is best suited for things like web applications requiring simple queries with more frequent transactions, and on a smaller scale.\n</b></details>\n\n<details>\n<summary>What is AWS ElastiCache? For what cases is it used?</summary><br><b>\n\nAmazon Elasticache is a fully managed Redis or Memcached in-memory data store.                                                                       \nIt's great for use cases like two-tier web applications where the most frequently accesses data is stored in ElastiCache so response time is optimal.\n</b></details>\n\n<details>\n<summary>What is Amazon Aurora</summary><br><b>\n\nA MySQL & Postgresql based relational database. Also, the default database proposed for the user when using RDS for creating a database.\nGreat for use cases like two-tier web applications that has a MySQL or Postgresql database layer and you need automated backups for your application.\n</b></details>\n\n<details>\n<summary>What is Amazon DocumentDB?</summary><br><b>\n\nAmazon definition: \"Amazon DocumentDB (with MongoDB compatibility) is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data.\"\n\nLearn more [here](https://aws.amazon.com/documentdb)\n</b></details>\n\n<details>\n<summary>What \"AWS Database Migration Service\" is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What type of storage is used by Amazon RDS?</summary><br><b>\n\nEBS\n</b></details>\n\n<details>\n<summary>Explain Amazon RDS Read Replicas</summary><br><b>\n\nAWS definition: \"Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\"\nRead more about [here](https://aws.amazon.com/rds/features/read-replicas)\n</b></details>\n\n#### AWS Serverless Compute\n\n<details>\n<summary>Explain what is AWS Lambda</summary><br><b>\n\nAWS definition: \"AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\"\n\nRead more on it [here](https://aws.amazon.com/lambda)\n</b></details>\n\n<details>\n<summary>True or False? In AWS Lambda, you are charged as long as a function exists, regardless of whether it's running or not</summary><br><b>\n\nFalse. Charges are being made when the code is executed.\n</b></details>\n\n<details>\n<summary>Which of the following set of languages Lambda supports?\n\n  * R, Swift, Rust, Kotlin\n  * Python, Ruby, Go \n  * Python, Ruby, PHP</summary><br><b>\n\n  * Python, Ruby, Go \n</b></details>\n\n#### Identify the service or tool\n\n<details>\n<summary>What would you use for automating code/software deployments?</summary><br><b>\n\nAWS CodeDeploy\n</b></details>\n\n<details>\n<summary>What would you use for easily creating similar AWS environments/resources for different customers?</summary><br><b>\n\nCloudFormation\n</b></details>\n\n<details>\n<summary>Which service would you use for building a website or web application?</summary><br><b>\n\nLightsail or Elastic Beanstalk\n</b></details>\n\n<details>\n<summary>Which tool would you use for choosing between Reserved instances or On-Demand instances?</summary><br><b>\n\nCost Explorer\n</b></details>\n\n<details>\n<summary>What would you use to check how many unassociated Elastic IP address you have?</summary><br><b>\n\nTrusted Advisor\n</b></details>\n\n<details>\n<summary>What service allows you to transfer large amounts (Petabytes) of data in and out of the AWS cloud?</summary><br><b>\n\nAWS Snowball\n</b></details>\n\n<details>\n<summary>What provides a virtual network dedicated to your AWS account?</summary><br><b>\n\nVPC\n</b></details>\n\n<details>\n<summary>What you would use for having automated backups for an application that has MySQL database layer?</summary><br><b>\n\nAmazon Aurora\n</b></details>\n\n<details>\n<summary>What would you use to migrate on-premise database to AWS?</summary><br><b>\n\nAWS Database Migration Service (DMS)\n</b></details>\n\n<details>\n<summary>What would you use to check why certain EC2 instances were terminated?</summary><br><b>\n\nAWS CloudTrail\n</b></details>\n\n<details>\n<summary>What would you use for SQL database?</summary><br><b>\n\nAWS RDS\n</b></details>\n\n<details>\n<summary>What would you use for NoSQL database?</summary><br><b>\n\nAWS DynamoDB\n</b></details>\n\n<details>\n<summary>What would you use for running SQL queries interactively on S3?</summary><br><b>\n\nAWS Athena\n</b></details>\n\n<details>\n<summary>What would you use for adding image and video analysis to your application?</summary><br><b>\n\nAWS Rekognition\n</b></details>\n\n<details>\n<summary>Which service would you use for debugging and improving performances issues with your applications?</summary><br><b>\n\nAWS X-Ray\n</b></details>\n\n<details>\n<summary>Which service is used for sending notifications?</summary><br><b>\n\nSNS\n</b></details>\n\n<details>\n<summary>Which service would you use for monitoring malicious activity and unauthorized behavior in regards to AWS accounts and workloads?</summary><br><b>\n\nAmazon GuardDuty\n</b></details>\n\n<details>\n<summary>Which service would you use for centrally manage billing, control access, compliance, and security across multiple AWS accounts?</summary><br><b>\n\nAWS Organizations\n</b></details>\n\n<details>\n<summary>Which service would you use for web application protection?</summary><br><b>\n\nAWS WAF\n</b></details>\n\n<details>\n<summary>You would like to monitor some of your resources in the different services. Which service would you use for that?</summary><br><b>\n\nCloudWatch\n</b></details>\n\n<details>\n<summary>Which service would you use for performing security assessment?</summary><br><b>\n\nAWS Inspector\n</b></details>\n\n<details>\n<summary>Which service would you use for creating DNS record?</summary><br><b>\n\nRoute 53\n</b></details>\n\n<details>\n<summary>What would you use if you need a fully managed document database?</summary><br><b>\n\nAmazon DocumentDB\n</b></details>\n\n<details>\n<summary>Which service would you use to add access control (or sign-up, sign-in forms) to your web/mobile apps?</summary><br><b>\n\nAWS Cognito\n</b></details>\n\n<details>\n<summary>Which service would you use if you need messaging queue?</summary><br><b>\n\nSimple Queue Service (SQS)\n</b></details>\n\n<details>\n<summary>Which service would you use if you need managed DDOS protection?</summary><br><b>\n\nAWS Shield\n</b></details>\n\n<details>\n<summary>Which service would you use if you need store frequently used data for low latency access?</summary><br><b>\n\nElastiCache\n</b></details>\n\n<details>\n<summary>What would you use to transfer files over long distances between a client and an S3 bucket?</summary><br><b>\n\nAmazon S3 Transfer Acceleration\n</b></details>\n\n#### AWS Billing & Support\n\n<details>\n<summary>What is AWS Organizations?</summary><br><b>\n\nAWS definition: \"AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS.\"\nMore on Organizations [here](https://aws.amazon.com/organizations)\n</b></details>\n\n<details>\n<summary>Explain AWS pricing model</summary><br><b>\n\nIt mainly works on \"pay-as-you-go\" meaning you pay only for what are using and when you are using it.\nIn s3 you pay for 1. How much data you are storing 2. Making requests (PUT, POST, ...)\nIn EC2 it's based on the purchasing option (on-demand, spot, ...), instance type, AMI type and the region used.\n\nMore on AWS pricing model [here](https://aws.amazon.com/pricing)\n</b></details>\n\n<details>\n<summary>How one should estimate AWS costs when for example comparing to on-premise solutions?</summary><br><b>\n\n* TCO calculator\n* AWS simple calculator\n* Cost Explorer\n</b></details>\n\n<details>\n<summary>What basic support in AWS includes?</summary><br><b>\n\n* 24x7 customer service\n* Trusted Advisor\n* AWS personal Health Dashoard\n</b></details>\n\n<details>\n<summary>How are EC2 instances billed?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What AWS Pricing Calculator is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is Amazon Connect?</summary><br><b>\n\nAmazon definition: \"Amazon Connect is an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost.\"\n\nLearn more [here](https://aws.amazon.com/connect)\n</b></details>\n\n<details>\n<summary>What are \"APN Consulting Partners\"?</summary><br><b>\n\nAmazon definition: \"APN Consulting Partners are professional services firms that help customers of all types and sizes design, architect, build, migrate, and manage their workloads and applications on AWS, accelerating their journey to the cloud.\"\n\nLearn more [here](https://aws.amazon.com/partners/consulting)\n</b></details>\n\n<details>\n<summary>Which of the following are AWS accounts types (and are sorted by order)?\n\n  * Basic, Developer, Business, Enterprise\n  * Newbie, Intermediate, Pro, Enterprise\n  * Developer, Basic, Business, Enterprise\n  * Beginner, Pro, Intermediate Enterprise</summary><br><b>\n\n  * Basic, Developer, Business, Enterprise\n</b></details>\n\n<details>\n<summary>True or False? Region is a factor when it comes to EC2 costs/pricing</summary><br><b>\n\nTrue. You pay differently based on the chosen region.\n</b></details>\n\n<details>\n<summary>What is \"AWS Infrastructure Event Management\"?</summary><br><b>\n\nAWS Definition: \"AWS Infrastructure Event Management is a structured program available to Enterprise Support customers (and Business Support customers for an additional fee) that helps you plan for large-scale events such as product or application launches, infrastructure migrations, and marketing events.\"\n</b></details>\n\n#### AWS Automation\n\n<details>\n<summary>What is AWS CodeDeploy?</summary><br><b>\n\nAmazon definition: \"AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.\"\n\nLearn more [here](https://aws.amazon.com/codedeploy)\n</b></details>\n\n<details>\n<summary>Explain what is CloudFormation</summary><br><b>\n</b></details>\n\n#### AWS Misc\n\n<details>\n<summary>What is AWS Lightsail?</summary><br><b>\n\nAWS definition: \"Lightsail is an easy-to-use cloud platform that offers you everything needed to build an application or website, plus a cost-effective, monthly plan.\"\n</b></details>\n\n<details>\n<summary>What is AWS Rekognition?</summary><br><b>\n\nAWS definition: \"Amazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use.\"\n\nLearn more [here](https://aws.amazon.com/rekognition)\n</b></details>\n\n<details>\n<summary>What AWS Resource Groups used for?</summary><br><b>\n\nAmazon definition: \"You can use resource groups to organize your AWS resources. Resource groups make it easier to manage and automate tasks on large numbers of resources at one time. \"\n\nLearn more [here](https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html)\n</b></details>\n\n<details>\n<summary>What is AWS Global Accelerator?</summary><br><b>\n\nAmazon definition: \"AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users...\"\n\nLearn more [here](https://aws.amazon.com/global-accelerator)\n</b></details>\n\n<details>\n<summary>What is AWS Config?</summary><br><b>\n\nAmazon definition: \"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources.\"\n\nLearn more [here](https://aws.amazon.com/config)\n</b></details>\n\n<details>\n<summary>What is AWS X-Ray?</summary><br><b>\n\nAWS definition: \"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.\"\nLearn more [here](https://aws.amazon.com/xray)\n</b></details>\n\n<details>\n<summary>What is AWS OpsWorks?</summary><br><b>\n\nAmazon definition: \"AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.\"\n\nLearn more about it [here](https://aws.amazon.com/opsworks)\n</b></details>\n\n<details>\n<summary>What is AWS Service Catalog?</summary><br><b>\n\nAmazon definition: \"AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS.\"\n\nLearn more [here](https://aws.amazon.com/servicecatalog)\n</b></details>\n\n<details>\n<summary>What is AWS CAF?</summary><br><b>\n\nAmazon definition: \"AWS Professional Services created the AWS Cloud Adoption Framework (AWS CAF) to help organizations design and travel an accelerated path to successful cloud adoption. \"\n\nLearn more [here](https://aws.amazon.com/professional-services/CAF)\n</b></details>\n\n<details>\n<summary>What is AWS Cloud9?</summary><br><b>\n\nAWS definition: \"AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser\"\n</b></details>\n\n<details>\n<summary>What is AWS Application Discovery Service?</summary><br><b>\n\nAmazon definition: \"AWS Application Discovery Service helps enterprise customers plan migration projects by gathering information about their on-premises data centers.\"\n\nLearn more [here](https://aws.amazon.com/application-discovery)\n</b></details>\n\n<details>\n<summary>What is the Trusted Advisor?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the AWS well-architected framework and what pillars it's based on?</summary><br><b>\n\nAWS definition: \"The Well-Architected Framework has been developed to help cloud architects build secure, high-performing, resilient, and efficient infrastructure for their applications. Based on five pillars ‚Äî operational excellence, security, reliability, performance efficiency, and cost optimization\"\n\nLearn more [here](https://aws.amazon.com/architecture/well-architected)\n</b></details>\n\n<details>\n<summary>What AWS services are serverless (or have the option to be serverless)?</summary><br><b>\n\nAWS Lambda\nAWS Athena\n</b></details>\n\n<details>\n<summary>What is AWS EMR?</summary><br><b>\n\nAWS definition: \"big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto.\"\n\nLearn more [here](https://aws.amazon.com/emr)\n</b></details>\n\n<details>\n<summary>What is AWS Athena?</summary><br><b>\n\n\"Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.\"\n\nLearn more about AWS Athena [here](https://aws.amazon.com/athena)\n</b></details>\n\n<details>\n<summary>What is Amazon Cloud Directory?</summary><br><b>\n\nAmazon definition: \"Amazon Cloud Directory is a highly available multi-tenant directory-based store in AWS. These directories scale automatically to hundreds of millions of objects as needed for applications.\"\n\nLearn more [here](https://docs.aws.amazon.com/clouddirectory/latest/developerguide/what_is_cloud_directory.html)\n</b></details>\n\n<details>\n<summary>What is AWS Elastic Beanstalk?</summary><br><b>\n\nAWS definition: \"AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services...You can simply upload your code and Elastic Beanstalk automatically handles the deployment\"\n\nLearn more about it [here](https://aws.amazon.com/elasticbeanstalk)\n</b></details>\n\n<details>\n<summary>What is AWS SWF?</summary><br><b>\n\nAmazon definition: \"Amazon SWF helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud.\"\n\nLearn more on Amazon Simple Workflow Service [here](https://aws.amazon.com/swf)\n</b></details>\n\n<details>\n<summary>What is Simple Queue Service (SQS)?</summary><br><b>\n\nAWS definition: \"Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications\".\n\nLearn more about it [here](https://aws.amazon.com/sqs)\n</b></details>\n\n#### AWS Disaster Recovery\n\n<details>\n<summary>In regards to disaster recovery, what is RTO and RPO?</summary><br><b>\n\nRTO - The maximum acceptable length of time that your application can be offline.\n\nRPO - The maximum acceptable length of time during which data might be lost from your application due to an incident.\n</b></details>\n\n<details>\n<summary>What types of disaster recovery techniques AWS supports?</summary><br><b>\n\n* The Cold Method - Periodically backups and sending the backups off-site<br>\n* Pilot Light - Data is mirrored to an environment which is always running\n* Warm Standby - Running scaled down version of production environment\n* Multi-site - Duplicated environment that is always running\n</b></details>\n\n<details>\n<summary>Which disaster recovery option has the highest downtime and which has the lowest?</summary><br><b>\n\nLowest - Multi-site\nHighest - The cold method\n</b></details>\n\n### Final Note\n\nGood luck! You can do it :)\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "certificates/aws-cloud-practitioner.md", "language": "markdown", "complexity": "complex", "star_count": 77725, "quality_score": 0.79, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 1281}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS Cloud SysOps Administration Associate\n\nA summary of what you need to know for the exam can be found [here](https://aws.amazon.com/certification/certified-sysops-admin-associate)\n\n### <b>  Who should take this exam? </b>\n<br>\nAWS Certified SysOps Administrator - Associate is intended for system administrators in cloud operations roles to validate technical skills. \n\n<summary>Before you take this exam, we recommend you have :</summary><br><b>\n\n  * A minimum of one year of hands-on experience with AWS technology\n  * Experience deploying, managing, and operating workloads on AWS as well as implementing security controls and compliance requirements\n  * Familiarity with using both the AWS Management Console and the AWS Command Line Interface (CLI)\n  * Understanding of the AWS Well-Architected Framework as well as AWS networking and security services\n</b>\n\n### <b>  Prepare for your exam </b>\n<br>\nGet started with free resources or explore additional resources, including Official Practice Exams, with a subscription to AWS Skill Builder.\n\n * AWS Cloud SysOps Guide (SOA-C02) [here](https://d1.awsstatic.com/training-and-certification/docs-sysops-associate/AWS-Certified-SysOps-Administrator-Associate_Exam-Guide.pdf)\n * AWS Certified SysOps Administrator - Associate Official Practice Question Set (FREE) [here](https://explore.skillbuilder.aws/learn/course/external/view/elearning/12485/aws-certified-sysops-administrator-associate-practice-question-set-soa-c02-english?syops=sec&sec=prep)\n * Exam Prep: AWS Certified SysOps Administrator - Associate (FREE) [here](https://explore.skillbuilder.aws/learn/course/external/view/elearning/9313/exam-prep-aws-certified-sysops-administrator-associate)\n\n ### <b> Certification resource </b>\n <br>\n This is a resource for studying and preparing for the AWS Cloud SysOps Associate exam.\n\n * Architecting for the cloud [here](https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf)\n * AWS Well-Architected Framework [here](https://d0.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf)\n * Development and Test on Amazon Web Services [here](https://media.amazonwebservices.com/AWS_Development_Test_Environments.pdf)\n * Backup, Archive, and Restore Approaches Using AWS [here](https://d0.awsstatic.com/whitepapers/Backup_Archive_and_Restore_Approaches_Using_AWS.pdf)\n * How AWS Pricing Works - AWS Pricing Overview [here](https://d0.awsstatic.com/whitepapers/aws_pricing_overview.pdf)\n * Sample Question AWS SOA-C02 [here](https://d1.awsstatic.com/training-and-certification/docs-sysops-associate/AWS-Certified-SysOps-Administrator-Associate_Sample-Questions.pdf)", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "certificates/aws-cloud-sysops-associate.md", "language": "markdown", "complexity": "medium", "star_count": 77725, "quality_score": 0.8, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 34}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS - Solutions Architect Associate \n\nLast update: 2021\n\n#### AWS Global Infrastructure\n\n<details>\n<summary>Explain the following\n\n  * Availability zone\n  * Region\n  * Edge location</summary><br><b>\n\nAWS regions are data centers hosted across different geographical locations worldwide, each region is completely independent of one another.<br>\n\nWithin each region, there are multiple isolated locations known as Availability Zones. Multiple availability zones ensure high availability in case one of them goes down.<br>\n\nEdge locations are basically content delivery network which caches data and insures lower latency and faster delivery to the users in any location. They are located in major cities in the world.\n</b></details>\n\n#### AWS - IAM\n\n<details>\n<summary>What is IAM? What are some of its features?</summary><br><b>\n\nFull explanation is [here](https://aws.amazon.com/iam)\nIn short: it's used for managing users, groups, access policies & roles\n</b></details>\n\n<details>\n<summary>True or False? IAM configuration is defined globally and not per region</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? When creating an AWS account, root account is created by default. This is the recommended account to use and share in your organization</summary><br><b>\n\nFalse. Instead of using the root account, you should be creating users and use them.\n</b></details>\n\n<details>\n<summary>True or False? Groups in AWS IAM, can contain only users and not other groups</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? Users in AWS IAM, can belong only to a single group</summary><br><b>\n\nFalse. Users can belong to multiple groups.\n</b></details>\n\n<details>\n<summary>What are Roles?</summary><br><b>\n\nA way for allowing a service of AWS to use another service of AWS. You assign roles to AWS resources.\nFor example, you can make use of a role which allows EC2 service to accesses s3 buckets (read and write).\n</b></details>\n\n<details>\n<summary>What are Policies?</summary><br><b>\n\nPolicies documents used to give permissions as to what a user, group or role are able to do. Their format is JSON.\n</b></details>\n\n<details>\n<summary>A user is unable to access an s3 bucket. What might be the problem?</summary><br><b>\n\nThere can be several reasons for that. One of them is lack of policy. To solve that, the admin has to attach the user with a policy what allows him to access the s3 bucket.\n</b></details>\n\n<details>\n<summary>What should you use to:\n\n  * Grant access between two services/resources?\n  * Grant user access to resources/services?</summary><br><b>\n\n  * Role\n  * Policy\n</b></details>\n\n<details>\n<summary>What permissions does a new user have?</summary><br><b>\n\nOnly a login access.\n</b></details>\n\n\n#### AWS Networking\n\n<details>\n<summary>What is VPC?</summary><br><b>\n\n\"A logically isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define\"\nRead more about it [here](https://aws.amazon.com/vpc).\n</b></details>\n\n<details>\n<summary>True or False? VPC spans multiple regions</summary><br><b>\n\nFalse\n</b></details>\n\n<details>\n<summary>True or False? Subnets belong to the same VPC, can be in different availability zones</summary><br><b>\n\nTrue. Just to clarify, a subnet must reside entirely in one AZ.\n</b></details>\n\n<details>\n<summary>What is an Internet Gateway?</summary><br><b>\n\n\"component that allows communication between instances in your VPC and the internet\" (AWS docs).\nRead more about it [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html)\n</b></details>\n\n<details>\n<summary>True or False? NACL allow or deny traffic on the subnet level</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? Multiple Internet Gateways can be attached to one VPC</summary><br><b>\n\nFalse. Only one internet gateway can be attached to a single VPC.\n</b></details>\n\n<details>\n<summary>True or False? Route Tables used to allow or deny traffic from the internet to AWS instances</summary><br><b>\n\nFalse.\n</b></details>\n\n<details>\n<summary>Explain Security Groups and Network ACLs</summary><br><b>\n\n* NACL - security layer on the subnet level.\n* Security Group - security layer on the instance level.\n\nRead more about it [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) and [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)\n</b></details>\n\n<details>\n<summary>What is AWS Direct Connect?</summary><br><b>\n\nAllows you to connect your corporate network to AWS network.\n</b></details>\n\n#### AWS Compute\n\n<details>\n<summary>What is EC2?</summary><br><b>\n\n\"a web service that provides secure, resizable compute capacity in the cloud\".\nRead more [here](https://aws.amazon.com/ec2)\n</b></details>\n\n<details>\n<summary>True or False? EC2 is a regional service</summary><br><b>\n\nTrue. As opposed to IAM for example, which is a global service, EC2 is a regional service.\n</b></details>\n\n<details>\n<summary>What is AMI?</summary><br><b>\n\nAmazon Machine Images is \"An Amazon Machine Image (AMI) provides the information required to launch an instance\".\nRead more [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html)\n</b></details>\n\n<details>\n<summary>What are the different source for AMIs?</summary><br><b>\n\n* Personal AMIs - AMIs you create\n* AWS Marketplace for AMIs - Paid AMIs usually with bundled with licensed software\n* Community AMIs - Free\n</b></details>\n\n<details>\n<summary>What is instance type?</summary><br><b>\n\n\"the instance type that you specify determines the hardware of the host computer used for your instance\"\nRead more about instance types [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html)\n</b></details>\n\n<details>\n<summary>True or False? The following are instance types available for a user in AWS:\n\n  * Compute optimized\n  * Network optimized\n  * Web optimized</summary><br><b>\n\nFalse. From the above list only compute optimized is available.\n</b></details>\n\n<details>\n<summary>What is EBS?</summary><br><b>\n\n\"provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices.\"\nMore on EBS [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html)\n</b></details>\n\n<details>\n<summary>What EC2 pricing models are there?</summary><br><b>\n\nOn Demand - pay a fixed rate by the hour/second with no commitment. You can provision and terminate it at any given time.\nReserved - you get capacity reservation, basically purchase an instance for a fixed time of period. The longer, the cheaper.\nSpot - Enables you to bid whatever price you want for instances or pay the spot price.\nDedicated Hosts - physical EC2 server dedicated for your use.\n</b></details>\n\n<details>\n<summary>What are Security Groups?</summary><br><b>\n\n\"A security group acts as a virtual firewall that controls the traffic for one or more instances\"\nMore on this subject [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html)\n</b></details>\n\n<details>\n<summary>What can you attach to an EC2 instance in order to store data?</summary><br><b>\n\nEBS\n</b></details>\n\n<details>\n<summary>What EC2 RI types are there?</summary><br><b>\n\nStandard RI - most significant discount + suited for steady-state usage\nConvertible RI - discount + change attribute of RI + suited for steady-state usage\nScheduled RI - launch within time windows you reserve\n\nLearn more about EC2 RI [here](https://aws.amazon.com/ec2/pricing/reserved-instances)\n</b></details>\n\n#### AWS Containers\n\n<details>\n<summary>What is Amazon ECS?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. Customers such as Duolingo, Samsung, GE, and Cook Pad use ECS to run their most sensitive and mission critical applications because of its security, reliability, and scalability.\"\n\nLearn more [here](https://aws.amazon.com/ecs)\n</b></details>\n\n<details>\n<summary>What is Amazon ECR?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images.\"\n\nLearn more [here](https://aws.amazon.com/ecr)\n</b></details>\n\n<details>\n<summary>What is AWS Fargate?</summary><br><b>\n\nAmazon definition: \"AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\"\n\nLearn more [here](https://aws.amazon.com/fargate)\n</b></details>\n\n#### AWS Storage\n \n<details>\n<summary>Explain what is AWS S3?</summary><br><b>\n\nS3 stands for 3 S, Simple Storage Service.\nS3 is a object storage service which is fast, scalable and durable. S3 enables customers to upload, download or store any file or object that is up to 5 TB in size.\n\nMore on S3 [here](https://aws.amazon.com/s3) \n</b></details>\n\n<details>\n<summary>What is a bucket?</summary><br><b>\n\nAn S3 bucket is a resource which is similar to folders in a file system and allows storing objects, which consist of data.\n</b></details>\n\n<details>\n<summary>True or False? A bucket name must be globally unique</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>Explain folders and objects in regards to buckets</summary><br><b>\n\n* Folder - any sub folder in an s3 bucket\n* Object - The files which are stored in a bucket\n</b></details>\n\n<details>\n<summary>Explain the following:\n\n  * Object Lifecycles\n  * Object Sharing\n  * Object Versioning</summary><br><b>\n\n  * Object Lifecycles - Transfer objects between storage classes based on defined rules of time periods\n  * Object Sharing - Share objects via a URL link\n  * Object Versioning - Manage multiple versions of an object\n</b></details>\n\n<details>\n<summary>Explain Object Durability and Object Availability</summary><br><b>\n\nObject Durability: The percent over a one-year time period that a file will not be lost\nObject Availability: The percent over a one-year time period that a file will be accessible\n</b></details>\n\n<details>\n<summary>What is a storage class? What storage classes are there?</summary><br><b>\n\nEach object has a storage class assigned to, affecting its availability and durability. This also has effect on costs.\nStorage classes offered today:\n  * Standard:\n    * Used for general, all-purpose storage (mostly storage that needs to be accessed frequently)\n    * The most expensive storage class \n    * 11x9% durability\n    * 2x9% availability\n    * Default storage class\n\n  * Standard-IA (Infrequent Access)\n    * Long lived, infrequently accessed data but must be available the moment it's being accessed\n    * 11x9% durability\n    * 99.90% availability\n\n  * One Zone-IA (Infrequent Access):\n    * Long-lived, infrequently accessed, non-critical data\n    * Less expensive than Standard and Standard-IA storage classes\n    * 2x9% durability\n    * 99.50% availability\n    \n  * Intelligent-Tiering:\n    * Long-lived data with changing or unknown access patterns. Basically, In this class the data automatically moves to the class most suitable for you based on usage patterns\n    * Price depends on the used class\n    * 11x9% durability\n    * 99.90% availability\n\n  * Glacier: Archive data with retrieval time ranging from minutes to hours\n  * Glacier Deep Archive: Archive data that rarely, if ever, needs to be accessed with retrieval times in hours\n  * Both Glacier and Glacier Deep Archive are:\n    * The most cheap storage classes\n    * have 9x9% durability \n\nMore on storage classes [here](https://aws.amazon.com/s3/storage-classes)\n\n</b></details>\n\n<details>\n<summary>A customer would like to move data which is rarely accessed from standard storage class to the most cheapest class there is. Which storage class should be used?\n\n  * One Zone-IA\n  * Glacier Deep Archive\n  * Intelligent-Tiering</summary><br><b>\n\nGlacier Deep Archive\n</b></details>\n\n<details>\n<summary>What Glacier retrieval options are available for the user?</summary><br><b>\n\nExpedited, Standard and Bulk\n</b></details>\n\n<details>\n<summary>True or False? Each AWS account can store up to 500 PetaByte of data. Any additional storage will cost double</summary><br><b>\n\nFalse. Unlimited capacity.\n</b></details>\n\n<details>\n<summary>Explain what is Storage Gateway</summary><br><b>\n\n\"AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage\".\nMore on Storage Gateway [here](https://aws.amazon.com/storagegateway)\n</b></details>\n\n<details>\n<summary>Explain the following Storage Gateway deployments types\n\n  * File Gateway\n  * Volume Gateway\n  * Tape Gateway</summary><br><b>\n\nExplained in detail [here](https://aws.amazon.com/storagegateway/faqs)\n</b></details>\n\n<details>\n<summary>What is the difference between stored volumes and cached volumes?</summary><br><b>\n\nStored Volumes - Data is located at customer's data center and periodically backed up to AWS\nCached Volumes - Data is stored in AWS cloud and cached at customer's data center for quick access\n</b></details>\n\n<details>\n<summary>What is \"Amazon S3 Transfer Acceleration\"?</summary><br><b>\n\nAWS definition: \"Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket\"\n\nLearn more [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html)\n</b></details>\n\n<details>\n<summary>What is Amazon EFS?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources.\"\n\nLearn more [here](https://aws.amazon.com/efs)\n</b></details>\n\n<details>\n<summary>What is AWS Snowmobile?</summary><br><b>\n\n\"AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS.\"\n\nLearn more [here](https://aws.amazon.com/snowmobile)\n</b></details>\n\n##### AWS ELB\n\n<details>\n<summary>What is ELB (Elastic Load Balancing)?</summary><br><b>\n\nAWS definition: \"Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.\"\n\nMore on ELB [here](https://aws.amazon.com/elasticloadbalancing)\n</b></details>\n\n<details>\n<summary>What is auto scaling?</summary><br><b>\n\nAWS definition: \"AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost\"\n\nRead more about auto scaling [here](https://aws.amazon.com/autoscaling)\n</b></details>\n\n<details>\n<summary>True or False? Auto Scaling is about adding resources (such as instances) and not about removing resource</summary><br><b>\n\nFalse. Auto scaling adjusts capacity and this can mean removing some resources based on usage and performances.\n</b></details>\n\n<details>\n<summary>What types of load balancers are supported in EC2 and what are they used for?</summary><br><b>\n\n  * Application LB - layer 7 traffic\n  * Network LB - ultra-high performances or static IP address\n  * Classic LB - low costs, good for test or dev environments\n</b></details>\n\n#### AWS DNS\n\n<details>\n<summary>What is Route 53?</summary><br><b>\n\n\"Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service\"\nSome of Route 53 features:\n  * Register domain\n  * DNS service - domain name translations\n  * Health checks - verify your app is available\n\nMore on Route 53 [here](https://aws.amazon.com/route53)\n</b></details>\n\n#### AWS CloudFront\n\n<details>\n<summary>Explain what is CloudFront</summary><br><b>\n\nAWS definition: \"Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\"\n\nMore on CloudFront [here](https://aws.amazon.com/cloudfront)\n</b></details>\n\n<details>\n<summary>Explain the following\n\n  * Origin\n  * Edge location\n  * Distribution</summary><br><b>\n</b></details>\n\n#### AWS Monitoring & Logging\n\n<details>\n<summary>What is AWS CloudWatch?</summary><br><b>\n\nAWS definition: \"Amazon CloudWatch is a monitoring and observability service...\"\n\nMore on CloudWatch [here](https://aws.amazon.com/cloudwatch)\n</b></details>\n\n<details>\n<summary>What is AWS CloudTrail?</summary><br><b>\n\nAWS definition: \"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\"\n\nRead more on CloudTrail [here](https://aws.amazon.com/cloudtrail)\n</b></details>\n\n<details>\n<summary>What is Simply Notification Service?</summary><br><b>\n\nAWS definition: \"a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\"\n\nRead more about it [here](https://aws.amazon.com/sns)\n</b></details>\n\n<details>\n<summary>Explain the following in regards to SNS:\n\n  * Topics\n  * Subscribers\n  * Publishers</summary><br><b>\n\n  * Topics - used for grouping multiple endpoints\n  * Subscribers - the endpoints where topics send messages to \n  * Publishers - the provider of the message (event, person, ...)\n</b></details>\n\n#### AWS Security \n\n<details>\n<summary>What is the shared responsibility model? What AWS is responsible for and what the user is responsible for based on the shared responsibility model?</summary><br><b>\n\nThe shared responsibility model defines what the customer is responsible for and what AWS is responsible for.\n\nMore on the shared responsibility model [here](https://aws.amazon.com/compliance/shared-responsibility-model)\n</b></details>\n\n<details>\n<summary>True or False? Based on the shared responsibility model, Amazon is responsible for physical CPUs and security groups on instances</summary><br><b>\n\nFalse. It is responsible for Hardware in its sites but not for security groups which created and managed by the users.\n</b></details>\n\n<details>\n<summary>Explain \"Shared Controls\" in regards to the shared responsibility model</summary><br><b>\n\nAWS definition: \"apply to both the infrastructure layer and customer layers, but in completely separate contexts or perspectives. In a shared control, AWS provides the requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services\"\n\nLearn more about it [here](https://aws.amazon.com/compliance/shared-responsibility-model)\n</b></details>\n\n<details>\n<summary>What is the AWS compliance program?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS Artifact?</summary><br><b>\n\nAWS definition: \"AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS‚Äô security and compliance reports and select online agreements.\"\n\nRead more about it [here](https://aws.amazon.com/artifact)\n</b></details>\n\n<details>\n<summary>What is AWS Inspector?</summary><br><b>\n\nAWS definition: \"Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.\"\"\n\nLearn more [here](https://aws.amazon.com/inspector)\n</b></details>\n\n<details>\n<summary>What is AWS Guarduty?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS Shield?</summary><br><b>\n\nAWS definition: \"AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.\"\n</b></details>\n\n<details>\n<summary>What is AWS WAF? Give an example of how it can used and describe what resources or services you can use it with</summary><br><b>\n</b></details>\n\n<details>\n<summary>What AWS VPN is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the difference between Site-to-Site VPN and Client VPN?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS CloudHSM?</summary><br><b>\n\nAmazon definition: \"AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud.\"\n\nLearn more [here](https://aws.amazon.com/cloudhsm)\n</b></details>\n\n<details>\n<summary>True or False? AWS Inspector can perform both network and host assessments</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>What is AWS Acceptable Use Policy?</summary><br><b>\n\nIt describes prohibited uses of the web services offered by AWS.\nMore on AWS Acceptable Use Policy [here](https://aws.amazon.com/aup)\n</b></details>\n\n<details>\n<summary>What is AWS Key Management Service (KMS)?</summary><br><b>\n\nAWS definition: \"KMS makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.\"\nMore on KMS [here](https://aws.amazon.com/kms)\n</b></details>\n\n<details>\n<summary>True or False? A user is not allowed to perform penetration testing on any of the AWS services</summary><br><b>\n\nFalse. On some services, like EC2, CloudFront and RDS, penetration testing is allowed.\n</b></details>\n\n<details>\n<summary>True or False? DDoS attack is an example of allowed penetration testing activity</summary><br><b>\n\nFalse.\n</b></details>\n\n<details>\n<summary>True or False? AWS Access Key is a type of MFA device used for AWS resources protection</summary><br><b>\n\nFalse. Security key is an example of an MFA device.\n</b></details>\n\n<details>\n<summary>What is Amazon Cognito?</summary><br><b>\n\nAmazon definition: \"Amazon Cognito handles user authentication and authorization for your web and mobile apps.\"\n\nLearn more [here](https://docs.aws.amazon.com/cognito/index.html)\n</b></details>\n\n<details>\n<summary>What is AWS ACM?</summary><br><b>\n\nAmazon definition: \"AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.\"\n\nLearn more [here](https://aws.amazon.com/certificate-manager)\n</b></details>\n\n#### AWS Databases\n\n<details>\n<summary>What is AWS RDS?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS DynamoDB?</summary><br><b>\n</b></details>\n\n<details>\n<summary>Explain \"Point-in-Time Recovery\" feature in DynamoDB</summary><br><b>\n\nAmazon definition: \"You can create on-demand backups of your Amazon DynamoDB tables, or you can enable continuous backups using point-in-time recovery. For more information about on-demand backups, see On-Demand Backup and Restore for DynamoDB.\"\n\nLearn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html)\n</b></details>\n\n<details>\n<summary>Explain \"Global Tables\" in DynamoDB</summary><br><b>\n\nAmazon definition: \"A global table is a collection of one or more replica tables, all owned by a single AWS account.\"\n\nLearn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html)\n</b></details>\n\n<details>\n<summary>What is DynamoDB Accelerator?</summary><br><b>\n\nAmazon definition: \"Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement ‚Äì from milliseconds to microseconds...\"\n\nLearn more [here](https://aws.amazon.com/dynamodb/dax)\n</b></details>\n\n<details>\n<summary>What is AWS Redshift and how is it different than RDS?</summary><br><b>\n\ncloud data warehouse\n</b></details>\n\n<details>\n<summary>What is AWS ElastiCache? For what cases is it used?</summary><br><b>\n\nAmazon Elasticache is a fully managed Redis or Memcached in-memory data store.                                                                       \nIt's great for use cases like two-tier web applications where the most frequently accesses data is stored in ElastiCache so response time is optimal.\n</b></details>\n\n<details>\n<summary>What is Amazon Aurora</summary><br><b>\n\nA MySQL & Postgresql based relational database. Also, the default database proposed for the user when using RDS for creating a database.\nGreat for use cases like two-tier web applications that has a MySQL or Postgresql database layer and you need automated backups for your application.\n</b></details>\n\n<details>\n<summary>What is Amazon DocumentDB?</summary><br><b>\n\nAmazon definition: \"Amazon DocumentDB (with MongoDB compatibility) is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data.\"\n\nLearn more [here](https://aws.amazon.com/documentdb)\n</b></details>\n\n<details>\n<summary>What \"AWS Database Migration Service\" is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What type of storage is used by Amazon RDS?</summary><br><b>\n\nEBS\n</b></details>\n\n<details>\n<summary>Explain Amazon RDS Read Replicas</summary><br><b>\n\nAWS definition: \"Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\"\nRead more about [here](https://aws.amazon.com/rds/features/read-replicas)\n</b></details>\n\n#### AWS Serverless Compute\n\n<details>\n<summary>Explain what is AWS Lambda</summary><br><b>\n\nAWS definition: \"AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\"\n\nRead more on it [here](https://aws.amazon.com/lambda)\n</b></details>\n\n<details>\n<summary>True or False? In AWS Lambda, you are charged as long as a function exists, regardless of whether it's running or not</summary><br><b>\n\nFalse. Charges are being made when the code is executed.\n</b></details>\n\n<details>\n<summary>Which of the following set of languages Lambda supports?\n\n  * R, Swift, Rust, Kotlin\n  * Python, Ruby, Go \n  * Python, Ruby, PHP</summary><br><b>\n\n  * Python, Ruby, Go \n</b></details>\n\n#### Identify the service or tool\n\n<details>\n<summary>What would you use for automating code/software deployments?</summary><br><b>\n\nAWS CodeDeploy\n</b></details>\n\n<details>\n<summary>What would you use for easily creating similar AWS environments/resources for different customers?</summary><br><b>\n\nCloudFormation\n</b></details>\n\n<details>\n<summary>Which service would you use for building a website or web application?</summary><br><b>\n\nLightsail\n</b></details>\n\n<details>\n<summary>Which tool would you use for choosing between Reserved instances or On-Demand instances?</summary><br><b>\n\nCost Explorer\n</b></details>\n\n<details>\n<summary>What would you use to check how many unassociated Elastic IP address you have?</summary><br><b>\n\nTrusted Advisor\n</b></details>\n\n<details>\n<summary>What service allows you to transfer large amounts (Petabytes) of data in and out of the AWS cloud?</summary><br><b>\n\nAWS Snowball\n</b></details>\n\n<details>\n<summary>What provides a virtual network dedicated to your AWS account?</summary><br><b>\n\nVPC\n</b></details>\n\n<details>\n<summary>What you would use for having automated backups for an application that has MySQL database layer?</summary><br><b>\n\nAmazon Aurora\n</b></details>\n\n<details>\n<summary>What would you use to migrate on-premise database to AWS?</summary><br><b>\n\nAWS Database Migration Service (DMS)\n</b></details>\n\n<details>\n<summary>What would you use to check why certain EC2 instances were terminated?</summary><br><b>\n\nAWS CloudTrail\n</b></details>\n\n<details>\n<summary>What would you use for SQL database?</summary><br><b>\n\nAWS RDS\n</b></details>\n\n<details>\n<summary>What would you use for NoSQL database?</summary><br><b>\n\nAWS DynamoDB\n</b></details>\n\n<details>\n<summary>What would you use for running SQL queries interactively on S3?</summary><br><b>\n\nAWS Athena\n</b></details>\n\n<details>\n<summary>What would you use for adding image and video analysis to your application?</summary><br><b>\n\nAWS Rekognition\n</b></details>\n\n<details>\n<summary>Which service would you use for debugging and improving performances issues with your applications?</summary><br><b>\n\nAWS X-Ray\n</b></details>\n\n<details>\n<summary>Which service is used for sending notifications?</summary><br><b>\n\nSNS\n</b></details>\n\n<details>\n<summary>Which service would you use for monitoring malicious activity and unauthorized behavior in regards to AWS accounts and workloads?</summary><br><b>\n\nAmazon GuardDuty\n</b></details>\n\n<details>\n<summary>Which service would you use for centrally manage billing, control access, compliance, and security across multiple AWS accounts?</summary><br><b>\n\nAWS Organizations\n</b></details>\n\n<details>\n<summary>Which service would you use for web application protection?</summary><br><b>\n\nAWS WAF\n</b></details>\n\n<details>\n<summary>You would like to monitor some of your resources in the different services. Which service would you use for that?</summary><br><b>\n\nCloudWatch\n</b></details>\n\n<details>\n<summary>Which service would you use for performing security assessment?</summary><br><b>\n\nAWS Inspector\n</b></details>\n\n<details>\n<summary>Which service would you use for creating DNS record?</summary><br><b>\n\nRoute 53\n</b></details>\n\n<details>\n<summary>What would you use if you need a fully managed document database?</summary><br><b>\n\nAmazon DocumentDB\n</b></details>\n\n<details>\n<summary>Which service would you use to add access control (or sign-up, sign-in forms) to your web/mobile apps?</summary><br><b>\n\nAWS Cognito\n</b></details>\n\n<details>\n<summary>Which service would you use if you need messaging queue?</summary><br><b>\n\nSimple Queue Service (SQS)\n</b></details>\n\n<details>\n<summary>Which service would you use if you need managed DDOS protection?</summary><br><b>\n\nAWS Shield\n</b></details>\n\n<details>\n<summary>Which service would you use if you need store frequently used data for low latency access?</summary><br><b>\n\nElastiCache\n</b></details>\n\n<details>\n<summary>What would you use to transfer files over long distances between a client and an S3 bucket?</summary><br><b>\n\nAmazon S3 Transfer Acceleration\n</b></details>\n\n#### AWS Billing & Support\n\n<details>\n<summary>What is AWS Organizations?</summary><br><b>\n\nAWS definition: \"AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS.\"\nMore on Organizations [here](https://aws.amazon.com/organizations)\n</b></details>\n\n<details>\n<summary>Explain AWS pricing model</summary><br><b>\n\nIt mainly works on \"pay-as-you-go\" meaning you pay only for what are using and when you are using it.\nIn s3 you pay for 1. How much data you are storing 2. Making requests (PUT, POST, ...)\nIn EC2 it's based on the purchasing option (on-demand, spot, ...), instance type, AMI type and the region used.\n\nMore on AWS pricing model [here](https://aws.amazon.com/pricing)\n</b></details>\n\n<details>\n<summary>How one should estimate AWS costs when for example comparing to on-premise solutions?</summary><br><b>\n\n* TCO calculator\n* AWS simple calculator\n* Cost Explorer\n</b></details>\n\n<details>\n<summary>What basic support in AWS includes?</summary><br><b>\n\n* 24x7 customer service\n* Trusted Advisor\n* AWS personal Health Dashoard\n</b></details>\n\n<details>\n<summary>How are EC2 instances billed?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What AWS Pricing Calculator is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is Amazon Connect?</summary><br><b>\n\nAmazon definition: \"Amazon Connect is an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost.\"\n\nLearn more [here](https://aws.amazon.com/connect)\n</b></details>\n\n<details>\n<summary>What are \"APN Consulting Partners\"?</summary><br><b>\n\nAmazon definition: \"APN Consulting Partners are professional services firms that help customers of all types and sizes design, architect, build, migrate, and manage their workloads and applications on AWS, accelerating their journey to the cloud.\"\n\nLearn more [here](https://aws.amazon.com/partners/consulting)\n</b></details>\n\n<details>\n<summary>Which of the following are AWS accounts types (and are sorted by order)?\n\n  * Basic, Developer, Business, Enterprise\n  * Newbie, Intermediate, Pro, Enterprise\n  * Developer, Basic, Business, Enterprise\n  * Beginner, Pro, Intermediate Enterprise</summary><br><b>\n\n  * Basic, Developer, Business, Enterprise\n</b></details>\n\n<details>\n<summary>True or False? Region is a factor when it comes to EC2 costs/pricing</summary><br><b>\n\nTrue. You pay differently based on the chosen region.\n</b></details>\n\n<details>\n<summary>What is \"AWS Infrastructure Event Management\"?</summary><br><b>\n\nAWS Definition: \"AWS Infrastructure Event Management is a structured program available to Enterprise Support customers (and Business Support customers for an additional fee) that helps you plan for large-scale events such as product or application launches, infrastructure migrations, and marketing events.\"\n</b></details>\n\n#### AWS Automation\n\n<details>\n<summary>What is AWS CodeDeploy?</summary><br><b>\n\nAmazon definition: \"AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.\"\n\nLearn more [here](https://aws.amazon.com/codedeploy)\n</b></details>\n\n<details>\n<summary>Explain what is CloudFormation</summary><br><b>\n</b></details>\n\n#### AWS Misc\n\n<details>\n<summary>What is AWS Lightsail?</summary><br><b>\n\nAWS definition: \"Lightsail is an easy-to-use cloud platform that offers you everything needed to build an application or website, plus a cost-effective, monthly plan.\"\n</b></details>\n\n<details>\n<summary>What is AWS Rekognition?</summary><br><b>\n\nAWS definition: \"Amazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use.\"\n\nLearn more [here](https://aws.amazon.com/rekognition)\n</b></details>\n\n<details>\n<summary>What AWS Resource Groups used for?</summary><br><b>\n\nAmazon definition: \"You can use resource groups to organize your AWS resources. Resource groups make it easier to manage and automate tasks on large numbers of resources at one time. \"\n\nLearn more [here](https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html)\n</b></details>\n\n<details>\n<summary>What is AWS Global Accelerator?</summary><br><b>\n\nAmazon definition: \"AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users...\"\n\nLearn more [here](https://aws.amazon.com/global-accelerator)\n</b></details>\n\n<details>\n<summary>What is AWS Config?</summary><br><b>\n\nAmazon definition: \"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources.\"\n\nLearn more [here](https://aws.amazon.com/config)\n</b></details>\n\n<details>\n<summary>What is AWS X-Ray?</summary><br><b>\n\nAWS definition: \"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.\"\nLearn more [here](https://aws.amazon.com/xray)\n</b></details>\n\n<details>\n<summary>What is AWS OpsWorks?</summary><br><b>\n\nAmazon definition: \"AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.\"\n\nLearn more about it [here](https://aws.amazon.com/opsworks)\n</b></details>\n\n<details>\n<summary>What is AWS Service Catalog?</summary><br><b>\n\nAmazon definition: \"AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS.\"\n\nLearn more [here](https://aws.amazon.com/servicecatalog)\n</b></details>\n\n<details>\n<summary>What is AWS CAF?</summary><br><b>\n\nAmazon definition: \"AWS Professional Services created the AWS Cloud Adoption Framework (AWS CAF) to help organizations design and travel an accelerated path to successful cloud adoption. \"\n\nLearn more [here](https://aws.amazon.com/professional-services/CAF)\n</b></details>\n\n<details>\n<summary>What is AWS Cloud9?</summary><br><b>\n\nAWS definition: \"AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser\"\n</b></details>\n\n<details>\n<summary>What is AWS Application Discovery Service?</summary><br><b>\n\nAmazon definition: \"AWS Application Discovery Service helps enterprise customers plan migration projects by gathering information about their on-premises data centers.\"\n\nLearn more [here](https://aws.amazon.com/application-discovery)\n</b></details>\n\n<details>\n<summary>What is the Trusted Advisor?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the AWS well-architected framework and what pillars it's based on?</summary><br><b>\n\nAWS definition: \"The Well-Architected Framework has been developed to help cloud architects build secure, high-performing, resilient, and efficient infrastructure for their applications. Based on five pillars ‚Äî operational excellence, security, reliability, performance efficiency, and cost optimization\"\n\nLearn more [here](https://aws.amazon.com/architecture/well-architected)\n</b></details>\n\n<details>\n<summary>What AWS services are serverless (or have the option to be serverless)?</summary><br><b>\n\nAWS Lambda\nAWS Athena\n</b></details>\n\n<details>\n<summary>What is AWS EMR?</summary><br><b>\n\nAWS definition: \"big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto.\"\n\nLearn more [here](https://aws.amazon.com/emr)\n</b></details>\n\n<details>\n<summary>What is AWS Athena?</summary><br><b>\n\n\"Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.\"\n\nLearn more about AWS Athena [here](https://aws.amazon.com/athena)\n</b></details>\n\n<details>\n<summary>What is Amazon Cloud Directory?</summary><br><b>\n\nAmazon definition: \"Amazon Cloud Directory is a highly available multi-tenant directory-based store in AWS. These directories scale automatically to hundreds of millions of objects as needed for applications.\"\n\nLearn more [here](https://docs.aws.amazon.com/clouddirectory/latest/developerguide/what_is_cloud_directory.html)\n</b></details>\n\n<details>\n<summary>What is AWS Elastic Beanstalk?</summary><br><b>\n\nAWS definition: \"AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services...You can simply upload your code and Elastic Beanstalk automatically handles the deployment\"\n\nLearn more about it [here](https://aws.amazon.com/elasticbeanstalk)\n</b></details>\n\n<details>\n<summary>What is AWS SWF?</summary><br><b>\n\nAmazon definition: \"Amazon SWF helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud.\"\n\nLearn more on Amazon Simple Workflow Service [here](https://aws.amazon.com/swf)\n</b></details>\n\n<details>\n<summary>What is Simple Queue Service (SQS)?</summary><br><b>\n\nAWS definition: \"Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications\".\n\nLearn more about it [here](https://aws.amazon.com/sqs)\n</b></details>\n\n#### AWS Disaster Recovery\n\n<details>\n<summary>In regards to disaster recovery, what is RTO and RPO?</summary><br><b>\n\nRTO - The maximum acceptable length of time that your application can be offline.\n\nRPO - The maximum acceptable length of time during which data might be lost from your application due to an incident.\n</b></details>\n\n<details>\n<summary>What types of disaster recovery techniques AWS supports?</summary><br><b>\n\n* The Cold Method - Periodically backups and sending the backups off-site<br>\n* Pilot Light - Data is mirrored to an environment which is always running\n* Warm Standby - Running scaled down version of production environment\n* Multi-site - Duplicated environment that is always running\n</b></details>\n\n<details>\n<summary>Which disaster recovery option has the highest downtime and which has the lowest?</summary><br><b>\n\nLowest - Multi-site\nHighest - The cold method\n</b></details>\n\n### Final Note\n\nGood luck! You can do it :)\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "certificates/aws-solutions-architect-associate.md", "language": "markdown", "complexity": "complex", "star_count": 77725, "quality_score": 0.79, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 1203}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AZ-900\n\n<details>\n<summary>What is cloud computing?</summary><br><b>\n\n[Wikipedia](https://en.wikipedia.org/wiki/Cloud_computing): \"Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user\"\n</b></details>\n\n<details>\n<summary>What types of clouds (or cloud deployments) are there?</summary><br><b>\n\n  * Public - Cloud services sharing computing resources among multiple customers\n  * Private - Cloud services having computing resources limited to specific customer or organization, managed by third party or organizations itself\n  * Hybrid - Combination of public and private clouds\n</b></details>\n\n\n<details>\n<summary>What is Azure Firewall?</summary><br><b>\n\nAzure Firewall is a cloud-native and intelligent network firewall security service that provides the best of breed threat protection for your cloud workloads running in Azure.\n</b></details>\n\n\n<details>\n<summary>What is Network Security Group?</summary><br><b>\n\nA network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. For each rule, you can specify source and destination, port, and protocol.\n</b></details>\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "certificates/azure-fundamentals-az-900.md", "language": "markdown", "complexity": "simple", "star_count": 77725, "quality_score": 0.8, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using Terraform infrastructure as code\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## Credits\n\nJenkins logo created by <a href='https://twitter.com/ks_nenasheva'>Ksenia Nenasheva</a> and published through <a href=\"https://jenkins.io\">jenkins.io</a> is licensed under <a href=\"https://creativecommons.org/licenses/by-sa/3.0/\">cc by-sa 3.0</a><br>\nGit Logo by <a href=\"https://twitter.com/jasonlong\">Jason Long</a> is licensed under the <a href=\"https://creativecommons.org/licenses/by/3.0/\">Creative Commons Attribution 3.0 Unported License</a><br>\nTerraform logo created by <a href=\"https://www.hashicorp.com\">Hashicorp¬Æ</a><br>\nDocker logo created by <a href=\"https://www.docker.com\">Docker¬Æ</a><br>\nThe Python logo is a trademark of the Python Software Foundation¬Æ<br>\nPuppet logo created by <a href=\"https://puppet.com\">Puppet¬Æ</a><br>\nBash logo created by Prospect One<br>\nOpenStack logo created by and a trademark of The <a href=\"https://www.openstack.org\">OpenStack Foundation¬Æ</a><br>\nLinux, Kubernetes and Prometheus logos are trademarks of The Linux Foundation¬Æ<br>\nMongo logo is a trademark of <a href=\"http://www.mongodb.com\">Mongo¬Æ</a><br>\nDistributed logo by <a href=\"https://www.iconfinder.com/Flatart\">Flatart</a><br>\nChallenge icon by Elizabeth Arostegui in Technology Mix\n\"Question you ask\" (man raising hand) and \"Database\" icons by [Webalys](https://www.iconfinder.com/webalys)\nTesting logo by [Flatart](https://www.iconfinder.com/Flatart)<br>\nGoogle Cloud Plataform Logo created by <a href=\"https://about.google/\">Google¬Æ</a><br>\nVirtualBox Logo created by <a href=\"http://www.iconarchive.com/artist/dakirby309.html\">dAKirby309</a>, under the <a href=\"https://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-Noncommercial 4.0 License</a>.\nCertificates logo by <a href=\"https://www.iconfinder.com/Flatart\">Flatart</a><br>\nStorage icon by <a href=\"https://www.iconfinder.com/iconic_hub\">Dinosoftlab</a><br>\nCI/CD icon made made by <a href=\"https://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a></div>\nChaos Engineering logo made by Arie Bregman\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "credits.md", "language": "markdown", "complexity": "simple", "star_count": 77725, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": true, "has_terraform": true, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## Directories Comparison\n\n### Objectives\n\n1. You are given two directories as arguments and the output should be any difference between the two directories\n\n### Solution\n\n```\n#!/usr/bin/env bash\n\n\nhelp () {\n  echo \"Usage: compare <filename1> <filename2>\"\n  echo\n}\n\nvalidate_args() {\n  # Ensure that 2 arguments are passed\n  if [ $# != 2 ]\n  then\n    help\n    exit 1\n  fi\n\n  i=1\n  for dir in \"$@\"\n  do\n      # Validate existence of directories\n      if [ ! -d \"$dir\" ]\n      then\n        echo \"Directory $dir does not exist\"\n        exit 1\n      fi\n      echo \"Directory $i: $dir\"\n      i=$((i + 1))\n  done\n  echo\n}\n\ncompare() {\n  echo \"Comparing directories...\"\n  echo\n  diff -r \"$1\" \"$2\"\n  \n  if [ $? -eq 0 ]\n  then\n  \techo \"No difference\"\n  fi\n  \n  exit 0\n}\n\nwhile getopts \":h\" option; do\n   case $option in\n      h) # display Help\n         help\n         exit 0;;\n      \\?) # invalid option\n         echo \"Error: Invalid option\"\n         exit 1;;\n   esac\ndone\n\nvalidate_args \"$@\"\ncompare \"$1\" \"$2\"\n\n\n```", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "exercises/shell/solutions/directories_comparison.md", "language": "markdown", "complexity": "simple", "star_count": 77725, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 69}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## FAQ\n\nMost frequently asked questions.\n\n### What is the purpose of repository?\n\nLearning, of course.\n\n### My goal is to prepare for a DevOps interviews. Should I use this repository?\n\nOverall, this repository should help you learn some concepts but, don't assume at any point that your interview will include similar questions to those that included in this repository.\nRegarding interviews, I've added a couple of suggestions [here](prepare_for_interview.md)<br>\n\n### Will you stop at some point adding questions and exercises?\n\nAll good things come to an end...\n\n### How do I become a better DevOps Engineer?\n\nThat's a great question.<br>\nI don't have a definitive answer for this question, I'm exploring it myself from time to time. What I believe helps is to:\n\n  * Practice - Practicing DevOps practically should be the primary way to become a DevOps engineer in my opinion\n  * Read - blogs, books, ... anything that can enrich your knowledge about DevOps or related DevOps topics\n  * Participate - there are great DevOps communities. I personally like [Reddit DevOps community](https://www.reddit.com/r/devops). Visiting there, I learn quite a lot on different topics.\n  * Share - This is one of the reasons I created this project. Primary goal was to help others but a secondary goal quickly became to learn more. By asking questions, you actually learn better a certain topic. Try it out, take a certain subject and try to come up with questions you would ask someone to test his/her skills about that topic.\n\n### Why most of the questions don't have answers?\n\n1. Because we need more contributors\n2. Because often asking questions is easier than answering them\n\n### Where can I find answers to some of the questions in this repository?\n\n1. Search for them using search engines, documentation pages, ... this is part of being a DevOps engineer\n2. Use the communities: many people will be happy to help and answer your questions\n3. Ask us. If you want, you can contact me or start a discussion on this project.\n\n### Where the questions and answers are coming from?\n\nWell, everywhere! - past experience, colleagues, contributors, ... but please note we do not allow copying interview questions from interview questions sites to here. There are people who worked hard on adding those to their sites and we respect that.<br>\nAs an evidence, we did deny pull requests with copied content from other sites.\n\n### What are the top DevOps skills required for being a DevOps Engineer?\n\nIt's a hard question and the reason is that if you'll ask 20 different people, you'll probably get at least 10 different answers but here is what I believe is common today:\n\n* OS - DevOps require you good understanding of operating system concepts. The level required is mainly depends on the company although in my opinion it should be the same level. You should understand how the operating system works, how to troubleshoot and debug issues, etc.\n* Programming is part of DevOps. The level again depends on the company. Some will require you to know basic level of scripting while others deep understanding of common algorithms, data structure, design patterns etc.\n* Cloud and Containers - while not 100% must in all companies/positions, this skill is on the rise every year and many (if not most) of the positions/companies require this skill. This specifically means: AWS/Azure/GCP, Docker/Podman, Kubernetes, ...\n* CI/CD - Be able to to answer questions like \"Why do we need CI/CD?\" and \"What ways and models are there to perform CI/CD?\". Eventually, practice assembling such processes and workflow, using whatever tools you feel comfortable with.\n\n### I feel like there are some questions that shouldn't be included in this project\n\nIs that a question? :)<br>\nIf you don't like some of the questions or think that some questions should be removed you can open an issue or submit a PR and we can discuss it there. We don't have rules against deleting questions (for now :P)\n\n### Can I copy the questions from here to my site?\n\nYou can (although I have no idea why would you want to), but:\n\n* Not without attribution. Many people worked hard on adding these questions and they deserve a proper credit for their work\n* Not if you plan to make money out of it. Directly or indirectly (e.g. ADS) as this is a free content and we would like it to stay this way :)\n\nSame goes for copying questions from different sources to this repository. We saw it happened already with a couple of pull requests and we rejected them. We will not merge pull requests with copied questions and answers from other sources.\n\n### Can I add questions and/or answers to this project?\n\nI'll simply imagine you didn't ask that on an open source project... :)\n\n### Why can't I add installation questions?\n\nIn general, I prefer questions added to this repository will have certain educational value for the user. Either regarding a certain concept or even a very general question, but one that will make the user research on a certain topic and will make him eventually more familiar with some of its core concepts.<br>\nI know that this is not the case for every question in this repo as of today (e.g. questions about specific commands) but this is definitely something to aspire for.\n\nI see little to none value in what is known as \"Installation Questions\". Let's say I ask you \"how to install Jenkins?\". Should I conclude from your answer that you are familiar with what is Jenkins and/or how it works? In other words, is there a value in knowing how to install Jenkins? In my opinion, no.\n\n### Where can I practice coding?\n\nPersonally, I really like the following sites\n\n* [HackerRank](https://www.hackerrank.com)\n* [LeetCode](https://leetcode.com)\n* [Exercism](https://exercism.io)\n\n### How to learn more DevOps?\n\nI listed some roadmaps in [devops-resources](https://github.com/bregman-arie/devops-resources)\n\n### Why some questions repeat themselves?\n\nIf you see two identical questions, that's a bug.<br>\nIf you see two similar questions, that's a feature :D (= it's intentional)\n\nFor example:\n\n1. What is horizontal scaling?\n2. The act of adding additional instances to the pool to handle scaling is called ________ scaling\n\nYou are right, both ask about horizontal scaling but it's done from a different angle in every question and in addition, I do believe repetition helps you to learn something in a way where you are not fixed on the way it's asked, rather you understand the concept itself.\n\n### Are you open for making big changes in the repository?\n\nAbsolutely. Don't be afraid to raise ideas and start discussions.<br>\nI'll be more than happy to discuss any change you think we should make to improve the learning experience\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "faq.md", "language": "markdown", "complexity": "medium", "star_count": 77725, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 106}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## How to prepare for DevOps/SRE/Production Engineer interviews?\n \nNote: the following is opinionated.\n \n### Skills you should have\n \n#### Linux\n \nEvery DevOps Engineer should have a deep understanding of at least one operating system and if you have the option to choose then I would say it should definitely be Linux as I believe it's a requirement of at least 90% of the DevOps jobs postings out there. In addition, Linux is almost integral part of any sub-area or domain in DevOps like Cloud, Containers, etc.\n \nUsually, the followup question is \"How extensive should my knowledge be?\" Out of all the DevOps skills, I would say this, along with coding, should be your strongest skills. Be familiar with OS processes, debugging tools, filesystem, networking, ... know your operating system, understand how it works, how to troubleshoot issues, etc.\n\nNot long ago, I've created a list of Linux resources right [here](https://dev.to/abregman/collection-of-linux-resources-3nhk). There are some good sites there that you can use for learning more about Linux.\n \n#### Programming\n \nMy personal belief is that any DevOps engineer should know programming, at least to some degree. Having this skill you can automate manual processes, improve some of the open source tools you are using today or build new tools & projects to provide a solution to existing problems. Knowing how to code = a lot of power.\n\nWhen it comes to interviews you'll notice that the level of knowledge very much depends on the company or position you are interviewing for. Some will require you just to be able to write simple scripts while others will deep dive into complex algorithms and data structures.\n\nThe best way to practice this skill is by doing some actual coding - scripts, online challenges, CLI tools, web applications, ... just code :)\n\nAlso, the following is probably clear to most people but let's still clarify it: when given the chance to choose any language for answering coding tasks/questions, choose the one you have experience with! Some candidates prefer to choose the language they think the company is using and this is a huge mistake since giving the right answer is always better than a wrong answer, no matter which language you have used :)\n \nI recommend the following sites for practicing coding:\n                                                 \n* [HackerRank](https://www.hackerrank.com)\n* [LeetCode](https://leetcode.com)               \n* [Exercism](https://exercism.io)\n \nStarting your own project is also a good idea. More on that later on.\n\n#### Architecture and Design\n  \nThis is also an important aspect of DevOps. You should be able to describe how to design different systems, workflows, and architectures. Also, the scale is an important aspect of that. A design which might work for a dozen of hosts or X amount of data, will not necessarily work well with bigger scale.\n \nSome ideas for you to explore: \n                               \n* How to design and implement a CI pipeline (or pipelines) for verifying PRs, run multiple different types of tests, package the project and deploy it somewhere\n* How to design and implement secured ELK architecture which will get logs from 10,000 apps and will display the data eventually to the user\n* Microservices designs are also quite popular these days\n\nIn general, you should be able to describe some designs, projects, architectures, ... you performed.\n\n#### Tooling\n\nSome interviews will focus on specific tools or technologies. Which tools? this is mainly based on a combination of what you mentioned in your C.V & those that are mentioned in the job posting and used in the company. Here are some questions I believe anyone should know to answer regarding the tools he/she is familiar with:\n                               \n* What the tool does? What it allows us to achieve that we couldn't do without it?                            \n* What its advantages over other tools in the same area, with the same purpose? Why you specifically using it?\n* How it works?\n* How to use it?\n* Best practices you apply/use when using it\n                               \nLet's deep dive into practical preparation steps\n                               \n### Scenarios || Challenges || Tasks              \n                               \nThis is a very common way to interview today for DevOps roles. The candidate is given a task which represents a common task of DevOps Engineers or a piece of common knowledge and the candidate has several hours or days to accomplish the task.<br>\n                               \nThis is a great way to prepare for interviews and I recommend to try it out before actually interviewing. How? Take requirements from job posts and convert them into scenarios. Let's see an example:\n                               \n\"Knowledge in CI/CD\" -> Scenario: create a CI/CD pipeline for a project.\n                               \nAt this point, some people ask: \"but what project?\" and the answer is: what about GitHub? it has only 9125912851285192 projects...and a free way to set up CI to any of them (also a great way to learn how to collaborate with others :) )\n                               \nLet's convert another scenario:\n\n\"Experience with provisioning servers\" -> Scenario: provision a server (to make it more interesting: create a web server).\n\nAnd the last example:                                                                                                                                                        \n\"Experience with scripting\" -> Scenario: write a script. Don't waste too much time thinking \"what script should I write?\". Simply automate something you are doing manually or even implement your own version of common small utils.\n  \n### Start your own DevOps project\n  \nStarting a DevOps project is a good idea because:\n  \n* It will make you practice coding\n* It will be something you can add to your resume and talk about with the interviewer\n* Depends on size and complexity, it can teach you something about design in general\n* Depends on adoption, it can teach you about managing Open Source projects\n  \nSame here, don't overthink what your project should be about. Just go and build something :)\n  \n### Sample interview questions\n  \nMake a sample list of interview questions on various topics/areas like technical, company, role, ... and try to answer them.\nSee if you can manage to answer them in a fluent, detailed way.\n  \nBetter yet, ask a good friend/colleague to challenge you with some questions. Your self-awareness might be an obstacle in objective self-review of your knowledge :)\n  \n### Networking\n  \nFor those who attend technical meetups and conferences, it can be a great opportunity to chat with people from other companies on their interviewing process. But don't start with it, it can be quite awkward. Say at least hello first... (:\n  \nDoing so can give you a lot of information on what to expect from an interview at some companies or how to better prepare.\n  \n### Know your resume\n  \nIt may sound trivial but the idea here is simple: be ready to answer any question regarding any line you included in your resume.\nSometimes candidates surprised when they are asked on a skill or line which seems to be not related to the position but the simple truth is: if you mentioned something on your resume, it's only fair to ask you about it.\n\n### Know the company\n\nBe familiar with the company you are interviewing at. Some ideas:\n\n  * What the company does?\n  * What products it has?\n  * Why its products are unique (or better than other products)? This can also be a good question for you to ask\n\n### Books\n\nFrom my experience, this is not done by many candidates but it's one of the best ways to deep dive into topics like operating system, virtualization, scale, distributed systems, etc.    \n\nIn most cases, you will do fine without reading books but for the AAA interviews (hardest level) you'll want to read some books and overall if you inspire to be better DevOps Engineer, books (also articles, blog posts) is a great way develop yourself :)\n\n### Consider starting in non-DevOps position\n\nWhile not a preparation step, you should know that landing DevOps as a first position can be challenging. No, it's not impossible but still, since DevOps covers many different practices, tools, ... it can be quite challenging and also overwhelming for someone to try and achieve it as a first position.<br>\nA possible path to becoming a DevOps engineer is to start with actually a different (but related) position and switch from there after 1-2 years or more.\n\nSome ideas:\n\n* System Administrator - This is perfect because every DevOps Engineer should have a solid understanding of the OS and sysadmins know their OS :)\n* Software Developer/Engineer - A DevOps should have coding skills and this position will provide more than the required knowledge in most cases\n* QA Engineer - This is a more tricky one because IMHO there are less overlapping areas/skills with DevOps Engineer. Sure, DevOps engineers should have some knowledge about testing but usually, it seems their solid skills/background is mainly composed out of system internals and coding skills.\n                                                                           \n### What to expect from a DevOps interview?                                \n                                                                           \nDevOps interviews can be very different. Some will include design questions, some will focus on coding, others will include short technical questions and you might even have an interview where the interviewer only goes over your resume and discussing your past experience.\n                                                                           \nThere are a couple of things you can do about it so it will be a less overwhelming experience:\n                                                                           \n1. You can and probably should ask the HR (in some cases even the team lead) how the interview process looks like. Some will be kind enough to even tell you how to prepare.\n2. Usually, the job posting gives more than a hint on where the focus will be and what you should focus on in your preparations so read it carefully.\n3. There are plenty of sites that have notes or a summary of the interview process in different companies, especially big enterprises.\n                                                                           \n### Don't forget to be an interviewer as well                              \n                                                                           \nSome people tend to look at interviews as a one-way road of \"Determining whether a candidate is qualified\" but in reality, a candidate should also determine whether\nthe company he/she is interviewing at, is the right place for him/her.            \n                                                                                                                 \n* Do I care about team size? More specifically, do I care about being a one-man show or being part of a bigger team?\n* Do I care about work-life balance?                                       \n* Do I care about personal growth and how it's practically done?           \n* Do I care about knowing what are my responsibilities as part of the role?                                                                                                  \nIf you do, you should also play the interviewer role :)\n\n### One Last Thing                                \n                                                  \n[Good luck](https://youtu.be/AFUrG1-BAt4?t=59) :)\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "prepare_for_interview.md", "language": "markdown", "complexity": "complex", "star_count": 77725, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 152}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/bin/bash\n\n# always put up the detail of scripts . version, author, what it does, what event triggers and all .. \n\n###\n# Author: Adarsh Rawat\n# Version: 1.0.0\n# Objective: Automate Notification for a object uploaded or created in s3 bucket.\n###\n\n# debug what is happening\nset -x\n\n# all these cmds are aws cli commands | abhishek veermalla day 4-5 devops\n\n# store aws account id in a variable\naws_account_id=$(aws sts get-caller-identity --query 'Account' --output text)\n\n# print the account id from the variable\necho \"aws account id: $aws_account_id\"\n\n# set aws region, bucket name and other variables\naws_region=\"us-east-1\" \naws_bucket=\"s3-lambda-event-trigger-bucket\"\naws_lambda=\"s3-lambda-function-1\"\naws_role=\"s3-lambda-sns\"\nemail_address=\"adarshrawat8304@gmail.com\"\n\n# create iam role for the project\nrole_response=$(aws iam create-role --role-name s3-lambda-sns --assume-role-policy-document '{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [{\n    \"Action\": \"sts:AssumeRole\",\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n      \"Service\": [\n        \"lambda.amazonaws.com\",\n        \"s3.amazonaws.com\",\n        \"sns.amazonaws.com\"\n      ]\n    }\n  }]\n}')\n\n# jq is json parser here parse the role we created \n\n# extract the role arn from json resposne and store in variable\nrole_arn=$(echo \"$role_response\" | jq -r '.Role.Arn')\n\n# print the role arn\necho \"Role ARN: $role_arn\"\n\n# attach permissions to the role\naws iam attach-role-policy --role-name $aws_role --policy-arn arn:aws:iam::aws:policy/AWSLambda_FullAccess\naws iam attach-role-policy --role-name $aws_role --policy-arn arn:aws:iam::aws:policy/AmazonSNSFullAccess\n\n# create s3 bucket and get the output in a variable\nbucket_output=$(aws s3api create-bucket --bucket \"$aws_bucket\" --region \"$aws_region\")\n\n# print the output from the variable \necho \"bucket output: $bucket_output\"\n\n# upload a file to the bucket\naws s3 cp ./sample.png s3://\"$aws_bucket\"/sample.png\n\n# create a zip file to upload lambda function \nzip -r s3-lambda.zip ./s3-lambda\n\nsleep 5\n\n# create a lambda function\naws lambda create-function \\\n  --region $aws_region \\\n  --function $aws_lambda \\\n  --runtime \"python3.8\" \\\n  --handler \"s3-lambda/s3-lambda.lambda_handler\" \\\n  --memory-size 128 \\\n  --timeout 30 \\\n  --role \"arn:aws:iam::$aws_account_id:role/$aws_role\" \\\n  --zip-file \"fileb://./s3-lambda.zip\"\n\n# add permissions to s3 bucket to invoke lambda\nLambdaFunctionArn=\"arn:aws:lambda:us-east-1:$aws_account_id:function:s3-lambda\"\naws s3api put-bucket-notification-configuration \\\n  --region \"$aws_region\" \\\n  --bucket \"$aws_bucket\" \\\n  --notification-configuration '{\n    \"LambdaFunctionConfigurations\": [{\n        \"LambdaFunctionArn\": \"'\"$LambdaFunctionArn\"'\",\n        \"Events\": [\"s3:ObjectCreated:*\"]\n    }]\n}'\n\naws s3api put-bucket-notification-configuration \\\n  --region \"$aws_region\" \\\n  --bucket \"$aws_bucket\" \\\n  --notification-configuration '{\n    \"LambdaFunctionConfigurations\": [{\n        \"LambdaFunctionArn\": \"'\"$LambdaFunctionArn\"'\",\n        \"Events\": [\"s3:ObjectCreated:*\"]\n    }]\n}'\n\n# create an sns topic and save the topic arn to a variable\ntopic_arn=$(aws sns create-topic --name s3-lambda-sns --output json | jq -r '.TopicArn')\n\n# print the topic arn\necho \"SNS Topic ARN: $topic_arn\"\n\n# Trigger SNS topic using lambda function\n\n# Add sns topic using lambda function\naws sns subscribe \\\n  --topic-arn \"$topic_arn\" \\\n  --protocol email \\\n  --notification-endpoint \"$email_address\"\n\n# publish sns\naws sns publish \\\n  --topic-arn \"$topic_arn\" \\\n  --subject \"A new object created in s3 bucket\" \\\n  --message \"Hey, a new data object just got delievered into the s3 bucket $aws_bucket\" \n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "scripts/aws s3 event triggering/aws_s3_event_trigger.sh", "language": "bash", "complexity": "medium", "star_count": 77725, "quality_score": 0.8600000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 123}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import boto3\nimport json\n\n\ndef lambda_handler(event, context):\n    # i want to know that event thing\n    print(event)\n\n    # extract relevant information from the s3 event trigger\n    bucket_name = event['Records'][0]['s3']['bucket']['name']\n    object_key = event['Records'][0]['s3']['object']['key']\n\n    # perform desired operations with the uploaded file\n    print(f\"File '{object_key}' was uploaded to bucket '{bucket_name}'\")\n\n    # example: send a notification via SNS\n    sns_client = boto3.client('sns')\n    topic_arn = 'arn:aws:sns:us-east-1:<account-id>:s3-lambda-sns'\n    sns_client.publish(\n        TopicArn=topic_arn,\n        Subject='s3 object created !!',\n        Message=f\"File '{object_key}' was uploaded to bucket '{bucket_name}'\"\n    )\n\n    # Example: Trigger another Lambda function\n    # lambda_client = boto3.client('lambda')\n    # target_function_name = 'my-another-lambda-function'\n    # lambda_client.invoke(\n    #     FunctionName=target_function_name,\n    #     InvocationType='Event',\n    #     Payload=json.dumps({'bucket_name': bucket_name, 'object_key': object_key})\n    # )\n    # in case of queuing and other objectives similar to the Netflix flow of triggering\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps(\"Lambda function executed successfully !!\")\n    }\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "scripts/aws s3 event triggering/s3-lambda/s3-lambda.py", "language": "python", "complexity": "simple", "star_count": 77725, "quality_score": 0.89, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 39}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<details>\n<summary>What is Docker? What are you using it for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>How containers are different from VMs?</summary><br><b>\n\nThe primary difference between containers and VMs is that containers allow you to virtualize\nmultiple workloads on the operating system while in the case of VMs the hardware is being virtualized to\nrun multiple machines each with its own OS.\n</b></details>\n\n<details>\n<summary>In which scenarios would you use containers and in which you would prefer to use VMs?</summary><br><b>\n\nYou should choose VMs when:\n  * you need run an application which requires all the resources and functionalities of an OS\n  * you need full isolation and security\n\nYou should choose containers when:\n  * you need a lightweight solution\n  * Running multiple versions or instances of a single application\n</b></details>\n\n<details>\n<summary>Explain Docker architecture</summary><br><b>\n</b></details>\n\n<details>\n<summary>Describe in detail what happens when you run `docker run hello-world`?</summary><br><b>\n\nDocker CLI passes your request to Docker daemon.\nDocker daemon downloads the image from Docker Hub\nDocker daemon creates a new container by using the image it downloaded\nDocker daemon redirects output from container to Docker CLI which redirects it to the standard output\n</b></details>\n\n<details>\n<summary>How do you run a container?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What `docker commit` does?. When will you use it?</summary><br><b>\n</b></details>\n\n<details>\n<summary>How would you transfer data from one container into another?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What happens to data of the container when a container exists?</summary><br><b>\n</b></details>\n\n<details>\n<summary>Explain what each of the following commands do:\n\n  * docker run\n  * docker rm\n  * docker ps\n  * docker pull\n  * docker build\n  * docker commit</summary><br><b>\n</b></details>\n\n<details>\n<summary>How do you remove old, non running, containers?</summary><br><b>\n</b></details>\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "tests/testcases/testcase1.md", "language": "markdown", "complexity": "simple", "star_count": 77725, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 68}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<details>\n<summary>Explain the following code:\n\n<code>:(){ :|:& };:</code>\n\n</summary><br><b>\n</b></details>\n\n<details>\n<summary>Can you give an example to some Bash best practices?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the ternary operator? How do you use it in bash?</summary><br><b>\n\nA short way of using if/else. An example:\n\n[[ $a = 1 ]] && b=\"yes, equal\" || b=\"nope\"\n</b></details>\n\n<details>\n<summary>What does the following code do and when would you use it?\n\t\n<code>diff <(ls /tmp) <(ls /var/tmp)</code>\n\n</summary><br>\nIt is called 'process substitution'. It provides a way to pass the output of a command to another command when using a pipe <code>|</code> is not possible. It can be used when a command does not support <code>STDIN</code> or you need the output of multiple commands. \nhttps://superuser.com/a/1060002/167769\n</details>\n\n\n## SQL\n\n<a name=\"sql-beginner\"></a>\n#### :baby: Beginner\n\n<details>\n<summary>What does SQL stand for?</summary><br><b>\n\nStructured Query Language\n\n</b></details>\n\n<details>\n<summary>How is SQL Different from NoSQL</summary><br><b>\n\nThe main difference is that SQL databases are structured (data is stored in the form of\ntables with rows and columns - like an excel spreadsheet table) while NoSQL is \nunstructured, and the data storage can vary depending on how the NoSQL DB is set up, such\nas key-value pair, document-oriented, etc.\n</b></details>\n\n<details>\n<summary>What does it mean when a database is ACID compliant?</summary><br>\n\nACID stands for Atomicity, Consistency, Isolation, Durability. In order to be ACID compliant, the database much meet each of the four criteria\n\n**Atomicity** - When a change occurs to the database, it should either succeed or fail as a whole. \n\nFor example, if you were to update a table, the update should completely execute. If it only partially executes, the \nupdate is considered failed as a whole, and will not go through - the DB will revert back to it's original\nstate before the update occurred. It should also be mentioned that Atomicity ensures that each \ntransaction is completed as it's own stand alone \"unit\" - if any part fails, the whole statement fails.\n\n**Consistency** - any change made to the database should bring it from one valid state into the next.\n\nFor example, if you make a change to the DB, it shouldn't corrupt it. Consistency is upheld by checks and constraints that\nare pre-defined in the DB. For example, if you tried to change a value from a string to an int when the column\nshould be of datatype string, a consistent DB would not allow this transaction to go through, and the action would\nnot be executed\n\n**Isolation** - this ensures that a database will never be seen \"mid-update\" - as multiple transactions are running at\nthe same time, it should still leave the DB in the same state as if the transactions were being run sequentially.\n\nFor example, let's say that 20 other people were making changes to the database at the same time. At the\ntime you executed your query, 15 of the 20 changes had gone through, but 5 were still in progress. You should\nonly see the 15 changes that had completed - you wouldn't see the database mid-update as the change goes through.\n\n**Durability** - Once a change is committed, it will remain committed regardless of what happens\n(power failure, system crash, etc.). This means that all completed transactions \nmust be recorded in non-volatile memory. \n\nNote that SQL is by nature ACID compliant. Certain NoSQL DB's can be ACID compliant depending on \nhow they operate, but as a general rule of thumb, NoSQL DB's are not considered ACID compliant\n</details>\n\n<details>\n<summary>When is it best to use SQL? NoSQL?</summary><br><b>\n\nSQL - Best used when data integrity is crucial. SQL is typically implemented with many\nbusinesses and areas within the finance field due to it's ACID compliance.\n\nNoSQL - Great if you need to scale things quickly. NoSQL was designed with web applications \nin mind, so it works great if you need to quickly spread the same information around to \nmultiple servers\n\nAdditionally, since NoSQL does not adhere to the strict table with columns and rows structure\nthat Relational Databases require, you can store different data types together.\n</b></details>\n\n<details>\n<summary>What is a Cartesian Product?</summary><br>\n\nA Cartesian product is when all rows from the first table are joined to all rows in the second\ntable. This can be done implicitly by not defining a key to join, or explicitly by \ncalling a CROSS JOIN on two tables, such as below:\n\nSelect * from customers **CROSS JOIN** orders;\n\nNote that a Cartesian product can also be a bad thing - when performing a join\non two tables in which both do not have unique keys, this could cause the returned information\nto be incorrect. \n</details>\n\n##### SQL Specific Questions\n\nFor these questions, we will be using the Customers and Orders tables shown below:\n\n**Customers**\n\nCustomer_ID | Customer_Name | Items_in_cart | Cash_spent_to_Date\n------------ | ------------- | ------------- | -------------\n100204 | John Smith | 0 | 20.00\n100205 | Jane Smith | 3 | 40.00\n100206 | Bobby Frank | 1 | 100.20\n\n**ORDERS**\n\nCustomer_ID | Order_ID | Item | Price | Date_sold\n------------ | ------------- | ------------- | ------------- | -------------\n100206 | A123 | Rubber Ducky | 2.20 | 2019-09-18\n100206 | A123 | Bubble Bath | 8.00 | 2019-09-18\n100206 | Q987 | 80-Pack TP | 90.00 | 2019-09-20\n100205 | Z001 | Cat Food - Tuna Fish | 10.00 | 2019-08-05\n100205 | Z001 | Cat Food - Chicken | 10.00 | 2019-08-05\n100205 | Z001 | Cat Food - Beef | 10.00 | 2019-08-05\n100205 | Z001 | Cat Food - Kitty quesadilla | 10.00 | 2019-08-05\n100204 | X202 | Coffee | 20.00 | 2019-04-29\n\n<details>\n<summary>How would I select all fields from this table?</summary><br><b>\n\nSelect * <br>\nFrom Customers;\n</b></details>\n\n<details>\n<summary>How many items are in John's cart?</summary><br><b>\n\nSelect Items_in_cart <br>\nFrom Customers <br>\nWhere Customer_Name = \"John Smith\";\n</b></details>\n\n<details>\n<summary>What is the sum of all the cash spent across all customers?</summary><br><b>\n\nSelect SUM(Cash_spent_to_Date) as SUM_CASH <br>\nFrom Customers;\n</b></details>\n\n<details>\n<summary>Tell me about your last big project/task you worked on</summary><br><b>\n</b></details>\n\n<details>\n<summary>What was most challenging part in the project you worked on?</summary><br><b>\n</b></details>\n\n<details>\n<summary>Why do you want to work here?</summary><br><b>\n</b></details>\n\n<details>\n<summary>How did you hear about us?</summary><br><b>\n\nTell them how did you hear about them :D\nRelax, there is no wrong or right answer here...I think.\n</b></details>", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "tests/testcases/testcase2.md", "language": "markdown", "complexity": "complex", "star_count": 77725, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 179}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## Ansible\n\n<!-- {% raw %} -->\n\n### Ansible Exercises\n\n|Name|Topic|Objective & Instructions|Solution|Comments|\n|--------|--------|------|----|----|\n| My First Task | Tasks | [Exercise](my_first_task.md) | [Solution](solutions/my_first_task.md)\n| Upgrade and Update Task | Tasks | [Exercise](update_upgrade_task.md) | [Solution](solutions/update_upgrade_task.md)\n| My First Playbook | Playbooks | [Exercise](my_first_playbook.md) | [Solution](solutions/my_first_playbook.md)\n\t\n\n### Ansible Self Assessment\n\n<details>\n<summary>Describe each of the following components in Ansible, including the relationship between them:\n\n  * Task\n  * Inventory\t\n  * Module\n  * Play\n  * Playbook\n  * Role\n</summary><br><b>\n\nTask ‚Äì a call to a specific Ansible module\nModule ‚Äì the actual unit of code executed by Ansible on your own host or a remote host. Modules are indexed by category (database, file, network, ‚Ä¶) and also referred to as task plugins.\n\t\nInventory ‚Äì An inventory file defines hosts and/or groups of hosts on which Ansible tasks executed upon. The inventory file can be in one of many formats, depending on the inventory plugins you have. The most common formats are INI and YAML.\n\nPlay ‚Äì One or more tasks executed on a given host(s)\n\nPlaybook ‚Äì One or more plays. Each play can be executed on the same or different hosts\n\nRole ‚Äì Ansible roles allows you to group resources based on certain functionality/service such that they can be easily reused. In a role, you have directories for variables, defaults, files, templates, handlers, tasks, and metadata. You can then use the role by simply specifying it in your playbook.\n</b></details>\n\n<details>\n<summary>How Ansible is different from other automation tools? (e.g. Chef, Puppet, etc.)</summary><br><b>\n\nAnsible is:\n\n* Agentless\n* Minimal run requirements (Python & SSH) and simple to use\n* Default mode is \"push\" (it supports also pull)\n* Focus on simpleness and ease-of-use\n</b></details>\n\n\n<details>\n<summary>True or False? Ansible follows the mutable infrastructure paradigm</summary><br><b>\n\nTrue. In immutable infrastructure approach, you'll replace infrastructure instead of modifying it.<br>\nAnsible rather follows the mutable infrastructure paradigm where it allows you to change the configuration of different components, but this approach is not perfect and has its own disadvantages like \"configuration drift\" where different components may reach different state for different reasons.\n</b></details>\n\n<details>\n<summary>True or False? Ansible uses declarative style to describe the expected end state</summary><br><b>\n\nFalse. It uses a procedural style.\n</b></details>\n\n<details>\n<summary>What kind of automation you wouldn't do with Ansible and why?</summary><br><b>\n\nWhile it's possible to provision resources with Ansible, some prefer to use tools that follow immutable infrastructure paradigm.\nAnsible doesn't saves state by default. So a task that creates 5 instances for example, when executed again will create additional 5 instances (unless\nadditional check is implemented or explicit names are provided) while other tools might check if 5 instances exist. If only 4 exist (by checking the state file for example), one additional instance will be created to reach the end goal of 5 instances.\n</b></details>\n\n<details>\n<summary>How do you list all modules and how can you see details on a specific module?</summary><br><br>\n\n1. Ansible online docs\n2. `ansible-doc -l` for list of modules and `ansible-doc [module_name]` for detailed information on a specific module\n</b></details>\n\n#### Ansible - Inventory\n\n<details>\n<summary>What is an inventory file and how do you define one?</summary><br><b>\n\nAn inventory file defines hosts and/or groups of hosts on which Ansible tasks executed upon.\n\nAn example of inventory file:\n\n```\n192.168.1.2\n192.168.1.3\n192.168.1.4\n\n[web_servers]\n190.40.2.20\n190.40.2.21\n190.40.2.22\n```\n</b></details>\n\n<details>\n<summary>What is a dynamic inventory file? When you would use one?</summary><br><br>\n\nA dynamic inventory file tracks hosts from one or more sources like cloud providers and CMDB systems.\n\nYou should use one when using external sources and especially when the hosts in your environment are being automatically<br>\nspun up and shut down, without you tracking every change in these sources.\n</b></details>\n\n#### Ansible - Variables\n\n<details>\n<summary>Modify the following task to use a variable instead of the value \"zlib\" and have \"zlib\" as the default in case the variable is not defined\n\n```\n- name: Install a package\n  package:\n    name: \"zlib\"\n    state: present\n```\n</summary><br><b>\n\n```\n- name: Install a package\n  package:\n    name: \"{{ package_name|default('zlib') }}\"\n    state: present\n```\n</b></details>\n\n<details>\n<summary>How to make the variable \"use_var\" optional?\n\n```\n- name: Install a package\n  package:\n    name: \"zlib\"\n    state: present\n    use: \"{{ use_var }}\"\n```\n</summary><br><b>\n\n\nWith \"default(omit)\"\n```\n- name: Install a package\n  package:\n    name: \"zlib\"\n    state: present\n    use: \"{{ use_var|default(omit) }}\"\n```\n</b></details>\n\n<details>\n<summary>What would be the result of the following play?</summary><br><b>\n\n```\n---\n- name: Print information about my host\n  hosts: localhost\n  gather_facts: 'no'\n  tasks:\n      - name: Print hostname\n        debug:\n            msg: \"It's me, {{ ansible_hostname }}\"\n```\n\nWhen given a written code, always inspect it thoroughly. If your answer is ‚Äúthis will fail‚Äù then you are right. We are using a fact (ansible_hostname), which is a gathered piece of information from the host we are running on. But in this case, we disabled facts gathering (gather_facts: no) so the variable would be undefined which will result in failure.\n</b></details>\n\n<details>\n<summary>When the value '2017'' will be used in this case: `{{ lookup('env', 'BEST_YEAR') | default('2017', true) }}`?</summary><br><b>\n\nwhen the environment variable 'BEST_YEAR' is empty or false.\n</b></details>\n\n<details>\n<summary>If the value of certain variable is 1, you would like to use the value \"one\", otherwise, use \"two\". How would you do it?</summary><br><b>\n\n`{{ (certain_variable == 1) | ternary(\"one\", \"two\") }}`\n</b></details>\n\n<details>\n<summary>The value of a certain variable you use is the string \"True\". You would like the value to be a boolean. How would you cast it?</summary><br><b>\n\n`{{ some_string_var | bool }}`\n</b></details>\n\n<details>\n<summary>You want to run Ansible playbook only on specific minor version of your OS, how would you achieve that?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What the \"become\" directive used for in Ansible?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What are facts? How to see all the facts of a certain host?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What would be the result of running the following task? How to fix it?\n\n```\n- hosts: localhost\n  tasks:\n      - name: Install zlib\n        package:\n          name: zlib\n          state: present\n```\n</summary><br><b>\n</b></details>\n\n<details>\n<summary>Which Ansible best practices are you familiar with?. Name at least three</summary><br><b>\n</b></details>\n\n<details>\n<summary>Explain the directory layout of an Ansible role</summary><br><b>\n</b></details>\n\n<details>\n<summary>What 'blocks' are used for in Ansible?</summary><br><b>\n</b></details>\n\n<details>\n<summary>How do you handle errors in Ansible?</summary><br><b>\n</b></details>\n\n<details>\n<summary>You would like to run a certain command if a task fails. How would you achieve that?</summary><br><b>\n</b></details>\n\n<details>\n<summary>Write a playbook to install ‚Äòzlib‚Äô and ‚Äòvim‚Äô on all hosts if the file ‚Äò/tmp/mario‚Äô exists on the system.</summary><br><b>\n\n```\n---\n- hosts: all\n  vars:\n      mario_file: /tmp/mario\n      package_list:\n          - 'zlib'\n          - 'vim'\n  tasks:\n      - name: Check for mario file\n        stat:\n            path: \"{{ mario_file }}\"\n        register: mario_f\n\n      - name: Install zlib and vim if mario file exists\n        become: \"yes\"\n        package:\n            name: \"{{ item }}\"\n            state: present\n        with_items: \"{{ package_list }}\"\n        when: mario_f.stat.exists\n```\n</b></details>\n\n<details>\n<summary>Write a single task that verifies all the files in files_list variable exist on the host</summary><br><b>\n\n```\n- name: Ensure all files exist\n  assert:\n    that:\n      - item.stat.exists\n  loop: \"{{ files_list }}\"\n```\n</b></details>\n\n<details>\n<summary>Write a playbook to deploy the file ‚Äò/tmp/system_info‚Äô on all hosts except for controllers group, with the following content</summary><br><b>\n\n  ```\n  I'm <HOSTNAME> and my operating system is <OS>\n  ```\n\n  Replace <HOSTNAME> and  <OS> with the actual data for the specific host you are running on\n\nThe playbook to deploy the system_info file\n\n```\n---\n- name: Deploy /tmp/system_info file\n  hosts: all:!controllers\n  tasks:\n      - name: Deploy /tmp/system_info\n        template:\n            src: system_info.j2\n            dest: /tmp/system_info\n```\n\nThe content of the system_info.j2 template\n\n```\n# {{ ansible_managed }}\nI'm {{ ansible_hostname }} and my operating system is {{ ansible_distribution }\n```\n</b></details>\n\n<details>\n<summary>The variable 'whoami' defined in the following places:\n\n  * role defaults -> whoami: mario\n  * extra vars (variables you pass to Ansible CLI with -e) -> whoami: toad\n  * host facts -> whoami: luigi\n  * inventory variables (doesn‚Äôt matter which type) -> whoami: browser\n\nAccording to variable precedence, which one will be used?</summary><br><b>\n\nThe right answer is ‚Äòtoad‚Äô.\n\nVariable precedence is about how variables override each other when they set in different locations. If you didn‚Äôt experience it so far I‚Äôm sure at some point you will, which makes it a useful topic to be aware of.\n\nIn the context of our question, the order will be extra vars (always override any other variable) -> host facts -> inventory variables -> role defaults (the weakest).\n\nHere is the order of precedence from least to greatest (the last listed variables winning prioritization):\n\n1. command line values (eg ‚Äú-u user‚Äù)\n2. role defaults [[1\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id15)\n3. inventory file or script group vars [[2\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id16)\n4. inventory group_vars/all [[3\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id17)\n5. playbook group_vars/all [[3\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id17)\n6. inventory group_vars/* [[3\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id17)\n7. playbook group_vars/* [[3\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id17)\n8. inventory file or script host vars [[2\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id16)\n9. inventory host_vars/* [[3\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id17)\n10. playbook host_vars/* [[3\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id17)\n11. host facts / cached set_facts [[4\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id18)\n12. play vars\n13. play vars_prompt\n14. play vars_files\n15. role vars (defined in role/vars/main.yml)\n16. block vars (only for tasks in block)\n17. task vars (only for the task)\n18. include_vars\n19. set_facts / registered vars\n20. role (and include_role) params\n21. include params\n22. extra vars (always win precedence)\n\nA full list can be found at  [PlayBook Variables](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#ansible-variable-precedence) . Also, note there is a significant difference between Ansible 1.x and 2.x.\n</b></details>\n\n<details>\n<summary>For each of the following statements determine if it's true or false:\n\n  * A module is a collection of tasks\n  * It‚Äôs better to use shell or command instead of a specific module\n  * Host facts override play variables\n  * A role might include the following: vars, meta, and handlers\n  * Dynamic inventory is generated by extracting information from external sources\n  * It‚Äôs a best practice to use indentation of 2 spaces instead of 4\n  * ‚Äònotify‚Äô used to trigger handlers\n  * This ‚Äúhosts: all:!controllers‚Äù means ‚Äòrun only on controllers group hosts</summary><br><b>\n</b></details>\n\n<details>\n<summary>Explain the Difference between Forks and Serial & Throttle.</summary><br><b>\n\n`Serial` is like running the playbook for each host in turn, waiting for completion of the complete playbook before moving on to the next host. `forks`=1 means run the first task in a play on one host before running the same task on the next host, so the first task will be run for each host before the next task is touched. Default fork is 5 in ansible.\n\n```\n[defaults]\nforks = 30\n```\n\n```\n- hosts: webservers\n  serial: 1\n  tasks:\n    - name: ...\n```\n\nAnsible also supports `throttle` This keyword limits the number of workers up to the maximum set via the forks setting or serial. This can be useful in restricting tasks that may be CPU-intensive or interact with a rate-limiting API\n\n```\ntasks:\n- command: /path/to/cpu_intensive_command\n  throttle: 1\n```\n\n</b></details>\n\n<details>\n<summary>What is ansible-pull? How is it different from how ansible-playbook works?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is Ansible Vault?</summary><br><b>\n</b></details>\n\n<details>\n<summary>Demonstrate each of the following with Ansible:\n\n  * Conditionals\n  * Loops\n</summary><br><b>\n</b></details>\n\n<details>\n<summary>What are filters? Do you have experience with writing filters?</summary><br><b>\n</b></details>\n\n<details>\n<summary>Write a filter to capitalize a string</summary><br><b>\n\n```\ndef cap(self, string):\n    return string.capitalize()\n```\n</b></details>\n\n<details>\n<summary>You would like to run a task only if previous task changed anything. How would you achieve that?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What are callback plugins? What can you achieve by using callback plugins?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the difference between `include_task` and `import_task`?</summary><br><b>\n</b></details>\n\n<details>\n<summary>File '/tmp/exercise' includes the following content\n\n```\nGoku = 9001\nVegeta = 5200\nTrunks = 6000\nGotenks = 32\n```\n\nWith one task, switch the content to:\n\n```\nGoku = 9001\nVegeta = 250\nTrunks = 40\nGotenks = 32\n```\n</summary><br><b>\n\n```\n- name: Change saiyans levels\n  lineinfile:\n    dest: /tmp/exercise\n    regexp: \"{{ item.regexp }}\"\n    line: \"{{ item.line }}\"\n  with_items:\n    - { regexp: '^Vegeta', line: 'Vegeta = 250' }\n    - { regexp: '^Trunks', line: 'Trunks = 40' }\n    ...\n```\n</b></details>\n\n\n#### Ansible - Execution and Strategy\n\n<details>\n<summary>True or False? By default, Ansible will execute all the tasks in play on a single host before proceeding to the next host</summary><br><b>\n\nFalse. Ansible will execute a single task on all hosts before moving to the next task in a play. As for today, it uses 5 forks by default.<br>\nThis behavior is described as \"strategy\" in Ansible and it's configurable.\n</b></details>\n\n<details>\n<summary>What is a \"strategy\" in Ansible? What is the default strategy?</summary><br><b>\n\nA strategy in Ansible describes how Ansible will execute the different tasks on the hosts. By default Ansible is using the \"Linear strategy\" which defines that each task will run on all hosts before proceeding to the next task.\n</b></details>\n\n<details>\n<summary>What strategies are you familiar with in Ansible?</summary><br><b>\n\n  - Linear: the default strategy in Ansible. Run each task on all hosts before proceeding.\n  - Free: For each host, run all the tasks until the end of the play as soon as possible\n  - Debug: Run tasks in an interactive way\n</b></details>\n\n<details>\n<summary>What the <code>serial</code> keyword is used for?</summary><br><b>\n\nIt's used to specify the number (or percentage) of hosts to run the full play on, before moving to next number of hosts in the group.\n\nFor example:\n```\n- name: Some play\n  hosts: databases\n  serial: 4\n```\n\nIf your group has 8 hosts. It will run the whole play on 4 hosts and then the same play on another 4 hosts.\n</b></details>\n\n#### Ansible Testing\n\n<details>\n<summary>How do you test your Ansible based projects?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is Molecule? How does it works?</summary><br><b>\n\nIt's used to rapidy develop and test Ansbile roles.  Molecule can be used to test Ansible roles against a varaitey of Linux Distros at the same time.  This testing ability helps instill confidence of the automation today and as time go on while a role is maintined.  \n\n</b></details>\n\n<details>\n<summary>You run Ansible tests and you get \"idempotence test failed\". What does it mean? Why idempotence is important?</summary><br><b>\n</b></details>\n\n#### Ansible - Debugging\n\n<details>\n<summary>How to find out the data type of a certain variable in one of the playbooks?</summary><br><b>\n\n\"{{ some_var | type_debug }}\"\n</b></details>\n\n#### Ansible - Collections\n\n<details>\n<summary>What are collections in Ansible?</summary><br><b>\nAnsible Collections are a way to package and distribute modules, roles, plugins, and documentation in a structured format. They help organize and distribute automation code efficiently, especially for complex environments.\n</b></details>\n\n<details>\n<summary>Why Use Ansible Collections?</summary><br><b>\n\n  - Modular and reusable components\n  - Simplifies management of custom and third-party modules\n  - Provides a standardized way to distribute automation content\n  - Helps in version control and dependency management\n</b></details>\n\n<!-- {% endraw %} -->\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/ansible/README.md", "language": "markdown", "complexity": "complex", "star_count": 77725, "quality_score": 0.92, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 542}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## Ansible - My First Playbook\n\n1. Write a playbook that will:\n  a. Install the package zlib\n  b. Create the file `/tmp/some_file`\n2. Run the playbook on a remote host\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/ansible/my_first_playbook.md", "language": "markdown", "complexity": "simple", "star_count": 77725, "quality_score": 0.6000000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 7}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## License Information\n\nCopyright 2023 budi utomo\n\nThis program is subject to the terms of the Apache License, Version 2.0 AND the following amendments on forks and data processing. Thus, this is a Custom Apache 2.0 License, NOT a dual-license model you may choose from.\n\n\nYou can obtain a copy of the license at http://www.apache.org/licenses/LICENSE-2.0\n\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n\n\n## Forks\nAdditionally to Apache-2.0, when you fork this repo you are required to either remove our Google Form ID: 1FAIpQLSdrKWQdMh6Nt8v8NQdYvTIntohebAgqWCpXT3T9NofAoxcpkw or stop usage gathering completely.\n\n## Data processing agreement\nBy using this software you agree that the following non-PII (non personally identifiable information) data will be collected, processed and used by the maintainers for the purpose of improving the docker-android project.\n\n\nBy using this software you also grant us a nonexclusive, irrevocable, world-wide, perpetual royalty-free permission to use, modify and publish these data for all purposes, internally or publicly, including the right to sub-license said permission rights.\n\n\nWe collect, process and use the following data:\n\n* Date and time when Docker-Android started\n* User (it will collect the information about Release Version of Machine), e.g. Linux-5.4.0-146-generic-x86_64-with-glibc2.29_#163-Ubuntu_SMP_Fri_Mar_17_18:26:02_UTC_2023. This does not allow us to track individual users but does enable us to accurately measure user counts\n* City (the information come from https://ipinfo.io)\n* Region (the information come from https://ipinfo.io)\n* Country (the information come from https://ipinfo.io)\n* Release version of Docker-Android\n* Appium (Whether user use Appium or not - The possible value will be \"true\" or \"false\")\n* Appium Additional Arguments\n* Web-Log (Whether user use Web-Log feature or not - The possible value will be \"true\" or \"false\")\n* Web-Vnc (Whether user use Web-Vnc feature or not - The possible value will be \"true\" or \"false\")\n* Screen-Resolution\n* Device Type (Which docker image is used - The possible value will be \"emulator\" or \"geny_cloud\" or \"geny_aws\")\n* Emulator Device (Which device profile and skin is used if the user use device_type \"emulator\")\n* Emulator Android Version (Which Android version is used if the user use device_type \"emulator\"\n* Emulator No-Skin feature (Whether user use no-skin feature or not - The possible value will be \"true\" or \"false\")\n* Emulator Data Partition\n* Emulator Additional Arguments\n\n## End of License Information\n\nMore information about anonymized data collection can be seen [here](./documentations/USER_BEHAVIOR_ANALYTICS.md)\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "LICENSE.md", "language": "markdown", "complexity": "simple", "star_count": 11121, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 47}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\n<p align=\"center\">\n  <img id=\"header\" src=\"./images/logo_docker-android.png\" />\n</p>\n\n[![Paypal Donate](https://img.shields.io/badge/paypal-donate-blue.svg)](http://paypal.me/budtmo) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) [![codecov](https://codecov.io/gh/budtmo/docker-android/branch/master/graph/badge.svg)](https://codecov.io/gh/budtmo/docker-android) [![Join the chat at https://gitter.im/budtmo/docker-android](https://badges.gitter.im/budtmo/docker-android.svg)](https://gitter.im/budtmo/docker-android?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![GitHub release](https://img.shields.io/github/release/budtmo/docker-android.svg)](https://github.com/budtmo/docker-android/releases)\n\nDocker-Android is a docker image built to be used for everything related to Android. It can be used for Application development and testing (native, web and hybrid-app).\n\nAdvantages of using this project\n--------------------------------\n1. Emulator with different device profile and skins, such as Samsung Galaxy S6, LG Nexus 4, HTC Nexus One and more.\n2. Support vnc to be able to see what happen inside docker container\n3. Support log sharing feature where all logs can be accessed from web-UI\n4. Ability to control emulator from outside container by using adb connect\n5. Integrated with other cloud solutions, e.g. [Genymotion Cloud](https://www.genymotion.com/cloud/)\n6. It can be used to build Android project\n7. It can be used to run unit and UI-Test with different test-frameworks, e.g. Appium, Espresso, etc.\n\nList of Docker-Images\n---------------------\n|Android   |API   |Image with latest release version   |Image with specific release version|\n|:---|:---|:---|:---|\n|9.0|28|budtmo/docker-android:emulator_9.0|budtmo/docker-android:emulator_9.0_<release_version>|\n|10.0|29|budtmo/docker-android:emulator_10.0|budtmo/docker-android:emulator_10.0_<release_version>|\n|11.0|30|budtmo/docker-android:emulator_11.0|budtmo/docker-android:emulator_11.0_<release_version>|\n|12.0|32|budtmo/docker-android:emulator_12.0|budtmo/docker-android:emulator_12.0_<release_version>|\n|13.0|33|budtmo/docker-android:emulator_13.0|budtmo/docker-android:emulator_13.0_<release_version>|\n|14.0|34|budtmo/docker-android:emulator_14.0|budtmo/docker-android:emulator_14.0_<release_version>|\n|-|-|budtmo/docker-android:genymotion|budtmo/docker-android:genymotion_<release_version>|\n\nList of Devices\n---------------\n\nType   | Device Name\n-----  | -----\nPhone  | Samsung Galaxy S10\nPhone  | Samsung Galaxy S9\nPhone  | Samsung Galaxy S8\nPhone  | Samsung Galaxy S7 Edge\nPhone  | Samsung Galaxy S7\nPhone  | Samsung Galaxy S6\nPhone  | Nexus 4\nPhone  | Nexus 5\nPhone  | Nexus One\nPhone  | Nexus S\nTablet | Nexus 7\nTablet | Pixel C\n\nRequirements\n------------\n\n1. Docker is installed on your system.\n\nQuick Start\n-----------\n\n1. If you use ***Ubuntu OS*** on your host machine, you can skip this step. For ***OSX*** and ***Windows OS*** user, you need to use Virtual Machine that support Virtualization with Ubuntu OS because the image can be run under ***Ubuntu OS only***.\n\n2. Your machine should support virtualization. To check if the virtualization is enabled is:\n    ```\n    sudo apt install cpu-checker\n    kvm-ok\n    ```\n\n3. Run Docker-Android container\n    ```\n    docker run -d -p 6080:6080 -e EMULATOR_DEVICE=\"Samsung Galaxy S10\" -e WEB_VNC=true --device /dev/kvm --name android-container budtmo/docker-android:emulator_11.0\n    ```\n\n4. Open ***http://localhost:6080*** to see inside running container.\n\n5. To check the status of the emulator\n    ```\n    docker exec -it android-container cat device_status\n    ```\n\nPersisting data\n-----------\n\nThe default behaviour is to destroy the emulated device on container restart. To persist data, you need to mount a volume at `/home/androidusr`:\n    ```\n    docker run -v data:/home/androidusr budtmo/docker-android:emulator_11.0\n    ```\n\nWSL2 Hardware acceleration (Windows 11 only)\n-----------\n\nCredit goes to [Guillaume - The Parallel Interface blog](https://www.paralint.com/2022/11/find-new-modified-and-unversioned-subversion-files-on-windows)\n\n[Microsoft - Advanced settings configuration in WSL](https://learn.microsoft.com/en-us/windows/wsl/wsl-config)\n\n\n1. Add yourself to the `kvm` usergroup.\n    ```\n    sudo usermod -a -G kvm ${USER}\n    ```\n\n2. Add necessary flags to `/etc/wsl2.conf` to their respective sections.\n    ```\n    [boot]\n    command = /bin/bash -c 'chown -v root:kvm /dev/kvm && chmod 660 /dev/kvm'\n\n    [wsl2]\n    nestedVirtualization=true\n    ```\n3. Restart WSL2 via CMD prompt or Powershell\n    ```\n    wsl --shutdown\n    ```\n\n\n`command = /bin/bash -c 'chown -v root:kvm /dev/kvm && chmod 660 /dev/kvm'` sets `/dev/kvm` to `kvm` usergroup rather than the default `root` usergroup on WSL2 startup.\n\n`nestedVirtualization` flag is only available to Windows 11.\n\nUse-Cases\n---------\n\n1. [Build Android project](./documentations/USE_CASE_BUILD_ANDROID_PROJECT.md)\n2. [UI-Test with Appium](./documentations/USE_CASE_APPIUM.md)\n3. [Control Android emulator on host machine](./documentations/USE_CASE_CONTROL_EMULATOR.md)\n4. [SMS Simulation](./documentations/USE_CASE_SMS.md)\n5. [Jenkins](./documentations/USE_CASE_JENKINS.md)\n6. [Deploying on cloud (Azure, AWS, GCP)](./documentations/USE_CASE_CLOUD.md)\n\nCustom-Configurations\n---------------------\n\nThis [document](./documentations/CUSTOM_CONFIGURATIONS.md) contains information about configurations that can be used to enable some features, e.g. log-sharing, etc.\n\nGenymotion\n----------\n\n<p align=\"center\">\n  <img id=\"geny\" src=\"./images/logo_genymotion_and_dockerandroid.png\" />\n</p>\n\nFor you who do not have ressources to maintain the simulator or to buy machines or need different device profiles, you can give a try by using [Genymotion SAAS](https://cloud.geny.io/). Docker-Android is [integrated with Genymotion](https://www.genymotion.com/blog/partner_tag/docker/) on different cloud services, e.g. Genymotion SAAS, AWS, GCP, Alibaba Cloud. Please follow [this document](./documentations/THIRD_PARTY_GENYMOTION.md) for more detail.\n\nEmulator Skins\n--------------\nThe Emulator skins are taken from [Android Studio IDE](https://developer.android.com/studio) and [Samsung Developer Website](https://developer.samsung.com/)\n\nUSERS\n-----\n\n<a href=\"https://lookerstudio.google.com/s/iGaemHJqQvg\">\n  <p align=\"center\">\n    <img src=\"./images/docker-android_users.png\" alt=\"docker-android-users\" width=\"800\" height=\"600\">\n  </p>\n</a>\n\nPRO VERSION\n-----------\n\nDue to high requests for help and to be able to actively maintain the projects, the creator has decided to create docker-android-pro. Docker-Android-Pro is a sponsor based project which mean that the docker image of pro-version can be pulled only by [active sponsor](https://github.com/sponsors/budtmo).\n\nThe differences between normal version and pro version are:\n\n|Feature   |Normal   |Pro   |Comment|\n|:---|:---|:---|:---|\n|user-behavior-analytics|Yes|No|-|\n|proxy|No|Yes|Set up company proxy on Android emulator on fly|\n|language|No|Yes|Set up language on Android emulator on fly|\n|Newer Android version|No|Yes|Support other newer Android version e.g. Android 15, etc|\n|root-privileged|No|Yes|Able to run command with security privileged|\n|headless-mode|No|Yes|Save resources by using headless mode|\n|Selenium 4.x integration|No|Yes|Running Appium UI-Tests againt one (Selenium Hub) endpoint for Android- and iOS emulator(s) / device(s)|\n|multiple Android-Simulators|No|Yes (soon)|Save resources by having multiple Android-Simulators on one docker-container|\n|Google Play Store|No|Yes (soon)|-|\n|Video Recording|No|Yes (soon)|Helpful for debugging|\n\nThis [document](./documentations/DOCKER-ANDROID-PRO.md) contains detail information about how to use docker-android-pro.\n\nLICENSE\n-------\nSee [License](LICENSE.md)\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 11121, "quality_score": 0.89, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 179}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\n\nfrom setuptools import setup, find_packages\n\n\napp_version = os.getenv(\"DOCKER_ANDROID_VERSION\", \"test-version\")\n\nwith open(\"requirements.txt\", \"r\") as f:\n    reqs = f.read().splitlines()\n\nsetup(\n    name=\"docker-android\",\n    version=\"0.1\",\n    url=\"https://github.com/budtmo/docker-android\",\n    description=\"CLI for docker-android\",\n    author=\"Budi Utomo\",\n    author_email=\"budtmo.os@gmail.com\",\n    install_requires=reqs,\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    py_modules=[\"cli\", \"docker-android\"],\n    entry_points={\"console_scripts\": \"docker-android=app:cli\"}\n)", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "cli/setup.py", "language": "python", "complexity": "simple", "star_count": 11121, "quality_score": 0.8600000000000001, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\nimport subprocess\nfrom typing import Union\n\nimport click\nimport logging\nimport os\n\nfrom enum import Enum\n\nfrom application import Application\nfrom device import DeviceType\nfrom device.emulator import Emulator\nfrom device.geny_aws import GenyAWS\nfrom device.geny_saas import GenySAAS\nfrom helper import convert_str_to_bool, get_env_value_or_raise\nfrom constants import ENV\nfrom logger import log\n\nlog.init()\nlogger = logging.getLogger(\"App\")\n\n\ndef get_device(given_input: str) -> Union[Emulator, GenyAWS, GenySAAS, None]:\n    \"\"\"\n    Get Device object based on given input\n\n    :param given_input: device in string\n    :return: Platform object\n    \"\"\"\n\n    input_lower = given_input.lower()\n\n    if input_lower == DeviceType.EMULATOR.value.lower():\n        emu_av = get_env_value_or_raise(ENV.EMULATOR_ANDROID_VERSION)\n        emu_img_type = get_env_value_or_raise(ENV.EMULATOR_IMG_TYPE)\n        emu_sys_img = get_env_value_or_raise(ENV.EMULATOR_SYS_IMG)\n\n        emu_device = os.getenv(ENV.EMULATOR_DEVICE, \"Nexus 5\")\n        emu_data_partition = os.getenv(ENV.EMULATOR_DATA_PARTITION, \"550m\")\n        emu_additional_args = os.getenv(ENV.EMULATOR_ADDITIONAL_ARGS, \"\")\n\n        emu_name = os.getenv(ENV.EMULATOR_NAME, \"{d}_{v}\".format(\n            d=emu_device.replace(\" \", \"_\").lower(), v=emu_av))\n        emu = Emulator(emu_name, emu_device, emu_av, emu_data_partition,\n                       emu_additional_args, emu_img_type, emu_sys_img)\n        return emu\n    elif input_lower == DeviceType.GENY_AWS.value.lower():\n        return GenyAWS()\n    elif input_lower == DeviceType.GENY_SAAS.value.lower():\n        return GenySAAS()\n    else:\n        return None\n\n\n@click.group(context_settings=dict(help_option_names=['-h', '--help']))\ndef cli():\n    pass\n\n\ndef start_appium() -> None:\n    if convert_str_to_bool(os.getenv(ENV.APPIUM)):\n        cmd = f\"/usr/bin/appium\"\n        app_appium = Application(\"Appium\", cmd,\n                                 os.getenv(ENV.APPIUM_ADDITIONAL_ARGS, \"\"), False)\n        app_appium.start()\n    else:\n        logger.info(\"env APPIUM cannot be found, Appium is not started!\")\n\n\ndef start_device() -> None:\n    given_pt = get_env_value_or_raise(ENV.DEVICE_TYPE)\n    selected_device = get_device(given_pt)\n    if selected_device is None:\n        raise RuntimeError(f\"'{given_pt}' is invalid! Please check again!\")\n    selected_device.create()\n    selected_device.start()\n    selected_device.wait_until_ready()\n    selected_device.reconfigure()\n    selected_device.keep_alive()\n\n\ndef start_display_screen() -> None:\n    cmd = \"/usr/bin/Xvfb\"\n    args = f\"{os.getenv(ENV.DISPLAY)} \" \\\n           f\"-screen {os.getenv(ENV.SCREEN_NUMBER)} \" \\\n           f\"{os.getenv(ENV.SCREEN_WIDTH)}x\" \\\n           f\"{os.getenv(ENV.SCREEN_HEIGHT)}x\" \\\n           f\"{os.getenv(ENV.SCREEN_DEPTH)}\"\n    d_screen = Application(\"d_screen\", cmd, args, False)\n    d_screen.start()\n\n\ndef start_display_wm() -> None:\n    cmd = \"/usr/bin/openbox-session\"\n    d_wm = Application(\"d_wm\", cmd)\n    d_wm.start()\n\n\ndef start_port_forwarder() -> None:\n    import socket\n    local_ip = socket.gethostbyname(socket.gethostname())\n    cmd = f\"/usr/bin/socat tcp-listen:5554,bind={local_ip},fork tcp:127.0.0.1:5554 & \" \\\n          f\"/usr/bin/socat tcp-listen:5555,bind={local_ip},fork tcp:127.0.0.1:5555\"\n    pf = Application(\"port_forwarder\", cmd)\n    pf.start()\n\n\ndef start_vnc_server() -> None:\n    cmd = \"/usr/bin/x11vnc\"\n    vnc_pass = os.getenv(ENV.VNC_PASSWORD)\n    if vnc_pass:\n        pass_path = os.path.join(os.getenv(ENV.WORK_PATH), \".vncpass\")\n        subprocess.check_call(f\"{cmd} -storepasswd {vnc_pass} {pass_path}\", shell=True)\n        last_arg = f\"-rfbauth {pass_path}\"\n    else:\n        last_arg = \"-nopw\"\n\n    display = os.getenv(ENV.DISPLAY)\n    args = f\"-display {display} -forever -shared {last_arg}\"\n    vnc_server = Application(\"vnc_web\", cmd, args, False)\n    vnc_server.start()\n\n\ndef start_vnc_web() -> None:\n    if convert_str_to_bool(os.getenv(ENV.WEB_VNC)):\n        vnc_port = get_env_value_or_raise(ENV.VNC_PORT)\n        vnc_web_port = get_env_value_or_raise(ENV.WEB_VNC_PORT)\n        cmd = \"/opt/noVNC/utils/novnc_proxy\"\n        args = f\"--vnc localhost:{vnc_port} localhost:{vnc_web_port}\"\n        vnc_web = Application(\"vnc_web\", cmd, args, False)\n        vnc_web.start()\n    else:\n        logger.info(\"env WEB_VNC cannot be found, VNC_WEB is not started!\")\n\n\n@cli.command()\n@click.argument(\"app\", type=click.Choice([app.value for app in Application.App]))\ndef start(app):\n    selected_app = str(app).lower()\n    if selected_app == Application.App.APPIUM.value.lower():\n        start_appium()\n    elif selected_app == Application.App.DEVICE.value.lower():\n        start_device()\n    elif selected_app == Application.App.DISPLAY_SCREEN.value.lower():\n        start_display_screen()\n    elif selected_app == Application.App.DISPLAY_WM.value.lower():\n        start_display_wm()\n    elif selected_app == Application.App.PORT_FORWARDER.value.lower():\n        start_port_forwarder()\n    elif selected_app == Application.App.VNC_SERVER.value.lower():\n        start_vnc_server()\n    elif selected_app == Application.App.VNC_WEB.value.lower():\n        start_vnc_web()\n    else:\n        logger.error(f\"application '{selected_app}' is not supported!\")\n\n\nclass SharedComponent(Enum):\n    LOG = \"log\"\n\n\ndef shared_log() -> None:\n    if convert_str_to_bool(os.getenv(ENV.WEB_LOG)):\n        from http.server import BaseHTTPRequestHandler, HTTPServer\n\n        log_path = get_env_value_or_raise(ENV.LOG_PATH)\n        log_port = int(get_env_value_or_raise(ENV.WEB_LOG_PORT))\n        logger.info(f\"Shared log is enabled! all logs can be found on port '{log_port}'\")\n\n        class LogSharedHandler(BaseHTTPRequestHandler):\n            def do_GET(self):\n                # root path\n                if self.path == \"/\":\n                    html = \"<html><body>\"\n                    for f in os.listdir(log_path):\n                        html += f\"<p><a href=\\\"{f}\\\">{f}</a></p>\"\n                    html += \"</body></html>\"\n\n                    self.send_response(200)\n                    self.send_header(\"Content-type\", \"text/html\")\n                    self.end_headers()\n                    self.wfile.write(html.encode())\n                # open each selected log file\n                else:\n                    p = log_path + self.path\n                    try:\n                        with open(p, \"rb\") as file:\n                            self.send_response(200)\n                            self.send_header(\"Content-type\", \"text/plain\")\n                            self.end_headers()\n                            self.wfile.write(file.read())\n                    except FileNotFoundError:\n                        self.send_error(404, \"File not found\")\n\n        httpd = HTTPServer(('0.0.0.0', log_port), LogSharedHandler)\n        httpd.serve_forever()\n    else:\n        logger.info(f\"Shared log is disabled! nothing to do!\")\n\n\n@cli.command()\n@click.argument(\"component\", type=click.Choice([component.value for component in SharedComponent]))\ndef share(component):\n    selected_component = str(component).lower()\n    if selected_component == SharedComponent.LOG.value.lower():\n        shared_log()\n    else:\n        logger.error(f\"component '{component}' is not supported!\")\n\n\nif __name__ == '__main__':\n    cli()\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "cli/src/app.py", "language": "python", "complexity": "complex", "star_count": 11121, "quality_score": 1.0, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 214}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# General\nDOCKER_ANDROID_VERSION = \"DOCKER_ANDROID_VERSION\"\nUSER_BEHAVIOR_ANALYTICS = \"USER_BEHAVIOR_ANALYTICS\"\nAPPIUM = \"APPIUM\"\nAPPIUM_ADDITIONAL_ARGS = \"APPIUM_ADDITIONAL_ARGS\"\nDISPLAY = \"DISPLAY\"\nSCREEN_DEPTH = \"SCREEN_DEPTH\"\nSCREEN_HEIGHT = \"SCREEN_HEIGHT\"\nSCREEN_NUMBER = \"SCREEN_NUMBER\"\nSCREEN_WIDTH = \"SCREEN_WIDTH\"\nVNC_PASSWORD = \"VNC_PASSWORD\"\nVNC_PORT = \"VNC_PORT\"\nWEB_VNC_PORT = \"WEB_VNC_PORT\"\nWEB_VNC = \"WEB_VNC\"\nWORK_PATH = \"WORK_PATH\"\nLOG_PATH = \"LOG_PATH\"\nWEB_LOG_PORT = \"WEB_LOG_PORT\"\nWEB_LOG = \"WEB_LOG\"\n\n# Device\nDEVICE_INTERVAL_WAITING = \"DEVICE_INTERVAL_WAITING\"\nDEVICE_TYPE = \"DEVICE_TYPE\"\n\n# Device (Emulator)\nEMULATOR_ADDITIONAL_ARGS = \"EMULATOR_ADDITIONAL_ARGS\"\nEMULATOR_ANDROID_VERSION = \"EMULATOR_ANDROID_VERSION\"\nEMULATOR_DATA_PARTITION = \"EMULATOR_DATA_PARTITION\"\nEMULATOR_DEVICE = \"EMULATOR_DEVICE\"\nEMULATOR_IMG_TYPE = \"EMULATOR_IMG_TYPE\"\nEMULATOR_NAME = \"EMULATOR_NAME\"\nEMULATOR_NO_SKIN = \"EMULATOR_NO_SKIN\"\nEMULATOR_SYS_IMG = \"EMULATOR_SYS_IMG\"\nEMULATOR_CONFIG_PATH = \"EMULATOR_CONFIG_PATH\"\n\n# Device (Genymotion - General)\nGENYMOTION_TEMPLATE_PATH = \"GENYMOTION_TEMPLATE_PATH\"\n\n# Device (Geny_SAAS)\nGENY_SAAS_USER = \"GENY_SAAS_USER\"\nGENY_SAAS_PASS = \"GENY_SAAS_PASS\"\nGENY_AUTH_TOKEN = \"GENY_AUTH_TOKEN\"\nGENY_SAAS_TEMPLATE_FILE_NAME = \"saas.json\"\n\n# Device (Geny_AWS)\nAWS_ACCESS_KEY_ID = \"AWS_ACCESS_KEY_ID\"\nAWS_SECRET_ACCESS_KEY = \"AWS_SECRET_ACCESS_KEY\"\nGENY_AWS_TEMPLATE_FILE_NAME = \"aws.json\"\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "cli/src/constants/ENV.py", "language": "python", "complexity": "simple", "star_count": 11121, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 48}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create infrastructure as code for cloud resource provisioning with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nimport logging\nimport os\nimport platform\nimport requests\nimport signal\nimport time\n\nfrom abc import ABC, abstractmethod\nfrom enum import Enum\n\nfrom helper import convert_str_to_bool, get_env_value_or_raise\nfrom constants import DEVICE, ENV\n\n\nclass DeviceType(Enum):\n    EMULATOR = \"emulator\"\n    GENY_SAAS = \"geny_saas\"\n    GENY_AWS = \"geny_aws\"\n\n\nclass Device(ABC):\n    FORM_ID = \"1FAIpQLSdrKWQdMh6Nt8v8NQdYvTIntohebAgqWCpXT3T9NofAoxcpkw\"\n    FORM_USER = \"user\"\n    FORM_CITY = \"city\"\n    FORM_REGION = \"region\"\n    FORM_COUNTRY = \"country\"\n    FORM_APP_VERSION = \"app_version\"\n    FORM_APPIUM = \"appium\"\n    FORM_APPIUM_ADDITIONAL_ARGS = \"appium_additional_args\"\n    FORM_WEB_LOG = \"web_log\"\n    FORM_WEB_VNC = \"web_vnc\"\n    FORM_SCREEN_RESOLUTION = \"screen_resolution\"\n    FORM_DEVICE_TYPE = \"device_type\"\n    FORM_EMU_DEVICE = \"emu_device\"\n    FORM_EMU_ANDROID_VERSION = \"emu_android_version\"\n    FORM_EMU_NO_SKIN = \"emu_no_skin\"\n    FORM_EMU_DATA_PARTITION = \"emu_data_partition\"\n    FORM_EMU_ADDITIONAL_ARGS = \"emu_additional_args\"\n\n    def __init__(self) -> None:\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.device_type = None\n        self.interval_waiting = int(os.getenv(ENV.DEVICE_INTERVAL_WAITING, 2))\n        self.user_behavior_analytics = convert_str_to_bool(os.getenv(ENV.USER_BEHAVIOR_ANALYTICS, \"true\"))\n        self.form_field = {\n            Device.FORM_USER: \"entry.108751316\",\n            Device.FORM_CITY: \"entry.2083022547\",\n            Device.FORM_REGION: \"entry.1083141079\",\n            Device.FORM_COUNTRY: \"entry.1946159560\",\n            Device.FORM_APP_VERSION: \"entry.818050927\",\n            Device.FORM_APPIUM: \"entry.181610571\",\n            Device.FORM_APPIUM_ADDITIONAL_ARGS: \"entry.727759656\",\n            Device.FORM_WEB_LOG: \"entry.1225589007\",\n            Device.FORM_WEB_VNC: \"entry.2055392048\",\n            Device.FORM_SCREEN_RESOLUTION: \"entry.709976626\",\n            Device.FORM_DEVICE_TYPE: \"entry.207096546\",\n            Device.FORM_EMU_DEVICE: \"entry.1960740382\",\n            Device.FORM_EMU_ANDROID_VERSION: \"entry.671872491\",\n            Device.FORM_EMU_NO_SKIN: \"entry.403556951\",\n            Device.FORM_EMU_DATA_PARTITION: \"entry.1052258875\",\n            Device.FORM_EMU_ADDITIONAL_ARGS: \"entry.57529972\"\n        }\n        self.form_data = {}\n        signal.signal(signal.SIGTERM, self.tear_down)\n\n    def set_status(self, current_status) -> None:\n        bashrc_file = f\"{os.getenv(ENV.WORK_PATH)}/device_status\"\n        with open(bashrc_file, \"w+\") as bf:\n            bf.write(current_status)\n        # It won't work using docker exec\n        # os.environ[constants.ENV_DEVICE_STATUS] = current_status\n\n    def _prepare_analytics_payload(self) -> None:\n        self.form_data.update({\n            self.form_field[Device.FORM_USER]: f\"{platform.platform()}_{platform.version().replace(' ', '_')}\",\n            self.form_field[Device.FORM_APP_VERSION]: os.getenv(ENV.DOCKER_ANDROID_VERSION),\n            self.form_field[Device.FORM_DEVICE_TYPE]: self.device_type,\n            self.form_field[Device.FORM_WEB_VNC]: convert_str_to_bool(os.getenv(ENV.WEB_VNC)),\n            self.form_field[Device.FORM_WEB_LOG]: convert_str_to_bool(os.getenv(ENV.WEB_LOG)),\n            self.form_field[Device.FORM_APPIUM]: convert_str_to_bool(os.getenv(ENV.APPIUM))\n        })\n\n        try:\n            res = requests.get(\"https://ipinfo.io\")\n            if res.ok:\n                json_res = res.json()\n                self.form_data.update({\n                    self.form_field[Device.FORM_CITY]: json_res[Device.FORM_CITY],\n                    self.form_field[Device.FORM_REGION]: json_res[Device.FORM_REGION],\n                    self.form_field[Device.FORM_COUNTRY]: json_res[Device.FORM_COUNTRY]\n                })\n        except requests.exceptions.RequestException as rer:\n            self.logger.warning(rer)\n            pass\n        except KeyError as ke:\n            self.logger.warning(ke)\n            pass\n\n    def create(self) -> None:\n        if self.user_behavior_analytics:\n            self.logger.info(\"Sending user behavior analytics to improve the tool\")\n            try:\n                form_url = f\"https://docs.google.com/forms/d/e/{Device.FORM_ID}/formResponse\"\n                self._prepare_analytics_payload()\n                requests.post(url=form_url, data=self.form_data)\n            except requests.exceptions.RequestException as rer:\n                self.logger.warning(rer)\n                pass\n        self.set_status(DEVICE.STATUS_CREATING)\n\n    def start(self) -> None:\n        self.set_status(DEVICE.STATUS_STARTING)\n\n    def wait_until_ready(self) -> None:\n        self.set_status(DEVICE.STATUS_BOOTING)\n\n    def reconfigure(self) -> None:\n        self.set_status(DEVICE.STATUS_RECONFIGURING)\n\n    def keep_alive(self) -> None:\n        self.set_status(DEVICE.STATUS_READY)\n        self.logger.warning(f\"{self.device_type} process will be kept alive to be able to get sigterm signal...\")\n        while True:\n            time.sleep(2)\n\n    @abstractmethod\n    def tear_down(self, *args) -> None:\n        pass\n\n\nclass Genymotion(Device):\n    def __init__(self) -> None:\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n    def get_data_from_template(self, filename: str) -> dict:\n        path_template_json = os.path.join(get_env_value_or_raise(ENV.GENYMOTION_TEMPLATE_PATH), filename)\n        data = {}\n        if os.path.isfile(path_template_json):\n            try:\n                self.logger.info(path_template_json)\n                with open(path_template_json, \"r\") as f:\n                    data = json.load(f)\n            except FileNotFoundError as fnf:\n                self.shutdown_and_logout()\n                self.logger.error(f\"File cannot be found: {fnf}\")\n            except json.JSONDecodeError as jde:\n                self.shutdown_and_logout()\n                self.logger.error(f\"Error Decoding Json: {jde}\")\n            except Exception as e:\n                self.shutdown_and_logout()\n                self.logger.error(e)\n        else:\n            self.shutdown_and_logout()\n            raise RuntimeError(f\"'{path_template_json}' cannot be found!\")\n        return data\n\n    @abstractmethod\n    def login(self) -> None:\n        pass\n\n    def create(self) -> None:\n        super().create()\n        self.login()\n\n    @abstractmethod\n    def shutdown_and_logout(self) -> None:\n        pass\n\n    def tear_down(self, *args) -> None:\n        self.shutdown_and_logout()\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "cli/src/device/__init__.py", "language": "python", "complexity": "complex", "star_count": 11121, "quality_score": 1.0, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 173}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create infrastructure as code for cloud resource provisioning with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nimport os\nimport subprocess\nimport time\n\nfrom enum import Enum\n\nfrom device import Device, DeviceType\nfrom helper import convert_str_to_bool, get_env_value_or_raise, symlink_force\nfrom constants import ENV, UTF8\n\n\nclass Emulator(Device):\n    DEVICE = (\n        \"Nexus 4\",\n        \"Nexus 5\",\n        \"Nexus 7\",\n        \"Nexus One\",\n        \"Nexus S\",\n        \"Samsung Galaxy S6\",\n        \"Samsung Galaxy S7\",\n        \"Samsung Galaxy S7 Edge\",\n        \"Samsung Galaxy S8\",\n        \"Samsung Galaxy S9\",\n        \"Samsung Galaxy S10\",\n        \"Pixel C\"\n    )\n\n    API_LEVEL = {\n        \"9.0\": \"28\",\n        \"10.0\": \"29\",\n        \"11.0\": \"30\",\n        \"12.0\": \"32\",\n        \"13.0\": \"33\",\n        \"14.0\": \"34\"\n    }\n\n    adb_name_id = 5554\n\n    class ReadinessCheck(Enum):\n        BOOTED = \"booted\"\n        RUN_STATE = \"in running state\"\n        WELCOME_SCREEN = \"in welcome screen\"\n        POP_UP_WINDOW = \"pop up window\"\n\n    def __init__(self, name: str, device: str, android_version: str, data_partition: str,\n                 additional_args: str, img_type: str, sys_img: str) -> None:\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.adb_name = f\"emulator-{Emulator.adb_name_id}\"\n        self.device_type = DeviceType.EMULATOR.value\n        self.name = name\n        if device in self.DEVICE:\n            self.device = device\n        else:\n            raise RuntimeError(f\"device '{device}' is not supported!\")\n        if android_version in self.API_LEVEL.keys():\n            self.android_version = android_version\n        else:\n            raise RuntimeError(f\"android version '{android_version}' is not supported!\")\n        self.api_level = self.API_LEVEL[self.android_version]\n        self.data_partition = data_partition\n        self.additional_args = additional_args\n        self.img_type = img_type\n        self.sys_img = sys_img\n        workdir = get_env_value_or_raise(ENV.WORK_PATH)\n        self.path_device_profile_target = os.path.join(workdir, \".android\", \"devices.xml\")\n        self.path_emulator = os.path.join(workdir, \"emulator\")\n        self.path_emulator_config = os.path.join(workdir, \"emulator\", \"config.ini\")\n        self.path_emulator_profiles = os.path.join(workdir, \"docker-android\", \"mixins\",\n                                                   \"configs\", \"devices\", \"profiles\")\n        self.path_emulator_skins = os.path.join(workdir, \"docker-android\", \"mixins\",\n                                                \"configs\", \"devices\", \"skins\")\n        self.file_name = self.device.replace(\" \", \"_\").lower()\n        self.no_skin = convert_str_to_bool(os.getenv(ENV.EMULATOR_NO_SKIN))\n        self.interval_after_booting = 15\n        Emulator.adb_name_id += 2\n        self.form_data.update({\n            self.form_field[Device.FORM_SCREEN_RESOLUTION]: f\"{os.getenv(ENV.SCREEN_WIDTH)}x\"\n                                                            f\"{os.getenv(ENV.SCREEN_HEIGHT)}x\"\n                                                            f\"{os.getenv(ENV.SCREEN_DEPTH)}\",\n            self.form_field[Device.FORM_EMU_DEVICE]: self.device,\n            self.form_field[Device.FORM_EMU_ANDROID_VERSION]: self.android_version,\n            self.form_field[Device.FORM_EMU_NO_SKIN]: self.no_skin,\n            self.form_field[Device.FORM_EMU_DATA_PARTITION]: self.data_partition,\n            self.form_field[Device.FORM_EMU_ADDITIONAL_ARGS]: self.additional_args\n        })\n\n    def is_initialized(self) -> bool:\n        import re\n        if os.path.exists(self.path_emulator_config):\n            self.logger.info(\"Config file exists\")\n            with open(self.path_emulator_config, 'r') as f:\n                if any(re.match(r'hw\\.device\\.name ?= ?{}'.format(self.device), line) for line in f):\n                    self.logger.info(\"Selected device is already created\")\n                    return True\n                else:\n                    self.logger.info(\"Selected device is not created\")\n                    return False\n\n        self.logger.info(\"Config file does not exist\")\n        return False\n\n    def _add_profile(self) -> None:\n        if \"samsung\" in self.device.lower():\n            path_device_profile_source = os.path.join(self.path_emulator_profiles,\n                                                      \"{fn}.xml\".format(fn=self.file_name))\n            symlink_force(path_device_profile_source, self.path_device_profile_target)\n            self.logger.info(\"Samsung device profile is linked\")\n\n    def _use_override_config(self) -> None:\n        override_confg_path = os.getenv(ENV.EMULATOR_CONFIG_PATH)\n\n        if override_confg_path is None:\n            self.logger.info(f\"The environment variable 'EMULATOR_CONFIG_PATH' is not set\")\n            return\n\n        self.logger.info(f\"Environment variable 'EMULATOR_CONFIG_PATH' found: {override_confg_path}\")\n\n        if not os.path.isfile(override_confg_path):\n            self.logger.warning(f\"Source file '{override_confg_path}' does not exist.\")\n            return\n\n        if not os.access(override_confg_path, os.R_OK):\n            self.logger.warning(f\"Source file '{override_confg_path}' is not readable.\")\n            return\n\n        try:\n            with open(override_confg_path, 'r') as src, open(self.path_emulator_config, 'a') as dst:\n                dst.write(src.read())\n            self.logger.info(f\"Content from '{override_confg_path}' successfully appended to '{self.path_emulator_config}'.\")\n        except Exception as e:\n            self.logger.error(f\"An error occurred while copying file content: {e}\")\n\n    def _add_skin(self) -> None:\n        device_skin_path = os.path.join(\n            self.path_emulator_skins, \"{fn}\".format(fn=self.file_name))\n        with open(self.path_emulator_config, \"a\") as cf:\n            cf.write(\"hw.keyboard=yes\\n\")\n            cf.write(\"disk.dataPartition.size={dp}\\n\".format(dp=self.data_partition))\n            cf.write(\"skin.path={sp}\\n\".format(\n                sp=\"_no_skin\" if self.no_skin else device_skin_path))\n        self.logger.info(f\"Skin is added in: '{self.path_emulator_config}'\")\n\n    def create(self) -> None:\n        super().create()\n        first_run = not self.is_initialized()\n        if first_run:\n            self.logger.info(f\"Creating the {self.device_type}...\")\n            self._add_profile()\n            creation_cmd = \"avdmanager create avd -f -n {n} -b {it}/{si} \" \\\n                           \"-k 'system-images;android-{al};{it};{si}' \" \\\n                           \"-d {d} -p {pe}\".format(n=self.name, it=self.img_type, si=self.sys_img,\n                                                   al=self.api_level, d=self.device.replace(\" \", \"\\ \"),\n                                                   pe=self.path_emulator)\n            self.logger.info(f\"Command to create emulator: '{creation_cmd}'\")\n            subprocess.check_call(creation_cmd, shell=True)\n            self._add_skin()\n            self._use_override_config()\n            self.logger.info(f\"{self.device_type} is created!\")\n\n    def change_permission(self) -> None:\n        kvm_path = \"/dev/kvm\"\n        if os.path.exists(kvm_path):\n            cmds = (f\"sudo chown 1300:1301 {kvm_path}\",\n                    \"sudo sed -i '1d' /etc/passwd\")\n            for c in cmds:\n                subprocess.check_call(c, shell=True)\n            self.logger.info(\"KVM permission is granted!\")\n        else:\n            raise RuntimeError(\"/dev/kvm cannot be found!\")\n\n    def deploy(self):\n        self.logger.info(f\"Deploying the {self.device_type}\")\n\n        basic_cmd = \"emulator @{n}\".format(n=self.name)\n        basic_args = \"-gpu swiftshader_indirect -accel on -writable-system -verbose\"\n        wipe_arg = \"-wipe-data\" if not self.is_initialized() else \"\"\n\n        start_cmd = f\"{basic_cmd} {basic_args} {wipe_arg} {self.additional_args}\"\n        self.logger.info(f\"Command to run {self.device_type}: '{start_cmd}'\")\n        subprocess.Popen(start_cmd.split())\n\n    def start(self) -> None:\n        super().start()\n        self.change_permission()\n        self.deploy()\n\n    def check_adb_command(self, readiness_check_type: ReadinessCheck, bash_command: str,\n                          expected_keyword: str, max_attempts: int, interval_waiting_time: int,\n                          adb_action: str = None) -> None:\n        success = False\n        for _ in range(1, max_attempts):\n            if success:\n                break\n            else:\n                try:\n                    output = subprocess.check_output(\n                        bash_command.split()).decode(UTF8)\n                    if expected_keyword in str(output).lower():\n                        if readiness_check_type is self.ReadinessCheck.POP_UP_WINDOW:\n                            subprocess.check_call(adb_action, shell=True)\n                        else:\n                            self.logger.info(\n                                f\"{self.device_type} is {readiness_check_type.value}!\")\n                            success = True\n                    else:\n                        self.logger.info(f\"[attempt: {_}] {self.device_type} is not {readiness_check_type.value}! \"\n                                         f\"will check again in {interval_waiting_time} seconds\")\n                        time.sleep(interval_waiting_time)\n                except subprocess.CalledProcessError:\n                    self.logger.warning(\"command cannot be executed! will continue...\")\n                    time.sleep(2)\n                    continue\n        else:\n            if readiness_check_type is self.ReadinessCheck.POP_UP_WINDOW:\n                self.logger.info(f\"Pop up windows '{expected_keyword}' is not found!\")\n            else:\n                raise RuntimeError(\n                    f\"{readiness_check_type.value} is checked {_} times!\")\n\n    def wait_until_ready(self) -> None:\n        super().wait_until_ready()\n        booting_cmd = f\"adb -s {self.adb_name} wait-for-device shell getprop sys.boot_completed\"\n        focus_cmd = f\"adb -s {self.adb_name} shell dumpsys window | grep -i mCurrentFocus\"\n        self.check_adb_command(self.ReadinessCheck.BOOTED,\n                               booting_cmd, \"1\", 60, self.interval_waiting)\n        time.sleep(self.interval_after_booting)\n\n        interval_pop_up = 0\n        max_attempt_pop_up = 3\n        pop_up_system_ui = \"Not Responding: com.android.systemui\"\n        system_ui_cmd = f\"adb shell su root 'kill $(pidof com.android.systemui)'\"\n        pop_up_key_enter = {\n            \"Not Responding: com.google.android.gms\",\n            \"Not Responding: system\",\n            \"ConversationListActivity\"\n        }\n        key_enter_cmd = \"adb shell input keyevent KEYCODE_ENTER\"\n        self.check_adb_command(self.ReadinessCheck.POP_UP_WINDOW, focus_cmd, pop_up_system_ui,\n                               max_attempt_pop_up, interval_pop_up, system_ui_cmd)\n        for pe in pop_up_key_enter:\n            self.check_adb_command(self.ReadinessCheck.POP_UP_WINDOW, focus_cmd, pe, max_attempt_pop_up,\n                                   interval_pop_up, key_enter_cmd)\n\n        self.check_adb_command(self.ReadinessCheck.WELCOME_SCREEN,\n                               focus_cmd, \"launcheractivity\", 60, self.interval_waiting)\n        self.logger.info(f\"{self.device_type} is ready to use\")\n\n    def tear_down(self, *args) -> None:\n        self.logger.warning(\"Sigterm is detected! Nothing to do!\")\n\n    def __repr__(self) -> str:\n        try:\n            return \"Emulator(name={n}, device={d}, adb_name={an}, android_version={av}, api_level={al}, \" \\\n                   \"data_partition={dp}, additional_args={aa}, img_type={it}, sys_img={si}, \" \\\n                   \"path_device_profile_target={pdpt}, path_emulator={pe}, path_emulator_config={pec}, \" \\\n                   \"file={f})\".format(n=self.name, d=self.device, an=self.adb_name, av=self.android_version,\n                                      al=self.api_level, dp=self.data_partition, aa=self.additional_args,\n                                      it=self.img_type, si=self.sys_img, pdpt=self.path_device_profile_target,\n                                      pe=self.path_emulator, pec=self.path_emulator_config, f=self.file_name)\n        except AttributeError as ae:\n            self.logger.error(ae)\n            return \"\"\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "cli/src/device/emulator.py", "language": "python", "complexity": "complex", "star_count": 11121, "quality_score": 1.0, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 265}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create infrastructure as code for cloud resource provisioning using Terraform infrastructure as code\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nimport logging\nimport os\nimport shutil\nimport subprocess\nimport time\n\nfrom device import Genymotion, DeviceType\nfrom helper import get_env_value_or_raise\nfrom constants import ENV, UTF8\n\n\nclass GenyAWS(Genymotion):\n    port = 5555\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.device_type = DeviceType.GENY_AWS.value\n        self.workdir = get_env_value_or_raise(ENV.WORK_PATH)\n        self.aws_credentials_path = os.path.join(self.workdir, \".aws\")\n        self.remove_cred_at_the_end = False  # for logout\n        self.geny_aws_template_path = os.path.join(self.workdir, \"docker-android\", \"mixins\",\n                                                   \"templates\", \"genymotion\", \"aws\")\n        self.created_devices = {}\n\n    def login(self) -> None:\n        aws_credentials_file = os.path.join(self.aws_credentials_path, \"credentials\")\n        if os.path.exists(self.aws_credentials_path):\n            self.logger.info(\".aws is found! It will be used as credentials\")\n        else:\n            self.logger.info(\".aws cannot be found! the template will be used!\")\n            self.remove_cred_at_the_end = True\n            aws_credentials_template_path = os.path.join(self.geny_aws_template_path, \".aws\")\n            shutil.move(aws_credentials_template_path, self.aws_credentials_path)\n\n            aws_key_id = get_env_value_or_raise(ENV.AWS_ACCESS_KEY_ID)\n            aws_secret_key = get_env_value_or_raise(ENV.AWS_SECRET_ACCESS_KEY)\n            replacements_cred = {\n                f\"<{ENV.AWS_ACCESS_KEY_ID.lower()}>\": aws_key_id,\n                f\"<{ENV.AWS_SECRET_ACCESS_KEY.lower()}>\": aws_secret_key\n            }\n            with open(aws_credentials_file, 'r+') as cred_file:\n                cred_file_contents = cred_file.read()\n                for old_str, new_str in replacements_cred.items():\n                    cred_file_contents = cred_file_contents.replace(old_str, new_str)\n                cred_file.seek(0)\n                cred_file.write(cred_file_contents)\n                cred_file.truncate()\n            self.logger.info(\"aws credentials is set!\")\n\n    def create_ssh_key(self) -> None:\n        subprocess.check_call('ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N \"\"', shell=True)\n        self.logger.info(\"ssh key is created!\")\n\n    def create_tf_files(self) -> None:\n        try:\n            for item in self.get_data_from_template(ENV.GENY_AWS_TEMPLATE_FILE_NAME):\n                name = item[\"name\"]\n                region = item[\"region\"]\n                ami = item[\"ami\"]\n                instance_type = item[\"instance_type\"]\n                if \"security_group\" in item:\n                    sg = item[\"security_group\"]\n                    tf_content = f'''\n                    provider \"aws\" {{\n                        alias  = \"provider_{name}\"\n                        region = \"{region}\"\n                    }}\n    \n                    resource \"aws_key_pair\" \"geny_key_{name}\" {{\n                        provider = aws.provider_{name}\n                        public_key = file(\"~/.ssh/id_rsa.pub\")\n                    }}\n    \n                    resource \"aws_instance\" \"geny_aws_{name}\" {{\n                        provider = aws.provider_{name}\n                        ami = \"{ami}\"\n                        instance_type = \"{instance_type}\"\n                        vpc_security_group_ids = [\"{sg}\"]\n                        key_name = aws_key_pair.geny_key_{name}.key_name\n                        tags = {{\n                            Name = \"DockerAndroid-GenyAWS-{ami}.id}}\"\n                        }}\n    \n                        provisioner \"remote-exec\" {{\n                            connection {{\n                                type = \"ssh\"\n                                user = \"shell\"\n                                host = self.public_ip\n                                private_key = file(\"~/.ssh/id_rsa\")\n                            }}\n                            script = \"/home/androidusr/docker-android/mixins/scripts/genymotion/aws/enable_adb.sh\"\n                        }}\n                    }}\n    \n                    output \"public_dns_{name}\" {{\n                        value = aws_instance.geny_aws_{name}.public_dns\n                    }}\n                    '''\n                else:\n                    ingress_rules = json.dumps(item[\"ingress_rules\"])\n                    egress_rules = json.dumps(item[\"egress_rules\"])\n                    tf_content = f'''\n                    locals {{\n                        ingress_rules = {ingress_rules}\n                        egress_rules = {egress_rules}\n                    }}\n                    \n                    provider \"aws\" {{\n                        alias  = \"provider_{name}\"\n                        region = \"{region}\"\n                    }}\n                    \n                    resource \"aws_security_group\" \"geny_sg_{name}\" {{\n                        provider = aws.provider_{name}\n                        dynamic \"ingress\" {{\n                            for_each = local.ingress_rules\n                            content {{\n                                from_port   = ingress.value.from_port\n                                to_port     = ingress.value.to_port\n                                protocol    = ingress.value.protocol\n                                cidr_blocks = ingress.value.cidr_blocks\n                            }}\n                        }}\n                        \n                        dynamic \"egress\" {{\n                            for_each = local.egress_rules\n                            content {{\n                                from_port   = egress.value.from_port\n                                to_port     = egress.value.to_port\n                                protocol    = egress.value.protocol\n                                cidr_blocks = egress.value.cidr_blocks\n                            }}\n                        }}\n                    }}\n        \n                    resource \"aws_key_pair\" \"geny_key_{name}\" {{\n                        provider = aws.provider_{name}\n                        public_key = file(\"~/.ssh/id_rsa.pub\")\n                    }}\n                    \n                    resource \"aws_instance\" \"geny_aws_{name}\" {{\n                        provider = aws.provider_{name}\n                        ami = \"{ami}\"\n                        instance_type = \"{instance_type}\"\n                        vpc_security_group_ids = [aws_security_group.geny_sg_{name}.name]\n                        key_name = aws_key_pair.geny_key_{name}.key_name\n                        tags = {{\n                            Name = \"DockerAndroid-GenyAWS-{ami}.id}}\"\n                        }}\n                        \n                        provisioner \"remote-exec\" {{\n                            connection {{\n                                type = \"ssh\"\n                                user = \"shell\"\n                                host = self.public_ip\n                                private_key = file(\"~/.ssh/id_rsa\")\n                            }}\n                            script = \"/home/androidusr/docker-android/mixins/scripts/genymotion/aws/enable_adb.sh\"\n                        }}\n                    }}\n        \n                    output \"public_dns_{name}\" {{\n                        value = aws_instance.geny_aws_{name}.public_dns\n                    }}\n                    '''\n                tf_deployment_filename = f\"{name}.tf\"\n                self.created_devices[name] = GenyAWS.port\n                GenyAWS.port += 1\n                with open(tf_deployment_filename, \"w\") as df:\n                    df.write(tf_content)\n            self.logger.info(\"Terraform files are created!\")\n        except Exception as e:\n            self.logger.error(e)\n            self.shutdown_and_logout()\n\n    def deploy_tf(self) -> None:\n        try:\n            cmds = (\n                \"terraform init\",\n                \"terraform plan\",\n                \"terraform apply -auto-approve\"\n            )\n            for c in cmds:\n                subprocess.check_call(c, shell=True)\n            self.logger.info(\"Genymotion-Device(s) are deployed on AWS\")\n        except subprocess.CalledProcessError as cpe:\n            self.logger.error(cpe)\n            self.shutdown_and_logout()\n\n    def connect_with_local_adb(self) -> None:\n        self.logger.info(f\"created devices: {self.created_devices}\")\n        try:\n            for d, p in self.created_devices.items():\n                dns_cmd = f\"terraform output public_dns_{d}\"\n                dns_ip = subprocess.check_output(dns_cmd.split()).decode(UTF8).replace('\"', '')\n                tunnel_cmd = f\"ssh -i ~/.ssh/id_rsa -o ServerAliveInterval=60 -o StrictHostKeyChecking=no -q -NL \" \\\n                             f\"{p}:localhost:5555 shell@{dns_ip}\"\n                subprocess.Popen(tunnel_cmd.split())\n                time.sleep(10)\n                subprocess.check_call(f\"adb connect localhost:{p} >/dev/null 2>&1\", shell=True)\n        except Exception as e:\n            self.logger.error(e)\n            self.shutdown_and_logout()\n\n    def create(self) -> None:\n        super().create()\n        self.create_ssh_key()\n        self.create_tf_files()\n        self.deploy_tf()\n        self.connect_with_local_adb()\n\n    def shutdown_and_logout(self) -> None:\n        try:\n            subprocess.check_call(\"terraform destroy -auto-approve -lock=false\", shell=True)\n            self.logger.info(\"device(s) is successfully removed!\")\n        except subprocess.CalledProcessError as cpe:\n            self.logger.error(cpe)\n        finally:\n            if self.remove_cred_at_the_end:\n                subprocess.check_call(f\"rm -rf {self.aws_credentials_path}\", shell=True)\n                self.logger.info(\"successfully logged out!\")\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "cli/src/device/geny_aws.py", "language": "python", "complexity": "complex", "star_count": 11121, "quality_score": 1.0, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": true, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 224}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nimport os\nimport subprocess\n\nfrom device import Genymotion, DeviceType\nfrom helper import get_env_value_or_raise\nfrom constants import ENV, UTF8\n\n\nclass GenySAAS(Genymotion):\n    def __init__(self) -> None:\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.device_type = DeviceType.GENY_SAAS.value\n        self.created_devices = []\n\n    def login(self) -> None:\n        if os.getenv(ENV.GENY_AUTH_TOKEN):\n            auth_token = get_env_value_or_raise(ENV.GENY_AUTH_TOKEN)\n            subprocess.check_call(f\"gmsaas auth token {auth_token} > /dev/null 2>&1\", shell=True)\n        else:\n            user = get_env_value_or_raise(ENV.GENY_SAAS_USER)\n            password = get_env_value_or_raise(ENV.GENY_SAAS_PASS)\n            subprocess.check_call(f\"gmsaas auth login {user} {password} > /dev/null 2>&1\", shell=True)\n        self.logger.info(\"successfully logged in!\")\n\n    def create(self) -> None:\n        super().create()\n        for item in self.get_data_from_template(ENV.GENY_SAAS_TEMPLATE_FILE_NAME):\n            name = \"\"\n            template = \"\"\n            local_port = \"\"\n\n            # implement like this because local_port param is not a must\n            for k, v in item.items():\n                if k.lower() == \"name\":\n                    name = v\n                elif k.lower() == \"template\":\n                    template = v\n                elif k.lower() == \"local_port\":\n                    local_port = v\n                else:\n                    self.logger.warning(f\"'{k}' is not supported! Please check the documentation!\")\n\n            if not name:\n                import uuid\n                name = str(uuid.uuid4())\n\n            if not template:\n                self.shutdown_and_logout()\n                raise RuntimeError(f\"'template' is a must parameter and not given!\")\n            else:\n                self.logger.info(f\"name: {name}, template: {template}\")\n                creation_cmd = f\"gmsaas instances start {template} {name}\"\n                try:\n                    instance_id = subprocess.check_output(creation_cmd.split()).decode(UTF8).replace(\"\\n\", \"\")\n                    created_device = {f\"{name}\": {instance_id}}\n                    self.created_devices.append(created_device)\n                    additional_args = \"\"\n                    if local_port:\n                        additional_args = f\"--adb-serial-port {local_port}\"\n                    connect_cmd = f\"gmsaas instances adbconnect {instance_id} {additional_args}\"\n                    subprocess.check_call(f\"{connect_cmd}\", shell=True)\n                except Exception as e:\n                    self.shutdown_and_logout()\n                    self.logger.error(e)\n                    exit(1)\n\n    def shutdown_and_logout(self) -> None:\n        if bool(self.created_devices):\n            self.logger.info(\"Created device(s) will be removed!\")\n            for d in self.created_devices:\n                for n, i in d.items():\n                    subprocess.check_call(f\"gmsaas instances stop {i}\", shell=True)\n                    self.logger.info(f\"device '{n}' is successfully removed!\")\n        if os.getenv(ENV.GENY_AUTH_TOKEN):\n            subprocess.check_call(\"gmsaas auth reset\", shell=True)\n        else:\n            subprocess.check_call(\"gmsaas auth logout\", shell=True)\n        self.logger.info(\"successfully logged out!\")\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "cli/src/device/geny_saas.py", "language": "python", "complexity": "medium", "star_count": 11121, "quality_score": 1.0, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 81}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nimport os\n\nlogger = logging.getLogger(\"helper\")\n\n\ndef convert_str_to_bool(given_str: str) -> bool:\n    \"\"\"\n    Convert String to Boolean value\n\n    :param given_str: given string\n    :return: converted string in Boolean\n    \"\"\"\n    if given_str:\n        if type(given_str) is str:\n            return given_str.lower() in (\"yes\", \"true\", \"t\", \"1\")\n        else:\n            raise AttributeError\n    else:\n        logger.info(f\"'{given_str}' is empty!\")\n        return False\n\n\ndef get_env_value_or_raise(env_key: str) -> str:\n    \"\"\"\n    Get value of necessary environment variable.\n\n    :param env_key: given environment variable\n    :return: env_value in String\n    \"\"\"\n    try:\n        env_value = os.getenv(env_key)\n        if not env_value:\n            raise RuntimeError(f\"'{env_key}' is missing.\")\n        elif env_value.isspace():\n            raise RuntimeError(f\"'{env_key}' contains only white space.\")\n        return env_value\n    except TypeError as t_err:\n        logger.error(t_err)\n\n\ndef symlink_force(source: str, target: str) -> None:\n    \"\"\"\n    Create Symbolic link\n\n    :param source: source file\n    :param target: target file\n    :return: None\n    \"\"\"\n    try:\n        os.symlink(source, target)\n    except FileNotFoundError as ffe_err:\n        logger.error(ffe_err)\n    except FileExistsError:\n        os.remove(target)\n        os.symlink(source, target)\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "cli/src/helper/__init__.py", "language": "python", "complexity": "simple", "star_count": 11121, "quality_score": 0.98, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 57}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import mock\n\nfrom device.emulator import Emulator\nfrom tests.device import BaseDeviceTest\n\n\nclass TestEmulator(BaseDeviceTest):\n    def setUp(self) -> None:\n        super().setUp()\n        self.name = \"my_emu\"\n        self.device = \"Nexus 4\"\n        self.a_version = \"10.0\"\n        self.d_partition = \"550m\"\n        self.additional_args = \"\"\n        self.i_type = \"google_apis\"\n        self.s_img = \"x86\"\n        self.emu = Emulator(self.name, self.device, self.a_version, self.d_partition,\n                            self.additional_args, self.i_type, self.s_img)\n\n    def tearDown(self) -> None:\n        super().tearDown()\n\n    def test_adb_name(self):\n        my_emu = Emulator(\"my_other_emu\", self.device, self.a_version, self.d_partition,\n                          self.additional_args, self.i_type, self.s_img)\n        self.assertNotEqual(self.emu.adb_name, my_emu.adb_name)\n\n    def test_invalid_device(self):\n        with self.assertRaises(RuntimeError):\n            Emulator(\"my_other_emu\", \"unknown device\", self.a_version, self.d_partition,\n                     self.additional_args, self.i_type, self.s_img)\n        with self.assertRaises(RuntimeError):\n            Emulator(\"my_other_emu\", \"NEXUS 5\", self.a_version, self.d_partition,\n                     self.additional_args, self.i_type, self.s_img)\n\n    def test_invalid_android_version(self):\n        with self.assertRaises(RuntimeError):\n            Emulator(\"my_other_emu\", self.device, \"0.0\", self.d_partition,\n                     self.additional_args, self.i_type, self.s_img)\n\n    @mock.patch(\"os.path.exists\", mock.MagicMock(return_value=False))\n    def test_initialisation_config_not_exist(self):\n        self.assertEqual(self.emu.is_initialized(), False)\n\n    @mock.patch(\"os.path.exists\", mock.MagicMock(return_value=True))\n    @mock.patch(\"builtins.open\", mock.mock_open(read_data=\"\"))\n    def test_initialisation_device_not_exist(self):\n        self.assertEqual(self.emu.is_initialized(), False)\n\n    @mock.patch(\"os.path.exists\", mock.MagicMock(return_value=True))\n    @mock.patch(\"builtins.open\", mock.mock_open(read_data=\"hw.device.name=Nexus 4\\n\"))\n    def test_initialisation_device_exists(self):\n        self.assertEqual(self.emu.is_initialized(), True)\n\n    def test_check_adb_command(self):\n        with mock.patch(\"subprocess.check_output\", mock.MagicMock(return_value=\"1\".encode(\"utf-8\"))):\n            self.emu.check_adb_command(\n                self.emu.ReadinessCheck.BOOTED, \"mocked_command\", \"1\", 3, 0)\n\n    def test_check_adb_command_out_of_attempts(self):\n        with mock.patch(\"subprocess.check_output\", mock.MagicMock(return_value=\" \".encode(\"utf-8\"))):\n            with self.assertRaises(RuntimeError):\n                self.emu.check_adb_command(\n                    self.emu.ReadinessCheck.BOOTED, \"mocked_command\", \"1\", 3, 0)\n\n    def test_use_override_config_no_env(self):\n        with mock.patch(\"os.getenv\", return_value=None):\n            self.emu._use_override_config()\n\n    def test_use_override_config_file_not_exist(self):\n        with mock.patch(\"os.getenv\", return_value=\"mock/path/to/config\"):\n            with mock.patch(\"os.path.isfile\", return_value=False):\n                self.emu._use_override_config()\n\n    def test_use_override_config_file_not_readable(self):\n        with mock.patch(\"os.getenv\", return_value=\"mock/path/to/config\"):\n            with mock.patch(\"os.path.isfile\", return_value=True):\n                with mock.patch(\"os.access\", return_value=False):\n                    self.emu._use_override_config()\n\n    def test_use_override_config_malformed_content(self):\n        with mock.patch(\"os.getenv\", return_value=\"mock/path/to/config\"):\n            with mock.patch(\"os.path.isfile\", return_value=True):\n                with mock.patch(\"os.access\", return_value=True):\n                    with mock.patch(\"builtins.open\", mock.mock_open(read_data=\"malformed data\")):\n                        self.emu._use_override_config()\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "cli/src/tests/device/test_emulator.py", "language": "python", "complexity": "medium", "star_count": 11121, "quality_score": 1.0, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 87}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "VNC-CLIENT\n----------\n\nVnc-server inside container is run on port 5900\n\n1. Run docker-android:\n    ```\n    docker run -d -p 5900:5900 -e EMULATOR_DEVICE=\"Samsung Galaxy S10\" --device /dev/kvm --name android-container budtmo/docker-android:emulator_11.0\n    ```\n\n2. Connect docker-container using vnc-client\n\nEnvironment variables that can be passed to docker container for vnc configuration:\n|Environment Variable   |Description   |Example|\n|:---|:---|:---|\n|VNC_PASSWORD|protect vnc connection with password|docker run ... -e VNC_PASSWORD=thisissecret ...|\n\n\nVNC-WEB\n-------\n\nVnc-web inside container is run on port 6080\n\nEnvironment variables that can be passed to docker container for vnc configuration:\n|Environment Variable   |Description   |Example|\n|:---|:---|:---|\n|WEB_VNC|access inside of the container through web-ui|docker run ... -p 6080:6080 -e WEB_VNC=true ...|\n|WEB_VNC_PORT|access inside of the container through web-ui on given port (default port 6080)|docker run ... -p 6081:6081 -e WEB_VNC=true -e WEB_VNC_PORT=6081 ...|\n\nPossible endpoints if WEB_VNC is activated:\n|Endpoint   |Description   |Example|\n|:---|:---|:---|\n|autoconnect|access web-ui of vnc direclty|http://localhost:6080/?autoconnect=true|\n|view_only|give only view access|http://localhost:6080/?autoconnect=true&view_only=true|\n|password|access web-ui of vnc directly with protected password|http://localhost:6080/?autoconnect=true&password=thisissecret|\n\n\nLOG-SHARING\n-----------\n\nThe user has possibility to access log-files through web-ui:\n|Environment Variable   |Description   |Example|\n|:---|:---|:---|\n|WEB_LOG|access log-files through web-ui|docker run ... -e WEB_LOG=true ...|\n|WEB_LOG_PORT|access log-files through web-ui on given port (default port 9000)|docker run ... -e WEB_LOG=true -e WEB_LOG_PORT=9001 ...|\n\n\nEMULATOR\n--------\n\nPossible environment variable to configure the Emulator:\n|Environment Variable   |Description   |Example|\n|:---|:---|:---|\n|EMULATOR_NAME|give emulator name (default name is a combination between Device name and Android version)|docker run ... -e EMULATOR_NAME=my_emu ...|\n|EMULATOR_DATA_PARTITION|set data partition on emulator (default value 550m)|docker run ... -e EMULATOR_DATA_PARTITION=900m ...|\n|EMULATOR_NO_SKIN|deploying emulator without skin|docker run ... -e EMULATOR_NO_SKIN=true ...|\n\nThe user can also pass needed arguments to android emulator through environment variable ***EMULATOR_ADDITIONAL_ARGS***. Please check [this page](https://developer.android.com/studio/run/emulator-commandline) for possible arguments that can be passed.\n\nEMULATOR - OVERRIDE CONFIG FILE\n-------------------------------\n\nTo utilize this feature, ensure the following setup:\n\n- Set the config file path into environment variable EMULATOR_CONFIG_PATH, eg. `/tmp/emulator-override-config.ini` (the file name doesn't matter)\n- Mount the file as Docker volume (parameter `-v` for Docker run command) to the path as you set into ENV\n\nExample:\n\n```shell\ndocker run -d -p 6080:6080 -e /tmp/emulator-override-config.ini -v /path/on/your/machine/emulator-override-config.ini:/tmp/emulator-override-config.ini -e EMULATOR_DEVICE=\"Samsung Galaxy S10\" -e WEB_VNC=true --device /dev/kvm --name android-container budtmo/docker-android:emulator_14.0\n```\n\n[<- BACK TO README](../README.md)\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "documentations/CUSTOM_CONFIGURATIONS.md", "language": "markdown", "complexity": "simple", "star_count": 11121, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 75}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Docker-Android-Pro\n------------------\n\nDocker-Android-Pro is a sponsor based project which mean that the docker image of pro-version can be pulled only by [active sponsor](https://github.com/sponsors/budtmo). After donation, please send email [here](mailto:budtmo2.os@gmail.com) with following format on subject email: ```<github_id>-<date-when-donation-is-made>-<email>``` e.g. ```budtmo-30.01.2021-myemail@test.com```. The script will validate everything and it will send the access token to that email within 48 hours to be able to pull the pro version of docker-android image. Contact [@budtmo](https://github.com/budtmo) if you dont get access token after donation. The access token will be removed as soon as the user become inactive sponsor.\n\nThe differences between normal version and pro version are:\n\n|Feature   |Normal   |Pro   |Comment|\n|:---|:---|:---|:---|\n|user-behavior-analytics|Yes|No|-|\n|proxy|No|Yes|Set up company proxy on Android emulator on fly|\n|language|No|Yes|Set up language on Android emulator on fly|\n|root-privileged|No|Yes|Able to run command with security privileged|\n|headless-mode|No|Yes|Save resources by using headless mode|\n|Selenium 4.x integration|No|Yes|Running Appium UI-Tests againt one (Selenium Hub) endpoint for Android- and iOS emulator(s) / device(s)|\n|multiple Android-Simulators|No|Yes (soon)|Save resources by having multiple Android-Simulators on one docker-container|\n|Google Play Store|No|Yes (soon)|-|\n|Video Recording|No|Yes (soon)|Helpful for debugging|\n\n\nList of Docker-Images\n---------------------\n|Android   |API   |Type  |Image with latest release version   |Image with specific release version|\n|:---|:---|:---|:---|:---|\n|9.0|28|Normal|budtmo2/docker-android-pro:emulator_9.0|budtmo2/docker-android-pro:emulator_9.0_<release_version>|\n|10.0|29|Normal|budtmo2/docker-android-pro:emulator_10.0|budtmo2/docker-android-pro:emulator_10.0_<release_version>|\n|11.0|30|Normal|budtmo2/docker-android-pro:emulator_11.0|budtmo2/docker-android-pro:emulator_11.0_<release_version>|\n|12.0|32|Normal|budtmo2/docker-android-pro:emulator_12.0|budtmo2/docker-android-pro:emulator_12.0_<release_version>|\n|13.0|33|Normal|budtmo2/docker-android-pro:emulator_13.0|budtmo2/docker-android-pro:emulator_13.0_<release_version>|\n|14.0|34|Normal|budtmo2/docker-android-pro:emulator_14.0|budtmo2/docker-android-pro:emulator_14.0_<release_version>|\n|15.0|35|Normal|budtmo2/docker-android-pro:emulator_15.0|budtmo2/docker-android-pro:emulator_15.0_<release_version>|\n|9.0|28|Headless|budtmo2/docker-android-pro:emulator_headless_9.0|budtmo2/docker-android-pro:emulator_headless_9.0_<release_version>|\n|10.0|29|Headless|budtmo2/docker-android-pro:emulator_headless_10.0|budtmo2/docker-android-pro:emulator_headless_10.0_<release_version>|\n|11.0|30|Headless|budtmo2/docker-android-pro:emulator_headless_11.0|budtmo2/docker-android-pro:emulator_headless_11.0_<release_version>|\n|12.0|32|Headless|budtmo2/docker-android-pro:emulator_headless_12.0|budtmo2/docker-android-pro:emulator_headless_12.0_<release_version>|\n|13.0|33|Headless|budtmo2/docker-android-pro:emulator_headless_13.0|budtmo2/docker-android-pro:emulator_headless_13.0_<release_version>|\n|14.0|34|Headless|budtmo2/docker-android-pro:emulator_headless_14.0|budtmo2/docker-android-pro:emulator_headless_14.0_<release_version>|\n|15.0|35|Headless|budtmo2/docker-android-pro:emulator_headless_15.0|budtmo2/docker-android-pro:emulator_headless_15.0_<release_version>|\n|-|-|Selenium|budtmo2/docker-android-pro:selenium|budtmo2/docker-android-pro:selenium_<release_version>|\n\n***Note: Headless mode does not have any Web-UI***\n\nYou can always pull the latest image tag. In case you want to see the release version that has been built with a changelog note and use that specific release version, the information will be sent to you as well.\n\n\nList of Devices\n---------------\n\nType   | Device Name\n-----  | -----\nPhone  | Samsung Galaxy S10\nPhone  | Samsung Galaxy S9\nPhone  | Samsung Galaxy S8\nPhone  | Samsung Galaxy S7 Edge\nPhone  | Samsung Galaxy S7\nPhone  | Samsung Galaxy S6\nPhone  | Nexus 4\nPhone  | Nexus 5\nPhone  | Nexus One\nPhone  | Nexus S\nTablet | Nexus 7\nTablet | Pixel C\n\n\nProxy\n-----\n\nYou can enable proxy inside container and Android emulator by passing following environment variables:\n\n- HTTP_PROXY=\"http://\\<docker\\_bridge\\_ip>:<port>\"\n- HTTPS_PROXY=\"http://\\<docker\\_bridge\\_ip>:<port>\"\n- NO_PROXY=\"localhost\"\n- EMULATOR_PROXY_URL=\"http://\\<docker\\_bridge\\_ip>:<port>\"\n- EMULATOR_PROXY_USER=\"\\<proxy_user>\"\n- EMULATOR_PROXY_PASS=\"\\<proxy_pass>\"\n\n\nLanguage\n--------\n\nYou can change the language setting of Android Emulator on the fly by passing following environment variable:\n\n- EMULATOR_LANGUAGE=\"\\<language>\"\n- EMULATOR_COUNTRY=\"\\<country>\"\n\n\nSelenium\n--------\n\nPull and run image that contains Selenium with Appium urls and its capabilities which is stored inside node.json file:\n\n```\ndocker run -t --rm --name selenium -p 4444:4444 -v $PWD/pro-example/node.json:/home/seleniumusr/selenium_node_config/node.json budtmo2/docker-android-pro:selenium\n```\n\n[<- BACK TO README](../README.md)\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "documentations/DOCKER-ANDROID-PRO.md", "language": "markdown", "complexity": "medium", "star_count": 11121, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 97}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\t```\n\texport USER=\"xxx\"\n\texport PASS=\"xxx\"\n\n\tdocker run -d -p 4723:4723 -v ${PWD}/example/genycloud/saas.json:/home/androidusr/genymotion_template/saas.json -e DEVICE_TYPE=geny_saas -e GENY_SAAS_USER=${USER} -e GENY_SAAS_PASS=${PASS} -e APPIUM=true --name android-container budtmo/docker-android:genymotion\n\t```\nGenymotion Cloud\n----------------\n\n![Genymotion](../images/logo_genymotion.png)\n\nYou can use Genymotion Android virtual devices in the cloud. They are available on [SaaS](http://bit.ly/2YP0P1l) or as virtual images on AWS, GCP or Alibaba Cloud.\n\n1. On SaaS <br />\n\tUse [saas.json](../example/genymotion/saas.json) to define the devices that you want to use. You can specify the port on which the device will start so you don't need to change the device name in your tests every time you need to run those tests. Then run following command\n\n\t```\n\texport AUTH_TOKEN=\"xxx\"\n\n\tdocker run -d -p 4723:4723 -v ${PWD}/example/genycloud/saas.json:/home/androidusr/genymotion_template/saas.json -e DEVICE_TYPE=geny_saas -e GENY_AUTH_TOKEN=${AUTH_TOKEN} -e APPIUM=true --name android-container budtmo/docker-android:genymotion\n\t```\n\n    Genymotion has deprecated credential based login since gmsaas 1.10.0, but if necessary, you can still provide them using `-e GENY_SAAS_USER=${USER} -e GENY_SAAS_PASS=${PASS}` instead of `-e GENY_AUTH_TOKEN=${AUTH_TOKEN}`.\n\n\tThe deployed device(s) are automatically connected with adb inside docker container. Stopping the emulator will remove all deployed device(s) on Genymotion SaaS and user will be logged out at the end.\n\n\t```\n\tdocker stop android-container\n\t```\n\n\n\tIn case you are interesed to play around with Genymotion on SaaS, you can register to [this link](http://bit.ly/2YP0P1l) to get free minutes for free.\n\n2. On AWS <br />\n\tUse [aws.json](../example/genymotion/aws.json) to define the devices that you want to use. You can specify the port on which the device will start so you don't need to change the device name in your tests every time you need to run those tests. Then run following command\n\n\t```\n\texport AWS_ACCESS_KEY_ID=\"xxx\"\n\texport AWS_SECRET_ACCESS_KEY=\"xxx\"\n\n\tdocker run -it --rm -p 4723:4723 -v ${PWD}/example/genycloud/aws.json:/home/androidusr/genymotion_template/aws.json -e DEVICE_TYPE=geny_aws -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} -e GENY_SAAS_PASS=${AWS_SECRET_ACCESS_KEY} -e APPIUM=true budtmo/docker-android:genymotion\n\t```\n\n\tThe deployed device(s) are automatically connected with adb inside docker container. Stopping the emulator will remove all deployed device(s) on Genymotion SaaS and user will be logged out at the end. As destroying all deployed ressources take time in AWS (it takes around 3 min), you need to specify the waiting time on docker stop\n\n\t```\n\tdocker stop --time=180 android-container\n\t```\n\n\n[<- BACK TO README](../README.md)\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "documentations/THIRD_PARTY_GENYMOTION.md", "language": "markdown", "complexity": "simple", "star_count": 11121, "quality_score": 0.8, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": false, "has_automation": true, "file_count": 1, "line_count": 52}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Docker-Android's Anonymous Aggregate User Behavior Analytics\nDocker-Android has begun gathering anonymous aggregate user behavior analytics and reporting these to Google Sheets through Google Forms API (Google Form ID: 1FAIpQLSdrKWQdMh6Nt8v8NQdYvTIntohebAgqWCpXT3T9NofAoxcpkw). You are notified about this when you start Docker-Android.\n\n## Why?\nWe don't have direct communication with its users nor time resources to ask directly for their feedback. As a result, we now use anonymous aggregate user behavior analytics to help us understand how Docker-Android is being used, the most common used features based on how, where and when people use it. With this information we can prioritize some features over other ones.\n\n## What?\nDocker-Android's Sheets record some shared information for every event:\n\n* Date and time when Docker-Android started\n* User (it will collect the information about Release Version of Machine), e.g. Linux-5.4.0-146-generic-x86_64-with-glibc2.29_#163-Ubuntu_SMP_Fri_Mar_17_18:26:02_UTC_2023. This does not allow us to track individual users but does enable us to accurately measure user counts\n* City (the information come from https://ipinfo.io)\n* Region (the information come from https://ipinfo.io)\n* Country (the information come from https://ipinfo.io)\n* Release version of Docker-Android\n* Appium (Whether user use Appium or not - The possible value will be \"true\" or \"false\")\n* Appium Additional Arguments\n* Web-Log (Whether user use Web-Log feature or not - The possible value will be \"true\" or \"false\")\n* Web-Vnc (Whether user use Web-Vnc feature or not - The possible value will be \"true\" or \"false\")\n* Screen-Resolution\n* Device Type (Which docker image is used - The possible value will be \"emulator\" or \"geny_cloud\" or \"geny_aws\")\n* Emulator Device (Which device profile and skin is used if the user use device_type \"emulator\")\n* Emulator Android Version (Which Android version is used if the user use device_type \"emulator\"\n* Emulator No-Skin feature (Whether user use no-skin feature or not - The possible value will be \"true\" or \"false\")\n* Emulator Data Partition\n* Emulator Additional Arguments\n\nWith the recorded information, it is not possible for us to match any particular real user.\n\n## When/Where?\nDocker-Android's user behavior analytics are sent throughout Docker-Android's execution to Google Sheets through Google Forms API over HTTPS.\n\n## Who?\nDocker-Android's analytics are accessible to Docker-Android's current maintainers. Contact [@budtmo](https://github.com/budtmo) if you are a maintainer and need access.\n\n## How?\nThe code is viewable in [these scripts](../cli/src/device).\n\n## Opting out before starting Docker-Android\nDocker-Android analytics helps us, maintainers and leaving it on is appreciated. However, if you want to opt out and not send any information, you can do this by using passing environment variable USER_BEHAVIOR_ANALYTICS=false to the Docker container.\n\n## Disclaimer\nThis document and the implementation are based on the great idea implemented by [Homebrew](https://github.com/Homebrew/brew/blob/master/docs/Analytics.md)\n\n\n[<- BACK TO LICENSE](../LICENSE.md)\n", "metadata": {"source": "github.com/budtmo/docker-android", "file_path": "documentations/USER_BEHAVIOR_ANALYTICS.md", "language": "markdown", "complexity": "simple", "star_count": 11121, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 47}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "### GNU GENERAL PUBLIC LICENSE\n\nVersion 3, 29 June 2007\n\nCopyright (C) 2007 Free Software Foundation, Inc.\n<https://fsf.org/>\n\nEveryone is permitted to copy and distribute verbatim copies of this\nlicense document, but changing it is not allowed.\n\n### Preamble\n\nThe GNU General Public License is a free, copyleft license for\nsoftware and other kinds of works.\n\nThe licenses for most software and other practical works are designed\nto take away your freedom to share and change the works. By contrast,\nthe GNU General Public License is intended to guarantee your freedom\nto share and change all versions of a program--to make sure it remains\nfree software for all its users. We, the Free Software Foundation, use\nthe GNU General Public License for most of our software; it applies\nalso to any other work released this way by its authors. You can apply\nit to your programs, too.\n\nWhen we speak of free software, we are referring to freedom, not\nprice. Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthem if you wish), that you receive source code or can get it if you\nwant it, that you can change the software or use pieces of it in new\nfree programs, and that you know you can do these things.\n\nTo protect your rights, we need to prevent others from denying you\nthese rights or asking you to surrender the rights. Therefore, you\nhave certain responsibilities if you distribute copies of the\nsoftware, or if you modify it: responsibilities to respect the freedom\nof others.\n\nFor example, if you distribute copies of such a program, whether\ngratis or for a fee, you must pass on to the recipients the same\nfreedoms that you received. You must make sure that they, too, receive\nor can get the source code. And you must show them these terms so they\nknow their rights.\n\nDevelopers that use the GNU GPL protect your rights with two steps:\n(1) assert copyright on the software, and (2) offer you this License\ngiving you legal permission to copy, distribute and/or modify it.\n\nFor the developers' and authors' protection, the GPL clearly explains\nthat there is no warranty for this free software. For both users' and\nauthors' sake, the GPL requires that modified versions be marked as\nchanged, so that their problems will not be attributed erroneously to\nauthors of previous versions.\n\nSome devices are designed to deny users access to install or run\nmodified versions of the software inside them, although the\nmanufacturer can do so. This is fundamentally incompatible with the\naim of protecting users' freedom to change the software. The\nsystematic pattern of such abuse occurs in the area of products for\nindividuals to use, which is precisely where it is most unacceptable.\nTherefore, we have designed this version of the GPL to prohibit the\npractice for those products. If such problems arise substantially in\nother domains, we stand ready to extend this provision to those\ndomains in future versions of the GPL, as needed to protect the\nfreedom of users.\n\nFinally, every program is threatened constantly by software patents.\nStates should not allow patents to restrict development and use of\nsoftware on general-purpose computers, but in those that do, we wish\nto avoid the special danger that patents applied to a free program\ncould make it effectively proprietary. To prevent this, the GPL\nassures that patents cannot be used to render the program non-free.\n\nThe precise terms and conditions for copying, distribution and\nmodification follow.\n\n### TERMS AND CONDITIONS\n\n#### 0. Definitions.\n\n\"This License\" refers to version 3 of the GNU General Public License.\n\n\"Copyright\" also means copyright-like laws that apply to other kinds\nof works, such as semiconductor masks.\n\n\"The Program\" refers to any copyrightable work licensed under this\nLicense. Each licensee is addressed as \"you\". \"Licensees\" and\n\"recipients\" may be individuals or organizations.\n\nTo \"modify\" a work means to copy from or adapt all or part of the work\nin a fashion requiring copyright permission, other than the making of\nan exact copy. The resulting work is called a \"modified version\" of\nthe earlier work or a work \"based on\" the earlier work.\n\nA \"covered work\" means either the unmodified Program or a work based\non the Program.\n\nTo \"propagate\" a work means to do anything with it that, without\npermission, would make you directly or secondarily liable for\ninfringement under applicable copyright law, except executing it on a\ncomputer or modifying a private copy. Propagation includes copying,\ndistribution (with or without modification), making available to the\npublic, and in some countries other activities as well.\n\nTo \"convey\" a work means any kind of propagation that enables other\nparties to make or receive copies. Mere interaction with a user\nthrough a computer network, with no transfer of a copy, is not\nconveying.\n\nAn interactive user interface displays \"Appropriate Legal Notices\" to\nthe extent that it includes a convenient and prominently visible\nfeature that (1) displays an appropriate copyright notice, and (2)\ntells the user that there is no warranty for the work (except to the\nextent that warranties are provided), that licensees may convey the\nwork under this License, and how to view a copy of this License. If\nthe interface presents a list of user commands or options, such as a\nmenu, a prominent item in the list meets this criterion.\n\n#### 1. Source Code.\n\nThe \"source code\" for a work means the preferred form of the work for\nmaking modifications to it. \"Object code\" means any non-source form of\na work.\n\nA \"Standard Interface\" means an interface that either is an official\nstandard defined by a recognized standards body, or, in the case of\ninterfaces specified for a particular programming language, one that\nis widely used among developers working in that language.\n\nThe \"System Libraries\" of an executable work include anything, other\nthan the work as a whole, that (a) is included in the normal form of\npackaging a Major Component, but which is not part of that Major\nComponent, and (b) serves only to enable use of the work with that\nMajor Component, or to implement a Standard Interface for which an\nimplementation is available to the public in source code form. A\n\"Major Component\", in this context, means a major essential component\n(kernel, window system, and so on) of the specific operating system\n(if any) on which the executable work runs, or a compiler used to\nproduce the work, or an object code interpreter used to run it.\n\nThe \"Corresponding Source\" for a work in object code form means all\nthe source code needed to generate, install, and (for an executable\nwork) run the object code and to modify the work, including scripts to\ncontrol those activities. However, it does not include the work's\nSystem Libraries, or general-purpose tools or generally available free\nprograms which are used unmodified in performing those activities but\nwhich are not part of the work. For example, Corresponding Source\nincludes interface definition files associated with source files for\nthe work, and the source code for shared libraries and dynamically\nlinked subprograms that the work is specifically designed to require,\nsuch as by intimate data communication or control flow between those\nsubprograms and other parts of the work.\n\nThe Corresponding Source need not include anything that users can\nregenerate automatically from other parts of the Corresponding Source.\n\nThe Corresponding Source for a work in source code form is that same\nwork.\n\n#### 2. Basic Permissions.\n\nAll rights granted under this License are granted for the term of\ncopyright on the Program, and are irrevocable provided the stated\nconditions are met. This License explicitly affirms your unlimited\npermission to run the unmodified Program. The output from running a\ncovered work is covered by this License only if the output, given its\ncontent, constitutes a covered work. This License acknowledges your\nrights of fair use or other equivalent, as provided by copyright law.\n\nYou may make, run and propagate covered works that you do not convey,\nwithout conditions so long as your license otherwise remains in force.\nYou may convey covered works to others for the sole purpose of having\nthem make modifications exclusively for you, or provide you with\nfacilities for running those works, provided that you comply with the\nterms of this License in conveying all material for which you do not\ncontrol copyright. Those thus making or running the covered works for\nyou must do so exclusively on your behalf, under your direction and\ncontrol, on terms that prohibit them from making any copies of your\ncopyrighted material outside their relationship with you.\n\nConveying under any other circumstances is permitted solely under the\nconditions stated below. Sublicensing is not allowed; section 10 makes\nit unnecessary.\n\n#### 3. Protecting Users' Legal Rights From Anti-Circumvention Law.\n\nNo covered work shall be deemed part of an effective technological\nmeasure under any applicable law fulfilling obligations under article\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\nsimilar laws prohibiting or restricting circumvention of such\nmeasures.\n\nWhen you convey a covered work, you waive any legal power to forbid\ncircumvention of technological measures to the extent such\ncircumvention is effected by exercising rights under this License with\nrespect to the covered work, and you disclaim any intention to limit\noperation or modification of the work as a means of enforcing, against\nthe work's users, your or third parties' legal rights to forbid\ncircumvention of technological measures.\n\n#### 4. Conveying Verbatim Copies.\n\nYou may convey verbatim copies of the Program's source code as you\nreceive it, in any medium, provided that you conspicuously and\nappropriately publish on each copy an appropriate copyright notice;\nkeep intact all notices stating that this License and any\nnon-permissive terms added in accord with section 7 apply to the code;\nkeep intact all notices of the absence of any warranty; and give all\nrecipients a copy of this License along with the Program.\n\nYou may charge any price or no price for each copy that you convey,\nand you may offer support or warranty protection for a fee.\n\n#### 5. Conveying Modified Source Versions.\n\nYou may convey a work based on the Program, or the modifications to\nproduce it from the Program, in the form of source code under the\nterms of section 4, provided that you also meet all of these\nconditions:\n\n-   a) The work must carry prominent notices stating that you modified\n    it, and giving a relevant date.\n-   b) The work must carry prominent notices stating that it is\n    released under this License and any conditions added under\n    section 7. This requirement modifies the requirement in section 4\n    to \"keep intact all notices\".\n-   c) You must license the entire work, as a whole, under this\n    License to anyone who comes into possession of a copy. This\n    License will therefore apply, along with any applicable section 7\n    additional terms, to the whole of the work, and all its parts,\n    regardless of how they are packaged. This License gives no\n    permission to license the work in any other way, but it does not\n    invalidate such permission if you have separately received it.\n-   d) If the work has interactive user interfaces, each must display\n    Appropriate Legal Notices; however, if the Program has interactive\n    interfaces that do not display Appropriate Legal Notices, your\n    work need not make them do so.\n\nA compilation of a covered work with other separate and independent\nworks, which are not by their nature extensions of the covered work,\nand which are not combined with it such as to form a larger program,\nin or on a volume of a storage or distribution medium, is called an\n\"aggregate\" if the compilation and its resulting copyright are not\nused to limit the access or legal rights of the compilation's users\nbeyond what the individual works permit. Inclusion of a covered work\nin an aggregate does not cause this License to apply to the other\nparts of the aggregate.\n\n#### 6. Conveying Non-Source Forms.\n\nYou may convey a covered work in object code form under the terms of\nsections 4 and 5, provided that you also convey the machine-readable\nCorresponding Source under the terms of this License, in one of these\nways:\n\n-   a) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by the\n    Corresponding Source fixed on a durable physical medium\n    customarily used for software interchange.\n-   b) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by a\n    written offer, valid for at least three years and valid for as\n    long as you offer spare parts or customer support for that product\n    model, to give anyone who possesses the object code either (1) a\n    copy of the Corresponding Source for all the software in the\n    product that is covered by this License, on a durable physical\n    medium customarily used for software interchange, for a price no\n    more than your reasonable cost of physically performing this\n    conveying of source, or (2) access to copy the Corresponding\n    Source from a network server at no charge.\n-   c) Convey individual copies of the object code with a copy of the\n    written offer to provide the Corresponding Source. This\n    alternative is allowed only occasionally and noncommercially, and\n    only if you received the object code with such an offer, in accord\n    with subsection 6b.\n-   d) Convey the object code by offering access from a designated\n    place (gratis or for a charge), and offer equivalent access to the\n    Corresponding Source in the same way through the same place at no\n    further charge. You need not require recipients to copy the\n    Corresponding Source along with the object code. If the place to\n    copy the object code is a network server, the Corresponding Source\n    may be on a different server (operated by you or a third party)\n    that supports equivalent copying facilities, provided you maintain\n    clear directions next to the object code saying where to find the\n    Corresponding Source. Regardless of what server hosts the\n    Corresponding Source, you remain obligated to ensure that it is\n    available for as long as needed to satisfy these requirements.\n-   e) Convey the object code using peer-to-peer transmission,\n    provided you inform other peers where the object code and\n    Corresponding Source of the work are being offered to the general\n    public at no charge under subsection 6d.\n\nA separable portion of the object code, whose source code is excluded\nfrom the Corresponding Source as a System Library, need not be\nincluded in conveying the object code work.\n\nA \"User Product\" is either (1) a \"consumer product\", which means any\ntangible personal property which is normally used for personal,\nfamily, or household purposes, or (2) anything designed or sold for\nincorporation into a dwelling. In determining whether a product is a\nconsumer product, doubtful cases shall be resolved in favor of\ncoverage. For a particular product received by a particular user,\n\"normally used\" refers to a typical or common use of that class of\nproduct, regardless of the status of the particular user or of the way\nin which the particular user actually uses, or expects or is expected\nto use, the product. A product is a consumer product regardless of\nwhether the product has substantial commercial, industrial or\nnon-consumer uses, unless such uses represent the only significant\nmode of use of the product.\n\n\"Installation Information\" for a User Product means any methods,\nprocedures, authorization keys, or other information required to\ninstall and execute modified versions of a covered work in that User\nProduct from a modified version of its Corresponding Source. The\ninformation must suffice to ensure that the continued functioning of\nthe modified object code is in no case prevented or interfered with\nsolely because modification has been made.\n\nIf you convey an object code work under this section in, or with, or\nspecifically for use in, a User Product, and the conveying occurs as\npart of a transaction in which the right of possession and use of the\nUser Product is transferred to the recipient in perpetuity or for a\nfixed term (regardless of how the transaction is characterized), the\nCorresponding Source conveyed under this section must be accompanied\nby the Installation Information. But this requirement does not apply\nif neither you nor any third party retains the ability to install\nmodified object code on the User Product (for example, the work has\nbeen installed in ROM).\n\nThe requirement to provide Installation Information does not include a\nrequirement to continue to provide support service, warranty, or\nupdates for a work that has been modified or installed by the\nrecipient, or for the User Product in which it has been modified or\ninstalled. Access to a network may be denied when the modification\nitself materially and adversely affects the operation of the network\nor violates the rules and protocols for communication across the\nnetwork.\n\nCorresponding Source conveyed, and Installation Information provided,\nin accord with this section must be in a format that is publicly\ndocumented (and with an implementation available to the public in\nsource code form), and must require no special password or key for\nunpacking, reading or copying.\n\n#### 7. Additional Terms.\n\n\"Additional permissions\" are terms that supplement the terms of this\nLicense by making exceptions from one or more of its conditions.\nAdditional permissions that are applicable to the entire Program shall\nbe treated as though they were included in this License, to the extent\nthat they are valid under applicable law. If additional permissions\napply only to part of the Program, that part may be used separately\nunder those permissions, but the entire Program remains governed by\nthis License without regard to the additional permissions.\n\nWhen you convey a copy of a covered work, you may at your option\nremove any additional permissions from that copy, or from any part of\nit. (Additional permissions may be written to require their own\nremoval in certain cases when you modify the work.) You may place\nadditional permissions on material, added by you to a covered work,\nfor which you have or can give appropriate copyright permission.\n\nNotwithstanding any other provision of this License, for material you\nadd to a covered work, you may (if authorized by the copyright holders\nof that material) supplement the terms of this License with terms:\n\n-   a) Disclaiming warranty or limiting liability differently from the\n    terms of sections 15 and 16 of this License; or\n-   b) Requiring preservation of specified reasonable legal notices or\n    author attributions in that material or in the Appropriate Legal\n    Notices displayed by works containing it; or\n-   c) Prohibiting misrepresentation of the origin of that material,\n    or requiring that modified versions of such material be marked in\n    reasonable ways as different from the original version; or\n-   d) Limiting the use for publicity purposes of names of licensors\n    or authors of the material; or\n-   e) Declining to grant rights under trademark law for use of some\n    trade names, trademarks, or service marks; or\n-   f) Requiring indemnification of licensors and authors of that\n    material by anyone who conveys the material (or modified versions\n    of it) with contractual assumptions of liability to the recipient,\n    for any liability that these contractual assumptions directly\n    impose on those licensors and authors.\n\nAll other non-permissive additional terms are considered \"further\nrestrictions\" within the meaning of section 10. If the Program as you\nreceived it, or any part of it, contains a notice stating that it is\ngoverned by this License along with a term that is a further\nrestriction, you may remove that term. If a license document contains\na further restriction but permits relicensing or conveying under this\nLicense, you may add to a covered work material governed by the terms\nof that license document, provided that the further restriction does\nnot survive such relicensing or conveying.\n\nIf you add terms to a covered work in accord with this section, you\nmust place, in the relevant source files, a statement of the\nadditional terms that apply to those files, or a notice indicating\nwhere to find the applicable terms.\n\nAdditional terms, permissive or non-permissive, may be stated in the\nform of a separately written license, or stated as exceptions; the\nabove requirements apply either way.\n\n#### 8. Termination.\n\nYou may not propagate or modify a covered work except as expressly\nprovided under this License. Any attempt otherwise to propagate or\nmodify it is void, and will automatically terminate your rights under\nthis License (including any patent licenses granted under the third\nparagraph of section 11).\n\nHowever, if you cease all violation of this License, then your license\nfrom a particular copyright holder is reinstated (a) provisionally,\nunless and until the copyright holder explicitly and finally\nterminates your license, and (b) permanently, if the copyright holder\nfails to notify you of the violation by some reasonable means prior to\n60 days after the cessation.\n\nMoreover, your license from a particular copyright holder is\nreinstated permanently if the copyright holder notifies you of the\nviolation by some reasonable means, this is the first time you have\nreceived notice of violation of this License (for any work) from that\ncopyright holder, and you cure the violation prior to 30 days after\nyour receipt of the notice.\n\nTermination of your rights under this section does not terminate the\nlicenses of parties who have received copies or rights from you under\nthis License. If your rights have been terminated and not permanently\nreinstated, you do not qualify to receive new licenses for the same\nmaterial under section 10.\n\n#### 9. Acceptance Not Required for Having Copies.\n\nYou are not required to accept this License in order to receive or run\na copy of the Program. Ancillary propagation of a covered work\noccurring solely as a consequence of using peer-to-peer transmission\nto receive a copy likewise does not require acceptance. However,\nnothing other than this License grants you permission to propagate or\nmodify any covered work. These actions infringe copyright if you do\nnot accept this License. Therefore, by modifying or propagating a\ncovered work, you indicate your acceptance of this License to do so.\n\n#### 10. Automatic Licensing of Downstream Recipients.\n\nEach time you convey a covered work, the recipient automatically\nreceives a license from the original licensors, to run, modify and\npropagate that work, subject to this License. You are not responsible\nfor enforcing compliance by third parties with this License.\n\nAn \"entity transaction\" is a transaction transferring control of an\norganization, or substantially all assets of one, or subdividing an\norganization, or merging organizations. If propagation of a covered\nwork results from an entity transaction, each party to that\ntransaction who receives a copy of the work also receives whatever\nlicenses to the work the party's predecessor in interest had or could\ngive under the previous paragraph, plus a right to possession of the\nCorresponding Source of the work from the predecessor in interest, if\nthe predecessor has it or can get it with reasonable efforts.\n\nYou may not impose any further restrictions on the exercise of the\nrights granted or affirmed under this License. For example, you may\nnot impose a license fee, royalty, or other charge for exercise of\nrights granted under this License, and you may not initiate litigation\n(including a cross-claim or counterclaim in a lawsuit) alleging that\nany patent claim is infringed by making, using, selling, offering for\nsale, or importing the Program or any portion of it.\n\n#### 11. Patents.\n\nA \"contributor\" is a copyright holder who authorizes use under this\nLicense of the Program or a work on which the Program is based. The\nwork thus licensed is called the contributor's \"contributor version\".\n\nA contributor's \"essential patent claims\" are all patent claims owned\nor controlled by the contributor, whether already acquired or\nhereafter acquired, that would be infringed by some manner, permitted\nby this License, of making, using, or selling its contributor version,\nbut do not include claims that would be infringed only as a\nconsequence of further modification of the contributor version. For\npurposes of this definition, \"control\" includes the right to grant\npatent sublicenses in a manner consistent with the requirements of\nthis License.\n\nEach contributor grants you a non-exclusive, worldwide, royalty-free\npatent license under the contributor's essential patent claims, to\nmake, use, sell, offer for sale, import and otherwise run, modify and\npropagate the contents of its contributor version.\n\nIn the following three paragraphs, a \"patent license\" is any express\nagreement or commitment, however denominated, not to enforce a patent\n(such as an express permission to practice a patent or covenant not to\nsue for patent infringement). To \"grant\" such a patent license to a\nparty means to make such an agreement or commitment not to enforce a\npatent against the party.\n\nIf you convey a covered work, knowingly relying on a patent license,\nand the Corresponding Source of the work is not available for anyone\nto copy, free of charge and under the terms of this License, through a\npublicly available network server or other readily accessible means,\nthen you must either (1) cause the Corresponding Source to be so\navailable, or (2) arrange to deprive yourself of the benefit of the\npatent license for this particular work, or (3) arrange, in a manner\nconsistent with the requirements of this License, to extend the patent\nlicense to downstream recipients. \"Knowingly relying\" means you have\nactual knowledge that, but for the patent license, your conveying the\ncovered work in a country, or your recipient's use of the covered work\nin a country, would infringe one or more identifiable patents in that\ncountry that you have reason to believe are valid.\n\nIf, pursuant to or in connection with a single transaction or\narrangement, you convey, or propagate by procuring conveyance of, a\ncovered work, and grant a patent license to some of the parties\nreceiving the covered work authorizing them to use, propagate, modify\nor convey a specific copy of the covered work, then the patent license\nyou grant is automatically extended to all recipients of the covered\nwork and works based on it.\n\nA patent license is \"discriminatory\" if it does not include within the\nscope of its coverage, prohibits the exercise of, or is conditioned on\nthe non-exercise of one or more of the rights that are specifically\ngranted under this License. You may not convey a covered work if you\nare a party to an arrangement with a third party that is in the\nbusiness of distributing software, under which you make payment to the\nthird party based on the extent of your activity of conveying the\nwork, and under which the third party grants, to any of the parties\nwho would receive the covered work from you, a discriminatory patent\nlicense (a) in connection with copies of the covered work conveyed by\nyou (or copies made from those copies), or (b) primarily for and in\nconnection with specific products or compilations that contain the\ncovered work, unless you entered into that arrangement, or that patent\nlicense was granted, prior to 28 March 2007.\n\nNothing in this License shall be construed as excluding or limiting\nany implied license or other defenses to infringement that may\notherwise be available to you under applicable patent law.\n\n#### 12. No Surrender of Others' Freedom.\n\nIf conditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License. If you cannot convey a\ncovered work so as to satisfy simultaneously your obligations under\nthis License and any other pertinent obligations, then as a\nconsequence you may not convey it at all. For example, if you agree to\nterms that obligate you to collect a royalty for further conveying\nfrom those to whom you convey the Program, the only way you could\nsatisfy both those terms and this License would be to refrain entirely\nfrom conveying the Program.\n\n#### 13. Use with the GNU Affero General Public License.\n\nNotwithstanding any other provision of this License, you have\npermission to link or combine any covered work with a work licensed\nunder version 3 of the GNU Affero General Public License into a single\ncombined work, and to convey the resulting work. The terms of this\nLicense will continue to apply to the part which is the covered work,\nbut the special requirements of the GNU Affero General Public License,\nsection 13, concerning interaction through a network will apply to the\ncombination as such.\n\n#### 14. Revised Versions of this License.\n\nThe Free Software Foundation may publish revised and/or new versions\nof the GNU General Public License from time to time. Such new versions\nwill be similar in spirit to the present version, but may differ in\ndetail to address new problems or concerns.\n\nEach version is given a distinguishing version number. If the Program\nspecifies that a certain numbered version of the GNU General Public\nLicense \"or any later version\" applies to it, you have the option of\nfollowing the terms and conditions either of that numbered version or\nof any later version published by the Free Software Foundation. If the\nProgram does not specify a version number of the GNU General Public\nLicense, you may choose any version ever published by the Free\nSoftware Foundation.\n\nIf the Program specifies that a proxy can decide which future versions\nof the GNU General Public License can be used, that proxy's public\nstatement of acceptance of a version permanently authorizes you to\nchoose that version for the Program.\n\nLater license versions may give you additional or different\npermissions. However, no additional obligations are imposed on any\nauthor or copyright holder as a result of your choosing to follow a\nlater version.\n\n#### 15. Disclaimer of Warranty.\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\nAPPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT\nWARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND\nPERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE\nDEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR\nCORRECTION.\n\n#### 16. Limitation of Liability.\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR\nCONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES,\nINCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES\nARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT\nNOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR\nLOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM\nTO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER\nPARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n#### 17. Interpretation of Sections 15 and 16.\n\nIf the disclaimer of warranty and limitation of liability provided\nabove cannot be given local legal effect according to their terms,\nreviewing courts shall apply local law that most closely approximates\nan absolute waiver of all civil liability in connection with the\nProgram, unless a warranty or assumption of liability accompanies a\ncopy of the Program in return for a fee.\n\nEND OF TERMS AND CONDITIONS\n\n### How to Apply These Terms to Your New Programs\n\nIf you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these\nterms.\n\nTo do so, attach the following notices to the program. It is safest to\nattach them to the start of each source file to most effectively state\nthe exclusion of warranty; and each file should have at least the\n\"copyright\" line and a pointer to where the full notice is found.\n\n        <one line to give the program's name and a brief idea of what it does.>\n        Copyright (C) <year>  <name of author>\n\n        This program is free software: you can redistribute it and/or modify\n        it under the terms of the GNU General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or\n        (at your option) any later version.\n\n        This program is distributed in the hope that it will be useful,\n        but WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n        GNU General Public License for more details.\n\n        You should have received a copy of the GNU General Public License\n        along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\nAlso add information on how to contact you by electronic and paper\nmail.\n\nIf the program does terminal interaction, make it output a short\nnotice like this when it starts in an interactive mode:\n\n        <program>  Copyright (C) <year>  <name of author>\n        This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n        This is free software, and you are welcome to redistribute it\n        under certain conditions; type `show c' for details.\n\nThe hypothetical commands \\`show w' and \\`show c' should show the\nappropriate parts of the General Public License. Of course, your\nprogram's commands might be different; for a GUI interface, you would\nuse an \"about box\".\n\nYou should also get your employer (if you work as a programmer) or\nschool, if any, to sign a \"copyright disclaimer\" for the program, if\nnecessary. For more information on this, and how to apply and follow\nthe GNU GPL, see <https://www.gnu.org/licenses/>.\n\nThe GNU General Public License does not permit incorporating your\nprogram into proprietary programs. If your program is a subroutine\nlibrary, you may consider it more useful to permit linking proprietary\napplications with the library. If this is what you want to do, use the\nGNU Lesser General Public License instead of this License. But first,\nplease read <https://www.gnu.org/licenses/why-not-lgpl.html>.\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "LICENSE.md", "language": "markdown", "complexity": "complex", "star_count": 8735, "quality_score": 0.82, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 676}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Khue's Homelab\n\n**[Features](#features) ‚Ä¢ [Get Started](#get-started) ‚Ä¢ [Documentation](https://homelab.khuedoan.com)**\n\n[![tag](https://img.shields.io/github/v/tag/khuedoan/homelab?style=flat-square&logo=semver&logoColor=white)](https://github.com/khuedoan/homelab/tags)\n[![document](https://img.shields.io/website?label=document&logo=gitbook&logoColor=white&style=flat-square&url=https%3A%2F%2Fhomelab.khuedoan.com)](https://homelab.khuedoan.com)\n[![license](https://img.shields.io/github/license/khuedoan/homelab?style=flat-square&logo=gnu&logoColor=white)](https://www.gnu.org/licenses/gpl-3.0.html)\n[![stars](https://img.shields.io/github/stars/khuedoan/homelab?logo=github&logoColor=white&color=gold&style=flat-square)](https://github.com/khuedoan/homelab)\n\nThis project utilizes [Infrastructure as Code](https://en.wikipedia.org/wiki/Infrastructure_as_code) and [GitOps](https://www.weave.works/technologies/gitops) to automate provisioning, operating, and updating self-hosted services in my homelab.\nIt can be used as a highly customizable framework to build your own homelab.\n\n> **What is a homelab?**\n>\n> Homelab is a laboratory at home where you can self-host, experiment with new technologies, practice for certifications, and so on.\n> For more information, please see the [r/homelab introduction](https://www.reddit.com/r/homelab/wiki/introduction) and the\n> [Home Operations Discord community](https://discord.gg/home-operations) (formerly known as [k8s-at-home](https://k8s-at-home.com)).\n\nIf you encounter an issue, please create [a bug report](https://github.com/khuedoan/homelab/issues/new?template=bug_report.md)\n(avoid asking for support about issues specific to this project in other communication channels).\n\n## Overview\n\nProject status: **ALPHA**\n\nThis project is still in the experimental stage, and I don't use anything critical on it.\nExpect breaking changes that may require a complete redeployment.\nA proper upgrade path is planned for the stable release.\nMore information can be found in [the roadmap](#roadmap) below.\n\n### Hardware\n\n![Hardware](https://user-images.githubusercontent.com/27996771/98970963-25137200-2543-11eb-8f2d-f9a2d45756ef.JPG)\n\n- 4 √ó NEC SFF `PC-MK26ECZDR` (Japanese version of the ThinkCentre M700):\n    - CPU: `Intel Core i5-6600T @ 2.70GHz`\n    - RAM: `16GB`\n    - SSD: `128GB`\n- TP-Link `TL-SG108` switch:\n    - Ports: `8`\n    - Speed: `1000Mbps`\n\n### Features\n\n- [x] Common applications: Gitea, Jellyfin, Paperless...\n- [x] Automated bare metal provisioning with PXE boot\n- [x] Automated Kubernetes installation and management\n- [x] Installing and managing applications using GitOps\n- [x] Automatic rolling upgrade for OS and Kubernetes\n- [x] Automatically update apps (with approval)\n- [x] Modular architecture, easy to add or remove features/components\n- [x] Automated certificate management\n- [x] Automatically update DNS records for exposed services\n- [x] VPN (Tailscale or Wireguard)\n- [x] Expose services to the internet securely with [Cloudflare Tunnel](https://www.cloudflare.com/products/tunnel/)\n- [x] CI/CD platform\n- [x] Private container registry\n- [x] Distributed storage\n- [x] Support multiple environments (dev, prod)\n- [x] Monitoring and alerting\n- [x] Automated backup and restore\n- [x] Single sign-on\n- [x] Infrastructure testing\n\nSome demo videos and screenshots are shown here.\nThey can't capture all the project's features, but they are sufficient to get a concept of it.\n\n| Demo                                                                                                            |\n| :--:                                                                                                            |\n| [![][deploy-demo]](https://asciinema.org/a/xkBRkwC6e9RAzVuMDXH3nGHp7)                                           |\n| Deploy with a single command (after updating the configuration files)                                           |\n| [![][pxe-demo]](https://www.youtube.com/watch?v=y-d7btNNAT8)                                                    |\n| PXE boot                                                                                                        |\n| [![][hubble-demo]][hubble-demo]                                                                                 |\n| Observe network traffic with Hubble, built on top of [Cilium](https://cilium.io) and eBPF                       |\n| [![][homepage-demo]][homepage-demo]                                                                             |\n| Homepage powered by... [Homepage](https://gethomepage.dev)                                                      |\n| [![][grafana-demo]][grafana-demo]                                                                               |\n| Monitoring dashboard powered by [Grafana](https://grafana.com)                                                  |\n| [![][gitea-demo]][gitea-demo]                                                                                   |\n| Git server powered by [Gitea](https://gitea.io/en-us)                                                           |\n| [![][matrix-demo]][matrix-demo]                                                                                 |\n| [Matrix](https://matrix.org/) chat server                                                                       |\n| [![][woodpecker-demo]][woodpecker-demo]                                                                         |\n| Continuous integration with [Woodpecker CI](https://woodpecker-ci.org)                                          |\n| [![][argocd-demo]][argocd-demo]                                                                                 |\n| Continuous deployment with [ArgoCD](https://argoproj.github.io/cd)                                              |\n| [![][alert-demo]][alert-demo]                                                                                   |\n| [ntfy](https://ntfy.sh) displaying received alerts                                                              |\n| [![][ai-demo]][ai-demo]                                                                                         |\n| Self-hosted AI powered by [Ollama](https://ollama.com) (experimental, not very fast because I don't have a GPU) |\n\n[deploy-demo]: https://asciinema.org/a/xkBRkwC6e9RAzVuMDXH3nGHp7.svg\n[pxe-demo]: https://user-images.githubusercontent.com/27996771/157303477-df2e7410-8f02-4648-a86c-71e6b7e89e35.png\n[hubble-demo]: https://github.com/khuedoan/homelab/assets/27996771/9c6677d0-3564-47c0-852b-24b6a554b4a3\n[homepage-demo]: https://github.com/khuedoan/homelab/assets/27996771/d0eaf620-be08-48d8-8420-40bcaa86093b\n[grafana-demo]: https://github.com/khuedoan/homelab/assets/27996771/ad937b26-e9bc-4761-83ae-1c7f512ea97f\n[gitea-demo]: https://github.com/khuedoan/homelab/assets/27996771/c245534f-88d9-4565-bde8-b39f60ccee9e\n[matrix-demo]: https://user-images.githubusercontent.com/27996771/149448510-7163310c-2049-4ccd-901d-f11f605bfc32.png\n[woodpecker-demo]: https://github.com/khuedoan/homelab/assets/27996771/5d887688-d20a-44c8-8f77-0c625527dfe4\n[argocd-demo]: https://github.com/khuedoan/homelab/assets/27996771/527e2529-4fe1-4664-ab8a-b9eb3c492d20\n[alert-demo]: https://github.com/khuedoan/homelab/assets/27996771/c922f755-e911-4ca0-9d4a-6e552d387f18\n[ai-demo]: https://github.com/khuedoan/homelab/assets/27996771/d77ba511-00b7-47c3-9032-55679a099e70\n\n### Tech stack\n\n<table>\n    <tr>\n        <th>Logo</th>\n        <th>Name</th>\n        <th>Description</th>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://simpleicons.org/icons/ansible.svg\"></td>\n        <td><a href=\"https://www.ansible.com\">Ansible</a></td>\n        <td>Automate bare metal provisioning and configuration</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://avatars.githubusercontent.com/u/30269780\"></td>\n        <td><a href=\"https://argoproj.github.io/cd\">ArgoCD</a></td>\n        <td>GitOps tool built to deploy applications to Kubernetes</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://github.com/jetstack/cert-manager/raw/master/logo/logo.png\"></td>\n        <td><a href=\"https://cert-manager.io\">cert-manager</a></td>\n        <td>Cloud native certificate management</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://avatars.githubusercontent.com/u/21054566?s=200&v=4\"></td>\n        <td><a href=\"https://cilium.io\">Cilium</a></td>\n        <td>eBPF-based Networking, Observability and Security (CNI, LB, Network Policy, etc.)</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://avatars.githubusercontent.com/u/314135?s=200&v=4\"></td>\n        <td><a href=\"https://www.cloudflare.com\">Cloudflare</a></td>\n        <td>DNS and Tunnel</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://www.docker.com/wp-content/uploads/2022/03/Moby-logo.png\"></td>\n        <td><a href=\"https://www.docker.com\">Docker</a></td>\n        <td>Ephemeral PXE server</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://github.com/kubernetes-sigs/external-dns/raw/master/docs/img/external-dns.png\"></td>\n        <td><a href=\"https://github.com/kubernetes-sigs/external-dns\">ExternalDNS</a></td>\n        <td>Synchronizes exposed Kubernetes Services and Ingresses with DNS providers</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Fedora_logo.svg/267px-Fedora_logo.svg.png\"></td>\n        <td><a href=\"https://getfedora.org/en/server\">Fedora Server</a></td>\n        <td>Base OS for Kubernetes nodes</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://upload.wikimedia.org/wikipedia/commons/b/bb/Gitea_Logo.svg\"></td>\n        <td><a href=\"https://gitea.com\">Gitea</a></td>\n        <td>Self-hosted Git service</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://grafana.com/static/img/menu/grafana2.svg\"></td>\n        <td><a href=\"https://grafana.com\">Grafana</a></td>\n        <td>Observability platform</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://helm.sh/img/helm.svg\"></td>\n        <td><a href=\"https://helm.sh\">Helm</a></td>\n        <td>The package manager for Kubernetes</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://avatars.githubusercontent.com/u/49319725\"></td>\n        <td><a href=\"https://k3s.io\">K3s</a></td>\n        <td>Lightweight distribution of Kubernetes</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://kanidm.com/images/logo.svg\"></td>\n        <td><a href=\"https://kanidm.com\">Kanidm</a></td>\n        <td>Modern and simple identity management platform</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://avatars.githubusercontent.com/u/13629408\"></td>\n        <td><a href=\"https://kubernetes.io\">Kubernetes</a></td>\n        <td>Container-orchestration system, the backbone of this project</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://github.com/grafana/loki/blob/main/docs/sources/logo.png?raw=true\"></td>\n        <td><a href=\"https://grafana.com/oss/loki\">Loki</a></td>\n        <td>Log aggregation system</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://avatars.githubusercontent.com/u/1412239?s=200&v=4\"></td>\n        <td><a href=\"https://www.nginx.com\">NGINX</a></td>\n        <td>Kubernetes Ingress Controller</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://raw.githubusercontent.com/NixOS/nixos-artwork/refs/heads/master/logo/nix-snowflake-colours.svg\"></td>\n        <td><a href=\"https://nixos.org\">Nix</a></td>\n        <td>Convenient development shell</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://ntfy.sh/_next/static/media/logo.077f6a13.svg\"></td>\n        <td><a href=\"https://ntfy.sh\">ntfy</a></td>\n        <td>Notification service to send notifications to your phone or desktop</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://avatars.githubusercontent.com/u/3380462\"></td>\n        <td><a href=\"https://prometheus.io\">Prometheus</a></td>\n        <td>Systems monitoring and alerting toolkit</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://docs.renovatebot.com/assets/images/logo.png\"></td>\n        <td><a href=\"https://www.whitesourcesoftware.com/free-developer-tools/renovate\">Renovate</a></td>\n        <td>Automatically update dependencies</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://raw.githubusercontent.com/rook/artwork/master/logo/blue.svg\"></td>\n        <td><a href=\"https://rook.io\">Rook Ceph</a></td>\n        <td>Cloud-Native Storage for Kubernetes</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://avatars.githubusercontent.com/u/48932923?s=200&v=4\"></td>\n        <td><a href=\"https://tailscale.com\">Tailscale</a></td>\n        <td>VPN without port forwarding</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://avatars.githubusercontent.com/u/13991055?s=200&v=4\"></td>\n        <td><a href=\"https://www.wireguard.com\">Wireguard</a></td>\n        <td>Fast, modern, secure VPN tunnel</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://avatars.githubusercontent.com/u/84780935?s=200&v=4\"></td>\n        <td><a href=\"https://woodpecker-ci.org\">Woodpecker CI</a></td>\n        <td>Simple yet powerful CI/CD engine with great extensibility</td>\n    </tr>\n    <tr>\n        <td><img width=\"32\" src=\"https://zotregistry.dev/v2.0.2/assets/images/logo.svg\"></td>\n        <td><a href=\"https://zotregistry.dev\">Zot Registry</a></td>\n        <td>Private container registry</td>\n    </tr>\n</table>\n\n## Get Started\n\n- [Try it out locally](https://homelab.khuedoan.com/installation/sandbox) without any hardware (just 4 commands!)\n- [Deploy on real hardware](https://homelab.khuedoan.com/installation/production/prerequisites) for production workload\n\n## Roadmap\n\nSee [roadmap](https://homelab.khuedoan.com/reference/roadmap) and [open issues](https://github.com/khuedoan/homelab/issues) for a list of proposed features and known issues.\n\n## Contributing\n\nAny contributions you make are greatly appreciated.\n\nPlease see [contributing guide](https://homelab.khuedoan.com/reference/contributing) for more information.\n\n## License\n\nCopyright &copy; 2020 - 2024 Khue Doan\n\nDistributed under the GPLv3 License.\nSee [license page](https://homelab.khuedoan.com/reference/license) or `LICENSE.md` file for more information.\n\n## Acknowledgements\n\nReferences:\n\n- [Ephemeral PXE server inspired by Minimal First Machine in the DC](https://speakerdeck.com/amcguign/minimal-first-machine-in-the-dc)\n- [ArgoCD usage and monitoring configuration in locmai/humble](https://github.com/locmai/humble)\n- [README template](https://github.com/othneildrew/Best-README-Template)\n- [Run the same Cloudflare Tunnel across many `cloudflared` processes](https://developers.cloudflare.com/cloudflare-one/tutorials/many-cfd-one-tunnel)\n- [MAC address environment variable in GRUB config](https://askubuntu.com/questions/1272400/how-do-i-automate-network-installation-of-many-ubuntu-18-04-systems-with-efi-and)\n- [Official k3s systemd service file](https://github.com/k3s-io/k3s/blob/master/k3s.service)\n- [Official Cloudflare Tunnel examples](https://github.com/cloudflare/argo-tunnel-examples)\n- [Initialize GitOps repository on Gitea and integrate with Tekton by RedHat](https://github.com/redhat-scholars/tekton-tutorial/tree/master/triggers)\n- [SSO configuration from xUnholy/k8s-gitops](https://github.com/xUnholy/k8s-gitops)\n- [Pre-commit config from k8s-at-home/flux-cluster-template](https://github.com/k8s-at-home/flux-cluster-template)\n- [Di√°taxis technical documentation framework](https://diataxis.fr)\n- [Official Terratest examples](https://github.com/gruntwork-io/terratest/tree/master/test)\n- [Self-host an automated Jellyfin media streaming stack](https://zerodya.net/self-host-jellyfin-media-streaming-stack)\n- [App Template Helm chart by bjw-s](https://bjw-s-labs.github.io/helm-charts/docs/app-template)\n- [Various application configurations in onedr0p/home-ops](https://github.com/onedr0p/home-ops)\n\nHere is a list of the contributors who have helped to improve this project.\nBig shout-out to them!\n\n- ![](https://github.com/locmai.png?size=24) [@locmai](https://github.com/locmai)\n- ![](https://github.com/MatthewJohn.png?size=24) [@MatthewJohn](https://github.com/MatthewJohn)\n- ![](https://github.com/karpfediem.png?size=24) [@karpfediem](https://github.com/karpfediem)\n- ![](https://github.com/linhng98.png?size=24) [@linhng98](https://github.com/linhng98)\n- ![](https://github.com/elliotblackburn.png?size=24) [@elliotblackburn](https://github.com/elliotblackburn)\n- ![](https://github.com/dotdiego.png?size=24) [@dotdiego](https://github.com/dotdiego)\n- ![](https://github.com/Crimrose.png?size=24) [@Crimrose](https://github.com/Crimrose)\n- ![](https://github.com/eventi.png?size=24) [@eventi](https://github.com/eventi)\n- ![](https://github.com/Bourne-ID.png?size=24) [@Bourne-ID](https://github.com/Bourne-ID)\n- ![](https://github.com/akwan.png?size=24) [@akwan](https://github.com/akwan)\n- ![](https://github.com/trangmaiq.png?size=24) [@trangmaiq](https://github.com/trangmaiq)\n- ![](https://github.com/tangowithfoxtrot.png?size=24) [@tangowithfoxtrot](https://github.com/tangowithfoxtrot)\n- ![](https://github.com/raedkit.png?size=24) [@raedkit](https://github.com/raedkit)\n- ![](https://github.com/ClashTheBunny.png?size=24) [@ClashTheBunny](https://github.com/ClashTheBunny)\n- ![](https://github.com/retX0.png?size=24) [@retX0](https://github.com/retX0)\n- ![](https://github.com/zalsader.png?size=24) [@zalsader](https://github.com/zalsader)\n- ![](https://github.com/serpro69.png?size=24) [@serpro69](https://github.com/serpro69)\n\nIf you feel you're missing from this list, please feel free to add yourself in a PR.\n\n## Stargazers over time\n\n[![Stargazers over time](https://starchart.cc/khuedoan/homelab.svg)](https://starchart.cc/khuedoan/homelab)\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 8735, "quality_score": 0.76, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": true, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 308}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using Terraform infrastructure as code\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "app-template:\n  controllers:\n    main:\n      containers:\n        main:\n          image:\n            repository: ghcr.io/gethomepage/homepage\n            tag: v0.8.8\n  service:\n    main:\n      ports:\n        http:\n          port: 3000\n          protocol: HTTP\n  ingress:\n    main:\n      enabled: true\n      className: nginx\n      annotations:\n        cert-manager.io/cluster-issuer: letsencrypt-prod\n      hosts:\n        - host: &host home.khuedoan.com\n          paths:\n            - path: /\n              pathType: Prefix\n              service:\n                name: main\n                port: http\n      tls:\n        - hosts:\n            - *host\n          secretName: homepage-tls-certificate\n  persistence:\n    config:\n      enabled: true\n      type: configMap\n      name: homepage-config\n      globalMounts:\n        - path: /app/config/settings.yaml\n          subPath: settings.yaml\n        - path: /app/config/widgets.yaml\n          subPath: widgets.yaml\n        - path: /app/config/services.yaml\n          subPath: services.yaml\n        - path: /app/config/bookmarks.yaml\n          subPath: bookmarks.yaml\n  configMaps:\n    config:\n      enabled: true\n      data:\n        settings.yaml: |\n          background: https://images.unsplash.com/photo-1502790671504-542ad42d5189?auto=format&fit=crop&w=2560&q=80\n          cardBlur: md\n          theme: dark\n          headerStyle: boxed\n          hideVersion: true\n        widgets.yaml: |\n          - search:\n              provider: google\n              target: _blank\n        services.yaml: |\n          - Popular:\n            - Matrix:\n                href: https://chat.khuedoan.com\n                description: Chat client\n                icon: element.svg\n            - Jellyfin:\n                href: https://jellyfin.khuedoan.com\n                description: Media system (movies, music, etc.)\n                icon: jellyfin.svg\n            - Jellyseerr:\n                href: https://jellyseerr.khuedoan.com\n                description: Request media\n                icon: jellyseerr.svg\n            - Paperless:\n                href: https://paperless.khuedoan.com\n                description: Document management system\n                icon: paperless.svg\n          - Management:\n            - Transmission:\n                href: https://transmission.khuedoan.com\n                description: Bittorrent client\n                icon: transmission.svg\n            - Prowlarr:\n                href: https://prowlarr.khuedoan.com\n                description: Indexer manager\n                icon: prowlarr.svg\n            - Radarr:\n                href: https://radarr.khuedoan.com\n                description: Movie manager\n                icon: radarr.svg\n            - Sonarr:\n                href: https://sonarr.khuedoan.com\n                description: TV show manager\n                icon: sonarr.svg\n            - Kanidm:\n                href: https://auth.khuedoan.com\n                description: Identity management\n                icon: https://auth.khuedoan.com/pkg/img/logo-square.svg\n          - Development:\n            - Gitea:\n                href: https://git.khuedoan.com\n                description: Git forge\n                icon: gitea.svg\n            - Woodpecker:\n                href: https://ci.khuedoan.com\n                description: Continuous integration\n                icon: woodpecker-ci.svg\n            - ArgoCD:\n                href: https://argocd.khuedoan.com\n                description: Continuous deployment\n                icon: argocd.svg\n            - Registry:\n                href: https://registry.khuedoan.com\n                description: Container registry\n                icon: docker.svg\n            - Grafana:\n                href: https://grafana.khuedoan.com\n                description: Observability dashboards\n                icon: grafana.svg\n          - Utilities:\n            - Excalidraw:\n                href: https://draw.khuedoan.com\n                description: Virtual whiteboard\n                icon: excalidraw.svg\n            - Speedtest:\n                href: https://speedtest.khuedoan.com\n                description: Internal network speed test\n                icon: openspeedtest.png\n        bookmarks.yaml: |\n          - Homelab:\n            - Documentation:\n              - href: https://homelab.khuedoan.com\n                icon: google-docs.svg\n            - Public homelab repository:\n              - href: https://github.com/khuedoan/homelab\n                icon: github.svg\n          - Managed services:\n            - Cloudflare:\n              - href: https://dash.cloudflare.com\n                icon: cloudflare.svg\n            - Terraform Cloud:\n              - href: https://app.terraform.io\n                icon: terraform.svg\n          - Infrastructure:\n            - Router:\n              - href: https://192.168.1.1\n                icon: router.svg\n            - Proxmox:\n              - href: https://proxmox:8006\n                icon: proxmox.svg\n            - Google Cloud:\n              - href: https://console.cloud.google.com\n                icon: google-cloud-platform.svg\n            - Oracle Cloud:\n              - href: https://cloud.oracle.com\n                icon: oracle-cloud.svg\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "apps/homepage/values.yaml", "language": "yaml", "complexity": "complex", "star_count": 8735, "quality_score": 0.7899999999999999, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": true, "has_ansible": false, "has_monitoring": true, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 158}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "app-template:\n  controllers:\n    wireguard:\n      containers:\n        app:\n          image:\n            repository: lscr.io/linuxserver/wireguard\n            tag: latest\n          env:\n            LOG_CONFS: false\n            USE_COREDNS: true\n          securityContext:\n            capabilities:\n              add:\n                - NET_ADMIN\n  service:\n    wireguard:\n      controller: wireguard\n      type: LoadBalancer\n      ports:\n        http:\n          port: 51820\n          protocol: UDP\n  persistence:\n    config:\n      type: secret\n      name: \"{{ .Release.Name }}-secret\"\n      globalMounts:\n        - path: /config/wg_confs\n  rawResources:\n    secret:\n      apiVersion: external-secrets.io/v1beta1\n      kind: ExternalSecret\n      spec:\n        spec:\n          secretStoreRef:\n            kind: ClusterSecretStore\n            name: global-secrets\n          data:\n            - secretKey: WIREGUARD_PRIVATE_KEY\n              remoteRef:\n                key: external\n                property: wireguard-private-key\n          target:\n            template:\n              data:\n                wg0.conf: |\n                  [Interface]\n                  Address = 172.16.0.1/32\n                  ListenPort = 51820\n                  PrivateKey = {{ `{{ .WIREGUARD_PRIVATE_KEY }}` }}\n                  PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o eth+ -j MASQUERADE\n                  PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o eth+ -j MASQUERADE\n\n                  # Note that WireGuard will ignore a peer whose public key matches\n                  # the interface's private key. So you can distribute a single\n                  # list of peers everywhere.\n                  # https://lists.zx2c4.com/pipermail/wireguard/2018-December/003703.html\n\n                  # Servers\n\n                  [Peer]\n                  # homelab\n                  PublicKey = sSAZS1Z3vB7Wx8e2yVqXfeHjgWTa80wnSYoma3mZkiU=\n                  AllowedIPs = 172.16.0.1/32, 192.168.1.100/32, 192.168.1.224/27\n\n                  [Peer]\n                  # horus\n                  PublicKey = zVwYqwvGn/IL7o6CD84y4/Y/OnRAUl/jw6T7DtNqWGM=\n                  Endpoint = horus.khuedoan.com:51820\n                  PersistentKeepalive = 25\n                  AllowedIPs = 172.16.0.2/32\n\n                  # Clients\n\n                  [Peer]\n                  # khuedoan-ryzentower\n                  PublicKey = qnDY23ZWUOlCRUmB8anpXH1uzX0A17F1YtQJmVi7GWM=\n                  AllowedIPs = 172.16.0.10/32\n\n                  [Peer]\n                  # khuedoan-thinkpadz13\n                  PublicKey = hSVWn2lBasJeueHApQYmYR0s2zNpXrqZX6F8gyEqpy0=\n                  AllowedIPs = 172.16.0.11/32\n\n                  [Peer]\n                  # khuedoan-phone\n                  PublicKey = nITHFdgTkNZOTWeSWqnGXjgwlCJMKRCnnUsjMx2yp2U=\n                  AllowedIPs = 172.16.0.12/32\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "apps/wireguard/values.yaml", "language": "yaml", "complexity": "medium", "star_count": 8735, "quality_score": 0.73, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 90}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Certificate management\n\nCertificates are generated and managed by [cert-manager](https://cert-manager.io) with [Let's Encrypt](https://letsencrypt.org).\nBy default certificates are valid for 90 days and will be renewed after 60 days.\n\ncert-manager watches `Ingress` resources across the cluster. When you create an `Ingress` with a [supported annotation](https://cert-manager.io/docs/usage/ingress/#supported-annotations):\n\n```yaml hl_lines=\"5 13 14\"\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n  name: foo\nspec:\n  rules:\n    - host: foo.example.com\n      # ...\n  tls:\n    - hosts:\n        - foo.example.com\n      secretName: foo-tls-certificate\n```\n\n```mermaid\nflowchart LR\n  User -- 6 --> Ingress\n\n  subgraph cluster[Homelab cluster]\n    Ingress --- Secret\n    Ingress -. 1 .-> Certificate\n    Certificate -. 5 .-> Secret\n    Certificate -- 2 --> CertificateRequest -- 3 --> Order -- 4 --> Challenge\n  end\n\n  Order -.- ACMEServer[ACME server]\n\n  subgraph dnsprovider[DNS provider]\n    TXT\n  end\n\n  Challenge -- 4.a --> TXT\n  ACMEServer -.- Challenge\n  ACMEServer -. 4.b .-> TXT\n```\n\n1. cert-manager creates a corresponding `Certificate` resources\n2. Based on the `Certificate` resource, cert-manager creates a `CertificateRequest` resource to request a signed certificate from the configured `ClusterIssuer`\n3. The `CertificateRequest` will create an order with an ACME server (we use Let's Encrypt), which is represented by the `Order` resource\n4. Then cert-manager will perform a [DNS-01](https://cert-manager.io/docs/configuration/acme/dns01) `Challenge`:\n    1. Create a DNS TXT record (contains a computed key)\n    2. The ACME server retrieve this key via a DNS lookup and validate that we own the domain for the requested certificate\n7. cert-manager stores the certificate (typically `tls.crt` and `tls.key`) in the `Secret` specified in the `Ingress` configuration\n8. Now you can access the HTTPS website with a valid certificate\n\nA much more detailed diagram can be found in the official documentation under [certificate lifecycle](https://cert-manager.io/docs/concepts/certificate/#certificate-lifecycle).\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "docs/concepts/certificate-management.md", "language": "markdown", "complexity": "simple", "star_count": 8735, "quality_score": 0.76, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 57}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# PXE boot\n\n```mermaid\nflowchart TD\n  subgraph controller[Initial controller]\n    Ansible\n    dhcp[DHCP server]\n    tftp[TFTP server]\n    http[HTTP server]\n  end\n\n  machine[Bare metal machine]\n\n  Ansible -. 1 .-> machine\n  machine <-. 2, 3 .-> dhcp\n  machine <-. 4, 5 .-> tftp\n  machine <-. 6, 7 .-> http\n```\n\n1. Ansible: Hey MAC address `xx:xx:xx:xx:xx:xx`, wake up!\n2. Machine: Hello everyone, I just woke up in network mode, could someone please show me how to boot?\n3. DHCP server: I hear you, here's your IP address, proceed to the next server to obtain your bootloader.\n4. Machine: Hello, could you please send me my bootloader?\n5. TFTP server: Here you go. Grab your boot configuration, kernel, and initial ramdisk as well.\n6. Machine: Hi, I just booted into my bootloader, and my boot parameters instructed me to get the installation instructions, packages, etc. from this site.\n7. HTTP server: It's all yours.\n8. Machine: Great, now I can install the OS and reboot!\n\nHere's how it looks like in action:\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/y-d7btNNAT8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "docs/concepts/pxe-boot.md", "language": "markdown", "complexity": "simple", "star_count": 8735, "quality_score": 0.76, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 32}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using Terraform infrastructure as code\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Secrets management\n\n## Overview\n\n- Global secrets are stored in the `global-secrets` namespace.\n- Integrate with GitOps using [External Secrets Operator](https://external-secrets.io).\n- Secrets that can be generated are automatically generated and stored in the `global-secrets` namespace.\n\n!!! info\n\n    Despite the name _External_ Secrets Operator, global secrets are created in the same cluster and synced\n    to other namespaces using the [Kubernetes provider](https://external-secrets.io/latest/provider/kubernetes).\n\n    While not supported by default in this project, you can also use other external providers such as HashiCorp Vault,\n    AWS Secret Manager, Google Cloud Secret Manager, Azure Key Vault, 1Password, etc.\n\n```mermaid\nflowchart TD\n  subgraph global-secrets-namespace[global-secrets namespace]\n    secret-generator[Secret Generator] -- generate if not exist --> source-secrets[Source Secrets]\n  end\n\n  subgraph app-namespace[application namespace]\n    ExternalSecret -- create --> Secret\n    App -- read --> Secret\n  end\n\n  ClusterSecretStore -- read --> source-secrets\n  ExternalSecret --- ClusterSecretStore\n```\n\n## Randomly generated secrets\n\nThis is useful when you want to generate random secrets like admin password and store in global secrets.\n\n```yaml title=\"./platform/global-secrets/files/secret-generator/config.yaml\" hl_lines=\"2-6\"\n--8<--\n./platform/global-secrets/files/secret-generator/config.yaml\n--8<--\n```\n\n## Extra third-party secrets\n\nFor third-party secrets that you don't control, add them to `external/terraform.tfvars` under the `extra_secrets` key,\nthen run `make external`.\n\nThey will be available as a Secret named `external` in the `global-secrets` namespace.\nYou can use it with `ExternalSecret` just like any other global secret.\n\n## How secrets are pulled from global secrets to other namespaces\n\nWhen you apply an `ExternalSecret` object, for example:\n\n```yaml hl_lines=\"4 21-23\"\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: gitea-admin-secret\n  namespace: gitea\nspec:\n  data:\n  - remoteRef:\n      conversionStrategy: Default\n      key: gitea.admin\n      property: password\n    secretKey: password\n  refreshInterval: 1h\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: global-secrets\n  target:\n    creationPolicy: Owner\n    deletionPolicy: Retain\n    template:\n      data:\n        password: '{{ .password }}'\n        username: gitea_admin\n      engineVersion: v2\n```\n\nThis will create a corresponding Kubernetes secret:\n\n`kubectl describe secrets -n gitea gitea-admin-secret`\n\n```yaml hl_lines=\"1 8-11\"\nName:         gitea-admin-secret\nNamespace:    gitea\nLabels:       <none>\nAnnotations:  reconcile.external-secrets.io/data-hash: <REDACTED>\n\nType:  Opaque\n\nData\n====\npassword:  32 bytes\nusername:  11 bytes\n```\n\nPlease see the official documentation for more information:\n\n- [External Secrets Operator](https://external-secrets.io)\n- [API specification](https://external-secrets.io/latest/spec)\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "docs/concepts/secrets-management.md", "language": "markdown", "complexity": "medium", "star_count": 8735, "quality_score": 0.76, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": true, "has_terraform": true, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 103}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create infrastructure as code for cloud resource provisioning using Terraform infrastructure as code\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# VPN setup\n\nYou can choose between [Tailscale](https://tailscale.com),\n[Wireguard](https://www.wireguard.com), or use both like me. I primarily use\nWireGuard but keep Tailscale as a backup for when the WireGuard server is down.\n\n## Tailscale (requires third-party account)\n\nGet an [auth key](https://tailscale.com/kb/1085/auth-keys) from [Tailscale admin console](https://login.tailscale.com/admin/authkeys):\n\n- Description: homelab\n- Reusable: optionally set this to true\n\nAdd it to `external/terraform.tfvars` as an extra secret:\n\n```hcl\nextra_secrets = {\n  tailscale-auth-key = \"tskey-auth-myauthkeyhere\"\n}\n```\n\nYou may want to back up the `external/terraform.tfvars` file to a secure location.\n\nApply the secret:\n\n```sh\nmake external\n```\n\nFinally, [enable subnet routes](https://tailscale.com/kb/1019/subnets#step-3-enable-subnet-routes-from-the-admin-console) for `homelab-router`\nfrom the [admin console](https://login.tailscale.com/admin/machines).\n\nYou can now connect to your homelab via Tailscale and [invite user to your Tailscale network](https://tailscale.com/kb/1371/invite-users).\n\n## Wireguard (requires port-forwarding)\n\n### Prerequisites\n\nFind your public IP address using:\n\n```sh\ncurl -4 ifconfig.me\n```\n\nIf you don‚Äôt have a static IP address, use dynamic DNS and replace the IP with\nyour domain name.\n\nNext, configure port forwarding in your router for the WireGuard service.\n\n!!! example\n\n    Each router is different, here's mine for reference:\n\n    - Protocol: `UDP`\n    - Start Port: `51820`\n    - End Port: `51820`\n    - Local IP Address: `192.168.1.226` (find it with `kubectl get service -n wireguard wireguard`)\n    - Start Port Local: `51820`\n    - End Port Local: `51820`\n\nGenerate a key pair for the server:\n\n```sh\nwg genkey | tee /dev/tty | wg pubkey\n```\n\nThis will generate a private key and a public key, in that order. Add the\nprivate key to `external/terraform.tfvars` as an extra secret:\n\n```hcl\nextra_secrets = {\n  wireguard-private-key = \"privatekeyhere\"\n}\n```\n\nYou may want to back up the `external/terraform.tfvars` file to a secure location.\n\nApply the secret:\n\n```sh\nmake external\n```\n\nI use `172.16.0.0/12` as the private IP range for WireGuard, but you can choose\nany private IP address range you prefer in `./apps/wireguard/values.yaml`. I\nalso recommend removing my peers and adding your own.\n\n### Add a new device to the server\n\n!!! info\n\n    Each device requires its own configuration.\n\nGenerate a new key pair for the device. You can generate it for the user, or\nthey can generate it themselves if they prefer to keep the private key\nconfidential:\n\n```sh\nwg genkey | tee /dev/tty | wg pubkey\n```\n\nThis will generate a private key and a public key, in that order. The private\nkey must be saved in a secure password manager, and save the public key for the\nnext step.\n\nUpdate the list of peers in `./apps/wireguard/values.yaml`, make sure you\nreplace all of my peers with yours.\n\n!!! example\n\n    Example configuration for my phone:\n\n    ```ini\n    [Peer]\n    PublicKey = nITHFdgTkNZOTWeSWqnGXjgwlCJMKRCnnUsjMx2yp2U=\n    AllowedIPs = 172.16.0.12/32\n    ```\n\n    - The public key is the one generated in the previous step.\n    - `172.16.0.12/32` is the device's private IP address, manually selected from\n      the `172.16.0.0/12` range mentioned above.\n\n### Add the Wireguard config to the device\n\nCreate a new configuration file for the device:\n\n```ini\n[Interface]\nAddress = <CLIENT PRIVATE IP>/32\nPrivateKey = <CLIENT PRIVATE KEY>\n\n[Peer]\nPublicKey = <SERVER PUBLIC KEY>\nEndpoint = <SERVER PUBLIC IP>:51820\nAllowedIPs = <SERVER PRIVATE IP>/32, <LOAD BALANCER IP RANGE>\n```\n\nReplace placeholders with actual values and save as `wg0.conf`.\n\n!!! example\n\n    Example configuration for my phone:\n\n    ```ini\n    [Interface]\n    Address = 172.16.0.12/32\n    PrivateKey = <REDACTED>\n\n    [Peer]\n    PublicKey = sSAZS1Z3vB7Wx8e2yVqXfeHjgWTa80wnSYoma3mZkiU\n    Endpoint = <HOME IP>:51820\n    AllowedIPs = 172.16.0.1/32, 192.168.1.224/27\n    ```\n\nThe client can now import this configuration and connect to your WireGuard\nmesh. Make sure you clean up the `wg0.conf` file after importing it to the\nclient.\n\n=== \"Mobile\"\n\n    Generate a QR code from the configuration file:\n\n    ```sh\n    qrencode -t ansiutf8 -r wg0.conf\n    ```\n\n    Then scan the QR code using the official WireGuard app.\n\n=== \"Linux\"\n\n    Import the WireGuard configuration using NetworkManager:\n\n    ```sh\n    nmcli connection import type wireguard file wg0.conf\n    ```\n\n    Activate the connection:\n\n    ```sh\n    nmcli connection up wg0\n    ```\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "docs/getting-started/vpn-setup.md", "language": "markdown", "complexity": "complex", "star_count": 8735, "quality_score": 0.7899999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": true, "has_terraform": true, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 182}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Add or remove nodes\n\nOr how to scale vertically. To replace the same node with a clean OS, remove it and add it again.\n\n## Add new nodes\n\n!!! tip\n\n    You can add multiple nodes at the same time\n\nAdd its details to the inventory **at the end of the group** (masters or workers):\n\n```diff title=\"metal/inventories/prod.yml\"\ndiff --git a/metal/inventories/prod.yml b/metal/inventories/prod.yml\nindex 7f6474a..1bb2cbc 100644\n--- a/metal/inventories/prod.yml\n+++ b/metal/inventories/prod.yml\n@@ -8,3 +8,4 @@ metal:\n     workers:\n       hosts:\n         metal3: {ansible_host: 192.168.1.113, mac: '00:23:24:d1:f5:69', disk: sda, network_interface: eno1}\n+        metal4: {ansible_host: 192.168.1.114, mac: '00:11:22:33:44:55', disk: sda, network_interface: eno1}\n```\n\nInstall the OS and join the cluster:\n\n```\nmake metal\n```\n\nThat's it!\n\n## Remove a node\n\n!!! danger\n\n    It is recommended to remove nodes one at a time\n\nRemove it from the inventory:\n\n```diff title=\"metal/inventories/prod.yml\"\ndiff --git a/metal/inventories/prod.yml b/metal/inventories/prod.yml\nindex 7f6474a..d12b50a 100644\n--- a/metal/inventories/prod.yml\n+++ b/metal/inventories/prod.yml\n@@ -4,7 +4,6 @@ metal:\n       hosts:\n         metal0: {ansible_host: 192.168.1.110, mac: '00:23:24:d1:f3:f0', disk: sda, network_interface: eno1}\n         metal1: {ansible_host: 192.168.1.111, mac: '00:23:24:d1:f4:d6', disk: sda, network_interface: eno1}\n-        metal2: {ansible_host: 192.168.1.112, mac: '00:23:24:e7:04:60', disk: sda, network_interface: eno1}\n     workers:\n       hosts:\n         metal3: {ansible_host: 192.168.1.113, mac: '00:23:24:d1:f5:69', disk: sda, network_interface: eno1}\n```\n\nDrain the node:\n\n```sh\nkubectl drain ${NODE_NAME} --delete-emptydir-data --ignore-daemonsets --force\n```\n\nRemove the node from the cluster\n\n```sh\nkubectl delete node ${NODE_NAME}\n```\n\nShutdown the node:\n\n```\nssh root@${NODE_IP} poweroff\n```\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "docs/how-to-guides/add-or-remove-nodes.md", "language": "markdown", "complexity": "simple", "star_count": 8735, "quality_score": 0.73, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": true, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 73}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Alternate DNS setup\n\n!!! info\n\n        Skip this step if you already use the included Cloudflare setup\n\nBefore you can access the home page at <https://home.example.com>, you'll need to update your DNS config.\n\nSome options for DNS config (choose one):\n\n- Change the DNS config at your domain registrar (already included and automated)\n- Change the DNS config in your router (also works if you don't own a domain)\n- Use [nip.io](https://nip.io) (suitable for a test environment)\n\n## At your domain registrar (recommended)\n\nThe default configuration is for Cloudflare DNS, but you can change the code to use other providers.\n\n## In your router\n\n!!! tip\n\n    If you don't have a domain, you can use the `home.arpa` domain (according to [RFC-8375](https://datatracker.ietf.org/doc/html/rfc8375)).\n\nYou can add each subdomain one by one, or use a wildcard `*.example.com` and point it to the IP address of the load balancer.\nTo acquire a list of subdomains and their addresses, use this command:\n\n```sh\n./scripts/get-dns-config\n```\n\n## Use [nip.io](https://nip.io)\n\nPreconfigured in the `dev` branch.\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "docs/how-to-guides/alternate-dns-setup.md", "language": "markdown", "complexity": "simple", "star_count": 8735, "quality_score": 0.73, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 35}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using Terraform infrastructure as code\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Backup and restore\n\n## Prerequisites\n\nCreate an S3 bucket to store backups. You can use AWS S3, Minio, or\nany other S3-compatible provider.\n\n- For AWS S3, your bucket URL might look something like this:\n  `https://s3.amazonaws.com/my-homelab-backup`.\n- For Minio, your bucket URL might look something like this:\n  `https://my-s3-host.example.com/homelab-backup`.\n\nFollow your provider's documentation to create a service account with the\nfollowing policy (replace `my-homelab-backup` with your actual bucket name):\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::my-homelab-backup\",\n        \"arn:aws:s3:::my-homelab-backup/*\"\n      ]\n    }\n  ]\n}\n```\n\nSave the access key and secret key to a secure location, such as a password\nmanager. While you're at it, generate a new password for Restic encryption and\nsave it there as well.\n\n!!! example\n\n    I use Minio for my homelab backups. Here's how I set it up:\n\n    - Create a bucket named `homelab-backup`.\n    - Create a service account under Identity -> Service Accounts -> Create\n      Service Account:\n        - Enable Restrict beyond user policy.\n        - Paste the policy above.\n        - Click Create and copy the access key and secret key\n    - I also set up Minio replication to store backups in two locations: one in\n      my house and one remotely.\n\n## Add backup credentials to global secrets\n\nAdd the following to `external/terraform.tfvars`:\n\n```hcl\nextra_secrets = {\n  restic-password = \"xxxxxxxxxxxxxxxxxxxxxxxx\"\n  restic-s3-bucket = \"https://s3.amazonaws.com/my-homelab-backup-xxxxxxxxxx\"\n  restic-s3-access-key = \"xxxxxxxxxxxxxxxx\"\n  restic-s3-secret-key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n}\n```\n\nThen apply the changes:\n\n```sh\nmake external\n```\n\nYou may want to back up the `external/terraform.tfvars` file to a secure location as well.\n\n## Add backup configuration for volumes\n\n!!! warning\n\n    Do not run the backup command when building a new cluster where you intend\n    to restore backups, as it may overwrite existing backup data. To restore\n    data on a new cluster, refer to the [restore from\n    backup](#restore-from-backup) section.\n\nFor now, you need to run a command to opt-in volumes until we have a better\nGitOps solution:\n\n```sh\nmake backup\n```\n\nThis command will set up Restic repositories and back up the volumes configured\nin `./Makefile`. You can adjust the list there to add or remove volumes from the\nbackup. You only need to run this command once, the backup configuration will\nbe stored in the cluster and run on a schedule.\n\n## Restore from backup\n\nThe restore process is ad-hoc, you need to run a command to restore application volumes:\n\n```sh\nmake restore\n```\n\nThe command above will restore the latest backup of recommended volumes. Like\nwith backups, you can modify `./Makefile` to adjust the list of volumes you\nwant to restore.\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "docs/how-to-guides/backup-and-restore.md", "language": "markdown", "complexity": "medium", "star_count": 8735, "quality_score": 0.76, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": true, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 107}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Run commands on multiple nodes\n\nUse [ansible-console](https://docs.ansible.com/ansible/latest/cli/ansible-console.html):\n\n```sh\ncd metal\nmake console\n```\n\nThen enter the command(s) you want to run.\n\n!!! example\n\n    `root@all (4)[f:5]$ uptime`\n\n    ```console\n    metal0 | CHANGED | rc=0 >>\n     10:52:02 up 2 min,  1 user,  load average: 0.17, 0.15, 0.06\n    metal1 | CHANGED | rc=0 >>\n     10:52:02 up 2 min,  1 user,  load average: 0.14, 0.11, 0.04\n    metal3 | CHANGED | rc=0 >>\n     10:52:02 up 2 min,  1 user,  load average: 0.03, 0.02, 0.00\n    metal2 | CHANGED | rc=0 >>\n     10:52:02 up 2 min,  1 user,  load average: 0.06, 0.06, 0.02\n    ```\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "docs/how-to-guides/run-commands-on-multiple-nodes.md", "language": "markdown", "complexity": "simple", "star_count": 8735, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using Terraform infrastructure as code\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Post-installation\n\n## Backup secrets\n\nSave the following files to a safe location like a password manager (if you're using the sandbox, you can skip this step):\n\n- `~/.ssh/id_ed25519`\n- `~/.ssh/id_ed25519.pub`\n- `./metal/kubeconfig.yaml`\n- `~/.terraform.d/credentials.tfrc.json`\n- `./external/terraform.tfvars`\n\n## Admin credentials\n\n- ArgoCD:\n    - Username: `admin`\n    - Password: run `./scripts/argocd-admin-password`\n- Gitea:\n    - Username: `gitea_admin`\n    - Password: get from `global-secrets` namespace\n- Kanidm:\n    - Usernames: `admin` and `idm_admin`\n    - Password: run `./scripts/kanidm-reset-password admin` and `./scripts/kanidm-reset-password idm_admin`\n- Jellyfin and other applications in the \\*arr stack: see the [dedicated guide for media management](../how-to-guides/media-management.md)\n- Other apps:\n    - Username: `admin`\n    - Password: get from `global-secrets` namespace\n\n## Backup\n\nNow is a good time to set up backups for your homelab.\nFollow the [backup and restore guide](../how-to-guides/backup-and-restore.md) to get started.\n\n## Run the full test suite\n\nAfter the homelab has been stabilized, you can run the full test suite to ensure that everything is working properly:\n\n```sh\nmake test\n```\n\n!!! info\n\n    The \"full\" test suit is still in its early stages, so any contribution is greatly appreciated.\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "docs/installation/post-installation.md", "language": "markdown", "complexity": "simple", "star_count": 8735, "quality_score": 0.76, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": true, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 45}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using Terraform infrastructure as code\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# External resources\n\n!!! info\n\n    These resources are optional, the homelab still works without them but will lack some features like trusted certificates and offsite backup\n\nAlthough I try to keep the amount of external resources to the minimum, there's still need for a few of them.\nBelow is a list of external resources and why we need them (also see some [alternatives](#alternatives) below).\n\n| Provider        | Resource  | Purpose                                                                                                     |\n| --------        | --------  | -------                                                                                                     |\n| Terraform Cloud | Workspace | Terraform state backend                                                                                     |\n| Cloudflare      | DNS       | DNS and [DNS-01 challenge](https://letsencrypt.org/docs/challenge-types/#dns-01-challenge) for certificates |\n| Cloudflare      | Tunnel    | Public services to the internet without port forwarding                                                     |\n| ntfy            | Topic     | External notification service to receive alerts                                                             |\n\n## Create credentials\n\nYou'll be asked to provide these credentials on first build.\n\n### Create Terraform workspace\n\nTerraform is stateful, which means it needs somewhere to store its state. Terraform Cloud is one option for a state backend with a generous free tier, perfect for a homelab.\n\n1. Sign up for a [Terraform Cloud](https://cloud.hashicorp.com/products/terraform) account\n2. Create a workspace named `homelab-external`, this is the workspace where your homelab state will be stored.\n3. Change the \"Execution Mode\" from \"Remote\" to \"Local\". This will ensure your local machine, which can access your lab, is the one executing the Terraform plan rather than the cloud runners.\n\nIf you decide to use a [different Terraform backend](https://www.terraform.io/language/settings/backends#available-backends), you'll need to edit the `external/versions.tf` file as required.\n\n### Cloudflare\n\n- Buy a domain and [transfer it to Cloudflare](https://developers.cloudflare.com/registrar/get-started/transfer-domain-to-cloudflare) if you haven't already\n- Get Cloudflare email and account ID\n- Global API key: <https://dash.cloudflare.com/profile/api-tokens>\n\n<!-- TODO switch to API token instead of API key? -->\n<!-- Terraform API token summary: -->\n\n<!-- ``` -->\n<!-- This API token will affect the below accounts and zones, along with their respective permissions -->\n\n<!-- ‚îî‚îÄ‚îÄ Khue Doan - Argo Tunnel:Edit, Account Settings:Read -->\n<!--     ‚îî‚îÄ‚îÄ khuedoan.com - Zone:Read, DNS:Edit -->\n\n<!-- Client IP Address Filtering -->\n\n<!-- ‚îî‚îÄ‚îÄ Is in - 117.xxx.xxx.xxx, 2402:xxx:xxx:xxx:xxx:xxx:xxx:xxx -->\n<!-- ``` -->\n\n<!-- ### Create Minio keys -->\n\n<!-- TODO: skip this for now -->\n\n<!-- ### Create AWS API key -->\n\n<!-- TODO: skip this for now -->\n\n### ntfy\n\n- Choose a topic name like <https://ntfy.sh/random_topic_name_here_a8sd7fkjxlkcjasdw33813> (treat it like you password)\n\n## Alternatives\n\nTo avoid vendor lock-in, each external provider must have an equivalent alternative that is easy to replace:\n\n- Terraform Cloud:\n    - Any other [Terraform backends](https://www.terraform.io/language/settings/backends)\n- Cloudflare DNS:\n    - Update cert-manager and external-dns to use a different provider\n    - [Alternate DNS setup](../../how-to-guides/alternate-dns-setup.md)\n- Cloudflare Tunnel:\n    - Use port forwarding if it's available\n    - Create a small VPS in the cloud and utilize Wireguard to route traffic via it\n    - Access everything via VPN\n    - See also [awesome tunneling](https://github.com/anderspitman/awesome-tunneling)\n- ntfy:\n    - [Self-host your own ntfy server](https://docs.ntfy.sh/install)\n    - Any other [integration supported by Grafana Alerting](https://grafana.com/docs/grafana/latest/alerting/alerting-rules/manage-contact-points/integrations/#list-of-supported-integrations)\n", "metadata": {"source": "github.com/khuedoan/homelab", "file_path": "docs/installation/production/external-resources.md", "language": "markdown", "complexity": "medium", "star_count": 8735, "quality_score": 0.76, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": true, "has_ansible": false, "has_monitoring": true, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 80}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Crash report\nabout: Create an issue for cases causing checkov to crash\ntitle: ''\nlabels: 'crash'\nassignees: ''\n\n---\n\n**Describe the issue**\nExplain what you expected to happen when checkov crashed.\n\n**Examples**\nPlease share an example code sample (in the IaC of your choice) + the expected outcomes.\n\n**Exception Trace**\nPlease share the trace for the exception and all relevant output by checkov.\nTo maximize the understanding, please run checkov with LOG_LEVEL set to debug\nas follows:\n```sh\nLOG_LEVEL=DEBUG checkov ...\n```\n\n**Desktop (please complete the following information):**\n - OS: [e.g. iOS]\n - Checkov Version [e.g. 22]\n\n**Additional context**\nAdd any other context about the problem here (e.g. code snippets).\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".github/ISSUE_TEMPLATE/crash_report.md", "language": "markdown", "complexity": "simple", "star_count": 7764, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Feature request\nabout: Feature requests or requests for enhancements that are not bugs.\ntitle: ''\nlabels: 'contribution requested'\nassignees: ''\n\n---\n\n**Describe the feature**\n\nExplain the feature in detail. Note that feature requests are always reviewed, but prioritized based on popularity, effort, and impact. We also welcome contributions.\n\n**Examples**\n\nPlease share an example code sample (in the IaC of your choice) + expected inputs and outputs from Checkov + the expected outcomes.\n\n**Additional context**\n\nAdd any other context about the problem here.\n\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".github/ISSUE_TEMPLATE/feature_request.md", "language": "markdown", "complexity": "simple", "star_count": 7764, "quality_score": 0.73, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 22}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Noise Issue\nabout: Create an issue regarding checkov's output and noise it generates.\ntitle: ''\nlabels: 'noise'\nassignees: ''\n\n---\n\n**Describe the issue**\nPlease explain the use case that leads to this noise being generated.\n\n**Examples**\nPlease share an example code sample (in the IaC of your choice) + the expected outcomes.\n\n**Version (please complete the following information):**\n - Checkov Version [e.g. 22]\n\n**Additional context**\nAdd any other context about the problem here.\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".github/ISSUE_TEMPLATE/noise_issue.md", "language": "markdown", "complexity": "simple", "star_count": 7764, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Outputs Issue\nabout: Create an issue regarding checkov's output (addition or fix)\ntitle: ''\nlabels: 'outputs'\nassignees: ''\n\n---\n\n**Describe the issue**\nIf regarding an existing output (json, junit-xml etc.) please note what is the current state\nand what is the expected state. For new outputs - please describe the use case to add it.\n\n**Additional context**\nAdd any other context about the problem here.\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".github/ISSUE_TEMPLATE/outputs_issue.md", "language": "markdown", "complexity": "simple", "star_count": 7764, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 16}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Skipping Issue\nabout: Create an issue regarding checkov's skipping mechanism\ntitle: ''\nlabels: 'skips'\nassignees: ''\n\n---\n\n**Describe the issue**\nPlease explain the functionality that is missing for you, what you did and \nwhat was the actual output.\n\n**Examples**\nPlease share an example code sample (in the IaC of your choice) + the expected outcomes.\n\n**Version (please complete the following information):**\n - Checkov Version [e.g. 22]\n\n**Additional context**\nAdd any other context about the problem here.\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".github/ISSUE_TEMPLATE/skips_issue.md", "language": "markdown", "complexity": "simple", "star_count": 7764, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 22}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "**By submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.**\n\n[//]: # \"\n    # PR Title\n    We use the title to create changelog automatically and therefore only allow specific prefixes\n    - break:    to indicate a breaking change, this supersedes any of the other types\n    - feat:     to indicate new features or checks\n    - fix:      to indicate a bugfix or handling of edge cases of existing checks\n    - docs:     to indicate an update to our documentation\n    - chore:    to indicate adjustments to workflow files or dependency updates\n    - platform: to indicate a change needed for the platform\n    Each prefix should be accompanied by a scope that specifies the targeted framework. If uncertain, use 'general'.\n    #    \n    Allowed prefixs:\n    ansible|argo|arm|azure|bicep|bitbucket|circleci|cloudformation|dockerfile|github|gha|gitlab|helm|kubernetes|kustomize|openapi|sast|sca|secrets|serverless|terraform|general|graph|terraform_plan|terraform_json\n    #\n    ex.\n    feat(terraform): add CKV_AWS_123 to ensure that VPC Endpoint Service is configured for Manual Acceptance\n\"\n\n## Description\n\n*Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.*\n\nFixes # (issue)\n\n## New/Edited policies (Delete if not relevant)\n\n### Description\n*Include a description of what makes it a violation and any relevant external links.*\n\n### Fix\n*How does someone fix the issue in code and/or in runtime?*\n\n## Checklist:\n\n- [ ] I have performed a self-review of my own code\n- [ ] I have commented my code, particularly in hard-to-understand areas\n- [ ] I have made corresponding changes to the documentation\n- [ ] I have added tests that prove my feature, policy, or fix is effective and works\n- [ ] New and existing tests pass locally with my changes\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".github/PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 7764, "quality_score": 0.73, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": true, "has_ansible": true, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 42}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using Terraform infrastructure as code\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "enable-secret-scan-all-files: true\nframework:\n- secrets\nquiet: true\nskip-path:\n- docs\n- tests/arm/checks/resource/example_AzureScaleSetPassword/FAILED.json\n- tests/arm/checks/resource/example_AzureScaleSetPassword/UNKNOWN.json\n- tests/arm/checks/resource/example_StorageAccountAzureServicesAccessEnabled/storageAccountAzureServicesAccessEnabled-FAILED2.json\n- tests/arm/checks/resource/example_StorageAccountDefaultNetworkAccessDeny/storageAccountDefaultNetworkAccessDeny-FAILED2.json\n- tests/terraform/checks/resource/azure/example_AzureInstanceExtensions/main.tf\n- tests/common/utils/conftest.py\n- tests/common/utils/test_secrets_utils.py\n- tests/sca_image/conftest.py\n- tests/sca_package_2/conftest.py\n- tests/secrets\n- tests/terraform/checks/provider\n- tests/terraform/parser/resources/plan_tags/tfplan.json\n- tests/terraform/runner/resources/plan/tfplan.json\n- tests/terraform/runner/tf_plan_skip_check_regex/resource/skip_directory/tfplan2.json\n- tests/terraform/runner/tf_plan_skip_check_regex/resource/tfplan1.json\n- tests/terraform/runner/tfplan2.json\n- tests/unit/test_secrets.py\n- tests/terraform/runner/resources/example/example.tf\n- tests/terraform/graph\n- tests/terraform/checks\n- /checkov/secrets/plugins/entropy_keyword_combinator.py\n- /checkov/secrets/plugins/detector_utils.py\n- /cdk_integration_tests/src/python/RedshiftClusterPubliclyAccessible/pass.py\n- /cdk_integration_tests/src/python/RedshiftClusterEncryption/pass.py\n- /cdk_integration_tests/src/python/RedshiftClusterEncryption/fail__1__.py\n- /cdk_integration_tests/src/python/RedshiftClusterPubliclyAccessible/fail__1__.py\n- /cdk_integration_tests/src/python/ElasticacheReplicationGroupEncryptionAtTransitAuthToken/fail__2__.py\n- /cdk_integration_tests/src/python/ElasticacheReplicationGroupEncryptionAtTransitAuthToken/pass.py\n- /cdk_integration_tests/src/typescript\n- /checkov/cdk/checks/python/GlueDataCatalogEncryption.yaml\n- /checkov/cdk/checks/python/GlueDataCatalogEncryption.yaml\n- /checkov/cdk/checks/python/GlueDataCatalogEncryption.yaml\n- /checkov/cdk/checks/python/GlueDataCatalogEncryption.yaml\n- tests/terraform/runner/resources/plan_with_providers\nsummary-position: bottom\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".github/checkov.yaml", "language": "yaml", "complexity": "medium", "star_count": 7764, "quality_score": 0.73, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": true, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 42}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using Terraform infrastructure as code\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{\n  \"LABEL\": {\n    \"name\": \"title needs adjustment\",\n    \"color\": \"EEEEEE\"\n  },\n  \"CHECKS\": {\n    \"prefixes\": [\n      \"chore: \"\n    ],\n    \"regexp\": \"^(fix|feat|break|docs|chore|platform)\\\\((ansible|argo|arm|azure|bicep|bitbucket|circleci|cloudformation|dockerfile|github|gha|gitlab|helm|kubernetes|kustomize|openapi|sast|sca|secrets|serverless|terraform|general|graph|terraform_plan|terraform_json)\\\\): \"\n  },\n  \"MESSAGES\": {\n    \"success\": \"PR title is valid\",\n    \"failure\": \"PR title is invalid\",\n    \"notice\": \"Title needs to pass regex '(fix|feat|break|docs|chore|platform)\\\\((ansible|argo|arm|azure|bicep|bitbucket|circleci|cloudformation|dockerfile|github|gha|gitlab|helm|kubernetes|kustomize|openapi|sast|sca|secrets|serverless|terraform|general|graph|terraform_plan|terraform_json)\\\\): '\"\n  }\n}\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".github/pr-title-checker-config.json", "language": "json", "complexity": "simple", "star_count": 7764, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": true, "has_terraform": true, "has_ansible": true, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: build\n\non:\n  workflow_dispatch:\n    inputs:\n      versionBump:\n        description: 'The part of the version to bump'\n        required: true\n        default: 'patch'\n        type: choice\n        options:\n          - patch\n          - minor\n          - major\n\n  push:\n    branches:\n      - main\n    paths-ignore:\n      - 'docs/**'\n      - 'INTHEWILD.md'\n      - 'README.md'\n      - 'CHANGELOG.md'\n      - '.github/**'\n      - checkov/version.py\n      - kubernetes/requirements.txt\n      - coverage.svg\n      - '.swm/**'\n      - '.pre-commit-config.yaml'\n\npermissions:\n  contents: read\n\nconcurrency:\n  group: 'build'\n  cancel-in-progress: true\n\njobs:\n  security:\n    uses: ./.github/workflows/security-shared.yml\n    secrets: inherit\n\n  integration-tests:\n    strategy:\n      fail-fast: true\n      matrix:\n        python: [\"3.10\", \"3.11\", \"3.12\", \"3.13\"]\n        os: [ubuntu-latest, macos-latest, windows-latest]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ matrix.python }}\n      - uses: actions/setup-node@cdca7365b2dadb8aad0a33bc7601856ffabcc48e  # v3\n      - uses: azure/setup-helm@5119fcb9089d432beecbf79bb2c7915207344b78  # v3\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n      - uses: imranismail/setup-kustomize@a76db1c6419124d51470b1e388c4b29476f495f1  # v2\n        if: ${{ runner.os != 'windows' }}\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Build & install checkov package\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ matrix.python }}\n          pipenv run pip install pytest pytest-xdist setuptools wheel\n          pipenv run python setup.py sdist bdist_wheel\n          bash -c 'pipenv run pip install dist/checkov-*.whl'\n      - name: Clone Terragoat - vulnerable terraform\n        run: git clone https://github.com/bridgecrewio/terragoat\n      - name: Clone Cfngoat - vulnerable cloudformation\n        run: git clone https://github.com/bridgecrewio/cfngoat\n      - name: Clone Kubernetes-goat - vulnerable kubernetes\n        run: git clone https://github.com/madhuakula/kubernetes-goat\n      - name: Clone kustomize-goat - vulnerable kustomize\n        run: git clone https://github.com/bridgecrewio/kustomizegoat\n      - name: Create checkov reports\n        run: |\n          # Just making sure the API key tests don't run on PRs\n          bash -c './integration_tests/prepare_data.sh \"${{ matrix.os }}\" \"${{ matrix.python }}\"'\n        env:\n          LOG_LEVEL: INFO\n          BC_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n          TF_REGISTRY_TOKEN: ${{ secrets.TFC_TOKEN }}\n          GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n      - name: Run integration tests\n        run: |\n          pipenv run pytest integration_tests\n\n  integration-tests-old-python:\n    strategy:\n      fail-fast: true\n      matrix:\n        python: [\"3.8\", \"3.9\"]\n        os: [ubuntu-latest, windows-latest]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ matrix.python }}\n      - uses: actions/setup-node@cdca7365b2dadb8aad0a33bc7601856ffabcc48e  # v3\n      - uses: azure/setup-helm@5119fcb9089d432beecbf79bb2c7915207344b78  # v3\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n      - uses: imranismail/setup-kustomize@a76db1c6419124d51470b1e388c4b29476f495f1  # v2\n        if: ${{ runner.os != 'windows' }}\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Build & install checkov package\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ matrix.python }}\n          pipenv run pip install pytest pytest-xdist setuptools wheel\n          pipenv run python setup.py sdist bdist_wheel\n          bash -c 'pipenv run pip install dist/checkov-*.whl'\n      - name: Clone Terragoat - vulnerable terraform\n        run: git clone https://github.com/bridgecrewio/terragoat\n      - name: Clone Cfngoat - vulnerable cloudformation\n        run: git clone https://github.com/bridgecrewio/cfngoat\n      - name: Clone Kubernetes-goat - vulnerable kubernetes\n        run: git clone https://github.com/madhuakula/kubernetes-goat\n      - name: Clone kustomize-goat - vulnerable kustomize\n        run: git clone https://github.com/bridgecrewio/kustomizegoat\n      - name: Create checkov reports\n        run: |\n          # Just making sure the API key tests don't run on PRs\n          bash -c './integration_tests/prepare_data.sh \"${{ matrix.os }}\" \"${{ matrix.python }}\"'\n        env:\n          LOG_LEVEL: INFO\n          BC_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n          TF_REGISTRY_TOKEN: ${{ secrets.TFC_TOKEN }}\n          GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n      - name: Run integration tests\n        run: |\n          pipenv run pytest integration_tests\n\n  prisma-tests:\n    runs-on: [ self-hosted, public, linux, x64 ]\n    env:\n      PYTHON_VERSION: \"3.8\"\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Build & install checkov package\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ env.PYTHON_VERSION }}\n          pipenv run pip install pytest pytest-xdist\n          pipenv run python setup.py sdist bdist_wheel\n          bash -c 'pipenv run pip install dist/checkov-*.whl'\n      - name: Clone Terragoat - vulnerable terraform\n        run: git clone https://github.com/bridgecrewio/terragoat\n      - name: Run checkov with Prisma creds\n        env:\n          PRISMA_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n        run: |\n          pipenv run checkov -s -d terragoat --bc-api-key \"$PRISMA_KEY\" --repo-id yuvalyacoby/terragoat > checkov_report_prisma.txt\n          grep \"prismacloud.io\" checkov_report_prisma.txt\n          exit $?\n  sast-integration-tests:\n    strategy:\n      fail-fast: true\n      matrix:\n        python: [\"3.10\", \"3.11\", \"3.12\", \"3.13\"]\n        os: [ubuntu-latest, macos-latest]\n    runs-on: ${{ matrix.os }}\n    continue-on-error: true # for now it is ok to fail\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ matrix.python }}\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Build & install checkov package\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ matrix.python }}\n          pipenv run pip install pytest pytest-xdist setuptools wheel\n          pipenv run python setup.py sdist bdist_wheel\n          bash -c 'pipenv run pip install dist/checkov-*.whl'\n      - name: Clone flask - Python repo for SAST\n        run: git clone https://github.com/pallets/flask\n      - name: Clone WebGoat - Java repo for SAST\n        run: git clone https://github.com/WebGoat/WebGoat\n      - name: Clone axios - JavaScript repo for SAST\n        run: git clone https://github.com/axios/axios\n      - name: Create checkov reports\n        env:\n          LOG_LEVEL: INFO\n          BC_API_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n        run: bash -c './sast_integration_tests/prepare_data.sh'\n      - name: Run integration tests\n        run: |\n          pipenv run pytest sast_integration_tests\n\n  sast-integration-tests-old-python:\n    strategy:\n      fail-fast: true\n      matrix:\n        python: [\"3.8\", \"3.9\"]\n        os: [ubuntu-latest]\n    runs-on: ${{ matrix.os }}\n    continue-on-error: true # for now it is ok to fail\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ matrix.python }}\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Build & install checkov package\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ matrix.python }}\n          pipenv run pip install pytest pytest-xdist setuptools wheel\n          pipenv run python setup.py sdist bdist_wheel\n          bash -c 'pipenv run pip install dist/checkov-*.whl'\n      - name: Clone flask - Python repo for SAST\n        run: git clone https://github.com/pallets/flask\n      - name: Clone WebGoat - Java repo for SAST\n        run: git clone https://github.com/WebGoat/WebGoat\n      - name: Clone axios - JavaScript repo for SAST\n        run: git clone https://github.com/axios/axios\n      - name: Create checkov reports\n        env:\n          LOG_LEVEL: INFO\n          BC_API_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n        run: bash -c './sast_integration_tests/prepare_data.sh'\n      - name: Run integration tests\n        run: |\n          pipenv run pytest sast_integration_tests\n\n  unit-tests:\n    timeout-minutes: 30\n    runs-on: ubuntu-latest\n    env:\n      PYTHON_VERSION: \"3.8\"\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - name: Set up Python ${{ env.PYTHON_VERSION }}\n        uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      - uses: azure/setup-helm@5119fcb9089d432beecbf79bb2c7915207344b78  # v3\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n      - uses: imranismail/setup-kustomize@a76db1c6419124d51470b1e388c4b29476f495f1  # v2\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Install dependencies\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ env.PYTHON_VERSION }}\n          pipenv install --dev\n      - name: Test with pytest\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          IS_TEST: true\n        run: |\n          pipenv run python -m pytest tests\n  bump-version:\n    needs: [integration-tests, unit-tests, prisma-tests, sast-integration-tests, integration-tests-old-python, sast-integration-tests-old-python]\n    runs-on: [self-hosted, public, linux, x64]\n    environment: release\n    permissions:\n      contents: write\n      # IMPORTANT: this permission is mandatory for trusted publishing to pypi\n      id-token: write\n    timeout-minutes: 30\n    env:\n      PYTHON_VERSION: \"3.8\"\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n        with:\n          token: ${{ secrets.GH_PAT_SECRET }}\n      - name: Import GPG key\n        id: import_gpg\n        uses: crazy-max/ghaction-import-gpg@e89d40939c28e39f97cf32126055eeae86ba74ec  # v5\n        with:\n          gpg_private_key: ${{ secrets.GPG_PRIVATE_KEY }}\n          passphrase: ${{ secrets.PASSPHRASE }}\n      - name: Set up Python ${{ env.PYTHON_VERSION }}\n        uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Install dependencies\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ env.PYTHON_VERSION }}\n          pipenv install\n      - name: Calculate version\n        run: |\n          git fetch --tags --force\n          latest_tag=\"$(git tag --sort=v:refname | tail -n 1)\"\n          echo \"latest tag: $latest_tag\"\n          if [[ -z \"${{ inputs.versionBump }}\" ]]\n          then\n            version=\"patch\"\n          else\n            version=\"${{ inputs.versionBump }}\"\n          fi\n          case $version in\n            minor)\n              new_tag=$(echo \"$latest_tag\" | awk -F. -v a=\"$1\" -v b=\"$2\" -v c=\"$3\" '{printf(\"%d.%d.%d\", $1+a, $2+b+1 , 0)}')\n              ;;\n            major)\n              new_tag=$(echo \"$latest_tag\" | awk -F. -v a=\"$1\" -v b=\"$2\" -v c=\"$3\" '{printf(\"%d.%d.%d\", $1+a+1, 0 , 0)}')\n              ;;\n            patch)\n              new_tag=$(echo \"$latest_tag\" | awk -F. -v a=\"$1\" -v b=\"$2\" -v c=\"$3\" '{printf(\"%d.%d.%d\", $1+a, $2+b , $3+1)}')\n              ;;\n          esac\n\n          echo \"new tag: $new_tag\"\n          echo \"version=$new_tag\" >> \"$GITHUB_OUTPUT\"\n          \n          # grab major version for later image tag usage         \n          major_version=$(echo \"${new_tag}\" | head -c1)\n          echo \"major_version=$major_version\" >> \"$GITHUB_OUTPUT\"\n        id: calculateVersion\n      - name: version\n        env:\n          GITHUB_TOKEN: ${{ secrets.PAT_TOKEN }}\n        run: |\n          ## update docs\n          export PYTHONPATH='.'\n          # change the doc links to proper markdown versions\n          export CHECKOV_CREATE_MARKDOWN_HYPERLINKS='True'\n          git pull\n\n          for i in cloudformation terraform kubernetes serverless arm dockerfile secrets github_configuration gitlab_configuration bitbucket_configuration github_actions gitlab_ci bicep openapi bitbucket_pipelines argo_workflows circleci_pipelines azure_pipelines ansible all\n          do\n            export scansdoc=\"docs/5.Policy Index/$i.md\"\n            echo \"---\" > \"$scansdoc\"\n            echo \"layout: default\" >> \"$scansdoc\"\n            echo \"title: $i resource scans\" >> \"$scansdoc\"\n            echo \"nav_order: 1\" >> \"$scansdoc\"\n            echo \"---\" >> \"$scansdoc\"\n            echo \"\" >> \"$scansdoc\"\n            echo \"# $i resource scans (auto generated)\" >> \"$scansdoc\"\n            echo \"\" >> \"$scansdoc\"\n            pipenv run python checkov/main.py --list --framework \"$i\" >> \"$scansdoc\"\n          done\n\n          #add cloudformation scans to serverless\n          export scansdoc=\"docs/5.Policy Index/serverless.md\"\n          pipenv run python checkov/main.py --list --framework cloudformation >> \"$scansdoc\"\n          git add \"docs/5.Policy Index/*\"\n          git commit --reuse-message=\"HEAD@{1}\" || echo \"No changes to commit\"\n          \n          git config --global user.name 'GitHub Actions Bot'\n          git config --global user.email 'actions@github.com'\n          \n          new_tag=${{ steps.calculateVersion.outputs.version }}\n          echo \"new tag: $new_tag\"\n          ## update python version\n          echo \"version = '$new_tag'\" > 'checkov/version.py'\n          echo \"checkov==$new_tag\" > 'kubernetes/requirements.txt'\n\n          git commit --reuse-message=\"HEAD@{1}\" checkov/version.py kubernetes/requirements.txt || echo \"No changes to commit\"\n          git push origin\n          git tag $new_tag\n          git push --tags\n        id: version\n      - name: create python package\n        run: |\n          pipenv run python setup.py sdist bdist_wheel\n      - name: Publish a Python distribution to PyPI\n        uses: pypa/gh-action-pypi-publish@ec4db0b4ddc65acdf4bff5fa45ac92d78b56bdf0  # v1\n      - name: sleep and wait for package to refresh\n        run: |\n          sleep 2m\n    outputs:\n      version: ${{ steps.calculateVersion.outputs.version }}\n      major_version: ${{ steps.calculateVersion.outputs.major_version }}\n  publish-checkov-dockerhub:\n    needs: bump-version\n    uses: bridgecrewio/gha-reusable-workflows/.github/workflows/publish-image.yaml@main\n    permissions:\n      contents: read\n      id-token: write  # Enable OIDC\n      packages: write\n    with:\n      image_name_dockerhub: bridgecrew/checkov\n      image_name_ghcr: ghcr.io/${{ github.repository }}\n      image_tag_full: ${{ needs.bump-version.outputs.version }}\n      image_tag_short: ${{ needs.bump-version.outputs.major_version }}\n      runner: \"['self-hosted', 'public', 'linux', 'x64']\"\n    secrets:\n      BC_API_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n      PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n      DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}\n      DOCKERHUB_PASSWORD: ${{ secrets.DOCKERHUB_PASSWORD }}\n  publish-checkov-k8s-dockerhub:\n    needs: bump-version\n    uses: bridgecrewio/gha-reusable-workflows/.github/workflows/publish-image.yaml@main\n    permissions:\n      contents: read\n      id-token: write  # Enable OIDC\n      packages: write\n    with:\n      image_name_dockerhub: bridgecrew/checkov-k8s\n      image_name_ghcr: ghcr.io/${{ github.repository }}-k8s\n      image_tag_full: ${{ needs.bump-version.outputs.version }}\n      image_tag_short: ${{ needs.bump-version.outputs.major_version }}\n      dockerfile_path: kubernetes/Dockerfile\n      runner: \"['self-hosted', 'public', 'linux', 'x64']\"\n    secrets:\n      BC_API_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n      PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n      DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}\n      DOCKERHUB_PASSWORD: ${{ secrets.DOCKERHUB_PASSWORD }}\n  update-bridgecrew-projects:\n    needs: publish-checkov-dockerhub\n    runs-on: [self-hosted, public, linux, x64]\n    environment: release\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - name: update checkov release\n        run: |\n          curl -X POST \"https://jenkins-webhook.bridgecrew.cloud/buildByToken/build?job=Open-Source/upgrade-checkov&token=${{ secrets.BC_JENKINS_TOKEN }}\"\n\n          # trigger checkov-action update\n          curl -XPOST -u \"${{ secrets.GH_PAT_USER}}:${{secrets.GH_PAT_SECRET}}\" -H \"Accept: application/vnd.github.everest-preview+json\" -H \"Content-Type: application/json\" https://api.github.com/repos/bridgecrewio/checkov-action/dispatches --data '{\"event_type\": \"build\"}'\n\n          # trigger bridgecrew-py update\n          curl -XPOST -u \"${{ secrets.GH_PAT_USER}}:${{secrets.GH_PAT_SECRET}}\" -H \"Accept: application/vnd.github.everest-preview+json\" -H \"Content-Type: application/json\" https://api.github.com/repos/bridgecrewio/bridgecrew-py/dispatches --data '{\"event_type\": \"build\"}'\n\n          # trigger whorf update\n          curl -XPOST -u \"${{ secrets.GH_PAT_USER}}:${{secrets.GH_PAT_SECRET}}\" -H \"Accept: application/vnd.github.everest-preview+json\" -H \"Content-Type: application/json\" https://api.github.com/repos/bridgecrewio/whorf/dispatches --data '{\"event_type\": \"release\"}'\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".github/workflows/build.yml", "language": "yaml", "complexity": "complex", "star_count": 7764, "quality_score": 0.76, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": true, "has_ansible": true, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 465}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# For most projects, this workflow file will not need changing; you simply need\n# to commit it to your repository.\n#\n# You may wish to alter this file to override the set of languages analyzed,\n# or to provide custom queries or build logic.\n#\n# ******** NOTE ********\n# We have attempted to detect the languages in your repository. Please check\n# the `language` matrix defined below to confirm you have the correct set of\n# supported CodeQL languages.\n#\nname: \"CodeQL\"\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    # The branches below must be a subset of the branches above\n    branches: [ main ]\n  schedule:\n    - cron: '17 4 * * 2'\n  workflow_dispatch:\n\npermissions:\n  contents: read\n\njobs:\n  analyze:\n    name: Analyze\n    runs-on: [self-hosted, public, linux, x64]\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - name: Set up Python\n        uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: '3.10'\n      - name: Setup python for CodeQL\n        run: |\n          python -m pip install --no-cache-dir --upgrade pip pipenv\n          echo \"CODEQL_PYTHON=$(which python)\" >> \"$GITHUB_ENV\"\n      - name: Check Pipfile.lock changed\n        uses: tj-actions/verify-changed-files@a1c6acee9df209257a246f2cc6ae8cb6581c1edf  # v13\n        id: changed_files\n        with:\n          files: Pipfile.lock\n      - name: Setup dependencies if they changed\n        if: steps.changed_files.outputs.files_changed == 'true'\n        run: |\n          pipenv lock -r > requirements.txt\n          pip install -r requirements.txt\n      - name: Initialize CodeQL\n        uses: github/codeql-action/init@45775bd8235c68ba998cffa5171334d58593da47  # v2\n        with:\n          languages: python\n          setup-python-dependencies: false\n          config-file: ./.github/codeql-config.yml\n      - name: Autobuild\n        uses: github/codeql-action/autobuild@45775bd8235c68ba998cffa5171334d58593da47  # v2\n      - name: Perform CodeQL Analysis\n        uses: github/codeql-action/analyze@45775bd8235c68ba998cffa5171334d58593da47  # v2\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".github/workflows/codeql-analysis.yml", "language": "yaml", "complexity": "simple", "star_count": 7764, "quality_score": 0.73, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 66}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Sample workflow for building and deploying a Jekyll site to GitHub Pages\nname: Deploy Jekyll with GitHub Pages dependencies preinstalled\n\non:\n  # Runs on pushes targeting the default branch\n  push:\n    branches: [\"main\"]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\n# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.\n# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.\nconcurrency:\n  group: \"pages\"\n  cancel-in-progress: false\n\njobs:\n  # Build job\n  build:\n    runs-on: [self-hosted, public, linux, x64]\n    steps:\n      - name: Checkout\n        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - name: Setup Pages\n        uses: actions/configure-pages@983d7736d9b0ae728b81ab479565c72886d7745b  # v3\n      - name: Build with Jekyll\n        uses: actions/jekyll-build-pages@44a6e6beabd48582f863aeeb6cb2151cc1716697  # v1\n        with:\n          source: ./docs\n          destination: ./_site\n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@0252fc4ba7626f0298f0cf00902a25c6afc77fa8  # v2\n\n  # Deployment job\n  deploy:\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    runs-on: [self-hosted, public, linux, x64]\n    needs: build\n    steps:\n      - name: Deploy to GitHub Pages\n        id: deployment\n        uses: actions/deploy-pages@d6db90164ac5ed86f2b6aed7e0febac5b3c0c03e  # v2\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".github/workflows/jekyll-gh-pages.yml", "language": "yaml", "complexity": "medium", "star_count": 7764, "quality_score": 0.76, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 52}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Nightly Run\n\non:\n  schedule:\n    # daily at 23:00 UTC\n    - cron: \"0 23 * * *\"\n  workflow_dispatch:\n\npermissions:\n  contents: read\n\njobs:\n  github-release:\n    runs-on: [self-hosted, public, linux, x64]\n    environment: release\n    permissions:\n      contents: write\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n        with:\n          fetch-depth: 0\n          token: ${{ secrets.GH_PAT_SECRET }}\n      - name: Prepare Release\n        id: prepare_release\n        run: |\n          # grab latest release and tag to compare and decide to create a new one\n          create_release=true\n          latest_gh_release=$(curl -s \"https://api.github.com/repos/${{ github.repository }}/releases/latest\"  | grep -Po '\"tag_name\": \"\\K.*?(?=\")')\n          latest_tag=$(git describe --abbrev=0 --tags)\n\n          if [ \"$latest_gh_release\" = \"$latest_tag\" ]\n          then\n            create_release=false\n          fi\n\n          echo \"create_release=$create_release\" >> \"$GITHUB_OUTPUT\"\n          echo \"latest_release_version=$latest_gh_release\" >> \"$GITHUB_OUTPUT\"\n          echo \"version=$latest_tag\" >> \"$GITHUB_OUTPUT\"\n      - name: Build GitHub Release changelog\n        if: steps.prepare_release.outputs.create_release == 'true'\n        id: build_github_release\n        uses: mikepenz/release-changelog-builder-action@5f3409748e2230350e149a7f7b5b8e9bcd785d44  # v3\n        env:\n          GITHUB_TOKEN: ${{ secrets.GH_PAT_SECRET }}\n        with:\n          configuration: \".github/release-changelog-config.json\"\n          fromTag: ${{ steps.prepare_release.outputs.latest_release_version }}\n          toTag: ${{ steps.prepare_release.outputs.version }}\n      - name: Create GitHub Release\n        if: steps.build_github_release.outputs.changelog != ''\n        uses: softprops/action-gh-release@da05d552573ad5aba039eaac05058a918a7bf631  # v2.2.2\n        id: create_github_release\n        with:\n          tag_name: ${{ steps.prepare_release.outputs.version }}\n          name: ${{ steps.prepare_release.outputs.version }}\n          body: ${{ steps.build_github_release.outputs.changelog }}\n      - name: Update CHANGELOG.md\n        if: steps.build_github_release.outputs.changelog != ''\n        uses: stefanzweifel/changelog-updater-action@a938690fad7edf25368f37e43a1ed1b34303eb36  # v1\n        with:\n          latest-version: ${{ steps.prepare_release.outputs.version }}\n          release-notes: ${{ steps.build_github_release.outputs.changelog }}\n      - name: Commit updated CHANGELOG.md\n        if: steps.build_github_release.outputs.changelog != ''\n        uses: stefanzweifel/git-auto-commit-action@b863ae1933cb653a53c021fe36dbb774e1fb9403  # v5\n        with:\n          commit_message: \"chore: update release notes\"\n          file_pattern: CHANGELOG.md\n    outputs:\n      upload_url: ${{ steps.create_github_release.outputs.upload_url }}\n      version: ${{ steps.prepare_release.outputs.version }}\n  build-release-artifacts:\n    strategy:\n      matrix:\n        include:\n          - os: macos-latest\n            name: darwin\n            suffix: ''\n          - os: ubuntu-latest\n            name: linux\n            suffix: ''\n          - os: windows-latest\n            name: windows\n            suffix: '.exe'\n    needs: [github-release]\n    if: needs.github-release.outputs.upload_url != ''\n    runs-on: ${{ matrix.os }}\n    permissions:\n      contents: write\n    env:\n      PYTHON_VERSION: \"3.8\"\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n        with:\n          fetch-depth: 0\n          token: ${{ secrets.GITHUB_TOKEN }}\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Install deps and run pyinstaller\n        run: |\n          pipenv sync\n          pipenv run pip install pyinstaller\n      - name: Build executable\n        run: pipenv run pyinstaller checkov.spec\n      - name: Windows - Test executable\n        if: matrix.os == 'windows-latest'\n        shell: bash\n        # make sure it doesn't crash\n        run: ./dist/checkov.exe -s -d tests/terraform/checks/resource/alicloud\n      - name: Windows - zip artifact\n        if: matrix.os == 'windows-latest'\n        run: tar.exe -a -c -f checkov.zip dist\\\\checkov.exe\n      - name: Linux/Mac - Test executable\n        if: matrix.os != 'windows-latest'\n        # make sure it doesn't crash\n        run: ./dist/checkov -s -d tests/terraform/checks/resource/alicloud\n      - name: Linux/Mac - zip artifact\n        if: matrix.os != 'windows-latest'\n        run: zip checkov.zip dist/checkov\n      - name: Upload Release Asset\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          upload_url: ${{ needs.github-release.outputs.upload_url }}\n          asset_path: checkov.zip\n          asset_name: checkov_${{ matrix.name }}_X86_64.zip\n          asset_content_type: application/zip\n  build-release-artifact-linux-arm:\n    needs: [ github-release ]\n    if: needs.github-release.outputs.upload_url != ''\n    runs-on: [self-hosted, public, linux, arm64]\n    container:\n      image: arm64v8/python:3.8\n    permissions:\n      contents: write\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n        with:\n          fetch-depth: 0\n          token: ${{ secrets.GITHUB_TOKEN }}\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Install deps and run pyinstaller\n        run: |\n          pipenv sync\n          pipenv run pip install pyinstaller\n      - name: Build executable\n        run: pipenv run pyinstaller checkov.spec\n      - name: zip artifact\n        run: |\n          apt-get update\n          apt install zip\n          zip checkov.zip dist/checkov\n      - name: Upload Release Asset\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          upload_url: ${{ needs.github-release.outputs.upload_url }}\n          asset_path: checkov.zip\n          asset_name: checkov_linux_arm64.zip\n          asset_content_type: application/zip\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".github/workflows/nightly.yml", "language": "yaml", "complexity": "complex", "star_count": 7764, "quality_score": 0.76, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": true, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 169}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: PR Test\n\non: pull_request\n\npermissions:\n  contents: read\n\njobs:\n  lint:\n    uses: bridgecrewio/gha-reusable-workflows/.github/workflows/pre-commit.yaml@main\n    with:\n      python-version: \"3.9\"\n\n  danger-check:\n    runs-on: [ self-hosted, public, linux, x64 ]\n    permissions:\n      contents: read\n      pull-requests: read\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - name: Install Node.js\n        uses: actions/setup-node@cdca7365b2dadb8aad0a33bc7601856ffabcc48e  # v4\n        with:\n          node-version: \"16\"\n      - name: Install and run DangerJS\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n          npm install -g danger\n          danger ci --verbose --failOnErrors\n  cfn-lint:\n    runs-on: ubuntu-latest\n    env:\n      PYTHON_VERSION: \"3.8\"\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      - name: Get changed CFN test files\n        id: changed-files-specific\n        uses: tj-actions/changed-files@ed68ef82c095e0d48ec87eccea555d944a631a4c # v44\n        with:\n          files: tests/cloudformation/checks/resource/aws/**/*\n      - name: Filter YAML and JSON files\n        if: steps.changed-files-specific.outputs.any_changed == 'true'\n        id: filter-files\n        run: |\n          YAML_JSON_FILES=$(echo ${{ steps.changed-files-specific.outputs.all_changed_files }} | tr ' ' '\\n' | grep -E '\\.ya?ml$|\\.json$' | tr '\\n' ' ')\n          if [ -n \"$YAML_JSON_FILES\" ]; then\n            echo \"YAML_JSON_FILES=$YAML_JSON_FILES\" >> \"$GITHUB_ENV\"\n          fi\n      - name: Install cfn-lint & Lint Cloudformation templates\n        if: env.YAML_JSON_FILES != ''\n        run: |\n          pip install -U cfn-lint\n          for file in $YAML_JSON_FILES; do\n            cfn-lint \"$file\" -i W\n          done\n\n  mypy:\n    uses: bridgecrewio/gha-reusable-workflows/.github/workflows/mypy.yaml@main\n    with:\n      python-version: \"3.8\"\n\n  unit-tests:\n    strategy:\n      fail-fast: true\n      matrix:\n        python: [\"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\", \"3.13\"]\n    runs-on: ubuntu-latest\n    timeout-minutes: 30\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - name: Set up Python ${{ matrix.python }}\n        uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ matrix.python }}\n          allow-prereleases: true\n          cache: \"pipenv\"\n          cache-dependency-path: \"Pipfile.lock\"\n      - uses: azure/setup-helm@5119fcb9089d432beecbf79bb2c7915207344b78  # v3\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n      - uses: imranismail/setup-kustomize@a76db1c6419124d51470b1e388c4b29476f495f1  # v2\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n      - name: Install pipenv\n        run: |\n          if [ \"${{ matrix.python }}\" = \"3.12\" ] || [ \"${{ matrix.python }}\" = \"3.13\" ]; then\n            # needed for numpy\n            python -m pip install --no-cache-dir --upgrade pipenv==2024.4.0\n          else\n            python -m pip install --no-cache-dir --upgrade pipenv\n          fi\n      - name: Install dependencies\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ matrix.python }}\n\n          if [ \"${{ matrix.python }}\" = \"3.12\" ] || [ \"${{ matrix.python }}\" = \"3.13\" ]; then\n            echo \"patching >3.12 issues\"\n            pipenv run pip install setuptools\n            # needed for numpy\n            pipenv install --skip-lock --dev -v\n          else\n            pipenv install --dev -v\n          fi\n\n          # list all dependencies to get a better view about installed package versions\n          pipenv run pip list\n\n      - name: Unit tests\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        run: pipenv run python -m pytest tests\n\n  integration-tests:\n    strategy:\n      fail-fast: true\n      matrix:\n        python: [\"3.10\", \"3.11\", \"3.12\", \"3.13\"]\n        os: [ubuntu-latest, macos-latest, windows-latest]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ matrix.python }}\n          allow-prereleases: true\n          cache: \"pipenv\"\n          cache-dependency-path: \"Pipfile.lock\"\n      - uses: actions/setup-node@cdca7365b2dadb8aad0a33bc7601856ffabcc48e  # v4\n      - uses: azure/setup-helm@5119fcb9089d432beecbf79bb2c7915207344b78  # v3\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n      - uses: imranismail/setup-kustomize@a76db1c6419124d51470b1e388c4b29476f495f1  # v2\n        if: ${{ runner.os != 'windows' }}\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Build & install checkov package\n        shell: bash\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ matrix.python }}\n          pipenv run pip install pytest pytest-xdist setuptools wheel\n          pipenv run python setup.py sdist bdist_wheel\n          bash -c 'pipenv run pip install dist/checkov-*.whl'\n      - name: Clone Terragoat - vulnerable terraform\n        run: git clone https://github.com/bridgecrewio/terragoat\n      - name: Clone Cfngoat - vulnerable cloudformation\n        run: git clone https://github.com/bridgecrewio/cfngoat\n      - name: Clone Kubernetes-goat - vulnerable kubernetes\n        run: git clone https://github.com/madhuakula/kubernetes-goat\n      - name: Clone kustomize-goat - vulnerable kustomize\n        run: git clone https://github.com/bridgecrewio/kustomizegoat\n      - name: Create checkov reports\n        env:\n          LOG_LEVEL: INFO\n          BC_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n        run: |\n          # Just making sure the API key tests don't run on PRs\n          bash -c './integration_tests/prepare_data.sh ${{ matrix.os }} 3.8'\n      - name: Run integration tests\n        run: |\n          pipenv run pytest integration_tests -k 'not api_key'\n\n  integration-tests-old-python:\n    strategy:\n      fail-fast: true\n      matrix:\n        python: [\"3.8\", \"3.9\"]\n        os: [ubuntu-latest, windows-latest]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ matrix.python }}\n          allow-prereleases: true\n          cache: \"pipenv\"\n          cache-dependency-path: \"Pipfile.lock\"\n      - uses: actions/setup-node@cdca7365b2dadb8aad0a33bc7601856ffabcc48e  # v4\n      - uses: azure/setup-helm@5119fcb9089d432beecbf79bb2c7915207344b78  # v3\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n      - uses: imranismail/setup-kustomize@a76db1c6419124d51470b1e388c4b29476f495f1  # v2\n        if: ${{ runner.os != 'windows' }}\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Build & install checkov package\n        shell: bash\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ matrix.python }}\n          pipenv run pip install pytest pytest-xdist setuptools wheel\n          pipenv run python setup.py sdist bdist_wheel\n          bash -c 'pipenv run pip install dist/checkov-*.whl'\n      - name: Clone Terragoat - vulnerable terraform\n        run: git clone https://github.com/bridgecrewio/terragoat\n      - name: Clone Cfngoat - vulnerable cloudformation\n        run: git clone https://github.com/bridgecrewio/cfngoat\n      - name: Clone Kubernetes-goat - vulnerable kubernetes\n        run: git clone https://github.com/madhuakula/kubernetes-goat\n      - name: Clone kustomize-goat - vulnerable kustomize\n        run: git clone https://github.com/bridgecrewio/kustomizegoat\n      - name: Create checkov reports\n        env:\n          LOG_LEVEL: INFO\n          BC_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n        run: |\n          # Just making sure the API key tests don't run on PRs\n          bash -c './integration_tests/prepare_data.sh ${{ matrix.os }} 3.8'\n      - name: Run integration tests\n        run: |\n          pipenv run pytest integration_tests -k 'not api_key'\n\n  sast-integration-tests:\n    strategy:\n      fail-fast: true\n      matrix:\n        python: [\"3.12\", \"3.13\"]\n        os: [ubuntu-latest, macos-latest]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ matrix.python }}\n          allow-prereleases: true\n          cache: \"pipenv\"\n          cache-dependency-path: \"Pipfile.lock\"\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Build & install checkov package\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ matrix.python }}\n          pipenv run pip install pytest pytest-xdist setuptools wheel\n          pipenv run python setup.py sdist bdist_wheel\n          bash -c 'pipenv run pip install dist/checkov-*.whl'\n      - name: Clone flask - Python repo for SAST\n        run: git clone https://github.com/pallets/flask\n      - name: Clone WebGoat - Java repo for SAST\n        run: git clone https://github.com/WebGoat/WebGoat\n      - name: Clone axios - JavaScript repo for SAST\n        run: git clone https://github.com/axios/axios\n      - name: Create checkov reports\n        env:\n          LOG_LEVEL: INFO\n          BC_API_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n        if: env.BC_API_KEY != null\n        run: bash -c './sast_integration_tests/prepare_data.sh'\n      - name: Run integration tests\n        env:\n          LOG_LEVEL: INFO\n          BC_API_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n        if: env.BC_API_KEY != null\n        run: |\n          pipenv run pytest sast_integration_tests\n\n  sast-integration-tests-old-python:\n    strategy:\n      fail-fast: true\n      matrix:\n        python: [\"3.8\"]\n        os: [ubuntu-latest]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ matrix.python }}\n          allow-prereleases: true\n          cache: \"pipenv\"\n          cache-dependency-path: \"Pipfile.lock\"\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Build & install checkov package\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ matrix.python }}\n          pipenv run pip install pytest pytest-xdist setuptools wheel\n          pipenv run python setup.py sdist bdist_wheel\n          bash -c 'pipenv run pip install dist/checkov-*.whl'\n      - name: Clone flask - Python repo for SAST\n        run: git clone https://github.com/pallets/flask\n      - name: Clone WebGoat - Java repo for SAST\n        run: git clone https://github.com/WebGoat/WebGoat\n      - name: Clone axios - JavaScript repo for SAST\n        run: git clone https://github.com/axios/axios\n      - name: Create checkov reports\n        env:\n          LOG_LEVEL: INFO\n          BC_API_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n        if: env.BC_API_KEY != null\n        run: bash -c './sast_integration_tests/prepare_data.sh'\n      - name: Run integration tests\n        env:\n          LOG_LEVEL: INFO\n          BC_API_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n        if: env.BC_API_KEY != null\n        run: |\n          pipenv run pytest sast_integration_tests\n\n  cdk-integration-tests:\n    strategy:\n      fail-fast: true\n      matrix:\n        python: [\"3.12\", \"3.13\"]\n        os: [ubuntu-latest, macos-latest]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ matrix.python }}\n          allow-prereleases: true\n          cache: \"pipenv\"\n          cache-dependency-path: \"Pipfile.lock\"\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Build & install checkov package\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ matrix.python }}\n          pipenv run pip install pytest pytest-xdist setuptools wheel\n          pipenv run python setup.py sdist bdist_wheel\n          bash -c 'pipenv run pip install dist/checkov-*.whl'\n      - name: Create checkov reports\n        env:\n          LOG_LEVEL: INFO\n          BC_API_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n        if: env.BC_API_KEY != null\n        run: bash -c './cdk_integration_tests/prepare_data.sh'\n      - name: Run integration tests\n        env:\n          LOG_LEVEL: INFO\n          BC_API_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n        if: env.BC_API_KEY != null\n        run: |\n          pipenv run pytest cdk_integration_tests\n\n  cdk-integration-tests-old-python:\n    strategy:\n      fail-fast: true\n      matrix:\n        python: [\"3.8\"]\n        os: [ubuntu-latest]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ matrix.python }}\n          allow-prereleases: true\n          cache: \"pipenv\"\n          cache-dependency-path: \"Pipfile.lock\"\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Build & install checkov package\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ matrix.python }}\n          pipenv run pip install pytest pytest-xdist setuptools wheel\n          pipenv run python setup.py sdist bdist_wheel\n          bash -c 'pipenv run pip install dist/checkov-*.whl'\n      - name: Create checkov reports\n        env:\n          LOG_LEVEL: INFO\n          BC_API_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n        if: env.BC_API_KEY != null\n        run: bash -c './cdk_integration_tests/prepare_data.sh'\n      - name: Run integration tests\n        env:\n          LOG_LEVEL: INFO\n          BC_API_KEY: ${{ secrets.PRISMA_KEY_API2 }}\n          PRISMA_API_URL: ${{ secrets.PRISMA_API_URL_2 }}\n        if: env.BC_API_KEY != null\n        run: |\n          pipenv run pytest cdk_integration_tests\n\n  performance-tests:\n    env:\n      PYTHON_VERSION: \"3.8\"\n      working-directory: ./performance_tests\n    runs-on: [self-hosted, public, linux, x64]\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: \"pipenv\"\n          cache-dependency-path: \"Pipfile.lock\"\n      - uses: actions/setup-node@cdca7365b2dadb8aad0a33bc7601856ffabcc48e  # v4\n      - uses: azure/setup-helm@5119fcb9089d432beecbf79bb2c7915207344b78  # v3\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n      - uses: imranismail/setup-kustomize@a76db1c6419124d51470b1e388c4b29476f495f1  # v2\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n      - name: Build & install checkov package\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ env.PYTHON_VERSION }}\n          # 'py' package is used in 'pytest-benchmark', but 'pytest' removed it in their latest version\n          pipenv run pip install pytest pytest-benchmark py\n          pipenv run python setup.py sdist bdist_wheel\n          bash -c 'pipenv run pip install dist/checkov-*.whl'\n      - name: Clone terraform-aws-components\n        run: git clone --branch 0.182.0 https://github.com/cloudposse/terraform-aws-components.git\n        working-directory: ${{ env.working-directory }}\n      - name: Clone aws-cloudformation-templates\n        run: git clone --branch 0.0.1 https://github.com/awslabs/aws-cloudformation-templates.git\n        working-directory: ${{ env.working-directory }}\n      - name: Clone kubernetes-yaml-templates\n        run: git clone https://github.com/dennyzhang/kubernetes-yaml-templates.git\n        working-directory: ${{ env.working-directory }}\n# TODO: migrate to separate performance tests\n#      - name: Clone Python-Mini-Projects\n#        run: git clone https://github.com/alimoustafa2000/Python-Mini-Projects.git\n#        working-directory: ${{ env.working-directory }}\n#      - name: Clone NodeJs\n#        run: git clone https://github.com/harshitbansal373/NodeJs.git\n#        working-directory: ${{ env.working-directory }}\n#      - name: Clone Mini-Project-using-Java\n#        run: git clone https://github.com/ikanurfitriani/Mini-Project-using-Java.git\n#        working-directory: ${{ env.working-directory }}\n      - name: Run performance tests\n        run: |\n          pipenv run pytest\n        working-directory: ${{ env.working-directory }}\n\n  dogfood-tests:\n    runs-on: ubuntu-latest\n    env:\n      PYTHON_VERSION: \"3.8\"\n      WORKING_DIRECTORY: ./dogfood_tests\n    steps:\n      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n      - uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: \"pipenv\"\n          cache-dependency-path: \"Pipfile.lock\"\n      - uses: azure/setup-helm@5119fcb9089d432beecbf79bb2c7915207344b78  # v3\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n      - uses: imranismail/setup-kustomize@a76db1c6419124d51470b1e388c4b29476f495f1  # v2\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n      - name: Install pipenv\n        run: |\n          python -m pip install --no-cache-dir --upgrade pipenv\n\n      - name: Build & install checkov package\n        run: |\n          # remove venv, if exists\n          pipenv --rm || true\n          pipenv --python ${{ env.PYTHON_VERSION }}\n          pipenv run pip install pytest pytest-xdist\n          pipenv run python setup.py sdist bdist_wheel\n          bash -c 'pipenv run pip install dist/checkov-*.whl'\n      - name: Run dogfood tests\n        run: |\n          pipenv run pytest\n        working-directory: ${{ env.WORKING_DIRECTORY }}\n\n  eval-keys-test:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v3\n\n      - name: Get changed Python files\n        id: changed-files\n        uses: tj-actions/changed-files@ed68ef82c095e0d48ec87eccea555d944a631a4c # v44\n        with:\n          files: checkov/**/*.py\n\n      - name: Validate 'BaseResourceCheck' use contains eval keys\n        if: steps.changed-files.outputs.any_changed == 'true'\n        run: |\n          # Define an array of exceptions (files to skip)\n          EXCEPTIONS=(\n            \"base_resource_check.py\" \n            \"VPCDefaultNetwork.py\"\n            \"IAMUserNotUsedForAccess.py\"  # Whole Resource type check\n          )\n          \n          echo \"Changed files:\"\n          echo \"${{ steps.changed-files.outputs.all_changed_files }}\"\n          \n          EXIT_CODE=0\n          IFS=$'\\n'  # Change Internal Field Separator to handle spaces in filenames too\n          for file in $(echo \"${{ steps.changed-files.outputs.all_changed_files }}\" | tr ',' '\\n'); do\n            # Check if the file is in the list of exceptions\n            SKIP_FILE=\"false\"\n            for exception in \"${EXCEPTIONS[@]}\"; do\n              # If the file ends with one of the exception file names, skip it\n              if [[ \"$file\" == *\"$exception\" ]]; then\n                echo \"Skipping $file (allowed exception)\"\n                SKIP_FILE=\"true\"\n                break\n              fi\n            done\n          \n            # Only run checks if not in exceptions list\n            if [[ \"$SKIP_FILE\" == \"false\" ]]; then\n              # If file contains 'BaseResourceCheck', check for 'get_inspected_key' or 'evaluated_keys'\n              if grep -q \"BaseResourceCheck\" \"$file\"; then\n                if ! grep -q \"get_inspected_key\" \"$file\" && ! grep -q \"evaluated_keys\" \"$file\"; then\n                  echo \"ERROR: $file has BaseResourceCheck but does NOT contain 'get_inspected_key' or 'evaluated_keys'\"\n                  EXIT_CODE=1\n                fi\n              fi\n            fi\n          done\n          unset IFS  # Restore IFS to default\n          \n          # Fail the job if any file violated the rule\n          if [ \"$EXIT_CODE\" -ne 0 ]; then\n            echo \"One or more files did not satisfy the requirement.\"\n            exit 1\n          fi\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".github/workflows/pr-test.yml", "language": "yaml", "complexity": "complex", "star_count": 7764, "quality_score": 0.7, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": true, "has_terraform": true, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 557}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using Terraform infrastructure as code\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\n\n# For use with pre-commit.\n# See usage instructions at http://pre-commit.com\n\n-   id: checkov\n    name: Checkov\n    description: This hook runs checkov.\n    entry: checkov -d .\n    language: python\n    pass_filenames: false\n    always_run: false\n    files: \\.tf$\n    exclude: \\.+.terraform\\/.*$\n    require_serial: true\n\n-   id: checkov_container\n    name: Checkov\n    description: This hook runs checkov.\n    entry: --tty bridgecrew/checkov:latest -d .\n    args: []\n    language: docker_image\n    pass_filenames: false\n    always_run: false\n    files: \\.tf$\n    exclude: \\.+.terraform\\/.*$\n    require_serial: true\n\n-   id: checkov_diff\n    name: Checkov Diff\n    description: This hook runs checkov against all changed files.\n    entry: checkov --enable-secret-scan-all-files\n    args: [\"-f\"] # required and must come last\n    language: python\n    require_serial: true\n\n-   id: checkov_diff_container\n    name: Checkov Diff\n    description: This hook runs checkov against all changed files.\n    entry: --tty bridgecrew/checkov:latest --enable-secret-scan-all-files\n    args: [\"-f\"] # required and must come last\n    language: docker_image\n    require_serial: true\n\n-   id: checkov_secrets\n    name: Checkov Secrets\n    description: This hook looks for secrets with checkov.\n    entry: checkov --framework secrets --enable-secret-scan-all-files\n    args: [\"-f\"] # required and must come last\n    language: python\n    always_run: true\n    require_serial: true\n\n-   id: checkov_secrets_container\n    name: Checkov Secrets\n    description: This hook looks for secrets with checkov.\n    entry: --tty bridgecrew/checkov:latest --framework secrets --enable-secret-scan-all-files\n    args: [\"-f\"] # required and must come last\n    language: docker_image\n    always_run: true\n    require_serial: true\n", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".pre-commit-hooks.yaml", "language": "yaml", "complexity": "simple", "star_count": 7764, "quality_score": 0.73, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": true, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 62}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nid: gm0ti\nname: Creating a Solver\nfile_version: 1.0.2\napp_version: 0.9.4-0\nfile_blobs:\n  checkov/common/checks_infra/solvers/complex_solvers/not_solver.py: 60e9301de2a35a51b0464babaf537104d82cf00a\n  checkov/common/checks_infra/checks_parser.py: 50130edc6639275b43dbd287572972b826eee687\n  checkov/common/checks_infra/solvers/complex_solvers/__init__.py: 2e25b8e1f51406fe5e2995019eb6046fdf3650f2\n  checkov/common/graph/checks_infra/solvers/base_solver.py: e84d471f6fc2e8ef12d82fa061784c57a7915d5c\n  checkov/common/checks_infra/solvers/complex_solvers/base_complex_solver.py: 186dd8805259132d32936fafc19c389d452869c4\n  checkov/common/checks_infra/solvers/connections_solvers/or_connection_solver.py: 38df2db8112768f7ee10facc3feac82b84affc32\n  checkov/common/checks_infra/solvers/attribute_solvers/any_attribute_solver.py: 5aa38478ce1174ea46d2cff94ec52358e8595369\n  checkov/common/checks_infra/solvers/attribute_solvers/not_contains_attribute_solver.py: 0d44d643a7ba2f1fc78fa86ad53b46c47e546ee1\n  checkov/common/checks_infra/solvers/attribute_solvers/not_ending_with_attribute_solver.py: 334cc79488dc5f5f52e3d66ef9b24e3ad89f1e99\n---\n\nA Solver is a major component in our system. This document will describe what it is and how to add a new one.\n\nA Solver is a graph operator that impelements a certain piece of logic, such as AttributeEquals, GreaterThan, Exists and more. There are also more complext solvers such as the `And` solver which implement logic between two or more solvers\n\nWhen we add a new Solver, we create a class that inherits from `BaseSolver`[<sup id=\"2wxET6\">‚Üì</sup>](#f-2wxET6).\n\nSome examples of `BaseSolver`[<sup id=\"2wxET6\">‚Üì</sup>](#f-2wxET6)s are `OrConnectionSolver`[<sup id=\"Z1oapTp\">‚Üì</sup>](#f-Z1oapTp), `AnyResourceSolver`[<sup id=\"Z7ghIg\">‚Üì</sup>](#f-Z7ghIg), `NotContainsAttributeSolver`[<sup id=\"Z136myH\">‚Üì</sup>](#f-Z136myH), and `NotEndingWithAttributeSolver`[<sup id=\"923Qq\">‚Üì</sup>](#f-923Qq). Note: some of these examples inherit indirectly from `BaseSolver`[<sup id=\"2wxET6\">‚Üì</sup>](#f-2wxET6).\n\n> **NOTE: Inherit from** `BaseComplexSolver`[<sup id=\"10523X\">‚Üì</sup>](#f-10523X)\n> \n> Most `BaseSolver`[<sup id=\"2wxET6\">‚Üì</sup>](#f-2wxET6)s inherit directly from `BaseComplexSolver`[<sup id=\"10523X\">‚Üì</sup>](#f-10523X) and almost none inherit directly from `BaseSolver`[<sup id=\"2wxET6\">‚Üì</sup>](#f-2wxET6). In this document we demonstrate inheriting from `BaseComplexSolver`[<sup id=\"10523X\">‚Üì</sup>](#f-10523X).\n\n## TL;DR - How to Add a `BaseComplexSolver`[<sup id=\"10523X\">‚Üì</sup>](#f-10523X)\n\n1.  Create a new class inheriting from `BaseComplexSolver`[<sup id=\"10523X\">‚Üì</sup>](#f-10523X)¬†\n    \n    *   Place the file under `üìÑ checkov/common/checks_infra/solvers/complex_solvers`, e.g. `NotSolver`[<sup id=\"Z2wW09R\">‚Üì</sup>](#f-Z2wW09R) is defined in `üìÑ checkov/common/checks_infra/solvers/complex_solvers/not_solver.py`.\n        \n2.  Define `operator`[<sup id=\"Z1HozjT\">‚Üì</sup>](#f-Z1HozjT).\n    \n3.  Implement `__init__`[<sup id=\"ZDc3b7\">‚Üì</sup>](#f-ZDc3b7), `_get_operation`[<sup id=\"Z1IWbj3\">‚Üì</sup>](#f-Z1IWbj3), and `get_operation`[<sup id=\"I3t5K\">‚Üì</sup>](#f-I3t5K).\n    \n4.  Update `üìÑ checkov/common/checks_infra/checks_parser.py`.\n    \n5.  Update `üìÑ checkov/common/checks_infra/solvers/complex_solvers/__init__.py`.\n    \n6.  **Profit** üí∞\n    \n\n## Example Walkthrough - `NotSolver`[<sup id=\"Z2wW09R\">‚Üì</sup>](#f-Z2wW09R)\n\nWe'll follow the implementation of `NotSolver`[<sup id=\"Z2wW09R\">‚Üì</sup>](#f-Z2wW09R) for this example.\n\nA `NotSolver`[<sup id=\"Z2wW09R\">‚Üì</sup>](#f-Z2wW09R) is a solver that inverts the logic of the solvers within it\n\n## Steps to Adding a new `BaseComplexSolver`[<sup id=\"10523X\">‚Üì</sup>](#f-10523X)\n\n### 1\\. Inherit from `BaseComplexSolver`[<sup id=\"10523X\">‚Üì</sup>](#f-10523X).\n\nAll `BaseComplexSolver`[<sup id=\"10523X\">‚Üì</sup>](#f-10523X)s are defined in files under `üìÑ checkov/common/checks_infra/solvers/complex_solvers`.\n\n<br/>\n\nWe first need to define our class in the relevant file, and inherit from `BaseComplexSolver`[<sup id=\"10523X\">‚Üì</sup>](#f-10523X):\n<!-- NOTE-swimm-snippet: the lines below link your snippet to Swimm -->\n### üìÑ checkov/common/checks_infra/solvers/complex_solvers/not_solver.py\n```python\n‚¨ú 5      from checkov.common.checks_infra.solvers.complex_solvers.base_complex_solver import BaseComplexSolver\n‚¨ú 6      \n‚¨ú 7      \nüü© 8      class NotSolver(BaseComplexSolver):\n‚¨ú 9          operator = Operators.NOT  # noqa: CCE003  # a static attribute\n‚¨ú 10     \n‚¨ú 11         def __init__(self, solvers: List[BaseSolver], resource_types: List[str]) -> None:\n```\n\n<br/>\n\n> **Note**: the class name should end with \"Solver\".\n\n### 2\\. Define `operator`[<sup id=\"Z1HozjT\">‚Üì</sup>](#f-Z1HozjT)\n\n`BaseSolver`[<sup id=\"2wxET6\">‚Üì</sup>](#f-2wxET6)s should define this variable:\n\n*   `operator`[<sup id=\"Z1HozjT\">‚Üì</sup>](#f-Z1HozjT)\n\n<br/>\n\n\n\n<!-- NOTE-swimm-snippet: the lines below link your snippet to Swimm -->\n### üìÑ checkov/common/checks_infra/solvers/complex_solvers/not_solver.py\n```python\n‚¨ú 6      \n‚¨ú 7      \n‚¨ú 8      class NotSolver(BaseComplexSolver):\nüü© 9          operator = Operators.NOT  # noqa: CCE003  # a static attribute\n‚¨ú 10     \n‚¨ú 11         def __init__(self, solvers: List[BaseSolver], resource_types: List[str]) -> None:\n‚¨ú 12             if len(solvers) != 1:\n```\n\n<br/>\n\n### 3\\. Implement `__init__`[<sup id=\"ZDc3b7\">‚Üì</sup>](#f-ZDc3b7), `_get_operation`[<sup id=\"Z1IWbj3\">‚Üì</sup>](#f-Z1IWbj3), and `get_operation`[<sup id=\"I3t5K\">‚Üì</sup>](#f-I3t5K)\n\nHere is how we do it for `NotSolver`[<sup id=\"Z2wW09R\">‚Üì</sup>](#f-Z2wW09R):\n\nImplement `__init__`[<sup id=\"ZDc3b7\">‚Üì</sup>](#f-ZDc3b7).\n\n<br/>\n\n\n\n<!-- NOTE-swimm-snippet: the lines below link your snippet to Swimm -->\n### üìÑ checkov/common/checks_infra/solvers/complex_solvers/not_solver.py\n```python\n‚¨ú 8      class NotSolver(BaseComplexSolver):\n‚¨ú 9          operator = Operators.NOT  # noqa: CCE003  # a static attribute\n‚¨ú 10     \nüü© 11         def __init__(self, solvers: List[BaseSolver], resource_types: List[str]) -> None:\nüü© 12             if len(solvers) != 1:\nüü© 13                 raise Exception('The \"not\" operator must have exactly one child')\nüü© 14             super().__init__(solvers, resource_types)\n‚¨ú 15     \n‚¨ú 16         def _get_operation(self, *args: Any, **kwargs: Any) -> Any:\n‚¨ú 17             if len(args) != 1:\n```\n\n<br/>\n\n\n\n<!-- NOTE-swimm-snippet: the lines below link your snippet to Swimm -->\n### üìÑ checkov/common/checks_infra/solvers/complex_solvers/not_solver.py\n```python\n‚¨ú 13                 raise Exception('The \"not\" operator must have exactly one child')\n‚¨ú 14             super().__init__(solvers, resource_types)\n‚¨ú 15     \nüü© 16         def _get_operation(self, *args: Any, **kwargs: Any) -> Any:\nüü© 17             if len(args) != 1:\nüü© 18                 raise Exception('The \"not\" operator must have exactly one child')\nüü© 19             return not args[0]\n‚¨ú 20     \n‚¨ú 21         def get_operation(self, vertex: Dict[str, Any]) -> bool:  # type:ignore[override]\n‚¨ú 22             return not self.solvers[0].get_operation(vertex)\n```\n\n<br/>\n\n\n\n<!-- NOTE-swimm-snippet: the lines below link your snippet to Swimm -->\n### üìÑ checkov/common/checks_infra/solvers/complex_solvers/not_solver.py\n```python\n‚¨ú 18                 raise Exception('The \"not\" operator must have exactly one child')\n‚¨ú 19             return not args[0]\n‚¨ú 20     \nüü© 21         def get_operation(self, vertex: Dict[str, Any]) -> bool:  # type:ignore[override]\nüü© 22             return not self.solvers[0].get_operation(vertex)\n‚¨ú 23     \n```\n\n<br/>\n\n## Update additional files with the new class\n\nEvery time we add new `BaseComplexSolver`[<sup id=\"10523X\">‚Üì</sup>](#f-10523X)s, we reference them in a few locations.\n\nWe will still look at `NotSolver`[<sup id=\"Z2wW09R\">‚Üì</sup>](#f-Z2wW09R) as our example.\n\n<br/>\n\n4\\. Update `üìÑ checkov/common/checks_infra/checks_parser.py`, as we do with `NotSolver`[<sup id=\"Z2wW09R\">‚Üì</sup>](#f-Z2wW09R) here:\n<!-- NOTE-swimm-snippet: the lines below link your snippet to Swimm -->\n### üìÑ checkov/common/checks_infra/checks_parser.py\n```python\n‚¨ú 19         NotEndingWithAttributeSolver,\n‚¨ú 20         AndSolver,\n‚¨ú 21         OrSolver,\nüü© 22         NotSolver,\n‚¨ú 23         ConnectionExistsSolver,\n‚¨ú 24         ConnectionNotExistsSolver,\n‚¨ú 25         AndConnectionSolver,\n```\n\n<br/>\n\nIn addition, in the same file:\n<!-- NOTE-swimm-snippet: the lines below link your snippet to Swimm -->\n### üìÑ checkov/common/checks_infra/checks_parser.py\n```python\n‚¨ú 93     operators_to_complex_solver_classes: dict[str, Type[BaseComplexSolver]] = {\n‚¨ú 94         \"and\": AndSolver,\n‚¨ú 95         \"or\": OrSolver,\nüü© 96         \"not\": NotSolver,\n‚¨ú 97     }\n‚¨ú 98     \n‚¨ú 99     operator_to_connection_solver_classes: dict[str, Type[BaseConnectionSolver]] = {\n```\n\n<br/>\n\n4\\. We modify `üìÑ checkov/common/checks_infra/solvers/complex_solvers/__init__.py`, for example:\n<!-- NOTE-swimm-snippet: the lines below link your snippet to Swimm -->\n### üìÑ checkov/common/checks_infra/solvers/complex_solvers/__init__.py\n```python\n‚¨ú 1      from checkov.common.checks_infra.solvers.complex_solvers.or_solver import OrSolver  # noqa\n‚¨ú 2      from checkov.common.checks_infra.solvers.complex_solvers.and_solver import AndSolver  # noqa\nüü© 3      from checkov.common.checks_infra.solvers.complex_solvers.not_solver import NotSolver  # noqa\n‚¨ú 4      \n```\n\n<br/>\n\n<!-- THIS IS AN AUTOGENERATED SECTION. DO NOT EDIT THIS SECTION DIRECTLY -->\n### Swimm Note\n\n<span id=\"f-ZDc3b7\">__init__</span>[^](#ZDc3b7) - \"checkov/common/checks_infra/solvers/complex_solvers/not_solver.py\" L11\n```python\n    def __init__(self, solvers: List[BaseSolver], resource_types: List[str]) -> None:\n```\n\n<span id=\"f-Z1IWbj3\">_get_operation</span>[^](#Z1IWbj3) - \"checkov/common/checks_infra/solvers/complex_solvers/not_solver.py\" L16\n```python\n    def _get_operation(self, *args: Any, **kwargs: Any) -> Any:\n```\n\n<span id=\"f-Z7ghIg\">AnyResourceSolver</span>[^](#Z7ghIg) - \"checkov/common/checks_infra/solvers/attribute_solvers/any_attribute_solver.py\" L7\n```python\nclass AnyResourceSolver(BaseAttributeSolver):\n```\n\n<span id=\"f-10523X\">BaseComplexSolver</span>[^](#10523X) - \"checkov/common/checks_infra/solvers/complex_solvers/base_complex_solver.py\" L9\n```python\nclass BaseComplexSolver(BaseSolver):\n```\n\n<span id=\"f-2wxET6\">BaseSolver</span>[^](#2wxET6) - \"checkov/common/graph/checks_infra/solvers/base_solver.py\" L9\n```python\nclass BaseSolver:\n```\n\n<span id=\"f-I3t5K\">get_operation</span>[^](#I3t5K) - \"checkov/common/checks_infra/solvers/complex_solvers/not_solver.py\" L21\n```python\n    def get_operation(self, vertex: Dict[str, Any]) -> bool:  # type:ignore[override]\n```\n\n<span id=\"f-Z136myH\">NotContainsAttributeSolver</span>[^](#Z136myH) - \"checkov/common/checks_infra/solvers/attribute_solvers/not_contains_attribute_solver.py\" L7\n```python\nclass NotContainsAttributeSolver(ContainsAttributeSolver):\n```\n\n<span id=\"f-923Qq\">NotEndingWithAttributeSolver</span>[^](#923Qq) - \"checkov/common/checks_infra/solvers/attribute_solvers/not_ending_with_attribute_solver.py\" L7\n```python\nclass NotEndingWithAttributeSolver(EndingWithAttributeSolver):\n```\n\n<span id=\"f-Z2wW09R\">NotSolver</span>[^](#Z2wW09R) - \"checkov/common/checks_infra/solvers/complex_solvers/not_solver.py\" L8\n```python\nclass NotSolver(BaseComplexSolver):\n```\n\n<span id=\"f-Z1HozjT\">operator</span>[^](#Z1HozjT) - \"checkov/common/checks_infra/solvers/complex_solvers/not_solver.py\" L9\n```python\n    operator = Operators.NOT  # noqa: CCE003  # a static attribute\n```\n\n<span id=\"f-Z1oapTp\">OrConnectionSolver</span>[^](#Z1oapTp) - \"checkov/common/checks_infra/solvers/connections_solvers/or_connection_solver.py\" L11\n```python\nclass OrConnectionSolver(ComplexConnectionSolver):\n```\n\n<br/>\n\nThis file was generated by Swimm. [Click here to view it in the app](https://app.swimm.io/repos/Z2l0aHViJTNBJTNBY2hlY2tvdiUzQSUzQWJyaWRnZWNyZXdpbw==/docs/gm0ti).", "metadata": {"source": "github.com/bridgecrewio/checkov", "file_path": ".swm/creating-a-solver.gm0ti.sw.md", "language": "markdown", "complexity": "complex", "star_count": 7764, "quality_score": 0.82, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 273}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## Review the Contributing Guidelines\n\nBefore submitting a pull request, verify it meets all requirements in the [Contributing Guidelines](https://github.com/donnemartin/awesome-aws/blob/master/CONTRIBUTING.md).\n\n## Describe Why This Is Awesome\n\nWhy is this awesome?\n\n--\n\nLike this pull request?  Vote for it by adding a :+1:\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": ".github/PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 13620, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 12}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/aws.png\">\n</p>\n<br/>\n\n# Awesome AWS [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\nA curated list of awesome AWS libraries, open source repos, guides, blogs, and other resources.\n\nInspired by the [awesome](https://github.com/sindresorhus/awesome) list.\n\n## The Fiery Meter of AWSome\n\n* Repo with 0100+ Stars: :fire:\n* Repo with 0200+ Stars: :fire::fire:\n* Repo with 0500+ Stars: :fire::fire::fire:\n* Repo with 1000+ Stars: :fire::fire::fire::fire:\n* Repo with 2000+ Stars: :fire::fire::fire::fire::fire:\n\nRepos not on `The Fiery Meter of AWSome` can still be awesome, see [A Note on Repo AWSomeness](https://github.com/donnemartin/awesome-aws/blob/master/CONTRIBUTING.md#a-note-on-repo-awsomeness).\n\n### `awesome-aws` Python Module\n\n[![Build Status](https://travis-ci.org/donnemartin/awesome-aws.svg?branch=master)](https://travis-ci.org/donnemartin/awesome-aws) [![Codecov](https://img.shields.io/codecov/c/github/donnemartin/awesome-aws.svg)](https://codecov.io/github/donnemartin/saws/awesome-aws) [![PyPI version](https://badge.fury.io/py/awesome-aws.svg)](http://badge.fury.io/py/awesome-aws)\n\nThe Python module [`awesome-aws`](https://github.com/donnemartin/awesome-aws/tree/master/awesome) regularly scans repos on [Awesome AWS](https://github.com/donnemartin/awesome-aws) to maintain the accuracy of the `Fiery Meter of AWSome`.\n\n## Contributing\n\nContributions are welcome!\n\nCheck out the [Contributing Guidelines](https://github.com/donnemartin/awesome-aws/blob/master/CONTRIBUTING.md).\n\n## Index\n\n* [SDKs and Samples](#sdks-and-samples)\n    * [Android](#android-sdk)\n    * [C++](#c-sdk)\n    * [Clojure](#clojure-sdk)\n    * [Go](#go-sdk)\n    * [iOS](#ios-sdk)\n    * [IoT](#iot-sdk)\n    * [Java](#java-sdk)\n    * [JavaScript](#javascript-sdk)\n    * [Haskell](#haskell-sdk)\n    * [OCaml](#ocaml-sdk)\n    * [Perl](#perl-sdk)\n    * [PHP](#php-sdk)\n    * [Python](#python-sdk)\n    * [Ruby](#ruby-sdk)\n    * [Scala](#scala-sdk)\n    * [Xamarin](#xamarin-sdk)\n    * [Unity](#unity-sdk)\n    * [.NET](#net-sdk)\n* [Command Line Tools](#command-line-tools)\n    * [Universal Command Line Interface](#universal-command-line-interface)\n    * [Windows PowerShell](#windows-powershell)\n* [IDE Toolkits](#ide-toolkits)\n    * [Eclipse Toolkit](#eclipse-toolkit)\n    * [Visual Studio Toolkit](#visual-studio-toolkit)\n* [Open Source Repos](#open-source-repos)\n    * [API Gateway](#api-gateway)\n    * [AppStream](#appstream)\n    * [CLI](#cli)\n    * [CloudFormation](#cloudformation)\n    * [CloudSearch](#cloudsearch)\n    * [CloudTrail](#cloudtrail)\n    * [CloudWatch](#cloudwatch)\n    * [Code Deploy](#code-deploy)\n    * [Code Pipeline](#code-pipeline)\n    * [Cognito](#cognito)\n    * [Data Pipeline](#data-pipeline)\n    * [Device Farm](#device-farm)\n    * [DynamoDB](#dynamodb)\n    * [Elastic Beanstalk](#elastic-beanstalk)\n    * [Elastic Container Service](#elastic-container-service)\n    * [Elastic File System](#elastic-file-system)\n    * [Elastic MapReduce](#elastic-mapreduce)\n    * [Elastic Search](#elastic-search)\n    * [Elasticache](#elasticache)\n    * [Glacier](#glacier)\n    * [Kinesis](#kinesis)\n    * [Lambda](#lambda)\n    * [Machine Learning](#machine-learning)\n    * [Mobile Analytics](#mobile-analytics)\n    * [OpsWorks](#opsworks)\n    * [Redshift](#redshift)\n    * [Route 53](#route-53)\n    * [S3](#s3)\n    * [SNS](#sns)\n    * [SQS](#sqs)\n    * [Data](#data)\n    * [DevOps](#devops)\n    * [Security](#security)\n    * [Accompanying](#accompanying-repos)\n    * [Miscellaneous](#miscellaneous-repos)\n* [Guides, Books, Documentation, and Training](#guides-books-documentation-and-training)\n    * [Getting Started Guides](#getting-started-guides)\n    * [General Guides](#general-guides)\n    * [Books](#books)\n    * [Whitepapers](#whitepapers)\n    * [Documentation](#documentation)\n    * [Training](#training)\n    * [Case Studies: Powered by AWS](#case-studies-powered-by-aws)\n* [Social](#social)\n    * [Blogs](#blogs)\n    * [Twitter Influencers](#twitter-influencers)\n    * [Facebook Pages](#facebook-pages)\n    * [YouTube Channels](#youtube-channels)\n    * [LinkedIn Groups](#linkedin-groups)\n    * [Subreddits](#subreddits)\n    * [Conferences](#conferences)\n* [Latest KPIs and Stats](#latest-kpis-and-stats)\n* [Appendix of Core Services](#appendix-of-core-services)\n    * [Services in Plain English](#services-in-plain-english)\n    * [Compute](#compute-services)\n    * [Networking](#networking-services)\n    * [Enterprise Applications](#enterprise-applications)\n    * [Analytics](#analytics-services)\n    * [Management Tools](#management-tools)\n    * [Security and Identity](#security-and-identity-services)\n    * [Internet of Things](#internet-of-things-service)\n    * [Mobile Services](#mobile-services)\n    * [Storage and Content Delivery](#storage-and-content-delivery-services)\n    * [Databases](#databases)\n    * [Application Services](#application-services)\n    * [Developer Tools](#developer-tools)\n    * [Miscellaneous Services](#miscellaneous-services)\n* [Contributing](#contributing)\n* [Credits](#credits)\n* [Other Awesome Lists](#other-awesome-lists)\n* [Contact Info](#contact-info)\n* [License](#license)\n\nChangelog\n=========\n\n0.1.0 (2015-12-21)\n------------------\n\n- Initial release.\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "CHANGELOG.md", "language": "markdown", "complexity": "complex", "star_count": 13620, "quality_score": 0.8, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 143}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributing Guidelines\n\nContributions are welcome!\n\nPlease ensure your pull request adheres to the following guidelines.\n\n## A Note on Repo AWSomeness\n\nEach repo listed meets at least one of the following requirements:\n\n* Community-authored repo with 100+ stars\n* Community-vouched repo with < 100 stars\n* Official repo from [aws](https://github.com/aws) or [awslabs](https://github.com/awslabs)\n\n100+ stars for community repos is not a strict requirement, it only serves as a guideline for the initial compilation.  If you can vouch for the awesomeness of a repo with < 100 stars and you can explain why it should be listed, please submit a pull request.\n\nPull requests might be left open for a period of time to let the community chime in and vouch for it.  An official repo from [aws](https://github.com/aws) or [awslabs](https://github.com/awslabs) can be removed if the community wishes.  Check out the [awesome manifesto](https://github.com/sindresorhus/awesome/blob/master/awesome.md).\n\n## Pull Requests\n\n* Search previous suggestions before making a new one, as yours may be a duplicate.\n* Add one link per pull request.\n* Add the link:\n    * `[name](http://example.com/) - A short description ends with a period.`\n    * Keep descriptions concise.\n    * Maintain alphabetical ordering where applicable.\n* Add a section if needed.\n    * Add the section description.\n    * Add the section title to the [Index](https://github.com/donnemartin/awesome-aws#index).\n* You generally do not need to mention `AWS` or `Amazon Web Services` in the description as it's implied.\n* Check your spelling and grammar.\n* Remove any trailing whitespace.\n* Send a pull request with the reason why the addition is awesome.\n* Use the following format for your pull request title:\n    * Add user/repo - Short repo description\n\n## Updates to Existing Links or Sections\n\n* Improvements to the existing sections are welcome.\n* If you think a listed link is **not** awesome, feel free to submit an issue or pull request to begin the discussion.\n\nThank you for your suggestions!\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "simple", "star_count": 13620, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 43}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# CREDITS\n\n## Content\n\n* [Awesome Python](https://github.com/vinta/awesome-python) as the template used to create this repo.\n* [Wikipedia](https://en.wikipedia.org/wiki/Amazon_Web_Services) for portions of the [Appendix of Core Services](#appendix-of-core-services).\n\n## Images\n\n* [Repo Header](https://aws.amazon.com)\n* [SDK Header](https://conceptdraw.com/a3125c3/p1/preview/640/pict--aws-architecture-diagram-icons-design-elements-aws-sdks)\n* [CLI Header](http://arcanesanctum.net/wp-content/uploads/hasher/terminal.jpg)\n* [Eclipse IDE Header](http://help.eclipse.org/juno/topic/org.eclipse.platform.doc.user/whatsNew/images/icon.png)\n* [Visual Studio IDE Header](http://www.iconarchive.com/download/i98390/dakirby309/simply-styled/Microsoft-Visual-Studio.ico)\n* [GitHub Header](http://itjobsco.com/wp-content/themes/wpjobus/images/github_logo.png)\n* [Innovation at Scale Header](https://d1.awsstatic.com/Digital%20Marketing/House/2up/products/quicksight/ha_2up_quicksight.png)\n* [Guides Header](http://d1.awsstatic.com/asset-repository/generics/whitepaper/editorial_whitepaper_green.png)\n* [Social Header](https://twitter.com/awscloud)\n* [Appendix Header](https://console.aws.amazon.com/)", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "CREDITS.md", "language": "markdown", "complexity": "simple", "star_count": 13620, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "I am providing code and resources in this repository to you under an open source\nlicense.  Because this is my personal repository, the license you receive to my\ncode and resources is from me and not my employer (Facebook).\n\nCopyright 2017 Donne Martin\n\nCreative Commons Attribution 4.0 International License (CC BY 4.0)\n\nhttp://creativecommons.org/licenses/by/4.0/\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "LICENSE.md", "language": "markdown", "complexity": "simple", "star_count": 13620, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 10}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# coding: utf-8\n\n# Copyright 2015 Donne Martin. All Rights Reserved.\n#\n# Creative Commons Attribution 4.0 International License (CC BY 4.0)\n# http://creativecommons.org/licenses/by/4.0/\n\nimport re\n\nimport click\nfrom githubcli.lib.github3 import null\n\nfrom awesome.lib.github import GitHub\n\n\nclass Awesome(object):\n    \"\"\"Awesomeness.\n\n    :type github: :class:`awesome.lib.GitHub`\n    :param github: Provides integration with the GitHub API.\n\n    :type output: list\n    :param output: The updated README content.\n\n    :type repos_broken: list\n    :param repos_broken: Dead repos found during parsing.\n\n    :type repos_exclude_score: list\n    :param repos_exclude_score: Repos to exclude in calculating the\n        Fiery Meter of AWSome score.\n\n    \"\"\"\n\n    def __init__(self, github=None):\n        self.github = github if github else GitHub()\n        self.output = []\n        self.repos_broken = []\n        self.repos_exclude_score = [\n            'https://github.com/donnemartin',\n            'https://github.com/donnemartin/awesome-aws',\n            'https://github.com/sindresorhus/awesome',\n            'https://github.com/kilimchoi/engineering-blogs',\n            'https://github.com/aws/aws-sdk-go/wiki',\n            '#',\n        ]\n\n    def extract_repo_params(self, url):\n        \"\"\"Extracts the user login and repo name from a repo url.\n\n        Expects the url to be valid of the form:\n        https://github.com/user/repo/...\n\n        :type regex_match: :class:`sre.SRE_Match`\n        :param regex_match:\n        \"\"\"\n        tokens = url.split('/')\n        POS_USER_ID = 3\n        POS_REPO_NAME = 4\n        return tokens[POS_USER_ID], tokens[POS_REPO_NAME]\n\n    def print_repos_broken(self):\n        \"\"\"Prints out any broken repos.\"\"\"\n        if self.repos_broken:\n            click.secho('Broken repos:', fg='red')\n            for repo in self.repos_broken:\n                click.secho('  ' + repo, fg='red')\n\n    def process_line(self, line):\n        \"\"\"Processes each line in the README.\n\n        :type line: str\n        :param line: The current line in the README.\n        \"\"\"\n        match = re.search(r'(https://github.com/[^)]*)', line)\n        # If we're not processing a repo, just output the line\n        if match is None:\n            self.output.append(line)\n            return\n        # If the repo is in the score exclude list, just output the line\n        if any(substr in match.group(0) for substr in self.repos_exclude_score):\n            self.output.append(line)\n            return\n        user_login, repo_name = self.extract_repo_params(match.group(0))\n        repo = self.github.api.repository(user_login, repo_name)\n        # Tag any broken repos\n        if type(repo) is null.NullObject:\n            self.repos_broken.append(match.group(0))\n            return\n        line = self.update_repo_score(repo, line)\n        self.output.append(line)\n\n    def rock_it(self, readme_path, result_path=None):\n        \"\"\"Processes the README.\n\n        :type readme_path: str\n        :param readme_path: The README file path.\n\n        :type result_path: str\n        :param result_path: The file path of where to output the results.\n        \"\"\"\n        with open(readme_path, 'r') as f:\n            for line in f:\n                line = line.strip('\\n')\n                self.process_line(line)\n        if result_path is None:\n            result_path = readme_path\n        self.write_output(result_path)\n        self.print_repos_broken()\n        click.secho('Rate limit: ' + str(self.github.api.ratelimit_remaining),\n                    fg='blue')\n        click.secho('Updated ' + result_path, fg='blue')\n\n    def score_repo(self, stars, cached_score):\n        \"\"\"Assigns the Fiery Meter of AWSome score.\n\n        :type stars: int\n        :param stars: The number of repo stars.\n\n        :type cached_score: int\n        :param cached_score: The previouslya assigned score.\n        \"\"\"\n        score = cached_score\n        if stars < 100:\n            score = 0 if score != 0 else cached_score\n        elif stars < 200:\n            score = 1 if score != 1 else cached_score\n        elif stars < 500:\n            score = 2 if score != 2 else cached_score\n        elif stars < 1000:\n            score = 3 if score != 3 else cached_score\n        elif stars < 2000:\n            score = 4 if score != 4 else cached_score\n        else:\n            score = 5 if score != 5 else cached_score\n        return score\n\n    def update_repo_score(self, repo, line):\n        \"\"\"Updates the repo's markdown, given its new score.\n\n        :type repo: :class:`github3.repos.repo.Repository`\n        :param repo: Contains repo information.\n\n        :type line: str\n        :param line: The current line of the README.\n        \"\"\"\n        stars = repo.stargazers_count\n        cached_score = line.count(':fire:')\n        score = self.score_repo(stars, cached_score)\n        if score != cached_score:\n            prefix = ''\n            if cached_score == 0:\n                prefix = ' '\n            cached_fires = ':fire:' * cached_score\n            fires = ':fire:' * score\n            line = line.replace(cached_fires + ']', prefix + fires + ']')\n        return line\n\n    def write_output(self, result_path):\n        \"\"\"Writes the results to the result_path.\n\n        :type result_path: str\n        :param result_path: The file path specifying where to write the output.\n        \"\"\"\n        with open(result_path, 'w') as f:\n            for output in self.output:\n                f.write(output + '\\n')\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "awesome/awesome.py", "language": "python", "complexity": "complex", "star_count": 13620, "quality_score": 0.9500000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 167}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\ngitchangelog > CHANGELOG_DRAFT\npandoc --from=markdown --to=rst --output=CHANGELOG.rst CHANGELOG.md\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "scripts/create_changelog.sh", "language": "bash", "complexity": "simple", "star_count": 13620, "quality_score": 0.6000000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 5}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\npandoc --from=markdown --to=rst --output=docs/source/README.rst README.md", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "scripts/create_readme_rst.sh", "language": "bash", "complexity": "simple", "star_count": 13620, "quality_score": 0.6000000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 3}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from awesome.__init__ import __version__\ntry:\n    from setuptools import setup, find_packages\nexcept ImportError:\n    from distutils.core import setup\n\nsetup(\n    description='A curated list of awesome Amazon Web Services (AWS) libraries, open source repos, guides, blogs, and other resources.',\n    author='Donne Martin',\n    url='https://github.com/donnemartin/awesome-aws',\n    download_url='https://github.com/donnemartin/awesome-aws',\n    author_email='donne.martin@gmail.com',\n    version=__version__,\n    license='Creative Commons Attribution 4.0',\n    install_requires=[\n        'click>=5.1',\n        'githubcli>=0.1.0',\n        'colorama>=0.3.3',\n    ],\n    extras_require={\n        'testing': [\n            'mock>=1.0.1',\n            'tox>=1.9.2'\n        ],\n    },\n    entry_points={\n        'console_scripts': 'awesome = awesome.awesome_cli:AwesomeCli.cli'\n    },\n    packages=find_packages(),\n    scripts=[],\n    name='awesome-aws',\n    classifiers=[\n        'Intended Audience :: Developers',\n        'Intended Audience :: System Administrators',\n        'Natural Language :: English',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Topic :: Software Development',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n    ],\n)\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "setup.py", "language": "python", "complexity": "simple", "star_count": 13620, "quality_score": 0.89, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 47}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## The Fiery Meter of AWSome\n* [user/foo](https://github.com/user/foo) - desc\n* [user/bar](https://github.com/user/bar) - desc\n* [user/baz :fire:](https://github.com/user/baz) - desc\n* [user/qux :fire::fire:](https://github.com/user/qux) - desc\n* [user/foobar :fire:](https://github.com/user/foobar) - desc\n* [user/bazqux :fire::fire::fire:](https://github.com/user/bazqux) - desc\n* [user/exclude](https://github.com/donnemartin) - desc\n* [user/exclude](https://github.com/donnemartin/awesome-aws) - desc\n* [user/exclude](https://github.com/sindresorhus/awesome) - desc\n* [user/exclude](https://github.com/kilimchoi/engineering-blogs) - desc\n* [user/exclude](https://github.com/user/foo#)\n* [user/broken](https://github.com/user/broken)\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "tests/data/README.md", "language": "markdown", "complexity": "simple", "star_count": 13620, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 14}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## The Fiery Meter of AWSome\n* [user/foo](https://github.com/user/foo) - desc\n* [user/bar :fire:](https://github.com/user/bar) - desc\n* [user/baz :fire::fire:](https://github.com/user/baz) - desc\n* [user/qux :fire::fire::fire:](https://github.com/user/qux) - desc\n* [user/foobar :fire::fire::fire::fire:](https://github.com/user/foobar) - desc\n* [user/bazqux :fire::fire::fire::fire::fire:](https://github.com/user/bazqux) - desc\n* [user/exclude](https://github.com/donnemartin) - desc\n* [user/exclude](https://github.com/donnemartin/awesome-aws) - desc\n* [user/exclude](https://github.com/sindresorhus/awesome) - desc\n* [user/exclude](https://github.com/kilimchoi/engineering-blogs) - desc\n* [user/exclude](https://github.com/user/foo#)\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "tests/data/README_RESULT.md", "language": "markdown", "complexity": "simple", "star_count": 13620, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 13}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# coding: utf-8\n\n# Copyright 2015 Donne Martin. All Rights Reserved.\n#\n# Creative Commons Attribution 4.0 International License (CC BY 4.0)\n# http://creativecommons.org/licenses/by/4.0/\n\nreadme_expected = [\n    '## The Fiery Meter of AWSome\\n',\n    '* [user/foo](https://github.com/user/foo) - desc\\n',\n    '* [user/bar :fire:](https://github.com/user/bar) - desc\\n',\n    '* [user/baz :fire::fire:](https://github.com/user/baz) - desc\\n',\n    '* [user/qux :fire::fire::fire:](https://github.com/user/qux) - desc\\n',\n    '* [user/foobar :fire::fire::fire::fire:](https://github.com/user/foobar) - desc\\n',\n    '* [user/bazqux :fire::fire::fire::fire::fire:](https://github.com/user/bazqux) - desc\\n',\n    '* [user/exclude](https://github.com/donnemartin) - desc\\n',\n    '* [user/exclude](https://github.com/donnemartin/awesome-aws) - desc\\n',\n    '* [user/exclude](https://github.com/sindresorhus/awesome) - desc\\n',\n    '* [user/exclude](https://github.com/kilimchoi/engineering-blogs) - desc\\n',\n    '* [user/exclude](https://github.com/user/foo#)\\n',\n]\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "tests/data/readme_expected.py", "language": "python", "complexity": "simple", "star_count": 13620, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 22}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n\n# Copyright 2015 Donne Martin. All Rights Reserved.\n#\n# Creative Commons Attribution 4.0 International License (CC BY 4.0)\n# http://creativecommons.org/licenses/by/4.0/\n\nimport mock\nimport os\nimport unittest\n\nfrom awesome.awesome import Awesome\nfrom tests.data.readme_expected import readme_expected\nfrom tests.mock_github import MockGitHub, MockRepo\n\n\nclass AwesomeTest(unittest.TestCase):\n\n    def setUp(self):\n        self.repos = self.create_repos()\n        self.github = MockGitHub(self.repos)\n        self.awesome = Awesome(self.github)\n\n    def create_repos(self):\n        return [\n            MockRepo('foo', 10),\n            MockRepo('bar', 150),\n            MockRepo('baz', 270),\n            MockRepo('qux', 550),\n            MockRepo('foobar', 1200),\n            MockRepo('bazqux', 9000),\n            MockRepo('awesome-aws', 9000),\n        ]\n\n    @mock.patch('awesome.awesome.click')\n    def test_rock_it(self, mock_click):\n        README = os.path.join(os.path.dirname(__file__),\n                              'data/README.md')\n        README_RESULT = os.path.join(os.path.dirname(__file__),\n                                     'data/README_RESULT.md')\n        self.awesome.rock_it(README, README_RESULT)\n        result = []\n        with open(README_RESULT, 'r') as f:\n            for line in f:\n                result.append(line)\n        assert result == readme_expected\n        assert mock.call.secho('Broken repos:', fg='red') \\\n            in mock_click.mock_calls\n        assert mock.call.secho('  https://github.com/user/broken', fg='red') \\\n            in mock_click.mock_calls\n        assert mock.call.secho('Rate limit: 9000', fg='blue') \\\n            in mock_click.mock_calls\n        assert mock.call.secho('Updated ' + README_RESULT, fg='blue') \\\n            in mock_click.mock_calls\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "tests/test_awesome.py", "language": "python", "complexity": "simple", "star_count": 13620, "quality_score": 0.98, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 55}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "templates:\n  - tests/translator/output/**/*.json\nignore_templates:\n  - tests/translator/output/**/error_*.json # Fail by design\n  - tests/translator/output/**/api_http_paths_with_if_condition.json\n  - tests/translator/output/**/api_http_paths_with_if_condition_no_value_else_case.json\n  - tests/translator/output/**/api_http_paths_with_if_condition_no_value_then_case.json\n  - tests/translator/output/**/api_http_with_default_iam_authorizer.json\n  - tests/translator/output/**/api_rest_paths_with_if_condition_openapi.json\n  - tests/translator/output/**/api_rest_paths_with_if_condition_openapi_no_value_else_case.json\n  - tests/translator/output/**/api_rest_paths_with_if_condition_openapi_no_value_then_case.json\n  - tests/translator/output/**/api_rest_paths_with_if_condition_swagger.json\n  - tests/translator/output/**/api_rest_paths_with_if_condition_swagger_no_value_else_case.json\n  - tests/translator/output/**/api_rest_paths_with_if_condition_swagger_no_value_then_case.json\n  - tests/translator/output/**/api_with_any_method_in_swagger.json\n  - tests/translator/output/**/api_with_auth_and_conditions_all_max.json\n  - tests/translator/output/**/api_with_basic_custom_domain.json\n  - tests/translator/output/**/api_with_basic_custom_domain_http.json\n  - tests/translator/output/**/api_with_basic_custom_domain_intrinsics.json\n  - tests/translator/output/**/api_with_basic_custom_domain_intrinsics_http.json\n  - tests/translator/output/**/api_with_binary_media_types.json\n  - tests/translator/output/**/api_with_binary_media_types_definition_body.json\n  - tests/translator/output/**/api_with_canary_setting.json\n  - tests/translator/output/**/api_with_cors_and_conditions_no_definitionbody.json\n  - tests/translator/output/**/api_with_custom_base_path.json\n  - tests/translator/output/**/api_with_custom_domain_route53.json\n  - tests/translator/output/**/api_with_custom_domain_route53_hosted_zone_name.json\n  - tests/translator/output/**/api_with_custom_domain_route53_hosted_zone_name_http.json\n  - tests/translator/output/**/api_with_custom_domain_route53_http.json\n  - tests/translator/output/**/api_with_custom_domain_route53_multiple_intrinsic_hostedzoneid.json\n  - tests/translator/output/**/api_with_identity_intrinsic.json\n  - tests/translator/output/**/api_with_if_conditional_with_resource_policy.json\n  - tests/translator/output/**/api_with_resource_policy_global.json\n  - tests/translator/output/**/api_with_security_definition_and_none_components.json\n  - tests/translator/output/**/api_with_source_vpc_whitelist.json\n  - tests/translator/output/**/api_with_usageplans.json\n  - tests/translator/output/**/api_with_usageplans_intrinsics.json\n  - tests/translator/output/**/api_with_usageplans_shared_attributes_three.json\n  - tests/translator/output/**/api_with_usageplans_shared_attributes_two.json\n  - tests/translator/output/**/api_with_usageplans_shared_no_side_effect_1.json\n  - tests/translator/output/**/api_with_usageplans_shared_no_side_effect_2.json\n  - tests/translator/output/**/application_with_intrinsics.json\n  - tests/translator/output/**/basic_function_withimageuri.json\n  - tests/translator/output/**/basic_layer.json\n  - tests/translator/output/**/cloudwatchevent_intrinsics.json\n  - tests/translator/output/**/congito_userpool_with_sms_configuration.json\n  - tests/translator/output/**/connector_bucket_to_function.json\n  - tests/translator/output/**/connector_dependson_replace.json\n  - tests/translator/output/**/connector_esm_dependson.json\n  - tests/translator/output/**/connector_function_to_sqs.json\n  - tests/translator/output/**/connector_function_to_table.json\n  - tests/translator/output/**/connector_hardcoded_lambda_arn.json\n  - tests/translator/output/**/connector_hardcoded_props.json\n  - tests/translator/output/**/connector_rule_to_eventbus.json\n  - tests/translator/output/**/connector_rule_to_sfn.json\n  - tests/translator/output/**/connector_rule_to_sns.json\n  - tests/translator/output/**/connector_sfn_to_function.json\n  - tests/translator/output/**/connector_sns_to_function.json\n  - tests/translator/output/**/connector_table_to_function.json\n  - tests/translator/output/**/documentdb_with_intrinsics.json  # TODO: remove once DocumentDDB is available\n  - tests/translator/output/**/eventbridgerule_with_dlq.json\n  - tests/translator/output/**/function_event_conditions.json\n  - tests/translator/output/**/function_with_alias_and_code_sha256.json\n  - tests/translator/output/**/function_with_alias_intrinsics.json\n  - tests/translator/output/**/function_with_condition.json\n  - tests/translator/output/**/function_with_conditional_managed_policy.json\n  - tests/translator/output/**/function_with_conditional_managed_policy_and_ref_no_value.json\n  - tests/translator/output/**/function_with_conditional_policy_template.json\n  - tests/translator/output/**/function_with_conditional_policy_template_and_ref_no_value.json\n  - tests/translator/output/**/function_with_custom_codedeploy_deployment_preference.json\n  - tests/translator/output/**/function_with_deployment_no_service_role_with_passthrough.json\n  - tests/translator/output/**/function_with_deployment_no_service_role_without_passthrough.json\n  - tests/translator/output/**/function_with_deployment_preference.json\n  - tests/translator/output/**/function_with_deployment_preference_condition_with_passthrough.json\n  - tests/translator/output/**/function_with_deployment_preference_condition_without_passthrough.json\n  - tests/translator/output/**/function_with_deployment_preference_from_parameters.json\n  - tests/translator/output/**/function_with_deployment_preference_multiple_combinations_conditions_with_passthrough.json\n  - tests/translator/output/**/function_with_deployment_preference_multiple_combinations_conditions_without_passthrough.json\n  - tests/translator/output/**/function_with_deployment_preference_passthrough_condition_with_supported_intrinsics.json\n  - tests/translator/output/**/function_with_dlq.json\n  - tests/translator/output/**/function_with_documentdb_with_kms.json  # TODO: remove once DocumentDDB is available\n  - tests/translator/output/**/function_with_documentdb.json  # TODO: remove once DocumentDDB is available\n  - tests/translator/output/**/function_with_event_dest.json\n  - tests/translator/output/**/function_with_event_dest_basic.json\n  - tests/translator/output/**/function_with_event_dest_conditional.json\n  - tests/translator/output/**/function_with_event_schedule_state.json\n  - tests/translator/output/**/function_with_file_system_config.json\n  - tests/translator/output/**/function_with_event_filtering.json  # TODO: remove once Event's KmsKeyArn is available\n  - tests/translator/output/**/function_with_function_url_config_conditions.json\n  - tests/translator/output/**/function_with_globals_role_path.json\n  - tests/translator/output/**/function_with_intrinsic_architecture.json\n  - tests/translator/output/**/function_with_kmskeyarn.json\n  - tests/translator/output/**/function_with_resource_refs.json\n  - tests/translator/output/**/function_with_role_and_role_path.json\n  - tests/translator/output/**/function_with_role_path.json\n  - tests/translator/output/**/http_api_custom_iam_auth.json\n  - tests/translator/output/**/http_api_existing_openapi.json\n  - tests/translator/output/**/http_api_existing_openapi_conditions.json\n  - tests/translator/output/**/http_api_explicit_stage.json\n  - tests/translator/output/**/http_api_global_iam_auth_enabled.json\n  - tests/translator/output/**/http_api_local_iam_auth_enabled.json\n  - tests/translator/output/**/http_api_with_cors.json\n  - tests/translator/output/**/implicit_and_explicit_api_with_conditions.json\n  - tests/translator/output/**/implicit_api_with_auth_and_conditions_max.json\n  - tests/translator/output/**/implicit_api_with_many_conditions.json\n  - tests/translator/output/**/implicit_http_api_with_many_conditions.json\n  - tests/translator/output/**/intrinsic_functions.json\n  - tests/translator/output/**/kinesis_intrinsics.json\n  - tests/translator/output/**/layers_all_properties.json\n  - tests/translator/output/**/layers_with_intrinsics.json\n  - tests/translator/output/**/s3_create_remove.json\n  - tests/translator/output/**/s3_intrinsics.json\n  - tests/translator/output/**/schema_validation_1.json\n  - tests/translator/output/**/self_managed_kafka_with_intrinsics.json\n  - tests/translator/output/**/sqs_with_scaling_config.json  # Invalid Property Resources/SQSFunctionMySqsQueue/Properties/ScalingConfig\n  - tests/translator/output/**/state_machine_with_condition.json\n  - tests/translator/output/**/state_machine_with_condition_and_events.json\n  - tests/translator/output/**/state_machine_with_eb_dlq_target_id.json\n  - tests/translator/output/**/state_machine_with_event_schedule_state.json\n  - tests/translator/output/**/state_machine_with_schedule.json\n  - tests/translator/output/**/state_machine_with_schedule_dlq_retry_policy.json\n  - tests/translator/output/**/state_machine_with_auto_publish_alias.json\n  - tests/translator/output/**/state_machine_with_deployment_preference_all_at_once.json\n  - tests/translator/output/**/state_machine_with_deployment_preference_canary.json\n  - tests/translator/output/**/state_machine_with_deployment_preference_linear.json\n  - tests/translator/output/**/state_machine_with_deletion_policy.json\n  - tests/translator/output/**/state_machine_with_update_replace_policy.json\n  - tests/translator/output/**/globals_for_function.json  # RuntimeManagementConfig\n  - tests/translator/output/**/function_with_runtime_config.json  # RuntimeManagementConfig\n  - tests/translator/output/**/managed_policies_minimal.json  # Intentionally has non-existent managed policy name\n  - tests/translator/output/**/function_with_mq.json  # Property \"EventSourceArn\" can Fn::GetAtt to a resource of types [AWS::DynamoDB::GlobalTable, AWS::DynamoDB::Table, AWS::Kinesis::Stream, AWS::Kinesis::StreamConsumer, AWS::SQS::Queue]\n  - tests/translator/output/**/function_with_mq_using_autogen_role.json  # Property \"EventSourceArn\" can Fn::GetAtt to a resource of types [AWS::DynamoDB::GlobalTable, AWS::DynamoDB::Table, AWS::Kinesis::Stream, AWS::Kinesis::StreamConsumer, AWS::SQS::Queue]\n  - tests/translator/output/**/function_with_recursive_loop.json  # Invalid Property Resources/RecursiveLoopParameterFunction/Properties/RecursiveLoop\n  - tests/translator/output/**/function_with_sourcekmskeyarn.json  # Invalid Property Resources/SourceKMSKeyArnParameterFunction/Properties/SourceKMSKeyArn\n  - tests/translator/output/**/function_with_tracing.json # Obsolete DependsOn on resource\n  - tests/translator/output/**/api_with_propagate_tags.json # TODO: Intentional error transform tests. Will be updated.\n  - tests/translator/output/**/function_with_intrinsics_resource_attribute.json # CFN now supports intrinsics in DeletionPolicy\n  - tests/translator/output/**/function_with_snapstart.json # Snapstart intentionally not attached to a lambda version which causes lint issues\n  - tests/translator/output/**/managed_policies_everything.json # intentionally contains wrong arns\n  - tests/translator/output/**/function_with_provisioned_poller_config.json\n  - tests/translator/output/**/function_with_metrics_config.json\n  - tests/translator/output/**/function_with_self_managed_kafka_and_schema_registry.json # cfnlint is not updated to recognize the SchemaRegistryConfig property\n  - tests/translator/output/**/function_with_msk_with_schema_registry_config.json # cfnlint is not updated to recognize the SchemaRegistryConfig property\n\nignore_checks:\n  - E2531 # Deprecated runtime; not relevant for transform tests\n  - E2533 # Another deprecated runtime; not relevant for transform tests\n  - W2531 # EOL runtime; not relevant for transform tests\n  - E3001 # Invalid or unsupported Type; common in transform tests since they focus on SAM resources\n  - W2001 # Parameter not used\n  - E3006 # Resource type check; we have some Foo Bar resources\n  - W3037 # Ignore cfn-lint check for non existing IAM permissions\n", "metadata": {"source": "github.com/aws/serverless-application-model", "file_path": ".cfnlintrc.yaml", "language": "yaml", "complexity": "complex", "star_count": 9468, "quality_score": 0.73, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 153}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Bug report\nabout: Report a bug concerning the AWS SAM transform\ntitle: ''\nlabels: stage/needs-triage\nassignees: ''\n\n---\n\n<!-- Make sure we don't have an existing issue that reports the bug you are encountering (both open and closed).\nIf you do find an existing issue, re-open or add a comment to that issue instead of creating a new one.\nAdd the --debug flag to any SAM CLI commands you are running. -->\n\n### Description\n<!-- Briefly describe the bug you are facing.-->\n\n\n### Steps to reproduce\n<!-- Provide detailed steps to reproduce the bug. Include relevant parts of the SAM template. -->\n\n\n### Observed result\n<!-- Describe what you observed. -->\n\n\n### Expected result\n<!-- Describe what you expected. -->\n\n\n### Additional environment details\n\n1. OS:\n2. If using the [SAM CLI](https://github.com/aws/aws-sam-cli), `sam --version`:\n3. AWS region:\n", "metadata": {"source": "github.com/aws/serverless-application-model", "file_path": ".github/ISSUE_TEMPLATE/bug_report.md", "language": "markdown", "complexity": "simple", "star_count": 9468, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 35}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "### Issue #, if available\n\n### Description of changes\n\n### Description of how you validated changes\n\n### Checklist\n\n- [ ] Adheres to the [development guidelines](https://github.com/aws/serverless-application-model/blob/develop/DEVELOPMENT_GUIDE.md#development-guidelines)\n- [ ] Add/update [transform tests](https://github.com/aws/serverless-application-model/blob/develop/DEVELOPMENT_GUIDE.md#unit-testing-with-multiple-python-versions)\n    - [ ] Using correct values\n    - [ ] Using wrong values\n- [ ] Add/update [integration tests](https://github.com/aws/serverless-application-model/blob/develop/INTEGRATION_TESTS.md)\n\n### Examples?\n\nPlease reach out in the comments if you want to add an example. Examples will be \nadded to `sam init` through [aws/aws-sam-cli-app-templates](https://github.com/aws/aws-sam-cli-app-templates).\n\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\n", "metadata": {"source": "github.com/aws/serverless-application-model", "file_path": ".github/PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 9468, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: 2\nupdates:\n\n  - package-ecosystem: \"github-actions\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n    commit-message:\n      prefix: chore\n      include: scope\n    reviewers:\n      - aws/sae-enterprise\n      - aws/aws-lambda-tooling\n    open-pull-requests-limit: 10\n", "metadata": {"source": "github.com/aws/serverless-application-model", "file_path": ".github/dependabot.yml", "language": "yaml", "complexity": "simple", "star_count": 9468, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Closed issue message\n\non:\n  issues:\n    types: [ closed ]\njobs:\n  auto_comment:\n    runs-on: ubuntu-latest\n    permissions:\n      issues: write\n    steps:\n      - uses: aws-actions/closed-issue-message@v2\n        with:\n          # These inputs are both required\n          repo-token: \"${{ secrets.GITHUB_TOKEN }}\"\n          message: |\n            This issue is now closed. Comments on closed issues are hard for our team to see. \n            If you need more assistance, please either tag a team member or open a new issue that references this one. ", "metadata": {"source": "github.com/aws/serverless-application-model", "file_path": ".github/workflows/close_issue_message.yml", "language": "yaml", "complexity": "simple", "star_count": 9468, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Update schema\n\non:\n  workflow_dispatch:\n  schedule:\n    - cron: \"0 16 * * 2\" # Every Tuesday at 4pm UTC (8am PST, 9am PDT)\n\njobs:\n  update:\n    if: github.repository == 'aws/serverless-application-model'\n    runs-on: ubuntu-latest\n\n    permissions:\n      contents: write\n      pull-requests: write\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.10\"\n\n      - name: Update schema\n        id: schema\n        run: |\n          make init\n          make schema-all\n          # Sets condition steps.schema.outputs.changed to true if anything changed\n          git diff --exit-code || echo \"changed=true\" >> $GITHUB_OUTPUT\n    \n      - name: Push change to temporary branch\n        if: steps.schema.outputs.changed\n        run: |\n          git config user.name github-actions\n          git config user.email github-actions@github.com\n          git checkout -b tmp/schema/$GITHUB_RUN_ID/$GITHUB_RUN_ATTEMPT\n          git add -u\n          git commit -m \"chore(schema): update\"\n          git push --set-upstream origin tmp/schema/$GITHUB_RUN_ID/$GITHUB_RUN_ATTEMPT\n\n      - name: Create PR\n        if: steps.schema.outputs.changed\n        run: printf '> **Note**\\n> If checks do not start, close then reopen this pull request.\\n\\nCreated by the [`schema.yml`](https://github.com/aws/serverless-application-model/blob/develop/.github/workflows/schema.yml) workflow.\\n' | gh pr create --fill --base develop --body-file -\n        env:\n          GH_TOKEN: ${{ github.token }}\n", "metadata": {"source": "github.com/aws/serverless-application-model", "file_path": ".github/workflows/schema.yml", "language": "yaml", "complexity": "simple", "star_count": 9468, "quality_score": 0.73, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 46}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## Code of Conduct\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct). \nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact \nopensource-codeofconduct@amazon.com with any additional questions or comments.\n", "metadata": {"source": "github.com/aws/serverless-application-model", "file_path": "CODE_OF_CONDUCT.md", "language": "markdown", "complexity": "simple", "star_count": 9468, "quality_score": 0.49999999999999994, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 5}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "infrastructure_as_code", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create infrastructure as code for cloud resource provisioning\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributing Guidelines\n\nInterested in contributing to the AWS Serverless Application Model (AWS SAM)?\nAwesome! Read this document to understand how to report issues, contribute\nfeatures and participate in the development process. We want to create a\ntransparent and open process for evolving AWS SAM.\n\nPlease read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.\n\n## Integrating AWS SAM into your tool\n\nWe encourage you to modify SAM to integrate it with other frameworks and deployment providers from the community for building serverless applications. If you're building a new tool that will use AWS SAM, let us know how we can help!\n\n## Submitting feature requests\n\nWe love features that help developers build serverless applications faster and further simplify development of serverless applications. If you have ideas for new event sources, new resource types, and new parameters within SAM, follow these steps to submit your feature request:\n\n1. Scan through the list\n   of\n   [feature requests](https://github.com/aws/serverless-application-model/discussions/categories/ideas).\n   If you find your feature there, mark it +1. This is a good indication of\n   interest from the community.\n2. If you don't find your feature in our backlog, [create a new discussion](https://github.com/aws/serverless-application-model/discussions/new?category=ideas) for it.\n\nKeep in mind that features should be driven by real use cases. Answer the\nfollowing questions in the issue that you create:\n\n- Does this feature simplify creating and deploying some aspect of a serverless application?\n- Is this feature solving a problem faced by other serverless application developers?\n- How do developers work around this problem today?\n- How is the proposed feature better than the work around?\n\n## Adding features to AWS SAM\n\nWe welcome pull requests to add new features to AWS SAM. Take a look at the\nbacklog of [Feature Requests](https://github.com/awslabs/serverless-application-specification/labels/feature-request) and pick an item that you find interesting. If the requirements have been well-scoped, feel free to make the change and send a pull request.\n\nIf you don't find an item tracking what you have in mind, start a conversation with your plan for implementing\nthe feature. When defining your feature, keep in mind that one of the core\ntenets of AWS SAM is to keep it easy to use while allowing customers access to\nuse more advanced components, should they so choose.\n\nThis repository contains the SAM specification, the code that translates SAM templates into AWS CloudFormation stacks, general information about the model, and examples of common applications. Make enhancements to all of SAM: if you make a change to the specification, please also make the corresponding change to the implementation.\n\nHere are some questions that you should answer in your plan:\n\n- **What is the problem you are solving?**\n\n    Example: Creating API endpoints require 24 steps, most of which are boilerplate and repetitive.\n\n- **Describe persona of someone who is facing this problem? This will give you\n    an understanding of how big of a problem it actually is.**\n\n    Example: John just finished coding bootcamp and wants to create a Serverless app. He has\n    basic coding skills and will be comfortable understanding infrastructure\n    concepts, but probably be intimidated by the 24 steps required to create an\n    API endpoint.\n\n- **How do developers work around this problem today?**\n\n    Example: Manually click through every step on the website while referring to\n    \"How To\" resources on the internet.\n\n- **Describe your proposed solution?**\n\n    Example: We are creating a new AWS SAM resource called \"API\". Here is how it\n    will look:\n\n    ```yaml\n    Type: 'AWS::Serverless::Api'\n    Properties:\n        # Name of API endpoint\n        Name: <string>\n        # Path to their endpoint. Example: /hello\n        Path: <string>\n        # HTTP Method for their endpoint. Example: GET, POST etc\n        Method: <string>\n    ```\n\n- **How is the proposed feature better than what the work around?**\n\n    Example: Developers can write a simple AWS SAM resource which will handle\n    generating the boilerplate for them.\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check [existing open](https://github.com/awslabs/serverless-application-model/issues), or [recently closed](https://github.com/awslabs/serverless-application-model/issues?utf8=%E2%9C%93&q=is%3Aissue%20is%3Aclosed%20), issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Contributing via Pull Requests\n\nContributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:\n\n1. You are working against the latest source on the *develop* branch.\n2. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.\n3. You open an issue to discuss any significant work - we would hate for your time to be wasted.\n4. You propose complete changes - if you make a change to the specification, please also make the corresponding change to the implementation.\n\nTo send us a pull request, please:\n\n1. Fork the repository.\n2. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.\n3. Ensure local tests pass.\n4. Commit to your fork using clear commit messages.\n5. Send us a pull request, answering any default questions in the pull request interface.\n6. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.\n\nGitHub provides additional document on [forking a repository](https://help.github.com/articles/fork-a-repo/) and [creating a pull request](https://help.github.com/articles/creating-a-pull-request/).\n\n## Finding contributions to work on\n\nLooking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels ((enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any ['help wanted'](https://github.com/awslabs/PRIVATE-aws-sam-development/labels/help%20wanted) issues is a great place to start.\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct). For more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact opensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public GitHub issue.\n\n## Licensing\n\nSee the [LICENSE](https://github.com/awslabs/PRIVATE-aws-sam-development/blob/master/LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "metadata": {"source": "github.com/aws/serverless-application-model", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "medium", "star_count": 9468, "quality_score": 0.73, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 134}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS - Cloud Practitioner\n\nA summary of what you need to know for the exam can be found [here](https://aws.amazon.com/certification/certified-cloud-practitioner/)\n\n#### Cloud 101\n\n<details>\n<summary>What is cloud computing?</summary><br><b>\n\n[Wikipedia](https://en.wikipedia.org/wiki/Cloud_computing): \"Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user\"\n\nCloud computing also allows you to scale resources up or down as needed, paying only for what you use.\n</b></details>\n\n<details>\n<summary>What types of Cloud Computing services are there?</summary><br><b>\n\nIAAS  \nPAAS  \nSAAS\n</b></details>\n\n<details>\n<summary>Explain each of the following and give an example:\n\n  * IAAS\n  * PAAS\n  * SAAS</summary><br><b>\n\n- IAAS - Infrastructure As A Service is a cloud computing service where a cloud provider rents out IT infrastructure such as compute, networking resources and storage over the internet (e.g., AWS EC2).<br>\n\n- PAAS - Platform As A Service is a cloud hosting platform with an on-demand access to ready-to-use set of deployment, application management and DevOps tools (e.g., AWS Elastic Beanstalk).<br>\n\n- SAAS - Software As A Service is a software distribution model in which services are hosted by a cloud service provider (e.g., AWS WorkSpaces or any web-based email service).\n</b></details>\n\n<details>\n<summary>What types of clouds (or cloud deployments) are there?</summary><br><b>\n\n  * Public\n  * Hybrid\n  * Private\n</b></details>\n\n<details>\n<summary>Explain each of the following Cloud Computing Deployments:\n\n  * Public\n  * Hybrid\n  * Private</summary><br><b>\n\n- Public - Public cloud is when you leverage cloud services over the open internet on hardware owned by the cloud provider, but its usage is shared by other companies. It offers cost-effectiveness and ease of scaling.<br>\n\n- Hybrid - A hybrid cloud is a cloud computing environment that uses a mix of combining a public and private cloud environment, like an on-premises data center, and public CSPs. It provides greater flexibility and more deployment options.<br>\n\n- Private - Private cloud means that the cloud infrastructure is provisioned for exclusive use by a single organization. Resources are not shared with others, so it offers more control over security and data.\n  \n[Read more](https://aws.amazon.com/types-of-cloud-computing/)\n</b></details>\n\n#### AWS Global Infrastructure\n\n<details>\n<summary>Explain the following\n\n  * Availability zone\n  * Region\n  * Edge location</summary><br><b>\n\nAWS regions are data centers hosted across different geographical locations worldwide, each region is completely independent of one another.<br>\n\nWithin each region, there are multiple isolated locations known as Availability Zones. Multiple availability zones ensure high availability in case one of them goes down. Each Availability Zone is physically separated from others, with its own power, networking, and connectivity.<br>\n\nEdge locations are basically content delivery network endpoints which cache data and ensure lower latency and faster delivery to the users in any location. They are located in major cities around the world.\n</b></details>\n\n#### AWS Networking\n\n<details>\n<summary>What is VPC?</summary><br><b>\n\n\"A logically isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define\".\nRead more about it [here](https://aws.amazon.com/vpc).\n\nA VPC spans all the Availability Zones within a single region.\n</b></details>\n\n<details>\n<summary>True or False? VPC spans multiple regions</summary><br><b>\n\nFalse. A VPC is region-specific and cannot span multiple regions.\n</b></details>\n\n<details>\n<summary>True or False? Subnets belong to the same VPC, can be in different availability zones</summary><br><b>\n\nTrue. Just to clarify, a subnet must reside entirely in one AZ, but a single VPC can contain subnets across multiple AZs.\n</b></details>\n\n<details>\n<summary>What is an Internet Gateway?</summary><br><b>\n\n\"component that allows communication between instances in your VPC and the internet\" (AWS docs).\nRead more about it [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html)\n\nIt scales horizontally and is highly available, allowing inbound and outbound traffic to flow without imposing availability risks or bandwidth constraints.\n</b></details>\n\n<details>\n<summary>True or False? NACL allow or deny traffic on the subnet level</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? Multiple Internet Gateways can be attached to one VPC</summary><br><b>\n\nFalse. Only one internet gateway can be attached to a single VPC.\n</b></details>\n\n<details>\n<summary>True or False? Route Tables used to allow or deny traffic from the internet to AWS instances</summary><br><b>\n\nFalse. Route tables are used to direct traffic to the right destination (e.g., Internet Gateway, NAT Gateway, etc.), not to allow or deny traffic.\n</b></details>\n\n<details>\n<summary>Explain Security Groups and Network ACLs</summary><br><b>\n\n* NACL - security layer on the subnet level. They are stateless, meaning inbound and outbound rules are evaluated separately.<br>\n* Security Group - security layer on the instance level. They are stateful, meaning if you allow inbound traffic, outbound traffic is automatically allowed, and vice versa.\n\nRead more about it [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) and [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)\n</b></details>\n\n<details>\n<summary>What is AWS Direct Connect?</summary><br><b>\n\nAllows you to connect your corporate network to AWS network. It provides a dedicated network connection that can offer more consistent performance than internet-based connections.\n</b></details>\n\n#### AWS Compute\n\n<details>\n<summary>What is EC2?</summary><br><b>\n\n\"a web service that provides secure, resizable compute capacity in the cloud\".\nRead more [here](https://aws.amazon.com/ec2)\n\nEC2 allows you to quickly scale up or down to match resource needs, paying only for the compute time you consume.\n</b></details>\n\n<details>\n<summary>What is AMI?</summary><br><b>\n\nAmazon Machine Images is \"An Amazon Machine Image (AMI) provides the information required to launch an instance\".\nRead more [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html)\n\nAn AMI typically includes an operating system, application server, and applications, so you can quickly spin up new instances with the same configuration.\n</b></details>\n\n<details>\n<summary>What are the different source for AMIs?</summary><br><b>\n\n* Personal AMIs - AMIs you create\n* AWS Marketplace for AMIs - Paid AMIs usually bundled with licensed software\n* Community AMIs - Free\n\nYou can also share AMIs across accounts if needed.\n</b></details>\n\n<details>\n<summary>What is instance type?</summary><br><b>\n\n\"the instance type that you specify determines the hardware of the host computer used for your instance\"\nRead more about instance types [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html)\n\nInstance types vary by CPU, memory, storage, and networking capacity, e.g., t2.micro, c5.large, etc.\n</b></details>\n\n<details>\n<summary>True or False? The following are instance types available for a user in AWS:\n\n  * Compute optimized\n  * Network optimized\n  * Web optimized</summary><br><b>\n\nFalse. From the above list only compute optimized is available. There's no \"Web optimized\" or \"Network optimized\" instance type. You do have memory optimized, storage optimized, etc.\n</b></details>\n\n<details>\n<summary>What is EBS?</summary><br><b>\n\n\"provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices.\"\nMore on EBS [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html)\n\nEBS volumes are tied to an Availability Zone. They can be snapshotted to Amazon S3 for durability and can be detached/reattached between EC2 instances in the same AZ.\n</b></details>\n\n<details>\n<summary>What EC2 pricing models are there?</summary><br><b>\n\nOn Demand - pay a fixed rate by the hour/second with no commitment. You can provision and terminate at any time.<br>\nReserved - you get capacity reservation, basically purchase an instance for a fixed time period (1 or 3 years). The longer, the cheaper.<br>\nSpot - Enables you to bid whatever price you want for instances or pay the spot price. Ideal for workloads that can be interrupted.<br>\nDedicated Hosts - physical EC2 server dedicated for your use. Helps you address compliance requirements and use your own software licenses.\n</b></details>\n\n<details>\n<summary>What are Security Groups?</summary><br><b>\n\n\"A security group acts as a virtual firewall that controls the traffic for one or more instances\"\nMore on this subject [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html)\n\nThey are stateful, so any rule applied for inbound automatically applies to outbound, and vice versa (if the inbound rule is allowed).\n</b></details>\n\n<details>\n<summary>What can you attach to an EC2 instance in order to store data?</summary><br><b>\n\nEBS\n\nAdditionally, some instance types support Instance Store (ephemeral storage), and you can also mount EFS (file storage) if you need a shared filesystem across multiple instances.\n</b></details>\n\n<details>\n<summary>What EC2 RI types are there?</summary><br><b>\n\nStandard RI - most significant discount + suited for steady-state usage<br>\nConvertible RI - discount + change attribute of RI + suited for steady-state usage<br>\nScheduled RI - launch within time windows you reserve\n\nLearn more about EC2 RI [here](https://aws.amazon.com/ec2/pricing/reserved-instances)\n\nSome RIs also offer different payment options (no upfront, partial upfront, or all upfront) affecting the discount level.\n</b></details>\n\n#### AWS Containers\n\n<details>\n<summary>What is Amazon ECS?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. Customers such as Duolingo, Samsung, GE, and Cook Pad use ECS to run their most sensitive and mission critical applications because of its security, reliability, and scalability.\"\n\nLearn more [here](https://aws.amazon.com/ecs)\n</b></details>\n\n<details>\n<summary>What is Amazon ECR?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images.\"\n\nLearn more [here](https://aws.amazon.com/ecr)\n</b></details>\n\n<details>\n<summary>What is AWS Fargate?</summary><br><b>\n\nAmazon definition: \"AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\"\n\nLearn more [here](https://aws.amazon.com/fargate)\n</b></details>\n\n#### AWS Storage\n \n<details>\n<summary>Explain what is AWS S3?</summary><br><b>\n\nS3 stands for 3 S, Simple Storage Service.\nS3 is a object storage service which is fast, scalable and durable. S3 enables customers to upload, download or store any file or object that is up to 5 TB in size.\n\nMore on S3 [here](https://aws.amazon.com/s3) \n</b></details>\n\n<details>\n<summary>What is a bucket?</summary><br><b>\n\nAn S3 bucket is a resource which is similar to folders in a file system and allows storing objects, which consist of data.\n</b></details>\n\n<details>\n<summary>True or False? A bucket name must be globally unique</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>Explain folders and objects in regards to buckets</summary><br><b>\n\n* Folder - any sub folder in an s3 bucket\n* Object - The files which are stored in a bucket\n</b></details>\n\n<details>\n<summary>Explain the following:\n\n  * Object Lifecycles\n  * Object Sharing\n  * Object Versioning</summary><br><b>\n\n  * Object Lifecycles - Transfer objects between storage classes based on defined rules of time periods\n  * Object Sharing - Share objects via a URL link\n  * Object Versioning - Manage multiple versions of an object\n</b></details>\n\n<details>\n<summary>Explain Object Durability and Object Availability</summary><br><b>\n\nObject Durability: The percent over a one-year time period that a file will not be lost\nObject Availability: The percent over a one-year time period that a file will be accessible\n</b></details>\n\n<details>\n<summary>What is a storage class? What storage classes are there?</summary><br><b>\n\nEach object has a storage class assigned to, affecting its availability and durability. This also has effect on costs.\nStorage classes offered today:\n  * Standard:\n    * Used for general, all-purpose storage (mostly storage that needs to be accessed frequently)\n    * The most expensive storage class \n    * 11x9% durability\n    * 2x9% availability\n    * Default storage class\n\n  * Standard-IA (Infrequent Access)\n    * Long lived, infrequently accessed data but must be available the moment it's being accessed\n    * 11x9% durability\n    * 99.90% availability\n\n  * One Zone-IA (Infrequent Access):\n    * Long-lived, infrequently accessed, non-critical data\n    * Less expensive than Standard and Standard-IA storage classes\n    * 2x9% durability\n    * 99.50% availability\n    \n  * Intelligent-Tiering:\n    * Long-lived data with changing or unknown access patterns. Basically, In this class the data automatically moves to the class most suitable for you based on usage patterns\n    * Price depends on the used class\n    * 11x9% durability\n    * 99.90% availability\n\n  * Glacier: Archive data with retrieval time ranging from minutes to hours\n  * Glacier Deep Archive: Archive data that rarely, if ever, needs to be accessed with retrieval times in hours\n  * Both Glacier and Glacier Deep Archive are:\n    * The most cheap storage classes\n    * have 9x9% durability \n\nMore on storage classes [here](https://aws.amazon.com/s3/storage-classes)\n\n</b></details>\n\n<details>\n<summary>A customer would like to move data which is rarely accessed from standard storage class to the most cheapest class there is. Which storage class should be used?\n\n  * One Zone-IA\n  * Glacier Deep Archive\n  * Intelligent-Tiering</summary><br><b>\n\nGlacier Deep Archive\n</b></details>\n\n<details>\n<summary>What Glacier retrieval options are available for the user?</summary><br><b>\n\nExpedited, Standard and Bulk\n</b></details>\n\n<details>\n<summary>True or False? Each AWS account can store up to 500 PetaByte of data. Any additional storage will cost double</summary><br><b>\n\nFalse. Unlimited capacity.\n</b></details>\n\n<details>\n<summary>Explain what is Storage Gateway</summary><br><b>\n\n\"AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage\".\nMore on Storage Gateway [here](https://aws.amazon.com/storagegateway)\n</b></details>\n\n<details>\n<summary>Explain the following Storage Gateway deployments types\n\n  * File Gateway\n  * Volume Gateway\n  * Tape Gateway</summary><br><b>\n\nExplained in detail [here](https://aws.amazon.com/storagegateway/faqs)\n</b></details>\n\n<details>\n<summary>What is the difference between stored volumes and cached volumes?</summary><br><b>\n\nStored Volumes - Data is located at customer's data center and periodically backed up to AWS\nCached Volumes - Data is stored in AWS cloud and cached at customer's data center for quick access\n</b></details>\n\n<details>\n<summary>What is \"Amazon S3 Transfer Acceleration\"?</summary><br><b>\n\nAWS definition: \"Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket\"\n\nLearn more [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html)\n</b></details>\n\n<details>\n<summary>What is Amazon EFS?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources.\"\n\nLearn more [here](https://aws.amazon.com/efs)\n</b></details>\n\n<details>\n<summary>What is AWS Snowmobile?</summary><br><b>\n\n\"AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS.\"\n\nLearn more [here](https://aws.amazon.com/snowmobile)\n</b></details>\n\n#### AWS IAM\n\n<details>\n<summary>What is IAM? What are some of its features?</summary><br><b>\n\nIAM stands for Identity and Access Management, and is used for managing users, groups, access policies & roles\nFull explanation is [here](https://aws.amazon.com/iam)\n</b></details>\n\n<details>\n<summary>True or False? IAM configuration is defined globally and not per region</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? When creating an AWS account, root account is created by default. This is the recommended account to use and share in your organization</summary><br><b>\n\nFalse. Instead of using the root account, you should be creating users and use them.\n</b></details>\n\n<details>\n<summary>True or False? Groups in AWS IAM, can contain only users and not other groups</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? Users in AWS IAM, can belong only to a single group</summary><br><b>\n\nFalse. Users can belong to multiple groups.\n</b></details>\n\n<details>\n<summary>What are Roles?</summary><br><b>\n\nA way for allowing a service of AWS to use another service of AWS. You assign roles to AWS resources.\nFor example, you can make use of a role which allows EC2 service to accesses s3 buckets (read and write).\n</b></details>\n\n<details>\n<summary>What are Policies?</summary><br><b>\n\nPolicies documents used to give permissions as to what a user, group or role are able to do. Their format is JSON.\n</b></details>\n\n<details>\n<summary>A user is unable to access an s3 bucket. What might be the problem?</summary><br><b>\n\nThere can be several reasons for that. One of them is lack of policy. To solve that, the admin has to attach the user with a policy what allows him to access the s3 bucket.\n</b></details>\n\n<details>\n<summary>What should you use to:\n\n  * Grant access between two services/resources?\n  * Grant user access to resources/services?</summary><br><b>\n\n  * Role\n  * Policy\n</b></details>\n\n<details>\n<summary>What permissions does a new user have?</summary><br><b>\n\nOnly a login access.\n</b></details>\n\n##### AWS ELB\n\n<details>\n<summary>What is ELB (Elastic Load Balancing)?</summary><br><b>\n\nAWS definition: \"Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.\"\n\nMore on ELB [here](https://aws.amazon.com/elasticloadbalancing)\n</b></details>\n\n<details>\n<summary>What is auto scaling?</summary><br><b>\n\nAWS definition: \"AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost\"\n\nRead more about auto scaling [here](https://aws.amazon.com/autoscaling)\n</b></details>\n\n<details>\n<summary>True or False? Auto Scaling is about adding resources (such as instances) and not about removing resource</summary><br><b>\n\nFalse. Auto scaling adjusts capacity and this can mean removing some resources based on usage and performances.\n</b></details>\n\n<details>\n<summary>What types of load balancers are supported in EC2 and what are they used for?</summary><br><b>\n\n  * Application LB - layer 7 traffic\n  * Network LB - ultra-high performances or static IP address\n  * Classic LB - low costs, good for test or dev environments\n</b></details>\n\n#### AWS DNS\n\n<details>\n<summary>What is Route 53?</summary><br><b>\n\n\"Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service\"\nSome of Route 53 features:\n  * Register domain\n  * DNS service - domain name translations\n  * Health checks - verify your app is available\n\nMore on Route 53 [here](https://aws.amazon.com/route53)\n</b></details>\n\n#### AWS CloudFront\n\n<details>\n<summary>Explain what is CloudFront</summary><br><b>\n\nAWS definition: \"Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\"\n\nMore on CloudFront [here](https://aws.amazon.com/cloudfront)\n</b></details>\n\n<details>\n<summary>Explain the following\n\n  * Origin\n  * Edge location\n  * Distribution</summary><br><b>\n</b></details>\n\n#### AWS Monitoring & Logging\n\n<details>\n<summary>What is AWS CloudWatch?</summary><br><b>\n\nAWS definition: \"Amazon CloudWatch is a monitoring and observability service...\"\n\nMore on CloudWatch [here](https://aws.amazon.com/cloudwatch)\n</b></details>\n\n<details>\n<summary>What is AWS CloudTrail?</summary><br><b>\n\nAWS definition: \"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\"\n\nRead more on CloudTrail [here](https://aws.amazon.com/cloudtrail)\n</b></details>\n\n<details>\n<summary>What is Simply Notification Service?</summary><br><b>\n\nAWS definition: \"a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\"\n\nRead more about it [here](https://aws.amazon.com/sns)\n</b></details>\n\n<details>\n<summary>Explain the following in regards to SNS:\n\n  * Topics\n  * Subscribers\n  * Publishers</summary><br><b>\n\n  * Topics - used for grouping multiple endpoints\n  * Subscribers - the endpoints where topics send messages to \n  * Publishers - the provider of the message (event, person, ...)\n</b></details>\n\n#### AWS Security \n\n<details>\n<summary>What is the shared responsibility model? What AWS is responsible for and what the user is responsible for based on the shared responsibility model?</summary><br><b>\n\nThe shared responsibility model defines what the customer is responsible for and what AWS is responsible for. For example, AWS is responsible for security \"of\" the cloud, while the customer is responsible for security \"in\" the cloud.\n\nMore on the shared responsibility model [here](https://aws.amazon.com/compliance/shared-responsibility-model)\n</b></details>\n\n<details>\n<summary>True or False? Based on the shared responsibility model, Amazon is responsible for physical CPUs and security groups on instances</summary><br><b>\n\nFalse. It is responsible for Hardware in its sites but not for security groups which created and managed by the users.\n</b></details>\n\n<details>\n<summary>Explain \"Shared Controls\" in regards to the shared responsibility model</summary><br><b>\n\nAWS definition: \"apply to both the infrastructure layer and customer layers, but in completely separate contexts or perspectives. In a shared control, AWS provides the requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services\"\n\nLearn more about it [here](https://aws.amazon.com/compliance/shared-responsibility-model)\n</b></details>\n\n<details>\n<summary>What is the AWS compliance program?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS Artifact?</summary><br><b>\n\nAWS definition: \"AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS‚Äô security and compliance reports and select online agreements.\"\n\nRead more about it [here](https://aws.amazon.com/artifact)\n</b></details>\n\n<details>\n<summary>What is AWS Inspector?</summary><br><b>\n\nAWS definition: \"Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.\"\"\n\nLearn more [here](https://aws.amazon.com/inspector)\n</b></details>\n\n<details>\n<summary>What is AWS Guarduty?</summary><br><b>\n\nGuarduty is a threat detection service that monitors your AWS accounts to help detect and mitigate malicious activity\n</b></details>\n\n<details>\n<summary>What is AWS Shield?</summary><br><b>\n\nAWS definition: \"AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.\"\n</b></details>\n\n<details>\n<summary>What is AWS WAF? Give an example of how it can used and describe what resources or services you can use it with</summary><br><b>\n\nAn AWS Web Application Firewall (WAF) can filter out unwanted web traffic (bots), and protect against attacks like SQL injection and cross-site scripting. One service you could use it with would be Amazon CloudFront, a CDN service, to block attacks before they reach your origin servers\n</b></details>\n\n<details>\n<summary>What AWS VPN is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the difference between Site-to-Site VPN and Client VPN?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS CloudHSM?</summary><br><b>\n\nAmazon definition: \"AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud.\"\n\nLearn more [here](https://aws.amazon.com/cloudhsm)\n</b></details>\n\n<details>\n<summary>True or False? AWS Inspector can perform both network and host assessments</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>What is AWS Acceptable Use Policy?</summary><br><b>\n\nIt describes prohibited uses of the web services offered by AWS.\nMore on AWS Acceptable Use Policy [here](https://aws.amazon.com/aup)\n</b></details>\n\n<details>\n<summary>What is AWS Key Management Service (KMS)?</summary><br><b>\n\nAWS definition: \"KMS makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.\"\nMore on KMS [here](https://aws.amazon.com/kms)\n</b></details>\n\n<details>\n<summary>True or False? A user is not allowed to perform penetration testing on any of the AWS services</summary><br><b>\n\nFalse. On some services, like EC2, CloudFront and RDS, penetration testing is allowed.\n</b></details>\n\n<details>\n<summary>True or False? DDoS attack is an example of allowed penetration testing activity</summary><br><b>\n\nFalse.\n</b></details>\n\n<details>\n<summary>True or False? AWS Access Key is a type of MFA device used for AWS resources protection</summary><br><b>\n\nFalse. Security key is an example of an MFA device.\n</b></details>\n\n<details>\n<summary>What is Amazon Cognito?</summary><br><b>\n\nAmazon definition: \"Amazon Cognito handles user authentication and authorization for your web and mobile apps.\"\n\nLearn more [here](https://docs.aws.amazon.com/cognito/index.html)\n</b></details>\n\n<details>\n<summary>What is AWS ACM?</summary><br><b>\n\nAmazon definition: \"AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.\"\n\nLearn more [here](https://aws.amazon.com/certificate-manager)\n</b></details>\n\n#### AWS Databases\n\n<details>\n<summary>What is AWS RDS?</summary><br><b>\n\nAmazon Relational Database Service (RDS) is a service for setting up  and managing resizable, cost-efficient relational databases\nresource\n\nLearn more [here](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html)\n</b></details>\n\n<details>\n<summary>What is AWS DynamoDB?</summary><br><b>\n</b></details>\n\n<details>\n<summary>Explain \"Point-in-Time Recovery\" feature in DynamoDB</summary><br><b>\n\nAmazon definition: \"You can create on-demand backups of your Amazon DynamoDB tables, or you can enable continuous backups using point-in-time recovery. For more information about on-demand backups, see On-Demand Backup and Restore for DynamoDB.\"\n\nLearn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html)\n</b></details>\n\n<details>\n<summary>Explain \"Global Tables\" in DynamoDB</summary><br><b>\n\nAmazon definition: \"A global table is a collection of one or more replica tables, all owned by a single AWS account.\"\n\nLearn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html)\n</b></details>\n\n<details>\n<summary>What is DynamoDB Accelerator?</summary><br><b>\n\nAmazon definition: \"Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement ‚Äì from milliseconds to microseconds...\"\n\nLearn more [here](https://aws.amazon.com/dynamodb/dax)\n</b></details>\n\n<details>\n<summary>What is AWS Redshift and how is it different than RDS?</summary><br><b>\n\nAWS Redshift is a cloud data warehousing service that is geared towards handling massive amounts of data (think petabytes) and being able to execute complex queries. In contrast, Amazon RDS is best suited for things like web applications requiring simple queries with more frequent transactions, and on a smaller scale.\n</b></details>\n\n<details>\n<summary>What is AWS ElastiCache? For what cases is it used?</summary><br><b>\n\nAmazon Elasticache is a fully managed Redis or Memcached in-memory data store.                                                                       \nIt's great for use cases like two-tier web applications where the most frequently accesses data is stored in ElastiCache so response time is optimal.\n</b></details>\n\n<details>\n<summary>What is Amazon Aurora</summary><br><b>\n\nA MySQL & Postgresql based relational database. Also, the default database proposed for the user when using RDS for creating a database.\nGreat for use cases like two-tier web applications that has a MySQL or Postgresql database layer and you need automated backups for your application.\n</b></details>\n\n<details>\n<summary>What is Amazon DocumentDB?</summary><br><b>\n\nAmazon definition: \"Amazon DocumentDB (with MongoDB compatibility) is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data.\"\n\nLearn more [here](https://aws.amazon.com/documentdb)\n</b></details>\n\n<details>\n<summary>What \"AWS Database Migration Service\" is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What type of storage is used by Amazon RDS?</summary><br><b>\n\nEBS\n</b></details>\n\n<details>\n<summary>Explain Amazon RDS Read Replicas</summary><br><b>\n\nAWS definition: \"Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\"\nRead more about [here](https://aws.amazon.com/rds/features/read-replicas)\n</b></details>\n\n#### AWS Serverless Compute\n\n<details>\n<summary>Explain what is AWS Lambda</summary><br><b>\n\nAWS definition: \"AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\"\n\nRead more on it [here](https://aws.amazon.com/lambda)\n</b></details>\n\n<details>\n<summary>True or False? In AWS Lambda, you are charged as long as a function exists, regardless of whether it's running or not</summary><br><b>\n\nFalse. Charges are being made when the code is executed.\n</b></details>\n\n<details>\n<summary>Which of the following set of languages Lambda supports?\n\n  * R, Swift, Rust, Kotlin\n  * Python, Ruby, Go \n  * Python, Ruby, PHP</summary><br><b>\n\n  * Python, Ruby, Go \n</b></details>\n\n#### Identify the service or tool\n\n<details>\n<summary>What would you use for automating code/software deployments?</summary><br><b>\n\nAWS CodeDeploy\n</b></details>\n\n<details>\n<summary>What would you use for easily creating similar AWS environments/resources for different customers?</summary><br><b>\n\nCloudFormation\n</b></details>\n\n<details>\n<summary>Which service would you use for building a website or web application?</summary><br><b>\n\nLightsail or Elastic Beanstalk\n</b></details>\n\n<details>\n<summary>Which tool would you use for choosing between Reserved instances or On-Demand instances?</summary><br><b>\n\nCost Explorer\n</b></details>\n\n<details>\n<summary>What would you use to check how many unassociated Elastic IP address you have?</summary><br><b>\n\nTrusted Advisor\n</b></details>\n\n<details>\n<summary>What service allows you to transfer large amounts (Petabytes) of data in and out of the AWS cloud?</summary><br><b>\n\nAWS Snowball\n</b></details>\n\n<details>\n<summary>What provides a virtual network dedicated to your AWS account?</summary><br><b>\n\nVPC\n</b></details>\n\n<details>\n<summary>What you would use for having automated backups for an application that has MySQL database layer?</summary><br><b>\n\nAmazon Aurora\n</b></details>\n\n<details>\n<summary>What would you use to migrate on-premise database to AWS?</summary><br><b>\n\nAWS Database Migration Service (DMS)\n</b></details>\n\n<details>\n<summary>What would you use to check why certain EC2 instances were terminated?</summary><br><b>\n\nAWS CloudTrail\n</b></details>\n\n<details>\n<summary>What would you use for SQL database?</summary><br><b>\n\nAWS RDS\n</b></details>\n\n<details>\n<summary>What would you use for NoSQL database?</summary><br><b>\n\nAWS DynamoDB\n</b></details>\n\n<details>\n<summary>What would you use for running SQL queries interactively on S3?</summary><br><b>\n\nAWS Athena\n</b></details>\n\n<details>\n<summary>What would you use for adding image and video analysis to your application?</summary><br><b>\n\nAWS Rekognition\n</b></details>\n\n<details>\n<summary>Which service would you use for debugging and improving performances issues with your applications?</summary><br><b>\n\nAWS X-Ray\n</b></details>\n\n<details>\n<summary>Which service is used for sending notifications?</summary><br><b>\n\nSNS\n</b></details>\n\n<details>\n<summary>Which service would you use for monitoring malicious activity and unauthorized behavior in regards to AWS accounts and workloads?</summary><br><b>\n\nAmazon GuardDuty\n</b></details>\n\n<details>\n<summary>Which service would you use for centrally manage billing, control access, compliance, and security across multiple AWS accounts?</summary><br><b>\n\nAWS Organizations\n</b></details>\n\n<details>\n<summary>Which service would you use for web application protection?</summary><br><b>\n\nAWS WAF\n</b></details>\n\n<details>\n<summary>You would like to monitor some of your resources in the different services. Which service would you use for that?</summary><br><b>\n\nCloudWatch\n</b></details>\n\n<details>\n<summary>Which service would you use for performing security assessment?</summary><br><b>\n\nAWS Inspector\n</b></details>\n\n<details>\n<summary>Which service would you use for creating DNS record?</summary><br><b>\n\nRoute 53\n</b></details>\n\n<details>\n<summary>What would you use if you need a fully managed document database?</summary><br><b>\n\nAmazon DocumentDB\n</b></details>\n\n<details>\n<summary>Which service would you use to add access control (or sign-up, sign-in forms) to your web/mobile apps?</summary><br><b>\n\nAWS Cognito\n</b></details>\n\n<details>\n<summary>Which service would you use if you need messaging queue?</summary><br><b>\n\nSimple Queue Service (SQS)\n</b></details>\n\n<details>\n<summary>Which service would you use if you need managed DDOS protection?</summary><br><b>\n\nAWS Shield\n</b></details>\n\n<details>\n<summary>Which service would you use if you need store frequently used data for low latency access?</summary><br><b>\n\nElastiCache\n</b></details>\n\n<details>\n<summary>What would you use to transfer files over long distances between a client and an S3 bucket?</summary><br><b>\n\nAmazon S3 Transfer Acceleration\n</b></details>\n\n#### AWS Billing & Support\n\n<details>\n<summary>What is AWS Organizations?</summary><br><b>\n\nAWS definition: \"AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS.\"\nMore on Organizations [here](https://aws.amazon.com/organizations)\n</b></details>\n\n<details>\n<summary>Explain AWS pricing model</summary><br><b>\n\nIt mainly works on \"pay-as-you-go\" meaning you pay only for what are using and when you are using it.\nIn s3 you pay for 1. How much data you are storing 2. Making requests (PUT, POST, ...)\nIn EC2 it's based on the purchasing option (on-demand, spot, ...), instance type, AMI type and the region used.\n\nMore on AWS pricing model [here](https://aws.amazon.com/pricing)\n</b></details>\n\n<details>\n<summary>How one should estimate AWS costs when for example comparing to on-premise solutions?</summary><br><b>\n\n* TCO calculator\n* AWS simple calculator\n* Cost Explorer\n</b></details>\n\n<details>\n<summary>What basic support in AWS includes?</summary><br><b>\n\n* 24x7 customer service\n* Trusted Advisor\n* AWS personal Health Dashoard\n</b></details>\n\n<details>\n<summary>How are EC2 instances billed?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What AWS Pricing Calculator is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is Amazon Connect?</summary><br><b>\n\nAmazon definition: \"Amazon Connect is an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost.\"\n\nLearn more [here](https://aws.amazon.com/connect)\n</b></details>\n\n<details>\n<summary>What are \"APN Consulting Partners\"?</summary><br><b>\n\nAmazon definition: \"APN Consulting Partners are professional services firms that help customers of all types and sizes design, architect, build, migrate, and manage their workloads and applications on AWS, accelerating their journey to the cloud.\"\n\nLearn more [here](https://aws.amazon.com/partners/consulting)\n</b></details>\n\n<details>\n<summary>Which of the following are AWS accounts types (and are sorted by order)?\n\n  * Basic, Developer, Business, Enterprise\n  * Newbie, Intermediate, Pro, Enterprise\n  * Developer, Basic, Business, Enterprise\n  * Beginner, Pro, Intermediate Enterprise</summary><br><b>\n\n  * Basic, Developer, Business, Enterprise\n</b></details>\n\n<details>\n<summary>True or False? Region is a factor when it comes to EC2 costs/pricing</summary><br><b>\n\nTrue. You pay differently based on the chosen region.\n</b></details>\n\n<details>\n<summary>What is \"AWS Infrastructure Event Management\"?</summary><br><b>\n\nAWS Definition: \"AWS Infrastructure Event Management is a structured program available to Enterprise Support customers (and Business Support customers for an additional fee) that helps you plan for large-scale events such as product or application launches, infrastructure migrations, and marketing events.\"\n</b></details>\n\n#### AWS Automation\n\n<details>\n<summary>What is AWS CodeDeploy?</summary><br><b>\n\nAmazon definition: \"AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.\"\n\nLearn more [here](https://aws.amazon.com/codedeploy)\n</b></details>\n\n<details>\n<summary>Explain what is CloudFormation</summary><br><b>\n</b></details>\n\n#### AWS Misc\n\n<details>\n<summary>What is AWS Lightsail?</summary><br><b>\n\nAWS definition: \"Lightsail is an easy-to-use cloud platform that offers you everything needed to build an application or website, plus a cost-effective, monthly plan.\"\n</b></details>\n\n<details>\n<summary>What is AWS Rekognition?</summary><br><b>\n\nAWS definition: \"Amazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use.\"\n\nLearn more [here](https://aws.amazon.com/rekognition)\n</b></details>\n\n<details>\n<summary>What AWS Resource Groups used for?</summary><br><b>\n\nAmazon definition: \"You can use resource groups to organize your AWS resources. Resource groups make it easier to manage and automate tasks on large numbers of resources at one time. \"\n\nLearn more [here](https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html)\n</b></details>\n\n<details>\n<summary>What is AWS Global Accelerator?</summary><br><b>\n\nAmazon definition: \"AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users...\"\n\nLearn more [here](https://aws.amazon.com/global-accelerator)\n</b></details>\n\n<details>\n<summary>What is AWS Config?</summary><br><b>\n\nAmazon definition: \"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources.\"\n\nLearn more [here](https://aws.amazon.com/config)\n</b></details>\n\n<details>\n<summary>What is AWS X-Ray?</summary><br><b>\n\nAWS definition: \"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.\"\nLearn more [here](https://aws.amazon.com/xray)\n</b></details>\n\n<details>\n<summary>What is AWS OpsWorks?</summary><br><b>\n\nAmazon definition: \"AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.\"\n\nLearn more about it [here](https://aws.amazon.com/opsworks)\n</b></details>\n\n<details>\n<summary>What is AWS Service Catalog?</summary><br><b>\n\nAmazon definition: \"AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS.\"\n\nLearn more [here](https://aws.amazon.com/servicecatalog)\n</b></details>\n\n<details>\n<summary>What is AWS CAF?</summary><br><b>\n\nAmazon definition: \"AWS Professional Services created the AWS Cloud Adoption Framework (AWS CAF) to help organizations design and travel an accelerated path to successful cloud adoption. \"\n\nLearn more [here](https://aws.amazon.com/professional-services/CAF)\n</b></details>\n\n<details>\n<summary>What is AWS Cloud9?</summary><br><b>\n\nAWS definition: \"AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser\"\n</b></details>\n\n<details>\n<summary>What is AWS Application Discovery Service?</summary><br><b>\n\nAmazon definition: \"AWS Application Discovery Service helps enterprise customers plan migration projects by gathering information about their on-premises data centers.\"\n\nLearn more [here](https://aws.amazon.com/application-discovery)\n</b></details>\n\n<details>\n<summary>What is the Trusted Advisor?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the AWS well-architected framework and what pillars it's based on?</summary><br><b>\n\nAWS definition: \"The Well-Architected Framework has been developed to help cloud architects build secure, high-performing, resilient, and efficient infrastructure for their applications. Based on five pillars ‚Äî operational excellence, security, reliability, performance efficiency, and cost optimization\"\n\nLearn more [here](https://aws.amazon.com/architecture/well-architected)\n</b></details>\n\n<details>\n<summary>What AWS services are serverless (or have the option to be serverless)?</summary><br><b>\n\nAWS Lambda\nAWS Athena\n</b></details>\n\n<details>\n<summary>What is AWS EMR?</summary><br><b>\n\nAWS definition: \"big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto.\"\n\nLearn more [here](https://aws.amazon.com/emr)\n</b></details>\n\n<details>\n<summary>What is AWS Athena?</summary><br><b>\n\n\"Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.\"\n\nLearn more about AWS Athena [here](https://aws.amazon.com/athena)\n</b></details>\n\n<details>\n<summary>What is Amazon Cloud Directory?</summary><br><b>\n\nAmazon definition: \"Amazon Cloud Directory is a highly available multi-tenant directory-based store in AWS. These directories scale automatically to hundreds of millions of objects as needed for applications.\"\n\nLearn more [here](https://docs.aws.amazon.com/clouddirectory/latest/developerguide/what_is_cloud_directory.html)\n</b></details>\n\n<details>\n<summary>What is AWS Elastic Beanstalk?</summary><br><b>\n\nAWS definition: \"AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services...You can simply upload your code and Elastic Beanstalk automatically handles the deployment\"\n\nLearn more about it [here](https://aws.amazon.com/elasticbeanstalk)\n</b></details>\n\n<details>\n<summary>What is AWS SWF?</summary><br><b>\n\nAmazon definition: \"Amazon SWF helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud.\"\n\nLearn more on Amazon Simple Workflow Service [here](https://aws.amazon.com/swf)\n</b></details>\n\n<details>\n<summary>What is Simple Queue Service (SQS)?</summary><br><b>\n\nAWS definition: \"Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications\".\n\nLearn more about it [here](https://aws.amazon.com/sqs)\n</b></details>\n\n#### AWS Disaster Recovery\n\n<details>\n<summary>In regards to disaster recovery, what is RTO and RPO?</summary><br><b>\n\nRTO - The maximum acceptable length of time that your application can be offline.\n\nRPO - The maximum acceptable length of time during which data might be lost from your application due to an incident.\n</b></details>\n\n<details>\n<summary>What types of disaster recovery techniques AWS supports?</summary><br><b>\n\n* The Cold Method - Periodically backups and sending the backups off-site<br>\n* Pilot Light - Data is mirrored to an environment which is always running\n* Warm Standby - Running scaled down version of production environment\n* Multi-site - Duplicated environment that is always running\n</b></details>\n\n<details>\n<summary>Which disaster recovery option has the highest downtime and which has the lowest?</summary><br><b>\n\nLowest - Multi-site\nHighest - The cold method\n</b></details>\n\n### Final Note\n\nGood luck! You can do it :)\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "certificates/aws-cloud-practitioner.md", "language": "markdown", "complexity": "complex", "star_count": 77729, "quality_score": 0.79, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 1281}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS - Solutions Architect Associate \n\nLast update: 2021\n\n#### AWS Global Infrastructure\n\n<details>\n<summary>Explain the following\n\n  * Availability zone\n  * Region\n  * Edge location</summary><br><b>\n\nAWS regions are data centers hosted across different geographical locations worldwide, each region is completely independent of one another.<br>\n\nWithin each region, there are multiple isolated locations known as Availability Zones. Multiple availability zones ensure high availability in case one of them goes down.<br>\n\nEdge locations are basically content delivery network which caches data and insures lower latency and faster delivery to the users in any location. They are located in major cities in the world.\n</b></details>\n\n#### AWS - IAM\n\n<details>\n<summary>What is IAM? What are some of its features?</summary><br><b>\n\nFull explanation is [here](https://aws.amazon.com/iam)\nIn short: it's used for managing users, groups, access policies & roles\n</b></details>\n\n<details>\n<summary>True or False? IAM configuration is defined globally and not per region</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? When creating an AWS account, root account is created by default. This is the recommended account to use and share in your organization</summary><br><b>\n\nFalse. Instead of using the root account, you should be creating users and use them.\n</b></details>\n\n<details>\n<summary>True or False? Groups in AWS IAM, can contain only users and not other groups</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? Users in AWS IAM, can belong only to a single group</summary><br><b>\n\nFalse. Users can belong to multiple groups.\n</b></details>\n\n<details>\n<summary>What are Roles?</summary><br><b>\n\nA way for allowing a service of AWS to use another service of AWS. You assign roles to AWS resources.\nFor example, you can make use of a role which allows EC2 service to accesses s3 buckets (read and write).\n</b></details>\n\n<details>\n<summary>What are Policies?</summary><br><b>\n\nPolicies documents used to give permissions as to what a user, group or role are able to do. Their format is JSON.\n</b></details>\n\n<details>\n<summary>A user is unable to access an s3 bucket. What might be the problem?</summary><br><b>\n\nThere can be several reasons for that. One of them is lack of policy. To solve that, the admin has to attach the user with a policy what allows him to access the s3 bucket.\n</b></details>\n\n<details>\n<summary>What should you use to:\n\n  * Grant access between two services/resources?\n  * Grant user access to resources/services?</summary><br><b>\n\n  * Role\n  * Policy\n</b></details>\n\n<details>\n<summary>What permissions does a new user have?</summary><br><b>\n\nOnly a login access.\n</b></details>\n\n\n#### AWS Networking\n\n<details>\n<summary>What is VPC?</summary><br><b>\n\n\"A logically isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define\"\nRead more about it [here](https://aws.amazon.com/vpc).\n</b></details>\n\n<details>\n<summary>True or False? VPC spans multiple regions</summary><br><b>\n\nFalse\n</b></details>\n\n<details>\n<summary>True or False? Subnets belong to the same VPC, can be in different availability zones</summary><br><b>\n\nTrue. Just to clarify, a subnet must reside entirely in one AZ.\n</b></details>\n\n<details>\n<summary>What is an Internet Gateway?</summary><br><b>\n\n\"component that allows communication between instances in your VPC and the internet\" (AWS docs).\nRead more about it [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html)\n</b></details>\n\n<details>\n<summary>True or False? NACL allow or deny traffic on the subnet level</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>True or False? Multiple Internet Gateways can be attached to one VPC</summary><br><b>\n\nFalse. Only one internet gateway can be attached to a single VPC.\n</b></details>\n\n<details>\n<summary>True or False? Route Tables used to allow or deny traffic from the internet to AWS instances</summary><br><b>\n\nFalse.\n</b></details>\n\n<details>\n<summary>Explain Security Groups and Network ACLs</summary><br><b>\n\n* NACL - security layer on the subnet level.\n* Security Group - security layer on the instance level.\n\nRead more about it [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) and [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)\n</b></details>\n\n<details>\n<summary>What is AWS Direct Connect?</summary><br><b>\n\nAllows you to connect your corporate network to AWS network.\n</b></details>\n\n#### AWS Compute\n\n<details>\n<summary>What is EC2?</summary><br><b>\n\n\"a web service that provides secure, resizable compute capacity in the cloud\".\nRead more [here](https://aws.amazon.com/ec2)\n</b></details>\n\n<details>\n<summary>True or False? EC2 is a regional service</summary><br><b>\n\nTrue. As opposed to IAM for example, which is a global service, EC2 is a regional service.\n</b></details>\n\n<details>\n<summary>What is AMI?</summary><br><b>\n\nAmazon Machine Images is \"An Amazon Machine Image (AMI) provides the information required to launch an instance\".\nRead more [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html)\n</b></details>\n\n<details>\n<summary>What are the different source for AMIs?</summary><br><b>\n\n* Personal AMIs - AMIs you create\n* AWS Marketplace for AMIs - Paid AMIs usually with bundled with licensed software\n* Community AMIs - Free\n</b></details>\n\n<details>\n<summary>What is instance type?</summary><br><b>\n\n\"the instance type that you specify determines the hardware of the host computer used for your instance\"\nRead more about instance types [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html)\n</b></details>\n\n<details>\n<summary>True or False? The following are instance types available for a user in AWS:\n\n  * Compute optimized\n  * Network optimized\n  * Web optimized</summary><br><b>\n\nFalse. From the above list only compute optimized is available.\n</b></details>\n\n<details>\n<summary>What is EBS?</summary><br><b>\n\n\"provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices.\"\nMore on EBS [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html)\n</b></details>\n\n<details>\n<summary>What EC2 pricing models are there?</summary><br><b>\n\nOn Demand - pay a fixed rate by the hour/second with no commitment. You can provision and terminate it at any given time.\nReserved - you get capacity reservation, basically purchase an instance for a fixed time of period. The longer, the cheaper.\nSpot - Enables you to bid whatever price you want for instances or pay the spot price.\nDedicated Hosts - physical EC2 server dedicated for your use.\n</b></details>\n\n<details>\n<summary>What are Security Groups?</summary><br><b>\n\n\"A security group acts as a virtual firewall that controls the traffic for one or more instances\"\nMore on this subject [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html)\n</b></details>\n\n<details>\n<summary>What can you attach to an EC2 instance in order to store data?</summary><br><b>\n\nEBS\n</b></details>\n\n<details>\n<summary>What EC2 RI types are there?</summary><br><b>\n\nStandard RI - most significant discount + suited for steady-state usage\nConvertible RI - discount + change attribute of RI + suited for steady-state usage\nScheduled RI - launch within time windows you reserve\n\nLearn more about EC2 RI [here](https://aws.amazon.com/ec2/pricing/reserved-instances)\n</b></details>\n\n#### AWS Containers\n\n<details>\n<summary>What is Amazon ECS?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. Customers such as Duolingo, Samsung, GE, and Cook Pad use ECS to run their most sensitive and mission critical applications because of its security, reliability, and scalability.\"\n\nLearn more [here](https://aws.amazon.com/ecs)\n</b></details>\n\n<details>\n<summary>What is Amazon ECR?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images.\"\n\nLearn more [here](https://aws.amazon.com/ecr)\n</b></details>\n\n<details>\n<summary>What is AWS Fargate?</summary><br><b>\n\nAmazon definition: \"AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\"\n\nLearn more [here](https://aws.amazon.com/fargate)\n</b></details>\n\n#### AWS Storage\n \n<details>\n<summary>Explain what is AWS S3?</summary><br><b>\n\nS3 stands for 3 S, Simple Storage Service.\nS3 is a object storage service which is fast, scalable and durable. S3 enables customers to upload, download or store any file or object that is up to 5 TB in size.\n\nMore on S3 [here](https://aws.amazon.com/s3) \n</b></details>\n\n<details>\n<summary>What is a bucket?</summary><br><b>\n\nAn S3 bucket is a resource which is similar to folders in a file system and allows storing objects, which consist of data.\n</b></details>\n\n<details>\n<summary>True or False? A bucket name must be globally unique</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>Explain folders and objects in regards to buckets</summary><br><b>\n\n* Folder - any sub folder in an s3 bucket\n* Object - The files which are stored in a bucket\n</b></details>\n\n<details>\n<summary>Explain the following:\n\n  * Object Lifecycles\n  * Object Sharing\n  * Object Versioning</summary><br><b>\n\n  * Object Lifecycles - Transfer objects between storage classes based on defined rules of time periods\n  * Object Sharing - Share objects via a URL link\n  * Object Versioning - Manage multiple versions of an object\n</b></details>\n\n<details>\n<summary>Explain Object Durability and Object Availability</summary><br><b>\n\nObject Durability: The percent over a one-year time period that a file will not be lost\nObject Availability: The percent over a one-year time period that a file will be accessible\n</b></details>\n\n<details>\n<summary>What is a storage class? What storage classes are there?</summary><br><b>\n\nEach object has a storage class assigned to, affecting its availability and durability. This also has effect on costs.\nStorage classes offered today:\n  * Standard:\n    * Used for general, all-purpose storage (mostly storage that needs to be accessed frequently)\n    * The most expensive storage class \n    * 11x9% durability\n    * 2x9% availability\n    * Default storage class\n\n  * Standard-IA (Infrequent Access)\n    * Long lived, infrequently accessed data but must be available the moment it's being accessed\n    * 11x9% durability\n    * 99.90% availability\n\n  * One Zone-IA (Infrequent Access):\n    * Long-lived, infrequently accessed, non-critical data\n    * Less expensive than Standard and Standard-IA storage classes\n    * 2x9% durability\n    * 99.50% availability\n    \n  * Intelligent-Tiering:\n    * Long-lived data with changing or unknown access patterns. Basically, In this class the data automatically moves to the class most suitable for you based on usage patterns\n    * Price depends on the used class\n    * 11x9% durability\n    * 99.90% availability\n\n  * Glacier: Archive data with retrieval time ranging from minutes to hours\n  * Glacier Deep Archive: Archive data that rarely, if ever, needs to be accessed with retrieval times in hours\n  * Both Glacier and Glacier Deep Archive are:\n    * The most cheap storage classes\n    * have 9x9% durability \n\nMore on storage classes [here](https://aws.amazon.com/s3/storage-classes)\n\n</b></details>\n\n<details>\n<summary>A customer would like to move data which is rarely accessed from standard storage class to the most cheapest class there is. Which storage class should be used?\n\n  * One Zone-IA\n  * Glacier Deep Archive\n  * Intelligent-Tiering</summary><br><b>\n\nGlacier Deep Archive\n</b></details>\n\n<details>\n<summary>What Glacier retrieval options are available for the user?</summary><br><b>\n\nExpedited, Standard and Bulk\n</b></details>\n\n<details>\n<summary>True or False? Each AWS account can store up to 500 PetaByte of data. Any additional storage will cost double</summary><br><b>\n\nFalse. Unlimited capacity.\n</b></details>\n\n<details>\n<summary>Explain what is Storage Gateway</summary><br><b>\n\n\"AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage\".\nMore on Storage Gateway [here](https://aws.amazon.com/storagegateway)\n</b></details>\n\n<details>\n<summary>Explain the following Storage Gateway deployments types\n\n  * File Gateway\n  * Volume Gateway\n  * Tape Gateway</summary><br><b>\n\nExplained in detail [here](https://aws.amazon.com/storagegateway/faqs)\n</b></details>\n\n<details>\n<summary>What is the difference between stored volumes and cached volumes?</summary><br><b>\n\nStored Volumes - Data is located at customer's data center and periodically backed up to AWS\nCached Volumes - Data is stored in AWS cloud and cached at customer's data center for quick access\n</b></details>\n\n<details>\n<summary>What is \"Amazon S3 Transfer Acceleration\"?</summary><br><b>\n\nAWS definition: \"Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket\"\n\nLearn more [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html)\n</b></details>\n\n<details>\n<summary>What is Amazon EFS?</summary><br><b>\n\nAmazon definition: \"Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources.\"\n\nLearn more [here](https://aws.amazon.com/efs)\n</b></details>\n\n<details>\n<summary>What is AWS Snowmobile?</summary><br><b>\n\n\"AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS.\"\n\nLearn more [here](https://aws.amazon.com/snowmobile)\n</b></details>\n\n##### AWS ELB\n\n<details>\n<summary>What is ELB (Elastic Load Balancing)?</summary><br><b>\n\nAWS definition: \"Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.\"\n\nMore on ELB [here](https://aws.amazon.com/elasticloadbalancing)\n</b></details>\n\n<details>\n<summary>What is auto scaling?</summary><br><b>\n\nAWS definition: \"AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost\"\n\nRead more about auto scaling [here](https://aws.amazon.com/autoscaling)\n</b></details>\n\n<details>\n<summary>True or False? Auto Scaling is about adding resources (such as instances) and not about removing resource</summary><br><b>\n\nFalse. Auto scaling adjusts capacity and this can mean removing some resources based on usage and performances.\n</b></details>\n\n<details>\n<summary>What types of load balancers are supported in EC2 and what are they used for?</summary><br><b>\n\n  * Application LB - layer 7 traffic\n  * Network LB - ultra-high performances or static IP address\n  * Classic LB - low costs, good for test or dev environments\n</b></details>\n\n#### AWS DNS\n\n<details>\n<summary>What is Route 53?</summary><br><b>\n\n\"Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service\"\nSome of Route 53 features:\n  * Register domain\n  * DNS service - domain name translations\n  * Health checks - verify your app is available\n\nMore on Route 53 [here](https://aws.amazon.com/route53)\n</b></details>\n\n#### AWS CloudFront\n\n<details>\n<summary>Explain what is CloudFront</summary><br><b>\n\nAWS definition: \"Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\"\n\nMore on CloudFront [here](https://aws.amazon.com/cloudfront)\n</b></details>\n\n<details>\n<summary>Explain the following\n\n  * Origin\n  * Edge location\n  * Distribution</summary><br><b>\n</b></details>\n\n#### AWS Monitoring & Logging\n\n<details>\n<summary>What is AWS CloudWatch?</summary><br><b>\n\nAWS definition: \"Amazon CloudWatch is a monitoring and observability service...\"\n\nMore on CloudWatch [here](https://aws.amazon.com/cloudwatch)\n</b></details>\n\n<details>\n<summary>What is AWS CloudTrail?</summary><br><b>\n\nAWS definition: \"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\"\n\nRead more on CloudTrail [here](https://aws.amazon.com/cloudtrail)\n</b></details>\n\n<details>\n<summary>What is Simply Notification Service?</summary><br><b>\n\nAWS definition: \"a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\"\n\nRead more about it [here](https://aws.amazon.com/sns)\n</b></details>\n\n<details>\n<summary>Explain the following in regards to SNS:\n\n  * Topics\n  * Subscribers\n  * Publishers</summary><br><b>\n\n  * Topics - used for grouping multiple endpoints\n  * Subscribers - the endpoints where topics send messages to \n  * Publishers - the provider of the message (event, person, ...)\n</b></details>\n\n#### AWS Security \n\n<details>\n<summary>What is the shared responsibility model? What AWS is responsible for and what the user is responsible for based on the shared responsibility model?</summary><br><b>\n\nThe shared responsibility model defines what the customer is responsible for and what AWS is responsible for.\n\nMore on the shared responsibility model [here](https://aws.amazon.com/compliance/shared-responsibility-model)\n</b></details>\n\n<details>\n<summary>True or False? Based on the shared responsibility model, Amazon is responsible for physical CPUs and security groups on instances</summary><br><b>\n\nFalse. It is responsible for Hardware in its sites but not for security groups which created and managed by the users.\n</b></details>\n\n<details>\n<summary>Explain \"Shared Controls\" in regards to the shared responsibility model</summary><br><b>\n\nAWS definition: \"apply to both the infrastructure layer and customer layers, but in completely separate contexts or perspectives. In a shared control, AWS provides the requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services\"\n\nLearn more about it [here](https://aws.amazon.com/compliance/shared-responsibility-model)\n</b></details>\n\n<details>\n<summary>What is the AWS compliance program?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS Artifact?</summary><br><b>\n\nAWS definition: \"AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS‚Äô security and compliance reports and select online agreements.\"\n\nRead more about it [here](https://aws.amazon.com/artifact)\n</b></details>\n\n<details>\n<summary>What is AWS Inspector?</summary><br><b>\n\nAWS definition: \"Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.\"\"\n\nLearn more [here](https://aws.amazon.com/inspector)\n</b></details>\n\n<details>\n<summary>What is AWS Guarduty?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS Shield?</summary><br><b>\n\nAWS definition: \"AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.\"\n</b></details>\n\n<details>\n<summary>What is AWS WAF? Give an example of how it can used and describe what resources or services you can use it with</summary><br><b>\n</b></details>\n\n<details>\n<summary>What AWS VPN is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the difference between Site-to-Site VPN and Client VPN?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS CloudHSM?</summary><br><b>\n\nAmazon definition: \"AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud.\"\n\nLearn more [here](https://aws.amazon.com/cloudhsm)\n</b></details>\n\n<details>\n<summary>True or False? AWS Inspector can perform both network and host assessments</summary><br><b>\n\nTrue\n</b></details>\n\n<details>\n<summary>What is AWS Acceptable Use Policy?</summary><br><b>\n\nIt describes prohibited uses of the web services offered by AWS.\nMore on AWS Acceptable Use Policy [here](https://aws.amazon.com/aup)\n</b></details>\n\n<details>\n<summary>What is AWS Key Management Service (KMS)?</summary><br><b>\n\nAWS definition: \"KMS makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.\"\nMore on KMS [here](https://aws.amazon.com/kms)\n</b></details>\n\n<details>\n<summary>True or False? A user is not allowed to perform penetration testing on any of the AWS services</summary><br><b>\n\nFalse. On some services, like EC2, CloudFront and RDS, penetration testing is allowed.\n</b></details>\n\n<details>\n<summary>True or False? DDoS attack is an example of allowed penetration testing activity</summary><br><b>\n\nFalse.\n</b></details>\n\n<details>\n<summary>True or False? AWS Access Key is a type of MFA device used for AWS resources protection</summary><br><b>\n\nFalse. Security key is an example of an MFA device.\n</b></details>\n\n<details>\n<summary>What is Amazon Cognito?</summary><br><b>\n\nAmazon definition: \"Amazon Cognito handles user authentication and authorization for your web and mobile apps.\"\n\nLearn more [here](https://docs.aws.amazon.com/cognito/index.html)\n</b></details>\n\n<details>\n<summary>What is AWS ACM?</summary><br><b>\n\nAmazon definition: \"AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.\"\n\nLearn more [here](https://aws.amazon.com/certificate-manager)\n</b></details>\n\n#### AWS Databases\n\n<details>\n<summary>What is AWS RDS?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is AWS DynamoDB?</summary><br><b>\n</b></details>\n\n<details>\n<summary>Explain \"Point-in-Time Recovery\" feature in DynamoDB</summary><br><b>\n\nAmazon definition: \"You can create on-demand backups of your Amazon DynamoDB tables, or you can enable continuous backups using point-in-time recovery. For more information about on-demand backups, see On-Demand Backup and Restore for DynamoDB.\"\n\nLearn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html)\n</b></details>\n\n<details>\n<summary>Explain \"Global Tables\" in DynamoDB</summary><br><b>\n\nAmazon definition: \"A global table is a collection of one or more replica tables, all owned by a single AWS account.\"\n\nLearn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html)\n</b></details>\n\n<details>\n<summary>What is DynamoDB Accelerator?</summary><br><b>\n\nAmazon definition: \"Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement ‚Äì from milliseconds to microseconds...\"\n\nLearn more [here](https://aws.amazon.com/dynamodb/dax)\n</b></details>\n\n<details>\n<summary>What is AWS Redshift and how is it different than RDS?</summary><br><b>\n\ncloud data warehouse\n</b></details>\n\n<details>\n<summary>What is AWS ElastiCache? For what cases is it used?</summary><br><b>\n\nAmazon Elasticache is a fully managed Redis or Memcached in-memory data store.                                                                       \nIt's great for use cases like two-tier web applications where the most frequently accesses data is stored in ElastiCache so response time is optimal.\n</b></details>\n\n<details>\n<summary>What is Amazon Aurora</summary><br><b>\n\nA MySQL & Postgresql based relational database. Also, the default database proposed for the user when using RDS for creating a database.\nGreat for use cases like two-tier web applications that has a MySQL or Postgresql database layer and you need automated backups for your application.\n</b></details>\n\n<details>\n<summary>What is Amazon DocumentDB?</summary><br><b>\n\nAmazon definition: \"Amazon DocumentDB (with MongoDB compatibility) is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data.\"\n\nLearn more [here](https://aws.amazon.com/documentdb)\n</b></details>\n\n<details>\n<summary>What \"AWS Database Migration Service\" is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What type of storage is used by Amazon RDS?</summary><br><b>\n\nEBS\n</b></details>\n\n<details>\n<summary>Explain Amazon RDS Read Replicas</summary><br><b>\n\nAWS definition: \"Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\"\nRead more about [here](https://aws.amazon.com/rds/features/read-replicas)\n</b></details>\n\n#### AWS Serverless Compute\n\n<details>\n<summary>Explain what is AWS Lambda</summary><br><b>\n\nAWS definition: \"AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\"\n\nRead more on it [here](https://aws.amazon.com/lambda)\n</b></details>\n\n<details>\n<summary>True or False? In AWS Lambda, you are charged as long as a function exists, regardless of whether it's running or not</summary><br><b>\n\nFalse. Charges are being made when the code is executed.\n</b></details>\n\n<details>\n<summary>Which of the following set of languages Lambda supports?\n\n  * R, Swift, Rust, Kotlin\n  * Python, Ruby, Go \n  * Python, Ruby, PHP</summary><br><b>\n\n  * Python, Ruby, Go \n</b></details>\n\n#### Identify the service or tool\n\n<details>\n<summary>What would you use for automating code/software deployments?</summary><br><b>\n\nAWS CodeDeploy\n</b></details>\n\n<details>\n<summary>What would you use for easily creating similar AWS environments/resources for different customers?</summary><br><b>\n\nCloudFormation\n</b></details>\n\n<details>\n<summary>Which service would you use for building a website or web application?</summary><br><b>\n\nLightsail\n</b></details>\n\n<details>\n<summary>Which tool would you use for choosing between Reserved instances or On-Demand instances?</summary><br><b>\n\nCost Explorer\n</b></details>\n\n<details>\n<summary>What would you use to check how many unassociated Elastic IP address you have?</summary><br><b>\n\nTrusted Advisor\n</b></details>\n\n<details>\n<summary>What service allows you to transfer large amounts (Petabytes) of data in and out of the AWS cloud?</summary><br><b>\n\nAWS Snowball\n</b></details>\n\n<details>\n<summary>What provides a virtual network dedicated to your AWS account?</summary><br><b>\n\nVPC\n</b></details>\n\n<details>\n<summary>What you would use for having automated backups for an application that has MySQL database layer?</summary><br><b>\n\nAmazon Aurora\n</b></details>\n\n<details>\n<summary>What would you use to migrate on-premise database to AWS?</summary><br><b>\n\nAWS Database Migration Service (DMS)\n</b></details>\n\n<details>\n<summary>What would you use to check why certain EC2 instances were terminated?</summary><br><b>\n\nAWS CloudTrail\n</b></details>\n\n<details>\n<summary>What would you use for SQL database?</summary><br><b>\n\nAWS RDS\n</b></details>\n\n<details>\n<summary>What would you use for NoSQL database?</summary><br><b>\n\nAWS DynamoDB\n</b></details>\n\n<details>\n<summary>What would you use for running SQL queries interactively on S3?</summary><br><b>\n\nAWS Athena\n</b></details>\n\n<details>\n<summary>What would you use for adding image and video analysis to your application?</summary><br><b>\n\nAWS Rekognition\n</b></details>\n\n<details>\n<summary>Which service would you use for debugging and improving performances issues with your applications?</summary><br><b>\n\nAWS X-Ray\n</b></details>\n\n<details>\n<summary>Which service is used for sending notifications?</summary><br><b>\n\nSNS\n</b></details>\n\n<details>\n<summary>Which service would you use for monitoring malicious activity and unauthorized behavior in regards to AWS accounts and workloads?</summary><br><b>\n\nAmazon GuardDuty\n</b></details>\n\n<details>\n<summary>Which service would you use for centrally manage billing, control access, compliance, and security across multiple AWS accounts?</summary><br><b>\n\nAWS Organizations\n</b></details>\n\n<details>\n<summary>Which service would you use for web application protection?</summary><br><b>\n\nAWS WAF\n</b></details>\n\n<details>\n<summary>You would like to monitor some of your resources in the different services. Which service would you use for that?</summary><br><b>\n\nCloudWatch\n</b></details>\n\n<details>\n<summary>Which service would you use for performing security assessment?</summary><br><b>\n\nAWS Inspector\n</b></details>\n\n<details>\n<summary>Which service would you use for creating DNS record?</summary><br><b>\n\nRoute 53\n</b></details>\n\n<details>\n<summary>What would you use if you need a fully managed document database?</summary><br><b>\n\nAmazon DocumentDB\n</b></details>\n\n<details>\n<summary>Which service would you use to add access control (or sign-up, sign-in forms) to your web/mobile apps?</summary><br><b>\n\nAWS Cognito\n</b></details>\n\n<details>\n<summary>Which service would you use if you need messaging queue?</summary><br><b>\n\nSimple Queue Service (SQS)\n</b></details>\n\n<details>\n<summary>Which service would you use if you need managed DDOS protection?</summary><br><b>\n\nAWS Shield\n</b></details>\n\n<details>\n<summary>Which service would you use if you need store frequently used data for low latency access?</summary><br><b>\n\nElastiCache\n</b></details>\n\n<details>\n<summary>What would you use to transfer files over long distances between a client and an S3 bucket?</summary><br><b>\n\nAmazon S3 Transfer Acceleration\n</b></details>\n\n#### AWS Billing & Support\n\n<details>\n<summary>What is AWS Organizations?</summary><br><b>\n\nAWS definition: \"AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS.\"\nMore on Organizations [here](https://aws.amazon.com/organizations)\n</b></details>\n\n<details>\n<summary>Explain AWS pricing model</summary><br><b>\n\nIt mainly works on \"pay-as-you-go\" meaning you pay only for what are using and when you are using it.\nIn s3 you pay for 1. How much data you are storing 2. Making requests (PUT, POST, ...)\nIn EC2 it's based on the purchasing option (on-demand, spot, ...), instance type, AMI type and the region used.\n\nMore on AWS pricing model [here](https://aws.amazon.com/pricing)\n</b></details>\n\n<details>\n<summary>How one should estimate AWS costs when for example comparing to on-premise solutions?</summary><br><b>\n\n* TCO calculator\n* AWS simple calculator\n* Cost Explorer\n</b></details>\n\n<details>\n<summary>What basic support in AWS includes?</summary><br><b>\n\n* 24x7 customer service\n* Trusted Advisor\n* AWS personal Health Dashoard\n</b></details>\n\n<details>\n<summary>How are EC2 instances billed?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What AWS Pricing Calculator is used for?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is Amazon Connect?</summary><br><b>\n\nAmazon definition: \"Amazon Connect is an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost.\"\n\nLearn more [here](https://aws.amazon.com/connect)\n</b></details>\n\n<details>\n<summary>What are \"APN Consulting Partners\"?</summary><br><b>\n\nAmazon definition: \"APN Consulting Partners are professional services firms that help customers of all types and sizes design, architect, build, migrate, and manage their workloads and applications on AWS, accelerating their journey to the cloud.\"\n\nLearn more [here](https://aws.amazon.com/partners/consulting)\n</b></details>\n\n<details>\n<summary>Which of the following are AWS accounts types (and are sorted by order)?\n\n  * Basic, Developer, Business, Enterprise\n  * Newbie, Intermediate, Pro, Enterprise\n  * Developer, Basic, Business, Enterprise\n  * Beginner, Pro, Intermediate Enterprise</summary><br><b>\n\n  * Basic, Developer, Business, Enterprise\n</b></details>\n\n<details>\n<summary>True or False? Region is a factor when it comes to EC2 costs/pricing</summary><br><b>\n\nTrue. You pay differently based on the chosen region.\n</b></details>\n\n<details>\n<summary>What is \"AWS Infrastructure Event Management\"?</summary><br><b>\n\nAWS Definition: \"AWS Infrastructure Event Management is a structured program available to Enterprise Support customers (and Business Support customers for an additional fee) that helps you plan for large-scale events such as product or application launches, infrastructure migrations, and marketing events.\"\n</b></details>\n\n#### AWS Automation\n\n<details>\n<summary>What is AWS CodeDeploy?</summary><br><b>\n\nAmazon definition: \"AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.\"\n\nLearn more [here](https://aws.amazon.com/codedeploy)\n</b></details>\n\n<details>\n<summary>Explain what is CloudFormation</summary><br><b>\n</b></details>\n\n#### AWS Misc\n\n<details>\n<summary>What is AWS Lightsail?</summary><br><b>\n\nAWS definition: \"Lightsail is an easy-to-use cloud platform that offers you everything needed to build an application or website, plus a cost-effective, monthly plan.\"\n</b></details>\n\n<details>\n<summary>What is AWS Rekognition?</summary><br><b>\n\nAWS definition: \"Amazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use.\"\n\nLearn more [here](https://aws.amazon.com/rekognition)\n</b></details>\n\n<details>\n<summary>What AWS Resource Groups used for?</summary><br><b>\n\nAmazon definition: \"You can use resource groups to organize your AWS resources. Resource groups make it easier to manage and automate tasks on large numbers of resources at one time. \"\n\nLearn more [here](https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html)\n</b></details>\n\n<details>\n<summary>What is AWS Global Accelerator?</summary><br><b>\n\nAmazon definition: \"AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users...\"\n\nLearn more [here](https://aws.amazon.com/global-accelerator)\n</b></details>\n\n<details>\n<summary>What is AWS Config?</summary><br><b>\n\nAmazon definition: \"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources.\"\n\nLearn more [here](https://aws.amazon.com/config)\n</b></details>\n\n<details>\n<summary>What is AWS X-Ray?</summary><br><b>\n\nAWS definition: \"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.\"\nLearn more [here](https://aws.amazon.com/xray)\n</b></details>\n\n<details>\n<summary>What is AWS OpsWorks?</summary><br><b>\n\nAmazon definition: \"AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.\"\n\nLearn more about it [here](https://aws.amazon.com/opsworks)\n</b></details>\n\n<details>\n<summary>What is AWS Service Catalog?</summary><br><b>\n\nAmazon definition: \"AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS.\"\n\nLearn more [here](https://aws.amazon.com/servicecatalog)\n</b></details>\n\n<details>\n<summary>What is AWS CAF?</summary><br><b>\n\nAmazon definition: \"AWS Professional Services created the AWS Cloud Adoption Framework (AWS CAF) to help organizations design and travel an accelerated path to successful cloud adoption. \"\n\nLearn more [here](https://aws.amazon.com/professional-services/CAF)\n</b></details>\n\n<details>\n<summary>What is AWS Cloud9?</summary><br><b>\n\nAWS definition: \"AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser\"\n</b></details>\n\n<details>\n<summary>What is AWS Application Discovery Service?</summary><br><b>\n\nAmazon definition: \"AWS Application Discovery Service helps enterprise customers plan migration projects by gathering information about their on-premises data centers.\"\n\nLearn more [here](https://aws.amazon.com/application-discovery)\n</b></details>\n\n<details>\n<summary>What is the Trusted Advisor?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the AWS well-architected framework and what pillars it's based on?</summary><br><b>\n\nAWS definition: \"The Well-Architected Framework has been developed to help cloud architects build secure, high-performing, resilient, and efficient infrastructure for their applications. Based on five pillars ‚Äî operational excellence, security, reliability, performance efficiency, and cost optimization\"\n\nLearn more [here](https://aws.amazon.com/architecture/well-architected)\n</b></details>\n\n<details>\n<summary>What AWS services are serverless (or have the option to be serverless)?</summary><br><b>\n\nAWS Lambda\nAWS Athena\n</b></details>\n\n<details>\n<summary>What is AWS EMR?</summary><br><b>\n\nAWS definition: \"big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto.\"\n\nLearn more [here](https://aws.amazon.com/emr)\n</b></details>\n\n<details>\n<summary>What is AWS Athena?</summary><br><b>\n\n\"Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.\"\n\nLearn more about AWS Athena [here](https://aws.amazon.com/athena)\n</b></details>\n\n<details>\n<summary>What is Amazon Cloud Directory?</summary><br><b>\n\nAmazon definition: \"Amazon Cloud Directory is a highly available multi-tenant directory-based store in AWS. These directories scale automatically to hundreds of millions of objects as needed for applications.\"\n\nLearn more [here](https://docs.aws.amazon.com/clouddirectory/latest/developerguide/what_is_cloud_directory.html)\n</b></details>\n\n<details>\n<summary>What is AWS Elastic Beanstalk?</summary><br><b>\n\nAWS definition: \"AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services...You can simply upload your code and Elastic Beanstalk automatically handles the deployment\"\n\nLearn more about it [here](https://aws.amazon.com/elasticbeanstalk)\n</b></details>\n\n<details>\n<summary>What is AWS SWF?</summary><br><b>\n\nAmazon definition: \"Amazon SWF helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud.\"\n\nLearn more on Amazon Simple Workflow Service [here](https://aws.amazon.com/swf)\n</b></details>\n\n<details>\n<summary>What is Simple Queue Service (SQS)?</summary><br><b>\n\nAWS definition: \"Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications\".\n\nLearn more about it [here](https://aws.amazon.com/sqs)\n</b></details>\n\n#### AWS Disaster Recovery\n\n<details>\n<summary>In regards to disaster recovery, what is RTO and RPO?</summary><br><b>\n\nRTO - The maximum acceptable length of time that your application can be offline.\n\nRPO - The maximum acceptable length of time during which data might be lost from your application due to an incident.\n</b></details>\n\n<details>\n<summary>What types of disaster recovery techniques AWS supports?</summary><br><b>\n\n* The Cold Method - Periodically backups and sending the backups off-site<br>\n* Pilot Light - Data is mirrored to an environment which is always running\n* Warm Standby - Running scaled down version of production environment\n* Multi-site - Duplicated environment that is always running\n</b></details>\n\n<details>\n<summary>Which disaster recovery option has the highest downtime and which has the lowest?</summary><br><b>\n\nLowest - Multi-site\nHighest - The cold method\n</b></details>\n\n### Final Note\n\nGood luck! You can do it :)\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "certificates/aws-solutions-architect-associate.md", "language": "markdown", "complexity": "complex", "star_count": 77729, "quality_score": 0.79, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 1203}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AZ-900\n\n<details>\n<summary>What is cloud computing?</summary><br><b>\n\n[Wikipedia](https://en.wikipedia.org/wiki/Cloud_computing): \"Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user\"\n</b></details>\n\n<details>\n<summary>What types of clouds (or cloud deployments) are there?</summary><br><b>\n\n  * Public - Cloud services sharing computing resources among multiple customers\n  * Private - Cloud services having computing resources limited to specific customer or organization, managed by third party or organizations itself\n  * Hybrid - Combination of public and private clouds\n</b></details>\n\n\n<details>\n<summary>What is Azure Firewall?</summary><br><b>\n\nAzure Firewall is a cloud-native and intelligent network firewall security service that provides the best of breed threat protection for your cloud workloads running in Azure.\n</b></details>\n\n\n<details>\n<summary>What is Network Security Group?</summary><br><b>\n\nA network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. For each rule, you can specify source and destination, port, and protocol.\n</b></details>\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "certificates/azure-fundamentals-az-900.md", "language": "markdown", "complexity": "simple", "star_count": 77729, "quality_score": 0.8, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability using Terraform infrastructure as code\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## Credits\n\nJenkins logo created by <a href='https://twitter.com/ks_nenasheva'>Ksenia Nenasheva</a> and published through <a href=\"https://jenkins.io\">jenkins.io</a> is licensed under <a href=\"https://creativecommons.org/licenses/by-sa/3.0/\">cc by-sa 3.0</a><br>\nGit Logo by <a href=\"https://twitter.com/jasonlong\">Jason Long</a> is licensed under the <a href=\"https://creativecommons.org/licenses/by/3.0/\">Creative Commons Attribution 3.0 Unported License</a><br>\nTerraform logo created by <a href=\"https://www.hashicorp.com\">Hashicorp¬Æ</a><br>\nDocker logo created by <a href=\"https://www.docker.com\">Docker¬Æ</a><br>\nThe Python logo is a trademark of the Python Software Foundation¬Æ<br>\nPuppet logo created by <a href=\"https://puppet.com\">Puppet¬Æ</a><br>\nBash logo created by Prospect One<br>\nOpenStack logo created by and a trademark of The <a href=\"https://www.openstack.org\">OpenStack Foundation¬Æ</a><br>\nLinux, Kubernetes and Prometheus logos are trademarks of The Linux Foundation¬Æ<br>\nMongo logo is a trademark of <a href=\"http://www.mongodb.com\">Mongo¬Æ</a><br>\nDistributed logo by <a href=\"https://www.iconfinder.com/Flatart\">Flatart</a><br>\nChallenge icon by Elizabeth Arostegui in Technology Mix\n\"Question you ask\" (man raising hand) and \"Database\" icons by [Webalys](https://www.iconfinder.com/webalys)\nTesting logo by [Flatart](https://www.iconfinder.com/Flatart)<br>\nGoogle Cloud Plataform Logo created by <a href=\"https://about.google/\">Google¬Æ</a><br>\nVirtualBox Logo created by <a href=\"http://www.iconarchive.com/artist/dakirby309.html\">dAKirby309</a>, under the <a href=\"https://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-Noncommercial 4.0 License</a>.\nCertificates logo by <a href=\"https://www.iconfinder.com/Flatart\">Flatart</a><br>\nStorage icon by <a href=\"https://www.iconfinder.com/iconic_hub\">Dinosoftlab</a><br>\nCI/CD icon made made by <a href=\"https://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a></div>\nChaos Engineering logo made by Arie Bregman\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "credits.md", "language": "markdown", "complexity": "simple", "star_count": 77729, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": true, "has_terraform": true, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## FAQ\n\nMost frequently asked questions.\n\n### What is the purpose of repository?\n\nLearning, of course.\n\n### My goal is to prepare for a DevOps interviews. Should I use this repository?\n\nOverall, this repository should help you learn some concepts but, don't assume at any point that your interview will include similar questions to those that included in this repository.\nRegarding interviews, I've added a couple of suggestions [here](prepare_for_interview.md)<br>\n\n### Will you stop at some point adding questions and exercises?\n\nAll good things come to an end...\n\n### How do I become a better DevOps Engineer?\n\nThat's a great question.<br>\nI don't have a definitive answer for this question, I'm exploring it myself from time to time. What I believe helps is to:\n\n  * Practice - Practicing DevOps practically should be the primary way to become a DevOps engineer in my opinion\n  * Read - blogs, books, ... anything that can enrich your knowledge about DevOps or related DevOps topics\n  * Participate - there are great DevOps communities. I personally like [Reddit DevOps community](https://www.reddit.com/r/devops). Visiting there, I learn quite a lot on different topics.\n  * Share - This is one of the reasons I created this project. Primary goal was to help others but a secondary goal quickly became to learn more. By asking questions, you actually learn better a certain topic. Try it out, take a certain subject and try to come up with questions you would ask someone to test his/her skills about that topic.\n\n### Why most of the questions don't have answers?\n\n1. Because we need more contributors\n2. Because often asking questions is easier than answering them\n\n### Where can I find answers to some of the questions in this repository?\n\n1. Search for them using search engines, documentation pages, ... this is part of being a DevOps engineer\n2. Use the communities: many people will be happy to help and answer your questions\n3. Ask us. If you want, you can contact me or start a discussion on this project.\n\n### Where the questions and answers are coming from?\n\nWell, everywhere! - past experience, colleagues, contributors, ... but please note we do not allow copying interview questions from interview questions sites to here. There are people who worked hard on adding those to their sites and we respect that.<br>\nAs an evidence, we did deny pull requests with copied content from other sites.\n\n### What are the top DevOps skills required for being a DevOps Engineer?\n\nIt's a hard question and the reason is that if you'll ask 20 different people, you'll probably get at least 10 different answers but here is what I believe is common today:\n\n* OS - DevOps require you good understanding of operating system concepts. The level required is mainly depends on the company although in my opinion it should be the same level. You should understand how the operating system works, how to troubleshoot and debug issues, etc.\n* Programming is part of DevOps. The level again depends on the company. Some will require you to know basic level of scripting while others deep understanding of common algorithms, data structure, design patterns etc.\n* Cloud and Containers - while not 100% must in all companies/positions, this skill is on the rise every year and many (if not most) of the positions/companies require this skill. This specifically means: AWS/Azure/GCP, Docker/Podman, Kubernetes, ...\n* CI/CD - Be able to to answer questions like \"Why do we need CI/CD?\" and \"What ways and models are there to perform CI/CD?\". Eventually, practice assembling such processes and workflow, using whatever tools you feel comfortable with.\n\n### I feel like there are some questions that shouldn't be included in this project\n\nIs that a question? :)<br>\nIf you don't like some of the questions or think that some questions should be removed you can open an issue or submit a PR and we can discuss it there. We don't have rules against deleting questions (for now :P)\n\n### Can I copy the questions from here to my site?\n\nYou can (although I have no idea why would you want to), but:\n\n* Not without attribution. Many people worked hard on adding these questions and they deserve a proper credit for their work\n* Not if you plan to make money out of it. Directly or indirectly (e.g. ADS) as this is a free content and we would like it to stay this way :)\n\nSame goes for copying questions from different sources to this repository. We saw it happened already with a couple of pull requests and we rejected them. We will not merge pull requests with copied questions and answers from other sources.\n\n### Can I add questions and/or answers to this project?\n\nI'll simply imagine you didn't ask that on an open source project... :)\n\n### Why can't I add installation questions?\n\nIn general, I prefer questions added to this repository will have certain educational value for the user. Either regarding a certain concept or even a very general question, but one that will make the user research on a certain topic and will make him eventually more familiar with some of its core concepts.<br>\nI know that this is not the case for every question in this repo as of today (e.g. questions about specific commands) but this is definitely something to aspire for.\n\nI see little to none value in what is known as \"Installation Questions\". Let's say I ask you \"how to install Jenkins?\". Should I conclude from your answer that you are familiar with what is Jenkins and/or how it works? In other words, is there a value in knowing how to install Jenkins? In my opinion, no.\n\n### Where can I practice coding?\n\nPersonally, I really like the following sites\n\n* [HackerRank](https://www.hackerrank.com)\n* [LeetCode](https://leetcode.com)\n* [Exercism](https://exercism.io)\n\n### How to learn more DevOps?\n\nI listed some roadmaps in [devops-resources](https://github.com/bregman-arie/devops-resources)\n\n### Why some questions repeat themselves?\n\nIf you see two identical questions, that's a bug.<br>\nIf you see two similar questions, that's a feature :D (= it's intentional)\n\nFor example:\n\n1. What is horizontal scaling?\n2. The act of adding additional instances to the pool to handle scaling is called ________ scaling\n\nYou are right, both ask about horizontal scaling but it's done from a different angle in every question and in addition, I do believe repetition helps you to learn something in a way where you are not fixed on the way it's asked, rather you understand the concept itself.\n\n### Are you open for making big changes in the repository?\n\nAbsolutely. Don't be afraid to raise ideas and start discussions.<br>\nI'll be more than happy to discuss any change you think we should make to improve the learning experience\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "faq.md", "language": "markdown", "complexity": "medium", "star_count": 77729, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 106}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/bin/bash\n\n# always put up the detail of scripts . version, author, what it does, what event triggers and all .. \n\n###\n# Author: Adarsh Rawat\n# Version: 1.0.0\n# Objective: Automate Notification for a object uploaded or created in s3 bucket.\n###\n\n# debug what is happening\nset -x\n\n# all these cmds are aws cli commands | abhishek veermalla day 4-5 devops\n\n# store aws account id in a variable\naws_account_id=$(aws sts get-caller-identity --query 'Account' --output text)\n\n# print the account id from the variable\necho \"aws account id: $aws_account_id\"\n\n# set aws region, bucket name and other variables\naws_region=\"us-east-1\" \naws_bucket=\"s3-lambda-event-trigger-bucket\"\naws_lambda=\"s3-lambda-function-1\"\naws_role=\"s3-lambda-sns\"\nemail_address=\"adarshrawat8304@gmail.com\"\n\n# create iam role for the project\nrole_response=$(aws iam create-role --role-name s3-lambda-sns --assume-role-policy-document '{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [{\n    \"Action\": \"sts:AssumeRole\",\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n      \"Service\": [\n        \"lambda.amazonaws.com\",\n        \"s3.amazonaws.com\",\n        \"sns.amazonaws.com\"\n      ]\n    }\n  }]\n}')\n\n# jq is json parser here parse the role we created \n\n# extract the role arn from json resposne and store in variable\nrole_arn=$(echo \"$role_response\" | jq -r '.Role.Arn')\n\n# print the role arn\necho \"Role ARN: $role_arn\"\n\n# attach permissions to the role\naws iam attach-role-policy --role-name $aws_role --policy-arn arn:aws:iam::aws:policy/AWSLambda_FullAccess\naws iam attach-role-policy --role-name $aws_role --policy-arn arn:aws:iam::aws:policy/AmazonSNSFullAccess\n\n# create s3 bucket and get the output in a variable\nbucket_output=$(aws s3api create-bucket --bucket \"$aws_bucket\" --region \"$aws_region\")\n\n# print the output from the variable \necho \"bucket output: $bucket_output\"\n\n# upload a file to the bucket\naws s3 cp ./sample.png s3://\"$aws_bucket\"/sample.png\n\n# create a zip file to upload lambda function \nzip -r s3-lambda.zip ./s3-lambda\n\nsleep 5\n\n# create a lambda function\naws lambda create-function \\\n  --region $aws_region \\\n  --function $aws_lambda \\\n  --runtime \"python3.8\" \\\n  --handler \"s3-lambda/s3-lambda.lambda_handler\" \\\n  --memory-size 128 \\\n  --timeout 30 \\\n  --role \"arn:aws:iam::$aws_account_id:role/$aws_role\" \\\n  --zip-file \"fileb://./s3-lambda.zip\"\n\n# add permissions to s3 bucket to invoke lambda\nLambdaFunctionArn=\"arn:aws:lambda:us-east-1:$aws_account_id:function:s3-lambda\"\naws s3api put-bucket-notification-configuration \\\n  --region \"$aws_region\" \\\n  --bucket \"$aws_bucket\" \\\n  --notification-configuration '{\n    \"LambdaFunctionConfigurations\": [{\n        \"LambdaFunctionArn\": \"'\"$LambdaFunctionArn\"'\",\n        \"Events\": [\"s3:ObjectCreated:*\"]\n    }]\n}'\n\naws s3api put-bucket-notification-configuration \\\n  --region \"$aws_region\" \\\n  --bucket \"$aws_bucket\" \\\n  --notification-configuration '{\n    \"LambdaFunctionConfigurations\": [{\n        \"LambdaFunctionArn\": \"'\"$LambdaFunctionArn\"'\",\n        \"Events\": [\"s3:ObjectCreated:*\"]\n    }]\n}'\n\n# create an sns topic and save the topic arn to a variable\ntopic_arn=$(aws sns create-topic --name s3-lambda-sns --output json | jq -r '.TopicArn')\n\n# print the topic arn\necho \"SNS Topic ARN: $topic_arn\"\n\n# Trigger SNS topic using lambda function\n\n# Add sns topic using lambda function\naws sns subscribe \\\n  --topic-arn \"$topic_arn\" \\\n  --protocol email \\\n  --notification-endpoint \"$email_address\"\n\n# publish sns\naws sns publish \\\n  --topic-arn \"$topic_arn\" \\\n  --subject \"A new object created in s3 bucket\" \\\n  --message \"Hey, a new data object just got delievered into the s3 bucket $aws_bucket\" \n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "scripts/aws s3 event triggering/aws_s3_event_trigger.sh", "language": "bash", "complexity": "medium", "star_count": 77729, "quality_score": 0.8600000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 123}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<details>\n<summary>Explain the following code:\n\n<code>:(){ :|:& };:</code>\n\n</summary><br><b>\n</b></details>\n\n<details>\n<summary>Can you give an example to some Bash best practices?</summary><br><b>\n</b></details>\n\n<details>\n<summary>What is the ternary operator? How do you use it in bash?</summary><br><b>\n\nA short way of using if/else. An example:\n\n[[ $a = 1 ]] && b=\"yes, equal\" || b=\"nope\"\n</b></details>\n\n<details>\n<summary>What does the following code do and when would you use it?\n\t\n<code>diff <(ls /tmp) <(ls /var/tmp)</code>\n\n</summary><br>\nIt is called 'process substitution'. It provides a way to pass the output of a command to another command when using a pipe <code>|</code> is not possible. It can be used when a command does not support <code>STDIN</code> or you need the output of multiple commands. \nhttps://superuser.com/a/1060002/167769\n</details>\n\n\n## SQL\n\n<a name=\"sql-beginner\"></a>\n#### :baby: Beginner\n\n<details>\n<summary>What does SQL stand for?</summary><br><b>\n\nStructured Query Language\n\n</b></details>\n\n<details>\n<summary>How is SQL Different from NoSQL</summary><br><b>\n\nThe main difference is that SQL databases are structured (data is stored in the form of\ntables with rows and columns - like an excel spreadsheet table) while NoSQL is \nunstructured, and the data storage can vary depending on how the NoSQL DB is set up, such\nas key-value pair, document-oriented, etc.\n</b></details>\n\n<details>\n<summary>What does it mean when a database is ACID compliant?</summary><br>\n\nACID stands for Atomicity, Consistency, Isolation, Durability. In order to be ACID compliant, the database much meet each of the four criteria\n\n**Atomicity** - When a change occurs to the database, it should either succeed or fail as a whole. \n\nFor example, if you were to update a table, the update should completely execute. If it only partially executes, the \nupdate is considered failed as a whole, and will not go through - the DB will revert back to it's original\nstate before the update occurred. It should also be mentioned that Atomicity ensures that each \ntransaction is completed as it's own stand alone \"unit\" - if any part fails, the whole statement fails.\n\n**Consistency** - any change made to the database should bring it from one valid state into the next.\n\nFor example, if you make a change to the DB, it shouldn't corrupt it. Consistency is upheld by checks and constraints that\nare pre-defined in the DB. For example, if you tried to change a value from a string to an int when the column\nshould be of datatype string, a consistent DB would not allow this transaction to go through, and the action would\nnot be executed\n\n**Isolation** - this ensures that a database will never be seen \"mid-update\" - as multiple transactions are running at\nthe same time, it should still leave the DB in the same state as if the transactions were being run sequentially.\n\nFor example, let's say that 20 other people were making changes to the database at the same time. At the\ntime you executed your query, 15 of the 20 changes had gone through, but 5 were still in progress. You should\nonly see the 15 changes that had completed - you wouldn't see the database mid-update as the change goes through.\n\n**Durability** - Once a change is committed, it will remain committed regardless of what happens\n(power failure, system crash, etc.). This means that all completed transactions \nmust be recorded in non-volatile memory. \n\nNote that SQL is by nature ACID compliant. Certain NoSQL DB's can be ACID compliant depending on \nhow they operate, but as a general rule of thumb, NoSQL DB's are not considered ACID compliant\n</details>\n\n<details>\n<summary>When is it best to use SQL? NoSQL?</summary><br><b>\n\nSQL - Best used when data integrity is crucial. SQL is typically implemented with many\nbusinesses and areas within the finance field due to it's ACID compliance.\n\nNoSQL - Great if you need to scale things quickly. NoSQL was designed with web applications \nin mind, so it works great if you need to quickly spread the same information around to \nmultiple servers\n\nAdditionally, since NoSQL does not adhere to the strict table with columns and rows structure\nthat Relational Databases require, you can store different data types together.\n</b></details>\n\n<details>\n<summary>What is a Cartesian Product?</summary><br>\n\nA Cartesian product is when all rows from the first table are joined to all rows in the second\ntable. This can be done implicitly by not defining a key to join, or explicitly by \ncalling a CROSS JOIN on two tables, such as below:\n\nSelect * from customers **CROSS JOIN** orders;\n\nNote that a Cartesian product can also be a bad thing - when performing a join\non two tables in which both do not have unique keys, this could cause the returned information\nto be incorrect. \n</details>\n\n##### SQL Specific Questions\n\nFor these questions, we will be using the Customers and Orders tables shown below:\n\n**Customers**\n\nCustomer_ID | Customer_Name | Items_in_cart | Cash_spent_to_Date\n------------ | ------------- | ------------- | -------------\n100204 | John Smith | 0 | 20.00\n100205 | Jane Smith | 3 | 40.00\n100206 | Bobby Frank | 1 | 100.20\n\n**ORDERS**\n\nCustomer_ID | Order_ID | Item | Price | Date_sold\n------------ | ------------- | ------------- | ------------- | -------------\n100206 | A123 | Rubber Ducky | 2.20 | 2019-09-18\n100206 | A123 | Bubble Bath | 8.00 | 2019-09-18\n100206 | Q987 | 80-Pack TP | 90.00 | 2019-09-20\n100205 | Z001 | Cat Food - Tuna Fish | 10.00 | 2019-08-05\n100205 | Z001 | Cat Food - Chicken | 10.00 | 2019-08-05\n100205 | Z001 | Cat Food - Beef | 10.00 | 2019-08-05\n100205 | Z001 | Cat Food - Kitty quesadilla | 10.00 | 2019-08-05\n100204 | X202 | Coffee | 20.00 | 2019-04-29\n\n<details>\n<summary>How would I select all fields from this table?</summary><br><b>\n\nSelect * <br>\nFrom Customers;\n</b></details>\n\n<details>\n<summary>How many items are in John's cart?</summary><br><b>\n\nSelect Items_in_cart <br>\nFrom Customers <br>\nWhere Customer_Name = \"John Smith\";\n</b></details>\n\n<details>\n<summary>What is the sum of all the cash spent across all customers?</summary><br><b>\n\nSelect SUM(Cash_spent_to_Date) as SUM_CASH <br>\nFrom Customers;\n</b></details>\n\n<details>\n<summary>Tell me about your last big project/task you worked on</summary><br><b>\n</b></details>\n\n<details>\n<summary>What was most challenging part in the project you worked on?</summary><br><b>\n</b></details>\n\n<details>\n<summary>Why do you want to work here?</summary><br><b>\n</b></details>\n\n<details>\n<summary>How did you hear about us?</summary><br><b>\n\nTell them how did you hear about them :D\nRelax, there is no wrong or right answer here...I think.\n</b></details>", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "tests/testcases/testcase2.md", "language": "markdown", "complexity": "complex", "star_count": 77729, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 179}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Argo\n\n<!-- {% raw %} -->\n\n- [Argo](#argo)\n  - [ArgoCD Exercises](#argocd-exercises)\n    - [ArgoCD 101](#argocd-101)\n    - [ArgoCD Secrets](#argocd-secrets)\n    - [ArgoCD Helm](#argocd-helm)\n    - [Argo Rollouts](#argo-rollouts)\n  - [ArgoCD Questions](#argocd-questions)\n    - [ArgoCD 101](#argocd-101-1)\n    - [Practical ArgoCD 101](#practical-argocd-101)\n      - [CLI](#cli)\n    - [ArgoCD Configuration](#argocd-configuration)\n    - [Advanced ArgoCD](#advanced-argocd)\n    - [ArgoCD Application Health](#argocd-application-health)\n    - [ArgoCD Syncs](#argocd-syncs)\n    - [ArgoCD and Helm](#argocd-and-helm)\n  - [Argo Rollouts Questions](#argo-rollouts-questions)\n    - [Argo Rollouts 101](#argo-rollouts-101)\n    - [Argo Advanced Rollouts](#argo-advanced-rollouts)\n    - [Argo Rollouts Commands](#argo-rollouts-commands)\n\n## ArgoCD Exercises\n\n### ArgoCD 101\n\n|Name|Topic|Objective & Instructions|Solution|Comments|\n|--------|--------|------|----|----|\n| Creating an App | App | [Exercise](exercises/app_creation/exercise.md) | [Solution](exercises/app_creation/solution.md)\n| Syncing App - Git | Sync | [Exercise](exercises/sync_app_git/exercise.md) | [Solution](exercises/sync_app_git/solution.md)\n| Syncing App - Cluster | Sync | [Exercise](exercises/sync_app_cluster/exercise.md) | [Solution](exercises/sync_app_cluster/solution.md)\n\n### ArgoCD Secrets\n\n|Name|Topic|Objective & Instructions|Solution|Comments|\n|--------|--------|------|----|----|\n| Secrets 101 | Secrets | [Exercise](exercises/secrets_101/exercise.md) | [Solution](exercises/secrets_101/solution.md)\n\n### ArgoCD Helm\n\n|Name|Topic|Objective & Instructions|Solution|Comments|\n|--------|--------|------|----|----|\n| Helm ArgoCD App | Secrets | [Exercise](exercises/argocd_helm_app/exercise.md) | [Solution](exercises/argocd_helm_app/solution.md)\n\n### Argo Rollouts\n\n|Name|Topic|Objective & Instructions|Solution|Comments|\n|--------|--------|------|----|----|\n| Blue/Green Rollout | Rollouts | [Exercise](exercises/blue_green_rollout/exercise.md) | [Solution](exercises/blue_green_rollout/solution.md)\n| Canary Rollout | Rollouts | [Exercise](exercises/canary_rollout/exercise.md) | [Solution](exercises/canary_rollout/solution.md)\n\n## ArgoCD Questions\n\n### ArgoCD 101\n\n<details>\n<summary>What is Argo CD?</summary><br><b>\n\n[ArgoCD](https://argo-cd.readthedocs.io/en/stable): \"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.\"\n\nAs to why Argo CD, they provide the following explanation: \"Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand.\"\n\n</b></details>\n\n<details>\n<summary>There been a lot of CI/CD systems before ArgoCD (Jenkins, Teamcity, CircleCI, etc.) What added value ArgoCD brought?</summary><br><b>\n\nSimply said, ArgoCD is CD, not CI. We still need CI systems. Secondly, ArgoCD is running on Kubernetes, it's part of its ecosystem, as opposed to some other CI/CD systems. Finally, ArgoCD was built specifically for Kubernetes, not other platforms and systems.\n\nEasier to explain the need for ArgoCD by direct comparison to another system that can do CD. Let's use Jenkins for this.\n\nWith Jenkins, you need make sure to install k8s related tools and set access for commands like kubectl.\nWith ArgoCD you simply need to install it in your namespace but no need to install additional tools as it's part of k8s.\n\nWith Jenkins, managing access is usually done per pipeline and even if set globally in Jenkins, you still need to configure each pipeline to use that access configuration.\nWith ArgoCD access management to k8s and other resources is given as it runs already on the cluster, in one or multiple namespaces.\n\nWith Jenkins, tracking the status of what got deployed to k8s can be done only as an extra step, by running the pipeline. This is because Jenkins isn't part of the k8s cluster.\nWith ArgoCD you get much better tracking and visibility of what gets deployed as it runs in the same cluster and the same namespace.\n\nWith ArgoCD it's really easy to roll back to a previous version because all the changes done, are done to git which is a versioned source control. So it's enough to get to a previous commit for ArgoCD to detect a change and sync to the cluster. Worth to mention, this point specifically is true for Jenkins as well :)\n</b></details>\n\n<details>\n<summary>Describe an example of workflow where ArgoCD is used</summary><br><b>\n\n1. A developer submitted change to an application repository\n2. Jenkins pipeline is triggered to run CI on the change\n3. If the Jenkins Pipeline completed successfully, build an image out of the new code\n4. Push to image to a registry\n5. Update K8S manifest file(s) in a separate app config repository\n6. ArgoCD tracks changes in the app config repository. Since there was a change in the repository, it will apply the changes from the repo\n</b></details>\n\n<details>\n<summary>True or False? ArgoCD supports Kubernetes YAML files but not other manifests formats like Helm Charts and Kustomize</summary><br><b>\n\nFalse. It supports Kubernetes YAML files as well as Helm Charts and Kustomize.\n\n</b></details>\n\n<details>\n<summary>What \"GitOps Repository\" means in regards to ArgoCD?</summary><br><b>\n\nIt's the repository that holds app configuration, the one updated most of the time by CI/CD processes or DevOps, SRE engineers. In regards to ArgoCD it's the repository ArgoCD tracks for changes and apply them when they are detected.\n\n</b></details>\n\n<details>\n<summary>What are the advantages in using GitOps approach/repository?</summary><br><b>\n\n* Your whole configuration is one place, defined as code so it's completely transparent, adjustable for changes and easily reproducible\n* Everyone go through the same interface hence you have more people experiencing and testing the code, even if not intentionally\n* Engineers can use it for testing, development, ... there is no more running manual commands and hoping to reach the same status as in the cluster/cloud.\n* Single source of truth: you know that your GitOps is the repo from which changes can be done to the cluster. So even if someone tries to manually override it, it won't work.\n</b></details>\n\n<details>\n<summary>Sorina, one of the engineers in your team, made manual changes to the cluster that override some of the configuration in a repo traced by ArgoCD. What will happen?</summary><br><b>\n\nOnce Sorina made the modifications, ArgoCD will detect the state diverged and will sync the changes from the GitOps repository, overwriting the manual changes done by Sorina.\n\n</b></details>\n\n<details>\n<summary>Nate, one of the engineers in your organization, asked whether it's possible if ArgoCD didn't sync for changes done manually to the cluster. What would be your answer?</summary><br><b>\n\nThe answer is yes, it's possible. You can configure ArgoCD to sync to desired state when changes done manually and instead do something like sending alerts.\n\n</b></details>\n\n<details>\n<summary>How cluster disaster recovery becomes easier with ArgoCD?</summary><br><b>\n\nImagine you have a cluster in the cloud, in one of the regions. Something happens to that cluster and it's either crashes or simply no longer operational.\n\nIf you have all your cluster configuration in a GitOps repository, ArgoCD can be pointed to that repository while be configured to use a new cluster you've set up and apply that configuration so your cluster is again up and running with the same status as o\n</b></details>\n\n<details>\n<summary>Ella, an engineer in your team, claims ArgoCD benefit is that it's an extension Kubernetes, it's part of the cluster. Sarah, also an engineer in your team, claims it's not a real benefit as Jenkins can be also deployed in the cluster hence being part of it. What's your take?</summary><br><b>\n\nElla is right, ArgoCD is an extension of the cluster, that is very different from simply being deployed in the cluster as other CI/CD systems like Jenkins. ArgoCD uses existing k8s resources like K8s controllers (for monitoring and state differences) and etcd for storing data.\n</b></details>\n\n<details>\n<summary>How the main resource in ArgoCD called?</summary><br><b>\n\n\"Application\"\n</b></details>\n\n<details>\n<summary>Explain what is an \"Application\" in regards to ArgoCD</summary><br><b>\n\nIt's a custom resource definitions which responsible for the deployment and synchronization of application resources to a Kubernetes cluster.\n</b></details>\n\n<details>\n<summary>How ArgoCD makes access management in the cluster easier?</summary><br><b>\n\nInstead of creating Kubernetes resources, you can use Git to manage who is allowed to push code, to review it, merge it, etc - either human users or 3rd party systems and services. There is no need to use ClusterRole or User resources in Kubernetes hence the management of access is much more simplified.\n\n</b></details>\n\n### Practical ArgoCD 101\n\n<details>\n<summary>Describe the purpose of the following section in a an Application YAML file\n\n```YAML\nsource:\n  repoURL: https://github.com/bregman-arie/devops-exercises\n  targetRevision: HEAD\n  path: main\n```\n</summary><br><b>\n\nThis section of an Application in ArgoCD, defines which Git repository should be synced\n</b></details>\n\n<details>\n<summary>Describe the purpose of the following section in a an Application YAML file\n\n```YAML\ndestination:\n  server: http://some.kubernetes.cluster.svc\n  namespace: devopsExercises\n```\n</summary><br><b>\n\nThis section defines with which Kubernetes cluster the app in the tracked Git repository should be synced with.\n</b></details>\n\n<details>\n<summary>What CRD would you use if you have multiple applications and you would like to group them together logically?</code></summary><br><b>\n\nAddProject\n</b></details>\n\n<details>\n<summary>True or False? ArgoCD sync period is 3 hours</summary><br><b>\n\nFalse. ArgoCD sync period is 3 minutes as of today (and not hours).\n</b></details>\n\n<details>\n<summary>Describe shortly what ArgoCD does every sync period</summary><br><b>\n\n1. Gathers list of all the apps to sync (those that are marked with \"auto-sync\")\n2. Gets Git state for each repository\n3. Performs comparison between the repository Git state and the Kubernetes cluster state\n  1. If states are different, the application marked as \"out-of-sync\" and further action might be taken (based on the configuration)\n  2. If states are equal, the application marked as \"synced\"\n</b></details>\n\n<details>\n<summary>You deployed a new application in a namespace called \"yay\" but when running <code>kubectl get ns yay</code> you see there is no such namespace. What happened?</summary><br><b>\n\nDeploying applications in non-existing namespaces doesn't create the namespace. For that you have to explicitly mark \"Auto-create namespace\".\n\nTo fix it, you can simply run `kubectl create namespace NAMESPACE_NAME` but it's better of course to have it stored in Git rather than running kubectl commands.\n</b></details>\n\n#### CLI\n\n<details>\n<summary>Create a new application with the following properties:\n\n* app name: some-app\n* repo: https://fake.repo.address\n* app path: ./app_path\n* namespace: default\n* cluster: my.kubernetes.cluster\n</summary><br><b>\n\n```\nargocd app create some-app \\\n--project  \\\n--repo https://fake.repo.address \\\n--path ./app_path \\\n--dest-namespace default \\\n--dest-server my.kubernetes.cluster\n```\n\n</b></details>\n\n<details>\n<summary>List all argocd apps</summary><br><b>\n\n`argocd app list`\n</b></details>\n\n<details>\n<summary>Print detailed information on the app called \"some-app\"</summary><br><b>\n\n`argocd app get some-app`\n</b></details>\n\n<details>\n<summary>How to add an additional (external) cluster for ArgoCD to manage?</summary><br><b>\n\n`argocd cluster add CLUSTER_ADDRESS/NAME`\n</b></details>\n\n<details>\n<summary>How to list all the clusters ArgoCD manage?</summary><br><b>\n\n`argocd cluster list`\n</b></details>\n\n### ArgoCD Configuration\n\n<details>\n<summary>Is it possible to change default sync period of ArgoCD?</summary><br><b>\n\nYes, it is possible by adding the following to the argocd-cm (ConfigMap):\n\n```\ndata:\n  timeout.reconciliation: 300s\n```\n\nThe value can be any number of seconds you would like to set.\n</b></details>\n\n<details>\n<summary>What will be the result of setting <code>timeout.reconciliation: 0s</code>?</summary><br><b>\n\nsync functionality will be disabled.\n</b></details>\n\n### Advanced ArgoCD\n\n<details>\n<summary>What is the \"App of Apps Patterns\"?</summary><br><b>\n\nA solution from Argo community in regards to managing multiple similar applications.\n\nBasically a pattern where you have root application that consists of other child applications.\n\nSo instead of creating multiple separate applications, you have the root application pointing to a repository with additional applications.\n</b></details>\n\n<details>\n<summary>Can you provide some use cases for using \"App of Apps Patterns\"?</summary><br><b>\n\n* Cluster Preparation: You would like to deploy multiple applications at once to bootstrap a Kubernetes cluster\n\n* Multiple environments: If deploying many versions of the same application, but with minor changes. For example, several test deployments to test different features\n\n* Multiple clusters: when the same application needs to be deployed across multiple K8s clusters connected to ArgoCD\n</b></details>\n\n<details>\n<summary>True or False? If you have multiple Kubernetes clusters you want to manage sync applications to with ArgoCD then, you must have ArgoCD installed on each one of them</summary><br><b>\n\nFalse, it can be deployed on one of them. ArgoCD is able to manage external clusters on which it doesn't run.\n</b></details>\n\n<details>\n<summary>You've three clusters - dev, staging and prod. Whenever you update the application GitOps repo, all three clusters are being updated. What's the problem with that and how to deal with it?</summary><br><b>\n\nYou don't usually want to go and update all of your clusters at once, especially when some for testing and development purposes and some for actual production usage.\n\nThere are multiple ways to deal with it:\n\n1. Branch driven: Have branches for your GitOps repo where you push first to development, do some testing, merge it then to staging and if everything works fine in staging, you merge it to production.\n\n2. Use overlays and Kustomize to control the context of where your changes synced based on the CI process/pipeline used.\n</b></details>\n\n### ArgoCD Application Health\n\n<details>\n<summary>What are some possible health statuses for an ArgoCD application?</summary><br><b>\n\n* Healthy\n* Missing: resource doesn't exist in the cluster\n* Suspended: resource is paused\n* Progressing: resources isn't healthy but will become healthy or has the chance to become healthy\n* Degraded: resource isn't healthy\n* Unknown: it's not known what's the app health\n</b></details>\n\n<details>\n<summary>True or False? A Deployment considered to be healthy if the Pods are running</summary><br><b>\n\nNot exactly. A Deployment (as well as StatefulSet, ReplicaSet and DaemonSet) considered healthy  if the desired state equals to actual/current state (this includes the number of replicas).\n</b></details>\n\n<details>\n<summary>True or False? An ingress is considered healthy if status.loadBalancer.ingress list includes at least one value</summary><br><b>\n\nTrue.\n</b></details>\n\n<details>\n<summary>What can you tell about the health of custom Kubernetes resources?</summary><br><b>\n\nThe health of custom Kubernetes resources is defined by writing Lua scripts.\n\nYou find such list of scripts here: https://github.com/argoproj/argo-cd/tree/master/resource_customizations\n</b></details>\n\n### ArgoCD Syncs\n\n<details>\n<summary>Explain manual syncs vs. automatic syncs</summary><br><b>\n\nAutomatic syncs means that once ArgoCD detected a change or a new version of your app in Git, it will apply the changes so the current/actual state can be equal to desired state.\n\nWith manual syncs, ArgoCD will identify there is a difference, but will do nothing to correct it.\n</b></details>\n\n<details>\n<summary>Explain auto-pruning</summary><br><b>\n\nIf enabled, auto-pruning will remove resources when files or content is removed from a tracked Git repository.\n\nIf disabled, ArgoCD will not remove anything, even when content or files are removed.\n</b></details>\n\n<details>\n<summary>Explain self-heal in regards to ArgoCD</summary><br><b>\n\nSelf-heal is the process of correcting the cluster state based on the desired state, when someone makes manual changes to the cluster.\n</b></details>\n\n### ArgoCD and Helm\n\n<details>\n<summary>What support is provided in ArgoCD for Helm?</summary><br><b>\n\nArgoCD is able to track packaged Helm chart in a sense where it will monitor for new versions.\n</b></details>\n\n<details>\n<summary>True or False? When ArgoCD tracks Helm chart the chart is no longer an Helm application and it's a ArgoCD app</summary><br><b>\n\nTrue. Trying to execute commands like `helm ls` will fail because helm metadata doesn't exist anymore and the application is tracked as ArgoCD app.\n</b></details>\n\n## Argo Rollouts Questions\n\n### Argo Rollouts 101\n\n<details>\n<summary>What is Argo Rollouts?</summary><br><b>\n\nA controller for Kubernetes to perform application deployments using different strategies like Blue/Green deployments, Canary deployments, etc.\n\nIn addition, it supports A/B tests, automatic rollbacks and integrated metric analysis.\n</b></details>\n\n<details>\n<summary>What happens when you rollout a new version of your app with argo rollouts?</summary><br><b>\n\n- Argo Rollouts creates a new replicaset (that is the new app version)\n  - Old version is still alive\n- ArgoCD marks the app as out-of-sync \n</b></details>\n\n<details>\n<summary>True or False? You need to install ArgoCD in order to use Argo Rollouts</summary><br><b>\n\nFalse. Quite common misconception today but both cab be used independency even though they work nicely together.\n</b></details>\n\n### Argo Advanced Rollouts\n\n<details>\n<summary>Scott, an engineer in your team, executes manually some smoke tests and monitors rollouts every time a new version is deployed. This way, if there is an issue he detects, he performs a rollback. What better approach you might suggest him to take?</summary><br><b>\n\nShift towards fully automated rollbacks. Argo Rollouts supports multiple metric providers (Datadog, NewRelic, etc.) so you can use data and metrics for automating the rollbacks based on different conditions\n\n</b></details>\n\n<details>\n<summary>Explain the concept of \"Analysis\" in regards to Argo Rollouts</summary><br><b>\n\nAnalysis is a resource deployed along a Rollout resources and defines the conditions and metrics threshols for performing a rollback\n\n</b></details>\n\n<details>\n<summary>Explain the following configuration\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: AnalysisTemplate\nmetadata:\n  name: success-rate\nspec:\n  args:\n  - name: service-name\n  metrics:\n  - name: success-rate\n    interval: 4m\n    count: 3\n    successCondition: result[0] >= 0.90\n    provider:\n      prometheus:\n        address: http:/some-prometheus-instance:80\n        query: sum(response_status{app=\"{{args.service-name}}\",role=\"canary\",status=~\"2.*\"})/sum(response_status{app=\"{{args.service-name}}\",role=\"canary\"}\n```\n</summary><br><b>\n\nIt's an Analysis resource that fetches response status from Prometheus (monitoring instance). If it's more than 0.90 the rollout will continue, if it's less than 0.90 a rollback will be performed meaning the canary deployment failed.\n\n</b></details>\n\n### Argo Rollouts Commands\n\n<details>\n<summary>How to list rollouts?</summary><br><b>\n\n`kubectl argo rollouts list rollouts`\n</b></details>\n\n<details>\n<summary>How to list the rollouts of a given application?</summary><br><b>\n\n`kubectl argo rollouts get rollout SOME-APP`\n</b></details>\n\n<details>\n<summary>How to check the status of a rollout?</summary><br><b>\n\n`kubectl argo rollouts status SOME-APP`\n</b></details>\n\n<details>\n<summary>How to rollout a new version (with new container tag)?</summary><br><b>\n\n`kubectl argo rollouts set image SOME-APP web-app=some/registry/and/image:v2.0`\n</b></details>\n\n<details>\n<summary>How to manually promote to new app version?</summary><br><b>\n\n`kubectl argo rollouts promote SOME-APP`\n</b></details>\n\n<details>\n<summary>How do you monitor a rollout?</summary><br><b>\n\n`kubectl argo rollouts get rollout SOME-APP --watch`\n</b></details>\n\n<!-- {% endraw %} -->\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/argo/README.md", "language": "markdown", "complexity": "complex", "star_count": 77729, "quality_score": 0.8600000000000001, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 513}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS ELB - ALB Multiple Target Groups\n\n### Requirements                   \n\nTwo EC2 instances with a simple web application that shows the web page with the string \"Hey, it's a me, `<HOSTNAME>`!\"\nOne EC2 instance with a simple web application that shows the web page with the string \"Hey, it's only a test...\" under the endpoint /test\n\n### Objectives\n\n1. Create an application load balancer for the two instances you have, with the following properties\n  1. healthy threshold: 3\n  2. unhealthy threshold: 3\n  3. interval: 10 seconds\n2. Create another target group for the third instance\n  1. Traffic should be forwarded to this group based on the \"/test\" path\n\n### Solution\n\n#### Console\n\n1. Go to EC2 service\n2. Click in the left side menu on \"Load balancers\" under \"Load balancing\"\n3. Click on \"Create load balancer\"\n4. Choose \"Application Load Balancer\"\n5. Insert a name for the LB\n6. Choose an AZ where you want the LB to operate\n7. Choose a security group\n8. Under \"Listeners and routing\" click on \"Create target group\" and choose \"Instances\"\n  1. Provide a name for the target group\n  2. Set healthy threshold to 3\n  3. Set unhealthy threshold to 3\n  4. Set interval to 10 seconds\n  5. Click on \"Next\" and choose two out of three instances you've created\n  6. Click on \"Create target group\"\n9. Refresh target groups and choose the one you've just created\n10. Click on \"Create load balancer\" and wait for it to be provisioned\n\n11. In the left side menu click on \"Target Groups\" under \"Load Balancing\"\n12. Click on \"Create target group\"\n13. Set it with the same properties as previous target group but this time, add the third instance that you didn't include in the previous target group\n14. Go back to your ALB and under \"Listeners\" click on \"Edit rules\" under your current listener\n  1. Add a rule where if the path is \"/test\" then traffic should be forwarded to the second target group you've created\n  2. Click on \"Save\"\n15. Test it by going to the browser, insert the address and add \"/test\" to the address\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/aws/exercises/alb_multiple_target_groups/solution.md", "language": "markdown", "complexity": "medium", "star_count": 77729, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 45}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS Auto Scaling Groups - Dynamic Scaling Policy\n\n### Requirements\n\n1. Existing Auto Scaling Group with maximum capacity set to at least 3\n2. One running EC2 instance with max of 4 CPUs\n\n### Objectives\n\n1. Create a dynamic scaling policy with the following properties\n  1. Track average CPU utilization\n  2. Target value should be 70%\n2. Increase the CPU utilization to at least 70%\n  1. Do you see change in number of instances?\n1. Decrease CPU utilization to less than 70%\n  1. Do you see change in number of instances?\n\n### Solution\n\n#### Console\n\n1. Go to EC2 service -> Auto Scaling Groups and click on the tab \"Automating scaling\"\n2. Choose \"Target tracking scaling\" under \"Policy Type\"\n3. Set metric type to Average CPU utilization\n4. Set target value to 70% and click on \"Create\"\n\n1. If you are using Amazon Linux 2, you can stress the instance with the following:\n\n```\nsudo amazon-linux-extras install epel -y\nsudo yum install stress -y\nstress -c 4  # assuming you have 4 CPUs\n```\n2. Yes, additional EC2 instance was added\n\n1. Simply stop the stress command\n2. Yes, one of the EC2 instances was terminated\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/aws/exercises/asg_dynamic_scaling_policy/solution.md", "language": "markdown", "complexity": "simple", "star_count": 77729, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 38}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS - Budget Setup\n\n### Objectives\n\nSetup a cost budget in your AWS account based on your needs.\n\n### Solution\n\n1. Go to \"Billing\"\n2. Click on \"Budgets\" in the menu\n3. Click on \"Create a budget\"\n4. Choose \"Cost Budget\" and click on \"Next\"\n5. Choose the values that work for you. For example, recurring monthly budget with a specific amount\n6. Insert a budget name and Click on \"Next\" \n7. Set up an alert but clicking on \"Add an alert threshold\"\n  1. Set a threshold (e.g. 75% of budgeted amount)\n  2. Set an email where a notification will be sent\n8. Click on \"Next\" until you can click on \"Create a budget\"\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/aws/exercises/budget_setup/solution.md", "language": "markdown", "complexity": "simple", "star_count": 77729, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS - Create EFS\n \n### Requirements\n\nTwo EC2 instances in different availability zones\n\n### Objectives\n\n1. Create an EFS with the following properties\n  1. Set lifecycle management to 60 days\n  2. The mode should match a use case of scaling to high levels of throughput and I/O operations per second\n2. Mount the EFS in both of your EC2 instances\n \n### Solution\n \n1. Go to EFS console\n2. Click on \"Create file system\"\n3. Create on \"customize\"\n  1. Set lifecycle management to \"60 days since last access\"\n  2. Set Performance mode to \"MAX I/O\" due to the requirement of \"Scaling to high levels of throughput\"\n  3. Click on \"Next\"\n  4. Choose security group to attach (if you don't have any, create one and make sure it has a rule to allow NFS traffic) and click on \"Next\" until you are able to review and create it\n5. SSH into your EC2 instances\n  1. Run `sudo yum install -y amazon-efs-utils`\n  2. Run `mkdir efs`\n  3. If you go to your EFS page and click on \"Attach\", you can see what ways are there to mount your EFS on your instancess\n    1. The command to mount the EFS should be similar to `sudo mount -t efs -o tls <EFS name>:/ efs` - copy and paste it in your ec2 instance's OS\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/aws/exercises/create_efs/solution.md", "language": "markdown", "complexity": "simple", "star_count": 77729, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 28}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS Route 53 - Health Checks\n\n## Requirements\n\n3 web instances in different AZs.\n\n## Objectives \n\n1. For each instance create a health checks with the following properties:\n  1. Name it after the AZ where the instance resides\n  2. Failure threshold should be 5\n\n2. Edit the security group of one of your instances and remove HTTP rules.\n  1. Did it change the status of the health check?\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/aws/exercises/health_checks/exercise.md", "language": "markdown", "complexity": "simple", "star_count": 77729, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS Route 53 - Health Checks\n\n## Requirements\n\n3 web instances in different AZs.\n\n## Objectives \n\n1. For each instance create a health checks with the following properties:\n  1. Name it after the AZ where the instance resides\n  2. Failure threshold should be 5\n\n2. Edit the security group of one of your instances and remove HTTP rules.\n  1. Did it change the status of the health check?\n\n### Solution\n\n#### Console\n\n1. Go to Route 53\n2. Click on \"Health Checks\" in the left-side menu\n3. Click on \"Create health check\"\n4. Insert the name: us-east-2\n5. What to monitor: endpoint\n6. Insert the IP address of the instance\n7. Insert the endpoint /health if your web instance supports that endpoint\n8. In advanced configuration, set Failure threshold to 5\n9. Click on \"next\" and then on \"Create health check\"\n10. Repeat steps 1-9 for the other two instances you have\n\n1. Go to security group of one of your instances\n2. Click on \"Actions\" -> Edit inbound rules -> Delete HTTP based rules\n3. Go back to health checks page and after a couple of seconds you should see that the status becomes \"unhealthy\"\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/aws/exercises/health_checks/solution.md", "language": "markdown", "complexity": "medium", "star_count": 77729, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 34}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability using Terraform infrastructure as code\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## AWS - Launch EC2 Web Instance\n\n### Objectives\n\nLaunch one EC2 instance with the following requirements:\n\n1. Amazon Linux 2 image\n2. Instance type: pick up one that has 1 vCPUs and 1 GiB memory\n3. Instance storage should be deleted upon the termination of the instance\n4. When the instance starts, it should install:\n  1. Install the httpd package\n  2. Start the httpd service\n  3. Make sure the content of /var/www/html/index.html is `I made it! This is is awesome!`\n5. It should have the tag: \"Type: web\" and the name of the instance should be \"web-1\"\n6. HTTP traffic (port 80) should be accepted from anywhere\n\n### Solution\n\n1. Choose a region close to you \n2. Go to EC2 service\n3. Click on \"Instances\" in the menu and click on \"Launch instances\"\n4. Choose image: Amazon Linux 2\n5. Choose instance type: t2.micro \n6. Make sure \"Delete on Termination\" is checked in the storage section\n7. Under the \"User data\" field the following:\n\n```\nyum update -y\nyum install -y httpd\nsystemctl start httpd\nsystemctl enable httpd\necho \"<h1>I made it! This is is awesome!</h1>\" > /var/www/html/index.html\n```\n8. Add tags with the following keys and values:\n  * key \"Type\" and the value \"web\"\n  * key \"Name\" and the value \"web-1\"\n9. In the security group section, add a rule to accept HTTP traffic (TCP) on port 80 from anywhere\n10. Click on \"Review\" and then click on \"Launch\" after reviewing.\n11. If you don't have a key pair, create one and download it.\n\n12. ### Solution using Terraform\n\n```\n \nprovider \"aws\" {\n  region = \"us-east-1\" // Or your desired region\n}\n\nresource \"aws_instance\" \"web_server\" {\n  ami           = \"ami-12345678\" // Replace with the correct AMI for Amazon Linux 2\n  instance_type = \"t2.micro\" // Or any instance type with 1 vCPU and 1 GiB memory\n\n  tags = {\n    Name = \"web-1\"\n    Type = \"web\"\n  }\n\n  root_block_device {\n    volume_size           = 8 // Or any desired size\n    delete_on_termination = true\n  }\n\n  provisioner \"remote-exec\" {\n    inline = [\n      \"sudo yum update -y\",\n      \"sudo yum install -y httpd\",\n      \"sudo systemctl start httpd\",\n      \"sudo bash -c 'echo \\\"I made it! This is awesome!\\\" > /var/www/html/index.html'\",\n      \"sudo systemctl enable httpd\"\n    ]\n\n    connection {\n      type        = \"ssh\"\n      user        = \"ec2-user\"\n      private_key = file(\"~/.ssh/your_private_key.pem\") // Replace with the path to your private key\n      host        = self.public_ip\n    }\n  }\n\n  security_group_ids = [aws_security_group.web_sg.id]\n}\n\nresource \"aws_security_group\" \"web_sg\" {\n  name        = \"web_sg\"\n  description = \"Security group for web server\"\n  \n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n```", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/aws/exercises/launch_ec2_web_instance/solution.md", "language": "markdown", "complexity": "medium", "star_count": 77729, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": true, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 94}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# For most projects, this workflow file will not need changing; you simply need\n# to commit it to your repository.\n#\n# You may wish to alter this file to override the set of languages analyzed,\n# or to provide custom queries or build logic.\n#\n# ******** NOTE ********\n# We have attempted to detect the languages in your repository. Please check\n# the `language` matrix defined below to confirm you have the correct set of\n# supported CodeQL languages.\n#\nname: \"CodeQL\"\n\non:\n  push:\n    branches: [ master ]\n  pull_request:\n    # The branches below must be a subset of the branches above\n    branches: [ master ]\n  schedule:\n    - cron: '26 16 * * 0'\n\njobs:\n  analyze:\n    if: |\n      !startsWith(github.event.head_commit.message, 'chore') &&\n      !startsWith(github.event.head_commit.message, 'build: hotfix') &&\n      !endsWith(github.event.head_commit.message, 'reformatted by jina-dev-bot')\n    name: Analyze\n    runs-on: ubuntu-latest\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language: [ 'python' ]\n        # CodeQL supports [ 'cpp', 'csharp', 'go', 'java', 'javascript', 'python', 'ruby' ]\n        # Learn more about CodeQL language support at https://aka.ms/codeql-docs/language-support\n\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v2.5.0\n\n    # Initializes the CodeQL tools for scanning.\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v2\n      with:\n        languages: ${{ matrix.language }}\n        # If you wish to specify custom queries, you can do so here or in a config file.\n        # By default, queries listed here will override any specified in a config file.\n        # Prefix the list here with \"+\" to use these queries and those in the config file.\n        \n        # Details on CodeQL's query packs refer to : https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs\n        # queries: security-extended,security-and-quality\n\n        \n    # Autobuild attempts to build any compiled languages  (C/C++, C#, or Java).\n    # If this step fails, then you should remove it and run the build manually (see below)\n    - name: Autobuild\n      uses: github/codeql-action/autobuild@v2\n\n    # ‚ÑπÔ∏è Command-line programs to run using the OS shell.\n    # üìö See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun\n\n    #   If the Autobuild fails above, remove it and uncomment the following three lines. \n    #   modify them (or add more) to build your code if your project, please refer to the EXAMPLE below for guidance.\n\n    # - run: |\n    #   echo \"Run, Build Application using script\"\n    #   ./location_of_script_within_repo/buildscript.sh\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v2\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": ".github/workflows/codeql-analysis.yml", "language": "yaml", "complexity": "medium", "star_count": 21665, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 77}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "> The best way to know more about contributing and how to get started is to **[join us on Discord](https://discord.jina.ai)** and ask questions in our public channels.\n\n# Contributing to Jina\n\nThanks for your interest in contributing to Jina. We're grateful for your initiative! ‚ù§Ô∏è\n\nIn this guide, we're going to go through the steps for each kind of contribution, and good and bad examples of what to do. We look forward to your contributions!\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n- [üêû Bugs and Issues](#-bugs-and-issues)\n- [ü•á Making Your First Submission](#-making-your-first-submission)\n- [‚òëÔ∏è Naming Conventions](#-naming-conventions)\n- [üí• Testing Jina Locally and on CI](#-testing-jina-locally-and-on-ci)\n- [üìñ Contributing Documentation](#-contributing-documentation)\n- [üôè Thank You](#-thank-you)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n<a name=\"-bugs-and-issues\"></a>\n## üêû Bugs and Issues\n\n### Submitting Issues\n\nWe love to get issue reports. But we love it even more if they're in the right format. For any bugs you encounter, we need you to:\n\n* **Describe your problem**: What exactly is the bug. Be as clear and concise as possible\n* **Why do you think it's happening?** If you have any insight, here's where to share it\n\nThere are also a couple of nice to haves:\n\n* **Environment:** You can find this with ``jina -vf``\n* **Screenshots:** If they're relevant\n\n<a name=\"-making-your-first-submission\"></a>\n## ü•á Making Your First Submission\n\n0. Associate your local git config with your GitHub account. If this is your first time using git you can follow [the steps](#associate-with-github-account).\n1. Fork the Jina repo and clone onto your computer. \n1. Configure git pre-commit hooks. Please follow [the steps](#install-pre-commit-hooks)\n1. Create a [new branch](#naming-your-branch), for example `fix-jina-typo-1`.\n1. Work on this branch to do the fix/improvement.\n1. Check if your code changes follow the [code review guidelines](.github/CODE_REVIEW_GUIDELINES.md).\n1. Commit the changes with the [correct commit style](#writing-your-commit-message).\n1. Make a pull request.\n1. Submit your pull request and wait for all checks to pass.\n1. Request reviews from one of [the code owners](.github/CODEOWNERS).\n1. Get a LGTM üëç and PR gets merged.\n\n**Note:** If you're just fixing a typo or grammatical issue, you can go straight to a pull request.\n\n### Associate with GitHub Account\n\n- Confirm username and email on [your profile page](https://github.com/settings/profile).\n- Set git config on your computer.\n\n```shell\ngit config user.name \"YOUR GITHUB NAME\"\ngit config user.email \"YOUR GITHUB EMAIL\"\n```\n\n- (Optional) Reset the commit author if you made commits before you set the git config.\n\n```shell\ngit checkout YOUR-WORKED-BRANCH\ngit commit --amend --author=\"YOUR-GITHUB-NAME <YOUR-GITHUB-EMAIL>\" --no-edit\ngit log  # to confirm the change is effective\ngit push --force\n```\n\nWhat happens after the merge? [Understand the development stage and release cycles here.](RELEASE.md)\n\n### Install pre-commit hooks\n\nIn Jina we use git's pre-commit hooks in order to make sure the code matches our standards of quality and documentation. At the moment we employ them for checking the style and the docstrings of the code. Documentation of code is crucial to maintaining productive developers and clear communication with new users. We also want to reduce all arguments about code styling.\n\nIt's easy to configure it:\n\n1. `pip install pre-commit`\n1. `pre-commit install`\n\nNow you will be automatically reminded to add docstrings to your code. `black` will take care that your code will match our style. Note that `black` will fail your commit but reformat your code, so you just need to add the files again and commit **again**.\n\nFor more about our docstring style, refer to [this guide](./.github/DOCSTRINGS.rst).\n\n#### Restoring correct git blame\n\nRun `git config blame.ignoreRevsFile .github/.git-blame-ignore-revs`\n\n<a name=\"-naming-conventions\"></a>\n## ‚òëÔ∏è Naming Conventions\n\nFor branches, commits, and PRs we follow some basic naming conventions:\n\n* Be descriptive\n* Use all lower-case\n* Limit punctuation\n* Include one of our specified [types](#specify-the-correct-types)\n* Short (under 70 characters is best)\n* In general, follow the [Conventional Commit](https://www.conventionalcommits.org/en/v1.0.0/#summary) guidelines\n\nNote: If you don't follow naming conventions, your commit will be automatically flagged to be fixed.\n\n### Specify the correct types\n\nType is an important prefix in PR, commit message. For each branch, commit, or PR, we need you to specify the type to help us keep things organized. For example,\n\n```\nfeat: add hat wobble\n^--^  ^------------^\n|     |\n|     +-> Summary in present tense.\n|\n+-------> Type: build, ci, chore, docs, feat, fix, refactor, style, or test.\n```\n\n- build: Changes that affect the build system or external dependencies (example scopes: gulp, broccoli, npm)\n- ci: Changes to our CI configuration files and scripts (example scopes: Travis, Circle, BrowserStack, SauceLabs)\n- docs: Documentation only changes\n- feat: A new feature\n- fix: A bug fix\n- perf: A code change that improves performance\n- refactor: A code change that neither fixes a bug nor adds a feature\n- style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc.)\n- test: Adding missing tests or correcting existing tests\n- chore: updating grunt tasks etc; no production code change\n\n### Naming your Branch\n\nYour branch name should follow the format `type-scope(-issue_id)`:\n\n* `type` is one of the [types above](#specify-the-correct-types)\n* `scope` is optional, and represents the module your branch is working on.\n* `issue_id` is [the GitHub issue](https://github.com/jina-ai/jina/issues) number. Having the correct issue number will automatically link the Pull Request on this branch to that issue.\n\n> Good examples:\n>\n```text\nfix-executor-loader-113\nchore-update-version\ndocs-add-cloud-section-33\n```\n\n> Bad examples:\n>\n\n| Branch name     | Feedback                                              |\n| ---             | ---                                                   |\n| `FIXAWESOME123` | Not descriptive enough, all caps, doesn't follow spec |\n| `NEW-test-1`    | Should be lower case, not descriptive                 |\n| `mybranch-1`    | No type, not descriptive                              |\n\n### Writing your Commit Message\n\nA good commit message helps us track Jina's development. A Pull Request with a bad commit message will be *rejected* automatically in the CI pipeline.\n\nCommit messages should stick to our [naming conventions](#-naming-conventions) outlined above, and use the format `type(scope?): subject`:\n\n* `type` is one of the [types above](#specify-the-correct-types).\n* `scope` is optional, and represents the module your commit is working on.\n* `subject` explains the commit, without an ending period`.`\n\nFor example, a commit that fixes a bug in the executor module should be phrased as: `fix(executor): fix the bad naming in init function`\n\n> Good examples:\n>\n```text\nfix(indexer): fix wrong sharding number in indexer\nfeat: add remote api\n```\n\n> Bad examples:\n>\n\n| Commit message                                                                                | Feedback                           |\n| ---                                                                                           | ---                                |\n| `doc(101): improved 101 document`                                                             | Should be `docs(101)`              |\n| `tests(flow): add unit test for flow exception`                                               | Should be `test(flow)`             |\n| `DOC(101): Improved 101 Documentation`                                                        | All letters should be in lowercase |\n| `fix(pea): i fix this pea and this looks really awesome and everything should be working now` | Too long                           |\n| `fix(pea):fix network receive of the pea`                                                     | Missing space after `:`            |\n| `hello: add hello-world`                                                                      | Type `hello` is not allowed          |\n\n#### What if I Mess Up?\n\nWe all make mistakes. GitHub has a guide on [rewriting commit messages](https://docs.github.com/en/free-pro-team@latest/github/committing-changes-to-your-project/changing-a-commit-message) so they can adhere to our standards.\n\nYou can also install [commitlint](https://commitlint.js.org/#/) onto your own machine and check your commit message by running:\n\n```bash\necho \"<commit message>\" | commitlint\n```\n\n### Naming your Pull Request\n\nWe don't enforce naming of PRs and branches, but we recommend you follow the same style. It can simply be one of your commit messages, just copy/paste it, e.g. `fix(readme): improve the readability and move sections`.\n\n<a name=\"-testing-jina-locally-and-on-ci\"></a>\n## üí• Testing Jina Locally and on CI\n\nYou need to build a local docker image tagged 'jinaai/jina:test-pip' for all the tests to run as in the CI, via:\n\n```bash\ndocker build --build-arg PIP_TAG=\"[devel]\" -f ${PATH_TO_JINA}/Dockerfiles/pip.Dockerfile -t jinaai/jina:test-pip ${PATH_TO_JINA}\n```\n\nLocally you can do unittest via:\n\n```bash\npip install \".[test]\"\npytest -v -s --ignore-glob='tests/integration/hub_usage/dummyhub*' tests\n```\n\nTips: If you want to run the k8s tests then you should install [linkerd cli](https://linkerd.io/2.11/getting-started/#step-1-install-the-cli) before.\n\nWhen you add an executor or a driver, you may introduce new dependencies to Jina. You can verify the dependencies via:\n\n```bash\njina check\n```\n, and via Docker container:\n\n```bash\ndocker run jinaai/jina:my-local-version check\n```\n\nIt prints a list of components the current version of Jina supports, and then exits. Make sure yours are not in red.\n\nOnce you submit the PR, your code will be tested in the environment of Python 3.7 and 3.8 with [full extra dependencies](extra-requirements.txt) (`pip install .[all]`) installed.\n\n<a name=\"-contributing-documentation\"></a>\n## üìñ Contributing Documentation\n\nGood docs make developers happy, and we love happy developers! We've got a few different types of docs:\n\n* General documentation\n* Tutorials/examples\n* Docstrings in Python functions in RST format - generated by Sphinx\n\n### Documentation guidelines\n\n1. Decide if your page is a **guide or a tutorial**. Make sure it fits its section.\n2. Use ‚Äú**you**‚Äù instead of ‚Äúwe‚Äù or ‚ÄúI‚Äù. It **engages** the reader more.\n3. **Sentence case** for headers. (Use [https://convertcase.net/](https://convertcase.net/) to check)\n4. Keep sentences short. If possible, **fewer than 13 words**.\n5. Only use `backticks` for direct references to code elements.\n6. Jina product names should be capitalized and not backticked (Flow, Executor, Hub etc.).\n7. All **acronyms** should be UPPERCASE (Ex. YAML, JSON, HTTP, SSL).\n8. Think about the **structure** of the page beforehand. Split it into headers before writing the content.\n9. If relevant, include a ‚Äú**See also**‚Äù section at the end.\n10. Link to any existing explanations of the concepts you are using.\n\nBonus: **Know when to break the rules**. Documentation writing is as much art as it is science. Sometimes you will have to deviate from these rules in order to write good documentation.\n\n\n[MyST](https://myst-parser.readthedocs.io/en/latest/) Elements Usage\n\n1. Use the `{tab}` element to show multiple ways of doing one thing. [Example](https://jina.ai/serve/concepts/flow/basics/#start-and-stop) \n2. Use the `{admonition}` boxes with care. We recommend restricting yourself to [Hint](https://jina.ai/serve/concepts/flow/basics/#create), [Caution](https://jina.ai/serve/concepts/gateway/customize-http-endpoints/#enable-graphql-endpoint) and [See Also](https://jina.ai/serve/concepts/gateway/customize-http-endpoints/#enable-graphql-endpoint).\n3. Use `{dropdown}` to hide optional content, such as long code snippets or console output. [Example](https://jina.ai/serve/concepts/client/third-party-clients/#use-curl)\n\n### Building documentation on your local machine\n\n#### Requirements\n\n* Python 3\n* [jq](https://stedolan.github.io/jq/download/)\n\n#### Steps to build locally\n\n```bash\ncd docs\npip install -r requirements.txt\nexport NUM_RELEASES=10\nbash makedoc.sh local-only\n```\n\nDocs website will be generated in `_build/dirhtml`\nTo serve it, run\n\n```bash\ncd _build/dirhtml\npython -m http.server\n```\n\nYou can now see docs website on [http://localhost:8000](http://localhost:8000) on your browser.\n\n## üôè Thank You\n\nOnce again, thanks so much for your interest in contributing to Jina. We're excited to see your contributions!\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.89, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 292}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Release & Version Identifier\n\nJina is shipped from two package management systems, PyPi and Docker Hub. This article clarifies the release cycle and version identifier behind each system.\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n- [PyPi package versioning](#pypi-package-versioning)\n- [Docker image versioning](#docker-image-versioning)\n- [Manual Release Entrypoint](#manual-release-entrypoint)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n\n## PyPi package versioning\n\nWe follow [PEP-440](https://www.python.org/dev/peps/pep-0440/), and a form of [semantic versioning](https://semver.org/) as explained above.\n\nTo install the latest final release:\n\n```bash\npip install -U jina\n```\n\nTo install a particular final release:\n\n```bash\npip install jina==x.y.z\n```\n\nThe term \"final release\" is relative to \"developmental release\" as described below.  \n\n### Install Jina with Recommended Extensions\n\n`pip install -U jina` only installs the core dependencies of Jina.\n\nThe recommended way of installing Jina is `pip install -U jina`\n\n`\"standard\"` include extra dependencies that enables:\n- Executor Hub + Docker support\n- FastAPI + Websocket support (required when using `Flow(protocol='http')` or `Flow(protocol='websocket')`)\n- the best compression via LZ4 algorithm\n- the best async eventloop management via `uvloop`\n\nOther extension tags such as  `[devel]` can be found in [extra-requirements.txt](extra-requirements.txt). \n\n##### Do I need \"[standard]\"?\n\nDepends on how you use and distribute Jina. \n\nIf you are using/distributing Jina as a microservice, you often only need to install the core dependencies via `pip install jina`.\n\n### Developmental releases versioning\n\nOne every master-merging event, we create early releases directly from source control which do not conflict with later project releases. The version identifier for development release is `x.y.z.devN`, which adds a suffix `.dev`, followed by a non-negative integer value `N`, which is reset on every release.\n\nTo install the latest development release:\n\n```bash\npip install --pre jina\n```\n\n### Version epochs\n\n\n#### Release cycle and versioning\nJina is developed continuously by the community and core team. Updates are grouped and released at regular intervals to align with software development best practices.\n\nJina follows a form of numbered versioning. The version number of the product is a three-part value `x.y.z` where `x`, `y`, and `z` are the major, minor, and patch components respectively.\n\n-   Patch release (`x.y.z` -> `x.y.(z+1)`): Contain bug fixes, new features and breaking changes. Released weekly on a Wednesday morning CET.\n-   Minor release (`x.y.z -> x.(y+1).0`): Contain bug fixes, new features and breaking changes. Released monthly on the first Wednesday of the month CET. This release is more QA tested and considered more stable than a patch release.\n-   Major release (`x.y.z -> (x+1).0.0`): Are released based on the development cycle of the Jina company. There is no set scheduled for when these will occur.\n\n\nThe following example shows how Jina is released from 0.9 to 0.9.2 according to the schema we defined above.\n\n|Event `e` | After `e`, `pip install jina` | After `e`, `pip install --pre jina` | After `e`, master `__init__.py` |\n|--- | ---  | --- | --- |\n| Release | 0.9.0 | 0.9.0 | 0.9.1.dev0 |\n| Master merging | 0.9.0 | 0.9.1.dev0 | 0.9.1.dev1 |\n| Master merging | 0.9.0 | 0.9.1.dev1 | 0.9.1.dev2 |\n| Master merging | 0.9.0 | 0.9.1.dev2 | 0.9.1.dev3 |\n| Release | 0.9.1 | 0.9.1 | 0.9.2.dev0 |\n| Master merging | 0.9.1 | 0.9.2.dev0 | 0.9.2.dev1 |\n\n## Docker image versioning\n\nOur universal Docker image is ready-to-use on linux/amd64, linux/armv7+, linux/arm/v6, linux/arm64. The Docker image name always starts with `jinaai/jina` followed by a tag composed of three parts:\n\n```text\njinaai/jina:{version}{python_version}{extra}\n```\n\n- `{version}`: The version of Jina. Possible values:\n    - `latest`: the last release;\n    - `master`: the master branch of `jina-ai/jina` repository;\n    - `x.y.z`: the release of a particular version;\n    - `x.y`: the alias to the last `x.y.z` patch release, i.e. `x.y` = `x.y.max(z)`;\n- `{python_version}`: The Python version of the image. Possible values:\n    - `-py37`: Python 3.7;\n    - ` `, `-py38` for Python 3.8;\n    - `-py39` for Python 3.9;\n    - `-py310` for Python 3.10;\n    - `-py311` for Python 3.11;\n- `{extra}`: the extra dependency installed along with Jina. Possible values:\n    - ` `: Jina is installed inside the image via `pip install jina`;\n    - `-standard`: Jina is installed inside the image via `pip install jina`. It includes all recommended dependencies;  \n    - `-devel`: Jina is installed inside the image via `pip install \"jina[devel]\"`. It includes `standard` plus some extra dependencies;\n\nExamples:\n\n- `jinaai/jina:0.9.6`: the `0.9.6` release with Python 3.7 and the entrypoint of `jina`.\n- `jinaai/jina:latest`: the latest release with Python 3.7 and the entrypoint of `jina`\n- `jinaai/jina:master`: the master with Python 3.7 and the entrypoint of `jina`\n\n### Image alias and updates\n\n| Event | Updated images | Aliases |\n| --- | --- | --- |\n| On Master Merge | `jinaai/jina:master{python_version}{extra}` | |\n| On `x.y.z` release | `jinaai/jina:x.y.z{python_version}{extra}` | `jinaai/jina:latest{python_version}{extra}`, `jinaai/jina:x.y{python_version}{extra}` |\n\nSix images are built, i.e. taking the combination of:\n  - `{python_version} = [\"-py37\", \"-py38\", \"-py39\"]`\n  - `{extra} = [\"\", \"-devel\", \"-standard\"]`\n\n\n### Image size on different tags\n\n|Image Size|\n| ---|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest?label=jinaai%2Fjina%3Alatest&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py38?label=jinaai%2Fjina%3Alatest-py38&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py39?label=jinaai%2Fjina%3Alatest-py39&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-devel?label=jinaai%2Fjina%3Alatest-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-daemon?label=jinaai%2Fjina%3Alatest-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-standard?label=jinaai%2Fjina%3Alatest-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py38-devel?label=jinaai%2Fjina%3Alatest-py38-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py38-daemon?label=jinaai%2Fjina%3Alatest-py38-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py38-standard?label=jinaai%2Fjina%3Alatest-py38-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py39-devel?label=jinaai%2Fjina%3Alatest-py39-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py39-daemon?label=jinaai%2Fjina%3Alatest-py39-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/latest-py39-standard?label=jinaai%2Fjina%3Alatest-py39-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master?label=jinaai%2Fjina%3Amaster&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py38?label=jinaai%2Fjina%3Amaster-py38&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py39?label=jinaai%2Fjina%3Amaster-py39&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-devel?label=jinaai%2Fjina%3Amaster-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-daemon?label=jinaai%2Fjina%3Amaster-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-standard?label=jinaai%2Fjina%3Amaster-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py38-devel?label=jinaai%2Fjina%3Amaster-py38-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py38-daemon?label=jinaai%2Fjina%3Amaster-py38-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py38-standard?label=jinaai%2Fjina%3Amaster-py38-standard&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py39-devel?label=jinaai%2Fjina%3Amaster-py39-devel&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py39-daemon?label=jinaai%2Fjina%3Amaster-py39-daemon&logo=docker)|\n|![](https://img.shields.io/docker/image-size/jinaai/jina/master-py39-standard?label=jinaai%2Fjina%3Amaster-py39-standard&logo=docker)|\n\n---\n\n## Manual Release Entrypoint\n\nManual release entrypoint is designed for authorized core developers of Jina.\n\n### Trigger weekly release manually\n\nOne can release a hotfix immediately without waiting for the weekly release. Here are the steps:\n\n1. [Go to here](https://github.com/jina-ai/jina/actions?query=workflow%3A%22Manual+Release%22)\n2. Click on \"Run Workflow\" dropdown button on the right and input the release token and the release reason.\n3. Click on \"Run Workflow\" green button.\n\n### Rebuild all Docker images for certain `git tag` manually\n\n1. [Go to here](https://github.com/jina-ai/jina/actions?query=workflow%3A%22Manual+Docker+Build%22)\n2. Click on \"Run Workflow\" dropdown button on the right and input the release token and the release reason.\n3. Click on \"Run Workflow\" green button.\n\n![Rebuild all Docker images](.github/images/manual-docker-build.png)\n\nNote, the manual rebuild on Docker images will *NOT* update `:latest-*` and `:x.y-*` aliases.\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "RELEASE.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 181}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{% set name = \"jina\" %}\n{% set version = \"3.11.0\" %}\n\npackage:\n  name: {{ name|lower }}-split\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz\n  sha256: 49a7fec74b3b0927143d73ce787c7e3fe5e52fa2a1079323248a20590d2bf49f\n\nbuild:\n  number: 0\n\noutputs:\n  - name: jina-core\n    build:\n      noarch: python\n      entry_points:\n        - jina = cli:main\n      script: python -m pip install . --no-deps -vv\n      script_env:\n        - JINA_PIP_INSTALL_CORE=1\n    test:\n      requires:\n        - pip\n      imports:\n        - jina\n      commands:\n        - pip check\n        - jina --version\n    requirements:\n      host:\n        - python >=3.7\n        - pip\n      run:\n        - __unix\n        - python >=3.7\n        - docarray >=0.16.4\n        - grpcio >=1.46.0,<1.48.1\n        - grpcio-health-checking >=1.46.0,<1.48.1\n        - grpcio-reflection >=1.46.0,<1.48.1\n        - jina-hubble-sdk ==0.23.3\n        - numpy\n        - opentelemetry-api >=1.12.0\n        - opentelemetry-instrumentation-grpc >=0.33b0\n        - packaging >=20.0\n        - protobuf >=3.20.0\n        - pyyaml >=5.3.1\n  - name: jina-perf\n    test:\n      requires:\n        - pip\n      imports:\n        - jina\n      commands:\n        - pip check\n        - jina --version\n    build:\n      noarch: python\n      entry_points:\n        - jina = cli:main\n      script: python -m pip install . --no-deps -vv\n      script_env:\n        - JINA_PIP_INSTALL_PERF=1\n    requirements:\n      host:\n        - python >=3.7\n        - pip\n      run:\n        - __unix\n        - python >=3.7\n        - docarray >=0.16.4\n        - grpcio >=1.46.0,<1.48.1\n        - grpcio-health-checking >=1.46.0,<1.48.1\n        - grpcio-reflection >=1.46.0,<1.48.1\n        - jina-hubble-sdk ==0.23.3\n        - numpy\n        - opentelemetry-api >=1.12.0\n        - opentelemetry-exporter-otlp >=1.12.0\n        - opentelemetry-instrumentation-aiohttp-client >=0.33b0\n        - opentelemetry-instrumentation-fastapi >=0.33b0\n        - opentelemetry-instrumentation-grpc >=0.33b0\n        - opentelemetry-sdk >=1.12.0\n        - opentelemetry-semantic-conventions >=0.33b0\n        - packaging >=20.0\n        - prometheus_client\n        - protobuf >=3.20.0\n        - pyyaml >=5.3.1\n        - uvloop\n  - name: jina\n    test:\n      requires:\n        - pip\n      imports:\n        - jina\n      commands:\n        - pip check\n        - jina --version\n    build:\n      noarch: python\n      entry_points:\n        - jina = cli:main\n      script: python -m pip install . --no-deps -vv\n    requirements:\n      host:\n        - python >=3.7\n        - pip\n      run:\n        - __unix\n        - python >=3.7\n        - aiofiles\n        - aiohttp\n        - aiostream\n        - cryptography\n        - docarray >=0.16.4\n        - docker-py\n        - fastapi >=0.76.0\n        - filelock\n        - grpcio >=1.46.0,<1.48.1\n        - grpcio-health-checking >=1.46.0,<1.48.1\n        - grpcio-reflection >=1.46.0,<1.48.1\n        - jina-hubble-sdk ==0.23.3\n        - numpy\n        - opentelemetry-api >=1.12.0\n        - opentelemetry-exporter-otlp >=1.12.0\n        - opentelemetry-instrumentation-aiohttp-client >=0.33b0\n        - opentelemetry-instrumentation-fastapi >=0.33b0\n        - opentelemetry-instrumentation-grpc >=0.33b0\n        - opentelemetry-sdk >=1.12.0\n        - opentelemetry-semantic-conventions >=0.33b0\n        - packaging >=20.0\n        - pathspec\n        - prometheus_client\n        - protobuf >=3.20.0\n        - pydantic\n        - python-multipart\n        - pyyaml >=5.3.1\n        - requests\n        - uvicorn\n        - uvloop\n        - websockets\n\nabout:\n  home: https://github.com/jina-ai/jina/\n  license: Apache-2.0\n  license_family: Apache\n  license_file: LICENSE\n  summary: \"Build multimodal AI services via cloud native technologies \\xB7 Neural Search \\xB7 Generative AI \\xB7 Cloud Native\"\n  doc_url: https://jina.ai/serve\n\nextra:\n  recipe-maintainers:\n    - JoanFM\n    - nan-wang\n    - hanxiao\n  feedstock-name: jina\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "conda/meta.yaml", "language": "yaml", "complexity": "complex", "star_count": 21665, "quality_score": 0.8, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 158}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(docker-compose)=\n# {fab}`docker` Docker Compose Support\n\nOne of the simplest ways to prototype or serve in\nproduction is to run your {class}`~jina.Flow` with `docker-compose`.\n\nA {class}`~jina.Flow` is composed of {class}`~jina.Executor`s which run Python code\nthat operates on `Documents`. These `Executors` live in different runtimes depending on how you want to deploy\nyour Flow. \n\nBy default, if you are serving your Flow locally they live within processes. Nevertheless, \nbecause Jina-serve is cloud native your Flow can easily manage Executors that live in containers and that are\norchestrated by your favorite tools. One of the simplest is Docker Compose which is supported out of the box. \n\nYou can deploy a Flow with Docker Compose in one line:\n\n\n```{code-block} python\n---\nemphasize-lines: 3\n---\nfrom jina import Flow\nflow = Flow(...).add(...).add(...)\nflow.to_docker_compose_yaml('docker-compose.yml')\n```\n\nJina-serve generates a `docker-compose.yml` configuration file corresponding with your Flow. You can use this directly with \nDocker Compose, avoiding the overhead of manually defining all of your Flow's services.\n\n````{admonition} Use Docker-based Executors\n:class: caution\nAll Executors in the Flow should be used with `jinaai+docker://...` or `docker://...`.\n````\n\n````{admonition} Health check available from 3.1.3\n:class: caution\nIf you use Executors that rely on Docker images built with a version of Jina-serve prior to 3.1.3, remove the \nhealth check from the dumped YAML file, otherwise your Docker Compose services will \nalways be \"unhealthy.\"\n````\n\n````{admonition} Matching Jina-serve versions\n:class: caution\nIf you change the Docker images in your Docker Compose generated file, ensure that all services included in\nthe Gateway are built with the same Jina-serve version to guarantee compatibility.\n````\n\n## Example: Index and search text using your own built Encoder and Indexer\n\nInstall [`Docker Compose`](https://docs.docker.com/compose/install/) locally before starting this tutorial.\n\nFor this example we recommend that you read {ref}`how to build and containerize the Executors to be run in Kubernetes. <build-containerize-for-k8s>`\n\n### Deploy the Flow\n\nFirst define the Flow and generate the Docker Compose YAML configuration:\n\n\n````{tab} YAML\nIn a `flow.yml` file :\n```yaml\njtype: Flow\nwith:\n  port: 8080\n  protocol: http\nexecutors:\n- name: encoder\n  uses: jinaai+docker://<user-id>/EncoderPrivate\n  replicas: 2\n- name: indexer\n  uses: jinaai+docker://<user-id>/IndexerPrivate\n  shards: 2\n```\nThen in a shell run:\n```shell\njina export docker-compose flow.yml docker-compose.yml \n```\n````\n\n\n````{tab} Python\nIn python run\n```python\nfrom jina import Flow\n\nflow = (\n    Flow(port=8080, protocol='http')\n    .add(name='encoder', uses='jinaai+docker://<user-id>/EncoderPrivate', replicas=2)\n    .add(\n        name='indexer',\n        uses='jinaai+docker://<user-id>/IndexerPrivate',\n        shards=2,\n    )\n)\nflow.to_docker_compose_yaml('docker-compose.yml')\n```\n````\n\n````{admonition} Hint\n:class: hint\nYou can use a custom jina Docker image for the Gateway service by setting the environment variable `JINA_GATEWAY_IMAGE` to the desired image before generating the configuration.\n````\n\nlet's take a look at the generated compose file:\n```yaml\nversion: '3.3'\n...\nservices:\n  encoder-rep-0:  # # # # # # # # # # #          \n                  #     Encoder       #\n  encoder-rep-1:  # # # # # # # # # # #\n\n  indexer-head:   # # # # # # # # # # #          \n                  #                   #\n  indexer-0:      #     Indexer       #\n                  #                   #\n  indexer-1:      # # # # # # # # # # #\n\n  gateway: \n    ...\n    ports:\n    - 8080:8080\n```\n\n```{tip} \n:class: caution\nThe default compose file generated by the Flow contains no special configuration or settings. You may want to \nadapt it to your own needs.\n```\n\nYou can see that six services are created:\n\n- 1 for the **Gateway** which is the entrypoint of the **Flow**.\n- 2 associated with the encoder for the two Replicas.\n- 3 associated with the indexer, one for the Head and two for the Shards.\n\nNow, you can deploy this Flow :\n\n```shell\ndocker-compose -f docker-compose.yml up\n```\n\n### Query the Flow\n\nOnce we see that all the services in the Flow are ready, we can send index and search requests.\n\nFirst define a client:\n```python\nfrom jina.clients import Client\n\nclient = Client(host='http://localhost:8080')\n```\n\n\n```python\nfrom typing import List, Optional\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass MyDocWithMatches(MyDoc):\n    matches: DocList[MyDoc] = []\n    scores: List[float] = []\n\ndocs = client.post(\n    '/index',\n    inputs=DocList[MyDoc]([MyDoc(text=f'This is document indexed number {i}') for i in range(100)]),\n    return_type=DocList[MyDoc],\n    request_size=10\n)\n\nprint(f'Indexed documents: {len(docs)}')\ndocs = client.post(\n    '/search',\n    inputs=DocList[MyDoc]([MyDoc(text=f'This is document query number {i}') for i in range(10)]),\n    return_type=DocList[MyDocWithMatches],\n    request_size=10\n)\nfor doc in docs:\n    print(f'Query {doc.text} has {len(doc.matches)} matches')\n```\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/docker-compose.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.9500000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 187}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kubernetes-docs)=\n# {fas}`dharmachakra` Kubernetes Support\n\n```{toctree}\n:hidden:\n\nkubernetes\n```\n\nJina-serve is a cloud-native framework and therefore runs natively and easily on Kubernetes.\nDeploying a Jina-serve Deploymenr or Flow on Kubernetes is actually the recommended way to use Jina-serve in production.\n\nA {class}`~jina.Deployment` and {class}`~jina.Flow` are services composed of single or multiple microservices called {class}`~jina.Executor` and {class}`~jina.Gateway`s which natively run in containers. This means that Kubernetes can natively take over the lifetime management of Executors. \n\nDeploying a {class}`~jina.Deployment` or `~jina.Flow` on Kubernetes means wrapping these services containers in the appropriate K8s abstraction (Deployment, StatefulSet, and so on), exposing them internally via K8s service and connecting them together by passing the right set of parameters.\n\n```{hint}\nThis documentation is designed for users who want to **manually** deploy a Jina-serve project on Kubernetes.\n\nCheck out {ref}`jcloud` if you want a **one-click** solution to deploy and host Jina, leveraging a cloud-native stack of Kubernetes, Prometheus and Grafana, **without worrying about provisioning**.\n```\n\n\n## Automatically translate a Deployment or Flow to Kubernetes concept\n\n```{hint}\nManually building these Kubernetes YAML object is long and cumbersome. Therefore we provide a helper function {meth}`~jina.Flow.to_kubernetes_yaml` that does most of this\ntranslation work automatically. \n```\n\nThis helper function can be called from:\n* Jina-serve's Python interface to translate a Flow defined in Python to K8s YAML files\n* Jina-serve's CLI interface to export a YAML Flow to K8s YAML files\n\n```{seealso}\nMore detail in the {ref}`Deployment export documentation<deployment-kubernetes-export>` and {ref}`Flow export documentation <flow-kubernetes-export>`\n```\n\n## Extra Kubernetes options\n\nIn general, Jina-serve follows a single principle when it comes to deploying in Kubernetes:\nYou, the user, know your use case and requirements the best.\nThis means that, while Jina-serve generates configurations for you that run out of the box, as a professional user you should always see them as just a starting point to get you off the ground.\n\n```{hint}\nThe export function {meth}`~jina.Deployment.to_kubernetes_yaml` and {meth}`~jina.Flow.to_kubernetes_yaml` are helper functions to get your stared off the ground. **There are meant to be updated and adapted to every use case**\n```\n````{admonition} Matching Jina versions\n:class: caution\nIf you change the Docker images for {class}`~jina.Executor` and {class}`~jina.Gateway` in your Kubernetes-generated file, ensure that all of them are built with the same Jina-serve version to guarantee compatibility.\n````\n\nYou can't add basic Kubernetes features like `Secrets`, `ConfigMap` or `Labels` via the Pythonic or YAML interface. This is intentional and doesn't mean that we don't support these features. On the contrary, we let you fully express your Kubernetes configuration by using the Kubernetes API to add your own Kubernetes standard to Jina-serve.\n\n````{admonition} Hint\n:class: hint\nWe recommend you dump the Kubernetes configuration files and then edit them to suit your needs.\n````\n\nHere are possible configuration options you may need to add or change\n\n- Add labels `selector`s to the Deployments to suit your case\n- Add `requests` and `limits` for the resources of the different Pods \n- Set up persistent volume storage to save your data on disk\n- Pass custom configuration to your Executor with `ConfigMap` \n- Manage credentials of your Executor with Kubernetes secrets, you can use `f.add(..., env_from_secret={'SECRET_PASSWORD': {'name': 'mysecret', 'key': 'password'}})` to map them to Pod environment variables\n- Edit the default rolling update configuration\n\n\n(service-mesh-k8s)=\n## Required service mesh\n\n```{caution}\nA Service Mesh is required to be installed and correctly configured in the K8s cluster in which your deployed your Flow.\n```\n\nService meshes work by attaching a tiny proxy to each of your Kubernetes Pods, allowing for smart rerouting, load balancing, request retrying, and host of [other features](https://linkerd.io/2.11/features/).\n\nJina relies on a service mesh to load balance requests between replicas of the same Executor.\nYou can use your favourite Kubernetes service mesh in combination with your Jina services, but the configuration files\ngenerated by `to_kubernetes_yaml()` already include all necessary annotations for the [Linkerd service mesh](https://linkerd.io).\n\n````{admonition} Hint\n:class: hint\nYou can use any service mesh with Jina-serve, but Jina-serve Kubernetes configurations come with Linkerd annotations out of the box.\n````\n\nTo use Linkerd you can follow the [install the Linkerd CLI guide](https://linkerd.io/2.11/getting-started/).\n\n````{admonition} Caution\n:class: caution\n\nMany service meshes can perform retries themselves.\nBe careful about setting up service mesh level retries in combination with Jina, as it may lead to unwanted behaviour in combination with\nJina's own {ref}`retry policy <flow-error-handling>`.\n\nInstead, you can disable Jina level retries by setting `Flow(retries=0)` in Python, or `retries: 0` in the Flow\nYAML's `with` block.\n````\n\n(kubernetes-replicas)=\n## Scaling Executors: Replicas and shards\n\nJina supports two types of scaling:\n\n- **Replicas** can be used with any Executor type and are typically used for performance and availability.\n- **Shards** are used for partitioning data and should only be used with indexers since they store state.\n\nCheck {ref}`here <scale-out>` for more information about these scaling mechanisms.\n\nFor shards, Jina creates one separate Deployment in Kubernetes per Shard.\nSetting `Deployment(..., shards=num_shards)` is sufficient to create a corresponding Kubernetes configuration.\n\nFor replicas, Jina-serve uses [Kubernetes native replica scaling](https://kubernetes.io/docs/tutorials/kubernetes-basics/scale/scale-intro/) and **relies on a service mesh** to load-balance requests between replicas of the same Executor.\nWithout a service mesh installed in your Kubernetes cluster, all traffic will be routed to the same replica.\n\n````{admonition} See Also\n:class: seealso\n\nThe impossibility of load balancing between different replicas is a limitation of Kubernetes in combination with gRPC.\nIf you want to learn more about this limitation, see [this](https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/) Kubernetes Blog post.\n````\n\n## Scaling the Gateway\nThe {ref}`Gateway <gateway>` is responsible for providing the API of the {ref}`Flow <flow>`.\nIf you have a large Flow with many Clients and many replicated Executors, the Gateway can become the bottleneck.\nIn this case you can also scale up the Gateway deployment to be backed by multiple Kubernetes Pods. For this reason, you can add `replicas` parameter to your Gateway before converting the Flow to Kubernetes.\nThis can be done in a Pythonic way or in YAML:\n\n````{tab} Using Python\n\nYou can use {meth}`~jina.Flow.config_gateway` to add `replicas` parameter\n```python\nfrom jina import Flow\n\nf = Flow().config_gateway(replicas=3).add()\n\nf.to_kubernetes_yaml('./k8s_yaml_path')\n```\n````\n\n````{tab} Using YAML\nYou can add `replicas` in the `gateway` section of your Flow YAML\n```yaml\njtype: Flow\ngateway:\n    replicas: 3\nexecutors:\n  - name: encoder\n```\n````\n\nAlternatively, this can be done by the regular means of Kubernetes: Either increase the number of replicas in the {ref}`generated yaml configuration files <kubernetes-export>` or [add replicas while running](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment).\nTo expose your Gateway replicas outside Kubernetes, you can add a load balancer as described {ref}`here <kubernetes-expose>`.\n\n````{admonition} Hint\n:class: hint\nYou can use a custom Docker image for the Gateway deployment by setting the environment variable `JINA_GATEWAY_IMAGE` to the desired image before generating the configuration.\n````\n\n## See also\n- {ref}`Step by step deployment of a Jina-serve Flow on Kubernetes <kubernetes>`\n- {ref}`Export a Flow to Kubernetes <kubernetes-export>`\n- {meth}`~jina.Flow.to_kubernetes_yaml`\n- {ref}`Deploy a standalone Executor on Kubernetes <kubernetes-executor>`\n- [Kubernetes Documentation](https://kubernetes.io/docs/home/)\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/k8s.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.89, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 167}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(kubernetes)=\n# Deploy on Kubernetes\n\nThis how-to will go through deploying a Deployment and a simple Flow using Kubernetes, customizing the Kubernetes configuration\nto your needs, and scaling Executors using replicas and shards.\n\nDeploying Jina-serve services in Kubernetes is the recommended way to use Jina-serve in production because Kubernetes can easily take over the lifetime management of Executors and Gateways.  \n\n```{seelaso}\nThis page is a step by step guide, refer to the {ref}`Kubernetes support documentation <kubernetes-docs>` for more details\n```\n\n\n```{hint}\nThis guide is designed for users who want to **manually** deploy a Jina-serve project on Kubernetes.\n\nCheck out {ref}`jcloud` if you want a **one-click** solution to deploy and host Jina, leveraging a cloud-native stack of Kubernetes, Prometheus and Grafana, **without worrying about provisioning**.\n```\n\n\n## Preliminaries\n\nTo follow this how-to, you need access to a Kubernetes cluster.\n\nYou can either set up [`minikube`](https://minikube.sigs.k8s.io/docs/start/), or use one of many managed Kubernetes\nsolutions in the cloud:\n- [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine)\n- [Amazon EKS](https://aws.amazon.com/eks)\n- [Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service)\n- [Digital Ocean](https://www.digitalocean.com/products/kubernetes/)\n\nYou need to install Linkerd in your K8s cluster. To use Linkerd, [install the Linkerd CLI](https://linkerd.io/2.11/getting-started/) and [its control plane](https://linkerd.io/2.11/getting-started/) in your cluster.\nThis automatically sets up and manages the service mesh proxies when you deploy the Flow.\n\nTo understand why you need to install a service mesh like Linkerd refer to this  {ref}`section <service-mesh-k8s>`\n\n(build-containerize-for-k8s)=\n## Build and containerize your Executors\n\nFirst, we need to build the Executors that we are going to use and containerize them {ref}`manually <dockerize-exec>` or by leveraging {ref}`Executor Hub <jina-hub>`. In this example,\nwe are going to use the Hub.\n\nWe are going to build two Executors, the first is going to use `CLIP` to encode textual Documents, and the second is going to use an in-memory vector index. This way \nwe can build a simple neural search system.\n\nFirst, we build the encoder Executor.\n\n````{tab} executor.py\n```{code-block} python\nimport torch\nfrom typing import Optional\nfrom transformers import CLIPModel, CLIPTokenizer\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\nfrom jina import Executor, requests\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass Encoder(Executor):\n    def __init__(\n            self, pretrained_model_name_or_path: str = 'openai/clip-vit-base-patch32', device: str = 'cpu', *args,**kwargs ):\n        super().__init__(*args, **kwargs)\n        self.device = device\n        self.tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path)\n        self.model = CLIPModel.from_pretrained(pretrained_model_name_or_path)\n        self.model.eval().to(device)\n\n    def _tokenize_texts(self, texts):\n        x = self.tokenizer(\n            texts,\n            max_length=77,\n            padding='longest',\n            truncation=True,\n            return_tensors='pt',\n        )\n        return {k: v.to(self.device) for k, v in x.items()}\n\n    @requests\n    def encode(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDoc]:\n        with torch.inference_mode():\n            input_tokens = self._tokenize_texts(docs.text)\n            docs.embedding = self.model.get_text_features(**input_tokens).cpu().numpy()\n        return docs\n\n```\n````\n````{tab} requirements.txt\n```\ntorch==1.12.0\ntransformers==4.16.2\n```\n````\n````{tab} config.yml\n```\njtype: Encoder\nmetas:\n  name: EncoderPrivate\n  py_modules:\n    - executor.py\n```\n````\n\nPutting all these files into a folder named CLIPEncoder and calling `jina hub push CLIPEncoder --private` should give:\n\n```shell\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Published ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                                                                ‚îÇ\n‚îÇ   üìõ Name           EncoderPrivate                         ‚îÇ\n‚îÇ   üîó Jina Hub URL   https://cloud.jina.ai/executor/<executor-id>/   ‚îÇ\n‚îÇ   üëÄ Visibility     private                                    ‚îÇ\n‚îÇ                                                                ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Usage ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ   Container   YAML     uses: jinaai+docker://<user-id>/EncoderPrivate:latest           ‚îÇ\n‚îÇ               Python   .add(uses='jinaai+docker://<user-id>/EncoderPrivate:latest')    ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ   Source      YAML     uses: jinaai://<user-id>/EncoderPrivate:latest                  ‚îÇ\n‚îÇ               Python   .add(uses='jinaai://<user-id>/EncoderPrivate:latest')           ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n```\n\nThen we can build an indexer to provide `index` and `search` endpoints:\n\n````{tab} executor.py\n```{code-block} python\nfrom typing import Optional, List\nfrom docarray import DocList, BaseDoc\nfrom docarray.index import InMemoryExactNNIndex\nfrom docarray.typing import NdArray\nfrom jina import Executor, requests\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass MyDocWithMatches(MyDoc):\n    matches: DocList[MyDoc] = []\n    scores: List[float] = []\n\n\nclass Indexer(Executor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._indexer = InMemoryExactNNIndex[MyDoc]()\n\n    @requests(on='/index')\n    def index(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDoc]:\n        self._indexer.index(docs)\n        return docs\n\n    @requests(on='/search')\n    def search(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDocWithMatches]:\n        res = DocList[MyDocWithMatches]()\n        ret = self._indexer.find_batched(docs, search_field='embedding')\n        matched_documents = ret.documents\n        matched_scores = ret.scores\n        for query, matches, scores in zip(docs, matched_documents, matched_scores):\n            output_doc = MyDocWithMatches(**query.dict())\n            output_doc.matches = matches\n            output_doc.scores = scores.tolist()\n            res.append(output_doc)\n        return res\n\n```\n````\n````{tab} config.yml\n```\njtype: Indexer\nmetas:\n  name: IndexerPrivate\n  py_modules:\n    - executor.py\n```\n````\n\nPutting all these files into a folder named Indexer and calling `jina hub push Indexer --private` should give:\n\n```shell\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Published ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                                                                ‚îÇ\n‚îÇ   üìõ Name           IndexerPrivate                         ‚îÇ\n‚îÇ   üîó Jina Hub URL   https://cloud.jina.ai/executor/<executor-id>/   ‚îÇ\n‚îÇ   üëÄ Visibility     private                                    ‚îÇ\n‚îÇ                                                                ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Usage ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ   Container   YAML     uses: jinaai+docker://<user-id>/IndexerPrivate:latest           ‚îÇ\n‚îÇ               Python   .add(uses='jinaai+docker://<user-id>/IndexerPrivate:latest')    ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ                      ‚îÇ\n‚îÇ   Source      YAML     uses: jinaai://<user-id>/IndexerPrivate:latest                  ‚îÇ\n‚îÇ               Python   .add(uses='jinaai://<user-id>/IndexerPrivate:latest')           ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n```\n\nNow, since we have created private Executors, we need to make sure that K8s has the right credentials to download\nfrom the private registry:\n\nFirst, we need to create the namespace where our Flow will run:\n\n```shell\nkubectl create namespace custom-namespace\n```\n\nSecond, we execute this python script:\n\n```python\nimport json\nimport os\nimport base64\n\nJINA_CONFIG_JSON_PATH = os.path.join(os.path.expanduser('~'), os.path.join('.jina', 'config.json'))\nCONFIG_JSON = 'config.json'\n\nwith open(JINA_CONFIG_JSON_PATH) as fp:\n    auth_token = json.load(fp)['auth_token']\n\nconfig_dict = dict()\nconfig_dict['auths'] = dict()\nconfig_dict['auths']['registry.hubble.jina.ai'] = {'auth': base64.b64encode(f'<token>:{auth_token}'.encode()).decode()}\n\nwith open(CONFIG_JSON, mode='w') as fp:\n    json.dump(config_dict, fp)\n```\n\nFinally, we add a secret to be used as [imagePullSecrets](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/) in the namespace from our config.json:\n\n```shell script\nkubectl -n custom-namespace create secret generic regcred --from-file=.dockerconfigjson=config.json --type=kubernetes.io/dockerconfigjson\n```\n\n## Deploy an embedding model inside a Deployment\n\nNow we are ready to first deploy our embedding model as an embedding service in Kubernetes.\n\nFor now, define a Deployment,\neither in {ref}`YAML <deployment-yaml-spec>` or directly in Python, as we do here:\n\n```python\nfrom jina import Deployment\n\nd = Deployment(port=8080, name='encoder', uses='jinaai+docker://<user-id>/EncoderPrivate', image_pull_secrets=['regcred'])\n```\n\nYou can serve any Deployment you want.\nJust ensure that the Executor is containerized, either by using *'jinaai+docker'*, or by {ref}`containerizing your local\nExecutors <dockerize-exec>`.\n\nNext, generate Kubernetes YAML configs from the Flow. Notice, that this step may be a little slow, because [Executor Hub](https://cloud.jina.ai/) may \nadapt the image to your Jina-serve and docarray version.\n\n```python\nd.to_kubernetes_yaml('./k8s_deployment', k8s_namespace='custom-namespace')\n```\n\nThe following file structure will be generated - don't worry if it's slightly different -- there can be \nchanges from one Jina-serve version to another:\n\n```\n.\n‚îî‚îÄ‚îÄ k8s_deployment\n    ‚îî‚îÄ‚îÄ encoder.yml\n```\n\nYou can inspect these files to see how Deployment and Executor concepts are mapped to Kubernetes entities.\nAnd as always, feel free to modify these files as you see fit for your use case.\n\n````{admonition} Caution: Executor YAML configurations\n:class: caution\n\nAs a general rule, the configuration files produced by `to_kubernetes_yaml()` should run out of the box, and if you strictly\nfollow this how-to they will.\n\nHowever, there is an exception to this: If you use a local dockerized Executor, and this Executors configuration is stored\nin a file other than `config.yaml`, you will have to adapt this Executor's Kubernetes YAML.\nTo do this, open the file and replace `config.yaml` with the actual path to the Executor configuration.\n\nThis is because when a Flow contains a Docker image, it can't see what Executor\nconfiguration was used to create that image.\nSince all of our tutorials use `config.yaml` for that purpose, the Flow uses this as a best guess.\nPlease adapt this if you named your Executor configuration file differently.\n````\n\nNext you can actually apply these configuration files to your cluster, using `kubectl`.\nThis launches the Deployment service.\n\nNow, deploy this Deployment to your cluster:\n```shell\nkubectl apply -R -f ./k8s_deployment\n```\n\nCheck that the Pods were created:\n```shell\nkubectl get pods -n custom-namespace\n```\n\n```text\nNAME                              READY   STATUS    RESTARTS   AGE\nencoder-81a5b3cf9-ls2m3           1/1     Running   0          60m\n```\n\nOnce you see that the Deployment ready, you can start embedding documents:\n\n```python\nfrom typing import Optional\nimport portforward\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\n\nfrom jina.clients import Client\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nwith portforward.forward('custom-namespace', 'encoder-81a5b3cf9-ls2m3', 8080, 8080):\n    client = Client(host='localhost', port=8080)\n    client.show_progress = True\n    docs = client.post(\n        '/encode',\n        inputs=DocList[MyDoc]([MyDoc(text=f'This is document indexed number {i}') for i in range(100)]),\n        return_type=DocList[MyDoc],\n        request_size=10\n    )\n\n    for doc in docs:\n        print(f'{doc.text}: {doc.embedding}')\n\n```\n\n## Deploy a simple Flow\n\nNow we are ready to build a Flow composed of multiple Executors.\n\nBy *simple* in this context we mean a Flow without replicated or sharded Executors - you can see how to use those in\nKubernetes {ref}`later on <kubernetes-replicas>`.\n\nFor now, define a Flow,\neither in {ref}`YAML <flow-yaml-spec>` or directly in Python, as we do here:\n\n```python\nfrom jina import Flow\n\nf = (\n    Flow(port=8080, image_pull_secrets=['regcred'])\n    .add(name='encoder', uses='jinaai+docker://<user-id>/EncoderPrivate')\n    .add(\n        name='indexer',\n        uses='jinaai+docker://<user-id>/IndexerPrivate',\n    )\n)\n```\n\nYou can essentially define any Flow of your liking.\nJust ensure that all Executors are containerized, either by using *'jinaai+docker'*, or by {ref}`containerizing your local\nExecutors <dockerize-exec>`.\n\nThe example Flow here simply encodes and indexes text data using two Executors pushed to the [Executor Hub](https://cloud.jina.ai/).\n \nNext, generate Kubernetes YAML configs from the Flow. Notice, that this step may be a little slow, because [Executor Hub](https://cloud.jina.ai/) may \nadapt the image to your Jina-serve and docarray version.\n\n```python\nf.to_kubernetes_yaml('./k8s_flow', k8s_namespace='custom-namespace')\n```\n\nThe following file structure will be generated - don't worry if it's slightly different -- there can be \nchanges from one Jina-serve version to another:\n\n```\n.\n‚îî‚îÄ‚îÄ k8s_flow\n    ‚îú‚îÄ‚îÄ gateway\n    ‚îÇ   ‚îî‚îÄ‚îÄ gateway.yml\n    ‚îî‚îÄ‚îÄ encoder\n    ‚îÇ   ‚îî‚îÄ‚îÄ encoder.yml\n    ‚îî‚îÄ‚îÄ indexer\n        ‚îî‚îÄ‚îÄ indexer.yml\n```\n\nYou can inspect these files to see how Flow concepts are mapped to Kubernetes entities.\nAnd as always, feel free to modify these files as you see fit for your use case.\n\nNext you can actually apply these configuration files to your cluster, using `kubectl`.\nThis launches all Flow microservices.\n\nNow, deploy this Flow to your cluster:\n```shell\nkubectl apply -R -f ./k8s_flow\n```\n\nCheck that the Pods were created:\n```shell\nkubectl get pods -n custom-namespace\n```\n\n```text\nNAME                              READY   STATUS    RESTARTS   AGE\nencoder-8b5575cb9-bh2x8           1/1     Running   0          60m\ngateway-66d5f45ff5-4q7sw          1/1     Running   0          60m\nindexer-8f676fc9d-4fh52           1/1     Running   0          60m\n```\n\nNote that the Jina gateway was deployed with name `gateway-7df8765bd9-xf5tf`.\n\nOnce you see that all the Deployments in the Flow are ready, you can start indexing documents:\n\n```python\nfrom typing import List, Optional\nimport portforward\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\n\nfrom jina.clients import Client\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass MyDocWithMatches(MyDoc):\n    matches: DocList[MyDoc] = []\n    scores: List[float] = []\n\n\nwith portforward.forward('custom-namespace', 'gateway-66d5f45ff5-4q7sw', 8080, 8080):\n    client = Client(host='localhost', port=8080)\n    client.show_progress = True\n    docs = client.post(\n        '/index',\n        inputs=DocList[MyDoc]([MyDoc(text=f'This is document indexed number {i}') for i in range(100)]),\n        return_type=DocList[MyDoc],\n        request_size=10\n    )\n\n    print(f'Indexed documents: {len(docs)}')\n    docs = client.post(\n        '/search',\n        inputs=DocList[MyDoc]([MyDoc(text=f'This is document query number {i}') for i in range(10)]),\n        return_type=DocList[MyDocWithMatches],\n        request_size=10\n    )\n    for doc in docs:\n        print(f'Query {doc.text} has {len(doc.matches)} matches')\n\n```\n\n### Deploy with shards and replicas\n\nAfter your service mesh is installed, your cluster is ready to run a Flow with scaled Executors.\nYou can adapt the Flow from above to work with two replicas for the encoder, and two shards for the indexer:\n\n```python\nfrom jina import Flow\n\nf = (\n    Flow(port=8080, image_pull_secrets=['regcred'])\n    .add(name='encoder', uses='jinaai+docker://<user-id>/CLIPEncoderPrivate', replicas=2)\n    .add(\n        name='indexer',\n        uses='jinaai+docker://<user-id>/IndexerPrivate',\n        shards=2,\n    )\n)\n```\n\nAgain, you can generate your Kubernetes configuration:\n\n```python\nf.to_kubernetes_yaml('./k8s_flow', k8s_namespace='custom-namespace')\n```\n\nNow you should see the following file structure:\n\n```\n.\n‚îî‚îÄ‚îÄ k8s_flow\n    ‚îú‚îÄ‚îÄ gateway\n    ‚îÇ   ‚îî‚îÄ‚îÄ gateway.yml\n    ‚îî‚îÄ‚îÄ encoder\n    ‚îÇ   ‚îî‚îÄ encoder.yml\n    ‚îî‚îÄ‚îÄ indexer\n        ‚îú‚îÄ‚îÄ indexer-0.yml\n        ‚îú‚îÄ‚îÄ indexer-1.yml\n        ‚îî‚îÄ‚îÄ indexer-head.yml\n```\n\nApply your configuration like usual:\n\n````{admonition} Hint: Cluster cleanup\n:class: hint\nIf you already have the simple Flow from the first example running on your cluster, make sure to delete it using `kubectl delete -R -f ./k8s_flow`.\n````\n\n```shell\nkubectl apply -R -f ./k8s_flow\n```\n\n### Deploy with custom environment variables and secrets\n\nYou can customize the environment variables that are available inside runtime, either defined directly or read from a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/):\n\n````{tab} with Python\n```python\nfrom jina import Flow\n\nf = (\n    Flow(port=8080, image_pull_secrets=['regcred'])\n    .add(\n        name='indexer',\n        uses='jinaai+docker://<user-id>/IndexerPrivate',\n        env={'k1': 'v1', 'k2': 'v2'},\n        env_from_secret={\n            'SECRET_USERNAME': {'name': 'mysecret', 'key': 'username'},\n            'SECRET_PASSWORD': {'name': 'mysecret', 'key': 'password'},\n        },\n    )\n)\n\nf.to_kubernetes_yaml('./k8s_flow', k8s_namespace='custom-namespace')\n```\n````\n````{tab} with flow YAML\nIn a `flow.yml` file :\n```yaml\njtype: Flow\nversion: '1'\nwith:\n  protocol: http\nexecutors:\n- name: indexer\n  uses: jinaai+docker://<user-id>/IndexerPrivate\n  env:\n    k1: v1\n    k2: v2\n  env_from_secret:\n    SECRET_USERNAME:\n      name: mysecret\n      key: username\n    SECRET_PASSWORD:\n      name: mysecret\n      key: password\n```\n\nYou can generate Kubernetes YAML configs using `jina export`:\n```shell\njina export kubernetes flow.yml ./k8s_flow --k8s-namespace custom-namespace\n```\n````\n\nAfter creating the namespace, you need to create the secrets mentioned above:\n\n```shell\nkubectl -n custom-namespace create secret generic mysecret --from-literal=username=jina --from-literal=password=123456\n```\n\nThen you can apply your configuration.\n\n\n(kubernetes-expose)=\n## Exposing the service\nThe previous examples use port-forwarding to send documents to the services. \nIn real world applications, \nyou may want to expose your service to make it reachable by users so that you can serve search requests.\n\n```{caution}\nExposing the Deployment or Flow only works if the environment of your `Kubernetes cluster` supports `External Loadbalancers`.\n```\n\nOnce the service is deployed, you can expose a service. In this case we give an example of exposing the encoder when using a Deployment,\nbut you can expose the gateway service when using a Flow:\n```bash\nkubectl expose deployment executor --name=executor-exposed --type LoadBalancer --port 80 --target-port 8080 -n custom-namespace\nsleep 60 # wait until the external ip is configured\n```\n\nExport the external IP address. This is needed for the client when sending Documents to the Flow in the next section. \n```bash\nexport EXTERNAL_IP=`kubectl get service executor-expose -n custom-namespace -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'`\n```\n\n### Client\nThe client:\n\n- Sends Documents to the exposed service on `$EXTERNAL_IP` \n- Gets the responses.\n\nYou should configure your Client to connect to the service via the external IP address as follows:\n\n```python\nimport os\nfrom typing import List, Optional\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\n\nfrom jina.clients import Client\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\n\n\nclass MyDocWithMatches(MyDoc):\n    matches: DocList[MyDoc] = []\n    scores: List[float] = []\n\nhost = os.environ['EXTERNAL_IP']\nport = 80\n\nclient = Client(host=host, port=port)\n\nclient.show_progress = True\ndocs = DocList[MyDoc]([MyDoc(text=f'This is document indexed number {i}') for i in range(100)])\nqueried_docs = client.post(\"/search\", inputs=docs, return_type=DocList[MyDocWithMatches])\n\nmatches = queried_docs[0].matches\nprint(f\"Matched documents: {len(matches)}\")\n```\n\n## Update your Executor in Kubernetes\n\nIn Kubernetes, you can update your Executors by patching the Deployment corresponding to your Executor.\n\nFor instance, in the example above, you can change the CLIPEncoderPrivate's `pretrained_model_name_or_path` parameter by changing the content of the Deployment inside the `executor.yml` dumped by `.to_kubernetes_yaml`.\n\nYou need to add `--uses_with` and pass the batch size argument to it. This is passed to the container inside the Deployment:\n\n```yaml\n    spec:\n      containers:\n      - args:\n        - executor\n        - --name\n        - encoder\n        - --k8s-namespace\n        - custom-namespace\n        - --uses\n        - config.yml\n        - --port\n        - '8080'\n        - --uses-metas\n        - '{}'\n        - --uses-with\n        - '{\"pretrained_model_name_or_path\": \"other_model\"}'\n        - --native\n        command:\n        - jina\n```\n\nAfter doing so, re-apply your configuration so the new Executor will be deployed without affecting the other unchanged Deployments:\n\n```shell script\nkubectl apply -R -f ./k8s_deployment\n```\n\n````{admonition} Other patching options\n:class: seealso\n\nIn Kubernetes Executors are ordinary Kubernetes Deployments, so you can use other patching options provided by Kubernetes:\n\n\n- `kubectl replace` to replace an Executor using a complete configuration file\n- `kubectl patch` to patch an Executor using only a partial configuration file\n- `kubectl edit` to edit an Executor configuration on the fly in your editor\n\nYou can find more information about these commands in the [official Kubernetes documentation](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/).\n````\n\n## Key takeaways\n\nIn short, there are just three key steps to deploy Jina on Kubernetes:\n\n1. Use `.to_kubernetes_yaml()` to generate Kubernetes configuration files from a Jina Deployment or Flow object.\n2. Apply the generated file via `kubectl`(Modify the generated files if necessary)\n3. Expose your service outside the K8s cluster\n\n## See also\n- {ref}`Kubernetes support documentation <kubernetes-docs>`\n- {ref}`Monitor service once it is deployed <monitoring>`\n- {ref}`See how failures and retries are handled <flow-error-handling>`\n- {ref}`Learn more about scaling Executors <scale-out>`\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/kubernetes.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.98, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 695}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(monitoring)=\n# Prometheus/Grafana Support (Legacy)\n\n```{admonition} Deprecated\n:class: caution\nThe Prometheus-only based feature will soon be deprecated in favor of the OpenTelemetry Setup. Refer to {ref}`OpenTelemetry Setup <opentelemetry>` for the details on OpenTelemetry setup for Jina-serve.\n\nRefer to the {ref}`OpenTelemetry migration guide <opentelemetry-migration>` for updating your existing Prometheus and Grafana configurations.\n```\n\nWe recommend the Prometheus/Grafana stack to leverage the metrics exposed by Jina-serve. In this setup, Jina-serve exposes different metrics, and Prometheus scrapes these endpoints, as well as\ncollecting, aggregating, and storing the metrics. \n\nExternal entities (like Grafana) can access these aggregated metrics via the query language [PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/) and let users visualize the metrics with dashboards.\n\n\n```{hint} \nJina supports exposing metrics, but you are in charge of installing and managing your Prometheus/Grafana instances.\n```\n\nIn this guide, we deploy the Prometheus/Grafana stack and use it to monitor a Flow.\n\n(deploy-flow-monitoring)=\n## Deploying the Flow and the monitoring stack\n\n### Deploying on Kubernetes\n\nOne challenge of monitoring a {class}`~jina.Flow` is communicating its different metrics endpoints to Prometheus.\nFortunately, the [Prometheus operator for Kubernetes](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/getting-started.md) makes this fairly easy because it can automatically discover new metrics endpoints to scrape.\n\nWe recommend deploying your Jina-serve Flow on Kubernetes to leverage the full potential of the monitoring feature because:\n* The Prometheus operator can automatically discover new endpoints to scrape.\n* You can extend monitoring with the rich built-in Kubernetes metrics.\n\nYou can deploy Prometheus and Grafana on your Kubernetes cluster by running:\n\n```bash\nhelm install prometheus prometheus-community/kube-prometheus-stack --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false\n```\n```{hint} \nsetting the `serviceMonitorSelectorNilUsesHelmValues` to false allows the Prometheus Operator to discover metrics endpoint outside of the helm scope which is needed to discover the Flow metrics endpoints.\n```\n\nDeploy the Flow that we want to monitor:\n\nFor this example we recommend reading {ref}`how to build and containerize the Executors to be run in Kubernetes. <build-containerize-for-k8s>`\n\n\n````{tab} via YAML\nThis example shows how to start a Flow with monitoring enabled via YAML:\n\nIn a `flow.yaml` file\n```yaml\njtype: Flow\nwith:\n  monitoring: true\nexecutors:\n- uses: jinaai+docker://<user-id>/EncoderPrivate\n```\n\n```bash\njina export kubernetes flow.yml ./config ```\n````\n\n````{tab} via Python API\n```python\nfrom jina import Flow\n\nf = Flow(monitoring=True).add(uses='jinaai+docker://<user-id>/EncoderPrivate')\nf.to_kubernetes_yaml('config')\n```\n````\n\nThis creates a `config` folder containing the Kubernetes YAML definition of the Flow.\n\n```{seealso}\nYou can see in-depth how to deploy a Flow on Kubernetes {ref}`here <kubernetes>`\n```\n\nThen deploy the Flow:\n\n```bash\nkubectl apply -R -f config\n```\n\nWait for a couple of minutes, and you should see that the Pods are ready:\n\n```bash\nkubectl get pods\n```\n\n```{figure} ../../.github/2.0/kubectl_pods.png\n:align: center\n```\n\nThen you can see that the new metrics endpoints are automatically discovered:\n\n```bash\nkubectl port-forward svc/prometheus-operated 9090:9090\n```\n\n```{figure} ../../.github/2.0/prometheus_target.png\n:align: center\n```\nBefore querying the gateway you need to port-forward\n```bash\nkubectl port-forward svc/gateway 8080:8080\n```\n\nTo access Grafana, run:\n\n```bash\nkb port-forward svc/prometheus-grafana 3000:80\n```\n\nThen open `http://localhost:3000` in your browser. The username is `admin` and password is `prom-operator`.\n\nYou should see the Grafana home page.\n\n\n### Deploying locally\n\nDeploy the Flow that we want to monitor:\n\n\n````{tab} via Python code\n```python\nfrom jina import Flow\n\nwith Flow(monitoring=True, port_monitoring=8000, port=8080).add(\n    uses='jinaai+docker://<user-id>/EncoderPrivate', port_monitoring=9000\n) as f:\n    f.block()\n```\n````\n\n````{tab} via docker-compose\n```python\nfrom jina import Flow\n\nFlow(monitoring=True, port_monitoring=8000, port=8080).add(\n    uses='jinaai+docker://<user-id>/EncoderPrivate', port_monitoring=9000\n).to_docker_compose_yaml('config.yaml')\n```\n```bash\ndocker-compose -f config.yaml up\n```\n````\n\nTo monitor a Flow locally you need to install Prometheus and Grafana locally. The easiest way to do this is with\nDocker Compose.\n\nFirst clone the repo which contains the config file:\n\n```bash\ngit clone https://github.com/jina-ai/example-grafana-prometheus\ncd example-grafana-prometheus/prometheus-grafana-local\n```\n\nthen \n\n```bash\ndocker-compose up\n```\n\nAccess the Grafana dashboard at `http://localhost:3000`. The username is `admin` and the password is `foobar`.\n\n```{caution}\nThis example works locally because Prometheus is configured to listen to ports 8000 and 9000. However,\nin contrast to deploying on Kubernetes, you need to tell Prometheus which port to look at. You can change these\nports by modifying [prometheus.yml](https://github.com/jina-ai/example-grafana-prometheus/blob/8baf519f7258da68cfe224775fc90537a749c305/prometheus-grafana-local/prometheus/prometheus.yml#L64).\n```\n\n### Deploying on Jcloud\n\nIf your Flow is deployed on JCloud, you don't need to provision a monitoring stack yourself. Prometheus and Grafana are \nhandled by JCloud and you can find a dashboard URL with `jc status <flow_id>`\n\n## Using Grafana to visualize metrics\n\nAccess the Grafana homepage, then go to `Browse` then `import` and copy and paste the [JSON file](https://github.com/jina-ai/example-grafana-prometheus/blob/main/grafana-dashboards/flow.json) \n\n\nYou should see the following dashboard:\n\n```{figure} ../../.github/2.0/grafana.png\n:align: center\n```\n\n\n````{admonition} Hint\n:class: hint\n\nYou should query your Flow to generate the first metrics. Otherwise the dashboard looks empty.\n````\n\nYou can query the Flow by running:\n\n```python\nfrom typing import Optional\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\nfrom jina import Client\n\n\nclass MyDoc(BaseDoc):\n    text: str\n    embedding: Optional[NdArray] = None\nclient = Client(port=51000)\n\nclient.post(on='/', inputs=DocList[MyDoc]([MyDoc(text=f'Text for document {i}') for in range(100)]), return_type=DocList[MyDoc], request_size=10,)\n```\n\n## See also\n\n- [Using Grafana to visualize Prometheus metrics](https://grafana.com/docs/grafana/latest/getting-started/getting-started-prometheus/)\n- {ref}`Defining custom metrics in an Executor <monitoring>`\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/monitoring.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.92, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 218}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(opentelemetry-migration)=\n# Migrate from Prometheus/Grafana to OpenTelemetry\n\nThe {ref}`Prometheus/Grafana <monitoring>` based monitoring setup will soon be deprecated in favor of the {ref}`OpenTelemetry setup <opentelemetry>`. This section provides the details required to update/migrate your Prometheus configuration and Grafana dashboard to continue monitoring with OpenTelemetry. Refer to {ref}`Opentelemetry setup <opentelemetry>` for the new setup before proceeding further.\n\n```{hint}\n:class: seealso\nRefer to {ref}`Prometheus/Grafana-only <monitoring>` section for the soon to be deprecated setup.\n```\n\n## Update Prometheus configuration\n\nWith a Prometheus-only setup, you need to set up a `scrape_configs` configuration or service discovery plugin to specify the targets for pulling metrics data. In the OpenTelemetry setup, each Pod pushes metrics to the OpenTelemetry Collector. The Prometheus configuration now only needs to scrape from the OpenTelemetry Collector to get all the data from OpenTelemetry-instrumented applications.\n\nThe new Prometheus configuration for the `otel-collector` Collector hostname is:\n\n```yaml\nscrape_configs:\n  - job_name: 'otel-collector'\n    scrape_interval: 500ms\n    static_configs:\n      - targets: ['otel-collector:8888'] # metrics from the collector itself\n      - targets: ['otel-collector:8889'] # metrics collected from other applications\n```\n\n## Update Grafana dashboard\n\nThe OpenTelemetry [Histogram](https://opentelemetry.io/docs/reference/specification/metrics/api/#histogram) provides quantile window buckets automatically (unlike the Prometheus [Summary](https://prometheus.io/docs/concepts/metric_types/#summary) instrument). You need to manually configure the required quantile window. The quatile window metric will then be available as a separate time series metric.\n\nIn addition, the OpenTelemetry `Counter/UpDownCounter` instruments do not add the `_total` suffix to the base metric name.\n\nTo adapt Prometheus queries in Grafana:\n- Use the [histogram_quantile](https://prometheus.io/docs/prometheus/latest/querying/functions/#histogram_quantile) function to query the average or desired quantile window time series data from Prometheus. For example, to view the 0.99 quantile of the `jina_receiving_request_seconds` metric over the last 10 minutes, use query `histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[10m]))`.\n- Remove the `_total` prefix from the Counter/UpDownCounter metric names.\n\nYou can download a [sample Grafana dashboard JSON file](https://github.com/jina-ai/example-grafana-prometheus/blob/main/grafana-dashboards/flow-histogram-metrics.json) and import it into Grafana to get started with some pre-built graphs.\n\n```{hint}\nA list of available metrics is in the {ref}`Flow Instrumentation <instrumenting-flow>` section.\n```", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/opentelemetry-migration.md", "language": "markdown", "complexity": "medium", "star_count": 21665, "quality_score": 0.89, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 40}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(opentelemetry)=\n# {octicon}`telescope-fill` OpenTelemetry Support\n\n```{toctree}\n:hidden:\n\nopentelemetry-migration\nmonitoring\n```\n\n```{hint}\nPrometheus-only based metrics collection will soon be deprecated. Refer to {ref}`Monitor with Prometheus and Grafana <monitoring>` for the old setup.\n```\n\nThere are two major setups required to visualize/monitor your application's signals using [OpenTelemetry](https://opentelemetry.io). The first setup is covered by Jina-serve which integrates the [OpenTelemetry API and SDK](https://opentelemetry-python.readthedocs.io/en/stable/api/index.html) at the application level. The {ref}`Flow Instrumentation <instrumenting-flow>` page covers in detail the steps required to enable OpenTelemetry in a Flow. A {class}`~jina.Client` can also be instrumented which is documented in the {ref}`Client Instrumentation <instrumenting-client>` section.\n\nThis section covers the OpenTelemetry infrastructure setup required to collect, store and visualize the traces and metrics data exported by the Pods. This setup is the user's responsibility, and this section only serves as the initial/introductory guide to running OpenTelemetry infrastructure components.\n\nSince OpenTelemetry is open source and is mostly responsible for the API standards and specification, various providers implement the specification. This section follows the default recommendations from the OpenTelemetry documentation that also fits into the Jina-serve implementations.\n\n## Exporting traces and metrics data\n\nPods created using a {class}`~jina.Flow` with tracing or metrics enabled use the [SDK Exporters](https://opentelemetry.io/docs/instrumentation/python/exporters/) to send the data to a central [Collector](https://opentelemetry.io/docs/collector/) component. You can use this collector to further process and store the data for visualization and alerting. \n\nThe push/export-based mechanism also allows the application to start pushing data immediately on startup. This differs from the pull-based mechanism where you need a separate scraping registry to discovery service to identify data scraping targets.\n\nYou can configure the exporter backend host and port using the `traces_exporter_host`, `traces_exporter_port`, `metrics_exporter_host` and `metrics_exporter_port`. Even though the Collector is metric data-type agnostic (it accepts any type of OpenTelemetry API data model), we provide separate configuration for Tracing and Metrics to give you more flexibility in choosing infrastructure components.\n\nJina-serve's default exporter implementation is  `OTLPSpanExporter` and `OTLPMetricExporter`. The exporters also use the gRPC data transfer protocol. The following environment variables can be used to further configure the exporter client based on your requirements. The full list of exporter related environment variables are documented by the [PythonSDK library](https://opentelemetry-python.readthedocs.io/en/latest/exporter/otlp/otlp.html). Apart from `OTEL_EXPORTER_OTLP_PROTOCOL` and `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT`, you can use all other library version specific environment variables to configure the exporter clients.\n\n\n## Collector\n\nThe [Collector](https://opentelemetry.io/docs/collector/) is a huge ecosystem of components that support features like scraping, collecting, processing and further exporting data to storage backends. The collector itself can also expose endpoints to allow scraping data. We recommend reading the official documentation to understand the the full set of features and configuration required to run a Collector. Read the below section to understand the minimum number of components and the respective configuration required for operating with Jina-serve.\n\nWe recommend using the [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/) from the contrib repository. We also use:\n- [Jaeger](https://www.jaegertracing.io) for collecting traces, visualizing tracing data and alerting based on tracing data.\n- [Prometheus](https://prometheus.io) for collecting metric data and/or alerting.\n- [Grafana](https://grafana.com) for visualizing data from Prometheus/Jaeger and/or alerting based on the data queried.\n\n```{hint}\nJaeger provides a comprehensive out of the box tools for end-to-end tracing monitoring, visualization and alerting. You can substitute other tools to achieve the necessary goals of observability and performance analysis. The same can be said for Prometheus and Grafana.\n```\n\n### Docker Compose\n\nA minimal `docker-compose.yml` file can look like:\n\n```yaml\nversion: \"3\"\nservices:\n  # Jaeger\n  jaeger:\n    image: jaegertracing/all-in-one:latest\n    ports:\n      - \"16686:16686\"\n\n  otel-collector:\n    image: otel/opentelemetry-collector:0.61.0\n    command: [ \"--config=/etc/otel-collector-config.yml\" ]\n    volumes:\n      - ${PWD}/otel-collector-config.yml:/etc/otel-collector-config.yml\n    ports:\n      - \"8888\" # Prometheus metrics exposed by the collector\n      - \"8889\" # Prometheus exporter metrics\n      - \"4317:4317\" # OTLP gRPC receiver\n    depends_on:\n      - jaeger\n\n  prometheus:\n    container_name: prometheus\n    image: prom/prometheus:latest\n    volumes:\n      - ${PWD}/prometheus-config.yml:/etc/prometheus/prometheus.yml\n    ports:\n      - \"9090:9090\"\n\n  grafana:\n    container_name: grafana\n    image: grafana/grafana-oss:latest\n    ports:\n      - 3000:3000\n```\n\nThe corresponding OpenTelemetry Collector configuration below needs to be stored in file `otel-collector-config.yml`:\n```yaml\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n\nexporters:\n  jaeger:\n    endpoint: jaeger:14250\n    tls:\n      insecure: true\n  \n  prometheus:\n    endpoint: \"0.0.0.0:8889\"\n    resource_to_telemetry_conversion:\n      enabled: true\n    # can be used to add additional labels\n    const_labels:\n      label1: value1\n\nprocessors:\n  batch:\n\nservice:\n  extensions: []\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [jaeger]\n      processors: [batch]\n    metrics:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [prometheus]\n```\n\nThis setup creates a gRPC Collector Receiver on port 4317 that collects data pushed by the Flow Pods. Collector exporters for Jaeger and Prometheus backends are configured to export tracing and metrics data respectively. The final **service** section creates a collector pipeline combining the receiver (collect data) and exporter (to backend), process (batching) sub-components.\n\nThe minimal Prometheus configuration needs to be stored in `prometheus-config.yml`.\n```yaml\nscrape_configs:\n  - job_name: 'otel-collector'\n    scrape_interval: 500ms\n    static_configs:\n      - targets: ['otel-collector:8889']\n      - targets: ['otel-collector:8888']\n```\n\nThe Prometheus configuration now only needs to scrape from the OpenTelemetry Collector to get all the data from OpenTelemetry Metrics instrumented applications.\n\n\n### Running a Flow locally\n\nRun the Flow and a sample request that we want to instrument locally. If the backends are running successfully the Flow has exported data to the Collector which can be queried and viewed.\n\nFirst start a Flow:\n\n```python\nfrom jina import Flow, Executor, requests\nfrom docarray import DocList, BaseDoc\nimport time\n\nclass MyExecutor(Executor):\n    \n    @requests\n    def foo(self, docs: DocList[BaseDoc], **kwargs) -> DocList[BaseDoc]:\n        time.sleep(0.5)\n        return docs\n\nwith Flow(\n    port=54321,\n    tracing=True,\n    traces_exporter_host='http://localhost',\n    traces_exporter_port=4317,\n    metrics=True,\n    metrics_exporter_host='http://localhost',\n    metrics_exporter_port=4317,\n).add(uses=MyExecutor) as f:\n    f.block()\n```\n\nSecond execute requests using the instrumented {class}`jina.Client`:\n\n```python\nfrom jina import Client\nfrom docarray import DocList, BaseDoc\n\nclient = Client(\n    host='grpc://localhost:54321',\n    tracing=True,\n    traces_exporter_host='http://localhost',\n    traces_exporter_port=4317,\n)\nclient.post('/', DocList[BaseDoc]([BaseDoc()]), return_type=DocList[BaseDoc])\nclient.teardown_instrumentation()\n```\n\n```{hint}\nThe {class}`jina.Client` currently only supports OpenTelemetry Tracing.\n```\n\n## Viewing Traces in Jaeger UI\n\nYou can open the Jaeger UI [here](http://localhost:16686). You can find more information on the Jaeger UI in the official [docs](https://www.jaegertracing.io/docs/1.38/external-guides/#using-jaeger).\n\n```{hint}\nThe list of available traces are documented in the {ref}`Flow Instrumentation <instrumenting-flow>` section.\n```\n\n## Monitor with Prometheus and Grafana\n\nExternal entities (like Grafana) can access these aggregated metrics via the [PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/) query language, and let users visualize metrics with dashboards. Check out a [comprehensive tutorial](https://prometheus.io/docs/visualization/grafana/) for more information.\n\nDownload a [sample Grafana dashboard JSON file](https://github.com/jina-ai/example-grafana-prometheus/blob/main/grafana-dashboards/flow-histogram-metrics.json) and import it into Grafana to get started with some pre-built graphs:\n\n```{figure} ../../.github/2.0/grafana-histogram-metrics.png\n:align: center\n```\n\n```{hint}\n:class: seealso\nA list of available metrics is in the {ref}`Flow Instrumentation <instrumenting-flow>` section.\nTo update your existing Prometheus and Grafana configurations, refer to the {ref}`OpenTelemetry migration guide <opentelemetry-migration>`.\n```\n\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/cloud-nativeness/opentelemetry.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.9500000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 211}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(jcloud-configuration)=\n# {octicon}`file-code` Configuration\n\nJCloud extends Jina-serve's {ref}`Flow YAML specification<flow-yaml-spec>` by introducing the special field `jcloud`. This lets you define resources and scaling policies for each Executor and Gateway.\n\nHere's a Flow with two Executors that have specific resource needs: `indexer` requires a 10 GB `ebs` disk, whereas `encoder` requires a G4 instance, which implies that two cores and 4 GB RAM are used. See the below sections for further information about instance types.\n\n```{code-block} yaml\n---\nemphasize-lines: 5-7,10-16\n---\njtype: Flow\nexecutors:\n  - name: encoder\n    uses: jinaai+docker://<username>/Encoder\n    jcloud:\n      resources:\n        instance: C4\n  - name: indexer\n    uses: jinaai+docker://<username>/Indexer\n    jcloud:\n      resources:\n        storage: \n          kind: ebs\n          size: 10G\n```\n\n## Allocate Executor resources\n\nSince each Executor has its own business logic, it may require different cloud resources. One Executor might need more RAM, whereas another might need a bigger disk. \n\nIn JCloud, you can pass highly customizable, finely-grained resource requests for each Executor using the `jcloud.resources` argument in your Flow YAML.\n\n### Instance\n\nJCloud uses the concept of an \"instance\" to represent a specific set of hardware specifications.\nIn the above example, a C4 instance type represents two cores and 4 GB RAM based on the CPU tiers instance definition table below.\n\n````{admonition} Note\n:class: note\nWe will translate the raw numbers from input to instance tier that fits most closely if you are still using the legacy resource specification interface, such as:\n\n```{code-block} yaml\njcloud:\n  resources:\n    cpu: 8\n    memory: 8G\n```\n\nThere are circumstances in the instance tier where they don't exactly fulfill the CPU cores and memory you need, like in the above example.\nIn cases like this we \"ceil\" the requests to the lowest tier that satisfies all the specifications. \nIn this case, `C6` would be considered, as `C5`'s `Cores` are lower than what's being requested (4 vs 8).\n````\n\nThere are also two types of instance tiers, one for CPU instances, one for GPU.\n\n(jcloud-pricing)=\n#### Pricing\nEach instance has a fixed `Credits Per Hour` number, indicating how many credits JCloud will charge\nif a certain instance is used. For example, if an Executor uses `C3`, it implies that `10` credits will be spent\nfrom the operating user account. Other important facts to note:\n\n   - If the Flow is powering other App(s) you create, you will be charged by the App(s), not the underlying Flow.\n   - `Credits Per Hour` is on an Executor/Gateway basis, the total `Credits Per Hour` of a Flow is the sum of all the credits\n     each components cost.\n   - If shards/replicas are used in an Executor/Gateway, the same instance type will be used, so `Credits Per Hour` will be multiplied.\n     For example, if an Executor uses `C3` and it has two replicas, the `Credits Per Hour` for the Executor would double to `20`.\n     The only exception is when sharding is used. In that case `C1` would be used for the shards head, regardless of what instance type has been entered for the shared Executor.\n\n```{hint}\nPlease visit [Jina AI Cloud Pricing](https://cloud.jina.ai/pricing/) for more information about billing and credits.\n```\n\n#### CPU tiers\n\n| Instance | Cores | Memory | Credits per hour |\n| -------- | ----- | ------ | ---------------- |\n| C1       | 0.1   | 0.2 GB | 1                |\n| C2       | 0.5   | 1 GB   | 5                |\n| C3       | 1     | 2 GB   | 10               |\n| C4       | 2     | 4 GB   | 20               |\n| C5       | 4     | 8 GB   | 40               |\n| C6       | 8     | 16 GB  | 80               |\n| C7       | 16    | 32 GB  | 160              |\n| C8       | 32    | 64 GB  | 320              |\n\n\nBy default, C1 is allocated to each Executor and Gateway.\n\nJCloud offers the general Intel Xeon processor (Skylake 8175M or Cascade Lake 8259CL) for the CPU instances. \n\n#### GPU tiers\n\nJCloud supports GPU workloads with two different usages: `shared` or `dedicated`. \n\nIf GPU is enabled, JCloud will provide NVIDIA A10G Tensor Core GPUs with 24 GB memory for workloads in both usage types.\n\n```{hint}\nWhen using GPU resources, it may take a few extra minutes before all Executors are ready to serve traffic.\n```\n\n| Instance | GPU    | Memory | Credits per hour |\n| -------- | ------ | ------ | ---------------- |\n| G1       | shared | 14 GB  | 100              |\n| G2       | 1      | 14 GB  | 125              |\n| G3       | 2      | 24 GB  | 250              |\n| G4       | 4      | 56 GB  | 500              |\n\n##### Shared GPU\n\nAn Executor using a `shared` GPU shares this GPU with up to four other Executors.\nThis enables time-slicing, which allows workloads that land on oversubscribed GPUs to interleave with one another.\n\nTo use `shared` GPU, `G1` needs to be specified as the instance type.\n\nThe tradeoffs with a `shared` GPU are increased latency, jitter, and potential out-of-memory (OOM) conditions when many different applications are time-slicing on the GPU. If your application is consuming a lot of memory, we suggest using a dedicated GPU.\n\n##### Dedicated GPU\n\nUsing a dedicated GPU is the default way to provision a GPU for an Executor. This automatically creates nodes or assigns the Executor to a GPU node. In this case, the Executor owns the whole GPU.\n\nTo use a `dedicated` GPU, `G2`/ `G3` / `G4` needs to be specified as instance type.\n\n### Storage\n\nJCloud supports three kinds of storage: ephemeral (default), [efs](https://aws.amazon.com/efs/) (network file storage) and [ebs](https://aws.amazon.com/ebs/) (block device).\n\n`ephemeral` storage will assign space to an Executor when it is created. Data in `ephemeral` storage is deleted permanently if Executors are restarted or rescheduled.\n\n````{hint}\n\nBy default, we assign `ephemeral` storage to all Executors in a Flow. This lets the storage resize dynamically, so you don't need to shrink/grow volumes manually.\n\nIf your Executor needs to share data with other Executors and retain data persistency, consider using `efs`. Note that:\n\n- IO performance is slower compared to `ebs` or `ephemeral`\n- The disk can be shared with other Executors or Flows.\n- Default storage size is 5 GB.\n\nIf your Executor needs high IO, you can use `ebs` instead. Note that:\n\n- The disk cannot be shared with other Executors or Flows.\n- Default storage size is 5 GB.\n````\n\nJCloud also supports retaining the data that a Flow was using while it was active. You can set the `retain` argument to `true` to enable this feature.\n\n```{code-block} yaml\n---\nemphasize-lines: 5-10,12,15\n---\njtype: Flow\nexecutors:\n  - name: executor1\n    uses: jinaai+docker://<username>/Executor1\n    jcloud:\n      resources:\n        storage:\n          kind: ebs\n          size: 10G\n          retain: true\n  - name: executor2\n    uses: jinaai+docker://<username>/Executor2\n    jcloud:\n      resources:\n        storage:\n          kind: efs\n```\n#### Pricing\nHere are the numbers in terms of credits per GB per month for the three kinds of storage described above.\n\n| Instance  | Credits per GB per month |\n| --------- | ------------------------ |\n| Ephemeral | 0                        |\n| EBS       | 30                       |\n| EFS       | 75                       |\n\nFor example, using 10 GB of EBS storage for a month costs `30` credits.\nIf shards/replicas are used, we will multiply credits further by the number of storages created.\n\n## Scale out Executors\n\nOn JCloud, demand-based autoscaling functionality is naturally offered thanks to the underlying Kubernetes architecture. This means that you can maintain [serverless](https://en.wikipedia.org/wiki/Serverless_computing) deployments in a cost-effective way with no headache of setting the [right number of replicas](https://jina.ai/serve/how-to/scale-out/#scale-out-your-executor) anymore!\n\n\n### Autoscaling with `jinaai+serverless://` \n\nThe easiest way to scale out your Executor is to use a Serverless Executor. This can be enabled by using `jinaai+serverless://` instead of `jinaai+docker://` in Executor's `uses`, such as:\n\n```{code-block} yaml\n---\nemphasize-lines: 4\n---\njtype: Flow\nexecutors:\n  - name: executor1\n    uses: jinaai+serverless://<username>/Executor1\n```\n\nJCloud autoscaling leverages [Knative](https://knative.dev/docs/) behind the scenes, and `jinahub+serverless` uses a set of Knative configurations as defaults.\n\n```{hint}\nFor more information about the Knative autoscaling configurations, please visit [Knative autoscaling](https://knative.dev/docs/serving/autoscaling/).\n```\n\n\n### Autoscaling with custom args\n\nIf `jinaai+serverless://` doesn't meet your requirements, you can further customize autoscaling configurations by using the `autoscale` argument on a per-Executor basis in the Flow YAML, such as:\n\n```{code-block} yaml\n---\nemphasize-lines: 5-10\n---\njtype: Flow\nexecutors:\n  - name: executor1\n    uses: jinaai+docker://<username>/Executor1\n    jcloud:\n      autoscale:\n        min: 1\n        max: 2\n        metric: rps\n        target: 50\n```\n\nBelow are the defaults and requirements for the configurations:\n\n| Name             | Default     | Allowed                                     | Description                                                               |\n| ------           | ----------- | ------------------------                    | -------------------------------------------------                         |\n| min              | 1           | int                                         | Minimum number of replicas (`0` means serverless)                         |\n| max              | 2           | int, up to 5                                | Maximum number of replicas                                                |\n| metric           | concurrency | `concurrency` / `rps` / `cpu` / `memory`    | Metric for scaling                                                        |\n| scale_down_delay | 30s         | str, `0s` <= value <= `1h`                  | Time window which must pass at reduced concurrency before a scaling down  |\n| target           | 100         | int                                         | Target number the replicas try to maintain.                               |\n\nThe unit of `target` depends of the metric specified. Refer to the table below:\n\n| Metric        | Target                                                                                                                                                  |\n| ----          | -----                                                                                                                                                   |\n| `concurrency` | Number of concurrent requests processed at any given time.                                                                                              |\n| `rps`         | Number of requests processed per second per replica.                                                                                                    |\n| `cpu`         | Average % CPU utilization of each pod<br>(e.g. `60` means replicas will be scaled up when pods on average reach 60% CPU utilization)                    |\n| `memory`      | Average mebibytes of memory used by each pod<br>(e.g. `200` means replicas will be scaled up when the average pods' memory consumption exceeds 200MiB). |\n\nAfter you make a JCloud deployment using the autoscaling configuration, the Flow serving part is just the same: the only difference you may notice is it takes a few extra seconds to handle the initial requests since it needs to scale the deployments behind the scenes. Let JCloud handle the scaling from now on, and you can deal with the code!\n\nNote, that if `metric` is `cpu` or `memory`, `min` will be reset to 1 if user sets it to set to 0.\n\n### Pricing\nAt present, pricing for autoscaled Executor/Gateway largely follows the same {ref}`JCloud pricing rules <jcloud-pricing>` as other Jina AI services.\nWe track the minimum number of replicas in autoscale configurations and use it as a multiplier for the replicas used when calculating the\n`Credits Per Hour`.\n\n### Restrictions\n```{admonition} **Restrictions**\n\n- Autoscale does not currently allow the use of `ebs` as a storage type in combination. Please use `efs` and `ephemeral` instead.\n- Autoscale is not supported for multi-protocol Gateways.\n```\n## Configure availability tolerance\n\nIf service issues cause disruption of Executors, JCloud lets you specify a tolerance level for number of replicas that stay up or go down.\n\nThe JCloud parameters `minAvailable` and `maxUnavailable` ensure that Executors will stay up even if a certain number of replicas go down.\n\n| Name             | Default |                                          Allowed                                          | Description                                              |\n| :--------------- | :-----: | :---------------------------------------------------------------------------------------: | :------------------------------------------------------- |\n| `minAvailable`   |   N/A   | Lower than number of [replicas](https://jina.ai/serve/concepts/flow/scale-out/#scale-out)  | Minimum number of replicas available during disruption   |\n| `maxUnavailable` |   N/A   | Lower than numbers of [replicas](https://jina.ai/serve/concepts/flow/scale-out/#scale-out) | Maximum number of replicas unavailable during disruption |\n\n```{code-block} yaml\n---\nemphasize-lines: 5-6\n---\njtype: Flow\nexecutors:\n  - uses: jinaai+docker://<username>/Executor1\n    replicas: 5\n    jcloud:\n      minAvailable: 2\n```\nIn case of disruption, ensure at least two replicas will still be available, while three may be down.\n\n```{code-block} yaml\n---\nemphasize-lines: 5-6\n---\njtype: Flow\nexecutors:\n  - uses: jinaai+docker://<username>/Executor1\n    replicas: 5\n    jcloud:\n      maxUnavailable: 2\n```\nIn case of disruption, ensure that if a maximum of two replicas are down, at least three replicas will still be available.\n\n\n## Configure Gateway\n\nThe Gateway can be customized just like an Executor.\n### Set timeout\n\nBy default, the Gateway will close connections that have been idle for over 600 seconds. If you want a longer connection timeout threshold, change the `timeout` parameter under `gateway.jcloud`.\n\n```{code-block} yaml\n---\nemphasize-lines: 2-4\n---\njtype: Flow\ngateway:\n  jcloud:\n    timeout: 800\nexecutors:\n  - name: executor1\n    uses: jinaai+docker://<username>/Executor1\n```\n\n### Control Gateway resources\n\nTo customize the Gateway's CPU or memory, specify the instance type under `gateway.jcloud.resources`:\n\n```{code-block} yaml\n---\nemphasize-lines: 2-6\n---\njtype: Flow\ngateway:\n  jcloud:\n    resources:\n      instance: C3\nexecutors:\n  - name: encoder\n    uses: jinaai+docker://<username>/Encoder\n```\n\n## Expose Executors\n\nA Flow deployment without a Gateway is often used for {ref}`external-executors`, which can be shared between different Flows. You can expose an Executor by setting `expose: true` (and un-expose the Gateway by setting `expose: false`):\n\n```{code-block} yaml\n---\nemphasize-lines: 2-4, 8-9\n---\njtype: Flow\ngateway:\n  jcloud:\n    expose: false       # don't expose the Gateway\nexecutors:\n  - name: custom\n    uses: jinaai+docker://<username>/CustomExecutor\n    jcloud:\n      expose: true    # expose the Executor\n```\n\n```{figure} img/expose-executor.png\n:width: 70%\n```\n\nYou can expose the Gateway along with Executors:\n\n```{code-block} yaml\n---\nemphasize-lines: 2-4,8-9\n---\njtype: Flow\ngateway:\n  jcloud:\n    expose: true\nexecutors:\n  - name: custom1\n    uses: jinaai+docker://<username>/CustomExecutor1\n    jcloud:\n      expose: true    # expose the Executor\n```\n\n```{figure} img/gateway-and-executors.png\n:width: 70%\n```\n\n## Other deployment options\n\n### Customize Flow name\n\nYou can use the `name` argument to specify the Flow name in the Flow YAML:\n\n```{code-block} yaml\n---\nemphasize-lines: 2-3\n---\njtype: Flow\njcloud:\n  name: my-name\nexecutors:\n  - name: executor1\n    uses: jinaai+docker://<username>/Executor1\n```\n\n### Specify Jina version\n\nTo control Jina's version while deploying a Flow to `jcloud`, you can pass the `version` argument in the Flow YAML:\n\n```{code-block} yaml\n---\nemphasize-lines: 2-3\n---\njtype: Flow\njcloud:\n  version: 3.10.0\nexecutors:\n  - name: executor1\n    uses: jinaai+docker://<username>/Executor1\n```\n\n### Add Labels\n\nYou can use `labels` (as key-value pairs) to attach metadata to your Flows and Executors:\n\nFlow level `labels`:\n```{code-block} yaml\n---\nemphasize-lines: 2-5\n---\njtype: Flow\njcloud:\n  labels: \n    username: johndoe\n    app: fashion-search\nexecutors:\n  - name: executor1\n    uses: jinaai+docker://<username>/Executor1\n```\n\nExecutor level `labels`:\n```{code-block} yaml\n---\nemphasize-lines: 5-8\n---\njtype: Flow\nexecutors:\n  - name: executor1\n    uses: jinaai+docker://<username>/Executor1\n    jcloud:\n      labels:\n        index: partial\n        group: backend\n```\n\n\n```{hint}\n\nKeys in `labels` have the following restrictions:\n  - Must be 63 characters or fewer.\n  - Must begin and end with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (_), dots (.), and alphanumerics between.\n  - The following keys are skipped if passed in the Flow YAML.\n    - `user`\n    - `jina`-version\n```\n\n### Monitoring\n\nTo enable [tracing support](https://jina.ai/serve/cloud-nativeness/opentelemetry/) in Flows, you can pass `enable: true` argument in the Flow YAML. (Tracing support is not enabled by default in JCloud)\n\n```{code-block} yaml\n---\nemphasize-lines: 2-5\n---\njtype: Flow\njcloud:\n  monitor:\n    traces:\n      enable: true\nexecutors:\n  - name: executor1\n    uses: jinaai+docker://<username>/Executor1\n```\n\nYou can pass the `enable: true` argument to `gateway` to only enable tracing support in the Gateway:\n\n```{code-block} yaml\n---\nemphasize-lines: 2-6\n---\njtype: Flow\ngateway:\n  jcloud:\n      monitor:\n        traces:\n          enable: true\nexecutors:\n  - name: executor1\n    uses: jinaai+docker://<username>/Executor1\n```\n\nYou can also only enable tracing support in `executor1`.\n\n```{code-block} yaml\n---\nemphasize-lines: 5-8\n---\njtype: Flow\nexecutors:\n  - name: executor1\n    uses: jinaai+docker://<username>/Executor1\n    jcloud:\n      monitor:\n        traces:\n          enable: true\n```\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/concepts/jcloud/configuration.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.8600000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 510}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(jcloud)=\n# Jina AI Cloud Hosting\n\n\n```{toctree}\n:hidden:\n\nconfiguration\n```\n\n```{figure} https://jina.ai/serve/_images/jcloud-banner.png\n:width: 0 %\n:scale: 0 %\n```\n\n```{figure} img/jcloud-banner.png\n:scale: 0 %\n:width: 0 %\n```\n\nAfter building a Jina-serve project, the next step is to deploy and host it on the cloud. [Jina AI Cloud](https://cloud.jina.ai/) is Jina-serve's reliable, scalable and production-ready cloud-hosting solution that manages your project lifecycle without surprises or hidden development costs.\n\n```{tip}\nAre you ready to unlock the power of AI with Jina AI Cloud? Take a look at our [pricing options](https://cloud.jina.ai/pricing) now!\n```\n\nIn addition to deploying Flows, `jcloud` supports the creation of secrets and jobs which are created in the Flow's namespace. \n\n## Basics\n\nJina AI Cloud provides a CLI that you can use via `jina cloud` from the terminal (or `jcloud` or simply `jc` for minimalists.)\n\n````{hint}\nYou can also install just the JCloud CLI without installing the Jina-serve package.\n\n```bash\npip install jcloud\njc -h\n```\n\nIf you installed the JCloud CLI individually, all of its commands fall under the `jc` or `jcloud` executable.\n\n\nIn case the command `jc` is already occupied by another tool, use `jcloud` instead. If your pip install doesn't register bash commands for you, you can run `python -m jcloud -h`.\n````\n\nFor the rest of this section, we use `jc` or `jcloud`. But again they are interchangeable with `jina cloud`.\n\n## Flows\n\n### Deploy\n\nIn Jina's idiom, a project is a [Flow](https://jina.ai/serve/concepts/orchestration/flow/), which represents an end-to-end task such as indexing, searching or recommending. In this document, we use \"project\" and \"Flow\" interchangeably.\n\nA Flow can have two types of file structure: a single YAML file or a project folder.\n\n#### Single YAML file\n\nA self-contained YAML file, consisting of all configuration at the [Flow](https://jina.ai/serve/concepts/orchestration/flow/)-level and [Executor](https://jina.ai/serve/concepts/serving/executor/)-level.\n\n> All Executors' `uses` must follow the format `jinaai+docker://<username>/MyExecutor` (from [Executor Hub](https://cloud.jina.ai)) to avoid any local file dependencies:\n\n```yaml\n# flow.yml\njtype: Flow\nexecutors:\n  - name: sentencizer\n    uses: jinaai+docker://jina-ai/Sentencizer\n```\n\nTo deploy:\n\n```bash\njc flow deploy flow.yml\n```\n\n````{caution}\nWhen `jcloud` deploys a flow it automatically appends the following global arguments to the `flow.yml`, if not present:\n\n```yaml\njcloud:\n  version: jina-version\n  docarray: docarray-version\n```\n\nThe `jina` and `docarray` corresponds to your development environment's `jina` and `docarray` versions.\n````\n\n````{tip}\nWe recommend testing locally before deployment:\n\n```bash\njina flow --uses flow.yml\n```\n````\n\n#### Project folder\n\n````{tip}\nThe best practice for creating a Jina AI Cloud project is to use:\n\n```bash\njc new\n```\nThis ensures the correct project structure that is accepted by Jina AI Cloud.\n\n````\n\nJust like a regular Python project, you can have sub-folders of Executor implementations and a `flow.yml` on the top-level to connect all Executors together.\n\nYou can create an example local project using `jc new hello`. The default structure looks like:\n\n```\nhello/\n‚îú‚îÄ‚îÄ .env\n‚îú‚îÄ‚îÄ executor1\n‚îÇ   ‚îú‚îÄ‚îÄ config.yml\n‚îÇ   ‚îú‚îÄ‚îÄ executor.py\n‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt\n‚îî‚îÄ‚îÄ flow.yml\n```\n\nWhere:\n\n- `hello/` is your top-level project folder.\n- `executor1` directory has all Executor related code/configuration. You can read the best practices for [file structures](https://jina.ai/serve/concepts/serving/executor/file-structure/). Multiple Executor directories can be created.\n- `flow.yml` Your Flow YAML.\n- `.env` All environment variables used during deployment.\n\nTo deploy:\n\n```bash\njc flow deploy hello\n```\n\nThe Flow is successfully deployed when you see:\n\n```{figure} img/deploy.png\n:width: 70%\n```\n---\n\nYou will get a Flow ID, say `merry-magpie-82b9c0897f`. This ID is required to manage, view logs and remove the Flow.\n\nAs this Flow is deployed with the default gRPC gateway (feel free to change it to `http` or `websocket`), you can use `jina.Client` to access it:\n\n```python\nfrom jina import Client, Document\n\nprint(\n    Client(host='grpcs://merry-magpie-82b9c0897f.wolf.jina.ai').post(\n        on='/', inputs=Document(text='hello')\n    )\n)\n```\n\n(jcloud-flow-status)=\n### Get status\n\nTo get the status of a Flow:\n```bash\njc flow status merry-magpie-82b9c0897f\n```\n\n```{figure} img/status.png\n:width: 70%\n```\n\n### Monitoring\n\nBasic monitoring is provided to Flows deployed on Jina AI Cloud.\n\nTo access the [Grafana](https://grafana.com/)-powered dashboard, first get {ref}`the status of the Flow<jcloud-flow-status>`. The `Grafana Dashboard` link is displayed at the bottom of the pane. Visit the URL to find basic metrics like 'Number of Request Gateway Received' and 'Time elapsed between receiving a request and sending back the response':\n\n```{figure} img/monitoring.png\n:width: 80%\n```\n\n### List Flows\n\nTo list all of your \"Starting\", \"Serving\", \"Failed\", \"Updating\", and \"Paused\" Flows:\n\n```bash\njc flows list\n```\n\n```{figure} img/list.png\n:width: 90%\n```\n\nYou can also filter your Flows by passing a phase:\n\n```bash\njc flows list --phase Deleted\n```\n\n\n```{figure} img/list_deleted.png\n:width: 90%\n```\n\nOr see all Flows:\n\n```bash\njc flows list --phase all\n```\n\n```{figure} img/list_all.png\n:width: 90%\n```\n\n### Remove Flows\nYou can remove a single Flow, multiple Flows or even all Flows by passing different identifiers.\n\nTo remove a single Flow:\n\n```bash\njc flow remove merry-magpie-82b9c0897f\n```\n\nTo remove multiple Flows:\n\n```bash\njc flow remove merry-magpie-82b9c0897f wondrous-kiwi-b02db6a066\n```\n\nTo remove all Flows:\n\n```bash\njc flow remove all\n```\n\nBy default, removing multiple or all Flows is an interactive process where you must give confirmation before each Flow is deleted. To make it non-interactive, set the below environment variable before running the command:\n\n```bash\nexport JCLOUD_NO_INTERACTIVE=1\n```\n\n### Update a Flow\nYou can update a Flow by providing an updated YAML.\n\nTo update a Flow:\n\n```bash\njc flow update super-mustang-c6cf06bc5b flow.yml\n```\n\n```{figure} img/update_flow.png\n:width: 70%\n```\n\n### Pause / Resume Flow\n\nYou have the option to pause a Flow that is not currently in use but may be needed later. This will allow the Flow to be resumed later when it is needed again by using `resume`.\n\nTo pause a Flow:\n\n```bash\njc flow pause super-mustang-c6cf06bc5b\n```\n\n```{figure} img/pause_flow.png\n:width: 70%\n```\n\nTo resume a Flow:\n\n```bash\njc flow resume super-mustang-c6cf06bc5b\n```\n\n```{figure} img/resume_flow.png\n:width: 70%\n```\n\n### Restart Flow, Executor or Gateway\n\nIf you need to restart a Flow, there are two options: restart all Executors and the Gateway associated with the Flow, or selectively restart only a specific Executor or the Gateway.\n\nTo restart a Flow:\n\n```bash\njc flow restart super-mustang-c6cf06bc5b\n```\n\n```{figure} img/restart_flow.png\n:width: 70%\n```\n\nTo restart the Gateway:\n\n```bash\njc flow restart super-mustang-c6cf06bc5b --gateway\n```\n\n```{figure} img/restart_gateway.png\n:width: 70%\n```\n\nTo restart an Executor:\n\n```bash\njc flow restart super-mustang-c6cf06bc5b --executor executor0\n```\n\n```{figure} img/restart_executor.png\n:width: 70%\n```\n\n### Recreate a Deleted Flow\n\nTo recreate a deleted Flow:\n\n```bash\njc flow recreate profound-rooster-eec4b17c73\n```\n\n```{figure} img/recreate_flow.png\n:width:  70%\n```\n\n### Scale an Executor\nYou can also manually scale any Executor.\n\n```bash\njc flow scale good-martin-ca6bfdef84 --executor executor0 --replicas 2\n```\n\n```{figure} img/scale_executor.png\n:width: 70%\n```\n\n### Normalize a Flow\nTo normalize a Flow:\n\n```bash\njc flow normalize flow.yml\n```\n\n```{hint}\nNormalizing a Flow is the process of building the Executor image and pushing the image to Hubble.\n```\n\n### Get Executor or Gateway logs\n\nTo get the Gateway logs:\n\n```bash\njc flow logs --gateway central-escargot-354a796df5\n```\n\n```{figure} img/gateway_logs.png\n:width: 70%\n```\n\nTo get the Executor logs:\n\n```bash\njc flow logs --executor executor0 central-escargot-354a796df5\n```\n\n```{figure} img/executor_logs.png\n:width: 70%\n```\n\n## Secrets\n\n### Create a Secret\n\nTo create a Secret for a Flow:\n\n```bash\njc secret create mysecret rich-husky-af14064067 --from-literal \"{'env-name': 'secret-value'}\"\n```\n\n```{tip}\nYou can optionally pass the `--update` flag to automatically update the Flow spec with the updated secret information. This flag will update the Flow which is hosted on the cloud. Finally, you can also optionally pass a Flow's yaml file path with `--path` to update the yaml file locally.  Refer to [this](https://jina.ai/serve/cloud-nativeness/kubernetes/#deploy-flow-with-custom-environment-variables-and-secrets) section for more information.\n```\n\n```{caution}\nIf the `--update` flag is not passed then you have to manually update the flow with `jc update flow rich-husky-af14064067 updated-flow.yml`\n```\n\n### List Secrets\n\nTo list all the Secrets created in a Flow's namespace:\n\n```bash\njc secret list rich-husky-af14064067\n```\n\n```{figure} img/list_secrets.png\n:width: 90%\n```\n\n### Get a Secret\n\nTo retrieve a Secret's details:\n\n```bash\njc secret get mysecret rich-husky-af14064067\n```\n\n```{figure} img/get_secret.png\n:width: 90%\n```\n\n### Remove Secret\n\n```bash\njc secret remove rich-husky-af14064067 mysecret\n```\n\n### Update a Secret\nYou can update a Secret for a Flow.\n\n```bash\njc secret update rich-husky-af14064067 mysecret --from-literal \"{'env-name': 'secret-value'}\"\n```\n\n```{tip}\nYou can optionally pass the `--update` flag to automatically update the Flow spec with the updated secret information. This flag will update the Flow which is hosted on the cloud. Finally, you can also optionally pass a Flow's yaml file path with `--path` to update the yaml file locally. Refer to [this](https://jina.ai/serve/cloud-nativeness/kubernetes/#deploy-flow-with-custom-environment-variables-and-secrets) section for more information.\n```\n\n```{caution}\nUpdating a Secret automatically restarts a Flow.\n```\n\n## Jobs\n\n### Create a Job\n\nTo create a Job for a Flow:\n\n```bash\njc job create job-name rich-husky-af14064067 image 'job entrypoint' --timeout 600 --backofflimit 2\n```\n\n```{tip}\n`image` can be any Executor image passed to a Flow's Executor `uses` or any normal docker image prefixed with `docker://`\n```\n\n### List Jobs\n\nTo listg all Jobs created in a Flow's namespace:\n\n```bash\njc jobs list rich-husky-af14064067\n```\n\n```{figure} img/list_jobs.png\n:width: 90%\n```\n\n### Get a Job\n\nTo retrieve a Job's details:\n\n```bash\njc job get myjob1 rich-husky-af14064067\n```\n\n```{figure} img/get_job.png\n:width: 90%\n```\n\n### Remove Job\n```bash\njc job remove rich-husky-af14064067 myjob1\n```\n\n\n### Get Job Logs\n\nTo get the Job logs:\n\n```bash\njc job logs myjob1 -f rich-husky-af14064067\n```\n\n```{figure} img/job_logs.png\n:width: 90%\n```\n\n## Deployments\n\n### Deploy\n\n```{caution}\nWhen `jcloud` deploys a deployment it automatically appends the following global arguments to the `deployment.yml`, if not present:\n```\n\n```yaml\njcloud:\n  version: jina-version\n  docarray: docarray-version\n```\n\n#### Single YAML file\n\nA self-contained YAML file, consisting of all configuration information at the [Deployment](https://jina.ai/serve/concepts/orchestration/deployment/)-level and [Executor](https://jina.ai/serve/concepts/serving/executor/)-level.\n\n> A Deployment's `uses` parameter must follow the format `jinaai+docker://<username>/MyExecutor` (from [Executor Hub](https://cloud.jina.ai)) to avoid any local file dependencies:\n\n```yaml\n# deployment.yml\njtype: Deployment\nwith:\n  protocol: grpc\n  uses: jinaai+docker://jina-ai/Sentencizer\n```\n\nTo deploy:\n\n```bash\njc deployment deploy ./deployment.yaml\n```\n\nThe Deployment is successfully deployed when you see:\n\n```{figure} img/deployment/deploy.png\n:width: 70%\n```\n---\n\nYou will get a Deployment ID, for example `pretty-monster-130a5ac952`. This ID is required to manage, view logs, and remove the Deployment.\n\nSince this Deployment is deployed with the default gRPC protocol (feel free to change it to `http`), you can use `jina.Client` to access it:\n\n```python\nfrom jina import Client, Document\n\nprint(\n    Client(host='grpcs://executor-pretty-monster-130a5ac952.wolf.jina.ai').post(\n        on='/', inputs=Document(text='hello')\n    )\n)\n```\n\n(jcloud-deployoment-status)=\n### Get status\n\nTo get the status of a Deployment:\n```bash\njc deployment status pretty-monster-130a5ac952\n```\n\n```{figure} img/deployment/status.png\n:width: 70%\n```\n\n### List Deployments\n\nTo list all of your \"Starting\", \"Serving\", \"Failed\", \"Updating\", and \"Paused\" Deployments:\n\n```bash\njc deployment list\n```\n\n```{figure} img/deployment/list.png\n:width: 90%\n```\n\nYou can also filter your Deployments by passing a phase:\n\n```bash\njc deployment list --phase Deleted\n```\n\n\n```{figure} img/deployment/list_deleted.png\n:width: 90%\n```\n\nOr see all Deployments:\n\n```bash\njc deployment list --phase all\n```\n\n```{figure} img/deployment/list_all.png\n:width: 90%\n```\n\n### Remove Deployments\nYou can remove a single Deployment, multiple Deployments, or even all Deployments by passing different commands to the `jc` executable at the command line.\n\nTo remove a single Deployment:\n\n```bash\njc deployment remove pretty-monster-130a5ac952\n```\n\nTo remove multiple Deployments:\n\n```bash\njc deployment remove pretty-monster-130a5ac952 artistic-tuna-ab154c4dcc\n```\n\nTo remove all Deployments:\n\n```bash\njc deployment remove all\n```\n\nBy default, removing all or multiple Deployments is an interactive process where you must give confirmation before each Deployment is deleted. To make it non-interactive, set the below environment variable before running the command:\n\n```bash\nexport JCLOUD_NO_INTERACTIVE=1\n```\n\n### Update a Deployment\nYou can update a Deployment by providing an updated YAML.\n\nTo update a Deployment:\n\n```bash\njc deployment update pretty-monster-130a5ac952 deployment.yml\n```\n\n```{figure} img/deployment/update.png\n:width: 70%\n```\n\n### Pause / Resume Deployment\n\nYou have the option to pause a Deployment that is not currently in use but may be needed later. This will allow the Deployment to be resumed later when it is needed again by using `resume`.\n\nTo pause a Deployment:\n\n```bash\njc deployment pause pretty-monster-130a5ac952\n```\n\n```{figure} img/deployment/pause.png\n:width: 70%\n```\n\nTo resume a Deployment:\n\n```bash\njc eployment resume pretty-monster-130a5ac952\n```\n\n```{figure} img/deployment/resume.png\n:width: 70%\n```\n\n### Restart Deployment\n\nTo restart a Deployment:\n\n```bash\njc deployment restart pretty-monster-130a5ac952\n```\n\n```{figure} img/deployment/restart.png\n:width: 70%\n```\n\n### Recreate a Deleted Deployment\n\nTo recreate a deleted Deployment:\n\n```bash\njc deployment recreate pretty-monster-130a5ac952\n```\n\n```{figure} img/deployment/recreate.png\n:width:  70%\n```\n\n### Scale a Deployment\nYou can also manually scale any Deployment.\n\n```bash\njc deployment scale pretty-monster-130a5ac952 --replicas 2\n```\n\n```{figure} img/deployment/scale.png\n:width: 70%\n```\n\n### Get Deployment logs\n\nTo get the Deployment logs:\n\n```bash\njc deployment logs pretty-monster-130a5ac952\n```\n\n```{figure} img/deployment/logs.png\n:width: 70%\n```\n\n## Configuration\n\nPlease refer to {ref}`Configuration <jcloud-configuration>` for configuring the Flow on Jina AI Cloud.\n\n## Restrictions\n\nJina AI Cloud scales according to your needs. You can demand different instance types with GPU/memory/CPU predefined based on the needs of your Flows and Executors. If you have specific resource requirements, please contact us [on Discord](https://discord.jina.ai) or raise a [GitHub issue](https://github.com/jina-ai/jcloud/issues/new/choose).\n\n\n```{admonition} Restrictions\n  \n- Deployments are only supported in the `us-east` region.\n```\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/concepts/jcloud/index.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.89, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 709}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(deployment)=\n# Deployment\n\n```{important}\nA Deployment is part of the orchestration layer {ref}`Orchestration <orchestration>`. Be sure to read up on that too!\n```\n\nA {class}`~jina.Deployment` orchestrates a single {class}`~jina.Executor` to accomplish a task. Documents are processed by Executors.\n\nYou can think of a Deployment as an interface to configure and launch your {ref}`microservice architecture <architecture-overview>`, while the heavy lifting is done by the {ref}`service <executor-cookbook>` itself.\n\n(why-deployment)=\n## Why use a Deployment?\n\nOnce you've learned about Documents, DocLists and Executors, you can split a big task into small independent modules and services.\n\n- Deployments let you scale these Executors independently to match your requirements.\n- Deployments let you easily use other cloud-native orchestrators, such as Kubernetes, to manage your service.\n\n(create-deployment)=\n## Create\n\nThe most trivial {class}`~jina.Deployment` is an empty one. It can be defined in Python or from a YAML file:\n\n````{tab} Python\n```python\nfrom jina import Deployment\n\ndep = Deployment()\n```\n````\n````{tab} YAML\n```yaml\njtype: Deployment\n```\n````\n\nFor production, you should define your Deployments with YAML. This is because YAML files are independent of the Python logic code and easier to maintain.\n\n\n## Minimum working example\n\n````{tab} Pythonic style\n\n\n```python\nfrom jina import Deployment, Executor, requests\nfrom docarray import DocList, BaseDoc\n\n\nclass MyExecutor(Executor):\n    @requests(on='/bar')\n    def foo(self, docs: DocList[BaseDoc], **kwargs) -> DocList[BaseDoc]:\n        print(docs)\n\n\ndep = Deployment(name='myexec1', uses=MyExecutor)\n\nwith dep:\n    dep.post(on='/bar', inputs=BaseDoc(), return_type=DocList[BaseDoc], on_done=print)\n```\n\n\n````\n\n````{tab} Deployment-as-a-Service style\n\nServer:\n\n```python\nfrom jina import Deployment, Executor, requests\nfrom docarray import DocList, BaseDoc\n\n\nclass MyExecutor(Executor):\n    @requests(on='/bar')\n    def foo(self, docs: DocList[BaseDoc], **kwargs) -> DocList[BaseDoc]:\n        print(docs)\n\n\ndep = Deployment(port=12345, name='myexec1', uses=MyExecutor)\n\nwith dep:\n    dep.block()\n```\n\nClient:\n\n```python\nfrom jina import Client\nfrom docarray import DocList, BaseDoc\n\nc = Client(port=12345)\nc.post(on='/bar', inputs=BaseDoc(), return_type=DocList[BaseDoc], on_done=print)\n```\n\n````\n\n````{tab} Load from YAML\n\n`deployment.yml`:\n```yaml\njtype: Deployment\nname: myexec1\nuses: FooExecutor\npy_modules: exec.py\n```\n\n`exec.py`:\n```python\nfrom jina import Deployment, Executor, requests\nfrom docarray import DocList, BaseDoc\nfrom docarray.documents import TextDoc\n \nclass FooExecutor(Executor):\n    @requests\n    def foo(self, docs: DocList[TextDoc], **kwargs) -> DocList[TextDoc]:\n        for doc in docs:\n            doc.text = 'foo was here'\n        docs.summary()\n        return docs\n```\n\n```python\nfrom jina import Deployment\nfrom docarray import DocList, BaseDoc\nfrom docarray.documents import TextDoc\n\ndep = Deployment.load_config('deployment.yml')\n\nwith dep:\n    try:\n        dep.post(on='/bar', inputs=TextDoc(), on_done=print)\n    except Exception as ex:\n        # handle exception\n        pass\n```\n\n````\n\n```{caution}\nThe statement `with dep:` starts the Deployment, and exiting the indented with block stops the Deployment, including its Executors.\nExceptions raised inside the `with dep:` block will close the Deployment context manager. If you don't want this, use a `try...except` block to surround the statements that could potentially raise an exception.\n```\n\n## Convert between Python and YAML\n\nA Python Deployment definition can easily be converted to/from a YAML definition:\n\n````{tab} Load from YAML\n```python\nfrom jina import Deployment\n\ndep = Deployment.load_config('flow.yml')\n```\n````\n````{tab} Export to YAML\n```python\nfrom jina import Deployment\n\ndep = Deployment()\n\ndep.save_config('deployment.yml')\n```\n````\n\n## Start and stop\n\nWhen a {class}`~jina.Deployment` starts, all the replicated Executors will start as well, making it possible to {ref}`reach the service through its API <third-party-client>`.\n\nThere are three ways to start a Deployment: In Python, from a YAML file, or from the terminal.\n\n- Generally in Python: use Deployment as a context manager.\n- As an entrypoint from terminal: use `Jina CLI <cli>` and a Deployment YAML file.\n- As an entrypoint from Python code: use Deployment as a context manager inside `if __name__ == '__main__'`\n- No context manager, manually call {meth}`~jina.Deployment.start` and {meth}`~jina.Deployment.close`.\n\n````{tab} General in Python\n```python\nfrom jina import Deployment\n\ndep = Deployment()\n\nwith dep:\n    pass\n```\nThe statement `with dep:` starts the Deployment, and exiting the indented `with` block stops the Deployment, including its Executor.\n````\n\n````{tab} Jina-serve CLI entrypoint\n```bash\njina deployment --uses deployment.yml\n```\n````\n\n````{tab} Python entrypoint\n```python\nfrom jina import Deployment\n\ndep = Deployment()\n\nif __name__ == '__main__':\n    with dep:\n        pass\n```\nThe statement `with dep:` starts the Deployment, and exiting the indented `with` block stops the Deployment, including its Executor.\n````\n\n````{tab} Python no context manager\n```python\nfrom jina import Deployment\n\ndep = Deployment()\n\ndep.start()\n\ndep.close()\n```\n````\n\nYour addresses and entrypoints can be found in the output. When you enable more features such as monitoring, HTTP gateway, TLS encryption, this display expands to contain more information.\n\n(multiprocessing-spawn)=\n### Set multiprocessing `spawn` \n\nSome corner cases require forcing a `spawn` start method for multiprocessing, for example if you encounter \"Cannot re-initialize CUDA in forked subprocess\". \n\nYou can use `JINA_MP_START_METHOD=spawn` before starting the Python script to enable this.\n\n```bash\nJINA_MP_START_METHOD=spawn python app.py\n```\n\n```{caution}\nIn case you set `JINA_MP_START_METHOD=spawn`, make sure to use Flow as a context manager inside `if __name__ == '__main__'`.\nThe script entrypoint (starting the flow) [needs to be protected when using `spawn` start method](https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods). \n```\n\n````{hint}\nThere's no need to set this for Windows, as it only supports spawn method for multiprocessing. \n````\n\n## Serve\n\n### Serve forever\n\nIn most scenarios, a Deployment should remain reachable for prolonged periods of time. This can be achieved from the terminal:\n\n````{tab} Python\n```python\nfrom jina import Deployment\n\ndep = Deployment()\n\nwith dep:\n    dep.block()\n````\n````{tab} YAML\n```shell\njina-serve deployment --uses deployment.yml\n```\n````\n\nThe `.block()` method blocks the execution of the current thread or process, enabling external clients to access the Deployment.\n\nIn this case, the Deployment can be stopped by interrupting the thread or process. \n\n### Serve until an event\n\nAlternatively, a `multiprocessing` or `threading` `Event` object can be passed to `.block()`, which stops the Deployment once set.\n\n```python\nfrom jina import Deployment\nimport threading\n\n\ndef start_deployment(stop_event):\n    \"\"\"start a blocking Deployment.\"\"\"\n    dep = Deployment()\n    \n    with dep:\n        dep.block(stop_event=stop_event)\n\n\ne = threading.Event()  # create new Event\n\nt = threading.Thread(name='Blocked-Flow', target=start_flow, args=(e,))\nt.start()  # start Deployment in new Thread\n\n# do some stuff\n\ne.set()  # set event and stop (unblock) the Deployment\n```\n\n## Export\n\nA Deployment YAML can be exported as a Docker Compose YAML or Kubernetes YAML bundle. \n\n(docker-compose-export)=\n### Docker Compose\n\n````{tab} Python\n```python\nfrom jina import Deployment\n\ndep = Deployment()\ndep.to_docker_compose_yaml()\n```\n````\n````{tab} Terminal\n```shell\njina-serve export docker-compose deployment.yml docker-compose.yml \n```\n````\n\nThis will generate a single `docker-compose.yml` file.\n\nFor advanced utilization of Docker Compose with Jina-serve, refer to {ref}`How to <docker-compose>` \n\n(deployment-kubernetes-export)=\n### Kubernetes\n\n````{tab} Python\n```python\nfrom jina import Deployment\n\ndep = Deployment\ndep.to_kubernetes_yaml('dep_k8s_configuration')\n```\n````\n````{tab} Terminal\n```shell\njina-serve export kubernetes deployment.yml ./my-k8s \n```\n````\n\nThe generated folder can be used directly with `kubectl` to deploy the Deployment to an existing Kubernetes cluster.\n\nFor advanced utilisation of Kubernetes with Jina-serve please refer to {ref}`How to <kubernetes>` \n\n```{tip}\nBased on your local Jina version, Executor Hub may rebuild the Docker image during the YAML generation process.\nIf you do not wish to rebuild the image, set the environment variable `JINA_HUB_NO_IMAGE_REBUILD`.\n```\n\n```{tip}\nIf an Executor requires volumes to be mapped to persist data, Jina will create a StatefulSet for that Executor instead of a Deployment.\nYou can control the access mode, storage class name and capacity of the attached Persistent Volume Claim by using {ref}`Jina environment variables <jina-serve-env-vars>`  \n`JINA_K8S_ACCESS_MODES`, `JINA_K8S_STORAGE_CLASS_NAME` and `JINA_K8S_STORAGE_CAPACITY`. Only the first volume will be considered to be mounted.\n```\n\n```{admonition} See also\n:class: seealso\nFor more in-depth guides on deployment, check our how-tos for {ref}`Docker compose <docker-compose>` and {ref}`Kubernetes <kubernetes>`.\n```\n\n```{caution}\nThe port or ports arguments are ignored when calling the Kubernetes YAML, Jina-serve will start the services binding to the ports 8080, except when multiple protocols\nneed to be served when the consecutive ports (8081, ...) will be used. This is because the Kubernetes service will direct the traffic from you and it is irrelevant\nto the services around because in Kubernetes services communicate via the service names irrespective of the internal port.\n```\n\n(logging-configuration)=\n## Logging\n\nThe default {class}`jina.logging.logger.JinaLogger` uses rich console logging that writes to the system console. The `log_config` argument can be used to pass in a string of the pre-configured logging configuration names in Jina-serve or the absolute YAML file path of the custom logging configuration. For most cases, the default logging configuration sufficiently covers local, Docker and Kubernetes environments.\n\nCustom logging handlers can be configured by following the Python official [Logging Cookbook](https://docs.python.org/3/howto/logging-cookbook.html#logging-cookbook) examples. An example custom logging configuration file defined in a YAML file `logging.json.yml` is:\n\n```yaml\nhandlers:\n  - StreamHandler\nlevel: INFO\nconfigs:\n  StreamHandler:\n    format: '%(asctime)s:{name:>15}@%(process)2d[%(levelname).1s]:%(message)s'\n    formatter: JsonFormatter\n```\n\nThe logging configuration can be used as follows:\n\n````{tab} Python\n```python\nfrom jina import Deployment\n\ndep = Deployment(log_config='./logging.json.yml')\n```\n````\n\n````{tab} YAML\n```yaml\njtype: Deployment\nwith:\n    log_config: './logging.json.yml'\n```\n````\n\n### Supported protocols\n\nA Deployment can be used to deploy an Executor and serve it using `gRPC` or `HTTP` protocol, or a composition of them. \n\n### gRPC protocol\n\ngRPC is the default protocol used by a Deployment to expose Executors to the outside world, and is used to communicate between the Gateway and an Executor inside a Flow.\n\n### HTTP protocol\n\nHTTP can be used for a stand-alone Deployment (without being part of a Flow), which allows external services to connect via REST. \n\n```python\nfrom jina import Deployment, Executor, requests\nfrom docarray import DocList\nfrom docarray.documents import TextDoc\n \nclass MyExec(Executor):\n    @requests\n    def foo(self, docs: DocList[TextDoc], **kwargs) -> DocList[TextDoc]:\n        for doc in docs:\n            doc.text = 'foo was here'\n        docs.summary()\n        return docs\n\ndep = Deployment(protocol='http', port=12345, uses=MyExec)\n\nwith dep:\n    dep.block()\n````\n\nThis will make it available at port 12345 and you can get the [OpenAPI schema](https://swagger.io/specification/) for the service.\n\n```{figure} images/http-deployment-swagger.png\n:scale: 70%\n```\n\n### Composite protocol\n\nA Deployment can also deploy an Executor and serve it with a combination of gRPC and HTTP protocols.\n\n```python\nfrom jina import Deployment, Executor, requests\nfrom docarray import DocList\nfrom docarray.documents import TextDoc\n \nclass MyExec(Executor):\n    @requests\n    def foo(self, docs: DocList[TextDoc], **kwargs) -> DocList[TextDoc]:\n        for doc in docs:\n            doc.text = 'foo was here'\n        docs.summary()\n        return docs\n\n\ndep = Deployment(protocol=['grpc', 'http'], port=[12345, 12346], uses=MyExec)\n\nwith dep:\n    dep.block()\n````\n\nThis will make the Deployment reachable via gRPC and HTTP simultaneously.\n\n## Methods\n\nThe most important methods of the `Deployment` object are the following:\n\n| Method                                                       | Description                                                                                                                                                                                                                                                                          |\n|--------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| {meth}`~jina.Deployment.start()`                                   | Starts the Deployment. This will start all its Executors and check if they are ready to be used.                                                                                                                                                                                           |\n| {meth}`~jina.Deployment.close()`                                   | Stops and closes the Deployment. This will stop and shutdown all its Executors.                                                                                                                                                                                                            |\n| `with` context manager                                       | Uses the Deployment as a context manager. It will automatically start and stop your Deployment.                                                                                                                                                                                                   |                                                                |\n| {meth}`~jina.clients.mixin.PostMixin.post()`                 | Sends requests to the Deployment API.                                                                                                                                                                                                                                                      |\n| {meth}`~jina.Deployment.block()`                                   | Blocks execution until the program is terminated. This is useful to keep the Deployment alive so it can be used from other places (clients, etc).                                                                                                                                          |\n| {meth}`~jina.Deployment.to_docker_compose_yaml()`                  | Generates a Docker-Compose file listing all Executors as services.                                                                                                                                                                                                                                                |\n| {meth}`~jina.Deployment.to_kubernetes_yaml()`                      | Generates Kubernetes configuration files in `<output_directory>`. Based on your local Jina-serve version, Executor Hub may rebuild the Docker image during the YAML generation process. If you do not wish to rebuild the image, set the environment variable `JINA_HUB_NO_IMAGE_REBUILD`.                                                                                                                                   |\n| {meth}`~jina.clients.mixin.HealthCheckMixin.is_deployment_ready()` | Check if the Deployment is ready to process requests. Returns a boolean indicating the readiness.                                                                                                                                                                                                                                                                                                                                 |\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/concepts/orchestration/deployment.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 1.0, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 475}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "(flow-cookbook)=\n# Flow\n\n```{important}\nA Flow is a set of {ref}`Deployments <deployment>`. Be sure to read up on those before diving more deeply into Flows!\n```\n\nA {class}`~jina.Flow` orchestrates {class}`~jina.Executor`s into a processing pipeline to accomplish a task. Documents \"flow\" through the pipeline and are processed by Executors.\n\nYou can think of Flow as an interface to configure and launch your {ref}`microservice architecture <architecture-overview>`, while the heavy lifting is done by the {ref}`services <executor-cookbook>` themselves. In particular, each Flow also launches a {ref}`Gateway <gateway>` service, which can expose all other services through an API that you define.\n\n## Why use a Flow?\n\nOnce you've learned about  Documents, DocList and Executor,, you can split a big task into small independent modules and services.\nBut you need to chain them together to create, build ,and serve an application. Flows enable you to do exactly this.\n\n- Flows connect microservices (Executors) to build a service with proper client/server style interfaces over HTTP, gRPC, or WebSockets.\n- Flows let you scale these Executors independently to match your requirements.\n- Flows let you easily use other cloud-native orchestrators, such as Kubernetes, to manage your service.\n\n(create-flow)=\n## Create\n\nThe most trivial {class}`~jina.Flow` is an empty one. It can be defined in Python or from a YAML file:\n\n````{tab} Python\n```python\nfrom jina import Flow\n\nf = Flow()\n```\n````\n````{tab} YAML\n```yaml\njtype: Flow\n```\n````\n\n```{important}\nAll arguments received by {class}`~jina.Flow()` API will be propagated to other entities (Gateway, Executor) with the following exceptions:\n\n- `uses` and `uses_with` won't be passed to Gateway\n- `port`, `port_monitoring`, `uses` and `uses_with` won't be passed to Executor\n```\n\n```{tip}\nAn empty Flow contains only {ref}`the Gateway<gateway>`.\n```\n\n```{figure} images/zero-flow.svg\n:scale: 70%\n```\n\nFor production, you should define your Flows with YAML. This is because YAML files are independent of the Python logic code and easier to maintain.\n\n## Minimum working example\n\n````{tab} Pythonic style\n\n\n```python\nfrom jina import Flow, Executor, requests\nfrom docarray import DocList, BaseDoc\n\n\nclass MyExecutor(Executor):\n    @requests(on='/bar')\n    def foo(self, docs: DocList[BaseDoc], **kwargs) -> DocList[BaseDoc]:\n        print(docs)\n\n\nf = Flow().add(name='myexec1', uses=MyExecutor)\n\nwith f:\n    f.post(on='/bar', inputs=BaseDoc(), return_type=DocList[BaseDoc], on_done=print)\n```\n\n\n````\n\n````{tab} Flow-as-a-Service style\n\nServer:\n\n```python\nfrom jina import Flow, Executor, requests\nfrom docarray import DocList, BaseDoc\n\n\nclass MyExecutor(Executor):\n    @requests(on='/bar')\n    def foo(self, docs: DocList[BaseDoc], **kwargs) -> DocList[BaseDoc]:\n        print(docs)\n\nf = Flow(port=12345).add(name='myexec1', uses=MyExecutor)\n\nwith f:\n    f.block()\n```\n\nClient:\n\n```python\nfrom jina import Client, Document\n\nc = Client(port=12345)\nc.post(on='/bar', inputs=BaseDoc(), return_type=DocList[BaseDoc], on_done=print)\n```\n\n````\n\n````{tab} Load from YAML\n\n`my.yml`:\n```yaml\njtype: Flow\nexecutors:\n  - name: myexec1\n    uses: FooExecutor\n    py_modules: exec.py\n```\n\n`exec.py`:\n```python\nfrom jina import Deployment, Executor, requests\nfrom docarray import DocList, BaseDoc\nfrom docarray.documents import TextDoc\n \nclass FooExecutor(Executor):\n    @requests\n    def foo(self, docs: DocList[TextDoc], **kwargs) -> DocList[TextDoc]:\n        for doc in docs:\n            doc.text = 'foo was here'\n        docs.summary()\n        return docs\n```\n\n```python\nfrom jina import Flow\nfrom docarray import DocList, BaseDoc\nfrom docarray.documents import TextDoc\n\nf = Flow.load_config('my.yml')\n\nwith f:\n    try:\n        f.post(on='/bar', inputs=TextDoc(), on_done=print)\n    except Exception as ex:\n        # handle exception\n        pass\n```\n\n````\n\n```{caution}\nThe statement `with f:` starts the Flow, and exiting the indented with block stops the Flow, including all Executors defined in it.\nExceptions raised inside the `with f:` block will close the Flow context manager. If you don't want this, use a `try...except` block to surround the statements that could potentially raise an exception.\n```\n\n## Start and stop\n\nWhen a {class}`~jina.Flow` starts, all included Executors (single for a Deployment, multiple for a Flow) will start as well, making it possible to {ref}`reach the service through its API <third-party-client>`.\n\nThere are three ways to start an Flow: In Python, from a YAML file, or from the terminal.\n\n- Generally in Python: use Deployment or Flow as a context manager in Python.\n- As an entrypoint from terminal: use `Jina CLI <cli>` and a Flow YAML file.\n- As an entrypoint from Python code: use Flow as a context manager inside `if __name__ == '__main__'`\n- No context manager: manually call {meth}`~jina.Flow.start` and {meth}`~jina.Flow.close`.\n\n````{tab} General in Python\n```python\nfrom jina import Flow\n\nf = Flow()\n\nwith f:\n    pass\n```\n````\n\n````{tab} Jina-serve CLI entrypoint\n```bash\njina flow --uses flow.yml\n```\n````\n\n````{tab} Python entrypoint\n```python\nfrom jina import Flow\n\nf = Flow()\n\nif __name__ == '__main__':\n    with f:\n        pass\n```\n````\n\n````{tab} Python no context manager\n```python\nfrom jina import Flow\n\nf = Flow()\n\nf.start()\n\nf.close()\n```\n````\n\nThe statement `with f:` starts the Flow, and exiting the indented `with` block stops the Flow, including all its Executors.\n\nA successful start of a Flow looks like this:\n\n```{figure} images/success-flow.png\n:scale: 70%\n```\n\nYour addresses and entrypoints can be found in the output. When you enable more features such as monitoring, HTTP gateway, TLS encryption, this display expands to contain more information.\n\n```{admonition} Multiprocessing spawn\n:class: warning\n\nSome corner cases require forcing a `spawn` start method for multiprocessing, for example if you encounter \"Cannot re-initialize CUDA in forked subprocess\". Read {ref}`more in the docs <multiprocessing-spawn>`\n```\n\n## Serve\n\n### Serve forever\n\nIn most scenarios, a Flow should remain reachable for prolonged periods of time. This can be achieved from Python or the terminal:\n\n````{tab} Python\n```python\nfrom jina import Flow\n\nf = Flow()\n\nwith f:\n    f.block()\n```\nThe `.block()` method blocks the execution of the current thread or process, enabling external clients to access the Flow.\n````\n````{tab} Terminal\n```shell\njina flow --uses flow.yml\n```\n````\n\nIn this case, the Flow can be stopped by interrupting the thread or process. \n\n### Serve until an event\n\nAlternatively, a `multiprocessing` or `threading` `Event` object can be passed to `.block()`, which stops the Flow once set.\n\n```python\nfrom jina import Flow\nimport threading\n\n\ndef start_flow(stop_event):\n    \"\"\"start a blocking Flow.\"\"\"\n    f = Flow()\n\n    with f:\n        f.block(stop_event=stop_event)\n\n\ne = threading.Event()  # create new Event\n\nt = threading.Thread(name='Blocked-Flow', target=start_flow, args=(e,))\nt.start()  # start Flow in new Thread\n\n# do some stuff\n\ne.set()  # set event and stop (unblock) the Flow\n```\n\n### Serve on Google Colab\n\n```{admonition} Example built with docarray<0.30\n:class: note\n\nThis example is built using docarray<0.30 version. Most of the concepts are similar, but some APIs of how Executors are built change when using newer docarray version.\n```\n\n[Google Colab](https://colab.research.google.com/) provides an easy-to-use Jupyter notebook environment with GPU/TPU support. Flows are fully compatible with Google Colab and you can use it in the following ways:\n\n```{figure} images/jina-on-colab.svg\n:align: center\n:width: 70%\n```\n\n```{button-link} https://colab.research.google.com/github/jina-ai/jina/blob/master/docs/Using_Jina_on_Colab.ipynb\n:color: primary\n:align: center\n\n{octicon}`link-external` Open the notebook on Google Colab \n```\n\nPlease follow the walkthrough and enjoy the free GPU/TPU!\n\n\n```{tip}\nHosing services on Google Colab is not recommended if your server aims to be long-lived or permanent. It is often used for quick experiments, demonstrations or leveraging its free GPU/TPU. For stable, secure and free hosting of your Flow, check out [JCloud](https://jina.ai/serve/concepts/jcloud/).\n```\n\n## Export\n\nA Flow YAML can be exported as a Docker Compose YAML or Kubernetes YAML bundle. \n\n(docker-compose-export)=\n### Docker Compose\n\n````{tab} Python\n```python\nfrom jina import Flow\n\nf = Flow().add()\nf.to_docker_compose_yaml()\n```\n````\n````{tab} Terminal\n```shell\njina export docker-compose flow.yml docker-compose.yml \n```\n````\n\nThis will generate a single `docker-compose.yml` file.\n\nFor advanced utilization of Docker Compose with Jina, refer to {ref}`How to <docker-compose>` \n\n(flow-kubernetes-export)=\n### Kubernetes\n\n````{tab} Python\n```python\nfrom jina import Flow\n\nf = Flow().add()\nf.to_kubernetes_yaml('flow_k8s_configuration')\n```\n````\n````{tab} Terminal\n```shell\njina export kubernetes flow.yml ./my-k8s \n```\n````\n\nThe generated folder can be used directly with `kubectl` to deploy the Flow to an existing Kubernetes cluster.\n\nFor advanced utilisation of Kubernetes with Jina please refer to {ref}`How to <kubernetes>` \n\n```{tip}\nBased on your local Jina-serve version, Executor Hub may rebuild the Docker image during the YAML generation process.\nIf you do not wish to rebuild the image, set the environment variable `JINA_HUB_NO_IMAGE_REBUILD`.\n```\n\n```{tip}\nIf an Executor requires volumes to be mapped to persist data, Jina-serve will create a StatefulSet for that Executor instead of a Deployment.\nYou can control the access mode, storage class name and capacity of the attached Persistent Volume Claim by using {ref}`Jina environment variables <jina-env-vars>`  \n`JINA_K8S_ACCESS_MODES`, `JINA_K8S_STORAGE_CLASS_NAME` and `JINA_K8S_STORAGE_CAPACITY`. Only the first volume will be considered to be mounted.\n```\n\n```{admonition} See also\n:class: seealso\nFor more in-depth guides on Flow deployment, check our how-tos for {ref}`Docker compose <docker-compose>` and {ref}`Kubernetes <kubernetes>`.\n```\n\n```{caution}\nThe port or ports arguments are ignored when calling the Kubernetes YAML, Jina will start the services binding to the ports 8080, except when multiple protocols\nneed to be served when the consecutive ports (8081, ...) will be used. This is because the Kubernetes service will direct the traffic from you and it is irrelevant\nto the services around because in Kubernetes services communicate via the service names irrespective of the internal port.\n```\n\n\n## Add Executors\n\n```{important}\nThis section is for Flow-specific considerations when working with Executors. Check more information on {ref}`working with Executors <add-executors>`.\n```\n\nA {class}`~jina.Flow` orchestrates its {class}`~jina.Executor`s as a graph and sends requests to all Executors in the order specified by {meth}`~jina.Flow.add` or listed in {ref}`a YAML file<flow-yaml-spec>`. \n\nWhen you start a Flow, Executors always run in **separate processes**. Multiple Executors run in **different processes**. Multiprocessing is the lowest level of separation when you run a Flow locally. When running a Flow on Kubernetes, Docker Swarm, {ref}`jcloud`, different Executors run in different containers, pods or instances.   \n\nExecutors can be added into a Flow with {meth}`~jina.Flow.add`.  \n\n```python\nfrom jina import Flow\n\nf = Flow().add()\n```\n\nThis adds an \"empty\" Executor called {class}`~jina.serve.executors.BaseExecutor` to the Flow. This Executor (without any parameters) performs no actions.\n\n```{figure} images/no-op-flow.svg\n:scale: 70%\n```\n\nTo more easily identify an Executor, you can change its name by passing the `name` parameter:\n\n```python\nfrom jina import Flow\n\nf = Flow().add(name='myVeryFirstExecutor').add(name='secondIsBest')\n```\n\n\n```{figure} images/named-flow.svg\n:scale: 70%\n```\n\nYou can also define the above Flow in YAML:\n\n```yaml\njtype: Flow\nexecutors:\n  - name: myVeryFirstExecutor\n  - name: secondIsBest\n```\n\nSave it as `flow.yml` and run it: \n\n```bash\njina flow --uses flow.yml\n```\n\nMore Flow YAML specifications can be found in {ref}`Flow YAML Specification<flow-yaml-spec>`.\n\n### How Executors process Documents in a Flow\n\nLet's understand how Executors process Documents's inside a Flow, and how changes are chained and applied, affecting downstream Executors in the Flow.\n\n```python \nfrom jina import Executor, requests, Flow\nfrom docarray import DocList, BaseDoc\nfrom docarray.documents import TextDoc\n\n\nclass PrintDocuments(Executor):\n    @requests\n    def foo(self, docs: DocList[TextDoc], **kwargs) -> DocList[TextDoc]:\n        for doc in docs:\n            print(f' PrintExecutor: received document with text: \"{doc.text}\"')\n        return docs\n\n\nclass ProcessDocuments(Executor):\n    @requests(on='/change_in_place')\n    def in_place(self, docs: DocList[TextDoc], **kwargs) -> DocList[TextDoc]:\n        # This Executor only works on `docs` and doesn't consider any other arguments\n        for doc in docs:\n            print(f'ProcessDocuments: received document with text \"{doc.text}\"')\n            doc.text = 'I changed the executor in place'\n\n    @requests(on='/return_different_docarray')\n    def ret_docs(self, docs: DocList[TextDoc], **kwargs) -> DocList[TextDoc]:\n        # This executor only works on `docs` and doesn't consider any other arguments\n        ret = DocList[TextDoc]()\n        for doc in docs:\n            print(f'ProcessDocuments: received document with text: \"{doc.text}\"')\n            ret.append(TextDoc(text='I returned a different Document'))\n        return ret\n\n\nf = Flow().add(uses=ProcessDocuments).add(uses=PrintDocuments)\n\nwith f:\n    f.post(on='/change_in_place', inputs=DocList[TextDoc]([TextDoc(text='request1')]), return_type=DocList[TextDoc])\n    f.post(\n        on='/return_different_docarray', inputs=DocList[TextDoc]([TextDoc(text='request2')]), return_type=DocList[TextDoc]))\n    )\n```\n\n\n```shell\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üéâ Flow is ready to serve! ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üîó Endpoint ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ  ‚õì     Protocol                    GRPC  ‚îÇ\n‚îÇ  üè†       Local           0.0.0.0:58746  ‚îÇ\n‚îÇ  üîí     Private     192.168.1.187:58746  ‚îÇ\n‚îÇ  üåç      Public    212.231.186.65:58746  ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n ProcessDocuments: received document with text \"request1\"\n PrintExecutor: received document with text: \"I changed the executor in place\"\n ProcessDocuments: received document with text: \"request2\"\n PrintExecutor: received document with text: \"I returned a different Document\"\n```\n\n### Define topologies over Executors\n\n{class}`~jina.Flow`s are not restricted to sequential execution. Internally they are modeled as graphs, so they can represent any complex, non-cyclic topology.\n\nA typical use case for such a Flow is a topology with a common pre-processing part, but different indexers separating embeddings and data.\n\nTo define a custom topology you can use the `needs` keyword when adding an {class}`~jina.Executor`. By default, a Flow assumes that every Executor needs the previously added Executor.\n\n```python\nfrom jina import Executor, requests, Flow\nfrom docarray import DocList\nfrom docarray.documents import TextDoc\n\n\nclass FooExecutor(Executor):\n    @requests\n    async def foo(self, docs: DocList[TextDoc], **kwargs) -> DocList[TextDoc]:\n        docs.append(TextDoc(text=f'foo was here and got {len(docs)} document'))\n\n\nclass BarExecutor(Executor):\n    @requests\n    async def bar(self, docs: DocList[TextDoc], **kwargs) -> DocList[TextDoc]:\n        docs.append(TextDoc(text=f'bar was here and got {len(docs)} document'))\n\n\nclass BazExecutor(Executor):\n    @requests\n    async def baz(self, docs: DocList[TextDoc], **kwargs) -> DocList[TextDoc]:\n        docs.append(TextDoc(text=f'baz was here and got {len(docs)} document'))\n\n\nclass MergeExecutor(Executor):\n    @requests\n    async def merge(self, docs: DocList[TextDoc], **kwargs) -> DocList[TextDoc]:\n        return docs\n\n\nf = (\n    Flow()\n    .add(uses=FooExecutor, name='fooExecutor')\n    .add(uses=BarExecutor, name='barExecutor', needs='fooExecutor')\n    .add(uses=BazExecutor, name='bazExecutor', needs='fooExecutor')\n    .add(uses=MergeExecutor, needs=['barExecutor', 'bazExecutor'])\n)\n```\n\n```{figure} images/needs-flow.svg\n:width: 70%\n:align: center\nComplex Flow where one Executor requires two Executors to process Documents beforehand\n```\n\nWhen sending message to this Flow,\n\n```python\nwith f:\n    print(f.post('/', return_type=DocList[TextDoc]).text)\n```\n\nThis gives the output:\n\n```text\n['foo was here and got 0 document', 'bar was here and got 1 document', 'baz was here and got 1 document']\n```\n\nBoth `BarExecutor` and `BazExecutor` only received a single `Document` from `FooExecutor` because they are run in parallel. The last Executor `executor3` receives both DocLists and merges them automatically.\nThis automated merging can be disabled with `no_reduce=True`. This is useful for providing custom merge logic in a separate Executor. In this case the last `.add()` call would look like `.add(needs=['barExecutor', 'bazExecutor'], uses=CustomMergeExecutor, no_reduce=True)`. This feature requires Jina >= 3.0.2.\n\n## Chain Executors in Flow with different schemas\n\nWhen using `docarray>=0.30.0`, when building a Flow you should ensure that the Document types used as input of an Executor match the schema \nof the output of its incoming previous Flow.\n\nFor instance, this Flow will fail to start because the Document types are wrongly chained.\n\n````{tab} Valid Flow\n```{code-block} python\nfrom jina import Executor, requests, Flow\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\nimport numpy as np\n\n\nclass SimpleStrDoc(BaseDoc):\n    text: str\n\nclass TextWithEmbedding(SimpleStrDoc):\n    embedding: NdArray\n\nclass TextEmbeddingExecutor(Executor):\n    @requests(on='/foo')\n    def foo(docs: DocList[SimpleStrDoc], **kwargs) -> DocList[TextWithEmbedding]\n        ret = DocList[TextWithEmbedding]()\n        for doc in docs:\n            ret.append(TextWithEmbedding(text=doc.text, embedding=np.ramdom.rand(10))\n        return ret\n\nclass ProcessEmbedding(Executor):\n    @requests(on='/foo')\n    def foo(docs: DocList[TextWithEmbedding], **kwargs) -> DocList[TextWithEmbedding]\n        for doc in docs:\n            self.logger.info(f'Getting embedding with shape {doc.embedding.shape}')\n\nflow = Flow().add(uses=TextEmbeddingExecutor, name='embed').add(uses=ProcessEmbedding, name='process')\nwith flow:\n    flow.block()\n```\n````\n````{tab} Invalid Flow\n```{code-block} python\nfrom jina import Executor, requests, Flow\nfrom docarray import DocList, BaseDoc\nfrom docarray.typing import NdArray\nimport numpy as np\n\n\nclass SimpleStrDoc(BaseDoc):\n    text: str\n\nclass TextWithEmbedding(SimpleStrDoc):\n    embedding: NdArray\n\nclass TextEmbeddingExecutor(Executor):\n    @requests(on='/foo')\n    def foo(docs: DocList[SimpleStrDoc], **kwargs) -> DocList[TextWithEmbedding]\n        ret = DocList[TextWithEmbedding]()\n        for doc in docs:\n            ret.append(TextWithEmbedding(text=doc.text, embedding=np.ramdom.rand(10))\n        return ret\n\nclass ProcessText(Executor):\n    @requests(on='/foo')\n    def foo(docs: DocList[SimpleStrDoc], **kwargs) -> DocList[TextWithEmbedding]\n        for doc in docs:\n            self.logger.info(f'Getting embedding with type {doc.text}')\n\n# This Flow will fail to start because the input type of \"process\" does not match the output type of \"embed\"\nflow = Flow().add(uses=TextEmbeddingExecutor, name='embed').add(uses=ProcessText, name='process')\nwith flow:\n    flow.block()\n```\n````\n\nJina is also compatible with docarray<0.30, when using that version, only a single Document schema existed (equivalent to [LegacyDocument]() in docarray>0.30) and therefore\nthere were no explicit compatibility issues between schemas. However, the complexity was implicitly there (An Executor may expect a Document to be filled with `text` and only fail at Runtime).\n\n(floating-executors)=\n### Floating Executors\n\nSome Executors in your Flow can be used for asynchronous background tasks that take time and don't generate a required output. For instance,\nlogging specific information in external services, storing partial results, etc.\n\nYou can unblock your Flow from such tasks by using *floating Executors*.\n\nNormally, all Executors form a pipeline that handles and transforms a given request until it is finally returned to the Client.\n\nHowever, floating Executors do not feed their outputs back into the pipeline. Therefore, the Executor's output does not affect the response for the Client, and the response can be returned without waiting for the floating Executor to complete its task.\n \nThose Executors are marked with the `floating` keyword when added to a `Flow`:\n\n```python\nimport time\nfrom jina import Executor, requests, Flow\nfrom docarray import DocList\nfrom docarray.documents import TextDoc\n\n\nclass FastChangingExecutor(Executor):\n    @requests()\n    def foo(self, docs: DocList[TextDoc], **kwargs) -> DocList[TextDoc]:\n        for doc in docs:\n            doc.text = 'Hello World'\n\n\nclass SlowChangingExecutor(Executor):\n    @requests()\n    def foo(self, docs: DocList[TextDoc], **kwargs) -> DocList[TextDoc]:\n        time.sleep(2)\n        print(f' Received {docs.text}')\n        for doc in docs:\n            doc.text = 'Change the document but will not affect response'\n\n\nf = (\n    Flow()\n    .add(name='executor0', uses=FastChangingExecutor)\n    .add(\n        name='floating_executor',\n        uses=SlowChangingExecutor,\n        needs=['gateway'],\n        floating=True,\n    )\n)\nwith f:\n    f.post(on='/endpoint', inputs=DocList[TextDoc]([TextDoc()]), return_type=DocList[TextDoc])  # we need to send a first\n    start_time = time.time()\n    response = f.post(on='/endpoint', inputs=DocList[TextDoc]([TextDoc(), TextDoc()]), return_type=DocList[TextDoc])\n    end_time = time.time()\n    print(f' Response time took {end_time - start_time}s')\n    print(f' {response.texts}')\n```\n\n```text\n Response time took 0.011997222900390625s\n ['Hello World', 'Hello World']\n Received ['Hello World', 'Hello World']\n```\n\nIn this example the response is returned without waiting for the floating Executor to complete. However, the Flow is not closed until\nthe floating Executor has handled the request.\n\nYou can plot the Flow and see the Executor is floating disconnected from the **Gateway**.\n\n```{figure} images/flow_floating.svg\n:width: 70%\n\n```\nA floating Executor can *never* come before a non-floating Executor in your Flow's {ref}`topology <flow-complex-topologies>`.\n\nThis leads to the following behaviors:\n\n- **Implicit reordering**: When you add a non-floating Executor after a floating Executor without specifying its `needs` parameter, the non-floating Executor is chained after the previous non-floating one.\n```python\nfrom jina import Flow\n\nf = Flow().add().add(name='middle', floating=True).add()\nf.plot()\n```\n\n```{figure} images/flow_middle_1.svg\n:width: 70%\n\n```\n\n- **Chaining floating Executors**: To chain more than one floating Executor, you need to add all of them with the `floating` flag, and explicitly specify the `needs` argument.\n\n```python\nfrom jina import Flow\n\nf = Flow().add().add(name='middle', floating=True).add(needs=['middle'], floating=True)\nf.plot()\n```\n\n```{figure} images/flow_chain_floating.svg\n:width: 70%\n\n```\n\n- **Overriding the `floating` flag**: If you add a floating Executor as part of `needs` parameter of a non-floating Executor, then the floating Executor is no longer considered floating.\n\n```python\nfrom jina import Flow\n\nf = Flow().add().add(name='middle', floating=True).add(needs=['middle'])\nf.plot()\n```\n\n```{figure} images/flow_cancel_floating.svg\n:width: 70%\n```\n\n(conditioning)=\n### Add Conditioning\n\nSometimes you may not want all Documents to be processed by all Executors. For example when you process text and image Documents you want to forward them to different Executors depending on their data type. \n\nYou can set conditioning for every {class}`~jina.Executor` in the Flow. Documents that don't meet the condition will be removed before reaching that Executor. This allows you to build a selection control in the Flow.\n\n#### Define conditions\n\nTo add a condition to an Executor, pass it to the `when` parameter of {meth}`~jina.Flow.add` method of the Flow. This then defines *when* a Document is processed by the Executor:\n\nYou can use the [MongoDB query language](https://www.mongodb.com/docs/compass/current/query/filter/#query-your-data) used in [docarray](https://docs.docarray.org/API_reference/utils/filter/) which follows  to specify a filter condition for each Executor.\n\n```python\nfrom jina import Flow\n\nf = Flow().add(when={'tags__key': {'$eq': 5}})\n```\n\nThen only Documents that satisfy the `when` condition will reach the associated Executor. Any Documents that don't satisfy that condition won't reach the Executor.\n\nIf you are trying to separate Documents according to the data modality they hold, you need to choose a condition accordingly.\n\n````{admonition} See Also\n:class: seealso\n\nIn addition to `$exists` you can use a number of other operators to define your filter: `$eq`, `$gte`, `$lte`, `$size`, `$and`, `$or` and many more. For details, consult [MongoDB query language](https://www.mongodb.com/docs/compass/current/query/filter/#query-your-data) and [docarray](https://docs.docarray.org/API_reference/utils/filter/).\n````\n\n```python\n# define filter conditions\ntext_condition = {'text': {'$exists': True}}\ntensor_condition = {'tensor': {'$exists': True}}\n```\n\nThese conditions specify that only Documents that hold data of a specific modality can pass the filter.\n\n````{tab} Python\n```{code-block} python\n---\nemphasize-lines: 16, 24\n---\n\nfrom jina import Flow, Executor, requests\nfrom docarray import DocList, BaseDoc\nfrom typing import Dict\n\nclass MyDoc(BaseDoc):\n    text: str = ''\n    tags: Dict[str, int]\n\nclass MyExec\n    @requests\n    def foo(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDoc]:\n        for doc in docs:\n            print(f'{doc.tags}')\n\nf = Flow().add(uses=MyExec).add(uses=MyExec, when={'tags__key': {'$eq': 5}})  # Create the empty Flow, add condition\n\nwith f:  # Using it as a Context Manager starts the Flow\n    ret = f.post(\n        on='/search',\n        inputs=DocList[MyDoc]([MyDoc(tags={'key': 5}), MyDoc(tags={'key': 4})]),\n        return_type=DocList[MyDoc]\n    )\n\nfor doc in ret:\n    print(f'{doc.tags}')  # only the Document fulfilling the condition is processed and therefore returned.\n```\n\n```shell\n{'key': 5.0}\n```\n\n````\n\n````{tab} YAML\n\n```yaml\njtype: Flow\nexecutors:\n  - name: executor\n    uses: MyExec\n    when:\n        tags__key:\n            $eq: 5\n```\n\n```{code-block} python\n---\nemphasize-lines: 9\n---\n\nfrom jina import Flow\n\nf = Flow.load_config('flow.yml')  # Load the Flow definition from Yaml file\n\nwith f:  # Using it as a Context Manager starts the Flow\n    ret = f.post(\n        on='/search',\n        inputs=DocList[MyDoc]([MyDoc(tags={'key': 5}), MyDoc(tags={'key': 4})]),\n        return_type=DocList[MyDoc]\n    )\n\nfor doc in ret:\n    print(f'{doc.tags}')  # only the Document fulfilling the condition is processed and therefore returned.\n```\n\n```shell\n{'key': 5.0}\n```\n````\n\nNote that if a Document does not satisfy the `when` condition of a filter, the filter removes the Document *for that entire branch of the Flow*.\nThis means that every Executor located behind a filter is affected by this, not just the specific Executor that defines the condition.\nAs with a real-life filter, once something fails to pass through it, it no longer continues down the pipeline.\n\nNaturally, parallel branches in a Flow do not affect each other. So if a Document gets filtered out in only one branch, it can\nstill be used in the other branch, and also after the branches are re-joined:\n\n````{tab} Parallel Executors\n\n```{code-block} python\n---\nemphasize-lines: 18, 19\n---\n\nfrom jina import Flow, Executor, requests\nfrom docarray import DocList, BaseDoc\nfrom typing import Dict\n\nclass MyDoc(BaseDoc):\n    text: str = ''\n    tags: Dict[str, int]\n\nclass MyExec\n    @requests\n    def foo(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDoc]:\n        for doc in docs:\n            print(f'{doc.tags}')\n\nf = (\n    Flow()\n    .add(uses=MyExec, name='first')\n    .add(uses=MyExec, when={'tags__key': {'$eq': 5}}, needs='first', name='exec1')\n    .add(uses=MyExec, when={'tags__key': {'$eq': 4}}, needs='first', name='exec2')\n    .needs_all(uses=MyExec, name='join')\n)\n```\n\n```{figure} images/conditional-flow.svg\n:width: 70%\n:align: center\n```\n\n```python\nwith f:\n    ret = f.post(\n        on='/search',\n        inputs=DocList[MyDoc]([MyDoc(tags={'key': 5}), MyDoc(tags={'key': 4})]),\n        return_type=DocList[MyDoc]\n    )\n\nfor doc in ret:\n    print(f'{doc.tags}') \n```\n\n```shell\n{'key': 5.0}\n{'key': 4.0}\n```\n\n````\n\n````{tab} Sequential Executors\n```{code-block} python\n---\nemphasize-lines: 18, 19\n---\n\nfrom jina import Flow, Executor, requests\nfrom docarray import DocList, BaseDoc\nfrom typing import Dict\n\nclass MyDoc(BaseDoc):\n    text: str = ''\n    tags: Dict[str, int]\n\nclass MyExec\n    @requests\n    def foo(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDoc]:\n        for doc in docs:\n            print(f'{doc.tags}')\n\nf = (\n    Flow()\n    .add(uses=MyExec, name='first')\n    .add(uses=MyExec, when={'tags__key': {'$eq': 5}}, name='exec1', needs='first')\n    .add(uses=MyExec, when={'tags__key': {'$eq': 4}}, needs='exec1', name='exec2')\n)\n```\n\n```{figure} images/sequential-flow.svg\n:width: 70%\n\n```\n\n\n```python\nwith f:\n    ret = f.post(\n        on='/search',\n        inputs=DocList[MyDoc]([MyDoc(tags={'key': 5}), MyDoc(tags={'key': 4})]),\n        return_type=DocList[MyDoc]\n    )\n\nfor doc in ret:\n    print(f'{doc.tags}') \n```\n\n```shell\n```\n````\n\nThis feature is useful to prevent some specialized Executors from processing certain Documents.\nIt can also be used to build *switch-like nodes*, where some Documents pass through one branch of the Flow,\nwhile other Documents pass through a different parallel branch.\n\nNote that whenever a Document does not satisfy the condition of an Executor, it is not even sent to that Executor.\nInstead, only a tailored Request without any payload is transferred.\nThis means that you can not only use this feature to build complex logic, but also to minimize your networking overhead.\n\n(merging-upstream)=\n### Merging upstream Documents\n\nOften when you're building a Flow, you want an Executor to receive Documents from multiple upstream Executors. \n\n```{figure} images/flow-merge-executor.svg\n:width: 70%\n:align: center\n```\n\nFor this you can use the `docs_matrix` or `docs_map` parameters (part of Executor endpoints signature). These Flow-specific arguments that can be used alongside an Executor's {ref}`default arguments <endpoint-arguments>`:\n\n```{code-block} python\n---\nemphasize-lines: 11, 12\n---\nfrom typing import Dict, Union, List, Optional\nfrom jina import Executor, requests\n\nfrom docarray import DocList\n\nclass MergeExec(Executor):\n    @requests\n    async def foo(\n        self,\n        docs: DocList[...],\n        parameters: Dict,\n        docs_matrix: Optional[List[DocList[...]]],\n        docs_map: Optional[Dict[str, DocList[...]]],\n    ) -> DocList[MyDoc]:\n        pass\n```\n\n- Use `docs_matrix` to receive a List of all incoming DocLists from upstream Executors:\n\n```python\n[\n    DocList[...](...),  # from Executor1\n    DocList[...](...),  # from Executor2\n    DocList[...](...),  # from Executor3\n]\n```\n\n- Use `docs_map` to receive a Dict, where each item's key is the name of an upstream Executor and the value is the DocList coming from that Executor:\n\n```python\n{\n    'Executor1': DocList[...](...),\n    'Executor2': DocList[...](...),\n    'Executor3': DocList[...](...),\n}\n```\n\n(no-reduce)=\n#### Reducing multiple DocLists to one DocList\n\nThe `no_reduce` argument determines whether DocLists are reduced into one when being received:\n\n- To reduce all incoming DocLists into **one single DocList**, do not set `no_reduce` or set it to `False`. The `docs_map` and `docs_matrix` will be `None`.\n- To receive **a list all incoming DocList** set `no_reduce` to `True`. The Executor will receive the DocLists independently under `docs_matrix` and `docs_map`.\n\n```python\nfrom jina import Flow, Executor, requests\n\nfrom docarray import DocList, BaseDoc\n\n\nclass MyDoc(BaseDoc):\n    text: str = ''\n\n\n\nclass Exec1(Executor):\n    @requests\n    def foo(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDoc]:\n        for doc in docs:\n            doc.text = 'Exec1'\n\n\nclass Exec2(Executor):\n    @requests\n    def foo(self, docs: DocList[MyDoc], **kwargs) -> DocList[MyDoc]:\n        for doc in docs:\n            doc.text = 'Exec2'\n\n\nclass MergeExec(Executor):\n    @requests\n    def foo(self, docs: DocList[MyDoc], docs_matrix, **kwargs) -> DocList[MyDoc]:\n        documents_to_return = DocList[MyDoc]()\n        for doc1, doc2 in zip(*docs_matrix):\n            print(\n                f'MergeExec processing pairs of Documents \"{doc1.text}\" and \"{doc2.text}\"'\n            )\n            documents_to_return.append(\n                MyDoc(text=f'Document merging from \"{doc1.text}\" and \"{doc2.text}\"')\n            )\n        return documents_to_return\n\n\nf = (\n    Flow()\n    .add(uses=Exec1, name='exec1')\n    .add(uses=Exec2, name='exec2')\n    .add(uses=MergeExec, needs=['exec1', 'exec2'], no_reduce=True)\n)\n\nwith f:\n    returned_docs = f.post(on='/', inputs=MyDoc(), return_type=DocList[MyDoc])\n\nprint(f'Resulting documents {returned_docs[0].text}')\n```\n\n\n```shell\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üéâ Flow is ready to serve! ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üîó Endpoint ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ  ‚õì     Protocol                    GRPC ‚îÇ\n‚îÇ  üè†       Local           0.0.0.0:55761  ‚îÇ\n‚îÇ  üîí     Private     192.168.1.187:55761  ‚îÇ\n‚îÇ  üåç      Public    212.231.186.65:55761  ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\nMergeExec processing pairs of Documents \"Exec1\" and \"Exec2\"\nResulting documents Document merging from \"Exec1\" and \"Exec2\"\n```\n\n## Visualize\n\nA {class}`~jina.Flow` has a built-in `.plot()` function which can be used to visualize the `Flow`:\n```python\nfrom jina import Flow\n\nf = Flow().add().add()\nf.plot('flow.svg')\n```\n\n```{figure} images/flow.svg\n:width: 70%\n\n```\n\n```python\nfrom jina import Flow\n\nf = Flow().add(name='e1').add(needs='e1').add(needs='e1')\nf.plot('flow-2.svg')\n```\n\n```{figure} images/flow-2.svg\n:width: 70%\n```\n\nYou can also do it in the terminal:\n\n```bash\njina export flowchart flow.yml flow.svg \n```\n\nYou can also visualize a remote Flow by passing the URL to `jina export flowchart`.\n\n(logging-configuration)=\n## Logging\n\nThe default {class}`jina.logging.logger.JinaLogger` uses rich console logging that writes to the system console. The `log_config` argument can be used to pass in a string of the pre-configured logging configuration names in Jina or the absolute YAML file path of the custom logging configuration. For most cases, the default logging configuration sufficiently covers local, Docker and Kubernetes environments.\n\nCustom logging handlers can be configured by following the Python official [Logging Cookbook](https://docs.python.org/3/howto/logging-cookbook.html#logging-cookbook) examples. An example custom logging configuration file defined in a YAML file `logging.json.yml` is:\n\n```yaml\nhandlers:\n  - StreamHandler\nlevel: INFO\nconfigs:\n  StreamHandler:\n    format: '%(asctime)s:{name:>15}@%(process)2d[%(levelname).1s]:%(message)s'\n    formatter: JsonFormatter\n```\n\nThe logging configuration can be used as follows:\n\n````{tab} Python\n```python\nfrom jina import Flow\n\nf = Flow(log_config='./logging.json.yml')\n```\n````\n````{tab} YAML\n```yaml\njtype: Flow\nwith:\n    log_config: './logging.json.yml'\n```\n````\n\n(logging-override)=\n### Custom logging configuration\n\nThe default {ref}`logging <logging-configuration>` or custom logging configuration at the Flow level will be propagated to the `Gateway` and `Executor` entities. If that is not desired, every `Gateway` or `Executor` entity can be provided with its own custom logging configuration. \n\nYou can configure two different `Executors` as in the below example:\n\n```python\nfrom jina import Flow\n\nf = (\n    Flow().add(log_config='./logging.json.yml').add(log_config='./logging.file.yml')\n)  # Create a Flow with two Executors\n```\n\n`logging.file.yml` is another YAML file with a custom `FileHandler` configuration.\n\n````{hint}\nRefer to {ref}`Gateway logging configuration <gateway-logging-configuration>` section for configuring the `Gateway` logging.\n````\n\n````{caution}\nWhen exporting the Flow to Kubernetes, the log_config file path must refer to the absolute local path of each container. The custom logging\nfile must be included during the containerization process. If the availability of the file is unknown then its best to rely on the default\nconfiguration. This restriction also applies to dockerized `Executors`. When running a dockerized Executor locally, the logging configuration\nfile can be mounted using {ref}`volumes <mount-local-volumes>`.\n````\n\n## Methods\n\nThe most important methods of the `Flow` object are the following:\n\n| Method                                                       | Description                                                                                                                                                                                                                                                                          |\n|--------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| {meth}`~jina.Flow.add`                                       | Adds an Executor to the Flow                                                                                                                                                                                                                                                         |\n| {meth}`~jina.Flow.start()`                                   | Starts the Flow. This will start all its Executors and check if they are ready to be used.                                                                                                                                                                                           |\n| {meth}`~jina.Flow.close()`                                   | Stops and closes the Flow. This will stop and shutdown all its Executors.                                                                                                                                                                                                            |\n| `with` context manager                                       | Uses the Flow as a context manager. It will automatically start and stop your Flow.                                                                                                                                                                                                   |                                                                |\n| {meth}`~jina.Flow.plot()`                                    | Visualizes the Flow. Helpful for building complex pipelines.                                                                                                                                                                                                                         |\n| {meth}`~jina.clients.mixin.PostMixin.post()`                 | Sends requests to the Flow API.                                                                                                                                                                                                                                                      |\n| {meth}`~jina.Flow.block()`                                   | Blocks execution until the program is terminated. This is useful to keep the Flow alive so it can be used from other places (clients, etc).                                                                                                                                          |\n| {meth}`~jina.Flow.to_docker_compose_yaml()`                  | Generates a Docker-Compose file listing all Executors as services.                                                                                                                                                                                                                                                |\n| {meth}`~jina.Flow.to_kubernetes_yaml()`                      | Generates Kubernetes configuration files in `<output_directory>`. Based on your local Jina and docarray versions, Executor Hub may rebuild the Docker image during the YAML generation process. If you do not wish to rebuild the image, set the environment variable `JINA_HUB_NO_IMAGE_REBUILD`.                                                                                                                                   |\n| {meth}`~jina.clients.mixin.HealthCheckMixin.is_flow_ready()` | Check if the Flow is ready to process requests. Returns a boolean indicating the readiness.                                                                                                                                                                                                                                                                                                                                 |\n\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/concepts/orchestration/flow.md", "language": "markdown", "complexity": "complex", "star_count": 21665, "quality_score": 0.9700000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 1227}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "| Name | Description | Type | Default |\n|----|----|----|----|\n| `name` | The name of this object.<br><br>    This will be used in the following places:<br>    - how you refer to this object in Python/YAML/CLI<br>    - visualization<br>    - log message header<br>    - ...<br><br>    When not given, then the default naming strategy will apply. | `string` | `gateway` |\n| `workspace` | The working directory for any IO operations in this object. If not set, then derive from its parent `workspace`. | `string` | `None` |\n| `log_config` | The config name or the absolute path to the YAML config file of the logger used in this object. | `string` | `default` |\n| `quiet` | If set, then no log will be emitted from this object. | `boolean` | `False` |\n| `quiet_error` | If set, then exception stack information will not be added to the log | `boolean` | `False` |\n| `timeout_ctrl` | The timeout in milliseconds of the control request, -1 for waiting forever | `number` | `60` |\n| `entrypoint` | The entrypoint command overrides the ENTRYPOINT in Docker image. when not set then the Docker image ENTRYPOINT takes effective. | `string` | `None` |\n| `docker_kwargs` | Dictionary of kwargs arguments that will be passed to Docker SDK when starting the docker '<br>container. <br><br>More details can be found in the Docker SDK docs:  https://docker-py.readthedocs.io/en/stable/ | `object` | `None` |\n| `prefetch` | Number of requests fetched from the client before feeding into the first Executor. <br>    <br>    Used to control the speed of data input into a Flow. 0 disables prefetch (1000 requests is the default) | `number` | `1000` |\n| `title` | The title of this HTTP server. It will be used in automatics docs such as Swagger UI. | `string` | `None` |\n| `description` | The description of this HTTP server. It will be used in automatics docs such as Swagger UI. | `string` | `None` |\n| `cors` | If set, a CORS middleware is added to FastAPI frontend to allow cross-origin access. | `boolean` | `False` |\n| `no_debug_endpoints` | If set, `/status` `/post` endpoints are removed from HTTP interface. | `boolean` | `False` |\n| `no_crud_endpoints` | If set, `/index`, `/search`, `/update`, `/delete` endpoints are removed from HTTP interface.<br><br>        Any executor that has `@requests(on=...)` bound with those values will receive data requests. | `boolean` | `False` |\n| `expose_endpoints` | A JSON string that represents a map from executor endpoints (`@requests(on=...)`) to HTTP endpoints. | `string` | `None` |\n| `uvicorn_kwargs` | Dictionary of kwargs arguments that will be passed to Uvicorn server when starting the server<br><br>More details can be found in Uvicorn docs: https://www.uvicorn.org/settings/ | `object` | `None` |\n| `ssl_certfile` | the path to the certificate file | `string` | `None` |\n| `ssl_keyfile` | the path to the key file | `string` | `None` |\n| `expose_graphql_endpoint` | If set, /graphql endpoint is added to HTTP interface. | `boolean` | `False` |\n| `protocol` | Communication protocol of the server exposed by the Gateway. This can be a single value or a list of protocols, depending on your chosen Gateway. Choose the convenient protocols from: ['GRPC', 'HTTP', 'WEBSOCKET']. | `array` | `[<GatewayProtocolType.GRPC: 0>]` |\n| `host` | The host address of the runtime, by default it is 0.0.0.0. | `string` | `0.0.0.0` |\n| `proxy` | If set, respect the http_proxy and https_proxy environment variables. otherwise, it will unset these proxy variables before start. gRPC seems to prefer no proxy | `boolean` | `False` |\n| `uses` | The config of the gateway, it could be one of the followings:<br>        * the string literal of an Gateway class name<br>        * a Gateway YAML file (.yml, .yaml, .jaml)<br>        * a docker image (must start with `docker://`)<br>        * the string literal of a YAML config (must start with `!` or `jtype: `)<br>        * the string literal of a JSON config<br><br>        When use it under Python, one can use the following values additionally:<br>        - a Python dict that represents the config<br>        - a text file stream has `.read()` interface | `string` | `None` |\n| `uses_with` | Dictionary of keyword arguments that will override the `with` configuration in `uses` | `object` | `None` |\n| `py_modules` | The customized python modules need to be imported before loading the gateway<br><br>Note that the recommended way is to only import a single module - a simple python file, if your<br>gateway can be defined in a single file, or an ``__init__.py`` file if you have multiple files,<br>which should be structured as a python package. | `array` | `None` |\n| `replicas` | The number of replicas of the Gateway. This replicas will only be applied when converted into Kubernetes YAML | `number` | `1` |\n| `grpc_server_options` | Dictionary of kwargs arguments that will be passed to the grpc server as options when starting the server, example : {'grpc.max_send_message_length': -1} | `object` | `None` |\n| `graph_description` | Routing graph for the gateway | `string` | `{}` |\n| `graph_conditions` | Dictionary stating which filtering conditions each Executor in the graph requires to receive Documents. | `string` | `{}` |\n| `deployments_addresses` | JSON dictionary with the input addresses of each Deployment | `string` | `{}` |\n| `deployments_metadata` | JSON dictionary with the request metadata for each Deployment | `string` | `{}` |\n| `deployments_no_reduce` | list JSON disabling the built-in merging mechanism for each Deployment listed | `string` | `[]` |\n| `compression` | The compression mechanism used when sending requests from the Head to the WorkerRuntimes. For more details, check https://grpc.github.io/grpc/python/grpc.html#compression. | `string` | `None` |\n| `timeout_send` | The timeout in milliseconds used when sending data requests to Executors, -1 means no timeout, disabled by default | `number` | `None` |\n| `runtime_cls` | The runtime class to run inside the Pod | `string` | `GatewayRuntime` |\n| `timeout_ready` | The timeout in milliseconds of a Pod waits for the runtime to be ready, -1 for waiting forever | `number` | `600000` |\n| `env` | The map of environment variables that are available inside runtime | `object` | `None` |\n| `env_from_secret` | The map of environment variables that are read from kubernetes cluster secrets | `object` | `None` |\n| `floating` | If set, the current Pod/Deployment can not be further chained, and the next `.add()` will chain after the last Pod/Deployment not this current one. | `boolean` | `False` |\n| `reload` | If set, the Gateway will restart while serving if YAML configuration source is changed. | `boolean` | `False` |\n| `port` | The port for input data to bind the gateway server to, by default, random ports between range [49152, 65535] will be assigned. The port argument can be either 1 single value in case only 1 protocol is used or multiple values when many protocols are used. | `number` | `random in [49152, 65535]` |\n| `monitoring` | If set, spawn an http server with a prometheus endpoint to expose metrics | `boolean` | `False` |\n| `port_monitoring` | The port on which the prometheus server is exposed, default is a random port between [49152, 65535] | `number` | `random in [49152, 65535]` |\n| `retries` | Number of retries per gRPC call. If <0 it defaults to max(3, num_replicas) | `number` | `-1` |\n| `tracing` | If set, the sdk implementation of the OpenTelemetry tracer will be available and will be enabled for automatic tracing of requests and customer span creation. Otherwise a no-op implementation will be provided. | `boolean` | `False` |\n| `traces_exporter_host` | If tracing is enabled, this hostname will be used to configure the trace exporter agent. | `string` | `None` |\n| `traces_exporter_port` | If tracing is enabled, this port will be used to configure the trace exporter agent. | `number` | `None` |\n| `metrics` | If set, the sdk implementation of the OpenTelemetry metrics will be available for default monitoring and custom measurements. Otherwise a no-op implementation will be provided. | `boolean` | `False` |\n| `metrics_exporter_host` | If tracing is enabled, this hostname will be used to configure the metrics exporter agent. | `string` | `None` |\n| `metrics_exporter_port` | If tracing is enabled, this port will be used to configure the metrics exporter agent. | `number` | `None` |\n| `stateful` | If set, start consensus module to make sure write operations are properly replicated between all the replicas | `boolean` | `False` |\n| `pod_ports` | When using StatefulExecutors, if they want to restart it is important to keep the RAFT cluster configuration | `number` | `None` |", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/concepts/orchestration/gateway-args.md", "language": "markdown", "complexity": "simple", "star_count": 21665, "quality_score": 0.98, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 54}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Changelog\n\n<!-- markdownlint-disable no-duplicate-header -->\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n------\n\n## [Unreleased]\n\n### Added\n\n-\n\n### Changed\n\n-\n\n### Fixed\n\n-\n\n### Removed\n\n-\n\n------\n\n## [1.5.2] - 2025-07-25\n\n### Changed\n\n- Minor tweak display for device name in TUI by [@XuehaiPan](https://github.com/XuehaiPan).\n\n### Fixed\n\n- Fix snapshot cache for GPU processes with shared host process by [@XuehaiPan](https://github.com/XuehaiPan) in [#172](https://github.com/XuehaiPan/nvitop/pull/172).\n\n------\n\n## [1.5.1] - 2025-05-26\n\n### Added\n\n- Add `docker-compose` template for `nvitop-exporter` by [@gianfranco-s](https://github.com/gianfranco-s) in [#159](https://github.com/XuehaiPan/nvitop/pull/159).\n\n### Fixed\n\n- Fix one-time output rendering on exit for TUI by [@XuehaiPan](https://github.com/XuehaiPan).\n\n------\n\n## [1.5.0] - 2025-04-25\n\n### Added\n\n- Show `%GMBW` in main screen by [@XuehaiPan](https://github.com/XuehaiPan) in [#156](https://github.com/XuehaiPan/nvitop/pull/156).\n- Add doctests and add type annotations in `nvitop.tui` by [@XuehaiPan](https://github.com/XuehaiPan) in [#164](https://github.com/XuehaiPan/nvitop/pull/164).\n\n### Changed\n\n- Deprecate `nvitop.callbacks` as officially unmaintained by [@XuehaiPan](https://github.com/XuehaiPan) in [#157](https://github.com/XuehaiPan/nvitop/pull/157).\n\n### Fixed\n\n- Ignore errors when collecting host metrics for host panel by [@XuehaiPan](https://github.com/XuehaiPan) in [#163](https://github.com/XuehaiPan/nvitop/pull/163).\n\n### Removed\n\n- Drop Python 3.7 support by [@XuehaiPan](https://github.com/XuehaiPan) in [#150](https://github.com/XuehaiPan/nvitop/pull/150).\n\n------\n\n## [1.4.2] - 2025-01-27\n\n### Removed\n\n- Vendor third-party dependency `termcolor` by [@XuehaiPan](https://github.com/XuehaiPan) in [#148](https://github.com/XuehaiPan/nvitop/pull/148).\n- Remove third-party dependency `cachetools` by [@XuehaiPan](https://github.com/XuehaiPan) in [#147](https://github.com/XuehaiPan/nvitop/pull/147).\n\n------\n\n## [1.4.1] - 2025-01-13\n\n### Fixed\n\n- Fix passing invalid device handle (e.g., GPU is lost) to NVML functions by [@XuehaiPan](https://github.com/XuehaiPan) in [#146](https://github.com/XuehaiPan/nvitop/pull/146).\n- Fix CUDA device selection tool `nvisel` by [@XuehaiPan](https://github.com/XuehaiPan).\n\n------\n\n## [1.4.0] - 2024-12-29\n\n### Added\n\n- Add Grafana dashboard for `nvitop-exporter` by [@XuehaiPan](https://github.com/XuehaiPan) in [#138](https://github.com/XuehaiPan/nvitop/pull/138).\n- Handle exceptions for function `getpass.getuser()` by [@XuehaiPan](https://github.com/XuehaiPan) in [#130](https://github.com/XuehaiPan/nvitop/pull/130). Issued by [@landgraf](https://github.com/landgraf).\n\n### Changed\n\n- Refactor setup scripts by [@XuehaiPan](https://github.com/XuehaiPan).\n\n### Fixed\n\n- Fix documentation for the `ResourceMetricCollector.clear()` method by [@MyGodItsFull0fStars](https://github.com/MyGodItsFull0fStars) in [#132](https://github.com/XuehaiPan/nvitop/pull/132).\n- Gracefully ignore UTF-8 decoding errors by [@XuehaiPan](https://github.com/XuehaiPan).\n\n------\n\n## [1.3.2] - 2023-10-17\n\n### Added\n\n- Add separate implementation for `GpuStatsLogger` callback for `lightning` by [@XuehaiPan](https://github.com/XuehaiPan) in [#114](https://github.com/XuehaiPan/nvitop/pull/114).\n- Remove metrics if process is gone in `nvitop-exporter` by [@XuehaiPan](https://github.com/XuehaiPan) in [#107](https://github.com/XuehaiPan/nvitop/pull/107).\n\n------\n\n## [1.3.1] - 2023-10-05\n\n### Added\n\n- Add Python 3.12 classifiers by [@XuehaiPan](https://github.com/XuehaiPan) in [#101](https://github.com/XuehaiPan/nvitop/pull/101).\n\n### Fixed\n\n- Fix `libcuda.cuDeviceGetUuid()` when the UUID contains `0x00` by [@XuehaiPan](https://github.com/XuehaiPan) in [#100](https://github.com/XuehaiPan/nvitop/pull/100).\n\n------\n\n## [1.3.0] - 2023-08-27\n\n### Added\n\n- Add Prometheus exporter by [@XuehaiPan](https://github.com/XuehaiPan) in [#92](https://github.com/XuehaiPan/nvitop/pull/92).\n- Add device APIs to query PCIe and NVLink throughput by [@XuehaiPan](https://github.com/XuehaiPan) in [#87](https://github.com/XuehaiPan/nvitop/pull/87).\n\n### Changed\n\n- Use recent timestamp for GPU process utilization query for more accurate per-process GPU usage by [@XuehaiPan](https://github.com/XuehaiPan) in [#85](https://github.com/XuehaiPan/nvitop/pull/85). We extend our heartfelt gratitude to [@2581543189](https://github.com/2581543189) for their invaluable assistance. Their timely comments and comprehensive feedback have greatly contributed to the improvement of this project.\n\n### Fixed\n\n- Fix upstream changes for process info v3 APIs on 535.104.05 driver by [@XuehaiPan](https://github.com/XuehaiPan) in [#94](https://github.com/XuehaiPan/nvitop/pull/94).\n- Fix removal for process info v3 APIs on the upstream 535.98 driver by [@XuehaiPan](https://github.com/XuehaiPan) in [#89](https://github.com/XuehaiPan/nvitop/pull/89).\n\n------\n\n## [1.2.0] - 2023-07-24\n\n### Added\n\n- Include last snapshot metrics in the log results for `ResourceMetricCollector` by [@XuehaiPan](https://github.com/XuehaiPan) in [#80](https://github.com/XuehaiPan/nvitop/pull/80).\n- Add `mypy` integration and update type annotations by [@XuehaiPan](https://github.com/XuehaiPan) in [#73](https://github.com/XuehaiPan/nvitop/pull/73).\n\n### Fixed\n\n- Fix process info support for NVIDIA R535 driver (CUDA 12.2+) by [@XuehaiPan](https://github.com/XuehaiPan) in [#79](https://github.com/XuehaiPan/nvitop/pull/79).\n- Fix inappropriate exception catching in function `libcuda.cuDeviceGetUuid` by [@XuehaiPan](https://github.com/XuehaiPan).\n\n------\n\n## [1.1.2] - 2023-04-11\n\n### Fixed\n\n- Further isolate the `CUDA_VISIBLE_DEVICES` parser in a subprocess by [@XuehaiPan](https://github.com/XuehaiPan) in [#70](https://github.com/XuehaiPan/nvitop/pull/70).\n\n------\n\n## [1.1.1] - 2023-04-07\n\n### Fixed\n\n- Fix MIG device support by [@XuehaiPan](https://github.com/XuehaiPan).\n\n------\n\n## [1.1.0] - 2023-04-07\n\n### Added\n\n- Support float number as snapshot interval that >= 0.25s by [@XuehaiPan](https://github.com/XuehaiPan) in [#67](https://github.com/XuehaiPan/nvitop/pull/67).\n- Show more host metrics (e.g., used virtual memory, uptime) in CLI by [@XuehaiPan](https://github.com/XuehaiPan) in [#59](https://github.com/XuehaiPan/nvitop/pull/59).\n\n### Changed\n\n- Move `TTLCache` usage to CLI-only by [@XuehaiPan](https://github.com/XuehaiPan) in [#66](https://github.com/XuehaiPan/nvitop/pull/66).\n\n### Fixed\n\n- Respect `FORCE_COLOR` and `NO_COLOR` environment variables by [@XuehaiPan](https://github.com/XuehaiPan).\n\n### Removed\n\n- Drop Python 3.6 support by [@XuehaiPan](https://github.com/XuehaiPan) in [#56](https://github.com/XuehaiPan/nvitop/pull/56).\n\n------\n\n## [1.0.0] - 2023-02-01\n\n### Added\n\n- The first stable release of `nvitop` by [@XuehaiPan](https://github.com/XuehaiPan).\n\n------\n\n[Unreleased]: https://github.com/XuehaiPan/nvitop/compare/v1.5.2...HEAD\n[1.5.2]: https://github.com/XuehaiPan/nvitop/compare/v1.5.1...v1.5.2\n[1.5.1]: https://github.com/XuehaiPan/nvitop/compare/v1.5.0...v1.5.1\n[1.5.0]: https://github.com/XuehaiPan/nvitop/compare/v1.4.2...v1.5.0\n[1.4.2]: https://github.com/XuehaiPan/nvitop/compare/v1.4.1...v1.4.2\n[1.4.1]: https://github.com/XuehaiPan/nvitop/compare/v1.4.0...v1.4.1\n[1.4.0]: https://github.com/XuehaiPan/nvitop/compare/v1.3.2...v1.4.0\n[1.3.2]: https://github.com/XuehaiPan/nvitop/compare/v1.3.1...v1.3.2\n[1.3.1]: https://github.com/XuehaiPan/nvitop/compare/v1.3.0...v1.3.1\n[1.3.0]: https://github.com/XuehaiPan/nvitop/compare/v1.2.0...v1.3.0\n[1.2.0]: https://github.com/XuehaiPan/nvitop/compare/v1.1.2...v1.2.0\n[1.1.2]: https://github.com/XuehaiPan/nvitop/compare/v1.1.1...v1.1.2\n[1.1.1]: https://github.com/XuehaiPan/nvitop/compare/v1.1.0...v1.1.1\n[1.1.0]: https://github.com/XuehaiPan/nvitop/compare/v1.0.0...v1.1.0\n[1.0.0]: https://github.com/XuehaiPan/nvitop/releases/tag/v1.0.0\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "CHANGELOG.md", "language": "markdown", "complexity": "complex", "star_count": 5792, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 226}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/bin/bash\n# ==============================================================================\n#\n# Usage: bash install-nvidia-driver.sh [--package=PKG] [--upgrade-only] [--latest] [--dry-run] [--yes] [--help]\n#\n# Examples:\n#\n#     bash install-nvidia-driver.sh\n#     bash install-nvidia-driver.sh --package=nvidia-driver-470\n#     bash install-nvidia-driver.sh --upgrade-only\n#     bash install-nvidia-driver.sh --latest\n#\n# ==============================================================================\n# This file is part of nvitop, the interactive NVIDIA-GPU process viewer.\n#\n# Copyright 2021-2025 Xuehai Pan. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# shellcheck disable=SC2016,SC2312\n\nset -u\nset -e\nshopt -s inherit_errexit &>/dev/null || true\n\nfunction abort() {\n\techo \"$@\" >&2\n\texit 1\n}\n\n# shellcheck disable=2292\nif [ -z \"${BASH_VERSION:-}\" ]; then\n\tabort \"Bash is required to interpret this script.\"\nfi\n\nexport LANGUAGE=\"C:en\"\n\n# shellcheck disable=1091\nif [[ \"$(uname -s)\" != \"Linux\" ]] || (source /etc/os-release && [[ \"${NAME:-}\" != \"Ubuntu\" ]]); then\n\tabort \"This script only supports Ubuntu Linux.\"\nelif [[ -n \"${WSL_DISTRO_NAME:-}\" ]] || (uname -r | grep -qiF 'microsoft'); then\n\tabort \"Please install the NVIDIA driver in the Windows host system rather than in WSL.\"\nfi\n\n# String formatters\nif [[ -t 1 ]]; then\n\ttty_escape() { printf \"\\033[%sm\" \"$1\"; }\nelse\n\ttty_escape() { :; }\nfi\ntty_mkbold() { tty_escape \"1;$1\"; }\ntty_blue=\"$(tty_mkbold 34)\"\ntty_red=\"$(tty_mkbold 31)\"\ntty_green=\"$(tty_mkbold 32)\"\ntty_yellow=\"$(tty_mkbold 33)\"\ntty_white=\"$(tty_mkbold 37)\"\ntty_bold=\"$(tty_mkbold 39)\"\ntty_reset=\"$(tty_escape 0)\"\n\nfunction usage() {\n\tcat <<EOS\nUsage: bash $(basename \"$0\") [--package=PKG] [--upgrade-only] [--latest] [--dry-run] [--yes] [--help]\n\nOptions:\n  --package PKG    Install the specified driver package. (e.g. ${tty_bold}nvidia-driver-470${tty_reset})\n  --upgrade-only   Keep the installed NVIDIA driver package and only upgrade the package version.\n  --latest         Upgrade to the latest NVIDIA driver, the old driver package may be removed.\n  --dry-run, -n    List all available NVIDIA driver packages and exit.\n  --yes, -y        Do not ask for confirmation.\n  --help, -h       Show this help and exit.\n\nExamples:\n\n    bash $(basename \"$0\")\n    bash $(basename \"$0\") --package=nvidia-driver-470\n    bash $(basename \"$0\") --upgrade-only\n    bash $(basename \"$0\") --latest\n\n${tty_yellow}NOTE:${tty_reset} During the install process, the script will offload the NVIDIA kernel modules. Please terminate the\nprocesses that are using the NVIDIA GPUs, e.g., \\`watch nvidia-smi\\`, \\`nvitop\\`, and the GNOME Display Manager.\nThe script will install the latest NVIDIA driver if no NVIDIA driver is installed in the system.\nEOS\n}\n\n### Parse the command line arguments ###############################################################\n\nREQUESTED_DRIVER=''\nUPGRADE_ONLY=''\nLATEST=''\nDRY_RUN=''\nYES=''\nunset HAVE_SUDO_ACCESS\nwhile [[ \"$#\" -gt 0 ]]; do\n\targ=\"$1\"\n\tshift\n\tcase \"${arg}\" in\n\t\t--package)\n\t\t\tREQUESTED_DRIVER=\"$1\"\n\t\t\tshift\n\t\t\t;;\n\t\t--package=*)\n\t\t\tREQUESTED_DRIVER=\"${arg#*=}\"\n\t\t\t;;\n\t\t--upgrade-only)\n\t\t\tif [[ -n \"${LATEST}\" ]]; then\n\t\t\t\tabort 'Both option `--upgrade-only` and `--latest` are set.'\n\t\t\tfi\n\t\t\tUPGRADE_ONLY=1\n\t\t\t;;\n\t\t--latest)\n\t\t\tif [[ -n \"${UPGRADE_ONLY}\" ]]; then\n\t\t\t\tabort 'Both option `--upgrade-only` and `--latest` are set.'\n\t\t\tfi\n\t\t\tLATEST=1\n\t\t\t;;\n\t\t--dry-run | -n)\n\t\t\tDRY_RUN=1\n\t\t\tHAVE_SUDO_ACCESS=1\n\t\t\t;;\n\t\t--yes | -y)\n\t\t\tYES=1\n\t\t\t;;\n\t\t--help | -h)\n\t\t\tusage\n\t\t\texit\n\t\t\t;;\n\t\t*)\n\t\t\tusage >&2\n\t\t\techo >&2\n\t\t\tabort \"Invalid option '${arg}'\"\n\t\t\t;;\n\tesac\ndone\n\n### Functions ######################################################################################\n\nfunction apt-list-packages() {\n\tdpkg-query --show --showformat='${Installed-Size} ${binary:Package} ${Version} ${Status}\\n' |\n\t\tgrep -v deinstall | sort -n | awk '{ print $1\" \"$2\" \"$3 }'\n}\n\nfunction apt-list-nvidia-packages() {\n\tlocal packages\n\tpackages=\"$(\n\t\tapt-list-packages |\n\t\t\tawk '$2 ~ /nvidia.*-([0-9]+)(:.*)?$/ { print $2 }' |\n\t\t\tsort\n\t)\"\n\techo \"${packages//$'\\n'/ }\"\n}\n\nfunction apt-installed-version() {\n\tapt-cache policy \"$1\" | grep -F 'Installed' | awk '{ print $2 }'\n}\n\nfunction apt-candidate-version() {\n\tapt-cache policy \"$1\" | grep -F 'Candidate' | awk '{ print $2 }'\n}\n\n# From https://github.com/XuehaiPan/Dev-Setup.git\nfunction exec_cmd() {\n\tprintf \"%s\" \"$@\" | awk \\\n\t\t'BEGIN {\n\t\t\tRESET = \"\\033[0m\";\n\t\t\tBOLD = \"\\033[1m\";\n\t\t\tUNDERLINE = \"\\033[4m\";\n\t\t\tUNDERLINEOFF = \"\\033[24m\";\n\t\t\tRED = \"\\033[31m\";\n\t\t\tGREEN = \"\\033[32m\";\n\t\t\tYELLOW = \"\\033[33m\";\n\t\t\tWHITE = \"\\033[37m\";\n\t\t\tGRAY = \"\\033[90m\";\n\t\t\tIDENTIFIER = \"[_a-zA-Z][_a-zA-Z0-9]*\";\n\t\t\tidx = 0;\n\t\t\tin_string = 0;\n\t\t\tdouble_quoted = 1;\n\t\t\tprintf(\"%s$\", BOLD WHITE);\n\t\t}\n\t\t{\n\t\t\tfor (i = 1; i <= NF; ++i) {\n\t\t\t\tstyle = WHITE;\n\t\t\t\tpost_style = WHITE;\n\t\t\t\tif (!in_string) {\n\t\t\t\t\tif ($i ~ /^-/)\n\t\t\t\t\t\tstyle = YELLOW;\n\t\t\t\t\telse if ($i == \"sudo\" && idx == 0) {\n\t\t\t\t\t\tstyle = UNDERLINE GREEN;\n\t\t\t\t\t\tpost_style = UNDERLINEOFF WHITE;\n\t\t\t\t\t}\n\t\t\t\t\telse if ($i ~ \"^\" IDENTIFIER \"=\" && idx == 0) {\n\t\t\t\t\t\tstyle = GRAY;\n\t\t\t\t\t\t'\"if (\\$i ~ \\\"^\\\" IDENTIFIER \\\"=[\\\\\\\"']\\\") {\"'\n\t\t\t\t\t\t\tin_string = 1;\n\t\t\t\t\t\t\tdouble_quoted = ($i ~ \"^\" IDENTIFIER \"=\\\"\");\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\telse if ($i ~ /^[12&]?>>?/ || $i == \"\\\\\")\n\t\t\t\t\t\tstyle = RED;\n\t\t\t\t\telse {\n\t\t\t\t\t\t++idx;\n\t\t\t\t\t\t'\"if (\\$i ~ /^[\\\"']/) {\"'\n\t\t\t\t\t\t\tin_string = 1;\n\t\t\t\t\t\t\tdouble_quoted = ($i ~ /^\"/);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (idx == 1)\n\t\t\t\t\t\t\tstyle = GREEN;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (in_string) {\n\t\t\t\t\tif (style == WHITE)\n\t\t\t\t\t\tstyle = \"\";\n\t\t\t\t\tpost_style = \"\";\n\t\t\t\t\t'\"if ((double_quoted && \\$i ~ /\\\";?\\$/ && \\$i !~ /\\\\\\\\\\\";?\\$/) || (!double_quoted && \\$i ~ /';?\\$/))\"'\n\t\t\t\t\t\tin_string = 0;\n\t\t\t\t}\n\t\t\t\tif (($i ~ /;$/ && $i !~ /\\\\;$/) || $i == \"|\" || $i == \"||\" || $i == \"&&\") {\n\t\t\t\t\tif (!in_string) {\n\t\t\t\t\t\tidx = 0;\n\t\t\t\t\t\tif ($i !~ /;$/)\n\t\t\t\t\t\t\tstyle = RED;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif ($i ~ /;$/ && $i !~ /\\\\;$/)\n\t\t\t\t\tprintf(\" %s%s%s;%s\", style, substr($i, 1, length($i) - 1), (in_string ? WHITE : RED), post_style);\n\t\t\t\telse\n\t\t\t\t\tprintf(\" %s%s%s\", style, $i, post_style);\n\t\t\t\tif ($i == \"\\\\\")\n\t\t\t\t\tprintf(\"\\n\\t\");\n\t\t\t}\n\t\t}\n\t\tEND {\n\t\t\tprintf(\"%s\\n\", RESET);\n\t\t}' >&2\n\t# shellcheck disable=SC2294\n\teval \"$@\"\n}\n\n# From https://github.com/Homebrew/install.git\nshell_join() {\n\tlocal arg\n\tprintf \"%s\" \"$1\"\n\tshift\n\tfor arg in \"$@\"; do\n\t\tprintf \" \"\n\t\tprintf \"%s\" \"${arg// /\\ }\"\n\tdone\n}\n\nchomp() {\n\tprintf \"%s\" \"${1/\"$'\\n'\"/}\"\n}\n\nohai() {\n\tprintf \"${tty_blue}==>${tty_bold} %s${tty_reset}\\n\" \"$(shell_join \"$@\")\"\n}\n\nwarn() {\n\tprintf \"${tty_red}Warning:${tty_reset} %s\\n\" \"$(chomp \"$1\")\"\n}\n\nring_bell() {\n\t# Use the shell's audible bell.\n\tif [[ -t 1 ]]; then\n\t\tprintf \"\\a\"\n\tfi\n}\n\ngetc() {\n\tlocal save_state\n\tsave_state=\"$(/bin/stty -g)\"\n\t/bin/stty raw -echo\n\tIFS='' read -r -n 1 -d '' \"$@\"\n\t/bin/stty \"${save_state}\"\n}\n\nCONFIRM_MESSAGE=\"Press ${tty_bold}RETURN${tty_reset}/${tty_bold}ENTER${tty_reset} to continue or any other key to abort:\"\nwait_for_user() {\n\tlocal c\n\techo\n\techo \"${CONFIRM_MESSAGE}\"\n\tgetc c\n\t# we test for \\r and \\n because some stuff does \\r instead\n\tif ! [[ \"${c}\" == $'\\r' || \"${c}\" == $'\\n' ]]; then\n\t\texit 1\n\tfi\n}\n\nfunction have_sudo_access() {\n\tif [[ \"${EUID:-\"${UID}\"}\" == \"0\" ]]; then\n\t\treturn 0\n\tfi\n\n\tif [[ ! -x \"/usr/bin/sudo\" ]]; then\n\t\treturn 1\n\tfi\n\n\tlocal -a SUDO=(\"/usr/bin/sudo\")\n\tif [[ -n \"${SUDO_ASKPASS-}\" ]]; then\n\t\tSUDO+=(\"-A\")\n\tfi\n\n\tif [[ -z \"${HAVE_SUDO_ACCESS-}\" ]]; then\n\t\tohai \"Checking sudo access (press ${tty_yellow}Ctrl+C${tty_white} to list the driver versions only).\" >&2\n\t\texec_cmd \"${SUDO[*]} -v && ${SUDO[*]} -l mkdir &>/dev/null\"\n\t\tHAVE_SUDO_ACCESS=\"$?\"\n\tfi\n\n\treturn \"${HAVE_SUDO_ACCESS}\"\n}\n\n### Add Graphics Drivers PPA #######################################################################\n\n# shellcheck disable=SC2310\nif have_sudo_access; then\n\texec_cmd 'sudo apt-get update'\n\tif ! (apt-cache policy | grep -qF 'graphics-drivers/ppa/ubuntu'); then\n\t\texec_cmd 'sudo apt-get install software-properties-common apt-transport-https --yes'\n\t\texec_cmd 'sudo add-apt-repository ppa:graphics-drivers/ppa --yes'\n\t\texec_cmd 'sudo apt-get update'\n\tfi\n\techo\nelse\n\tDRY_RUN=1\nfi\n\n### Query and list available driver packages #######################################################\n\n# shellcheck disable=SC2207\nAVAILABLE_DRIVERS=($(\n\tapt-cache search --names-only nvidia-driver |\n\t\tawk '$1 ~ /^nvidia-driver-([0-9]+)$/ { print $1 }' |\n\t\tsort -V\n))\n\nif [[ \"${#AVAILABLE_DRIVERS[@]}\" -eq 0 ]]; then\n\tabort \"No available drivers found from APT.\"\nfi\n\nLATEST_DRIVER=\"${AVAILABLE_DRIVERS[-1]}\"\nLATEST_DRIVER_VERSION=\"$(apt-candidate-version \"${LATEST_DRIVER}\")\"\n\nINSTALLED_DRIVER=\"$(apt-list-packages | awk '$2 ~ /nvidia-driver-([0-9]+)$/ { print $2 }')\"\nif [[ -n \"${INSTALLED_DRIVER}\" ]]; then\n\tINSTALLED_DRIVER_VERSION=\"$(apt-installed-version \"${INSTALLED_DRIVER}\")\"\n\tINSTALLED_DRIVER_CANDIDATE_VERSION=\"$(apt-candidate-version \"${INSTALLED_DRIVER}\")\"\nelse\n\tINSTALLED_DRIVER_VERSION=''\n\tINSTALLED_DRIVER_CANDIDATE_VERSION=''\nfi\n\nif [[ -z \"${REQUESTED_DRIVER}\" ]]; then\n\tif [[ -n \"${LATEST}\" || -z \"${INSTALLED_DRIVER}\" ]]; then\n\t\tREQUESTED_DRIVER=\"${LATEST_DRIVER}\"\n\t\tREQUESTED_DRIVER_VERSION=\"${LATEST_DRIVER_VERSION}\"\n\telse\n\t\tREQUESTED_DRIVER=\"${INSTALLED_DRIVER}\"\n\t\tREQUESTED_DRIVER_VERSION=\"${INSTALLED_DRIVER_CANDIDATE_VERSION}\"\n\tfi\nelse\n\tREQUESTED_DRIVER_VERSION=\"$(apt-candidate-version \"${REQUESTED_DRIVER}\")\"\n\tif [[ -z \"${REQUESTED_DRIVER_VERSION}\" ]]; then\n\t\tabort \"Unable to locate package ${REQUESTED_DRIVER}.\"\n\tfi\nfi\n\nohai \"Available NVIDIA drivers:\"\nfor driver in \"${AVAILABLE_DRIVERS[@]}\"; do\n\tprefix=\"    \"\n\tif [[ \"${driver}\" == \"${REQUESTED_DRIVER}\" ]]; then\n\t\tprefix=\"--> \"\n\tfi\n\tif [[ \"${driver}\" == \"${INSTALLED_DRIVER}\" ]]; then\n\t\tif [[ \"${driver}\" != \"${REQUESTED_DRIVER}\" ]]; then\n\t\t\tprefix=\"<-- \"\n\t\telif [[ \"${REQUESTED_DRIVER_VERSION}\" != \"${INSTALLED_DRIVER_VERSION}\" ]]; then\n\t\t\tprefix=\"--> \"\n\t\telse\n\t\t\tprefix=\"--- \"\n\t\tfi\n\t\tif [[ \"${INSTALLED_DRIVER_VERSION}\" == \"${INSTALLED_DRIVER_CANDIDATE_VERSION}\" ]]; then\n\t\t\tif [[ \"${driver}\" == \"${LATEST_DRIVER}\" ]]; then\n\t\t\t\techo \"${prefix}${tty_green}${driver} [${INSTALLED_DRIVER_VERSION}]${tty_reset} ${tty_yellow}[installed]${tty_reset} (up-to-date)\"\n\t\t\telse\n\t\t\t\techo \"${prefix}${tty_bold}${driver} [${INSTALLED_DRIVER_VERSION}]${tty_reset} ${tty_yellow}[installed]${tty_reset} (up-to-date)\"\n\t\t\tfi\n\t\telse\n\t\t\techo \"${prefix}${tty_bold}${driver} [${INSTALLED_DRIVER_VERSION}]${tty_reset} ${tty_yellow}[installed]${tty_reset} (upgradable to [${INSTALLED_DRIVER_CANDIDATE_VERSION}])\"\n\t\tfi\n\telif [[ \"${driver}\" == \"${LATEST_DRIVER}\" ]]; then\n\t\techo \"${prefix}${tty_green}${driver} [${LATEST_DRIVER_VERSION}]${tty_reset} (latest)\"\n\telse\n\t\techo \"${prefix}${driver} [$(apt-candidate-version \"${driver}\")]\"\n\tfi\ndone\n\nif [[ \"${INSTALLED_DRIVER}@${INSTALLED_DRIVER_VERSION}\" == \"${REQUESTED_DRIVER}@${REQUESTED_DRIVER_VERSION}\" ]]; then\n\techo\n\tohai \"Your NVIDIA driver is already up-to-date.\"\n\texit\nelif [[ \"${INSTALLED_DRIVER}\" == \"${REQUESTED_DRIVER}\" && -z \"${UPGRADE_ONLY}\" && -z \"${LATEST}\" ]]; then\n\techo\n\tohai \"The requested driver ${REQUESTED_DRIVER} is already installed. Run \\`bash $(basename \"$0\") --upgrade-only\\` to upgrade.\"\n\texit\nelif [[ -n \"${DRY_RUN}\" ]]; then\n\texit\nfi\n\necho\n\n### Show the installation plan and wait for user confirmation ######################################\n\nif [[ -z \"${INSTALLED_DRIVER}\" ]]; then\n\tohai \"Install the NVIDIA driver ${REQUESTED_DRIVER} [${REQUESTED_DRIVER_VERSION}].\"\nelif [[ \"${REQUESTED_DRIVER#nvidia-driver-}\" -ge \"${INSTALLED_DRIVER#nvidia-driver-}\" ]]; then\n\tohai \"Upgrade the NVIDIA driver from ${INSTALLED_DRIVER} [${INSTALLED_DRIVER_VERSION}] to ${REQUESTED_DRIVER} [${REQUESTED_DRIVER_VERSION}].\"\nelse\n\tohai \"Downgrade the NVIDIA driver from ${INSTALLED_DRIVER} [${INSTALLED_DRIVER_VERSION}] to ${REQUESTED_DRIVER} [${REQUESTED_DRIVER_VERSION}].\"\nfi\n\nDM_SERVICES=()\nif [[ -n \"$(sudo lsof -t /dev/nvidia* 2>/dev/null || true)\" ]]; then\n\tfor dm in gdm3 lightdm; do\n\t\tif service \"${dm}\" status &>/dev/null; then\n\t\t\tDM_SERVICES+=(\"${dm}\")\n\t\tfi\n\tdone\nfi\n\nif [[ \"${#DM_SERVICES[@]}\" -gt 0 ]]; then\n\tif [[ \"${#DM_SERVICES[@]}\" -gt 1 ]]; then\n\t\twarn \"The following display manager services are running:\"\n\telse\n\t\twarn \"The following display manager service is running:\"\n\tfi\n\tprintf \"  - %s\\n\" \"${DM_SERVICES[@]}\"\n\techo \"The service will be stopped during the installation which may shut down the GUI desktop. The service will be restarted after the installation.\"\nfi\n\nif [[ -z \"${YES}\" ]]; then\n\tring_bell\n\tCONFIRM_MESSAGE=\"Press ${tty_bold}RETURN${tty_reset}/${tty_bold}ENTER${tty_reset} to continue or any other key to abort:\" wait_for_user\nfi\n\n### Do install/upgrade the requested driver package ################################################\n\n# Stop display manager services\nfor dm in \"${DM_SERVICES[@]}\"; do\n\texec_cmd \"sudo service ${dm} stop\"\n\t# shellcheck disable=SC2064\n\ttrap \"exec_cmd 'sudo service ${dm} start'\" EXIT # restart the service on exit\ndone\n\n# Disable persistence mode\nif [[ -n \"$(sudo lsmod | grep '^nvidia' | awk '{ print $1 }')\" ]]; then\n\texec_cmd \"sudo nvidia-smi -pm 0 || true\"\nfi\n\nsleep 1 # ensure the processes are stopped\n\n# Ensure no processes are using the NVIDIA devices\n# shellcheck disable=SC2207\nPIDS=($(sudo lsof -t /dev/nvidia* 2>/dev/null || true))\nif [[ \"${#PIDS[@]}\" -gt 0 ]]; then\n\tcat >&2 <<EOS\n\n${tty_red}FATAL:${tty_white} Some processes are still using the NVIDIA devices.${tty_reset}\n\n$(ps --format=pid,user,command --pid=\"${PIDS[*]}\")\nEOS\n\n\tif [[ -z \"${YES}\" ]]; then\n\t\tring_bell\n\t\tCONFIRM_MESSAGE=\"Press ${tty_bold}RETURN${tty_reset}/${tty_bold}ENTER${tty_reset} to kill (\\`kill -9\\`) and continue or any other key to abort:\" wait_for_user\n\tfi\n\n\texec_cmd \"sudo kill -9 ${PIDS[*]}\"\n\tsleep 3 # ensure the processes are stopped\nfi\n\n# Offload the NVIDIA kernel modules\nMODULES=\"$(sudo lsmod | grep '^nvidia' | awk '{ print $1 }')\"\nMODULES=\"${MODULES//$'\\n'/ }\"\nif [[ -n \"${MODULES}\" ]]; then\n\texec_cmd \"sudo modprobe -r -f ${MODULES}\"\nfi\n\nif [[ -n \"${INSTALLED_DRIVER}\" ]]; then\n\tNVIDIA_PACKAGES=\"$(apt-list-nvidia-packages)\"\n\tif [[ -n \"${NVIDIA_PACKAGES}\" ]]; then\n\t\texec_cmd \"sudo apt-mark unhold ${NVIDIA_PACKAGES}\"\n\tfi\n\tif [[ \"${INSTALLED_DRIVER}\" == \"${REQUESTED_DRIVER}\" ]]; then\n\t\t# Upgrade the existing driver packages\n\t\texec_cmd \"sudo apt-get install --only-upgrade --yes ${NVIDIA_PACKAGES}\"\n\telse\n\t\t# Uninstall the existing driver packages\n\t\texec_cmd \"sudo apt-get purge --yes ${NVIDIA_PACKAGES}\"\n\tfi\nfi\n\nif [[ \"${INSTALLED_DRIVER}\" != \"${REQUESTED_DRIVER}\" ]]; then\n\t# Install the required driver packages\n\texec_cmd \"sudo apt-get install --yes ${REQUESTED_DRIVER}\"\nfi\n\n# Hold the NVIDIA driver packages for `apt upgrade`\nNVIDIA_PACKAGES=\"$(apt-list-nvidia-packages)\"\nif [[ -n \"${NVIDIA_PACKAGES}\" ]]; then\n\texec_cmd \"sudo apt-mark hold ${NVIDIA_PACKAGES}\"\nfi\n\n### Reload the NVIDIA kernel modules ###############################################################\n\nexec_cmd 'nvidia-smi'\n\n# Enable persistence mode\nexec_cmd \"sudo nvidia-smi -pm 1 || true\"\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "install-nvidia-driver.sh", "language": "bash", "complexity": "complex", "star_count": 5792, "quality_score": 0.73, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 529}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# nvitop-exporter\n\nPrometheus exporter built on top of `nvitop`.\n\n## Quickstart\n\nStart the exporter with the following command:\n\n```bash\nuvx nvitop-exporter --bind-address 0.0.0.0 --port 5050\n# or\npipx run nvitop-exporter --bind-address 0.0.0.0 --port 5050\n```\n\nThen you can access the metrics at [`http://localhost:5050/metrics`](http://localhost:5050/metrics).\n\nYou will need to configure Prometheus to scrape the metrics from the exporter.\n\n```yaml\nscrape_configs:\n  - job_name: 'nvitop-exporter'\n    static_configs:\n      - targets: ['localhost:5050']\n```\n\n## Grafana Dashboard\n\nA Grafana dashboard is provided to visualize the metrics collected by the exporter.\nThe source of the dashboard is [`dashboard.json`](grafana/dashboard.json).\nThe Grafana dashboard can also be imported as by ID [22589](https://grafana.com/grafana/dashboards/22589-nvitop-dashboard).\n\nIf you are using [`docker-compose`](https://docs.docker.com/compose), you can start a dashboard at [`http://localhost:3000`](http://localhost:3000) with the following command:\n\n```bash\ncd nvitop-exporter/grafana\ndocker compose up --build --detach\n```\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://github.com/user-attachments/assets/e4867e64-2ca9-45bc-b524-929053f9673d\" alt=\"Grafana Dashboard\">\n  <br/>\n  The Grafana dashboard for the exporter.\n</p>\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "nvitop-exporter/README.md", "language": "markdown", "complexity": "simple", "star_count": 5792, "quality_score": 0.76, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 44}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "apiVersion: 1\nproviders:\n  - name: 'nvitop-exporter'\n    folder: nvitop-exporter\n    type: file\n    allowUiUpdates: true\n    options:\n      path: /grafana/dashboards\n      foldersFromFilesStructure: true\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "nvitop-exporter/grafana/dashboard.yml", "language": "yaml", "complexity": "simple", "star_count": 5792, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 10}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "apiVersion: 1\ndatasources:\n  - name: nvitop-prometheus\n    type: prometheus\n    access: proxy\n    uid: nvitop-prometheus\n    url: http://prometheus:9090\n    isDefault: true\n    version: 1\n    editable: true\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "nvitop-exporter/grafana/datasource.yml", "language": "yaml", "complexity": "simple", "star_count": 5792, "quality_score": 0.7, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 11}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "services:\n  prometheus:\n    image: prom/prometheus:latest\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n      - prometheus_storage:/var/lib/prometheus\n    restart: unless-stopped\n\n  grafana:\n    build:\n      dockerfile: Dockerfile\n      context: .\n    volumes:\n      - ./datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml\n      - ./dashboard.yml:/etc/grafana/provisioning/dashboards/dashboard.yml\n      - grafana_storage:/var/lib/grafana\n    environment:\n      - GF_AUTH_ANONYMOUS_ENABLED=true\n      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin\n      - GF_AUTH_DISABLE_LOGIN_FORM=true\n    ports:\n      - 3000:3000\n    restart: unless-stopped\n    depends_on:\n      - prometheus\n\n  nvitop-exporter:\n    build:\n      dockerfile: Dockerfile\n      context: ..\n    command: [\"--bind-address\", \"0.0.0.0\", \"--port\", \"5050\"]\n    pid: host\n    restart: unless-stopped\n    runtime: nvidia\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - capabilities: [gpu]\n\nvolumes:\n  prometheus_storage: {}\n  grafana_storage: {}\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "nvitop-exporter/grafana/docker-compose.yml", "language": "yaml", "complexity": "medium", "star_count": 5792, "quality_score": 0.7, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 44}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This file is part of nvitop, the interactive NVIDIA-GPU process viewer.\n#\n# Copyright 2021-2025 Xuehai Pan. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Prometheus exporter built on top of ``nvitop``.\"\"\"\n\nfrom nvitop_exporter.exporter import PrometheusExporter\nfrom nvitop_exporter.utils import get_ip_address\nfrom nvitop_exporter.version import __version__\n\n\n__all__ = ['PrometheusExporter', 'get_ip_address']\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "nvitop-exporter/nvitop_exporter/__init__.py", "language": "python", "complexity": "simple", "star_count": 5792, "quality_score": 0.7899999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 25}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This file is part of nvitop, the interactive NVIDIA-GPU process viewer.\n#\n# Copyright 2021-2025 Xuehai Pan. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Prometheus exporter built on top of ``nvitop``.\"\"\"\n\nimport sys\n\nfrom nvitop_exporter.cli import main\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "nvitop-exporter/nvitop_exporter/__main__.py", "language": "python", "complexity": "simple", "star_count": 5792, "quality_score": 0.7899999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This file is part of nvitop, the interactive NVIDIA-GPU process viewer.\n#\n# Copyright 2021-2025 Xuehai Pan. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Prometheus exporter built on top of ``nvitop``.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport sys\nfrom typing import TextIO\n\nfrom prometheus_client import start_wsgi_server\n\nimport nvitop\nfrom nvitop import Device, colored, libnvml\nfrom nvitop_exporter.exporter import PrometheusExporter\nfrom nvitop_exporter.utils import get_ip_address\nfrom nvitop_exporter.version import __version__\n\n\ndef cprint(text: str = '', *, file: TextIO | None = None) -> None:\n    \"\"\"Print colored text to a file.\"\"\"\n    for prefix, color in (\n        ('INFO: ', 'yellow'),\n        ('WARNING: ', 'yellow'),\n        ('ERROR: ', 'red'),\n        ('NVML ERROR: ', 'red'),\n    ):\n        if text.startswith(prefix):\n            text = text.replace(\n                prefix.rstrip(),\n                colored(prefix.rstrip(), color=color, attrs=('bold',)),  # type: ignore[arg-type]\n                1,\n            )\n    print(text, file=file)\n\n\ndef parse_arguments() -> argparse.Namespace:\n    \"\"\"Parse command-line arguments for ``nvitop-exporter``.\"\"\"\n\n    def posfloat(argstring: str) -> float:\n        num = float(argstring)\n        if num <= 0:\n            raise ValueError\n        return num\n\n    posfloat.__name__ = 'positive float'\n\n    parser = argparse.ArgumentParser(\n        prog='nvitop-exporter',\n        description='Prometheus exporter built on top of `nvitop`.',\n        formatter_class=argparse.RawTextHelpFormatter,\n        add_help=False,\n    )\n    parser.add_argument(\n        '--help',\n        '-h',\n        dest='help',\n        action='help',\n        default=argparse.SUPPRESS,\n        help='Show this help message and exit.',\n    )\n    parser.add_argument(\n        '--version',\n        '-V',\n        dest='version',\n        action='version',\n        version=f'%(prog)s {__version__} (nvitop {nvitop.__version__})',\n        help=\"Show %(prog)s's version number and exit.\",\n    )\n\n    parser.add_argument(\n        '--hostname',\n        '--host',\n        '-H',\n        dest='hostname',\n        type=str,\n        default=get_ip_address(),\n        metavar='HOSTNAME',\n        help='Hostname to display in the exporter. (default: %(default)s)',\n    )\n    parser.add_argument(\n        '--bind-address',\n        '--bind',\n        '-B',\n        dest='bind_address',\n        type=str,\n        default='127.0.0.1',\n        metavar='ADDRESS',\n        help='Local address to bind to. (default: %(default)s)',\n    )\n    parser.add_argument(\n        '--port',\n        '-p',\n        type=int,\n        default=8000,\n        help='Port to listen on. (default: %(default)d)',\n    )\n    parser.add_argument(\n        '--interval',\n        dest='interval',\n        type=posfloat,\n        default=1.0,\n        metavar='SEC',\n        help='Interval between updates in seconds. (default: %(default)s)',\n    )\n\n    args = parser.parse_args()\n    if args.interval < 0.25:\n        parser.error(\n            f'the interval {args.interval:0.2g}s is too short, which may cause performance issues. '\n            f'Expected 1/4 or higher.',\n        )\n\n    return args\n\n\ndef main() -> int:  # pylint: disable=too-many-locals,too-many-statements\n    \"\"\"Main function for ``nvitop-exporter`` CLI.\"\"\"\n    args = parse_arguments()\n\n    try:\n        device_count = Device.count()\n    except libnvml.NVMLError_LibraryNotFound:\n        return 1\n    except libnvml.NVMLError as ex:\n        cprint(f'NVML ERROR: {ex}', file=sys.stderr)\n        return 1\n\n    if device_count == 0:\n        cprint('NVML ERROR: No NVIDIA devices found.', file=sys.stderr)\n        return 1\n\n    physical_devices = Device.from_indices(range(device_count))\n    mig_devices = []\n    for device in physical_devices:\n        mig_devices.extend(device.mig_devices())\n    cprint(\n        'INFO: Found {}{}.'.format(\n            colored(str(device_count), color='green', attrs=('bold',)),\n            (\n                ' physical device(s) and {} MIG device(s)'.format(\n                    colored(str(len(mig_devices)), color='blue', attrs=('bold',)),\n                )\n                if mig_devices\n                else ' device(s)'\n            ),\n        ),\n        file=sys.stderr,\n    )\n\n    devices = sorted(\n        physical_devices + mig_devices,  # type: ignore[operator]\n        key=lambda d: (d.index,) if isinstance(d.index, int) else d.index,\n    )\n    for device in devices:\n        name = device.name()\n        uuid = device.uuid()\n        if device.is_mig_device():\n            name = name.rpartition(' ')[-1]\n            cprint(\n                f'INFO:   MIG {name:<11} Device {device.mig_index:>2d}: (UUID: {uuid})',\n                file=sys.stderr,\n            )\n        else:\n            cprint(f'INFO: GPU {device.index}: {name} (UUID: {uuid})', file=sys.stderr)\n\n    exporter = PrometheusExporter(devices, hostname=args.hostname, interval=args.interval)\n\n    try:\n        start_wsgi_server(port=args.port, addr=args.bind_address)\n    except OSError as ex:\n        if 'address already in use' in str(ex).lower():\n            cprint(\n                (\n                    'ERROR: Address {} is already in use. '\n                    'Please specify a different port via `--port <PORT>`.'\n                ).format(\n                    colored(\n                        f'http://{args.bind_address}:{args.port}',\n                        color='blue',\n                        attrs=('bold', 'underline'),\n                    ),\n                ),\n                file=sys.stderr,\n            )\n        elif 'cannot assign requested address' in str(ex).lower():\n            cprint(\n                (\n                    'ERROR: Cannot assign requested address at {}. '\n                    'Please specify a different address via `--bind-address <ADDRESS>`.'\n                ).format(\n                    colored(\n                        f'http://{args.bind_address}:{args.port}',\n                        color='blue',\n                        attrs=('bold', 'underline'),\n                    ),\n                ),\n                file=sys.stderr,\n            )\n        else:\n            cprint(f'ERROR: {ex}', file=sys.stderr)\n        return 1\n\n    cprint(\n        'INFO: Start the exporter on {} at {}.'.format(\n            colored(args.hostname, color='magenta', attrs=('bold',)),\n            colored(\n                f'http://{args.bind_address}:{args.port}/metrics',\n                color='green',\n                attrs=('bold', 'underline'),\n            ),\n        ),\n        file=sys.stderr,\n    )\n\n    try:\n        exporter.collect()\n    except KeyboardInterrupt:\n        cprint(file=sys.stderr)\n        cprint('INFO: Interrupted by user.', file=sys.stderr)\n\n    return 0\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "nvitop-exporter/nvitop_exporter/cli.py", "language": "python", "complexity": "complex", "star_count": 5792, "quality_score": 0.85, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 241}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This file is part of nvitop, the interactive NVIDIA-GPU process viewer.\n#\n# Copyright 2021-2025 Xuehai Pan. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Prometheus exporter built on top of ``nvitop``.\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nimport time\nfrom typing import Sequence\n\nfrom prometheus_client import REGISTRY, CollectorRegistry, Gauge, Info\n\nfrom nvitop import Device, GpuProcess, MiB, MigDevice, PhysicalDevice, host\nfrom nvitop_exporter.utils import get_ip_address\n\n\nclass PrometheusExporter:  # pylint: disable=too-many-instance-attributes\n    \"\"\"Prometheus exporter built on top of ``nvitop``.\"\"\"\n\n    def __init__(  # pylint: disable=too-many-statements\n        self,\n        devices: Sequence[Device],\n        hostname: str | None = None,\n        *,\n        registry: CollectorRegistry = REGISTRY,\n        interval: float = 1.0,\n    ) -> None:\n        \"\"\"Initialize the Prometheus exporter.\"\"\"\n        if not isinstance(devices, (list, tuple)):\n            raise TypeError(f'Expected a list or tuple of devices, got {type(devices)}')\n        devices = list(devices)\n\n        for device in devices:\n            if not isinstance(device, (PhysicalDevice, MigDevice)):\n                raise TypeError(f'Expected a PhysicalDevice or MigDevice, got {type(device)}')\n\n        self.devices = devices\n        self.hostname = hostname or get_ip_address()\n        self.registry = registry\n        self.interval = interval\n        self.alive_pids: dict[Device, set[tuple[int, str]]] = {\n            device: set() for device in self.devices\n        }\n\n        self.info = Info(\n            'nvitop',\n            documentation='NVITOP Prometheus Exporter.',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.info.labels(hostname=self.hostname).info(\n            {\n                'device_count': str(Device.count()),\n                'driver_version': Device.driver_version(),\n                'cuda_driver_version': Device.cuda_driver_version(),\n            },\n        )\n\n        # Create gauges for host metrics\n        self.host_uptime = Gauge(\n            name='host_uptime',\n            documentation='Host uptime (s).',\n            unit='Second',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.host_cpu_percent = Gauge(\n            name='host_cpu_percent',\n            documentation='Host CPU percent (%).',\n            unit='Percentage',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.host_virtual_memory_total = Gauge(\n            name='host_virtual_memory_total',\n            documentation='Host virtual memory total (MiB).',\n            unit='MiB',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.host_virtual_memory_used = Gauge(\n            name='host_virtual_memory_used',\n            documentation='Host virtual memory used (MiB).',\n            unit='MiB',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.host_virtual_memory_free = Gauge(\n            name='host_virtual_memory_free',\n            documentation='Host virtual memory free (MiB).',\n            unit='MiB',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.host_virtual_memory_percent = Gauge(\n            name='host_virtual_memory_percent',\n            documentation='Host virtual memory percent (%).',\n            unit='Percentage',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.host_swap_memory_total = Gauge(\n            name='host_swap_memory_total',\n            documentation='Host swap total (MiB).',\n            unit='MiB',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.host_swap_memory_used = Gauge(\n            name='host_swap_memory_used',\n            documentation='Host swap used (MiB).',\n            unit='MiB',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.host_swap_memory_free = Gauge(\n            name='host_swap_memory_free',\n            documentation='Host swap free (MiB).',\n            unit='MiB',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.host_swap_memory_percent = Gauge(\n            name='host_swap_memory_percent',\n            documentation='Host swap percent (%).',\n            unit='Percentage',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.host_load_average_1m = Gauge(\n            name='host_load_average_1m',\n            documentation='Host load average for the last minute.',\n            unit='Percentage',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.host_load_average_5m = Gauge(\n            name='host_load_average_5m',\n            documentation='Host load average for the last 5 minutes.',\n            unit='Percentage',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.host_load_average_15m = Gauge(\n            name='host_load_average_15m',\n            documentation='Host load average for the last 15 minutes.',\n            unit='Percentage',\n            labelnames=['hostname'],\n            registry=self.registry,\n        )\n        self.host_net_io_tx_data = Gauge(\n            name='host_net_io_tx_data',\n            documentation='Host network I/O transmitted data (MiB).',\n            unit='MiB',\n            labelnames=['hostname', 'interface'],\n            registry=self.registry,\n        )\n        self.host_net_io_rx_data = Gauge(\n            name='host_net_io_rx_data',\n            documentation='Host network I/O received data (MiB).',\n            unit='MiB',\n            labelnames=['hostname', 'interface'],\n            registry=self.registry,\n        )\n        self.host_net_io_tx_packets = Gauge(\n            name='host_net_io_tx_packets',\n            documentation='Host network I/O transmitted packets.',\n            unit='Packet',\n            labelnames=['hostname', 'interface'],\n            registry=self.registry,\n        )\n        self.host_net_io_rx_packets = Gauge(\n            name='host_net_io_rx_packets',\n            documentation='Host network I/O received packets.',\n            unit='Packet',\n            labelnames=['hostname', 'interface'],\n            registry=self.registry,\n        )\n        self.host_disk_io_read_data = Gauge(\n            name='host_disk_io_read_data',\n            documentation='Host disk I/O read data (MiB).',\n            unit='MiB',\n            labelnames=['hostname', 'partition'],\n            registry=self.registry,\n        )\n        self.host_disk_io_write_data = Gauge(\n            name='host_disk_io_write_data',\n            documentation='Host disk I/O write data (MiB).',\n            unit='MiB',\n            labelnames=['hostname', 'partition'],\n            registry=self.registry,\n        )\n        self.host_disk_usage_total = Gauge(\n            name='host_disk_usage_total',\n            documentation='Host disk usage total (MiB).',\n            unit='MiB',\n            labelnames=['hostname', 'mountpoint'],\n            registry=self.registry,\n        )\n        self.host_disk_usage_used = Gauge(\n            name='host_disk_usage_used',\n            documentation='Host disk usage used (MiB).',\n            unit='MiB',\n            labelnames=['hostname', 'mountpoint'],\n            registry=self.registry,\n        )\n        self.host_disk_usage_free = Gauge(\n            name='host_disk_usage_free',\n            documentation='Host disk usage free (MiB).',\n            unit='MiB',\n            labelnames=['hostname', 'mountpoint'],\n            registry=self.registry,\n        )\n        self.host_disk_usage_percent = Gauge(\n            name='host_disk_usage_percent',\n            documentation='Host disk usage percent (%).',\n            unit='Percentage',\n            labelnames=['hostname', 'mountpoint'],\n            registry=self.registry,\n        )\n\n        # Create gauges for GPU metrics\n        self.gpu_utilization = Gauge(\n            name='gpu_utilization',\n            documentation='GPU utilization (%).',\n            unit='Percentage',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_memory_utilization = Gauge(\n            name='gpu_memory_utilization',\n            documentation='GPU memory utilization (%).',\n            unit='Percentage',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_encoder_utilization = Gauge(\n            name='gpu_encoder_utilization',\n            documentation='GPU encoder utilization (%).',\n            unit='Percentage',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_decoder_utilization = Gauge(\n            name='gpu_decoder_utilization',\n            documentation='GPU decoder utilization (%).',\n            unit='Percentage',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_memory_total = Gauge(\n            name='gpu_memory_total',\n            documentation='GPU memory total (MiB).',\n            unit='MiB',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_memory_used = Gauge(\n            name='gpu_memory_used',\n            documentation='GPU memory used (MiB).',\n            unit='MiB',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_memory_free = Gauge(\n            name='gpu_memory_free',\n            documentation='GPU memory free (MiB).',\n            unit='MiB',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_memory_percent = Gauge(\n            name='gpu_memory_percent',\n            documentation='GPU memory percent (%).',\n            unit='Percentage',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_clock_sm = Gauge(\n            name='gpu_clock_sm',\n            documentation='GPU SM clock (MHz).',\n            unit='MHz',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_clock_memory = Gauge(\n            name='gpu_clock_memory',\n            documentation='GPU memory clock (MHz).',\n            unit='MHz',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_clock_graphics = Gauge(\n            name='gpu_clock_graphics',\n            documentation='GPU graphics clock (MHz).',\n            unit='MHz',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_clock_video = Gauge(\n            name='gpu_clock_video',\n            documentation='GPU video clock (MHz).',\n            unit='MHz',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_power_usage = Gauge(\n            name='gpu_power_usage',\n            documentation='GPU power usage (W).',\n            unit='W',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_power_limit = Gauge(\n            name='gpu_power_limit',\n            documentation='GPU power limit (W).',\n            unit='W',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_temperature = Gauge(\n            name='gpu_temperature',\n            documentation='GPU temperature (C).',\n            unit='C',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_fan_speed = Gauge(\n            name='gpu_fan_speed',\n            documentation='GPU fan speed (%).',\n            unit='Percentage',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_pcie_tx_throughput = Gauge(\n            name='gpu_pcie_tx_throughput',\n            documentation='GPU PCIe transmit throughput (MiB/s).',\n            unit='MiBps',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_pcie_rx_throughput = Gauge(\n            name='gpu_pcie_rx_throughput',\n            documentation='GPU PCIe receive throughput (MiB/s).',\n            unit='MiBps',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_nvlink_total_tx_throughput = Gauge(\n            name='gpu_nvlink_total_tx_throughput',\n            documentation='GPU total NVLink transmit throughput (MiB/s).',\n            unit='MiBps',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_nvlink_total_rx_throughput = Gauge(\n            name='gpu_nvlink_total_rx_throughput',\n            documentation='GPU total NVLink receive throughput (MiB/s).',\n            unit='MiBps',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_nvlink_mean_tx_throughput = Gauge(\n            name='gpu_nvlink_mean_tx_throughput',\n            documentation='GPU mean NVLink transmit throughput (MiB/s).',\n            unit='MiBps',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_nvlink_mean_rx_throughput = Gauge(\n            name='gpu_nvlink_mean_rx_throughput',\n            documentation='GPU mean NVLink receive throughput (MiB/s).',\n            unit='MiBps',\n            labelnames=['hostname', 'index', 'devicename', 'uuid'],\n            registry=self.registry,\n        )\n        self.gpu_nvlink_tx_throughput = Gauge(\n            name='gpu_nvlink_tx_throughput',\n            documentation='GPU NVLink transmit throughput (MiB/s).',\n            unit='MiBps',\n            labelnames=['hostname', 'index', 'devicename', 'uuid', 'link'],\n            registry=self.registry,\n        )\n        self.gpu_nvlink_rx_throughput = Gauge(\n            name='gpu_nvlink_rx_throughput',\n            documentation='GPU NVLink receive throughput (MiB/s).',\n            unit='MiBps',\n            labelnames=['hostname', 'index', 'devicename', 'uuid', 'link'],\n            registry=self.registry,\n        )\n\n        # Create gauges for process metrics\n        self.process_info = Info(\n            name='process_info',\n            documentation='Process information.',\n            labelnames=['hostname', 'index', 'devicename', 'uuid', 'pid', 'username'],\n            registry=self.registry,\n        )\n        self.process_running_time = Gauge(\n            name='process_running_time',\n            documentation='Process running time (s).',\n            unit='Second',\n            labelnames=['hostname', 'index', 'devicename', 'uuid', 'pid', 'username'],\n            registry=self.registry,\n        )\n        self.process_cpu_percent = Gauge(\n            name='process_cpu_percent',\n            documentation='Process CPU percent (%).',\n            unit='Percentage',\n            labelnames=['hostname', 'index', 'devicename', 'uuid', 'pid', 'username'],\n            registry=self.registry,\n        )\n        self.process_rss_memory = Gauge(\n            name='process_rss_memory',\n            documentation='Process memory resident set size (MiB).',\n            unit='MiB',\n            labelnames=['hostname', 'index', 'devicename', 'uuid', 'pid', 'username'],\n            registry=self.registry,\n        )\n        self.process_memory_percent = Gauge(\n            name='process_memory_percent',\n            documentation='Process memory percent (%).',\n            unit='Percentage',\n            labelnames=['hostname', 'index', 'devicename', 'uuid', 'pid', 'username'],\n            registry=self.registry,\n        )\n        self.process_gpu_memory = Gauge(\n            name='process_gpu_memory',\n            documentation='Process GPU memory (MiB).',\n            unit='MiB',\n            labelnames=['hostname', 'index', 'devicename', 'uuid', 'pid', 'username'],\n            registry=self.registry,\n        )\n        self.process_gpu_sm_utilization = Gauge(\n            name='process_gpu_sm_utilization',\n            documentation='Process GPU SM utilization (%).',\n            unit='Percentage',\n            labelnames=['hostname', 'index', 'devicename', 'uuid', 'pid', 'username'],\n            registry=self.registry,\n        )\n        self.process_gpu_memory_utilization = Gauge(\n            name='process_gpu_memory_utilization',\n            documentation='Process GPU memory utilization (%).',\n            unit='Percentage',\n            labelnames=['hostname', 'index', 'devicename', 'uuid', 'pid', 'username'],\n            registry=self.registry,\n        )\n        self.process_gpu_encoder_utilization = Gauge(\n            name='process_gpu_encoder_utilization',\n            documentation='Process GPU encoder utilization (%).',\n            unit='Percentage',\n            labelnames=['hostname', 'index', 'devicename', 'uuid', 'pid', 'username'],\n            registry=self.registry,\n        )\n        self.process_gpu_decoder_utilization = Gauge(\n            name='process_gpu_decoder_utilization',\n            documentation='Process GPU decoder utilization (%).',\n            unit='Percentage',\n            labelnames=['hostname', 'index', 'devicename', 'uuid', 'pid', 'username'],\n            registry=self.registry,\n        )\n\n    def collect(self) -> None:\n        \"\"\"Collect metrics.\"\"\"\n        while True:\n            next_update_time = time.monotonic() + self.interval\n            self.update_host()\n            for device in self.devices:\n                self.update_device(device)\n            time.sleep(max(0.0, next_update_time - time.monotonic()))\n\n    def update_host(self) -> None:\n        \"\"\"Update metrics for the host.\"\"\"\n        load_average = host.load_average()\n        if load_average is None:\n            load_average = (0.0, 0.0, 0.0)  # type: ignore[unreachable]\n        virtual_memory = host.virtual_memory()\n        swap_memory = host.swap_memory()\n        net_io_counters = host.net_io_counters(pernic=True)  # type: ignore[attr-defined]\n        disk_io_counters = host.disk_io_counters(perdisk=True)  # type: ignore[attr-defined]\n\n        for gauge, value in (\n            (self.host_uptime, host.uptime()),\n            (self.host_cpu_percent, host.cpu_percent()),\n            (self.host_virtual_memory_total, virtual_memory.total / MiB),\n            (self.host_virtual_memory_used, virtual_memory.used / MiB),\n            (self.host_virtual_memory_free, virtual_memory.free / MiB),\n            (self.host_virtual_memory_percent, virtual_memory.percent),\n            (self.host_swap_memory_total, swap_memory.total / MiB),\n            (self.host_swap_memory_used, swap_memory.used / MiB),\n            (self.host_swap_memory_free, swap_memory.free / MiB),\n            (self.host_swap_memory_percent, swap_memory.percent),\n            (self.host_load_average_1m, load_average[0]),\n            (self.host_load_average_5m, load_average[1]),\n            (self.host_load_average_15m, load_average[2]),\n        ):\n            gauge.labels(self.hostname).set(value)\n\n        for interface, net_io_counter in net_io_counters.items():\n            for gauge, value in (\n                (self.host_net_io_tx_data, net_io_counter.bytes_sent / MiB),\n                (self.host_net_io_rx_data, net_io_counter.bytes_recv / MiB),\n                (self.host_net_io_tx_packets, net_io_counter.packets_sent),\n                (self.host_net_io_rx_packets, net_io_counter.packets_recv),\n            ):\n                gauge.labels(hostname=self.hostname, interface=interface).set(value)\n\n        for partition, disk_io_counter in disk_io_counters.items():\n            for gauge, value in (\n                (self.host_disk_io_read_data, disk_io_counter.read_bytes / MiB),\n                (self.host_disk_io_write_data, disk_io_counter.write_bytes / MiB),\n            ):\n                gauge.labels(hostname=self.hostname, partition=partition).set(value)\n\n        for partition in host.disk_partitions():  # type: ignore[attr-defined]\n            try:\n                partition_usage = host.disk_usage(partition.mountpoint)  # type: ignore[attr-defined]\n            except (OSError, host.PsutilError):\n                continue\n            for gauge, value in (\n                (self.host_disk_usage_total, partition_usage.total / MiB),\n                (self.host_disk_usage_used, partition_usage.used / MiB),\n                (self.host_disk_usage_free, partition_usage.free / MiB),\n                (self.host_disk_usage_percent, partition_usage.percent),\n            ):\n                gauge.labels(hostname=self.hostname, mountpoint=partition.mountpoint).set(value)\n\n    def update_device(self, device: Device) -> None:  # pylint: disable=too-many-locals\n        \"\"\"Update metrics for a single device.\"\"\"\n        index = (\n            str(device.index) if isinstance(device.index, int) else ':'.join(map(str, device.index))\n        )\n        name = device.name()\n        uuid = device.uuid()\n\n        with device.oneshot():\n            for gauge, value in (\n                (self.gpu_utilization, float(device.gpu_utilization())),\n                (self.gpu_memory_utilization, float(device.memory_utilization())),\n                (self.gpu_encoder_utilization, float(device.encoder_utilization())),\n                (self.gpu_decoder_utilization, float(device.decoder_utilization())),\n                (self.gpu_memory_total, device.memory_total() / MiB),\n                (self.gpu_memory_used, device.memory_used() / MiB),\n                (self.gpu_memory_free, device.memory_free() / MiB),\n                (self.gpu_memory_percent, float(device.memory_percent())),\n                (self.gpu_clock_sm, float(device.clock_infos().sm)),\n                (self.gpu_clock_memory, float(device.clock_infos().memory)),\n                (self.gpu_clock_graphics, float(device.clock_infos().graphics)),\n                (self.gpu_clock_video, float(device.clock_infos().video)),\n                (self.gpu_power_usage, device.power_usage() / 1000.0),\n                (self.gpu_power_limit, device.power_limit() / 1000.0),\n                (self.gpu_temperature, float(device.temperature())),\n                (self.gpu_fan_speed, float(device.fan_speed())),\n                (self.gpu_pcie_tx_throughput, device.pcie_tx_throughput() / 1024.0),\n                (self.gpu_pcie_rx_throughput, device.pcie_rx_throughput() / 1024.0),\n                (self.gpu_nvlink_total_tx_throughput, device.nvlink_total_tx_throughput() / 1024.0),\n                (self.gpu_nvlink_total_rx_throughput, device.nvlink_total_rx_throughput() / 1024.0),\n                (self.gpu_nvlink_mean_tx_throughput, device.nvlink_mean_tx_throughput() / 1024.0),\n                (self.gpu_nvlink_mean_rx_throughput, device.nvlink_mean_rx_throughput() / 1024.0),\n            ):\n                gauge.labels(\n                    hostname=self.hostname,\n                    index=index,\n                    devicename=name,\n                    uuid=uuid,\n                ).set(value)\n\n            for gauge, nvlink_throughput in (\n                (self.gpu_nvlink_tx_throughput, device.nvlink_tx_throughput()),\n                (self.gpu_nvlink_rx_throughput, device.nvlink_rx_throughput()),\n            ):\n                for link, throughput in enumerate(nvlink_throughput):\n                    gauge.labels(\n                        hostname=self.hostname,\n                        index=index,\n                        devicename=name,\n                        uuid=uuid,\n                        link=link,\n                    ).set(throughput / 1024.0)\n\n        alive_pids = self.alive_pids[device]\n        previous_alive_pids = alive_pids.copy()\n        alive_pids.clear()\n\n        with GpuProcess.failsafe():\n            host_snapshots = {}\n            for pid, process in device.processes().items():\n                with process.oneshot():\n                    username = process.username()\n                    if (pid, username) not in host_snapshots:  # noqa: SIM401,RUF100\n                        host_snapshot = host_snapshots[pid, username] = process.host_snapshot()\n                    else:\n                        host_snapshot = host_snapshots[pid, username]\n                    self.process_info.labels(\n                        hostname=self.hostname,\n                        index=index,\n                        devicename=name,\n                        uuid=uuid,\n                        pid=pid,\n                        username=username,\n                    ).info(\n                        {\n                            'status': host_snapshot.status,\n                            'command': host_snapshot.command,\n                        },\n                    )\n                    for gauge, value in (\n                        (\n                            self.process_running_time,\n                            (\n                                host_snapshot.running_time.total_seconds()\n                                if host_snapshot.running_time\n                                else math.nan\n                            ),\n                        ),\n                        (self.process_cpu_percent, host_snapshot.cpu_percent),\n                        (self.process_rss_memory, host_snapshot.host_memory / MiB),\n                        (self.process_memory_percent, float(host_snapshot.memory_percent)),\n                        (self.process_gpu_memory, process.gpu_memory() / MiB),\n                        (\n                            self.process_gpu_sm_utilization,\n                            float(process.gpu_sm_utilization()),\n                        ),\n                        (\n                            self.process_gpu_memory_utilization,\n                            float(process.gpu_memory_utilization()),\n                        ),\n                        (\n                            self.process_gpu_encoder_utilization,\n                            float(process.gpu_encoder_utilization()),\n                        ),\n                        (\n                            self.process_gpu_decoder_utilization,\n                            float(process.gpu_decoder_utilization()),\n                        ),\n                    ):\n                        gauge.labels(\n                            hostname=self.hostname,\n                            index=index,\n                            devicename=name,\n                            uuid=uuid,\n                            pid=pid,\n                            username=username,\n                        ).set(value)\n\n        alive_pids.update(host_snapshots)\n        for pid, username in previous_alive_pids.difference(alive_pids):\n            for collector in (\n                self.process_info,\n                self.process_running_time,\n                self.process_cpu_percent,\n                self.process_rss_memory,\n                self.process_memory_percent,\n                self.process_gpu_memory,\n                self.process_gpu_sm_utilization,\n                self.process_gpu_memory_utilization,\n                self.process_gpu_encoder_utilization,\n                self.process_gpu_decoder_utilization,\n            ):\n                try:\n                    collector.remove(\n                        self.hostname,\n                        index,\n                        name,\n                        uuid,\n                        pid,\n                        username,\n                    )\n                except KeyError:  # noqa: PERF203\n                    pass\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "nvitop-exporter/nvitop_exporter/exporter.py", "language": "python", "complexity": "complex", "star_count": 5792, "quality_score": 0.8799999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 685}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This file is part of nvitop, the interactive NVIDIA-GPU process viewer.\n#\n# Copyright 2021-2025 Xuehai Pan. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Prometheus exporter built on top of ``nvitop``.\"\"\"\n\n# pylint: disable=invalid-name\n\n__version__ = '1.5.2'\n__license__ = 'Apache-2.0'\n__author__ = __maintainer__ = 'Xuehai Pan'\n__email__ = 'XuehaiPan@pku.edu.cn'\n__release__ = False\n\nif not __release__:\n    import os\n    import subprocess\n\n    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    try:\n        prefix, sep, suffix = (\n            subprocess.check_output(  # noqa: S603\n                [  # noqa: S607\n                    'git',\n                    f'--git-dir={os.path.join(root_dir, \".git\")}',\n                    'describe',\n                    '--abbrev=7',\n                ],\n                cwd=root_dir,\n                stderr=subprocess.DEVNULL,\n                text=True,\n            )\n            .strip()\n            .lstrip('v')\n            .replace('-', '.dev', 1)\n            .replace('-', '+', 1)\n            .partition('.dev')\n        )\n        if sep:\n            version_prefix, dot, version_tail = prefix.rpartition('.')\n            prefix = f'{version_prefix}{dot}{int(version_tail) + 1}'\n            __version__ = f'{prefix}{sep}{suffix}'\n            del version_prefix, dot, version_tail\n        else:\n            __version__ = prefix\n        del prefix, sep, suffix\n    except (OSError, subprocess.CalledProcessError):\n        pass\n\n    del os, subprocess, root_dir\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "nvitop-exporter/nvitop_exporter/version.py", "language": "python", "complexity": "simple", "star_count": 5792, "quality_score": 0.7899999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 63}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This file is part of nvitop, the interactive NVIDIA-GPU process viewer.\n#\n# Copyright 2021-2025 Xuehai Pan. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"The core APIs of nvitop.\"\"\"\n\nfrom nvitop.api import (\n    caching,\n    collector,\n    device,\n    host,\n    libcuda,\n    libcudart,\n    libnvml,\n    process,\n    termcolor,\n    utils,\n)\nfrom nvitop.api.caching import ttl_cache\nfrom nvitop.api.collector import ResourceMetricCollector, collect_in_background, take_snapshots\nfrom nvitop.api.device import (\n    CudaDevice,\n    CudaMigDevice,\n    Device,\n    MigDevice,\n    PhysicalDevice,\n    normalize_cuda_visible_devices,\n    parse_cuda_visible_devices,\n)\nfrom nvitop.api.libnvml import NVMLError, nvmlCheckReturn\nfrom nvitop.api.process import GpuProcess, HostProcess, command_join\nfrom nvitop.api.utils import (  # explicitly export these to appease mypy\n    NA,\n    SIZE_UNITS,\n    UINT_MAX,\n    ULONGLONG_MAX,\n    GiB,\n    KiB,\n    MiB,\n    NaType,\n    NotApplicable,\n    NotApplicableType,\n    PiB,\n    Snapshot,\n    TiB,\n    boolify,\n    bytes2human,\n    colored,\n    human2bytes,\n    set_color,\n    timedelta2human,\n    utilization2string,\n)\n\n\n__all__ = [  # noqa: RUF022\n    'NVMLError',\n    'nvmlCheckReturn',\n    'libnvml',\n    'libcuda',\n    'libcudart',\n    # nvitop.api.device\n    'Device',\n    'PhysicalDevice',\n    'MigDevice',\n    'CudaDevice',\n    'CudaMigDevice',\n    'parse_cuda_visible_devices',\n    'normalize_cuda_visible_devices',\n    # nvitop.api.process\n    'host',\n    'HostProcess',\n    'GpuProcess',\n    'command_join',\n    # nvitop.api.collector\n    'take_snapshots',\n    'collect_in_background',\n    'ResourceMetricCollector',\n    # nvitop.api.caching\n    'ttl_cache',\n    # nvitop.api.utils\n    'NA',\n    'NaType',\n    'NotApplicable',\n    'NotApplicableType',\n    'UINT_MAX',\n    'ULONGLONG_MAX',\n    'KiB',\n    'MiB',\n    'GiB',\n    'TiB',\n    'PiB',\n    'SIZE_UNITS',\n    'bytes2human',\n    'human2bytes',\n    'timedelta2human',\n    'utilization2string',\n    'colored',\n    'set_color',\n    'boolify',\n    'Snapshot',\n]\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "nvitop/api/__init__.py", "language": "python", "complexity": "medium", "star_count": 5792, "quality_score": 0.7899999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 115}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This file is part of nvitop, the interactive NVIDIA-GPU process viewer.\n#\n# Copyright 2021-2025 Xuehai Pan. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Resource metrics collectors.\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport itertools\nimport math\nimport os\nimport threading\nimport time\nfrom collections import OrderedDict, defaultdict\nfrom typing import TYPE_CHECKING, ClassVar, NamedTuple, TypeVar\nfrom weakref import WeakSet\n\nfrom nvitop.api import host\nfrom nvitop.api.device import CudaDevice, Device\nfrom nvitop.api.process import GpuProcess, HostProcess\nfrom nvitop.api.utils import GiB, MiB, Snapshot\n\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Generator, Iterable\n\n\n__all__ = ['ResourceMetricCollector', 'collect_in_background', 'take_snapshots']\n\n\nclass SnapshotResult(NamedTuple):  # pylint: disable=missing-class-docstring\n    devices: list[Snapshot]\n    gpu_processes: list[Snapshot]\n\n\ntimer = time.monotonic\n\n\n_T = TypeVar('_T')\n\n\ndef _unique(iterable: Iterable[_T], /) -> list[_T]:\n    return list(OrderedDict.fromkeys(iterable).keys())\n\n\n# pylint: disable-next=too-many-branches\ndef take_snapshots(\n    devices: Device | Iterable[Device] | None = None,\n    *,\n    gpu_processes: bool | GpuProcess | Iterable[GpuProcess] | None = None,\n) -> SnapshotResult:\n    \"\"\"Retrieve status of demanded devices and GPU processes.\n\n    Args:\n        devices (Optional[Union[Device, Iterable[Device]]]):\n            Requested devices for snapshots. If not given, the devices will be determined from GPU\n            processes: **(1)** All devices (no GPU processes are given); **(2)** Devices that used\n            by given GPU processes.\n        gpu_processes (Optional[Union[bool, GpuProcess, Iterable[GpuProcess]]]):\n            Requested GPU processes snapshots. If not given, all GPU processes running on the\n            requested device will be returned. The GPU process snapshots can be suppressed by\n            specifying ``gpu_processes=False``.\n\n    Returns: SnapshotResult\n        A named tuple containing two lists of snapshots.\n\n    Note:\n        If not arguments are specified, all devices and all GPU processes will\n        be returned.\n\n    Examples:\n        >>> from nvitop import take_snapshots, Device\n        >>> import os\n        >>> os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n        >>> os.environ['CUDA_VISIBLE_DEVICES'] = '1,0'\n\n        >>> take_snapshots()  # equivalent to `take_snapshots(Device.all())`\n        SnapshotResult(\n            devices=[\n                PhysicalDeviceSnapshot(\n                    real=PhysicalDevice(index=0, ...),\n                    ...\n                ),\n                ...\n            ],\n            gpu_processes=[\n                GpuProcessSnapshot(\n                    real=GpuProcess(pid=xxxxxx, device=PhysicalDevice(index=0, ...), ...),\n                    ...\n                ),\n                ...\n            ]\n        )\n\n        >>> device_snapshots, gpu_process_snapshots = take_snapshots(Device.all())  # type: Tuple[List[DeviceSnapshot], List[GpuProcessSnapshot]]\n\n        >>> device_snapshots, _ = take_snapshots(gpu_processes=False)  # ignore process snapshots\n\n        >>> take_snapshots(Device.cuda.all())  # use CUDA device enumeration\n        SnapshotResult(\n            devices=[\n                CudaDeviceSnapshot(\n                    real=CudaDevice(cuda_index=0, physical_index=1, ...),\n                    ...\n                ),\n                CudaDeviceSnapshot(\n                    real=CudaDevice(cuda_index=1, physical_index=0, ...),\n                    ...\n                ),\n            ],\n            gpu_processes=[\n                GpuProcessSnapshot(\n                    real=GpuProcess(pid=xxxxxx, device=CudaDevice(cuda_index=0, ...), ...),\n                    ...\n                ),\n                ...\n            ]\n        )\n\n        >>> take_snapshots(Device.cuda(1))  # <CUDA 1> only\n        SnapshotResult(\n            devices=[\n                CudaDeviceSnapshot(\n                    real=CudaDevice(cuda_index=1, physical_index=0, ...),\n                    ...\n                )\n            ],\n            gpu_processes=[\n                GpuProcessSnapshot(\n                    real=GpuProcess(pid=xxxxxx, device=CudaDevice(cuda_index=1, ...), ...),\n                    ...\n                ),\n                ...\n            ]\n        )\n    \"\"\"  # pylint: disable=line-too-long\n    if isinstance(devices, Device):\n        devices = [devices]\n    if isinstance(gpu_processes, GpuProcess):\n        gpu_processes = [gpu_processes]\n\n    if gpu_processes is not None and gpu_processes is not True:\n        if gpu_processes:  # is a non-empty list/tuple\n            gpu_processes = list(gpu_processes)\n            process_devices = _unique(process.device for process in gpu_processes)\n            for device in process_devices:\n                device.processes()  # update GPU status for requested GPU processes\n            if devices is None:\n                devices = process_devices\n        else:\n            gpu_processes = []  # False or empty list/tuple\n            if devices is None:\n                devices = Device.all()\n    else:\n        if devices is None:\n            physical_devices = Device.all()\n            devices = []\n            leaf_devices: list[Device] = []\n            for physical_device in physical_devices:\n                devices.append(physical_device)\n                mig_devices = physical_device.mig_devices()\n                if len(mig_devices) > 0:\n                    devices.extend(mig_devices)\n                    leaf_devices.extend(mig_devices)\n                else:\n                    leaf_devices.append(physical_device)\n        else:\n            leaf_devices = devices = list(devices)\n        gpu_processes = list(\n            itertools.chain.from_iterable(device.processes().values() for device in leaf_devices),\n        )\n\n    devices = [device.as_snapshot() for device in devices]  # type: ignore[union-attr]\n    gpu_processes = GpuProcess.take_snapshots(gpu_processes, failsafe=True)\n\n    return SnapshotResult(devices, gpu_processes)\n\n\n# pylint: disable-next=too-many-arguments\ndef collect_in_background(\n    on_collect: Callable[[dict[str, float]], bool],\n    collector: ResourceMetricCollector | None = None,\n    interval: float | None = None,\n    *,\n    on_start: Callable[[ResourceMetricCollector], None] | None = None,\n    on_stop: Callable[[ResourceMetricCollector], None] | None = None,\n    tag: str = 'metrics-daemon',\n    start: bool = True,\n) -> threading.Thread:\n    \"\"\"Start a background daemon thread that collect and call the callback function periodically.\n\n    See also :func:`ResourceMetricCollector.daemonize`.\n\n    Args:\n        on_collect (Callable[[Dict[str, float]], bool]):\n            A callback function that will be called periodically. It takes a dictionary containing\n            the resource metrics and returns a boolean indicating whether to continue monitoring.\n        collector (Optional[ResourceMetricCollector]):\n            A :class:`ResourceMetricCollector` instance to collect metrics. If not given, it will\n            collect metrics for all GPUs and subprocess of the current process.\n        interval (Optional[float]):\n            The collect interval. If not given, use ``collector.interval``.\n        on_start (Optional[Callable[[ResourceMetricCollector], None]]):\n            A function to initialize the daemon thread and collector.\n        on_stop (Optional[Callable[[ResourceMetricCollector], None]]):\n            A function that do some necessary cleanup after the daemon thread is stopped.\n        tag (str):\n            The tag prefix used for metrics results.\n        start (bool):\n            Whether to start the daemon thread on return.\n\n    Returns: threading.Thread\n        A daemon thread object.\n\n    Examples:\n        .. code-block:: python\n\n            logger = ...\n\n            def on_collect(metrics):  # will be called periodically\n                if logger.is_closed():  # closed manually by user\n                    return False\n                logger.log(metrics)\n                return True\n\n            def on_stop(collector):  # will be called only once at stop\n                if not logger.is_closed():\n                    logger.close()  # cleanup\n\n            # Record metrics to the logger in the background every 5 seconds.\n            # It will collect 5-second mean/min/max for each metric.\n            collect_in_background(\n                on_collect,\n                ResourceMetricCollector(Device.cuda.all()),\n                interval=5.0,\n                on_stop=on_stop,\n            )\n    \"\"\"\n    if collector is None:\n        collector = ResourceMetricCollector()\n    if isinstance(interval, (int, float)) and interval > 0:\n        interval = float(interval)\n    elif interval is None:\n        interval = collector.interval\n    else:\n        raise ValueError(f'Invalid argument interval={interval!r}')\n\n    def target() -> None:\n        if on_start is not None:\n            on_start(collector)\n        try:\n            with collector(tag):\n                try:\n                    next_snapshot = timer() + interval\n                    while on_collect(collector.collect()):\n                        time.sleep(max(0.0, next_snapshot - timer()))\n                        next_snapshot += interval\n                except KeyboardInterrupt:\n                    pass\n        finally:\n            if on_stop is not None:\n                on_stop(collector)\n\n    daemon = threading.Thread(target=target, name=tag, daemon=True)\n    daemon.collector = collector  # type: ignore[attr-defined]\n    if start:\n        daemon.start()\n    return daemon\n\n\nclass ResourceMetricCollector:  # pylint: disable=too-many-instance-attributes\n    \"\"\"A class for collecting resource metrics.\n\n    Args:\n        devices (Iterable[Device]):\n            Set of Device instances for logging. If not given, all physical devices on board will be\n            used.\n        root_pids (Set[int]):\n            A set of PIDs, only the status of the descendant processes on the GPUs will be collected.\n            If not given, the PID of the current process will be used.\n        interval (float):\n            The snapshot interval for background daemon thread.\n\n    Core methods:\n\n    .. code-block:: python\n\n        collector.activate(tag='<tag>')  # alias: start\n        collector.deactivate()           # alias: stop\n        collector.clear(tag='<tag>')\n        collector.collect()\n\n        with collector(tag='<tag>'):\n            ...\n\n        collector.daemonize(on_collect_fn)\n\n    Examples:\n        >>> import os\n        >>> os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n        >>> os.environ['CUDA_VISIBLE_DEVICES'] = '3,2,1,0'\n\n        >>> from nvitop import ResourceMetricCollector, Device\n\n        >>> collector = ResourceMetricCollector()                           # log all devices and descendant processes of the current process on the GPUs\n        >>> collector = ResourceMetricCollector(root_pids={1})              # log all devices and all GPU processes\n        >>> collector = ResourceMetricCollector(devices=Device.cuda.all())  # use the CUDA ordinal\n\n        >>> with collector(tag='<tag>'):\n        ...     # Do something\n        ...     collector.collect()  # -> Dict[str, float]\n        # key -> '<tag>/<scope>/<metric (unit)>/<mean/min/max>'\n        {\n            '<tag>/host/cpu_percent (%)/mean': 8.967849777683456,\n            '<tag>/host/cpu_percent (%)/min': 6.1,\n            '<tag>/host/cpu_percent (%)/max': 28.1,\n            ...,\n            '<tag>/host/memory_percent (%)/mean': 21.5,\n            '<tag>/host/swap_percent (%)/mean': 0.3,\n            '<tag>/host/memory_used (GiB)/mean': 91.0136418208109,\n            '<tag>/host/load_average (%) (1 min)/mean': 10.251427386878328,\n            '<tag>/host/load_average (%) (5 min)/mean': 10.072539414569503,\n            '<tag>/host/load_average (%) (15 min)/mean': 11.91126970422139,\n            ...,\n            '<tag>/cuda:0 (gpu:3)/memory_used (MiB)/mean': 3.875,\n            '<tag>/cuda:0 (gpu:3)/memory_free (MiB)/mean': 11015.562499999998,\n            '<tag>/cuda:0 (gpu:3)/memory_total (MiB)/mean': 11019.437500000002,\n            '<tag>/cuda:0 (gpu:3)/memory_percent (%)/mean': 0.0,\n            '<tag>/cuda:0 (gpu:3)/gpu_utilization (%)/mean': 0.0,\n            '<tag>/cuda:0 (gpu:3)/memory_utilization (%)/mean': 0.0,\n            '<tag>/cuda:0 (gpu:3)/fan_speed (%)/mean': 22.0,\n            '<tag>/cuda:0 (gpu:3)/temperature (C)/mean': 25.0,\n            '<tag>/cuda:0 (gpu:3)/power_usage (W)/mean': 19.11166264116916,\n            ...,\n            '<tag>/cuda:1 (gpu:2)/memory_used (MiB)/mean': 8878.875,\n            ...,\n            '<tag>/cuda:2 (gpu:1)/memory_used (MiB)/mean': 8182.875,\n            ...,\n            '<tag>/cuda:3 (gpu:0)/memory_used (MiB)/mean': 9286.875,\n            ...,\n            '<tag>/pid:12345/host/cpu_percent (%)/mean': 151.34342772112265,\n            '<tag>/pid:12345/host/host_memory (MiB)/mean': 44749.72373447514,\n            '<tag>/pid:12345/host/host_memory_percent (%)/mean': 8.675082352111717,\n            '<tag>/pid:12345/host/running_time (min)': 336.23803206741576,\n            '<tag>/pid:12345/cuda:1 (gpu:4)/gpu_memory (MiB)/mean': 8861.0,\n            '<tag>/pid:12345/cuda:1 (gpu:4)/gpu_memory_percent (%)/mean': 80.4,\n            '<tag>/pid:12345/cuda:1 (gpu:4)/gpu_memory_utilization (%)/mean': 6.711118172407917,\n            '<tag>/pid:12345/cuda:1 (gpu:4)/gpu_sm_utilization (%)/mean': 48.23283397736476,\n            ...,\n            '<tag>/duration (s)': 7.247399162035435,\n            '<tag>/timestamp': 1655909466.9981883\n        }\n    \"\"\"  # pylint: disable=line-too-long\n\n    DEVICE_METRICS: ClassVar[list[tuple[str, str, float | int]]] = [\n        # (<attribute>, <name>, <unit>)\n        # GPU memory metrics\n        ('memory_used', 'memory_used (MiB)', MiB),\n        ('memory_free', 'memory_free (MiB)', MiB),\n        ('memory_total', 'memory_total (MiB)', MiB),\n        ('memory_percent', 'memory_percent (%)', 1.0),\n        # GPU utilization metrics\n        ('gpu_utilization', 'gpu_utilization (%)', 1.0),\n        ('memory_utilization', 'memory_utilization (%)', 1.0),\n        # Miscellaneous\n        ('fan_speed', 'fan_speed (%)', 1.0),\n        ('temperature', 'temperature (C)', 1.0),\n        ('power_usage', 'power_usage (W)', 1000.0),\n    ]\n\n    PROCESS_METRICS: ClassVar[list[tuple[str, str | None, str, float | int]]] = [\n        # (<attribute>, <scope>, <name>, <unit>)\n        # Host resource metrics\n        ('cpu_percent', 'host', 'cpu_percent (%)', 1.0),\n        ('host_memory', 'host', 'host_memory (MiB)', MiB),\n        ('host_memory_percent', 'host', 'host_memory_percent (%)', 1.0),\n        ('running_time_in_seconds', 'host', 'running_time (min)', 60.0),\n        # GPU memory metrics\n        ('gpu_memory', None, 'gpu_memory (MiB)', MiB),\n        ('gpu_memory_percent', None, 'gpu_memory_percent (%)', 1.0),\n        ('gpu_memory_utilization', None, 'gpu_memory_utilization (%)', 1.0),\n        # GPU utilization metrics\n        ('gpu_sm_utilization', None, 'gpu_sm_utilization (%)', 1.0),\n    ]\n\n    def __init__(\n        self,\n        devices: Iterable[Device] | None = None,\n        *,\n        root_pids: Iterable[int] | None = None,\n        interval: float = 1.0,\n    ) -> None:\n        \"\"\"Initialize the resource metric collector.\"\"\"\n        if isinstance(interval, (int, float)) and interval > 0:\n            interval = float(interval)\n        else:\n            raise ValueError(f'Invalid argument interval={interval!r}')\n\n        if devices is None:\n            devices = Device.all()\n\n        root_pids: set[int] = {os.getpid()} if root_pids is None else set(root_pids)\n\n        self.interval: float = interval\n\n        self.devices: list[Device] = list(devices)\n        self.all_devices: list[Device] = []\n        self.leaf_devices: list[Device] = []\n        for device in self.devices:\n            self.all_devices.append(device)\n            mig_devices = device.mig_devices()\n            if len(mig_devices) > 0:\n                self.all_devices.extend(mig_devices)\n                self.leaf_devices.extend(mig_devices)\n            else:\n                self.leaf_devices.append(device)\n\n        self.root_pids: set[int] = root_pids\n        self._positive_processes: WeakSet[HostProcess] = WeakSet(\n            HostProcess(pid) for pid in self.root_pids\n        )\n        self._negative_processes: WeakSet[HostProcess] = WeakSet()\n\n        self._last_timestamp: float = timer() - 2.0 * self.interval\n        self._lock: threading.RLock = threading.RLock()\n        self._metric_buffer: _MetricBuffer | None = None\n        self._tags: set[str] = set()\n\n        self._daemon: threading.Thread = threading.Thread(\n            name='metrics-collector-daemon',\n            target=self._target,\n            daemon=True,\n        )\n        self._daemon_running: threading.Event = threading.Event()\n\n    def activate(self, tag: str) -> ResourceMetricCollector:\n        \"\"\"Start a new metric collection with the given tag.\n\n        Args:\n            tag (str):\n                The name of the new metric collection. The tag will be used to identify the metric\n                collection. It must be a unique string.\n\n        Examples:\n            >>> collector = ResourceMetricCollector()\n\n            >>> collector.activate(tag='train')  # key prefix -> 'train'\n            >>> collector.activate(tag='batch')  # key prefix -> 'train/batch'\n            >>> collector.deactivate()           # key prefix -> 'train'\n            >>> collector.deactivate()           # the collector has been stopped\n            >>> collector.activate(tag='test')   # key prefix -> 'test'\n        \"\"\"\n        with self._lock:\n            if self._metric_buffer is None or tag not in self._tags:\n                self._tags.add(tag)\n                self._metric_buffer = _MetricBuffer(tag, self, prev=self._metric_buffer)\n                self._last_timestamp = timer() - 2.0 * self.interval\n            else:\n                raise RuntimeError(f'Resource metric collector is already started with tag \"{tag}\"')\n\n        self._daemon_running.set()\n        try:\n            self._daemon.start()\n        except RuntimeError:\n            pass\n\n        return self\n\n    start = activate\n\n    def deactivate(self, tag: str | None = None) -> ResourceMetricCollector:\n        \"\"\"Stop the current collection with the given tag and remove all sub-tags.\n\n        If the tag is not specified, deactivate the current active collection. For nested\n        collections, the sub-collections will be deactivated as well.\n\n        Args:\n            tag (Optional[str]):\n                The tag to deactivate. If :data:`None`, the current active collection will be used.\n        \"\"\"\n        with self._lock:\n            if self._metric_buffer is None:\n                if tag is not None:\n                    raise RuntimeError('Resource metric collector has not been started yet.')\n                return self\n\n            if tag is None:\n                tag = self._metric_buffer.tag\n            elif tag not in self._tags:\n                raise RuntimeError(\n                    f'Resource metric collector has not been started with tag \"{tag}\".',\n                )\n\n            buffer = self._metric_buffer\n            while True:\n                self._tags.remove(buffer.tag)\n                if buffer.tag == tag:\n                    self._metric_buffer = buffer.prev\n                    break\n                buffer = buffer.prev  # type: ignore[assignment]\n\n            if self._metric_buffer is None:\n                self._daemon_running.clear()\n\n        return self\n\n    stop = deactivate\n\n    @contextlib.contextmanager\n    def context(self, tag: str) -> Generator[ResourceMetricCollector]:\n        \"\"\"A context manager for starting and stopping resource metric collection.\n\n        Args:\n            tag (str):\n                The name of the new metric collection. The tag will be used to identify the metric\n                collection. It must be a unique string.\n\n        Examples:\n            >>> collector = ResourceMetricCollector()\n\n            >>> with collector.context(tag='train'):  # key prefix -> 'train'\n            ...     # Do something\n            ...     collector.collect()  # -> Dict[str, float]\n        \"\"\"\n        try:\n            self.activate(tag=tag)\n            yield self\n        finally:\n            self.deactivate(tag=tag)\n\n    __call__ = context  # alias for `with collector(tag='<tag>')`\n\n    def clear(self, tag: str | None = None) -> None:\n        \"\"\"Clear the metric collection with the given tag.\n\n        If the tag is not specified, clear the current active collection. For nested collections,\n        the sub-collections will be cleared as well.\n\n        Args:\n            tag (Optional[str]):\n                The tag to clear. If :data:`None`, the current active collection will be reset.\n\n        Examples:\n            >>> collector = ResourceMetricCollector()\n\n            >>> with collector(tag='train'):          # key prefix -> 'train'\n            ...     time.sleep(5.0)\n            ...     collector.collect()               # metrics within the 5.0s interval\n            ...\n            ...     time.sleep(5.0)\n            ...     collector.collect()               # metrics within the cumulative 10.0s interval\n            ...\n            ...     collector.clear()                 # clear the active collection\n            ...     time.sleep(5.0)\n            ...     collector.collect()               # metrics within the 5.0s interval\n            ...\n            ...     with collector(tag='batch'):      # key prefix -> 'train/batch'\n            ...         collector.clear(tag='train')  # clear both 'train' and 'train/batch'\n        \"\"\"\n        with self._lock:\n            if self._metric_buffer is None:\n                if tag is not None:\n                    raise RuntimeError('Resource metric collector has not been started yet.')\n                return\n\n            if tag is None:\n                tag = self._metric_buffer.tag\n            elif tag not in self._tags:\n                raise RuntimeError(\n                    f'Resource metric collector has not been started with tag \"{tag}\".',\n                )\n\n            buffer = self._metric_buffer\n            while True:\n                buffer.clear()\n                if buffer.tag == tag:\n                    break\n                buffer = buffer.prev  # type: ignore[assignment]\n\n    reset = clear\n\n    def collect(self) -> dict[str, float]:\n        \"\"\"Get the average resource consumption during collection.\"\"\"\n        with self._lock:\n            if self._metric_buffer is None:\n                raise RuntimeError('Resource metric collector has not been started yet.')\n\n            if timer() - self._last_timestamp > self.interval / 2.0:\n                self.take_snapshots()\n            return self._metric_buffer.collect()\n\n    # pylint: disable-next=too-many-arguments\n    def daemonize(\n        self,\n        on_collect: Callable[[dict[str, float]], bool],\n        interval: float | None = None,\n        *,\n        on_start: Callable[[ResourceMetricCollector], None] | None = None,\n        on_stop: Callable[[ResourceMetricCollector], None] | None = None,\n        tag: str = 'metrics-daemon',\n        start: bool = True,\n    ) -> threading.Thread:\n        \"\"\"Start a background daemon thread that collect and call the callback function periodically.\n\n        See also :func:`collect_in_background`.\n\n        Args:\n            on_collect (Callable[[Dict[str, float]], bool]):\n                A callback function that will be called periodically. It takes a dictionary containing\n                the resource metrics and returns a boolean indicating whether to continue monitoring.\n            interval (Optional[float]):\n                The collect interval. If not given, use ``collector.interval``.\n            on_start (Optional[Callable[[ResourceMetricCollector], None]]):\n                A function to initialize the daemon thread and collector.\n            on_stop (Optional[Callable[[ResourceMetricCollector], None]]):\n                A function that do some necessary cleanup after the daemon thread is stopped.\n            tag (str):\n                The tag prefix used for metrics results.\n            start (bool):\n                Whether to start the daemon thread on return.\n\n        Returns: threading.Thread\n            A daemon thread object.\n\n        Examples:\n            .. code-block:: python\n\n                logger = ...\n\n                def on_collect(metrics):  # will be called periodically\n                    if logger.is_closed():  # closed manually by user\n                        return False\n                    logger.log(metrics)\n                    return True\n\n                def on_stop(collector):  # will be called only once at stop\n                    if not logger.is_closed():\n                        logger.close()  # cleanup\n\n                # Record metrics to the logger in the background every 5 seconds.\n                # It will collect 5-second mean/min/max for each metric.\n                ResourceMetricCollector(Device.cuda.all()).daemonize(\n                    on_collect,\n                    ResourceMetricCollector(Device.cuda.all()),\n                    interval=5.0,\n                    on_stop=on_stop,\n                )\n        \"\"\"\n        return collect_in_background(\n            on_collect,\n            collector=self,\n            interval=interval,\n            on_start=on_start,\n            on_stop=on_stop,\n            tag=tag,\n            start=start,\n        )\n\n    def __del__(self) -> None:\n        \"\"\"Clean up the demon thread on destruction.\"\"\"\n        self._daemon_running.clear()\n\n    # pylint: disable-next=too-many-branches,too-many-locals,too-many-statements\n    def take_snapshots(self) -> SnapshotResult:\n        \"\"\"Take snapshots of the current resource metrics and update the metric buffer.\"\"\"\n        if len(self.root_pids) > 0:\n            all_gpu_processes: list[GpuProcess] = []\n            for device in self.leaf_devices:\n                all_gpu_processes.extend(device.processes().values())\n\n            gpu_processes = []\n            for process in all_gpu_processes:\n                if process.host in self._negative_processes:\n                    continue\n\n                positive = True\n                if process.host not in self._positive_processes:\n                    positive = False\n                    p = process.host\n                    parents = []\n                    while p is not None:\n                        parents.append(p)\n                        if p in self._positive_processes:\n                            positive = True\n                            break\n                        try:\n                            p = p.parent()  # type: ignore[assignment]\n                        except host.PsutilError:\n                            break\n                    if positive:\n                        self._positive_processes.update(parents)\n                    else:\n                        self._negative_processes.update(parents)\n\n                if positive:\n                    gpu_processes.append(process)\n        else:\n            gpu_processes = []\n\n        timestamp = timer()\n        epoch_timestamp = time.time()\n        metrics = {}\n        device_snapshots = [device.as_snapshot() for device in self.all_devices]\n        gpu_process_snapshots = GpuProcess.take_snapshots(gpu_processes, failsafe=True)\n\n        metrics.update(\n            {\n                'host/cpu_percent (%)': host.cpu_percent(),\n                'host/memory_percent (%)': host.memory_percent(),\n                'host/swap_percent (%)': host.swap_percent(),\n                'host/memory_used (GiB)': host.virtual_memory().used / GiB,\n            },\n        )\n        load_average = host.load_average()\n        if load_average is not None:\n            metrics.update(\n                {\n                    'host/load_average (%) (1 min)': load_average[0],\n                    'host/load_average (%) (5 min)': load_average[1],\n                    'host/load_average (%) (15 min)': load_average[2],\n                },\n            )\n\n        device_identifiers = {}\n        for device_snapshot in device_snapshots:\n            identifier = f'gpu:{device_snapshot.index}'\n            if isinstance(device_snapshot.real, CudaDevice):\n                identifier = f'cuda:{device_snapshot.cuda_index} ({identifier})'\n            device_identifiers[device_snapshot.real] = identifier\n\n            for attr, name, unit in self.DEVICE_METRICS:\n                value = float(getattr(device_snapshot, attr)) / unit\n                metrics[f'{identifier}/{name}'] = value\n\n        for process_snapshot in gpu_process_snapshots:\n            device_identifier = device_identifiers[process_snapshot.device]\n            identifier = f'pid:{process_snapshot.pid}'\n\n            for attr, scope, name, unit in self.PROCESS_METRICS:\n                scope = scope or device_identifier\n                value = float(getattr(process_snapshot, attr)) / unit\n                metrics[f'{identifier}/{scope}/{name}'] = value\n\n        with self._lock:\n            if self._metric_buffer is not None:\n                self._metric_buffer.add(\n                    metrics,\n                    timestamp=timestamp,\n                    epoch_timestamp=epoch_timestamp,\n                )\n                self._last_timestamp = timestamp\n\n        return SnapshotResult(device_snapshots, gpu_process_snapshots)\n\n    def _target(self) -> None:\n        self._daemon_running.wait()\n        while self._daemon_running.is_set():\n            next_snapshot = timer() + self.interval\n            self.take_snapshots()\n            time.sleep(max(0.0, next_snapshot - timer()))\n            next_snapshot += self.interval\n\n\nclass _MetricBuffer:  # pylint: disable=missing-class-docstring,missing-function-docstring,too-many-instance-attributes\n    def __init__(\n        self,\n        tag: str,\n        collector: ResourceMetricCollector,\n        *,\n        prev: _MetricBuffer | None = None,\n    ) -> None:\n        self.collector: ResourceMetricCollector = collector\n        self.prev: _MetricBuffer | None = prev\n\n        self.tag: str = tag\n        self.key_prefix: str\n        if self.prev is not None:\n            self.key_prefix = f'{self.prev.key_prefix}/{self.tag}'\n        else:\n            self.key_prefix = self.tag\n\n        self.last_timestamp = self.start_timestamp = timer()\n        self.last_epoch_timestamp = time.time()\n        self.buffer: defaultdict[str, _StatisticsMaintainer] = defaultdict(\n            lambda: _StatisticsMaintainer(self.last_timestamp),\n        )\n\n        self.len = 0\n\n    def add(\n        self,\n        metrics: dict[str, float],\n        timestamp: float | None = None,\n        epoch_timestamp: float | None = None,\n    ) -> None:\n        if timestamp is None:\n            timestamp = timer()\n        if epoch_timestamp is None:\n            epoch_timestamp = time.time()\n\n        for key in set(self.buffer).difference(metrics):\n            self.buffer[key].add(math.nan, timestamp=timestamp)\n        for key, value in metrics.items():\n            self.buffer[key].add(value, timestamp=timestamp)\n        self.len += 1\n        self.last_timestamp = timestamp\n        self.last_epoch_timestamp = epoch_timestamp\n\n        if self.prev is not None:\n            self.prev.add(metrics, timestamp=timestamp)\n\n    def clear(self) -> None:\n        self.last_timestamp = self.start_timestamp = timer()\n        self.last_epoch_timestamp = time.time()\n        self.buffer.clear()\n        self.len = 0\n\n    def collect(self) -> dict[str, float]:\n        metrics = {\n            f'{self.key_prefix}/{key}/{name}': value\n            for key, stats in self.buffer.items()\n            for name, value in stats.items()\n        }\n        for key in tuple(metrics.keys()):\n            if key.endswith('host/running_time (min)/max'):\n                metrics[key[:-4]] = metrics[key]\n                del metrics[key]\n            elif key.endswith(('host/running_time (min)/mean', 'host/running_time (min)/min')):\n                del metrics[key]\n        metrics[f'{self.key_prefix}/duration (s)'] = timer() - self.start_timestamp\n        metrics[f'{self.key_prefix}/timestamp'] = time.time()\n        metrics[f'{self.key_prefix}/last_timestamp'] = self.last_epoch_timestamp\n        return metrics\n\n    def __len__(self) -> int:\n        return self.len\n\n\nclass _StatisticsMaintainer:  # pylint: disable=missing-class-docstring,missing-function-docstring\n    def __init__(self, timestamp: float) -> None:\n        self.start_timestamp: float = timestamp\n        self.last_timestamp: float = math.nan\n        self.integral: float | None = None\n        self.last_value: float | None = None\n        self.min_value: float | None = None\n        self.max_value: float | None = None\n        self.has_nan: bool = False\n\n    def add(self, value: float, timestamp: float | None = None) -> None:\n        if timestamp is None:\n            timestamp = timer()\n\n        if math.isnan(value):\n            self.has_nan = True\n            return\n\n        if self.last_value is None:\n            self.integral = value * (timestamp - self.start_timestamp)\n            self.last_value = self.min_value = self.max_value = value\n        else:\n            # pylint: disable-next=line-too-long\n            self.integral += (value + self.last_value) * (timestamp - self.last_timestamp) / 2.0  # type: ignore[operator]\n            self.last_value = value\n            self.min_value = min(self.min_value, value)  # type: ignore[type-var]\n            self.max_value = max(self.max_value, value)  # type: ignore[type-var]\n\n        self.last_timestamp = timestamp\n\n    def mean(self) -> float:\n        if self.integral is None:\n            return math.nan\n\n        if self.has_nan:\n            return self.integral / (self.last_timestamp - self.start_timestamp)\n\n        timestamp = timer()\n        integral = self.integral + self.last_value * (timestamp - self.last_timestamp)  # type: ignore[operator]\n        return integral / (timestamp - self.start_timestamp)\n\n    def min(self) -> float:\n        if self.min_value is None:\n            return math.nan\n        return self.min_value\n\n    def max(self) -> float:\n        if self.max_value is None:\n            return math.nan\n        return self.max_value\n\n    def last(self) -> float:\n        if self.last_value is None:\n            return math.nan\n        return self.last_value\n\n    def items(self) -> Iterable[tuple[str, float]]:\n        yield ('mean', self.mean())\n        yield ('min', self.min())\n        yield ('max', self.max())\n        yield ('last', self.last())\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "nvitop/api/collector.py", "language": "python", "complexity": "complex", "star_count": 5792, "quality_score": 0.97, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 913}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This file is part of nvitop, the interactive NVIDIA-GPU process viewer.\n#\n# Copyright 2021-2025 Xuehai Pan. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Python bindings for the `CUDA Driver APIs <https://docs.nvidia.com/cuda/cuda-driver-api>`_.\"\"\"\n\n# pylint: disable=invalid-name\n\nfrom __future__ import annotations\n\nimport ctypes as _ctypes\nimport itertools as _itertools\nimport platform as _platform\nimport string as _string\nimport sys as _sys\nimport threading as _threading\nfrom typing import TYPE_CHECKING as _TYPE_CHECKING\nfrom typing import Any as _Any\nfrom typing import ClassVar as _ClassVar\n\n\nif _TYPE_CHECKING:\n    from collections.abc import Callable as _Callable\n    from typing_extensions import Self as _Self  # Python 3.11+\n    from typing_extensions import TypeAlias as _TypeAlias  # Python 3.10+\n\n\n# pylint: disable-next=missing-class-docstring,too-few-public-methods\nclass _struct_c_CUdevice_t(_ctypes.Structure):\n    pass  # opaque handle\n\n\n_c_CUdevice_t: _TypeAlias = _ctypes.POINTER(  # type: ignore[valid-type] # noqa: PYI042\n    _struct_c_CUdevice_t,\n)\n\n_CUresult_t: _TypeAlias = _ctypes.c_uint\n\n# Error codes #\n# pylint: disable=line-too-long\nCUDA_SUCCESS = 0\n\"\"\"The API call returned with no errors. In the case of query calls, this also means that the operation being queried is complete (see :func:`cuEventQuery` and :func:`cuStreamQuery`).\"\"\"\nCUDA_ERROR_INVALID_VALUE = 1\n\"\"\"This indicates that one or more of the parameters passed to the API call is not within an acceptable range of values.\"\"\"\nCUDA_ERROR_OUT_OF_MEMORY = 2\n\"\"\"The API call failed because it was unable to allocate enough memory to perform the requested operation.\"\"\"\nCUDA_ERROR_NOT_INITIALIZED = 3\n\"\"\"This indicates that the CUDA driver has not been initialized with :func:`cuInit` or that initialization has failed.\"\"\"\nCUDA_ERROR_DEINITIALIZED = 4\n\"\"\"This indicates that the CUDA driver is in the process of shutting down.\"\"\"\nCUDA_ERROR_PROFILER_DISABLED = 5\n\"\"\"This indicates profiler is not initialized for this run. This can happen when the application is running with external profiling tools like visual profiler.\"\"\"\nCUDA_ERROR_STUB_LIBRARY = 34\n\"\"\"This indicates that the CUDA driver that the application has loaded is a stub library. Applications that run with the stub rather than a real driver loaded will result in CUDA API returning this error.\"\"\"\nCUDA_ERROR_DEVICE_UNAVAILABLE = 46\n\"\"\"This indicates that requested CUDA device is unavailable at the current time. Devices are often unavailable due to use of :data:`CU_COMPUTEMODE_EXCLUSIVE_PROCESS` or :data:`CU_COMPUTEMODE_PROHIBITED`.\"\"\"\nCUDA_ERROR_NO_DEVICE = 100\n\"\"\"This indicates that no CUDA - capable devices were detected by the installed CUDA driver.\"\"\"\nCUDA_ERROR_INVALID_DEVICE = 101\n\"\"\"This indicates that the device ordinal supplied by the user does not correspond to a valid CUDA device or that the action requested is invalid for the specified device.\"\"\"\nCUDA_ERROR_DEVICE_NOT_LICENSED = 102\n\"\"\"This error indicates that the Grid license is not applied.\"\"\"\nCUDA_ERROR_INVALID_IMAGE = 200\n\"\"\"This indicates that the device kernel image is invalid. This can also indicate an invalid CUDA module.\"\"\"\nCUDA_ERROR_INVALID_CONTEXT = 201\n\"\"\"This most frequently indicates that there is no context bound to the current thread. This can also be returned if the context passed to an API call is not a valid handle (such as a context that has had :func:`cuCtxDestroy` invoked on it). This can also be returned if a user mixes different API versions (i.e. 3010 context with 3020 API calls). See :func:`cuCtxGetApiVersion` for more details.\"\"\"\nCUDA_ERROR_MAP_FAILED = 205\n\"\"\"This indicates that a map or register operation has failed.\"\"\"\nCUDA_ERROR_UNMAP_FAILED = 206\n\"\"\"This indicates that an unmap or unregister operation has failed.\"\"\"\nCUDA_ERROR_ARRAY_IS_MAPPED = 207\n\"\"\"This indicates that the specified array is currently mapped and thus cannot be destroyed.\"\"\"\nCUDA_ERROR_ALREADY_MAPPED = 208\n\"\"\"This indicates that the resource is already mapped.\"\"\"\nCUDA_ERROR_NO_BINARY_FOR_GPU = 209\n\"\"\"This indicates that there is no kernel image available that is suitable for the device. This can occur when a user specifies code generation options for a particular CUDA source file that do not include the corresponding device configuration.\"\"\"\nCUDA_ERROR_ALREADY_ACQUIRED = 210\n\"\"\"This indicates that a resource has already been acquired.\"\"\"\nCUDA_ERROR_NOT_MAPPED = 211\n\"\"\"This indicates that a resource is not mapped.\"\"\"\nCUDA_ERROR_NOT_MAPPED_AS_ARRAY = 212\n\"\"\"This indicates that a mapped resource is not available for access as an array.\"\"\"\nCUDA_ERROR_NOT_MAPPED_AS_POINTER = 213\n\"\"\"This indicates that a mapped resource is not available for access as a pointer.\"\"\"\nCUDA_ERROR_ECC_UNCORRECTABLE = 214\n\"\"\"This indicates that an uncorrectable ECC error was detected during execution.\"\"\"\nCUDA_ERROR_UNSUPPORTED_LIMIT = 215\n\"\"\"This indicates that the :class:`CUlimit` passed to the API call is not supported by the active device.\"\"\"\nCUDA_ERROR_CONTEXT_ALREADY_IN_USE = 216\n\"\"\"This indicates that the :class:`CUcontext` passed to the API call can only be bound to a single CPU thread at a time but is already bound to a CPU thread.\"\"\"\nCUDA_ERROR_PEER_ACCESS_UNSUPPORTED = 217\n\"\"\"This indicates that peer access is not supported across the given devices.\"\"\"\nCUDA_ERROR_INVALID_PTX = 218\n\"\"\"This indicates that a PTX JIT compilation failed.\"\"\"\nCUDA_ERROR_INVALID_GRAPHICS_CONTEXT = 219\n\"\"\"This indicates an error with OpenGL or DirectX context.\"\"\"\nCUDA_ERROR_NVLINK_UNCORRECTABLE = 220\n\"\"\"This indicates that an uncorrectable NVLink error was detected during the execution.\"\"\"\nCUDA_ERROR_JIT_COMPILER_NOT_FOUND = 221\n\"\"\"This indicates that the PTX JIT compiler library was not found.\"\"\"\nCUDA_ERROR_UNSUPPORTED_PTX_VERSION = 222\n\"\"\"This indicates that the provided PTX was compiled with an unsupported toolchain.\"\"\"\nCUDA_ERROR_JIT_COMPILATION_DISABLED = 223\n\"\"\"This indicates that the PTX JIT compilation was disabled.\"\"\"\nCUDA_ERROR_UNSUPPORTED_EXEC_AFFINITY = 224\n\"\"\"This indicates that the :class:`CUexecAffinityType` passed to the API call is not supported by the active device.\"\"\"\nCUDA_ERROR_INVALID_SOURCE = 300\n\"\"\"This indicates that the device kernel source is invalid. This includes compilation / linker errors encountered in device code or user error.\"\"\"\nCUDA_ERROR_FILE_NOT_FOUND = 301\n\"\"\"This indicates that the file specified was not found.\"\"\"\nCUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND = 302\n\"\"\"This indicates that a link to a shared object failed to resolve.\"\"\"\nCUDA_ERROR_SHARED_OBJECT_INIT_FAILED = 303\n\"\"\"This indicates that initialization of a shared object failed.\"\"\"\nCUDA_ERROR_OPERATING_SYSTEM = 304\n\"\"\"This indicates that an OS call failed.\"\"\"\nCUDA_ERROR_INVALID_HANDLE = 400\n\"\"\"This indicates that a resource handle passed to the API call was not valid. Resource handles are opaque types like :class:`CUstream` and :class:`CUevent`.\"\"\"\nCUDA_ERROR_ILLEGAL_STATE = 401\n\"\"\"This indicates that a resource required by the API call is not in a valid state to perform the requested operation.\"\"\"\nCUDA_ERROR_NOT_FOUND = 500\n\"\"\"This indicates that a named symbol was not found. Examples of symbols are global / constant variable names, driver function names, texture names, and surface names.\"\"\"\nCUDA_ERROR_NOT_READY = 600\n\"\"\"This indicates that asynchronous operations issued previously have not completed yet. This result is not actually an error, but must be indicated differently than :data:`CUDA_SUCCESS` (which indicates completion). Calls that may return this value include :func:`cuEventQuery` and :func:`cuStreamQuery`.\"\"\"\nCUDA_ERROR_ILLEGAL_ADDRESS = 700\n\"\"\"While executing a kernel, the device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\nCUDA_ERROR_LAUNCH_OUT_OF_RESOURCES = 701\n\"\"\"This indicates that a launch did not occur because it did not have appropriate resources. This error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernel launch specifies too many threads for the kernel's register count. Passing arguments of the wrong size (i.e. a 64 - bit pointer when a 32 - bit int is expected) is equivalent to passing too many arguments and can also result in this error.\"\"\"\nCUDA_ERROR_LAUNCH_TIMEOUT = 702\n\"\"\"This indicates that the device kernel took too long to execute. This can only occur if timeouts are enabled - see the device attribute :data:`CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT` for more information. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\nCUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING = 703\n\"\"\"This error indicates a kernel launch that uses an incompatible texturing mode.\"\"\"\nCUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED = 704\n\"\"\"This error indicates that a call to :func:`cuCtxEnablePeerAccess` is trying to re - enable peer access to a context which has already had peer access to it enabled.\"\"\"\nCUDA_ERROR_PEER_ACCESS_NOT_ENABLED = 705\n\"\"\"This error indicates that :func:`cuCtxDisablePeerAccess` is trying to disable peer access which has not been enabled yet via :func:`cuCtxEnablePeerAccess`.\"\"\"\nCUDA_ERROR_PRIMARY_CONTEXT_ACTIVE = 708\n\"\"\"This error indicates that the primary context for the specified device has already been initialized.\"\"\"\nCUDA_ERROR_CONTEXT_IS_DESTROYED = 709\n\"\"\"This error indicates that the context current to the calling thread has been destroyed using :func:`cuCtxDestroy`, or is a primary context which has not yet been initialized.\"\"\"\nCUDA_ERROR_ASSERT = 710\n\"\"\"A device - side assert triggered during kernel execution. The context cannot be used anymore, and must be destroyed. All existing device memory allocations from this context are invalid and must be reconstructed if the program is to continue using CUDA.\"\"\"\nCUDA_ERROR_TOO_MANY_PEERS = 711\n\"\"\"This error indicates that the hardware resources required to enable peer access have been exhausted for one or more of the devices passed to :func:`cuCtxEnablePeerAccess`.\"\"\"\nCUDA_ERROR_HOST_MEMORY_ALREADY_REGISTERED = 712\n\"\"\"This error indicates that the memory range passed to :func:`cuMemHostRegister` has already been registered.\"\"\"\nCUDA_ERROR_HOST_MEMORY_NOT_REGISTERED = 713\n\"\"\"This error indicates that the pointer passed to :func:`cuMemHostUnregister` does not correspond to any currently registered memory region.\"\"\"\nCUDA_ERROR_HARDWARE_STACK_ERROR = 714\n\"\"\"While executing a kernel, the device encountered a stack error. This can be due to stack corruption or exceeding the stack size limit. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\nCUDA_ERROR_ILLEGAL_INSTRUCTION = 715\n\"\"\"While executing a kernel, the device encountered an illegal instruction. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\nCUDA_ERROR_MISALIGNED_ADDRESS = 716\n\"\"\"While executing a kernel, the device encountered a load or store instruction on a memory address which is not aligned. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\nCUDA_ERROR_INVALID_ADDRESS_SPACE = 717\n\"\"\"While executing a kernel, the device encountered an instruction which can only operate on memory locations in certain address spaces (global, shared, or local), but was supplied a memory address not belonging to an allowed address space. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\nCUDA_ERROR_INVALID_PC = 718\n\"\"\"While executing a kernel, the device program counter wrapped its address space. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\nCUDA_ERROR_LAUNCH_FAILED = 719\n\"\"\"An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointer and accessing out of bounds shared memory. Less common cases can be system specific - more information about these cases can be found in the system specific user guide. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\nCUDA_ERROR_COOPERATIVE_LAUNCH_TOO_LARGE = 720\n\"\"\"This error indicates that the number of blocks launched per grid for a kernel that was launched via either :func:`cuLaunchCooperativeKernel` or :func:`cuLaunchCooperativeKernelMultiDevice` exceeds the maximum number of blocks as allowed by :func:`cuOccupancyMaxActiveBlocksPerMultiprocessor` or :func:`cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags` times the number of multiprocessors as specified by the device attribute :data:`CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT`.\"\"\"\nCUDA_ERROR_NOT_PERMITTED = 800\n\"\"\"This error indicates that the attempted operation is not permitted.\"\"\"\nCUDA_ERROR_NOT_SUPPORTED = 801\n\"\"\"This error indicates that the attempted operation is not supported on the current system or device.\"\"\"\nCUDA_ERROR_SYSTEM_NOT_READY = 802\n\"\"\"This error indicates that the system is not yet ready to start any CUDA work. To continue using CUDA, verify the system configuration is in a valid state and all required driver daemons are actively running. More information about this error can be found in the system specific user guide.\"\"\"\nCUDA_ERROR_SYSTEM_DRIVER_MISMATCH = 803\n\"\"\"This error indicates that there is a mismatch between the versions of the display driver and the CUDA driver. Refer to the compatibility documentation for supported versions.\"\"\"\nCUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE = 804\n\"\"\"This error indicates that the system was upgraded to run with forward compatibility but the visible hardware detected by CUDA does not support this configuration. Refer to the compatibility documentation for the supported hardware matrix or ensure that only supported hardware is visible during initialization via the ``CUDA_VISIBLE_DEVICES`` environment variable.\"\"\"\nCUDA_ERROR_MPS_CONNECTION_FAILED = 805\n\"\"\"This error indicates that the MPS client failed to connect to the MPS control daemon or the MPS server.\"\"\"\nCUDA_ERROR_MPS_RPC_FAILURE = 806\n\"\"\"This error indicates that the remote procedural call between the MPS server and the MPS client failed.\"\"\"\nCUDA_ERROR_MPS_SERVER_NOT_READY = 807\n\"\"\"This error indicates that the MPS server is not ready to accept new MPS client requests. This error can be returned when the MPS server is in the process of recovering from a fatal failure.\"\"\"\nCUDA_ERROR_MPS_MAX_CLIENTS_REACHED = 808\n\"\"\"This error indicates that the hardware resources required to create MPS client have been exhausted.\"\"\"\nCUDA_ERROR_MPS_MAX_CONNECTIONS_REACHED = 809\n\"\"\"This error indicates the the hardware resources required to support device connections have been exhausted.\"\"\"\nCUDA_ERROR_STREAM_CAPTURE_UNSUPPORTED = 900\n\"\"\"This error indicates that the operation is not permitted when the stream is capturing.\"\"\"\nCUDA_ERROR_STREAM_CAPTURE_INVALIDATED = 901\n\"\"\"This error indicates that the current capture sequence on the stream has been invalidated due to a previous error.\"\"\"\nCUDA_ERROR_STREAM_CAPTURE_MERGE = 902\n\"\"\"This error indicates that the operation would have resulted in a merge of two independent capture sequences.\"\"\"\nCUDA_ERROR_STREAM_CAPTURE_UNMATCHED = 903\n\"\"\"This error indicates that the capture was initiated not in this stream.\"\"\"\nCUDA_ERROR_STREAM_CAPTURE_UNJOINED = 904\n\"\"\"This error indicates that the capture sequence contains a fork that was not joined to the primary stream.\"\"\"\nCUDA_ERROR_STREAM_CAPTURE_ISOLATION = 905\n\"\"\"This error indicates that a dependency would have been created which crosses the capture sequence boundary. Only implicit in -stream ordering dependencies are allowed to cross the boundary.\"\"\"\nCUDA_ERROR_STREAM_CAPTURE_IMPLICIT = 906\n\"\"\"This error indicates a disallowed implicit dependency on a current capture sequence from :func:`cudaStreamLegacy`.\"\"\"\nCUDA_ERROR_CAPTURED_EVENT = 907\n\"\"\"This error indicates that the operation is not permitted on an event which was last recorded in a capturing stream.\"\"\"\nCUDA_ERROR_STREAM_CAPTURE_WRONG_THREAD = 908\n\"\"\"A stream capture sequence not initiated with the :data:`CU_STREAM_CAPTURE_MODE_RELAXED` argument to :func:`cuStreamBeginCapture` was passed to :func:`cuStreamEndCapture` in a different thread.\"\"\"\nCUDA_ERROR_TIMEOUT = 909\n\"\"\"This error indicates that the timeout specified for the wait operation has lapsed.\"\"\"\nCUDA_ERROR_GRAPH_EXEC_UPDATE_FAILURE = 910\n\"\"\"This error indicates that the graph update was not performed because it included changes which violated constraints specific to instantiated graph update.\"\"\"\nCUDA_ERROR_EXTERNAL_DEVICE = 911\n\"\"\"This indicates that an async error has occurred in a device outside of CUDA. If CUDA was waiting for an external device's signal before consuming shared data, the external device signaled an error indicating that the data is not valid for consumption. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\nCUDA_ERROR_UNKNOWN = 999\n\"\"\"This indicates that an unknown internal error has occurred.\"\"\"\n# pylint: enable=line-too-long\n\n\n# Error Checking #\nclass CUDAError(Exception):\n    \"\"\"Base exception class for CUDA driver query errors.\"\"\"\n\n    _value_class_mapping: _ClassVar[dict[int, type[CUDAError]]] = {}\n    _errcode_to_string: _ClassVar[dict[int, str]] = {  # List of currently known error codes\n        CUDA_ERROR_NOT_INITIALIZED:                'Initialization error.',\n        CUDA_ERROR_NOT_FOUND:                      'Named symbol not found.',\n        CUDA_ERROR_INVALID_VALUE:                  'Invalid argument.',\n        CUDA_ERROR_NO_DEVICE:                      'No CUDA-capable device is detected.',\n        CUDA_ERROR_INVALID_DEVICE:                 'Invalid device ordinal.',\n        CUDA_ERROR_SYSTEM_DRIVER_MISMATCH:         'System has unsupported display driver / CUDA driver combination.',\n        CUDA_ERROR_DEINITIALIZED:                  'Driver shutting down.',\n        CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: 'Forward compatibility was attempted on non supported Hardware.',\n        CUDA_ERROR_INVALID_CONTEXT:                'Invalid device context.',\n    }  # fmt:skip\n    _errcode_to_name: _ClassVar[dict[int, str]] = {}\n    value: int\n\n    def __new__(cls, value: int) -> _Self:\n        \"\"\"Map value to a proper subclass of :class:`CUDAError`.\"\"\"\n        if cls is CUDAError:\n            # pylint: disable-next=self-cls-assignment\n            cls = CUDAError._value_class_mapping.get(value, cls)  # type: ignore[assignment]\n        obj = Exception.__new__(cls)\n        obj.value = value\n        return obj\n\n    def __repr__(self) -> str:\n        \"\"\"Return a string representation of the error.\"\"\"\n        # pylint: disable=no-member\n        try:\n            if self.value not in CUDAError._errcode_to_string:\n                CUDAError._errcode_to_string[self.value] = '{}.'.format(\n                    cuGetErrorString(self.value).rstrip('.').capitalize(),\n                )\n            if self.value not in CUDAError._errcode_to_name:\n                CUDAError._errcode_to_name[self.value] = cuGetErrorName(self.value)\n            return (\n                f'{CUDAError._errcode_to_string[self.value]} '\n                f'Code: {CUDAError._errcode_to_name[self.value]} ({self.value}).'\n            )\n        except CUDAError:\n            return f'CUDA Error with code {self.value}.'\n\n    def __eq__(self, other: object) -> bool:\n        \"\"\"Test equality to other object.\"\"\"\n        if not isinstance(other, CUDAError):\n            return NotImplemented\n        return self.value == other.value  # pylint: disable=no-member\n\n    def __reduce__(self) -> tuple[type[CUDAError], tuple[int]]:\n        \"\"\"Return state information for pickling.\"\"\"\n        return CUDAError, (self.value,)  # pylint: disable=no-member\n\n\ndef cudaExceptionClass(cudaErrorCode: int, /) -> type[CUDAError]:\n    \"\"\"Map value to a proper subclass of :class:`CUDAError`.\n\n    Raises:\n        ValueError: If the error code is not valid.\n    \"\"\"\n    if cudaErrorCode not in CUDAError._value_class_mapping:  # pylint: disable=protected-access\n        raise ValueError(f'cudaErrorCode {cudaErrorCode} is not valid.')\n    return CUDAError._value_class_mapping[cudaErrorCode]  # pylint: disable=protected-access\n\n\ndef _extract_cuda_errors_as_classes() -> None:\n    \"\"\"Generate a hierarchy of classes on top of :class:`CUDAError` class.\n\n    Each CUDA Error gets a new :class:`CUDAError` subclass. This way try-except blocks can filter\n    appropriate exceptions more easily.\n\n    :class:`CUDAError` is a parent class. Each ``CUDA_ERROR_*`` gets it's own subclass.\n    e.g. :data:`CUDA_ERROR_INVALID_VALUE` will be turned into :class:`CUDAError_InvalidValue`.\n    \"\"\"\n    this_module = _sys.modules[__name__]\n    cuda_error_names = [x for x in dir(this_module) if x.startswith('CUDA_ERROR_')]\n    for err_name in cuda_error_names:\n        # e.g. Turn CUDA_ERROR_INVALID_VALUE into CUDAError_InvalidValue\n        pascal_case = _string.capwords(err_name.replace('CUDA_ERROR_', ''), '_').replace('_', '')\n        class_name = f'CUDAError_{pascal_case}'\n        err_val = getattr(this_module, err_name)\n\n        def gen_new(value: int) -> _Callable[[type[CUDAError]], CUDAError]:\n            def new(cls: type[CUDAError]) -> CUDAError:\n                return CUDAError.__new__(cls, value)\n\n            return new\n\n        # pylint: disable=protected-access\n        new_error_class = type(class_name, (CUDAError,), {'__new__': gen_new(err_val)})\n        new_error_class.__module__ = __name__\n        if err_val in CUDAError._errcode_to_string:\n            new_error_class.__doc__ = (\n                f'CUDA Error: {CUDAError._errcode_to_string[err_val]} '\n                f'Code: :data:`{err_name}` ({err_val}).'\n            )\n        else:\n            new_error_class.__doc__ = f'CUDA Error with code :data:`{err_name}` ({err_val})'\n        setattr(this_module, class_name, new_error_class)\n        CUDAError._value_class_mapping[err_val] = new_error_class\n        CUDAError._errcode_to_name[err_val] = err_name\n\n\n# Add explicit references to appease linters\nclass __CUDAError(CUDAError):\n    value: int\n\n    def __new__(cls) -> CUDAError:  # type: ignore[misc,empty-body]\n        ...\n\n\nCUDAError_NotInitialized: type[__CUDAError]\nCUDAError_NotFound: type[__CUDAError]\nCUDAError_InvalidValue: type[__CUDAError]\nCUDAError_NoDevice: type[__CUDAError]\nCUDAError_InvalidDevice: type[__CUDAError]\nCUDAError_SystemDriverMismatch: type[__CUDAError]\nCUDAError_Deinitialized: type[__CUDAError]\nCUDAError_CompatNotSupportedOnDevice: type[__CUDAError]\nCUDAError_InvalidContext: type[__CUDAError]\n\n_extract_cuda_errors_as_classes()\ndel _extract_cuda_errors_as_classes\n\n\ndef _cudaCheckReturn(ret: _Any, /) -> _Any:\n    if ret != CUDA_SUCCESS:\n        raise CUDAError(ret)\n    return ret\n\n\n# Function access #\n__cudaLib: _ctypes.CDLL | None = None\n__initialized: bool = False\n__libLoadLock: _threading.Lock = _threading.Lock()\n# Function pointers are cached to prevent unnecessary libLoadLock locking\n__cudaGetFunctionPointer_cache: dict[str, _ctypes._CFuncPtr] = {}  # type: ignore[name-defined]\n\n\ndef __cudaGetFunctionPointer(name: str) -> _ctypes._CFuncPtr:  # type: ignore[name-defined]\n    \"\"\"Get the function pointer from the CUDA driver library.\n\n    Raises:\n        CUDAError_NotInitialized:\n            If cannot found the CUDA driver library.\n        CUDAError_NotFound:\n            If cannot found the function pointer.\n    \"\"\"\n    if name in __cudaGetFunctionPointer_cache:\n        return __cudaGetFunctionPointer_cache[name]\n\n    with __libLoadLock:\n        # Ensure library was loaded\n        if __cudaLib is None:\n            raise CUDAError(CUDA_ERROR_NOT_INITIALIZED)\n        try:\n            __cudaGetFunctionPointer_cache[name] = getattr(__cudaLib, name)\n            return __cudaGetFunctionPointer_cache[name]\n        except AttributeError as ex:\n            raise CUDAError(CUDA_ERROR_NOT_FOUND) from ex\n\n\ndef __LoadCudaLibrary() -> None:\n    \"\"\"Load the library if it isn't loaded already.\n\n    Raises:\n        CUDAError_NotInitialized:\n            If cannot found the CUDA driver library.\n    \"\"\"\n    global __cudaLib  # pylint: disable=global-statement\n\n    if __cudaLib is None:\n        # Lock to ensure only one caller loads the library\n        with __libLoadLock:\n            # Ensure the library still isn't loaded\n            if __cudaLib is None:\n                # Platform specific libcuda location\n                system = _platform.system()\n                if system == 'Darwin':\n                    lib_filenames = [\n                        'libcuda.1.dylib',  # check library path first\n                        'libcuda.dylib',\n                        '/usr/local/cuda/lib/libcuda.1.dylib',\n                        '/usr/local/cuda/lib/libcuda.dylib',\n                    ]\n                elif system == 'Linux':\n                    lib_filenames = [\n                        'libcuda.so',  # check library path first\n                        '/usr/lib64/nvidia/libcuda.so',  # Redhat/CentOS/Fedora\n                        '/usr/lib/x86_64-linux-gnu/libcuda.so',  # Ubuntu\n                        '/usr/lib/wsl/lib/libcuda.so',  # WSL\n                    ]\n                    # Also add libraries with version suffix `.1`\n                    lib_filenames = list(\n                        _itertools.chain.from_iterable((f'{lib}.1', lib) for lib in lib_filenames),\n                    )\n                elif system == 'Windows':\n                    bits = 8 * _ctypes.sizeof(_ctypes.c_void_p)  # 64 or 32\n                    lib_filenames = [f'nvcuda{bits}.dll', 'nvcuda.dll']\n                # Open library\n                for lib_filename in lib_filenames:\n                    try:\n                        __cudaLib = _ctypes.CDLL(lib_filename)\n                        break\n                    except OSError:\n                        pass\n                if __cudaLib is None:\n                    _cudaCheckReturn(CUDA_ERROR_NOT_INITIALIZED)\n\n\ndef cuInit(flags: int = 0) -> None:\n    \"\"\"Initialize the CUDA driver API.\n\n    Initialize the driver API and must be called before any other function from the driver API.\n    Currently, the ``flags`` parameter must be :data:`0`. If :func:`cuInit` has not been called,\n    any function from the driver API will return :data:`CUDA_ERROR_NOT_INITIALIZED`.\n\n    Raises:\n        CUDAError_NoDevice:\n            If no CUDA-capable device is available.\n        CUDAError_InvalidDevice:\n            If the device ordinal supplied by the user does not correspond to a valid CUDA device or\n            that the action requested is invalid for the specified device.\n        CUDAError_SystemDriverMismatch:\n            If there is a mismatch between the versions of the display driver and the CUDA driver.\n        CUDAError_CompatNotSupportedOnDevice:\n            If the system was upgraded to run with forward compatibility but the visible hardware\n            detected by CUDA does not support this configuration.\n        CUDAError_InvalidValue:\n            If passed with invalid flag value.\n        CUDAError_NotInitialized:\n            If cannot found the CUDA driver library.\n    \"\"\"\n    global __initialized  # pylint: disable=global-statement\n\n    __LoadCudaLibrary()\n\n    if __initialized:\n        return\n\n    fn = __cudaGetFunctionPointer('cuInit')\n\n    ret = fn(_ctypes.c_uint(flags))\n    _cudaCheckReturn(ret)\n\n    with __libLoadLock:\n        __initialized = True\n\n\ndef cuGetErrorName(error: int) -> str:\n    \"\"\"Get the string representation of an error code enum name.\n\n    Raises:\n        CUDAError_InvalidValue:\n            If the error code is not recognized.\n        CUDAError_NotInitialized:\n            If the CUDA driver API is not initialized.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cuGetErrorName')\n\n    p_name = _ctypes.POINTER(_ctypes.c_char_p)()\n    ret = fn(_CUresult_t(error), _ctypes.byref(p_name))\n    _cudaCheckReturn(ret)\n    name = _ctypes.string_at(p_name)\n    return name.decode('utf-8', errors='replace')\n\n\ndef cuGetErrorString(error: int) -> str:\n    \"\"\"Get the string description of an error code.\n\n    Raises:\n        CUDAError_InvalidValue:\n            If the error code is not recognized.\n        CUDAError_NotInitialized:\n            If the CUDA driver API is not initialized.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cuGetErrorString')\n\n    p_name = _ctypes.POINTER(_ctypes.c_char_p)()\n    ret = fn(_CUresult_t(error), _ctypes.byref(p_name))\n    _cudaCheckReturn(ret)\n    name = _ctypes.string_at(p_name)\n    return name.decode('utf-8', errors='replace')\n\n\ndef cuDriverGetVersion() -> str:\n    \"\"\"Get the latest CUDA version supported by driver.\n\n    Returns:\n        A string of the form :data:`'<major>.<minor>'`.\n\n    Raises:\n        CUDAError_InvalidValue:\n            If the driver call fails.\n        CUDAError_NotInitialized:\n            If the CUDA driver API is not initialized.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cuDriverGetVersion')\n\n    driver_version = _ctypes.c_int(0)\n    ret = fn(_ctypes.byref(driver_version))\n    _cudaCheckReturn(ret)\n    major = driver_version.value // 1000\n    minor = (driver_version.value % 1000) // 10\n    return f'{major}.{minor}'\n\n\ndef cuDeviceGetCount() -> int:\n    \"\"\"Get the number of compute-capable devices.\n\n    Returns: int\n        The number of devices with compute capability greater than or equal to 2.0 that are available\n        for execution. If there is no such device, :func:`cuDeviceGetCount` returns :data:`0`.\n\n    Raises:\n        CUDAError_InvalidContext:\n            If there is no context bound to the current thread.\n        CUDAError_InvalidValue:\n            If the driver call fails.\n        CUDAError_Deinitialized:\n            If the CUDA driver in the process is shutting down.\n        CUDAError_NotInitialized:\n            If the CUDA driver API is not initialized.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cuDeviceGetCount')\n\n    count = _ctypes.c_int(0)\n    ret = fn(_ctypes.byref(count))\n    _cudaCheckReturn(ret)\n    return count.value\n\n\ndef cuDeviceGet(ordinal: int) -> _c_CUdevice_t:\n    \"\"\"Get a handle to a compute device.\n\n    Returns:\n        A device handle given an ordinal in the range :code:`[0, ..., cuDeviceGetCount() - 1]`.\n\n    Raises:\n        CUDAError_InvalidContext:\n            If there is no context bound to the current thread.\n        CUDAError_InvalidDevice:\n            If the device ordinal supplied by the user does not correspond to a valid CUDA device or\n            that the action requested is invalid for the specified device.\n        CUDAError_InvalidValue:\n            If the driver call fails.\n        CUDAError_Deinitialized:\n            If the CUDA driver in the process is shutting down.\n        CUDAError_NotInitialized:\n            If the CUDA driver API is not initialized.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cuDeviceGet')\n\n    device = _c_CUdevice_t()\n    ret = fn(_ctypes.byref(device), _ctypes.c_int(ordinal))\n    _cudaCheckReturn(ret)\n    return device\n\n\ndef cuDeviceGetByPCIBusId(pciBusId: str) -> _c_CUdevice_t:\n    \"\"\"Get a handle to a compute device.\n\n    Args:\n        pciBusId (str):\n            String in one of the following forms: ``[domain]:[bus]:[device].[function]``,\n            ``[domain]:[bus]:[device]``, ``[bus]:[device].[function]`` where ``domain``, ``bus``,\n            ``device``, and ``function`` are all hexadecimal values.\n\n    Returns: int\n        A device handle given a PCI bus ID string.\n\n    Raises:\n        CUDAError_InvalidDevice:\n            If the device ordinal supplied by the user does not correspond to a valid CUDA device or\n            that the action requested is invalid for the specified device.\n        CUDAError_InvalidValue:\n            If the value of :data:`pciBusId` is not a valid PCI bus identifier.\n        CUDAError_Deinitialized:\n            If the CUDA driver in the process is shutting down.\n        CUDAError_NotInitialized:\n            If the CUDA driver API is not initialized.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cuDeviceGetByPCIBusId')\n\n    device = _c_CUdevice_t()\n    ret = fn(_ctypes.byref(device), _ctypes.c_char_p(pciBusId.encode('utf-8')))\n    _cudaCheckReturn(ret)\n    return device\n\n\ndef cuDeviceGetPCIBusId(device: _c_CUdevice_t) -> str:\n    \"\"\"Get a PCI Bus Id string for the device.\n\n    Returns: str\n        An identifier string for the device in the following format ``[domain]:[bus]:[device].[function]``\n        where ``domain``, ``bus``, ``device``, and ``function`` are all hexadecimal values.\n\n    Raises:\n        CUDAError_InvalidDevice:\n            If the device ordinal supplied by the user does not correspond to a valid CUDA device or\n            that the action requested is invalid for the specified device.\n        CUDAError_InvalidValue:\n            If the driver call fails.\n        CUDAError_Deinitialized:\n            If the CUDA driver in the process is shutting down.\n        CUDAError_NotInitialized:\n            If the CUDA driver API is not initialized.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cuDeviceGetPCIBusId')\n\n    pciBusId = _ctypes.create_string_buffer(256)\n    ret = fn(pciBusId, _ctypes.c_int(256), device)\n    _cudaCheckReturn(ret)\n    return pciBusId.value.decode('utf-8', errors='replace')\n\n\ndef cuDeviceGetName(device: _c_CUdevice_t) -> str:\n    \"\"\"Get an identifier string for the device.\n\n    Returns: str\n        An ASCII string identifying the device.\n\n    Raises:\n        CUDAError_InvalidContext:\n            If there is no context bound to the current thread.\n        CUDAError_InvalidDevice:\n            If the device ordinal supplied by the user does not correspond to a valid CUDA device or\n            that the action requested is invalid for the specified device.\n        CUDAError_InvalidValue:\n            If the driver call fails.\n        CUDAError_Deinitialized:\n            If the CUDA driver in the process is shutting down.\n        CUDAError_NotInitialized:\n            If the CUDA driver API is not initialized.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cuDeviceGetName')\n\n    name = _ctypes.create_string_buffer(256)\n    ret = fn(name, _ctypes.c_int(256), device)\n    _cudaCheckReturn(ret)\n    return name.value.decode('utf-8', errors='replace')\n\n\ndef cuDeviceGetUuid(device: _c_CUdevice_t) -> str:\n    \"\"\"Get a UUID for the device.\n\n    Raises:\n        CUDAError_InvalidDevice:\n            If the device ordinal supplied by the user does not correspond to a valid CUDA device or\n            that the action requested is invalid for the specified device.\n        CUDAError_InvalidValue:\n            If the driver call fails.\n        CUDAError_Deinitialized:\n            If the CUDA driver in the process is shutting down.\n        CUDAError_NotInitialized:\n            If the CUDA driver API is not initialized.\n    \"\"\"\n    try:\n        fn = __cudaGetFunctionPointer('cuDeviceGetUuid_v2')\n    except CUDAError_NotFound:  # noqa: F821 # pylint: disable=undefined-variable\n        fn = __cudaGetFunctionPointer('cuDeviceGetUuid')\n\n    uuid = (_ctypes.c_ubyte * 16)()\n    ret = fn(uuid, device)\n    _cudaCheckReturn(ret)\n    uuid = ''.join(map('{:02x}'.format, uuid))\n    return '-'.join((uuid[:8], uuid[8:12], uuid[12:16], uuid[16:20], uuid[20:32]))\n\n\ndef cuDeviceGetUuid_v2(device: _c_CUdevice_t) -> str:\n    \"\"\"Get a UUID for the device (CUDA 11.4+).\n\n    Raises:\n        CUDAError_InvalidDevice:\n            If the device ordinal supplied by the user does not correspond to a valid CUDA device or\n            that the action requested is invalid for the specified device.\n        CUDAError_InvalidValue:\n            If the driver call fails.\n        CUDAError_Deinitialized:\n            If the CUDA driver in the process is shutting down.\n        CUDAError_NotInitialized:\n            If the CUDA driver API is not initialized.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cuDeviceGetUuid_v2')\n\n    uuid = (_ctypes.c_ubyte * 16)()\n    ret = fn(uuid, device)\n    _cudaCheckReturn(ret)\n    uuid = ''.join(map('{:02x}'.format, uuid))\n    return '-'.join((uuid[:8], uuid[8:12], uuid[12:16], uuid[16:20], uuid[20:32]))\n\n\ndef cuDeviceTotalMem(device: _c_CUdevice_t) -> int:\n    \"\"\"Get the total amount of memory on the device (in bytes).\n\n    Raises:\n        CUDAError_InvalidContext:\n            If there is no context bound to the current thread.\n        CUDAError_InvalidDevice:\n            If the device ordinal supplied by the user does not correspond to a valid CUDA device or\n            that the action requested is invalid for the specified device.\n        CUDAError_InvalidValue:\n            If the driver call fails.\n        CUDAError_Deinitialized:\n            If the CUDA driver in the process is shutting down.\n        CUDAError_NotInitialized:\n            If the CUDA driver API is not initialized.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cuDeviceTotalMem')\n\n    bytes = _ctypes.c_size_t(0)  # pylint: disable=redefined-builtin\n    ret = fn(_ctypes.byref(bytes), device)\n    _cudaCheckReturn(ret)\n    return bytes.value\n\n\ndef is_available() -> bool:\n    \"\"\"Test whether there are any CUDA visible devices.\"\"\"\n    try:\n        return cuDeviceGetCount() > 0\n    except CUDAError:\n        return False\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "nvitop/api/libcuda.py", "language": "python", "complexity": "complex", "star_count": 5792, "quality_score": 0.94, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 748}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This file is part of nvitop, the interactive NVIDIA-GPU process viewer.\n#\n# Copyright 2021-2025 Xuehai Pan. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Python bindings for the `CUDA Runtime APIs <https://docs.nvidia.com/cuda/cuda-runtime-api>`_.\"\"\"\n\n# pylint: disable=invalid-name\n\nfrom __future__ import annotations\n\nimport ctypes as _ctypes\nimport glob as _glob\nimport os as _os\nimport platform as _platform\nimport sys as _sys\nimport threading as _threading\nfrom typing import TYPE_CHECKING as _TYPE_CHECKING\nfrom typing import Any as _Any\nfrom typing import ClassVar as _ClassVar\n\n\nif _TYPE_CHECKING:\n    from collections.abc import Callable as _Callable\n    from typing_extensions import Self as _Self  # Python 3.11+\n\n\n_cudaError_t = _ctypes.c_int\n\n# Error codes #\n# pylint: disable=line-too-long\ncudaSuccess = 0\n\"\"\"The API call returned with no errors. In the case of query calls, this also means that the operation being queried is complete (see :func:`cudaEventQuery` and :func:`cudaStreamQuery`).\"\"\"\ncudaErrorInvalidValue = 1\n\"\"\"This indicates that one or more of the parameters passed to the API call is not within an acceptable range of values.\"\"\"\ncudaErrorMemoryAllocation = 2\n\"\"\"The API call failed because it was unable to allocate enough memory to perform the requested operation.\"\"\"\ncudaErrorInitializationError = 3\n\"\"\"The API call failed because the CUDA driver and runtime could not be initialized.\"\"\"\ncudaErrorCudartUnloading = 4\n\"\"\"This indicates that a CUDA Runtime API call cannot be executed because it is being called during process shut down, at a point in time after CUDA driver has been unloaded.\"\"\"\ncudaErrorProfilerDisabled = 5\n\"\"\"This indicates profiler is not initialized for this run. This can happen when the application is running with external profiling tools like visual profiler.\"\"\"\ncudaErrorInvalidConfiguration = 9\n\"\"\"This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requesting more shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks. See cudaDeviceProp for more device limitations.\"\"\"\ncudaErrorInvalidPitchValue = 12\n\"\"\"This indicates that one or more of the pitch-related parameters passed to the API call is not within the acceptable range for pitch.\"\"\"\ncudaErrorInvalidSymbol = 13\n\"\"\"This indicates that the symbol name / identifier passed to the API call is not a valid name or identifier.\"\"\"\ncudaErrorInvalidTexture = 18\n\"\"\"This indicates that the texture passed to the API call is not a valid texture.\"\"\"\ncudaErrorInvalidTextureBinding = 19\n\"\"\"This indicates that the texture binding is not valid. This occurs if you call :func:`cudaGetTextureAlignmentOffset` with an unbound texture.\"\"\"\ncudaErrorInvalidChannelDescriptor = 20\n\"\"\"This indicates that the channel descriptor passed to the API call is not valid. This occurs if the format is not one of the formats specified by :data:`cudaChannelFormatKind`, or if one of the dimensions is invalid.\"\"\"\ncudaErrorInvalidMemcpyDirection = 21\n\"\"\"This indicates that the direction of the :func:`memcpy` passed to the API call is not one of the types specified by :data:`cudaMemcpyKind`.\"\"\"\ncudaErrorInvalidFilterSetting = 26\n\"\"\"This indicates that a non-float texture was being accessed with linear filtering. This is not supported by CUDA.\"\"\"\ncudaErrorInvalidNormSetting = 27\n\"\"\"This indicates that an attempt was made to read a non-float texture as a normalized float. This is not supported by CUDA.\"\"\"\ncudaErrorStubLibrary = 34\n\"\"\"This indicates that the CUDA driver that the application has loaded is a stub library. Applications that run with the stub rather than a real driver loaded will result in CUDA API returning this error.\"\"\"\ncudaErrorInsufficientDriver = 35\n\"\"\"This indicates that the installed NVIDIA CUDA driver is older than the CUDA Runtime library. This is not a supported configuration. Users should install an updated NVIDIA display driver to allow the application to run.\"\"\"\ncudaErrorCallRequiresNewerDriver = 36\n\"\"\"This indicates that the API call requires a newer CUDA driver than the one currently installed. Users should install an updated NVIDIA CUDA driver to allow the API call to succeed.\"\"\"\ncudaErrorInvalidSurface = 37\n\"\"\"This indicates that the surface passed to the API call is not a valid surface.\"\"\"\ncudaErrorDuplicateVariableName = 43\n\"\"\"This indicates that multiple global or constant variables (across separate CUDA source files in the application) share the same string name.\"\"\"\ncudaErrorDuplicateTextureName = 44\n\"\"\"This indicates that multiple textures (across separate CUDA source files in the application) share the same string name.\"\"\"\ncudaErrorDuplicateSurfaceName = 45\n\"\"\"This indicates that multiple surfaces (across separate CUDA source files in the application) share the same string name.\"\"\"\ncudaErrorDevicesUnavailable = 46\n\"\"\"This indicates that all CUDA devices are busy or unavailable at the current time. Devices are often busy / unavailable due to use of :data:`cudaComputeModeProhibited`, :data:`cudaComputeModeExclusiveProcess`, or when long running CUDA kernels have filled up the GPU and are blocking new work from starting. They can also be unavailable due to memory constraints on a device that already has active CUDA work being performed.\"\"\"\ncudaErrorIncompatibleDriverContext = 49\n\"\"\"This indicates that the current context is not compatible with this the CUDA Runtime. This can only occur if you are using CUDA Runtime / Driver interoperability and have created an existing Driver context using the driver API. The Driver context may be incompatible either because the Driver context was created using an older version of the API, because the Runtime API call expects a primary driver context and the Driver context is not primary, or because the Driver context has been destroyed.\"\"\"\ncudaErrorMissingConfiguration = 52\n\"\"\"The device function being invoked (usually via :func:`cudaLaunchKernel`) was not previously configured via the :func:`cudaConfigureCall` function.\"\"\"\ncudaErrorLaunchMaxDepthExceeded = 65\n\"\"\"This error indicates that a device runtime grid launch did not occur because the depth of the child grid would exceed the maximum supported number of nested grid launches.\"\"\"\ncudaErrorLaunchFileScopedTex = 66\n\"\"\"This error indicates that a grid launch did not occur because the kernel uses file-scoped textures which are unsupported by the device runtime. Kernels launched via the device runtime only support textures created with the Texture Object API's.\"\"\"\ncudaErrorLaunchFileScopedSurf = 67\n\"\"\"This error indicates that a grid launch did not occur because the kernel uses file-scoped surfaces which are unsupported by the device runtime. Kernels launched via the device runtime only support surfaces created with the Surface Object API's.\"\"\"\ncudaErrorSyncDepthExceeded = 68\n\"\"\"This error indicates that a call to :func:`cudaDeviceSynchronize` made from the device runtime failed because the call was made at grid depth greater than than either the default (2 levels of grids) or user specified device limit :data:`cudaLimitDevRuntimeSyncDepth`. To be able to synchronize on launched grids at a greater depth successfully, the maximum nested depth at which :func:`cudaDeviceSynchronize` will be called must be specified with the :data:`cudaLimitDevRuntimeSyncDepth` limit to the :func:`cudaDeviceSetLimit` api before the host-side launch of a kernel using the device runtime. Keep in mind that additional levels of sync depth require the runtime to reserve large amounts of device memory that cannot be used for user allocations. Note that :func:`cudaDeviceSynchronize` made from device runtime is only supported on devices of compute capability < 9.0.\"\"\"\ncudaErrorLaunchPendingCountExceeded = 69\n\"\"\"This error indicates that a device runtime grid launch failed because the launch would exceed the limit :data:`cudaLimitDevRuntimePendingLaunchCount`. For this launch to proceed successfully, :func:`cudaDeviceSetLimit` must be called to set the :data:`cudaLimitDevRuntimePendingLaunchCount` to be higher than the upper bound of outstanding launches that can be issued to the device runtime. Keep in mind that raising the limit of pending device runtime launches will require the runtime to reserve device memory that cannot be used for user allocations.\"\"\"\ncudaErrorInvalidDeviceFunction = 98\n\"\"\"The requested device function does not exist or is not compiled for the proper device architecture.\"\"\"\ncudaErrorNoDevice = 100\n\"\"\"This indicates that no CUDA-capable devices were detected by the installed CUDA driver.\"\"\"\ncudaErrorInvalidDevice = 101\n\"\"\"This indicates that the device ordinal supplied by the user does not correspond to a valid CUDA device or that the action requested is invalid for the specified device.\"\"\"\ncudaErrorDeviceNotLicensed = 102\n\"\"\"This indicates that the device doesn't have a valid Grid License.\"\"\"\ncudaErrorSoftwareValidityNotEstablished = 103\n\"\"\"By default, the CUDA Runtime may perform a minimal set of self-tests, as well as CUDA driver tests, to establish the validity of both. Introduced in CUDA 11.2, this error return indicates that at least one of these tests has failed and the validity of either the runtime or the driver could not be established.\"\"\"\ncudaErrorStartupFailure = 127\n\"\"\"This indicates an internal startup failure in the CUDA Runtime.\"\"\"\ncudaErrorInvalidKernelImage = 200\n\"\"\"This indicates that the device kernel image is invalid.\"\"\"\ncudaErrorDeviceUninitialized = 201\n\"\"\"This most frequently indicates that there is no context bound to the current thread. This can also be returned if the context passed to an API call is not a valid handle (such as a context that has had :func`cuCtxDestroy` invoked on it). This can also be returned if a user mixes different API versions (i.e. 3010 context with 3020 API calls).\"\"\"\ncudaErrorMapBufferObjectFailed = 205\n\"\"\"This indicates that the buffer object could not be mapped.\"\"\"\ncudaErrorUnmapBufferObjectFailed = 206\n\"\"\"This indicates that the buffer object could not be unmapped.\"\"\"\ncudaErrorArrayIsMapped = 207\n\"\"\"This indicates that the specified array is currently mapped and thus cannot be destroyed.\"\"\"\ncudaErrorAlreadyMapped = 208\n\"\"\"This indicates that the resource is already mapped.\"\"\"\ncudaErrorNoKernelImageForDevice = 209\n\"\"\"This indicates that there is no kernel image available that is suitable for the device. This can occur when a user specifies code generation options for a particular CUDA source file that do not include the corresponding device configuration.\"\"\"\ncudaErrorAlreadyAcquired = 210\n\"\"\"This indicates that a resource has already been acquired.\"\"\"\ncudaErrorNotMapped = 211\n\"\"\"This indicates that a resource is not mapped.\"\"\"\ncudaErrorNotMappedAsArray = 212\n\"\"\"This indicates that a mapped resource is not available for access as an array.\"\"\"\ncudaErrorNotMappedAsPointer = 213\n\"\"\"This indicates that a mapped resource is not available for access as a pointer.\"\"\"\ncudaErrorECCUncorrectable = 214\n\"\"\"This indicates that an uncorrectable ECC error was detected during execution.\"\"\"\ncudaErrorUnsupportedLimit = 215\n\"\"\"This indicates that the :class:`cudaLimit` passed to the API call is not supported by the active device.\"\"\"\ncudaErrorDeviceAlreadyInUse = 216\n\"\"\"This indicates that a call tried to access an exclusive-thread device that is already in use by a different thread.\"\"\"\ncudaErrorPeerAccessUnsupported = 217\n\"\"\"This error indicates that P2P access is not supported across the given devices.\"\"\"\ncudaErrorInvalidPtx = 218\n\"\"\"A PTX compilation failed. The runtime may fall back to compiling PTX if an application does not contain a suitable binary for the current device.\"\"\"\ncudaErrorInvalidGraphicsContext = 219\n\"\"\"This indicates an error with the OpenGL or DirectX context.\"\"\"\ncudaErrorNvlinkUncorrectable = 220\n\"\"\"This indicates that an uncorrectable NVLink error was detected during the execution.\"\"\"\ncudaErrorJitCompilerNotFound = 221\n\"\"\"This indicates that the PTX JIT compiler library was not found. The JIT Compiler library is used for PTX compilation. The runtime may fall back to compiling PTX if an application does not contain a suitable binary for the current device.\"\"\"\ncudaErrorUnsupportedPtxVersion = 222\n\"\"\"This indicates that the provided PTX was compiled with an unsupported toolchain. The most common reason for this, is the PTX was generated by a compiler newer than what is supported by the CUDA driver and PTX JIT compiler.\"\"\"\ncudaErrorJitCompilationDisabled = 223\n\"\"\"This indicates that the JIT compilation was disabled. The JIT compilation compiles PTX. The runtime may fall back to compiling PTX if an application does not contain a suitable binary for the current device.\"\"\"\ncudaErrorUnsupportedExecAffinity = 224\n\"\"\"This indicates that the provided execution affinity is not supported by the device.\"\"\"\ncudaErrorInvalidSource = 300\n\"\"\"This indicates that the device kernel source is invalid.\"\"\"\ncudaErrorFileNotFound = 301\n\"\"\"This indicates that the file specified was not found.\"\"\"\ncudaErrorSharedObjectSymbolNotFound = 302\n\"\"\"This indicates that a link to a shared object failed to resolve.\"\"\"\ncudaErrorSharedObjectInitFailed = 303\n\"\"\"This indicates that initialization of a shared object failed.\"\"\"\ncudaErrorOperatingSystem = 304\n\"\"\"This error indicates that an OS call failed.\"\"\"\ncudaErrorInvalidResourceHandle = 400\n\"\"\"This indicates that a resource handle passed to the API call was not valid. Resource handles are opaque types like :data:`cudaStream_t` and :data:`cudaEvent_t`.\"\"\"\ncudaErrorIllegalState = 401\n\"\"\"This indicates that a resource required by the API call is not in a valid state to perform the requested operation.\"\"\"\ncudaErrorSymbolNotFound = 500\n\"\"\"This indicates that a named symbol was not found. Examples of symbols are global / constant variable names, driver function names, texture names, and surface names.\"\"\"\ncudaErrorNotReady = 600\n\"\"\"This indicates that asynchronous operations issued previously have not completed yet. This result is not actually an error, but must be indicated differently than :data:`cudaSuccess` (which indicates completion). Calls that may return this value include :func:`cudaEventQuery` and :func:`cudaStreamQuery`.\"\"\"\ncudaErrorIllegalAddress = 700\n\"\"\"The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\ncudaErrorLaunchOutOfResources = 701\n\"\"\"This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar to :data:`cudaErrorInvalidConfiguration`, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernel launch specifies too many threads for the kernel's register count.\"\"\"\ncudaErrorLaunchTimeout = 702\n\"\"\"This indicates that the device kernel took too long to execute. This can only occur if timeouts are enabled - see the device property kernelExecTimeoutEnabled for more information. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\ncudaErrorLaunchIncompatibleTexturing = 703\n\"\"\"This error indicates a kernel launch that uses an incompatible texturing mode.\"\"\"\ncudaErrorPeerAccessAlreadyEnabled = 704\n\"\"\"This error indicates that a call to :func:`cudaDeviceEnablePeerAccess` is trying to re-enable peer addressing on from a context which has already had peer addressing enabled.\"\"\"\ncudaErrorPeerAccessNotEnabled = 705\n\"\"\"This error indicates that :func:`cudaDeviceDisablePeerAccess` is trying to disable peer addressing which has not been enabled yet via :func:`cudaDeviceEnablePeerAccess`.\"\"\"\ncudaErrorSetOnActiveProcess = 708\n\"\"\"This indicates that the user has called :func:`cudaSetValidDevices`, :func:`cudaSetDeviceFlags`, :func:`cudaD3D9SetDirect3DDevice`, :func:`cudaD3D10SetDirect3DDevice`, :func:`cudaD3D11SetDirect3DDevice`, or :func:`cudaVDPAUSetVDPAUDevice` after initializing the CUDA Runtime by calling non-device management operations (allocating memory and launching kernels are examples of non-device management operations). This error can also be returned if using runtime / driver interoperability and there is an existing :class:`CUcontext` active on the host thread.\"\"\"\ncudaErrorContextIsDestroyed = 709\n\"\"\"This error indicates that the context current to the calling thread has been destroyed using cuCtxDestroy, or is a primary context which has not yet been initialized.\"\"\"\ncudaErrorAssert = 710\n\"\"\"An assert triggered in device code during kernel execution. The device cannot be used again. All existing allocations are invalid. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\ncudaErrorTooManyPeers = 711\n\"\"\"This error indicates that the hardware resources required to enable peer access have been exhausted for one or more of the devices passed to :func:`cudaEnablePeerAccess`.\"\"\"\ncudaErrorHostMemoryAlreadyRegistered = 712\n\"\"\"This error indicates that the memory range passed to :func:`cudaHostRegister` has already been registered.\"\"\"\ncudaErrorHostMemoryNotRegistered = 713\n\"\"\"This error indicates that the pointer passed to :func:`cudaHostUnregister` does not correspond to any currently registered memory region.\"\"\"\ncudaErrorHardwareStackError = 714\n\"\"\"Device encountered an error in the call stack during kernel execution, possibly due to stack corruption or exceeding the stack size limit. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\ncudaErrorIllegalInstruction = 715\n\"\"\"The device encountered an illegal instruction during kernel execution This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\ncudaErrorMisalignedAddress = 716\n\"\"\"The device encountered a load or store instruction on a memory address which is not aligned. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\ncudaErrorInvalidAddressSpace = 717\n\"\"\"While executing a kernel, the device encountered an instruction which can only operate on memory locations in certain address spaces (global, shared, or local), but was supplied a memory address not belonging to an allowed address space. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\ncudaErrorInvalidPc = 718\n\"\"\"The device encountered an invalid program counter. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\ncudaErrorLaunchFailure = 719\n\"\"\"An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointer and accessing out of bounds shared memory. Less common cases can be system specific - more information about these cases can be found in the system specific user guide. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\ncudaErrorCooperativeLaunchTooLarge = 720\n\"\"\"This error indicates that the number of blocks launched per grid for a kernel that was launched via either :func:`cudaLaunchCooperativeKernel` or :func:`cudaLaunchCooperativeKernelMultiDevice` exceeds the maximum number of blocks as allowed by :func:`cudaOccupancyMaxActiveBlocksPerMultiprocessor` or :func:`cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags` times the number of multiprocessors as specified by the device attribute :func:`cudaDevAttrMultiProcessorCount`.\"\"\"\ncudaErrorNotPermitted = 800\n\"\"\"This error indicates the attempted operation is not permitted.\"\"\"\ncudaErrorNotSupported = 801\n\"\"\"This error indicates the attempted operation is not supported on the current system or device.\"\"\"\ncudaErrorSystemNotReady = 802\n\"\"\"This error indicates that the system is not yet ready to start any CUDA work. To continue using CUDA, verify the system configuration is in a valid state and all required driver daemons are actively running. More information about this error can be found in the system specific user guide.\"\"\"\ncudaErrorSystemDriverMismatch = 803\n\"\"\"This error indicates that there is a mismatch between the versions of the display driver and the CUDA driver. Refer to the compatibility documentation for supported versions.\"\"\"\ncudaErrorCompatNotSupportedOnDevice = 804\n\"\"\"This error indicates that the system was upgraded to run with forward compatibility but the visible hardware detected by CUDA does not support this configuration. Refer to the compatibility documentation for the supported hardware matrix or ensure that only supported hardware is visible during initialization via the ``CUDA_VISIBLE_DEVICES`` environment variable.\"\"\"\ncudaErrorMpsConnectionFailed = 805\n\"\"\"This error indicates that the MPS client failed to connect to the MPS control daemon or the MPS server.\"\"\"\ncudaErrorMpsRpcFailure = 806\n\"\"\"This error indicates that the remote procedural call between the MPS server and the MPS client failed.\"\"\"\ncudaErrorMpsServerNotReady = 807\n\"\"\"This error indicates that the MPS server is not ready to accept new MPS client requests. This error can be returned when the MPS server is in the process of recovering from a fatal failure.\"\"\"\ncudaErrorMpsMaxClientsReached = 808\n\"\"\"This error indicates that the hardware resources required to create MPS client have been exhausted.\"\"\"\ncudaErrorMpsMaxConnectionsReached = 809\n\"\"\"This error indicates the the hardware resources required to device connections have been exhausted.\"\"\"\ncudaErrorMpsClientTerminated = 810\n\"\"\"This error indicates that the MPS client has been terminated by the server. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\ncudaErrorCdpNotSupported = 811\n\"\"\"This error indicates, that the program is using CUDA Dynamic Parallelism, but the current configuration, like MPS, does not support it.\"\"\"\ncudaErrorCdpVersionMismatch = 812\n\"\"\"This error indicates, that the program contains an unsupported interaction between different versions of CUDA Dynamic Parallelism.\"\"\"\ncudaErrorStreamCaptureUnsupported = 900\n\"\"\"The operation is not permitted when the stream is capturing.\"\"\"\ncudaErrorStreamCaptureInvalidated = 901\n\"\"\"The current capture sequence on the stream has been invalidated due to a previous error.\"\"\"\ncudaErrorStreamCaptureMerge = 902\n\"\"\"The operation would have resulted in a merge of two independent capture sequences.\"\"\"\ncudaErrorStreamCaptureUnmatched = 903\n\"\"\"The capture was not initiated in this stream.\"\"\"\ncudaErrorStreamCaptureUnjoined = 904\n\"\"\"The capture sequence contains a fork that was not joined to the primary stream.\"\"\"\ncudaErrorStreamCaptureIsolation = 905\n\"\"\"A dependency would have been created which crosses the capture sequence boundary. Only implicit in-stream ordering dependencies are allowed to cross the boundary.\"\"\"\ncudaErrorStreamCaptureImplicit = 906\n\"\"\"The operation would have resulted in a disallowed implicit dependency on a current capture sequence from :data:`cudaStreamLegacy`.\"\"\"\ncudaErrorCapturedEvent = 907\n\"\"\"The operation is not permitted on an event which was last recorded in a capturing stream.\"\"\"\ncudaErrorStreamCaptureWrongThread = 908\n\"\"\"A stream capture sequence not initiated with the :data:`cudaStreamCaptureModeRelaxed` argument to :func:`cudaStreamBeginCapture` was passed to :func:`cudaStreamEndCapture` in a different thread.\"\"\"\ncudaErrorTimeout = 909\n\"\"\"This indicates that the wait operation has timed out.\"\"\"\ncudaErrorGraphExecUpdateFailure = 910\n\"\"\"This error indicates that the graph update was not performed because it included changes which violated constraints specific to instantiated graph update.\"\"\"\ncudaErrorExternalDevice = 911\n\"\"\"This indicates that an async error has occurred in a device outside of CUDA. If CUDA was waiting for an external device's signal before consuming shared data, the external device signaled an error indicating that the data is not valid for consumption. This leaves the process in an inconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched.\"\"\"\ncudaErrorInvalidClusterSize = 912\n\"\"\"This indicates that a kernel launch error has occurred due to cluster misconfiguration.\"\"\"\ncudaErrorUnknown = 999\n\"\"\"This indicates that an unknown internal error has occurred.\"\"\"\n# pylint: enable=line-too-long\n\n\n# Error Checking #\nclass cudaError(Exception):\n    \"\"\"Base exception class for CUDA driver query errors.\"\"\"\n\n    _value_class_mapping: _ClassVar[dict[int, type[cudaError]]] = {}\n    _errcode_to_string: _ClassVar[dict[int, str]] = {  # List of currently known error codes\n        cudaErrorInitializationError:        'Initialization error.',\n        cudaErrorSymbolNotFound:             'Named symbol not found.',\n        cudaErrorInvalidValue:               'Invalid argument.',\n        cudaErrorNoDevice:                   'No CUDA-capable device is detected.',\n        cudaErrorInvalidDevice:              'Invalid device ordinal.',\n        cudaErrorSystemDriverMismatch:       'System has unsupported display driver / CUDA driver combination.',\n        cudaErrorCudartUnloading:            'Driver shutting down.',\n        cudaErrorCompatNotSupportedOnDevice: 'Forward compatibility was attempted on non supported Hardware.',\n        cudaErrorDeviceUninitialized:        'Invalid device context.',\n    }  # fmt:skip\n    _errcode_to_name: _ClassVar[dict[int, str]] = {}\n    value: int\n\n    def __new__(cls, value: int) -> _Self:\n        \"\"\"Map value to a proper subclass of :class:`cudaError`.\"\"\"\n        if cls is cudaError:\n            # pylint: disable-next=self-cls-assignment\n            cls = cudaError._value_class_mapping.get(value, cls)  # type: ignore[assignment]\n        obj = Exception.__new__(cls)\n        obj.value = value\n        return obj\n\n    def __repr__(self) -> str:\n        \"\"\"Return a string representation of the error.\"\"\"\n        # pylint: disable=no-member\n        try:\n            if self.value not in cudaError._errcode_to_string:\n                cudaError._errcode_to_string[self.value] = '{}.'.format(\n                    cuGetErrorString(self.value).rstrip('.').capitalize(),\n                )\n            if self.value not in cudaError._errcode_to_name:\n                cudaError._errcode_to_name[self.value] = cudaGetErrorName(self.value)\n            return (\n                f'{cudaError._errcode_to_string[self.value]} '\n                f'Code: {cudaError._errcode_to_name[self.value]} ({self.value}).'\n            )\n        except cudaError:\n            return f'CUDA Error with code {self.value}.'\n\n    def __eq__(self, other: object) -> bool:\n        \"\"\"Test equality to other object.\"\"\"\n        if not isinstance(other, cudaError):\n            return NotImplemented\n        return self.value == other.value  # pylint: disable=no-member\n\n    def __reduce__(self) -> tuple[type[cudaError], tuple[int]]:\n        \"\"\"Return state information for pickling.\"\"\"\n        return cudaError, (self.value,)  # pylint: disable=no-member\n\n\ndef cudaExceptionClass(cudaErrorCode: int, /) -> type[cudaError]:\n    \"\"\"Map value to a proper subclass of :class:`cudaError`.\n\n    Raises:\n        ValueError: If the error code is not valid.\n    \"\"\"\n    if cudaErrorCode not in cudaError._value_class_mapping:  # pylint: disable=protected-access\n        raise ValueError(f'cudaErrorCode {cudaErrorCode} is not valid.')\n    return cudaError._value_class_mapping[cudaErrorCode]  # pylint: disable=protected-access\n\n\ndef _extract_cuda_errors_as_classes() -> None:\n    \"\"\"Generate a hierarchy of classes on top of :class:`cudaError` class.\n\n    Each CUDA Error gets a new :class:`cudaError` subclass. This way try-except blocks can filter\n    appropriate exceptions more easily.\n\n    :class:`cudaError` is a parent class. Each ``cudaError*`` gets it's own subclass.\n    e.g. :data:`cudaErrorInvalidValue` will be turned into :class:`cudaError_InvalidValue`.\n    \"\"\"\n    this_module = _sys.modules[__name__]\n    cuda_error_names = [\n        x\n        for x in dir(this_module)\n        if x.startswith('cudaError') and not x.startswith('cudaError_') and x != 'cudaError'\n    ]\n    for err_name in cuda_error_names:\n        # e.g. Turn cudaErrorInvalidValue into cudaError_InvalidValue\n        class_name = err_name.replace('cudaError', 'cudaError_')\n        err_val = getattr(this_module, err_name)\n\n        def gen_new(value: int) -> _Callable[[type[cudaError]], cudaError]:\n            def new(cls: type[cudaError]) -> cudaError:\n                return cudaError.__new__(cls, value)\n\n            return new\n\n        # pylint: disable=protected-access\n        new_error_class = type(class_name, (cudaError,), {'__new__': gen_new(err_val)})\n        new_error_class.__module__ = __name__\n        if err_val in cudaError._errcode_to_string:\n            new_error_class.__doc__ = (\n                f'cudaError: {cudaError._errcode_to_string[err_val]} '\n                f'Code: :data:`{err_name}` ({err_val}).'\n            )\n        else:\n            new_error_class.__doc__ = f'CUDA Error with code :data:`{err_name}` ({err_val})'\n        setattr(this_module, class_name, new_error_class)\n        cudaError._value_class_mapping[err_val] = new_error_class\n        cudaError._errcode_to_name[err_val] = err_name\n\n\n# Add explicit references to appease linters\nclass __cudaError(cudaError):\n    value: int\n\n    def __new__(cls) -> cudaError:  # type: ignore[misc,empty-body]\n        ...\n\n\ncudaError_InitializationError: type[__cudaError]\ncudaError_SymbolNotFound: type[__cudaError]\ncudaError_InvalidValue: type[__cudaError]\ncudaError_NoDevice: type[__cudaError]\ncudaError_InvalidDevice: type[__cudaError]\ncudaError_SystemDriverMismatch: type[__cudaError]\ncudaError_CudartUnloading: type[__cudaError]\ncudaError_CompatNotSupportedOnDevice: type[__cudaError]\ncudaError_DeviceUninitialized: type[__cudaError]\n\n_extract_cuda_errors_as_classes()\ndel _extract_cuda_errors_as_classes\n\n\ndef _cudaCheckReturn(ret: _Any, /) -> _Any:\n    if ret != cudaSuccess:\n        raise cudaError(ret)\n    return ret\n\n\n# Function access #\n__cudaLib: _ctypes.CDLL | None = None\n__libLoadLock: _threading.Lock = _threading.Lock()\n# Function pointers are cached to prevent unnecessary libLoadLock locking\n__cudaGetFunctionPointer_cache: dict[str, _ctypes._CFuncPtr] = {}  # type: ignore[name-defined]\n\n\ndef __cudaGetFunctionPointer(name: str, /) -> _ctypes._CFuncPtr:  # type: ignore[name-defined]\n    \"\"\"Get the function pointer from the CUDA Runtime library.\n\n    Raises:\n        cudaError_InitializationError:\n            If cannot found the CUDA Runtime library.\n        cudaError_SymbolNotFound:\n            If cannot found the function pointer.\n    \"\"\"\n    if name in __cudaGetFunctionPointer_cache:\n        return __cudaGetFunctionPointer_cache[name]\n\n    if __cudaLib is None:\n        __LoadCudaLibrary()\n\n    with __libLoadLock:\n        try:\n            __cudaGetFunctionPointer_cache[name] = getattr(__cudaLib, name)\n            return __cudaGetFunctionPointer_cache[name]\n        except AttributeError as ex:\n            raise cudaError(cudaErrorSymbolNotFound) from ex\n\n\ndef __LoadCudaLibrary() -> None:  # pylint: disable=too-many-branches\n    \"\"\"Load the library if it isn't loaded already.\n\n    Raises:\n        cudaError_InitializationError:\n            If cannot found the CUDA Runtime library.\n    \"\"\"\n    global __cudaLib  # pylint: disable=global-statement\n\n    if __cudaLib is None:\n        # Lock to ensure only one caller loads the library\n        with __libLoadLock:\n            # Ensure the library still isn't loaded\n            if __cudaLib is None:  # pylint: disable=too-many-nested-blocks\n                # Platform specific libcudart location\n                system = _platform.system()\n                bits = 8 * _ctypes.sizeof(_ctypes.c_void_p)  # 64 or 32\n                if system == 'Darwin':\n                    lib_filenames = ['libcudart.dylib']\n                elif system == 'Linux':\n                    lib_filenames = ['libcudart.so']\n                elif system == 'Windows':\n                    lib_filenames = [f'cudart{bits}.dll', 'cudart.dll']\n                else:\n                    lib_filenames = []\n\n                # Open library\n                for lib_filename in lib_filenames:\n                    try:\n                        __cudaLib = _ctypes.CDLL(lib_filename)\n                        break\n                    except OSError:\n                        pass\n\n                # Try to load the library from the CUDA_PATH environment variable\n                if __cudaLib is None:\n                    cuda_paths = [\n                        _os.getenv(env_name, '')\n                        for env_name in ('CUDA_PATH', 'CUDA_HOME', 'CUDA_ROOT')\n                    ]\n                    if system != 'Windows':\n                        cuda_paths.append('/usr/local/cuda')\n                        candidate_paths = []\n                        for cuda_path in cuda_paths:\n                            if _os.path.isdir(cuda_path):\n                                for lib_filename in lib_filenames:\n                                    candidate_paths.extend(\n                                        [\n                                            _os.path.join(cuda_path, f'lib{bits}', lib_filename),\n                                            _os.path.join(cuda_path, 'lib', lib_filename),\n                                        ],\n                                    )\n                    else:\n                        candidate_dirs = _os.getenv('PATH', '').split(_os.path.pathsep)\n                        candidate_paths = []\n                        for cuda_path in cuda_paths:\n                            if _os.path.isdir(cuda_path):\n                                candidate_dirs.extend(\n                                    [\n                                        _os.path.join(cuda_path, 'bin'),\n                                        _os.path.join(cuda_path, f'lib{bits}'),\n                                        _os.path.join(cuda_path, 'lib'),\n                                    ],\n                                )\n                        for candidate_dir in candidate_dirs:\n                            candidate_paths.extend(\n                                _glob.iglob(_os.path.join(candidate_dir, f'cudart{bits}*.dll')),\n                            )\n\n                    # Normalize paths and remove duplicates\n                    candidate_paths = list(\n                        dict.fromkeys(\n                            _os.path.normpath(_os.path.normcase(p)) for p in candidate_paths\n                        ),\n                    )\n                    for lib_filename in candidate_paths:\n                        try:\n                            __cudaLib = _ctypes.CDLL(lib_filename)\n                            break\n                        except OSError:\n                            pass\n\n                if __cudaLib is None:\n                    _cudaCheckReturn(cudaErrorInitializationError)\n\n\ndef cudaGetErrorName(error: int) -> str:\n    \"\"\"Get the string representation of an error code enum name.\n\n    Returns: str\n        A string containing the name of an error code in the enum. If the error code is not\n        recognized, \"unrecognized error code\" is returned.\n\n    Raises:\n        cudaError_InitializationError:\n            If cannot found the CUDA Runtime library.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cudaGetErrorName')\n\n    fn.restype = _ctypes.c_char_p  # otherwise return is an int\n    p_name = fn(_cudaError_t(error))\n    name = _ctypes.string_at(p_name)\n    return name.decode('utf-8', errors='replace')\n\n\ndef cuGetErrorString(error: int) -> str:\n    \"\"\"Get the description string for an error code.\n\n    Returns: str\n        The description string for an error code. If the error code is not recognized, \"unrecognized\n        error code\" is returned.\n\n    Raises:\n        cudaError_InitializationError:\n            If cannot found the CUDA Runtime library.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cudaGetErrorString')\n\n    fn.restype = _ctypes.c_char_p  # otherwise return is an int\n    p_name = fn(_cudaError_t(error))\n    name = _ctypes.string_at(p_name)\n    return name.decode('utf-8', errors='replace')\n\n\ndef cudaGetLastError() -> int:\n    \"\"\"Get the last error from a runtime call.\n\n    Returns: int\n        The last error that has been produced by any of the runtime calls in the same instance of\n        the CUDA Runtime library in the host thread and resets it to :data:`cudaSuccess`.\n\n    Raises:\n        cudaError_InitializationError:\n            If cannot found the CUDA Runtime library.\n        cudaError_InsufficientDriver:\n            If the installed NVIDIA CUDA driver is older than the CUDA Runtime library.\n        cudaError_NoDevice:\n            If no CUDA-capable devices were detected by the installed CUDA driver.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cudaGetLastError')\n    return fn()\n\n\ndef cudaPeekAtLastError() -> int:\n    \"\"\"Get the last error from a runtime call.\n\n    Returns: int\n        The last error that has been produced by any of the runtime calls in the same instance of\n        the CUDA Runtime library in the host thread. This call does not reset the error to\n        :data:`cudaSuccess` like :func:`cudaGetLastError`.\n\n    Raises:\n        cudaError_InitializationError:\n            If cannot found the CUDA Runtime library.\n        cudaError_InsufficientDriver:\n            If the installed NVIDIA CUDA driver is older than the CUDA Runtime library.\n        cudaError_NoDevice:\n            If no CUDA-capable devices were detected by the installed CUDA driver.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cudaPeekAtLastError')\n    return fn()\n\n\ndef cudaDriverGetVersion() -> str:\n    \"\"\"Get the latest CUDA version supported by driver.\n\n    Returns: str\n        The latest version of CUDA supported by the driver of the form :data:`'<major>.<minor>'`.\n\n    Raises:\n        cudaError_InitializationError:\n            If cannot found the CUDA Runtime library.\n        cudaError_InsufficientDriver:\n            If the installed NVIDIA CUDA driver is older than the CUDA Runtime library.\n        cudaError_NoDevice:\n            If no CUDA-capable devices were detected by the installed CUDA driver.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cudaDriverGetVersion')\n\n    driver_version = _ctypes.c_int(0)\n    ret = fn(_ctypes.byref(driver_version))\n    _cudaCheckReturn(ret)\n    major = driver_version.value // 1000\n    minor = (driver_version.value % 1000) // 10\n    return f'{major}.{minor}'\n\n\ndef cudaRuntimeGetVersion() -> str:\n    \"\"\"Get the CUDA Runtime version.\n\n    Returns: str\n        The version number of the current CUDA Runtime instance of the form :data:`'<major>.<minor>'`.\n\n    Raises:\n        cudaError_InitializationError:\n            If cannot found the CUDA Runtime library.\n        cudaError_InsufficientDriver:\n            If the installed NVIDIA CUDA driver is older than the CUDA Runtime library.\n        cudaError_NoDevice:\n            If no CUDA-capable devices were detected by the installed CUDA driver.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cudaRuntimeGetVersion')\n\n    runtime_version = _ctypes.c_int(0)\n    ret = fn(_ctypes.byref(runtime_version))\n    _cudaCheckReturn(ret)\n    major = runtime_version.value // 1000\n    minor = (runtime_version.value % 1000) // 10\n    return f'{major}.{minor}'\n\n\ndef cudaGetDeviceCount() -> int:\n    \"\"\"Get the number of compute-capable devices.\n\n    Returns: int\n        The number of devices with compute capability greater or equal to 2.0 that are available for\n        execution.\n\n    Raises:\n        cudaError_InitializationError:\n            If cannot found the CUDA Runtime library.\n        cudaError_InsufficientDriver:\n            If the installed NVIDIA CUDA driver is older than the CUDA Runtime library.\n        cudaError_NoDevice:\n            If no CUDA-capable devices were detected by the installed CUDA driver.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cudaGetDeviceCount')\n\n    count = _ctypes.c_int(0)\n    ret = fn(_ctypes.byref(count))\n    _cudaCheckReturn(ret)\n    return count.value\n\n\ndef cudaDeviceGetByPCIBusId(pciBusId: str) -> int:\n    \"\"\"Get a handle to a compute device.\n\n    Args:\n        pciBusId (str):\n            String in one of the following forms: ``[domain]:[bus]:[device].[function]``,\n            ``[domain]:[bus]:[device]``, ``[bus]:[device].[function]`` where ``domain``, ``bus``,\n            ``device``, and ``function`` are all hexadecimal values.\n\n    Returns: int\n        A device ordinal given a PCI bus ID string.\n\n    Raises:\n        cudaError_InitializationError:\n            If cannot found the CUDA Runtime library.\n        cudaError_InsufficientDriver:\n            If the installed NVIDIA CUDA driver is older than the CUDA Runtime library.\n        cudaError_NoDevice:\n            If no CUDA-capable devices were detected by the installed CUDA driver.\n        cudaError_InvalidValue:\n            If the value of :data:`pciBusId` is not a valid PCI bus identifier.\n        cudaError_InvalidDevice:\n            If the device ordinal supplied by the user does not correspond to a valid CUDA device.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cudaDeviceGetByPCIBusId')\n\n    device = _ctypes.c_int(0)\n    ret = fn(_ctypes.byref(device), _ctypes.c_char_p(pciBusId.encode('utf-8')))\n    _cudaCheckReturn(ret)\n    return device.value\n\n\ndef cudaDeviceGetPCIBusId(device: int) -> str:\n    \"\"\"Get a PCI Bus Id string for the device.\n\n    Returns: str\n        An ASCII string identifying the device.\n\n    Raises:\n        cudaError_InitializationError:\n            If cannot found the CUDA Runtime library.\n        cudaError_InsufficientDriver:\n            If the installed NVIDIA CUDA driver is older than the CUDA Runtime library.\n        cudaError_NoDevice:\n            If no CUDA-capable devices were detected by the installed CUDA driver.\n        cudaError_InvalidValue:\n            If the value of :data:`device` is not a valid device ordinal.\n        cudaError_InvalidDevice:\n            If the device ordinal supplied by the user does not correspond to a valid CUDA device.\n    \"\"\"\n    fn = __cudaGetFunctionPointer('cudaDeviceGetPCIBusId')\n\n    pciBusId = _ctypes.create_string_buffer(256)\n    ret = fn(pciBusId, _ctypes.c_int(256), _ctypes.c_int(device))\n    _cudaCheckReturn(ret)\n    return pciBusId.value.decode('utf-8', errors='replace')\n\n\ndef is_available() -> bool:\n    \"\"\"Test whether there are any CUDA visible devices.\"\"\"\n    try:\n        return cudaGetDeviceCount() > 0\n    except cudaError:\n        return False\n", "metadata": {"source": "github.com/XuehaiPan/nvitop", "file_path": "nvitop/api/libcudart.py", "language": "python", "complexity": "complex", "star_count": 5792, "quality_score": 0.94, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 736}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Prometheus Community Code of Conduct\n\nPrometheus follows the [CNCF Code of Conduct](https://github.com/cncf/foundation/blob/main/code-of-conduct.md).\n", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "CODE_OF_CONDUCT.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.39999999999999997, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 4}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributing\n\nPrometheus uses GitHub to manage reviews of pull requests.\n\n* If you have a trivial fix or improvement, go ahead and create a pull request,\n  addressing (with `@...`) the maintainer of this repository (see\n  [MAINTAINERS.md](MAINTAINERS.md)) in the description of the pull request.\n\n* If you plan to do something more involved, first discuss your ideas on\n  [our mailing list]. This will avoid unnecessary work and surely give you and\n  us a good deal of inspiration.\n\n* Before your contributions can be landed, they must be signed off under the\n  [Developer Certificate of Origin] which asserts you own and have the right to\n  submit the change under the open source licence used by the project.\n\n## Testing\n\nSubmitted changes should pass the current tests, and be covered by new test\ncases when adding functionality.\n\n* Run the tests locally using [tox] which executes the full suite on all\n  supported Python versions installed.\n\n* Each pull request is gated using [Travis CI] with the results linked on the\n  github page. This must pass before the change can land, note pushing a new\n  change will trigger a retest.\n\n## Style\n\n* Code style should follow [PEP 8] generally, and can be checked by running:\n  ``tox -e flake8``.\n\n* Import statements can be automatically formatted using [isort].\n\n\n[our mailing list]: https://groups.google.com/forum/?fromgroups#!forum/prometheus-developers\n[Developer Certificate of Origin]: https://github.com/prometheus/prometheus/wiki/DCO-signing\n[isort]: https://pypi.org/project/isort/\n[PEP 8]: https://www.python.org/dev/peps/pep-0008/\n[tox]: https://tox.readthedocs.io/en/latest/\n[Travis CI]: https://docs.travis-ci.com/\n", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.63, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 43}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Prometheus Python Client\n\nThe official Python client for [Prometheus](https://prometheus.io).\n\n## Installation\n\n```\npip install prometheus-client\n```\n\nThis package can be found on [PyPI](https://pypi.python.org/pypi/prometheus_client).\n\n## Documentation\n\nDocumentation is available on https://prometheus.github.io/client_python\n\n## Links\n\n* [Releases](https://github.com/prometheus/client_python/releases): The releases page shows the history of the project and acts as a changelog.\n* [PyPI](https://pypi.python.org/pypi/prometheus_client)\n", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "README.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.6, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Reporting a security issue\n\nThe Prometheus security policy, including how to report vulnerabilities, can be\nfound here:\n\n<https://prometheus.io/docs/operating/security/>\n", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "SECURITY.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.39999999999999997, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 7}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\ntitle: client_python\nweight: 1\n---\n\nThis tutorial shows the quickest way to get started with the Prometheus Python library.\n\n**One**: Install the client:\n\n```shell\npip install prometheus-client\n```\n\n**Two**: Paste the following into a Python interpreter:\n\n```python\nfrom prometheus_client import start_http_server, Summary\nimport random\nimport time\n\n# Create a metric to track time spent and requests made.\nREQUEST_TIME = Summary('request_processing_seconds', 'Time spent processing request')\n\n# Decorate function with metric.\n@REQUEST_TIME.time()\ndef process_request(t):\n    \"\"\"A dummy function that takes some time.\"\"\"\n    time.sleep(t)\n\nif __name__ == '__main__':\n    # Start up the server to expose the metrics.\n    start_http_server(8000)\n    # Generate some requests.\n    while True:\n        process_request(random.random())\n```\n\n**Three**: Visit [http://localhost:8000/](http://localhost:8000/) to view the metrics.\n\nFrom one easy to use decorator you get:\n\n* `request_processing_seconds_count`: Number of times this function was called.\n* `request_processing_seconds_sum`: Total amount of time spent in this function.\n\nPrometheus's `rate` function allows calculation of both requests per second,\nand latency over time from this data.\n\nIn addition if you're on Linux the `process` metrics expose CPU, memory and\nother information about the process for free!\n", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "docs/content/_index.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.72, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 50}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\ntitle: Bridges\nweight: 5\n---\n\nIt is also possible to expose metrics to systems other than Prometheus.\nThis allows you to take advantage of Prometheus instrumentation even\nif you are not quite ready to fully transition to Prometheus yet.", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "docs/content/bridges/_index.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.39999999999999997, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 8}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\ntitle: Graphite\nweight: 1\n---\n\nMetrics are pushed over TCP in the Graphite plaintext format.\n\n```python\nfrom prometheus_client.bridge.graphite import GraphiteBridge\n\ngb = GraphiteBridge(('graphite.your.org', 2003))\n# Push once.\ngb.push()\n# Push every 10 seconds in a daemon thread.\ngb.start(10.0)\n```\n\nGraphite [tags](https://grafana.com/blog/2018/01/11/graphite-1.1-teaching-an-old-dog-new-tricks/) are also supported.\n\n```python\nfrom prometheus_client.bridge.graphite import GraphiteBridge\n\ngb = GraphiteBridge(('graphite.your.org', 2003), tags=True)\nc = Counter('my_requests_total', 'HTTP Failures', ['method', 'endpoint'])\nc.labels('get', '/').inc()\ngb.push()\n```", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "docs/content/bridges/graphite.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 27}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\ntitle: Collector\nweight: 3\n---\n\n# Process Collector\n\nThe Python client automatically exports metrics about process CPU usage, RAM,\nfile descriptors and start time. These all have the prefix `process`, and\nare only currently available on Linux.\n\nThe namespace and pid constructor arguments allows for exporting metrics about\nother processes, for example:\n```\nProcessCollector(namespace='mydaemon', pid=lambda: open('/var/run/daemon.pid').read())\n```\n\n# Platform Collector\n\nThe client also automatically exports some metadata about Python. If using Jython,\nmetadata about the JVM in use is also included. This information is available as \nlabels on the `python_info` metric. The value of the metric is 1, since it is the \nlabels that carry information.\n\n# Disabling Default Collector metrics\n\nBy default the collected `process`, `gc`, and `platform` collector metrics are exported.\nIf this information is not helpful, it can be disabled using the following:\n\n```python\nimport prometheus_client\n\nprometheus_client.REGISTRY.unregister(prometheus_client.GC_COLLECTOR)\nprometheus_client.REGISTRY.unregister(prometheus_client.PLATFORM_COLLECTOR)\nprometheus_client.REGISTRY.unregister(prometheus_client.PROCESS_COLLECTOR)\n```", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "docs/content/collector/_index.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.63, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 36}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\ntitle: Custom Collectors\nweight: 1\n---\n\nSometimes it is not possible to directly instrument code, as it is not\nin your control. This requires you to proxy metrics from other systems.\n\nTo do so you need to create a custom collector, for example:\n\n```python\nfrom prometheus_client.core import GaugeMetricFamily, CounterMetricFamily, REGISTRY\nfrom prometheus_client.registry import Collector\n\nclass CustomCollector(Collector):\n    def collect(self):\n        yield GaugeMetricFamily('my_gauge', 'Help text', value=7)\n        c = CounterMetricFamily('my_counter_total', 'Help text', labels=['foo'])\n        c.add_metric(['bar'], 1.7)\n        c.add_metric(['baz'], 3.8)\n        yield c\n\nREGISTRY.register(CustomCollector())\n```\n\n`SummaryMetricFamily`, `HistogramMetricFamily` and `InfoMetricFamily` work similarly.\n\nA collector may implement a `describe` method which returns metrics in the same\nformat as `collect` (though you don't have to include the samples). This is\nused to predetermine the names of time series a `CollectorRegistry` exposes and\nthus to detect collisions and duplicate registrations.\n\nUsually custom collectors do not have to implement `describe`. If `describe` is\nnot implemented and the CollectorRegistry was created with `auto_describe=True`\n(which is the case for the default registry) then `collect` will be called at\nregistration time instead of `describe`. If this could cause problems, either\nimplement a proper `describe`, or if that's not practical have `describe`\nreturn an empty list.", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "docs/content/collector/custom.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.72, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 38}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create monitoring and alerting configuration for system observability\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\ntitle: Exporting\nweight: 4\n---\n\nThere are several options for exporting metrics.\n", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "docs/content/exporting/_index.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.39999999999999997, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 7}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\ntitle: HTTP/HTTPS\nweight: 1\n---\n\n# HTTP\n\nMetrics are usually exposed over HTTP, to be read by the Prometheus server.\n\nThe easiest way to do this is via `start_http_server`, which will start a HTTP\nserver in a daemon thread on the given port:\n\n```python\nfrom prometheus_client import start_http_server\n\nstart_http_server(8000)\n```\n\nVisit [http://localhost:8000/](http://localhost:8000/) to view the metrics.\n\nThe function will return the HTTP server and thread objects, which can be used\nto shutdown the server gracefully:\n\n```python\nserver, t = start_http_server(8000)\nserver.shutdown()\nt.join()\n```\n\nTo add Prometheus exposition to an existing HTTP server, see the `MetricsHandler` class\nwhich provides a `BaseHTTPRequestHandler`. It also serves as a simple example of how\nto write a custom endpoint.\n\n# HTTPS\n\nBy default, the prometheus client will accept only HTTP requests from Prometheus.\nTo enable HTTPS, `certfile` and `keyfile` need to be provided. The certificate is\npresented to Prometheus as a server certificate during the TLS handshake, and\nthe private key in the key file must belong to the public key in the certificate.\n\nWhen HTTPS is enabled, you can enable mutual TLS (mTLS) by setting `client_auth_required=True`\n(i.e. Prometheus is required to present a client certificate during TLS handshake) and the\nclient certificate including its hostname is validated against the CA certificate chain.\n\n`client_cafile` can be used to specify a certificate file containing a CA certificate\nchain that is used to validate the client certificate. `client_capath` can be used to\nspecify a certificate directory containing a CA certificate chain that is used to\nvalidate the client certificate. If neither of them is provided, a default CA certificate\nchain is used (see Python [ssl.SSLContext.load_default_certs()](https://docs.python.org/3/library/ssl.html#ssl.SSLContext.load_default_certs))\n\n```python\nfrom prometheus_client import start_http_server\n\nstart_http_server(8000, certfile=\"server.crt\", keyfile=\"server.key\")\n```\n\n# Supported HTTP methods\n\nThe prometheus client will handle the following HTTP methods and resources:\n\n* `OPTIONS (any)` - returns HTTP status 200 and an 'Allow' header indicating the\n  allowed methods (OPTIONS, GET)\n* `GET (any)` - returns HTTP status 200 and the metrics data\n* `GET /favicon.ico` - returns HTTP status 200 and an empty response body. Some\n  browsers support this to display the returned icon in the browser tab.\n\nOther HTTP methods than these are rejected with HTTP status 405 \"Method Not Allowed\"\nand an 'Allow' header indicating the allowed methods (OPTIONS, GET).\n\nAny returned HTTP errors are also displayed in the response body after a hash\nsign and with a brief hint. Example:\n```\n# HTTP 405 Method Not Allowed: XXX; use OPTIONS or GET\n```\n", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "docs/content/exporting/http/_index.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 75}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\ntitle: ASGI\nweight: 3\n---\n\nTo use Prometheus with [ASGI](http://asgi.readthedocs.org/en/latest/), there is\n`make_asgi_app` which creates an ASGI application.\n\n```python\nfrom prometheus_client import make_asgi_app\n\napp = make_asgi_app()\n```\nSuch an application can be useful when integrating Prometheus metrics with ASGI\napps.\n\nBy default, the ASGI application will respect `Accept-Encoding:gzip` headers used by Prometheus\nand compress the response if such a header is present. This behaviour can be disabled by passing \n`disable_compression=True` when creating the app, like this:\n\n```python\napp = make_asgi_app(disable_compression=True)\n```\n", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "docs/content/exporting/http/asgi.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 24}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate function-based implementation for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\ntitle: FastAPI + Gunicorn\nweight: 5\n---\n\nTo use Prometheus with [FastAPI](https://fastapi.tiangolo.com/) and [Gunicorn](https://gunicorn.org/) we need to serve metrics through a Prometheus ASGI application.\n\nSave the snippet below in a `myapp.py` file\n\n```python\nfrom fastapi import FastAPI\nfrom prometheus_client import make_asgi_app\n\n# Create app\napp = FastAPI(debug=False)\n\n# Add prometheus asgi middleware to route /metrics requests\nmetrics_app = make_asgi_app()\napp.mount(\"/metrics\", metrics_app)\n```\n\nFor Multiprocessing support, use this modified code snippet. Full multiprocessing instructions are provided [here]({{< ref \"/multiprocess\" >}}).\n\n```python\nfrom fastapi import FastAPI\nfrom prometheus_client import make_asgi_app\n\napp = FastAPI(debug=False)\n\n# Using multiprocess collector for registry\ndef make_metrics_app():\n    registry = CollectorRegistry()\n    multiprocess.MultiProcessCollector(registry)\n    return make_asgi_app(registry=registry)\n\n\nmetrics_app = make_metrics_app()\napp.mount(\"/metrics\", metrics_app)\n```\n\nRun the example web application like this\n\n```bash\n# Install gunicorn if you do not have it\npip install gunicorn\n# If using multiple workers, add `--workers n` parameter to the line below\ngunicorn -b 127.0.0.1:8000 myapp:app -k uvicorn.workers.UvicornWorker\n```\n\nVisit http://localhost:8000/metrics to see the metrics\n", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "docs/content/exporting/http/fastapi-gunicorn.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.69, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 51}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\ntitle: Flask\nweight: 4\n---\n\nTo use Prometheus with [Flask](http://flask.pocoo.org/) we need to serve metrics through a Prometheus WSGI application. This can be achieved using [Flask's application dispatching](http://flask.pocoo.org/docs/latest/patterns/appdispatch/). Below is a working example.\n\nSave the snippet below in a `myapp.py` file\n\n```python\nfrom flask import Flask\nfrom werkzeug.middleware.dispatcher import DispatcherMiddleware\nfrom prometheus_client import make_wsgi_app\n\n# Create my app\napp = Flask(__name__)\n\n# Add prometheus wsgi middleware to route /metrics requests\napp.wsgi_app = DispatcherMiddleware(app.wsgi_app, {\n    '/metrics': make_wsgi_app()\n})\n```\n\nRun the example web application like this\n\n```bash\n# Install uwsgi if you do not have it\npip install uwsgi\nuwsgi --http 127.0.0.1:8000 --wsgi-file myapp.py --callable app\n```\n\nVisit http://localhost:8000/metrics to see the metrics", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "docs/content/exporting/http/flask.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 32}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "monitoring_alerting", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create monitoring and alerting configuration for system observability with Prometheus/Grafana monitoring stack\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\ntitle: Twisted\nweight: 1\n---\n\nTo use prometheus with [twisted](https://twistedmatrix.com/), there is `MetricsResource` which exposes metrics as a twisted resource.\n\n```python\nfrom prometheus_client.twisted import MetricsResource\nfrom twisted.web.server import Site\nfrom twisted.web.resource import Resource\nfrom twisted.internet import reactor\n\nroot = Resource()\nroot.putChild(b'metrics', MetricsResource())\n\nfactory = Site(root)\nreactor.listenTCP(8000, factory)\nreactor.run()\n```", "metadata": {"source": "github.com/prometheus/client_python", "file_path": "docs/content/exporting/http/twisted.md", "language": "markdown", "complexity": "simple", "star_count": 4195, "quality_score": 0.6599999999999999, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": true, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Defines a list of files that are part of main core of Home Assistant.\n# Changes to these files/filters define how our CI test suite is ran.\ncore: &core\n  - homeassistant/*.py\n  - homeassistant/auth/**\n  - homeassistant/helpers/**\n  - homeassistant/package_constraints.txt\n  - homeassistant/util/**\n  - mypy.ini\n  - pyproject.toml\n  - requirements.txt\n  - setup.cfg\n\n# Our base platforms, that are used by other integrations\nbase_platforms: &base_platforms\n  - homeassistant/components/air_quality/**\n  - homeassistant/components/alarm_control_panel/**\n  - homeassistant/components/assist_satellite/**\n  - homeassistant/components/binary_sensor/**\n  - homeassistant/components/button/**\n  - homeassistant/components/calendar/**\n  - homeassistant/components/camera/**\n  - homeassistant/components/climate/**\n  - homeassistant/components/cover/**\n  - homeassistant/components/date/**\n  - homeassistant/components/datetime/**\n  - homeassistant/components/device_tracker/**\n  - homeassistant/components/diagnostics/**\n  - homeassistant/components/event/**\n  - homeassistant/components/fan/**\n  - homeassistant/components/geo_location/**\n  - homeassistant/components/humidifier/**\n  - homeassistant/components/image/**\n  - homeassistant/components/image_processing/**\n  - homeassistant/components/lawn_mower/**\n  - homeassistant/components/light/**\n  - homeassistant/components/lock/**\n  - homeassistant/components/media_player/**\n  - homeassistant/components/notify/**\n  - homeassistant/components/number/**\n  - homeassistant/components/remote/**\n  - homeassistant/components/scene/**\n  - homeassistant/components/select/**\n  - homeassistant/components/sensor/**\n  - homeassistant/components/siren/**\n  - homeassistant/components/stt/**\n  - homeassistant/components/switch/**\n  - homeassistant/components/text/**\n  - homeassistant/components/time/**\n  - homeassistant/components/todo/**\n  - homeassistant/components/tts/**\n  - homeassistant/components/update/**\n  - homeassistant/components/vacuum/**\n  - homeassistant/components/valve/**\n  - homeassistant/components/water_heater/**\n  - homeassistant/components/weather/**\n\n# Extra components that trigger the full suite\ncomponents: &components\n  - homeassistant/components/alexa/**\n  - homeassistant/components/application_credentials/**\n  - homeassistant/components/assist_pipeline/**\n  - homeassistant/components/auth/**\n  - homeassistant/components/automation/**\n  - homeassistant/components/backup/**\n  - homeassistant/components/blueprint/**\n  - homeassistant/components/bluetooth/**\n  - homeassistant/components/cloud/**\n  - homeassistant/components/config/**\n  - homeassistant/components/configurator/**\n  - homeassistant/components/conversation/**\n  - homeassistant/components/demo/**\n  - homeassistant/components/device_automation/**\n  - homeassistant/components/dhcp/**\n  - homeassistant/components/discovery/**\n  - homeassistant/components/energy/**\n  - homeassistant/components/ffmpeg/**\n  - homeassistant/components/frontend/**\n  - homeassistant/components/google_assistant/**\n  - homeassistant/components/group/**\n  - homeassistant/components/hassio/**\n  - homeassistant/components/homeassistant/**\n  - homeassistant/components/homeassistant_hardware/**\n  - homeassistant/components/http/**\n  - homeassistant/components/image/**\n  - homeassistant/components/input_boolean/**\n  - homeassistant/components/input_button/**\n  - homeassistant/components/input_datetime/**\n  - homeassistant/components/input_number/**\n  - homeassistant/components/input_select/**\n  - homeassistant/components/input_text/**\n  - homeassistant/components/logbook/**\n  - homeassistant/components/logger/**\n  - homeassistant/components/lovelace/**\n  - homeassistant/components/media_source/**\n  - homeassistant/components/mjpeg/**\n  - homeassistant/components/modbus/**\n  - homeassistant/components/mqtt/**\n  - homeassistant/components/network/**\n  - homeassistant/components/onboarding/**\n  - homeassistant/components/otp/**\n  - homeassistant/components/persistent_notification/**\n  - homeassistant/components/person/**\n  - homeassistant/components/recorder/**\n  - homeassistant/components/recovery_mode/**\n  - homeassistant/components/repairs/**\n  - homeassistant/components/script/**\n  - homeassistant/components/shopping_list/**\n  - homeassistant/components/ssdp/**\n  - homeassistant/components/stream/**\n  - homeassistant/components/sun/**\n  - homeassistant/components/system_health/**\n  - homeassistant/components/tag/**\n  - homeassistant/components/template/**\n  - homeassistant/components/timer/**\n  - homeassistant/components/trace/**\n  - homeassistant/components/usb/**\n  - homeassistant/components/webhook/**\n  - homeassistant/components/websocket_api/**\n  - homeassistant/components/zeroconf/**\n  - homeassistant/components/zone/**\n\n# Testing related files that affect the whole test/linting suite\ntests: &tests\n  - codecov.yaml\n  - pylint/**\n  - requirements_test_pre_commit.txt\n  - requirements_test.txt\n  - tests/*.py\n  - tests/auth/**\n  - tests/backports/**\n  - tests/components/conftest.py\n  - tests/components/diagnostics/**\n  - tests/components/history/**\n  - tests/components/light/common.py\n  - tests/components/logbook/**\n  - tests/components/recorder/**\n  - tests/components/repairs/**\n  - tests/components/sensor/**\n  - tests/hassfest/**\n  - tests/helpers/**\n  - tests/mock/**\n  - tests/pylint/**\n  - tests/scripts/**\n  - tests/test_util/**\n  - tests/testing_config/**\n  - tests/util/**\n\nother: &other\n  - .github/workflows/**\n  - homeassistant/scripts/**\n\nrequirements: &requirements\n  - .github/workflows/**\n  - homeassistant/package_constraints.txt\n  - requirements*.txt\n  - pyproject.toml\n  - script/licenses.py\n\nany:\n  - *base_platforms\n  - *components\n  - *core\n  - *other\n  - *requirements\n  - *tests\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".core_files.yaml", "language": "yaml", "complexity": "complex", "star_count": 80370, "quality_score": 0.89, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 167}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{\n  \"name\": \"Home Assistant Dev\",\n  \"context\": \"..\",\n  \"dockerFile\": \"../Dockerfile.dev\",\n  \"postCreateCommand\": \"git config --global --add safe.directory ${containerWorkspaceFolder} && script/setup\",\n  \"postStartCommand\": \"script/bootstrap\",\n  \"containerEnv\": {\n    \"PYTHONASYNCIODEBUG\": \"1\"\n  },\n  \"features\": {\n    \"ghcr.io/anthropics/devcontainer-features/claude-code:1.0\": {},\n    \"ghcr.io/devcontainers/features/github-cli:1\": {}\n  },\n  // Port 5683 udp is used by Shelly integration\n  \"appPort\": [\"8123:8123\", \"5683:5683/udp\"],\n  \"runArgs\": [\n    \"-e\",\n    \"GIT_EDITOR=code --wait\",\n    \"--security-opt\",\n    \"label=disable\"\n  ],\n  \"customizations\": {\n    \"vscode\": {\n      \"extensions\": [\n        \"charliermarsh.ruff\",\n        \"ms-python.pylint\",\n        \"ms-python.vscode-pylance\",\n        \"visualstudioexptteam.vscodeintellicode\",\n        \"redhat.vscode-yaml\",\n        \"esbenp.prettier-vscode\",\n        \"GitHub.vscode-pull-request-github\",\n        \"GitHub.copilot\"\n      ],\n      // Please keep this file in sync with settings in home-assistant/.vscode/settings.default.json\n      \"settings\": {\n        \"python.experiments.optOutFrom\": [\"pythonTestAdapter\"],\n        \"python.defaultInterpreterPath\": \"/home/vscode/.local/ha-venv/bin/python\",\n        \"python.pythonPath\": \"/home/vscode/.local/ha-venv/bin/python\",\n        \"python.terminal.activateEnvInCurrentTerminal\": true,\n        \"python.testing.pytestArgs\": [\"--no-cov\"],\n        \"pylint.importStrategy\": \"fromEnvironment\",\n        \"editor.formatOnPaste\": false,\n        \"editor.formatOnSave\": true,\n        \"editor.formatOnType\": true,\n        \"files.trimTrailingWhitespace\": true,\n        \"terminal.integrated.profiles.linux\": {\n          \"zsh\": {\n            \"path\": \"/usr/bin/zsh\"\n          }\n        },\n        \"terminal.integrated.defaultProfile.linux\": \"zsh\",\n        \"yaml.customTags\": [\n          \"!input scalar\",\n          \"!secret scalar\",\n          \"!include_dir_named scalar\",\n          \"!include_dir_list scalar\",\n          \"!include_dir_merge_list scalar\",\n          \"!include_dir_merge_named scalar\"\n        ],\n        \"[python]\": {\n          \"editor.defaultFormatter\": \"charliermarsh.ruff\"\n        },\n        \"json.schemas\": [\n          {\n            \"fileMatch\": [\"homeassistant/components/*/manifest.json\"],\n            \"url\": \"${containerWorkspaceFolder}/script/json_schemas/manifest_schema.json\"\n          }\n        ]\n      }\n    }\n  }\n}\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".devcontainer/devcontainer.json", "language": "json", "complexity": "simple", "star_count": 80370, "quality_score": 0.8600000000000001, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 73}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Report an issue with Home Assistant Core\ndescription: Report an issue with Home Assistant Core.\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        This issue form is for reporting bugs only!\n\n        If you have a feature or enhancement request, please [request them here instead][fr].\n\n        [fr]: https://github.com/orgs/home-assistant/discussions\n  - type: textarea\n    validations:\n      required: true\n    attributes:\n      label: The problem\n      description: >-\n        Describe the issue you are experiencing here, to communicate to the\n        maintainers. Tell us what you were trying to do and what happened.\n\n        Provide a clear and concise description of what the problem is.\n  - type: markdown\n    attributes:\n      value: |\n        ## Environment\n  - type: input\n    id: version\n    validations:\n      required: true\n    attributes:\n      label: What version of Home Assistant Core has the issue?\n      placeholder: core-\n      description: >\n        Can be found in: [Settings ‚áí System ‚áí Repairs ‚áí Three Dots in Upper Right ‚áí System information](https://my.home-assistant.io/redirect/system_health/).\n\n        [![Open your Home Assistant instance and show the system information.](https://my.home-assistant.io/badges/system_health.svg)](https://my.home-assistant.io/redirect/system_health/)\n  - type: input\n    attributes:\n      label: What was the last working version of Home Assistant Core?\n      placeholder: core-\n      description: >\n        If known, otherwise leave blank.\n  - type: dropdown\n    validations:\n      required: true\n    attributes:\n      label: What type of installation are you running?\n      description: >\n        Can be found in: [Settings ‚áí System ‚áí Repairs ‚áí Three Dots in Upper Right ‚áí System information](https://my.home-assistant.io/redirect/system_health/).\n\n        [![Open your Home Assistant instance and show the system information.](https://my.home-assistant.io/badges/system_health.svg)](https://my.home-assistant.io/redirect/system_health/)\n      options:\n        - Home Assistant OS\n        - Home Assistant Container\n        - Home Assistant Supervised\n        - Home Assistant Core\n  - type: input\n    id: integration_name\n    attributes:\n      label: Integration causing the issue\n      description: >\n        The name of the integration, for example Automation or Philips Hue.\n  - type: input\n    id: integration_link\n    attributes:\n      label: Link to integration documentation on our website\n      placeholder: \"https://www.home-assistant.io/integrations/...\"\n      description: |\n        Providing a link [to the documentation][docs] helps us categorize the issue and might speed up the\n        investigation by automatically informing a contributor, while also providing a useful reference for others.\n\n        [docs]: https://www.home-assistant.io/integrations\n\n  - type: markdown\n    attributes:\n      value: |\n        # Details\n  - type: textarea\n    attributes:\n      label: Diagnostics information\n      placeholder: \"drag-and-drop the diagnostics data file here (do not copy-and-paste the content)\"\n      description: >-\n        Many integrations provide the ability to [download diagnostic data](https://www.home-assistant.io/docs/configuration/troubleshooting/#debug-logs-and-diagnostics).\n\n        **It would really help if you could download the diagnostics data for the device you are having issues with,\n        and <ins>drag-and-drop that file into the textbox below.</ins>**\n\n        It generally allows pinpointing defects and thus resolving issues faster.\n  - type: textarea\n    attributes:\n      label: Example YAML snippet\n      description: |\n        If applicable, please provide an example piece of YAML that can help reproduce this problem.\n        This can be from an automation, script, scene or configuration.\n      render: yaml\n  - type: textarea\n    attributes:\n      label: Anything in the logs that might be useful for us?\n      description: For example, error message, or stack traces.\n      render: txt\n  - type: textarea\n    attributes:\n      label: Additional information\n      description: >\n        If you have any additional information for us, use the field below.\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".github/ISSUE_TEMPLATE/bug_report.yml", "language": "yaml", "complexity": "medium", "star_count": 80370, "quality_score": 0.8600000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 106}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Task\ndescription: For staff only - Create a task\ntype: Task\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        ## ‚ö†Ô∏è RESTRICTED ACCESS\n\n        **This form is restricted to Open Home Foundation staff, authorized contributors, and integration code owners only.**\n\n        If you are a community member wanting to contribute, please:\n        - For bug reports: Use the [bug report form](https://github.com/home-assistant/core/issues/new?template=bug_report.yml)\n        - For feature requests: Submit to [Feature Requests](https://github.com/orgs/home-assistant/discussions)\n\n        ---\n\n        ### For authorized contributors\n\n        Use this form to create tasks for development work, improvements, or other actionable items that need to be tracked.\n  - type: textarea\n    id: description\n    attributes:\n      label: Description\n      description: |\n        Provide a clear and detailed description of the task that needs to be accomplished.\n\n        Be specific about what needs to be done, why it's important, and any constraints or requirements.\n      placeholder: |\n        Describe the task, including:\n        - What needs to be done\n        - Why this task is needed\n        - Expected outcome\n        - Any constraints or requirements\n    validations:\n      required: true\n  - type: textarea\n    id: additional_context\n    attributes:\n      label: Additional context\n      description: |\n        Any additional information, links, research, or context that would be helpful.\n\n        Include links to related issues, research, prototypes, roadmap opportunities etc.\n      placeholder: |\n        - Roadmap opportunity: [link]\n        - Epic: [link]\n        - Feature request: [link]\n        - Technical design documents: [link]\n        - Prototype/mockup: [link]\n        - Dependencies: [links]\n    validations:\n      required: false\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".github/ISSUE_TEMPLATE/task.yml", "language": "yaml", "complexity": "simple", "star_count": 80370, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 54}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!--\n  You are amazing! Thanks for contributing to our project!\n  Please, DO NOT DELETE ANY TEXT from this template! (unless instructed).\n-->\n## Breaking change\n<!--\n  If your PR contains a breaking change for existing users, it is important\n  to tell them what breaks, how to make it work again and why we did this.\n  This piece of text is published with the release notes, so it helps if you\n  write it towards our users, not us.\n  Note: Remove this section if this PR is NOT a breaking change.\n-->\n\n\n## Proposed change\n<!--\n  Describe the big picture of your changes here to communicate to the\n  maintainers why we should accept this pull request. If it fixes a bug\n  or resolves a feature request, be sure to link to that issue in the\n  additional information section.\n-->\n\n\n## Type of change\n<!--\n  What type of change does your PR introduce to Home Assistant?\n  NOTE: Please, check only 1! box!\n  If your PR requires multiple boxes to be checked, you'll most likely need to\n  split it into multiple PRs. This makes things easier and faster to code review.\n-->\n\n- [ ] Dependency upgrade\n- [ ] Bugfix (non-breaking change which fixes an issue)\n- [ ] New integration (thank you!)\n- [ ] New feature (which adds functionality to an existing integration)\n- [ ] Deprecation (breaking change to happen in the future)\n- [ ] Breaking change (fix/feature causing existing functionality to break)\n- [ ] Code quality improvements to existing code or addition of tests\n\n## Additional information\n<!--\n  Details are important, and help maintainers processing your PR.\n  Please be sure to fill out additional details, if applicable.\n-->\n\n- This PR fixes or closes issue: fixes #\n- This PR is related to issue: \n- Link to documentation pull request: \n- Link to developer documentation pull request: \n- Link to frontend pull request: \n\n## Checklist\n<!--\n  Put an `x` in the boxes that apply. You can also fill these out after\n  creating the PR. If you're unsure about any of them, don't hesitate to ask.\n  We're here to help! This is simply a reminder of what we are going to look\n  for before merging your code.\n-->\n\n- [ ] The code change is tested and works locally.\n- [ ] Local tests pass. **Your PR cannot be merged unless tests pass**\n- [ ] There is no commented out code in this PR.\n- [ ] I have followed the [development checklist][dev-checklist]\n- [ ] I have followed the [perfect PR recommendations][perfect-pr]\n- [ ] The code has been formatted using Ruff (`ruff format homeassistant tests`)\n- [ ] Tests have been added to verify that the new code works.\n\nIf user exposed functionality or configuration variables are added/changed:\n\n- [ ] Documentation added/updated for [www.home-assistant.io][docs-repository]\n\nIf the code communicates with devices, web services, or third-party tools:\n\n- [ ] The [manifest file][manifest-docs] has all fields filled out correctly.  \n      Updated and included derived files by running: `python3 -m script.hassfest`.\n- [ ] New or updated dependencies have been added to `requirements_all.txt`.  \n      Updated by running `python3 -m script.gen_requirements_all`.\n- [ ] For the updated dependencies - a link to the changelog, or at minimum a diff between library versions is added to the PR description.\n\n<!--\n  This project is very active and we have a high turnover of pull requests.\n\n  Unfortunately, the number of incoming pull requests is higher than what our\n  reviewers can review and merge so there is a long backlog of pull requests\n  waiting for review. You can help here!\n  \n  By reviewing another pull request, you will help raise the code quality of\n  that pull request and the final review will be faster. This way the general\n  pace of pull request reviews will go up and your wait time will go down.\n  \n  When picking a pull request to review, try to choose one that hasn't yet\n  been reviewed.\n\n  Thanks for helping out!\n-->\n\nTo help with the load of incoming pull requests:\n\n- [ ] I have reviewed two other [open pull requests][prs] in this repository.\n\n[prs]: https://github.com/home-assistant/core/pulls?q=is%3Aopen+is%3Apr+-author%3A%40me+-draft%3Atrue+-label%3Awaiting-for-upstream+sort%3Acreated-desc+review%3Anone+-status%3Afailure\n\n<!--\n  Thank you for contributing <3\n\n  Below, some useful links you could explore:\n-->\n[dev-checklist]: https://developers.home-assistant.io/docs/development_checklist/\n[manifest-docs]: https://developers.home-assistant.io/docs/creating_integration_manifest/\n[quality-scale]: https://developers.home-assistant.io/docs/integration_quality_scale_index/\n[docs-repository]: https://github.com/home-assistant/home-assistant.io\n[perfect-pr]: https://developers.home-assistant.io/docs/review-process/#creating-the-perfect-pr\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".github/PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "medium", "star_count": 80370, "quality_score": 0.8600000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 113}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# GitHub Copilot & Claude Code Instructions\n\nThis repository contains the core of Home Assistant, a Python 3 based home automation application.\n\n## Integration Quality Scale\n\nHome Assistant uses an Integration Quality Scale to ensure code quality and consistency. The quality level determines which rules apply:\n\n### Quality Scale Levels\n- **Bronze**: Basic requirements (ALL Bronze rules are mandatory)\n- **Silver**: Enhanced functionality \n- **Gold**: Advanced features\n- **Platinum**: Highest quality standards\n\n### How Rules Apply\n1. **Check `manifest.json`**: Look for `\"quality_scale\"` key to determine integration level\n2. **Bronze Rules**: Always required for any integration with quality scale\n3. **Higher Tier Rules**: Only apply if integration targets that tier or higher\n4. **Rule Status**: Check `quality_scale.yaml` in integration folder for:\n   - `done`: Rule implemented\n   - `exempt`: Rule doesn't apply (with reason in comment)\n   - `todo`: Rule needs implementation\n\n### Example `quality_scale.yaml` Structure\n```yaml\nrules:\n  # Bronze (mandatory)\n  config-flow: done\n  entity-unique-id: done\n  action-setup:\n    status: exempt\n    comment: Integration does not register custom actions.\n  \n  # Silver (if targeting Silver+)\n  entity-unavailable: done\n  parallel-updates: done\n  \n  # Gold (if targeting Gold+)\n  devices: done\n  diagnostics: done\n  \n  # Platinum (if targeting Platinum)\n  strict-typing: done\n```\n\n**When Reviewing/Creating Code**: Always check the integration's quality scale level and exemption status before applying rules.\n\n## Code Review Guidelines\n\n**When reviewing code, do NOT comment on:**\n- **Missing imports** - We use static analysis tooling to catch that\n- **Code formatting** - We have ruff as a formatting tool that will catch those if needed (unless specifically instructed otherwise in these instructions)\n\n## Python Requirements\n\n- **Compatibility**: Python 3.13+\n- **Language Features**: Use the newest features when possible:\n  - Pattern matching\n  - Type hints\n  - f-strings (preferred over `%` or `.format()`)\n  - Dataclasses\n  - Walrus operator\n\n### Strict Typing (Platinum)\n- **Comprehensive Type Hints**: Add type hints to all functions, methods, and variables\n- **Custom Config Entry Types**: When using runtime_data:\n  ```python\n  type MyIntegrationConfigEntry = ConfigEntry[MyClient]\n  ```\n- **Library Requirements**: Include `py.typed` file for PEP-561 compliance\n\n## Code Quality Standards\n\n- **Formatting**: Ruff\n- **Linting**: PyLint and Ruff\n- **Type Checking**: MyPy\n- **Testing**: pytest with plain functions and fixtures\n- **Language**: American English for all code, comments, and documentation (use sentence case, including titles)\n\n### Writing Style Guidelines\n- **Tone**: Friendly and informative\n- **Perspective**: Use second-person (\"you\" and \"your\") for user-facing messages\n- **Inclusivity**: Use objective, non-discriminatory language\n- **Clarity**: Write for non-native English speakers\n- **Formatting in Messages**:\n  - Use backticks for: file paths, filenames, variable names, field entries\n  - Use sentence case for titles and messages (capitalize only the first word and proper nouns)\n  - Avoid abbreviations when possible\n\n## Code Organization\n\n### Core Locations\n- Shared constants: `homeassistant/const.py` (use these instead of hardcoding)\n- Integration structure:\n  - `homeassistant/components/{domain}/const.py` - Constants\n  - `homeassistant/components/{domain}/models.py` - Data models\n  - `homeassistant/components/{domain}/coordinator.py` - Update coordinator\n  - `homeassistant/components/{domain}/config_flow.py` - Configuration flow\n  - `homeassistant/components/{domain}/{platform}.py` - Platform implementations\n\n### Common Modules\n- **coordinator.py**: Centralize data fetching logic\n  ```python\n  class MyCoordinator(DataUpdateCoordinator[MyData]):\n      def __init__(self, hass: HomeAssistant, client: MyClient, config_entry: ConfigEntry) -> None:\n          super().__init__(\n              hass, \n              logger=LOGGER, \n              name=DOMAIN, \n              update_interval=timedelta(minutes=1),\n              config_entry=config_entry,  # ‚úÖ Pass config_entry - it's accepted and recommended\n          )\n  ```\n- **entity.py**: Base entity definitions to reduce duplication\n  ```python\n  class MyEntity(CoordinatorEntity[MyCoordinator]):\n      _attr_has_entity_name = True\n  ```\n\n### Runtime Data Storage\n- **Use ConfigEntry.runtime_data**: Store non-persistent runtime data\n  ```python\n  type MyIntegrationConfigEntry = ConfigEntry[MyClient]\n  \n  async def async_setup_entry(hass: HomeAssistant, entry: MyIntegrationConfigEntry) -> bool:\n      client = MyClient(entry.data[CONF_HOST])\n      entry.runtime_data = client\n  ```\n\n### Manifest Requirements\n- **Required Fields**: `domain`, `name`, `codeowners`, `integration_type`, `documentation`, `requirements`\n- **Integration Types**: `device`, `hub`, `service`, `system`, `helper`\n- **IoT Class**: Always specify connectivity method (e.g., `cloud_polling`, `local_polling`, `local_push`)\n- **Discovery Methods**: Add when applicable: `zeroconf`, `dhcp`, `bluetooth`, `ssdp`, `usb`\n- **Dependencies**: Include platform dependencies (e.g., `application_credentials`, `bluetooth_adapters`)\n\n### Config Flow Patterns\n- **Version Control**: Always set `VERSION = 1` and `MINOR_VERSION = 1`\n- **Unique ID Management**:\n  ```python\n  await self.async_set_unique_id(device_unique_id)\n  self._abort_if_unique_id_configured()\n  ```\n- **Error Handling**: Define errors in `strings.json` under `config.error`\n- **Step Methods**: Use standard naming (`async_step_user`, `async_step_discovery`, etc.)\n\n### Integration Ownership\n- **manifest.json**: Add GitHub usernames to `codeowners`:\n  ```json\n  {\n    \"domain\": \"my_integration\",\n    \"name\": \"My Integration\",\n    \"codeowners\": [\"@me\"]\n  }\n  ```\n\n### Documentation Standards\n- **File Headers**: Short and concise\n  ```python\n  \"\"\"Integration for Peblar EV chargers.\"\"\"\n  ```\n- **Method/Function Docstrings**: Required for all\n  ```python\n  async def async_setup_entry(hass: HomeAssistant, entry: PeblarConfigEntry) -> bool:\n      \"\"\"Set up Peblar from a config entry.\"\"\"\n  ```\n- **Comment Style**:\n  - Use clear, descriptive comments\n  - Explain the \"why\" not just the \"what\"\n  - Keep code block lines under 80 characters when possible\n  - Use progressive disclosure (simple explanation first, complex details later)\n\n## Async Programming\n\n- All external I/O operations must be async\n- **Best Practices**:\n  - Avoid sleeping in loops\n  - Avoid awaiting in loops - use `gather` instead\n  - No blocking calls\n  - Group executor jobs when possible - switching between event loop and executor is expensive\n\n### Async Dependencies (Platinum)\n- **Requirement**: All dependencies must use asyncio\n- Ensures efficient task handling without thread context switching\n\n### WebSession Injection (Platinum)\n- **Pass WebSession**: Support passing web sessions to dependencies\n  ```python\n  async def async_setup_entry(hass: HomeAssistant, entry: MyConfigEntry) -> bool:\n      \"\"\"Set up integration from config entry.\"\"\"\n      client = MyClient(entry.data[CONF_HOST], async_get_clientsession(hass))\n  ```\n- For cookies: Use `async_create_clientsession` (aiohttp) or `create_async_httpx_client` (httpx)\n\n### Blocking Operations\n- **Use Executor**: For blocking I/O operations\n  ```python\n  result = await hass.async_add_executor_job(blocking_function, args)\n  ```\n- **Never Block Event Loop**: Avoid file operations, `time.sleep()`, blocking HTTP calls\n- **Replace with Async**: Use `asyncio.sleep()` instead of `time.sleep()`\n\n### Thread Safety\n- **@callback Decorator**: For event loop safe functions\n  ```python\n  @callback\n  def async_update_callback(self, event):\n      \"\"\"Safe to run in event loop.\"\"\"\n      self.async_write_ha_state()\n  ```\n- **Sync APIs from Threads**: Use sync versions when calling from non-event loop threads\n- **Registry Changes**: Must be done in event loop thread\n\n### Data Update Coordinator\n- **Standard Pattern**: Use for efficient data management\n  ```python\n  class MyCoordinator(DataUpdateCoordinator):\n      def __init__(self, hass: HomeAssistant, client: MyClient, config_entry: ConfigEntry) -> None:\n          super().__init__(\n              hass,\n              logger=LOGGER,\n              name=DOMAIN,\n              update_interval=timedelta(minutes=5),\n              config_entry=config_entry,  # ‚úÖ Pass config_entry - it's accepted and recommended\n          )\n          self.client = client\n      \n      async def _async_update_data(self):\n          try:\n              return await self.client.fetch_data()\n          except ApiError as err:\n              raise UpdateFailed(f\"API communication error: {err}\")\n  ```\n- **Error Types**: Use `UpdateFailed` for API errors, `ConfigEntryAuthFailed` for auth issues\n- **Config Entry**: Always pass `config_entry` parameter to coordinator - it's accepted and recommended\n\n## Integration Guidelines\n\n### Configuration Flow\n- **UI Setup Required**: All integrations must support configuration via UI\n- **Manifest**: Set `\"config_flow\": true` in `manifest.json`\n- **Data Storage**:\n  - Connection-critical config: Store in `ConfigEntry.data`\n  - Non-critical settings: Store in `ConfigEntry.options`\n- **Validation**: Always validate user input before creating entries\n- **Config Entry Naming**: \n  - ‚ùå Do NOT allow users to set config entry names in config flows\n  - Names are automatically generated or can be customized later in UI\n  - ‚úÖ Exception: Helper integrations MAY allow custom names in config flow\n- **Connection Testing**: Test device/service connection during config flow:\n  ```python\n  try:\n      await client.get_data()\n  except MyException:\n      errors[\"base\"] = \"cannot_connect\"\n  ```\n- **Duplicate Prevention**: Prevent duplicate configurations:\n  ```python\n  # Using unique ID\n  await self.async_set_unique_id(identifier)\n  self._abort_if_unique_id_configured()\n  \n  # Using unique data\n  self._async_abort_entries_match({CONF_HOST: user_input[CONF_HOST]})\n  ```\n\n### Reauthentication Support\n- **Required Method**: Implement `async_step_reauth` in config flow\n- **Credential Updates**: Allow users to update credentials without re-adding\n- **Validation**: Verify account matches existing unique ID:\n  ```python\n  await self.async_set_unique_id(user_id)\n  self._abort_if_unique_id_mismatch(reason=\"wrong_account\")\n  return self.async_update_reload_and_abort(\n      self._get_reauth_entry(),\n      data_updates={CONF_API_TOKEN: user_input[CONF_API_TOKEN]}\n  )\n  ```\n\n### Reconfiguration Flow\n- **Purpose**: Allow configuration updates without removing device\n- **Implementation**: Add `async_step_reconfigure` method\n- **Validation**: Prevent changing underlying account with `_abort_if_unique_id_mismatch`\n\n### Device Discovery\n- **Manifest Configuration**: Add discovery method (zeroconf, dhcp, etc.)\n  ```json\n  {\n    \"zeroconf\": [\"_mydevice._tcp.local.\"]\n  }\n  ```\n- **Discovery Handler**: Implement appropriate `async_step_*` method:\n  ```python\n  async def async_step_zeroconf(self, discovery_info):\n      \"\"\"Handle zeroconf discovery.\"\"\"\n      await self.async_set_unique_id(discovery_info.properties[\"serialno\"])\n      self._abort_if_unique_id_configured(updates={CONF_HOST: discovery_info.host})\n  ```\n- **Network Updates**: Use discovery to update dynamic IP addresses\n\n### Network Discovery Implementation\n- **Zeroconf/mDNS**: Use async instances\n  ```python\n  aiozc = await zeroconf.async_get_async_instance(hass)\n  ```\n- **SSDP Discovery**: Register callbacks with cleanup\n  ```python\n  entry.async_on_unload(\n      ssdp.async_register_callback(\n          hass, _async_discovered_device, \n          {\"st\": \"urn:schemas-upnp-org:device:ZonePlayer:1\"}\n      )\n  )\n  ```\n\n### Bluetooth Integration\n- **Manifest Dependencies**: Add `bluetooth_adapters` to dependencies\n- **Connectable**: Set `\"connectable\": true` for connection-required devices\n- **Scanner Usage**: Always use shared scanner instance\n  ```python\n  scanner = bluetooth.async_get_scanner()\n  entry.async_on_unload(\n      bluetooth.async_register_callback(\n          hass, _async_discovered_device,\n          {\"service_uuid\": \"example_uuid\"},\n          bluetooth.BluetoothScanningMode.ACTIVE\n      )\n  )\n  ```\n- **Connection Handling**: Never reuse `BleakClient` instances, use 10+ second timeouts\n\n### Setup Validation\n- **Test Before Setup**: Verify integration can be set up in `async_setup_entry`\n- **Exception Handling**:\n  - `ConfigEntryNotReady`: Device offline or temporary failure\n  - `ConfigEntryAuthFailed`: Authentication issues\n  - `ConfigEntryError`: Unresolvable setup problems\n\n### Config Entry Unloading\n- **Required**: Implement `async_unload_entry` for runtime removal/reload\n- **Platform Unloading**: Use `hass.config_entries.async_unload_platforms`\n- **Cleanup**: Register callbacks with `entry.async_on_unload`:\n  ```python\n  async def async_unload_entry(hass: HomeAssistant, entry: MyConfigEntry) -> bool:\n      \"\"\"Unload a config entry.\"\"\"\n      if unload_ok := await hass.config_entries.async_unload_platforms(entry, PLATFORMS):\n          entry.runtime_data.listener()  # Clean up resources\n      return unload_ok\n  ```\n\n### Service Actions\n- **Registration**: Register all service actions in `async_setup`, NOT in `async_setup_entry`\n- **Validation**: Check config entry existence and loaded state:\n  ```python\n  async def async_setup(hass: HomeAssistant, config: ConfigType) -> bool:\n      async def service_action(call: ServiceCall) -> ServiceResponse:\n          if not (entry := hass.config_entries.async_get_entry(call.data[ATTR_CONFIG_ENTRY_ID])):\n              raise ServiceValidationError(\"Entry not found\")\n          if entry.state is not ConfigEntryState.LOADED:\n              raise ServiceValidationError(\"Entry not loaded\")\n  ```\n- **Exception Handling**: Raise appropriate exceptions:\n  ```python\n  # For invalid input\n  if end_date < start_date:\n      raise ServiceValidationError(\"End date must be after start date\")\n  \n  # For service errors\n  try:\n      await client.set_schedule(start_date, end_date)\n  except MyConnectionError as err:\n      raise HomeAssistantError(\"Could not connect to the schedule\") from err\n  ```\n\n### Service Registration Patterns\n- **Entity Services**: Register on platform setup\n  ```python\n  platform.async_register_entity_service(\n      \"my_entity_service\",\n      {vol.Required(\"parameter\"): cv.string},\n      \"handle_service_method\"\n  )\n  ```\n- **Service Schema**: Always validate input\n  ```python\n  SERVICE_SCHEMA = vol.Schema({\n      vol.Required(\"entity_id\"): cv.entity_ids,\n      vol.Required(\"parameter\"): cv.string,\n      vol.Optional(\"timeout\", default=30): cv.positive_int,\n  })\n  ```\n- **Services File**: Create `services.yaml` with descriptions and field definitions\n\n### Polling\n- Use update coordinator pattern when possible\n- **Polling intervals are NOT user-configurable**: Never add scan_interval, update_interval, or polling frequency options to config flows or config entries\n- **Integration determines intervals**: Set `update_interval` programmatically based on integration logic, not user input\n- **Minimum Intervals**:\n  - Local network: 5 seconds\n  - Cloud services: 60 seconds\n- **Parallel Updates**: Specify number of concurrent updates:\n  ```python\n  PARALLEL_UPDATES = 1  # Serialize updates to prevent overwhelming device\n  # OR\n  PARALLEL_UPDATES = 0  # Unlimited (for coordinator-based or read-only)\n  ```\n\n### Error Handling\n- **Exception Types**: Choose most specific exception available\n  - `ServiceValidationError`: User input errors (preferred over `ValueError`)\n  - `HomeAssistantError`: Device communication failures\n  - `ConfigEntryNotReady`: Temporary setup issues (device offline)\n  - `ConfigEntryAuthFailed`: Authentication problems\n  - `ConfigEntryError`: Permanent setup issues\n- **Try/Catch Best Practices**:\n  - Only wrap code that can throw exceptions\n  - Keep try blocks minimal - process data after the try/catch\n  - **Avoid bare exceptions** except in specific cases:\n    - ‚ùå Generally not allowed: `except:` or `except Exception:`\n    - ‚úÖ Allowed in config flows to ensure robustness\n    - ‚úÖ Allowed in functions/methods that run in background tasks\n  - Bad pattern:\n    ```python\n    try:\n        data = await device.get_data()  # Can throw\n        # ‚ùå Don't process data inside try block\n        processed = data.get(\"value\", 0) * 100\n        self._attr_native_value = processed\n    except DeviceError:\n        _LOGGER.error(\"Failed to get data\")\n    ```\n  - Good pattern:\n    ```python\n    try:\n        data = await device.get_data()  # Can throw\n    except DeviceError:\n        _LOGGER.error(\"Failed to get data\")\n        return\n    \n    # ‚úÖ Process data outside try block\n    processed = data.get(\"value\", 0) * 100\n    self._attr_native_value = processed\n    ```\n- **Bare Exception Usage**:\n  ```python\n  # ‚ùå Not allowed in regular code\n  try:\n      data = await device.get_data()\n  except Exception:  # Too broad\n      _LOGGER.error(\"Failed\")\n  \n  # ‚úÖ Allowed in config flow for robustness\n  async def async_step_user(self, user_input=None):\n      try:\n          await self._test_connection(user_input)\n      except Exception:  # Allowed here\n          errors[\"base\"] = \"unknown\"\n  \n  # ‚úÖ Allowed in background tasks\n  async def _background_refresh():\n      try:\n          await coordinator.async_refresh()\n      except Exception:  # Allowed in task\n          _LOGGER.exception(\"Unexpected error in background task\")\n  ```\n- **Setup Failure Patterns**:\n  ```python\n  try:\n      await device.async_setup()\n  except (asyncio.TimeoutError, TimeoutException) as ex:\n      raise ConfigEntryNotReady(f\"Timeout connecting to {device.host}\") from ex\n  except AuthFailed as ex:\n      raise ConfigEntryAuthFailed(f\"Credentials expired for {device.name}\") from ex\n  ```\n\n### Logging\n- **Format Guidelines**:\n  - No periods at end of messages\n  - No integration names/domains (added automatically)\n  - No sensitive data (keys, tokens, passwords)\n- Use debug level for non-user-facing messages\n- **Use Lazy Logging**:\n  ```python\n  _LOGGER.debug(\"This is a log message with %s\", variable)\n  ```\n\n### Unavailability Logging\n- **Log Once**: When device/service becomes unavailable (info level)\n- **Log Recovery**: When device/service comes back online\n- **Implementation Pattern**:\n  ```python\n  _unavailable_logged: bool = False\n  \n  if not self._unavailable_logged:\n      _LOGGER.info(\"The sensor is unavailable: %s\", ex)\n      self._unavailable_logged = True\n  # On recovery:\n  if self._unavailable_logged:\n      _LOGGER.info(\"The sensor is back online\")\n      self._unavailable_logged = False\n  ```\n\n## Entity Development\n\n### Unique IDs\n- **Required**: Every entity must have a unique ID for registry tracking\n- Must be unique per platform (not per integration)\n- Don't include integration domain or platform in ID\n- **Implementation**:\n  ```python\n  class MySensor(SensorEntity):\n      def __init__(self, device_id: str) -> None:\n          self._attr_unique_id = f\"{device_id}_temperature\"\n  ```\n\n**Acceptable ID Sources**:\n- Device serial numbers\n- MAC addresses (formatted using `format_mac` from device registry)\n- Physical identifiers (printed/EEPROM)\n- Config entry ID as last resort: `f\"{entry.entry_id}-battery\"`\n\n**Never Use**:\n- IP addresses, hostnames, URLs\n- Device names\n- Email addresses, usernames\n\n### Entity Descriptions\n- **Lambda/Anonymous Functions**: Often used in EntityDescription for value transformation\n- **Multiline Lambdas**: When lambdas exceed line length, wrap in parentheses for readability\n- **Bad pattern**:\n  ```python\n  SensorEntityDescription(\n      key=\"temperature\",\n      name=\"Temperature\",\n      value_fn=lambda data: round(data[\"temp_value\"] * 1.8 + 32, 1) if data.get(\"temp_value\") is not None else None,  # ‚ùå Too long\n  )\n  ```\n- **Good pattern**:\n  ```python\n  SensorEntityDescription(\n      key=\"temperature\", \n      name=\"Temperature\",\n      value_fn=lambda data: (  # ‚úÖ Parenthesis on same line as lambda\n          round(data[\"temp_value\"] * 1.8 + 32, 1)\n          if data.get(\"temp_value\") is not None\n          else None\n      ),\n  )\n  ```\n\n### Entity Naming\n- **Use has_entity_name**: Set `_attr_has_entity_name = True`\n- **For specific fields**:\n  ```python\n  class MySensor(SensorEntity):\n      _attr_has_entity_name = True\n      def __init__(self, device: Device, field: str) -> None:\n          self._attr_device_info = DeviceInfo(\n              identifiers={(DOMAIN, device.id)},\n              name=device.name,\n          )\n          self._attr_name = field  # e.g., \"temperature\", \"humidity\"\n  ```\n- **For device itself**: Set `_attr_name = None`\n\n### Event Lifecycle Management\n- **Subscribe in `async_added_to_hass`**:\n  ```python\n  async def async_added_to_hass(self) -> None:\n      \"\"\"Subscribe to events.\"\"\"\n      self.async_on_remove(\n          self.client.events.subscribe(\"my_event\", self._handle_event)\n      )\n  ```\n- **Unsubscribe in `async_will_remove_from_hass`** if not using `async_on_remove`\n- Never subscribe in `__init__` or other methods\n\n### State Handling\n- Unknown values: Use `None` (not \"unknown\" or \"unavailable\")\n- Availability: Implement `available()` property instead of using \"unavailable\" state\n\n### Entity Availability\n- **Mark Unavailable**: When data cannot be fetched from device/service\n- **Coordinator Pattern**:\n  ```python\n  @property\n  def available(self) -> bool:\n      \"\"\"Return if entity is available.\"\"\"\n      return super().available and self.identifier in self.coordinator.data\n  ```\n- **Direct Update Pattern**:\n  ```python\n  async def async_update(self) -> None:\n      \"\"\"Update entity.\"\"\"\n      try:\n          data = await self.client.get_data()\n      except MyException:\n          self._attr_available = False\n      else:\n          self._attr_available = True\n          self._attr_native_value = data.value\n  ```\n\n### Extra State Attributes\n- All attribute keys must always be present\n- Unknown values: Use `None`\n- Provide descriptive attributes\n\n## Device Management\n\n### Device Registry\n- **Create Devices**: Group related entities under devices\n- **Device Info**: Provide comprehensive metadata:\n  ```python\n  _attr_device_info = DeviceInfo(\n      connections={(CONNECTION_NETWORK_MAC, device.mac)},\n      identifiers={(DOMAIN, device.id)},\n      name=device.name,\n      manufacturer=\"My Company\",\n      model=\"My Sensor\",\n      sw_version=device.version,\n  )\n  ```\n- For services: Add `entry_type=DeviceEntryType.SERVICE`\n\n### Dynamic Device Addition\n- **Auto-detect New Devices**: After initial setup\n- **Implementation Pattern**:\n  ```python\n  def _check_device() -> None:\n      current_devices = set(coordinator.data)\n      new_devices = current_devices - known_devices\n      if new_devices:\n          known_devices.update(new_devices)\n          async_add_entities([MySensor(coordinator, device_id) for device_id in new_devices])\n  \n  entry.async_on_unload(coordinator.async_add_listener(_check_device))\n  ```\n\n### Stale Device Removal\n- **Auto-remove**: When devices disappear from hub/account\n- **Device Registry Update**:\n  ```python\n  device_registry.async_update_device(\n      device_id=device.id,\n      remove_config_entry_id=self.config_entry.entry_id,\n  )\n  ```\n- **Manual Deletion**: Implement `async_remove_config_entry_device` when needed\n\n## Diagnostics and Repairs\n\n### Integration Diagnostics\n- **Required**: Implement diagnostic data collection\n- **Implementation**:\n  ```python\n  TO_REDACT = [CONF_API_KEY, CONF_LATITUDE, CONF_LONGITUDE]\n  \n  async def async_get_config_entry_diagnostics(\n      hass: HomeAssistant, entry: MyConfigEntry\n  ) -> dict[str, Any]:\n      \"\"\"Return diagnostics for a config entry.\"\"\"\n      return {\n          \"entry_data\": async_redact_data(entry.data, TO_REDACT),\n          \"data\": entry.runtime_data.data,\n      }\n  ```\n- **Security**: Never expose passwords, tokens, or sensitive coordinates\n\n### Repair Issues\n- **Actionable Issues Required**: All repair issues must be actionable for end users\n- **Issue Content Requirements**:\n  - Clearly explain what is happening\n  - Provide specific steps users need to take to resolve the issue\n  - Use friendly, helpful language\n  - Include relevant context (device names, error details, etc.)\n- **Implementation**:\n  ```python\n  ir.async_create_issue(\n      hass,\n      DOMAIN,\n      \"outdated_version\",\n      is_fixable=False,\n      issue_domain=DOMAIN,\n      severity=ir.IssueSeverity.ERROR,\n      translation_key=\"outdated_version\",\n  )\n  ```\n- **Translation Strings Requirements**: Must contain user-actionable text in `strings.json`:\n  ```json\n  {\n    \"issues\": {\n      \"outdated_version\": {\n        \"title\": \"Device firmware is outdated\",\n        \"description\": \"Your device firmware version {current_version} is below the minimum required version {min_version}. To fix this issue: 1) Open the manufacturer's mobile app, 2) Navigate to device settings, 3) Select 'Update Firmware', 4) Wait for the update to complete, then 5) Restart Home Assistant.\"\n      }\n    }\n  }\n  ```\n- **String Content Must Include**:\n  - What the problem is\n  - Why it matters\n  - Exact steps to resolve (numbered list when multiple steps)\n  - What to expect after following the steps\n- **Avoid Vague Instructions**: Don't just say \"update firmware\" - provide specific steps\n- **Severity Guidelines**:\n  - `CRITICAL`: Reserved for extreme scenarios only\n  - `ERROR`: Requires immediate user attention  \n  - `WARNING`: Indicates future potential breakage\n- **Additional Attributes**:\n  ```python\n  ir.async_create_issue(\n      hass, DOMAIN, \"issue_id\",\n      breaks_in_ha_version=\"2024.1.0\",\n      is_fixable=True,\n      is_persistent=True,\n      severity=ir.IssueSeverity.ERROR,\n      translation_key=\"issue_description\",\n  )\n  ```\n- Only create issues for problems users can potentially resolve\n\n### Entity Categories\n- **Required**: Assign appropriate category to entities\n- **Implementation**: Set `_attr_entity_category`\n  ```python\n  class MySensor(SensorEntity):\n      _attr_entity_category = EntityCategory.DIAGNOSTIC\n  ```\n- Categories include: `DIAGNOSTIC` for system/technical information\n\n### Device Classes\n- **Use When Available**: Set appropriate device class for entity type\n  ```python\n  class MyTemperatureSensor(SensorEntity):\n      _attr_device_class = SensorDeviceClass.TEMPERATURE\n  ```\n- Provides context for: unit conversion, voice control, UI representation\n\n### Disabled by Default\n- **Disable Noisy/Less Popular Entities**: Reduce resource usage\n  ```python\n  class MySignalStrengthSensor(SensorEntity):\n      _attr_entity_registry_enabled_default = False\n  ```\n- Target: frequently changing states, technical diagnostics\n\n### Entity Translations\n- **Required with has_entity_name**: Support international users\n- **Implementation**:\n  ```python\n  class MySensor(SensorEntity):\n      _attr_has_entity_name = True\n      _attr_translation_key = \"phase_voltage\"\n  ```\n- Create `strings.json` with translations:\n  ```json\n  {\n    \"entity\": {\n      \"sensor\": {\n        \"phase_voltage\": {\n          \"name\": \"Phase voltage\"\n        }\n      }\n    }\n  }\n  ```\n\n### Exception Translations (Gold)\n- **Translatable Errors**: Use translation keys for user-facing exceptions\n- **Implementation**:\n  ```python\n  raise ServiceValidationError(\n      translation_domain=DOMAIN,\n      translation_key=\"end_date_before_start_date\",\n  )\n  ```\n- Add to `strings.json`:\n  ```json\n  {\n    \"exceptions\": {\n      \"end_date_before_start_date\": {\n        \"message\": \"The end date cannot be before the start date.\"\n      }\n    }\n  }\n  ```\n\n### Icon Translations (Gold)\n- **Dynamic Icons**: Support state and range-based icon selection\n- **State-based Icons**:\n  ```json\n  {\n    \"entity\": {\n      \"sensor\": {\n        \"tree_pollen\": {\n          \"default\": \"mdi:tree\",\n          \"state\": {\n            \"high\": \"mdi:tree-outline\"\n          }\n        }\n      }\n    }\n  }\n  ```\n- **Range-based Icons** (for numeric values):\n  ```json\n  {\n    \"entity\": {\n      \"sensor\": {\n        \"battery_level\": {\n          \"default\": \"mdi:battery-unknown\",\n          \"range\": {\n            \"0\": \"mdi:battery-outline\",\n            \"90\": \"mdi:battery-90\",\n            \"100\": \"mdi:battery\"\n          }\n        }\n      }\n    }\n  }\n  ```\n\n## Testing Requirements\n\n- **Location**: `tests/components/{domain}/`\n- **Coverage Requirement**: Above 95% test coverage for all modules\n- **Best Practices**:\n  - Use pytest fixtures from `tests.common`\n  - Mock all external dependencies\n  - Use snapshots for complex data structures\n  - Follow existing test patterns\n\n### Config Flow Testing\n- **100% Coverage Required**: All config flow paths must be tested\n- **Test Scenarios**:\n  - All flow initiation methods (user, discovery, import)\n  - Successful configuration paths\n  - Error recovery scenarios\n  - Prevention of duplicate entries\n  - Flow completion after errors\n\n## Development Commands\n\n### Code Quality & Linting\n- **Run all linters on all files**: `pre-commit run --all-files`\n- **Run linters on staged files only**: `pre-commit run`\n- **PyLint on everything** (slow): `pylint homeassistant`\n- **PyLint on specific folder**: `pylint homeassistant/components/my_integration`\n- **MyPy type checking (whole project)**: `mypy homeassistant/`\n- **MyPy on specific integration**: `mypy homeassistant/components/my_integration`\n\n### Testing\n- **Integration-specific tests** (recommended):\n  ```bash\n  pytest ./tests/components/<integration_domain> \\\n    --cov=homeassistant.components.<integration_domain> \\\n    --cov-report term-missing \\\n    --durations-min=1 \\\n    --durations=0 \\\n    --numprocesses=auto\n  ```\n- **Quick test of changed files**: `pytest --timeout=10 --picked`\n- **Update test snapshots**: Add `--snapshot-update` to pytest command\n  - ‚ö†Ô∏è Omit test results after using `--snapshot-update`\n  - Always run tests again without the flag to verify snapshots\n- **Full test suite** (AVOID - very slow): `pytest ./tests`\n\n### Dependencies & Requirements\n- **Update generated files after dependency changes**: `python -m script.gen_requirements_all`\n- **Install all Python requirements**: \n  ```bash\n  uv pip install -r requirements_all.txt -r requirements.txt -r requirements_test.txt\n  ```\n- **Install test requirements only**: \n  ```bash\n  uv pip install -r requirements_test_all.txt -r requirements.txt\n  ```\n\n### Translations\n- **Update translations after strings.json changes**: \n  ```bash\n  python -m script.translations develop --all\n  ```\n\n### Project Validation\n- **Run hassfest** (checks project structure and updates generated files): \n  ```bash\n  python -m script.hassfest\n  ```\n\n### File Locations\n- **Integration code**: `./homeassistant/components/<integration_domain>/`\n- **Integration tests**: `./tests/components/<integration_domain>/`\n\n## Integration Templates\n\n### Standard Integration Structure\n```\nhomeassistant/components/my_integration/\n‚îú‚îÄ‚îÄ __init__.py          # Entry point with async_setup_entry\n‚îú‚îÄ‚îÄ manifest.json        # Integration metadata and dependencies\n‚îú‚îÄ‚îÄ const.py            # Domain and constants\n‚îú‚îÄ‚îÄ config_flow.py      # UI configuration flow\n‚îú‚îÄ‚îÄ coordinator.py      # Data update coordinator (if needed)\n‚îú‚îÄ‚îÄ entity.py          # Base entity class (if shared patterns)\n‚îú‚îÄ‚îÄ sensor.py          # Sensor platform\n‚îú‚îÄ‚îÄ strings.json        # User-facing text and translations\n‚îú‚îÄ‚îÄ services.yaml       # Service definitions (if applicable)\n‚îî‚îÄ‚îÄ quality_scale.yaml  # Quality scale rule status\n```\n\n### Quality Scale Progression\n- **Bronze ‚Üí Silver**: Add entity unavailability, parallel updates, auth flows\n- **Silver ‚Üí Gold**: Add device management, diagnostics, translations  \n- **Gold ‚Üí Platinum**: Add strict typing, async dependencies, websession injection\n\n### Minimal Integration Checklist\n- [ ] `manifest.json` with required fields (domain, name, codeowners, etc.)\n- [ ] `__init__.py` with `async_setup_entry` and `async_unload_entry`\n- [ ] `config_flow.py` with UI configuration support\n- [ ] `const.py` with `DOMAIN` constant\n- [ ] `strings.json` with at least config flow text\n- [ ] Platform files (`sensor.py`, etc.) as needed\n- [ ] `quality_scale.yaml` with rule status tracking\n\n## Common Anti-Patterns & Best Practices\n\n### ‚ùå **Avoid These Patterns**\n```python\n# Blocking operations in event loop\ndata = requests.get(url)  # ‚ùå Blocks event loop\ntime.sleep(5)  # ‚ùå Blocks event loop\n\n# Reusing BleakClient instances\nself.client = BleakClient(address)\nawait self.client.connect()\n# Later...\nawait self.client.connect()  # ‚ùå Don't reuse\n\n# Hardcoded strings in code\nself._attr_name = \"Temperature Sensor\"  # ‚ùå Not translatable\n\n# Missing error handling\ndata = await self.api.get_data()  # ‚ùå No exception handling\n\n# Storing sensitive data in diagnostics\nreturn {\"api_key\": entry.data[CONF_API_KEY]}  # ‚ùå Exposes secrets\n\n# Accessing hass.data directly in tests\ncoordinator = hass.data[DOMAIN][entry.entry_id]  # ‚ùå Don't access hass.data\n\n# User-configurable polling intervals\n# In config flow\nvol.Optional(\"scan_interval\", default=60): cv.positive_int  # ‚ùå Not allowed\n# In coordinator\nupdate_interval = timedelta(minutes=entry.data.get(\"scan_interval\", 1))  # ‚ùå Not allowed\n\n# User-configurable config entry names (non-helper integrations)\nvol.Optional(\"name\", default=\"My Device\"): cv.string  # ‚ùå Not allowed in regular integrations\n\n# Too much code in try block\ntry:\n    response = await client.get_data()  # Can throw\n    # ‚ùå Data processing should be outside try block\n    temperature = response[\"temperature\"] / 10\n    humidity = response[\"humidity\"] \n    self._attr_native_value = temperature\nexcept ClientError:\n    _LOGGER.error(\"Failed to fetch data\")\n\n# Bare exceptions in regular code\ntry:\n    value = await sensor.read_value()\nexcept Exception:  # ‚ùå Too broad - catch specific exceptions\n    _LOGGER.error(\"Failed to read sensor\")\n```\n\n### ‚úÖ **Use These Patterns Instead**\n```python\n# Async operations with executor\ndata = await hass.async_add_executor_job(requests.get, url)\nawait asyncio.sleep(5)  # ‚úÖ Non-blocking\n\n# Fresh BleakClient instances\nclient = BleakClient(address)  # ‚úÖ New instance each time\nawait client.connect()\n\n# Translatable entity names\n_attr_translation_key = \"temperature_sensor\"  # ‚úÖ Translatable\n\n# Proper error handling\ntry:\n    data = await self.api.get_data()\nexcept ApiException as err:\n    raise UpdateFailed(f\"API error: {err}\") from err\n\n# Redacted diagnostics data\nreturn async_redact_data(data, {\"api_key\", \"password\"})  # ‚úÖ Safe\n\n# Test through proper integration setup and fixtures\n@pytest.fixture\nasync def init_integration(hass, mock_config_entry, mock_api):\n    mock_config_entry.add_to_hass(hass)\n    await hass.config_entries.async_setup(mock_config_entry.entry_id)  # ‚úÖ Proper setup\n\n# Integration-determined polling intervals (not user-configurable)\nSCAN_INTERVAL = timedelta(minutes=5)  # ‚úÖ Common pattern: constant in const.py\n\nclass MyCoordinator(DataUpdateCoordinator[MyData]):\n    def __init__(self, hass: HomeAssistant, client: MyClient, config_entry: ConfigEntry) -> None:\n        # ‚úÖ Integration determines interval based on device capabilities, connection type, etc.\n        interval = timedelta(minutes=1) if client.is_local else SCAN_INTERVAL\n        super().__init__(\n            hass, \n            logger=LOGGER, \n            name=DOMAIN, \n            update_interval=interval,\n            config_entry=config_entry,  # ‚úÖ Pass config_entry - it's accepted and recommended\n        )\n```\n\n### Entity Performance Optimization\n```python\n# Use __slots__ for memory efficiency\nclass MySensor(SensorEntity):\n    __slots__ = (\"_attr_native_value\", \"_attr_available\")\n    \n    @property \n    def should_poll(self) -> bool:\n        \"\"\"Disable polling when using coordinator.\"\"\"\n        return False  # ‚úÖ Let coordinator handle updates\n```\n\n## Testing Patterns\n\n### Testing Best Practices\n- **Never access `hass.data` directly** - Use fixtures and proper integration setup instead\n- **Use snapshot testing** - For verifying entity states and attributes\n- **Test through integration setup** - Don't test entities in isolation\n- **Mock external APIs** - Use fixtures with realistic JSON data\n- **Verify registries** - Ensure entities are properly registered with devices\n\n### Config Flow Testing Template\n```python\nasync def test_user_flow_success(hass, mock_api):\n    \"\"\"Test successful user flow.\"\"\"\n    result = await hass.config_entries.flow.async_init(\n        DOMAIN, context={\"source\": config_entries.SOURCE_USER}\n    )\n    assert result[\"type\"] == FlowResultType.FORM\n    assert result[\"step_id\"] == \"user\"\n\n    # Test form submission\n    result = await hass.config_entries.flow.async_configure(\n        result[\"flow_id\"], user_input=TEST_USER_INPUT\n    )\n    assert result[\"type\"] == FlowResultType.CREATE_ENTRY\n    assert result[\"title\"] == \"My Device\"\n    assert result[\"data\"] == TEST_USER_INPUT\n\nasync def test_flow_connection_error(hass, mock_api_error):\n    \"\"\"Test connection error handling.\"\"\"\n    result = await hass.config_entries.flow.async_init(\n        DOMAIN, context={\"source\": config_entries.SOURCE_USER}\n    )\n    result = await hass.config_entries.flow.async_configure(\n        result[\"flow_id\"], user_input=TEST_USER_INPUT\n    )\n    assert result[\"type\"] == FlowResultType.FORM\n    assert result[\"errors\"] == {\"base\": \"cannot_connect\"}\n```\n\n### Entity Testing Patterns\n```python\n@pytest.mark.parametrize(\"init_integration\", [Platform.SENSOR], indirect=True)\n@pytest.mark.usefixtures(\"entity_registry_enabled_by_default\", \"init_integration\")\nasync def test_entities(\n    hass: HomeAssistant,\n    snapshot: SnapshotAssertion,\n    entity_registry: er.EntityRegistry,\n    device_registry: dr.DeviceRegistry,\n    mock_config_entry: MockConfigEntry,\n) -> None:\n    \"\"\"Test the sensor entities.\"\"\"\n    await snapshot_platform(hass, entity_registry, snapshot, mock_config_entry.entry_id)\n\n    # Ensure entities are correctly assigned to device\n    device_entry = device_registry.async_get_device(\n        identifiers={(DOMAIN, \"device_unique_id\")}\n    )\n    assert device_entry\n    entity_entries = er.async_entries_for_config_entry(\n        entity_registry, mock_config_entry.entry_id\n    )\n    for entity_entry in entity_entries:\n        assert entity_entry.device_id == device_entry.id\n```\n\n### Mock Patterns\n```python\n# Modern integration fixture setup\n@pytest.fixture\ndef mock_config_entry() -> MockConfigEntry:\n    \"\"\"Return the default mocked config entry.\"\"\"\n    return MockConfigEntry(\n        title=\"My Integration\",\n        domain=DOMAIN,\n        data={CONF_HOST: \"127.0.0.1\", CONF_API_KEY: \"test_key\"},\n        unique_id=\"device_unique_id\",\n    )\n\n@pytest.fixture\ndef mock_device_api() -> Generator[MagicMock]:\n    \"\"\"Return a mocked device API.\"\"\"\n    with patch(\"homeassistant.components.my_integration.MyDeviceAPI\", autospec=True) as api_mock:\n        api = api_mock.return_value\n        api.get_data.return_value = MyDeviceData.from_json(\n            load_fixture(\"device_data.json\", DOMAIN)\n        )\n        yield api\n\n@pytest.fixture\nasync def init_integration(\n    hass: HomeAssistant,\n    mock_config_entry: MockConfigEntry,\n    mock_device_api: MagicMock,\n) -> MockConfigEntry:\n    \"\"\"Set up the integration for testing.\"\"\"\n    mock_config_entry.add_to_hass(hass)\n    await hass.config_entries.async_setup(mock_config_entry.entry_id)\n    await hass.async_block_till_done()\n    return mock_config_entry\n```\n\n## Debugging & Troubleshooting\n\n### Common Issues & Solutions\n- **Integration won't load**: Check `manifest.json` syntax and required fields\n- **Entities not appearing**: Verify `unique_id` and `has_entity_name` implementation  \n- **Config flow errors**: Check `strings.json` entries and error handling\n- **Discovery not working**: Verify manifest discovery configuration and callbacks\n- **Tests failing**: Check mock setup and async context\n\n### Debug Logging Setup\n```python\n# Enable debug logging in tests\ncaplog.set_level(logging.DEBUG, logger=\"my_integration\")\n\n# In integration code - use proper logging\n_LOGGER = logging.getLogger(__name__)\n_LOGGER.debug(\"Processing data: %s\", data)  # Use lazy logging\n```\n\n### Validation Commands\n```bash\n# Check specific integration\npython -m script.hassfest --integration-path homeassistant/components/my_integration\n\n# Validate quality scale\n# Check quality_scale.yaml against current rules\n\n# Run integration tests with coverage\npytest ./tests/components/my_integration \\\n  --cov=homeassistant.components.my_integration \\\n  --cov-report term-missing\n```", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".github/copilot-instructions.md", "language": "markdown", "complexity": "complex", "star_count": 80370, "quality_score": 1.0, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 1167}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Build images\n\n# yamllint disable-line rule:truthy\non:\n  workflow_dispatch:\n  release:\n    types: [\"published\"]\n  schedule:\n    - cron: \"0 2 * * *\"\n\nenv:\n  BUILD_TYPE: core\n  DEFAULT_PYTHON: \"3.13\"\n  PIP_TIMEOUT: 60\n  UV_HTTP_TIMEOUT: 60\n  UV_SYSTEM_PYTHON: \"true\"\n\njobs:\n  init:\n    name: Initialize build\n    if: github.repository_owner == 'home-assistant'\n    runs-on: ubuntu-latest\n    outputs:\n      architectures: ${{ steps.info.outputs.architectures }}\n      version: ${{ steps.version.outputs.version }}\n      channel: ${{ steps.version.outputs.channel }}\n      publish: ${{ steps.version.outputs.publish }}\n    steps:\n      - name: Checkout the repository\n        uses: actions/checkout@v4.2.2\n        with:\n          fetch-depth: 0\n\n      - name: Set up Python ${{ env.DEFAULT_PYTHON }}\n        uses: actions/setup-python@v5.6.0\n        with:\n          python-version: ${{ env.DEFAULT_PYTHON }}\n\n      - name: Get information\n        id: info\n        uses: home-assistant/actions/helpers/info@master\n\n      - name: Get version\n        id: version\n        uses: home-assistant/actions/helpers/version@master\n        with:\n          type: ${{ env.BUILD_TYPE }}\n\n      - name: Verify version\n        uses: home-assistant/actions/helpers/verify-version@master\n        with:\n          ignore-dev: true\n\n      - name: Fail if translations files are checked in\n        run: |\n          if [ -n \"$(find homeassistant/components/*/translations -type f)\" ]; then\n              echo \"Translations files are checked in, please remove the following files:\"\n              find homeassistant/components/*/translations -type f\n              exit 1\n          fi\n\n      - name: Download Translations\n        run: python3 -m script.translations download\n        env:\n          LOKALISE_TOKEN: ${{ secrets.LOKALISE_TOKEN }}\n\n      - name: Archive translations\n        shell: bash\n        run: find ./homeassistant/components/*/translations -name \"*.json\" | tar zcvf translations.tar.gz -T -\n\n      - name: Upload translations\n        uses: actions/upload-artifact@v4.6.2\n        with:\n          name: translations\n          path: translations.tar.gz\n          if-no-files-found: error\n\n  build_base:\n    name: Build ${{ matrix.arch }} base core image\n    if: github.repository_owner == 'home-assistant'\n    needs: init\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n      id-token: write\n    strategy:\n      fail-fast: false\n      matrix:\n        arch: ${{ fromJson(needs.init.outputs.architectures) }}\n    steps:\n      - name: Checkout the repository\n        uses: actions/checkout@v4.2.2\n\n      - name: Download nightly wheels of frontend\n        if: needs.init.outputs.channel == 'dev'\n        uses: dawidd6/action-download-artifact@v11\n        with:\n          github_token: ${{secrets.GITHUB_TOKEN}}\n          repo: home-assistant/frontend\n          branch: dev\n          workflow: nightly.yaml\n          workflow_conclusion: success\n          name: wheels\n\n      - name: Download nightly wheels of intents\n        if: needs.init.outputs.channel == 'dev'\n        uses: dawidd6/action-download-artifact@v11\n        with:\n          github_token: ${{secrets.GITHUB_TOKEN}}\n          repo: OHF-Voice/intents-package\n          branch: main\n          workflow: nightly.yaml\n          workflow_conclusion: success\n          name: package\n\n      - name: Set up Python ${{ env.DEFAULT_PYTHON }}\n        if: needs.init.outputs.channel == 'dev'\n        uses: actions/setup-python@v5.6.0\n        with:\n          python-version: ${{ env.DEFAULT_PYTHON }}\n\n      - name: Adjust nightly version\n        if: needs.init.outputs.channel == 'dev'\n        shell: bash\n        env:\n          UV_PRERELEASE: allow\n        run: |\n          python3 -m pip install \"$(grep '^uv' < requirements.txt)\"\n          uv pip install packaging tomli\n          uv pip install .\n          python3 script/version_bump.py nightly --set-nightly-version \"${{ needs.init.outputs.version }}\"\n\n          if [[ \"$(ls home_assistant_frontend*.whl)\" =~ ^home_assistant_frontend-(.*)-py3-none-any.whl$ ]]; then\n            echo \"Found frontend wheel, setting version to: ${BASH_REMATCH[1]}\"\n            frontend_version=\"${BASH_REMATCH[1]}\" yq \\\n              --inplace e -o json \\\n              '.requirements = [\"home-assistant-frontend==\"+env(frontend_version)]' \\\n              homeassistant/components/frontend/manifest.json\n\n            sed -i \"s|home-assistant-frontend==.*|home-assistant-frontend==${BASH_REMATCH[1]}|\" \\\n              homeassistant/package_constraints.txt\n\n            sed -i \"s|home-assistant-frontend==.*||\" requirements_all.txt\n          fi\n\n          if [[ \"$(ls home_assistant_intents*.whl)\" =~ ^home_assistant_intents-(.*)-py3-none-any.whl$ ]]; then\n            echo \"Found intents wheel, setting version to: ${BASH_REMATCH[1]}\"\n            yq \\\n              --inplace e -o json \\\n              'del(.requirements[] | select(contains(\"home-assistant-intents\")))' \\\n              homeassistant/components/conversation/manifest.json\n\n            intents_version=\"${BASH_REMATCH[1]}\" yq \\\n              --inplace e -o json \\\n              '.requirements += [\"home-assistant-intents==\"+env(intents_version)]' \\\n              homeassistant/components/conversation/manifest.json\n\n            sed -i \"s|home-assistant-intents==.*|home-assistant-intents==${BASH_REMATCH[1]}|\" \\\n              homeassistant/package_constraints.txt\n\n            sed -i \"s|home-assistant-intents==.*||\" requirements_all.txt\n          fi\n\n      - name: Adjustments for armhf\n        if: matrix.arch == 'armhf'\n        run: |\n          # Pandas has issues building on armhf, it is expected they\n          # will drop the platform in the near future (they consider it\n          # \"flimsy\" on 386). The following packages depend on pandas,\n          # so we comment them out.\n          sed -i \"s|env-canada|# env-canada|g\" requirements_all.txt\n          sed -i \"s|noaa-coops|# noaa-coops|g\" requirements_all.txt\n          sed -i \"s|pyezviz|# pyezviz|g\" requirements_all.txt\n          sed -i \"s|pykrakenapi|# pykrakenapi|g\" requirements_all.txt\n\n      - name: Download translations\n        uses: actions/download-artifact@v4.3.0\n        with:\n          name: translations\n\n      - name: Extract translations\n        run: |\n          tar xvf translations.tar.gz\n          rm translations.tar.gz\n\n      - name: Write meta info file\n        shell: bash\n        run: |\n          echo \"${{ github.sha }};${{ github.ref }};${{ github.event_name }};${{ github.actor }}\" > rootfs/OFFICIAL_IMAGE\n\n      - name: Login to GitHub Container Registry\n        uses: docker/login-action@v3.4.0\n        with:\n          registry: ghcr.io\n          username: ${{ github.repository_owner }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Build base image\n        uses: home-assistant/builder@2025.03.0\n        with:\n          args: |\n            $BUILD_ARGS \\\n            --${{ matrix.arch }} \\\n            --cosign \\\n            --target /data \\\n            --generic ${{ needs.init.outputs.version }}\n\n  build_machine:\n    name: Build ${{ matrix.machine }} machine core image\n    if: github.repository_owner == 'home-assistant'\n    needs: [\"init\", \"build_base\"]\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n      id-token: write\n    strategy:\n      matrix:\n        machine:\n          - generic-x86-64\n          - intel-nuc\n          - khadas-vim3\n          - odroid-c2\n          - odroid-c4\n          - odroid-m1\n          - odroid-n2\n          - odroid-xu\n          - qemuarm\n          - qemuarm-64\n          - qemux86\n          - qemux86-64\n          - raspberrypi\n          - raspberrypi2\n          - raspberrypi3\n          - raspberrypi3-64\n          - raspberrypi4\n          - raspberrypi4-64\n          - raspberrypi5-64\n          - tinker\n          - yellow\n          - green\n    steps:\n      - name: Checkout the repository\n        uses: actions/checkout@v4.2.2\n\n      - name: Set build additional args\n        run: |\n          # Create general tags\n          if [[ \"${{ needs.init.outputs.version }}\" =~ d ]]; then\n            echo \"BUILD_ARGS=--additional-tag dev\" >> $GITHUB_ENV\n          elif [[ \"${{ needs.init.outputs.version }}\" =~ b ]]; then\n            echo \"BUILD_ARGS=--additional-tag beta\" >> $GITHUB_ENV\n          else\n            echo \"BUILD_ARGS=--additional-tag stable\" >> $GITHUB_ENV\n          fi\n\n      - name: Login to GitHub Container Registry\n        uses: docker/login-action@v3.4.0\n        with:\n          registry: ghcr.io\n          username: ${{ github.repository_owner }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Build base image\n        uses: home-assistant/builder@2025.03.0\n        with:\n          args: |\n            $BUILD_ARGS \\\n            --target /data/machine \\\n            --cosign \\\n            --machine \"${{ needs.init.outputs.version }}=${{ matrix.machine }}\"\n\n  publish_ha:\n    name: Publish version files\n    environment: ${{ needs.init.outputs.channel }}\n    if: github.repository_owner == 'home-assistant'\n    needs: [\"init\", \"build_machine\"]\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout the repository\n        uses: actions/checkout@v4.2.2\n\n      - name: Initialize git\n        uses: home-assistant/actions/helpers/git-init@master\n        with:\n          name: ${{ secrets.GIT_NAME }}\n          email: ${{ secrets.GIT_EMAIL }}\n          token: ${{ secrets.GIT_TOKEN }}\n\n      - name: Update version file\n        uses: home-assistant/actions/helpers/version-push@master\n        with:\n          key: \"homeassistant[]\"\n          key-description: \"Home Assistant Core\"\n          version: ${{ needs.init.outputs.version }}\n          channel: ${{ needs.init.outputs.channel }}\n\n      - name: Update version file (stable -> beta)\n        if: needs.init.outputs.channel == 'stable'\n        uses: home-assistant/actions/helpers/version-push@master\n        with:\n          key: \"homeassistant[]\"\n          key-description: \"Home Assistant Core\"\n          version: ${{ needs.init.outputs.version }}\n          channel: beta\n\n  publish_container:\n    name: Publish meta container for ${{ matrix.registry }}\n    environment: ${{ needs.init.outputs.channel }}\n    if: github.repository_owner == 'home-assistant'\n    needs: [\"init\", \"build_base\"]\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n      id-token: write\n    strategy:\n      fail-fast: false\n      matrix:\n        registry: [\"ghcr.io/home-assistant\", \"docker.io/homeassistant\"]\n    steps:\n      - name: Checkout the repository\n        uses: actions/checkout@v4.2.2\n\n      - name: Install Cosign\n        uses: sigstore/cosign-installer@v3.9.2\n        with:\n          cosign-release: \"v2.2.3\"\n\n      - name: Login to DockerHub\n        if: matrix.registry == 'docker.io/homeassistant'\n        uses: docker/login-action@v3.4.0\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n      - name: Login to GitHub Container Registry\n        if: matrix.registry == 'ghcr.io/home-assistant'\n        uses: docker/login-action@v3.4.0\n        with:\n          registry: ghcr.io\n          username: ${{ github.repository_owner }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Build Meta Image\n        shell: bash\n        run: |\n          export DOCKER_CLI_EXPERIMENTAL=enabled\n\n          function create_manifest() {\n            local tag_l=${1}\n            local tag_r=${2}\n            local registry=${{ matrix.registry }}\n\n            docker manifest create \"${registry}/home-assistant:${tag_l}\" \\\n              \"${registry}/amd64-homeassistant:${tag_r}\" \\\n              \"${registry}/i386-homeassistant:${tag_r}\" \\\n              \"${registry}/armhf-homeassistant:${tag_r}\" \\\n              \"${registry}/armv7-homeassistant:${tag_r}\" \\\n              \"${registry}/aarch64-homeassistant:${tag_r}\"\n\n            docker manifest annotate \"${registry}/home-assistant:${tag_l}\" \\\n              \"${registry}/amd64-homeassistant:${tag_r}\" \\\n              --os linux --arch amd64\n\n            docker manifest annotate \"${registry}/home-assistant:${tag_l}\" \\\n              \"${registry}/i386-homeassistant:${tag_r}\" \\\n              --os linux --arch 386\n\n            docker manifest annotate \"${registry}/home-assistant:${tag_l}\" \\\n              \"${registry}/armhf-homeassistant:${tag_r}\" \\\n              --os linux --arch arm --variant=v6\n\n            docker manifest annotate \"${registry}/home-assistant:${tag_l}\" \\\n              \"${registry}/armv7-homeassistant:${tag_r}\" \\\n              --os linux --arch arm --variant=v7\n\n            docker manifest annotate \"${registry}/home-assistant:${tag_l}\" \\\n              \"${registry}/aarch64-homeassistant:${tag_r}\" \\\n              --os linux --arch arm64 --variant=v8\n\n            docker manifest push --purge \"${registry}/home-assistant:${tag_l}\"\n            cosign sign --yes \"${registry}/home-assistant:${tag_l}\"\n          }\n\n          function validate_image() {\n            local image=${1}\n            if ! cosign verify --certificate-oidc-issuer https://token.actions.githubusercontent.com --certificate-identity-regexp https://github.com/home-assistant/core/.* \"${image}\"; then\n              echo \"Invalid signature!\"\n              exit 1\n            fi\n          }\n\n          function push_dockerhub() {\n            local image=${1}\n            local tag=${2}\n\n            docker tag \"ghcr.io/home-assistant/${image}:${tag}\" \"docker.io/homeassistant/${image}:${tag}\"\n            docker push \"docker.io/homeassistant/${image}:${tag}\"\n            cosign sign --yes \"docker.io/homeassistant/${image}:${tag}\"\n          }\n\n          # Pull images from github container registry and verify signature\n          docker pull \"ghcr.io/home-assistant/amd64-homeassistant:${{ needs.init.outputs.version }}\"\n          docker pull \"ghcr.io/home-assistant/i386-homeassistant:${{ needs.init.outputs.version }}\"\n          docker pull \"ghcr.io/home-assistant/armhf-homeassistant:${{ needs.init.outputs.version }}\"\n          docker pull \"ghcr.io/home-assistant/armv7-homeassistant:${{ needs.init.outputs.version }}\"\n          docker pull \"ghcr.io/home-assistant/aarch64-homeassistant:${{ needs.init.outputs.version }}\"\n\n          validate_image \"ghcr.io/home-assistant/amd64-homeassistant:${{ needs.init.outputs.version }}\"\n          validate_image \"ghcr.io/home-assistant/i386-homeassistant:${{ needs.init.outputs.version }}\"\n          validate_image \"ghcr.io/home-assistant/armhf-homeassistant:${{ needs.init.outputs.version }}\"\n          validate_image \"ghcr.io/home-assistant/armv7-homeassistant:${{ needs.init.outputs.version }}\"\n          validate_image \"ghcr.io/home-assistant/aarch64-homeassistant:${{ needs.init.outputs.version }}\"\n\n          if [[ \"${{ matrix.registry }}\" ==  \"docker.io/homeassistant\" ]]; then\n            # Upload images to dockerhub\n            push_dockerhub \"amd64-homeassistant\" \"${{ needs.init.outputs.version }}\"\n            push_dockerhub \"i386-homeassistant\" \"${{ needs.init.outputs.version }}\"\n            push_dockerhub \"armhf-homeassistant\" \"${{ needs.init.outputs.version }}\"\n            push_dockerhub \"armv7-homeassistant\" \"${{ needs.init.outputs.version }}\"\n            push_dockerhub \"aarch64-homeassistant\" \"${{ needs.init.outputs.version }}\"\n          fi\n\n          # Create version tag\n          create_manifest \"${{ needs.init.outputs.version }}\" \"${{ needs.init.outputs.version }}\"\n\n          # Create general tags\n          if [[ \"${{ needs.init.outputs.version }}\" =~ d ]]; then\n            create_manifest \"dev\" \"${{ needs.init.outputs.version }}\"\n          elif [[ \"${{ needs.init.outputs.version }}\" =~ b ]]; then\n            create_manifest \"beta\" \"${{ needs.init.outputs.version }}\"\n            create_manifest \"rc\" \"${{ needs.init.outputs.version }}\"\n          else\n            create_manifest \"stable\" \"${{ needs.init.outputs.version }}\"\n            create_manifest \"latest\" \"${{ needs.init.outputs.version }}\"\n            create_manifest \"beta\" \"${{ needs.init.outputs.version }}\"\n            create_manifest \"rc\" \"${{ needs.init.outputs.version }}\"\n\n            # Create series version tag (e.g. 2021.6)\n            v=\"${{ needs.init.outputs.version }}\"\n            create_manifest \"${v%.*}\" \"${{ needs.init.outputs.version }}\"\n          fi\n\n  build_python:\n    name: Build PyPi package\n    environment: ${{ needs.init.outputs.channel }}\n    needs: [\"init\", \"build_base\"]\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      id-token: write\n    if: github.repository_owner == 'home-assistant' && needs.init.outputs.publish == 'true'\n    steps:\n      - name: Checkout the repository\n        uses: actions/checkout@v4.2.2\n\n      - name: Set up Python ${{ env.DEFAULT_PYTHON }}\n        uses: actions/setup-python@v5.6.0\n        with:\n          python-version: ${{ env.DEFAULT_PYTHON }}\n\n      - name: Download translations\n        uses: actions/download-artifact@v4.3.0\n        with:\n          name: translations\n\n      - name: Extract translations\n        run: |\n          tar xvf translations.tar.gz\n          rm translations.tar.gz\n\n      - name: Build package\n        shell: bash\n        run: |\n          # Remove dist, build, and homeassistant.egg-info\n          # when build locally for testing!\n          pip install build\n          python -m build\n\n      - name: Upload package to PyPI\n        uses: pypa/gh-action-pypi-publish@v1.12.4\n        with:\n          skip-existing: true\n\n  hassfest-image:\n    name: Build and test hassfest image\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n      attestations: write\n      id-token: write\n    needs: [\"init\"]\n    if: github.repository_owner == 'home-assistant'\n    env:\n      HASSFEST_IMAGE_NAME: ghcr.io/home-assistant/hassfest\n      HASSFEST_IMAGE_TAG: ghcr.io/home-assistant/hassfest:${{ needs.init.outputs.version }}\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n\n      - name: Login to GitHub Container Registry\n        uses: docker/login-action@74a5d142397b4f367a81961eba4e8cd7edddf772 # v3.4.0\n        with:\n          registry: ghcr.io\n          username: ${{ github.repository_owner }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Build Docker image\n        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83 # v6.18.0\n        with:\n          context: . # So action will not pull the repository again\n          file: ./script/hassfest/docker/Dockerfile\n          load: true\n          tags: ${{ env.HASSFEST_IMAGE_TAG }}\n\n      - name: Run hassfest against core\n        run: docker run --rm -v ${{ github.workspace }}:/github/workspace ${{ env.HASSFEST_IMAGE_TAG }} --core-path=/github/workspace\n\n      - name: Push Docker image\n        if: needs.init.outputs.channel != 'dev' && needs.init.outputs.publish == 'true'\n        id: push\n        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83 # v6.18.0\n        with:\n          context: . # So action will not pull the repository again\n          file: ./script/hassfest/docker/Dockerfile\n          push: true\n          tags: ${{ env.HASSFEST_IMAGE_TAG }},${{ env.HASSFEST_IMAGE_NAME }}:latest\n\n      - name: Generate artifact attestation\n        if: needs.init.outputs.channel != 'dev' && needs.init.outputs.publish == 'true'\n        uses: actions/attest-build-provenance@e8998f949152b193b063cb0ec769d69d929409be # v2.4.0\n        with:\n          subject-name: ${{ env.HASSFEST_IMAGE_NAME }}\n          subject-digest: ${{ steps.push.outputs.digest }}\n          push-to-registry: true\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".github/workflows/builder.yml", "language": "yaml", "complexity": "complex", "star_count": 80370, "quality_score": 0.8600000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 539}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Auto-detect duplicate issues\n\n# yamllint disable-line rule:truthy\non:\n  issues:\n    types: [labeled]\n\npermissions:\n  issues: write\n  models: read\n\njobs:\n  detect-duplicates:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Check if integration label was added and extract details\n        id: extract\n        uses: actions/github-script@v7.0.1\n        with:\n          script: |\n            // Debug: Log the event payload\n            console.log('Event name:', context.eventName);\n            console.log('Event action:', context.payload.action);\n            console.log('Event payload keys:', Object.keys(context.payload));\n\n            // Check the specific label that was added\n            const addedLabel = context.payload.label;\n            if (!addedLabel) {\n              console.log('No label found in labeled event payload');\n              core.setOutput('should_continue', 'false');\n              return;\n            }\n\n            console.log(`Label added: ${addedLabel.name}`);\n\n            if (!addedLabel.name.startsWith('integration:')) {\n              console.log('Added label is not an integration label, skipping duplicate detection');\n              core.setOutput('should_continue', 'false');\n              return;\n            }\n\n            console.log(`Integration label added: ${addedLabel.name}`);\n\n            let currentIssue;\n            let integrationLabels = [];\n\n            try {\n              const issue = await github.rest.issues.get({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: context.payload.issue.number\n              });\n\n              currentIssue = issue.data;\n\n              // Check if potential-duplicate label already exists\n              const hasPotentialDuplicateLabel = currentIssue.labels\n                .some(label => label.name === 'potential-duplicate');\n\n              if (hasPotentialDuplicateLabel) {\n                console.log('Issue already has potential-duplicate label, skipping duplicate detection');\n                core.setOutput('should_continue', 'false');\n                return;\n              }\n\n              integrationLabels = currentIssue.labels\n                .filter(label => label.name.startsWith('integration:'))\n                .map(label => label.name);\n            } catch (error) {\n              core.error(`Failed to fetch issue #${context.payload.issue.number}:`, error.message);\n              core.setOutput('should_continue', 'false');\n              return;\n            }\n\n            // Check if we've already posted a duplicate detection comment recently\n            let comments;\n            try {\n              comments = await github.rest.issues.listComments({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: context.payload.issue.number,\n                per_page: 10\n              });\n            } catch (error) {\n              core.error('Failed to fetch comments:', error.message);\n              // Continue anyway, worst case we might post a duplicate comment\n              comments = { data: [] };\n            }\n\n            // Check if we've already posted a duplicate detection comment\n            const recentDuplicateComment = comments.data.find(comment =>\n              comment.user && comment.user.login === 'github-actions[bot]' &&\n              comment.body.includes('<!-- workflow: detect-duplicate-issues -->')\n            );\n\n            if (recentDuplicateComment) {\n              console.log('Already posted duplicate detection comment, skipping');\n              core.setOutput('should_continue', 'false');\n              return;\n            }\n\n            core.setOutput('should_continue', 'true');\n            core.setOutput('current_number', currentIssue.number);\n            core.setOutput('current_title', currentIssue.title);\n            core.setOutput('current_body', currentIssue.body);\n            core.setOutput('current_url', currentIssue.html_url);\n            core.setOutput('integration_labels', JSON.stringify(integrationLabels));\n\n            console.log(`Current issue: #${currentIssue.number}`);\n            console.log(`Integration labels: ${integrationLabels.join(', ')}`);\n\n      - name: Fetch similar issues\n        id: fetch_similar\n        if: steps.extract.outputs.should_continue == 'true'\n        uses: actions/github-script@v7.0.1\n        env:\n          INTEGRATION_LABELS: ${{ steps.extract.outputs.integration_labels }}\n          CURRENT_NUMBER: ${{ steps.extract.outputs.current_number }}\n        with:\n          script: |\n            const integrationLabels = JSON.parse(process.env.INTEGRATION_LABELS);\n            const currentNumber = parseInt(process.env.CURRENT_NUMBER);\n\n            if (integrationLabels.length === 0) {\n              console.log('No integration labels found, skipping duplicate detection');\n              core.setOutput('has_similar', 'false');\n              return;\n            }\n\n            // Use GitHub search API to find issues with matching integration labels\n            console.log(`Searching for issues with integration labels: ${integrationLabels.join(', ')}`);\n\n            // Build search query for issues with any of the current integration labels\n            const labelQueries = integrationLabels.map(label => `label:\"${label}\"`);\n\n            // Calculate date 6 months ago\n            const sixMonthsAgo = new Date();\n            sixMonthsAgo.setMonth(sixMonthsAgo.getMonth() - 6);\n            const dateFilter = `created:>=${sixMonthsAgo.toISOString().split('T')[0]}`;\n\n            let searchQuery;\n\n            if (labelQueries.length === 1) {\n              searchQuery = `repo:${context.repo.owner}/${context.repo.repo} is:issue ${labelQueries[0]} ${dateFilter}`;\n            } else {\n              searchQuery = `repo:${context.repo.owner}/${context.repo.repo} is:issue (${labelQueries.join(' OR ')}) ${dateFilter}`;\n            }\n\n            console.log(`Search query: ${searchQuery}`);\n\n            let result;\n            try {\n              result = await github.rest.search.issuesAndPullRequests({\n                q: searchQuery,\n                per_page: 15,\n                sort: 'updated',\n                order: 'desc'\n              });\n            } catch (error) {\n              core.error('Failed to search for similar issues:', error.message);\n              if (error.status === 403 && error.message.includes('rate limit')) {\n                core.error('GitHub API rate limit exceeded');\n              }\n              core.setOutput('has_similar', 'false');\n              return;\n            }\n\n            // Filter out the current issue, pull requests, and newer issues (higher numbers)\n            const similarIssues = result.data.items\n              .filter(item =>\n                item.number !== currentNumber &&\n                !item.pull_request &&\n                item.number < currentNumber // Only include older issues (lower numbers)\n              )\n              .map(item => ({\n                number: item.number,\n                title: item.title,\n                body: item.body,\n                url: item.html_url,\n                state: item.state,\n                createdAt: item.created_at,\n                updatedAt: item.updated_at,\n                comments: item.comments,\n                labels: item.labels.map(l => l.name)\n              }));\n\n            console.log(`Found ${similarIssues.length} issues with matching integration labels`);\n            console.log('Raw similar issues:', JSON.stringify(similarIssues.slice(0, 3), null, 2));\n\n            if (similarIssues.length === 0) {\n              console.log('No similar issues found, setting has_similar to false');\n              core.setOutput('has_similar', 'false');\n              return;\n            }\n\n            console.log('Similar issues found, setting has_similar to true');\n            core.setOutput('has_similar', 'true');\n\n            // Clean the issue data to prevent JSON parsing issues\n            const cleanedIssues = similarIssues.slice(0, 15).map(item => {\n              // Handle body with improved truncation and null handling\n              let cleanBody = '';\n              if (item.body && typeof item.body === 'string') {\n                // Remove control characters\n                const cleaned = item.body.replace(/[\\u0000-\\u001F\\u007F-\\u009F]/g, '');\n                // Truncate to 1000 characters and add ellipsis if needed\n                cleanBody = cleaned.length > 1000\n                  ? cleaned.substring(0, 1000) + '...'\n                  : cleaned;\n              }\n\n              return {\n                number: item.number,\n                title: item.title.replace(/[\\u0000-\\u001F\\u007F-\\u009F]/g, ''), // Remove control characters\n                body: cleanBody,\n                url: item.url,\n                state: item.state,\n                createdAt: item.createdAt,\n                updatedAt: item.updatedAt,\n                comments: item.comments,\n                labels: item.labels\n              };\n            });\n\n            console.log(`Cleaned issues count: ${cleanedIssues.length}`);\n            console.log('First cleaned issue:', JSON.stringify(cleanedIssues[0], null, 2));\n\n            core.setOutput('similar_issues', JSON.stringify(cleanedIssues));\n\n      - name: Detect duplicates using AI\n        id: ai_detection\n        if: steps.extract.outputs.should_continue == 'true' && steps.fetch_similar.outputs.has_similar == 'true'\n        uses: actions/ai-inference@v1.2.3\n        with:\n          model: openai/gpt-4o\n          system-prompt: |\n            You are a Home Assistant issue duplicate detector. Your task is to identify TRUE DUPLICATES - issues that report the EXACT SAME problem, not just similar or related issues.\n\n            CRITICAL: An issue is ONLY a duplicate if:\n            - It describes the SAME problem with the SAME root cause\n            - Issues about the same integration but different problems are NOT duplicates\n            - Issues with similar symptoms but different causes are NOT duplicates\n\n            Important considerations:\n            - Open issues are more relevant than closed ones for duplicate detection\n            - Recently updated issues may indicate ongoing work or discussion\n            - Issues with more comments are generally more relevant and active\n            - Older closed issues might be resolved differently than newer approaches\n            - Consider the time between issues - very old issues may have different contexts\n\n            Rules:\n            1. ONLY mark as duplicate if the issues describe IDENTICAL problems\n            2. Look for issues that report the same problem or request the same functionality\n            3. Different error messages = NOT a duplicate (even if same integration)\n            4. For CLOSED issues, only mark as duplicate if they describe the EXACT same problem\n            5. For OPEN issues, use a lower threshold (90%+ similarity)\n            6. Prioritize issues with higher comment counts as they indicate more activity/relevance\n            7. When in doubt, do NOT mark as duplicate\n            8. Return ONLY a JSON array of issue numbers that are duplicates\n            9. If no duplicates are found, return an empty array: []\n            10. Maximum 5 potential duplicates, prioritize open issues with comments\n            11. Consider the age of issues - prefer recent duplicates over very old ones\n\n            Example response format:\n            [1234, 5678, 9012]\n\n          prompt: |\n            Current issue (just created):\n            Title: ${{ steps.extract.outputs.current_title }}\n            Body: ${{ steps.extract.outputs.current_body }}\n\n            Other issues to compare against (each includes state, creation date, last update, and comment count):\n            ${{ steps.fetch_similar.outputs.similar_issues }}\n\n            Analyze these issues and identify which ones describe IDENTICAL problems and thus are duplicates of the current issue. When sorting them, consider their state (open/closed), how recently they were updated, and their comment count (higher = more relevant).\n\n          max-tokens: 100\n\n      - name: Post duplicate detection results\n        id: post_results\n        if: steps.extract.outputs.should_continue == 'true' && steps.fetch_similar.outputs.has_similar == 'true'\n        uses: actions/github-script@v7.0.1\n        env:\n          AI_RESPONSE: ${{ steps.ai_detection.outputs.response }}\n          SIMILAR_ISSUES: ${{ steps.fetch_similar.outputs.similar_issues }}\n        with:\n          script: |\n            const aiResponse = process.env.AI_RESPONSE;\n\n            console.log('Raw AI response:', JSON.stringify(aiResponse));\n\n            let duplicateNumbers = [];\n            try {\n              // Clean the response of any potential control characters\n              const cleanResponse = aiResponse.trim().replace(/[\\u0000-\\u001F\\u007F-\\u009F]/g, '');\n              console.log('Cleaned AI response:', cleanResponse);\n\n              duplicateNumbers = JSON.parse(cleanResponse);\n\n              // Ensure it's an array and contains only numbers\n              if (!Array.isArray(duplicateNumbers)) {\n                console.log('AI response is not an array, trying to extract numbers');\n                const numberMatches = cleanResponse.match(/\\d+/g);\n                duplicateNumbers = numberMatches ? numberMatches.map(n => parseInt(n)) : [];\n              }\n\n              // Filter to only valid numbers\n              duplicateNumbers = duplicateNumbers.filter(n => typeof n === 'number' && !isNaN(n));\n\n            } catch (error) {\n              console.log('Failed to parse AI response as JSON:', error.message);\n              console.log('Raw response:', aiResponse);\n\n              // Fallback: try to extract numbers from the response\n              const numberMatches = aiResponse.match(/\\d+/g);\n              duplicateNumbers = numberMatches ? numberMatches.map(n => parseInt(n)) : [];\n              console.log('Extracted numbers as fallback:', duplicateNumbers);\n            }\n\n            if (!Array.isArray(duplicateNumbers) || duplicateNumbers.length === 0) {\n              console.log('No duplicates detected by AI');\n              return;\n            }\n\n            console.log(`AI detected ${duplicateNumbers.length} potential duplicates: ${duplicateNumbers.join(', ')}`);\n\n            // Get details of detected duplicates\n            const similarIssues = JSON.parse(process.env.SIMILAR_ISSUES);\n            const duplicates = similarIssues.filter(issue => duplicateNumbers.includes(issue.number));\n\n            if (duplicates.length === 0) {\n              console.log('No matching issues found for detected numbers');\n              return;\n            }\n\n            // Create comment with duplicate detection results\n            const duplicateLinks = duplicates.map(issue => `- [#${issue.number}: ${issue.title}](${issue.url})`).join('\\n');\n\n            const commentBody = [\n              '<!-- workflow: detect-duplicate-issues -->',\n              '### üîç **Potential duplicate detection**',\n              '',\n              'I\\'ve analyzed similar issues and found the following potential duplicates:',\n              '',\n              duplicateLinks,\n              '',\n              '**What to do next:**',\n              '1. Please review these issues to see if they match your issue',\n              '2. If you find an existing issue that covers your problem:',\n              '   - Consider closing this issue',\n              '   - Add your findings or üëç on the existing issue instead',\n              '3. If your issue is different or adds new aspects, please clarify how it differs',\n              '',\n              'This helps keep our issues organized and ensures similar issues are consolidated for better visibility.',\n              '',\n              '*This message was generated automatically by our duplicate detection system.*'\n            ].join('\\n');\n\n            try {\n              await github.rest.issues.createComment({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: context.payload.issue.number,\n                body: commentBody\n              });\n\n              console.log(`Posted duplicate detection comment with ${duplicates.length} potential duplicates`);\n\n              // Add the potential-duplicate label\n              await github.rest.issues.addLabels({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: context.payload.issue.number,\n                labels: ['potential-duplicate']\n              });\n\n              console.log('Added potential-duplicate label to the issue');\n            } catch (error) {\n              core.error('Failed to post duplicate detection comment or add label:', error.message);\n              if (error.status === 403) {\n                core.error('Permission denied or rate limit exceeded');\n              }\n              // Don't throw - we've done the analysis, just couldn't post the result\n            }\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".github/workflows/detect-duplicate-issues.yml", "language": "yaml", "complexity": "complex", "star_count": 80370, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 386}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Auto-detect non-English issues\n\n# yamllint disable-line rule:truthy\non:\n  issues:\n    types: [opened]\n\npermissions:\n  issues: write\n  models: read\n\njobs:\n  detect-language:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Check issue language\n        id: detect_language\n        uses: actions/github-script@v7.0.1\n        env:\n          ISSUE_NUMBER: ${{ github.event.issue.number }}\n          ISSUE_TITLE: ${{ github.event.issue.title }}\n          ISSUE_BODY: ${{ github.event.issue.body }}\n          ISSUE_USER_TYPE: ${{ github.event.issue.user.type }}\n        with:\n          script: |\n            // Get the issue details from environment variables\n            const issueNumber = process.env.ISSUE_NUMBER;\n            const issueTitle = process.env.ISSUE_TITLE || '';\n            const issueBody = process.env.ISSUE_BODY || '';\n            const userType = process.env.ISSUE_USER_TYPE;\n\n            // Skip language detection for bot users\n            if (userType === 'Bot') {\n              console.log('Skipping language detection for bot user');\n              core.setOutput('should_continue', 'false');\n              return;\n            }\n\n            console.log(`Checking language for issue #${issueNumber}`);\n            console.log(`Title: ${issueTitle}`);\n\n            // Combine title and body for language detection\n            const fullText = `${issueTitle}\\n\\n${issueBody}`;\n\n            // Check if the text is too short to reliably detect language\n            if (fullText.trim().length < 20) {\n              console.log('Text too short for reliable language detection');\n              core.setOutput('should_continue', 'false'); // Skip processing for very short text\n              return;\n            }\n\n            core.setOutput('issue_number', issueNumber);\n            core.setOutput('issue_text', fullText);\n            core.setOutput('should_continue', 'true');\n\n      - name: Detect language using AI\n        id: ai_language_detection\n        if: steps.detect_language.outputs.should_continue == 'true'\n        uses: actions/ai-inference@v1.2.3\n        with:\n          model: openai/gpt-4o-mini\n          system-prompt: |\n            You are a language detection system. Your task is to determine if the provided text is written in English or another language.\n\n            Rules:\n            1. Analyze the text and determine the primary language of the USER'S DESCRIPTION only\n            2. IGNORE markdown headers (lines starting with #, ##, ###, etc.) as these are from issue templates, not user input\n            3. IGNORE all code blocks (text between ``` or ` markers) as they may contain system-generated error messages in other languages\n            4. IGNORE error messages, logs, and system output even if not in code blocks - these often appear in the user's system language\n            5. Consider technical terms, code snippets, URLs, and file paths as neutral (they don't indicate non-English)\n            6. Focus ONLY on the actual sentences and descriptions written by the user explaining their issue\n            7. If the user's explanation/description is in English but includes non-English error messages or logs, consider it ENGLISH\n            8. Return ONLY a JSON object with two fields:\n               - \"is_english\": boolean (true if the user's description is primarily in English, false otherwise)\n               - \"detected_language\": string (the name of the detected language, e.g., \"English\", \"Spanish\", \"Chinese\", etc.)\n            9. Be lenient - if the user's explanation is in English with non-English system output, it's still English\n            10. Common programming terms, error messages, and technical jargon should not be considered as non-English\n            11. If you cannot reliably determine the language, set detected_language to \"undefined\"\n\n            Example response:\n            {\"is_english\": false, \"detected_language\": \"Spanish\"}\n\n          prompt: |\n            Please analyze the following issue text and determine if it is written in English:\n\n            ${{ steps.detect_language.outputs.issue_text }}\n\n          max-tokens: 50\n\n      - name: Process non-English issues\n        if: steps.detect_language.outputs.should_continue == 'true'\n        uses: actions/github-script@v7.0.1\n        env:\n          AI_RESPONSE: ${{ steps.ai_language_detection.outputs.response }}\n          ISSUE_NUMBER: ${{ steps.detect_language.outputs.issue_number }}\n        with:\n          script: |\n            const issueNumber = parseInt(process.env.ISSUE_NUMBER);\n            const aiResponse = process.env.AI_RESPONSE;\n\n            console.log('AI language detection response:', aiResponse);\n\n            let languageResult;\n            try {\n              languageResult = JSON.parse(aiResponse.trim());\n\n              // Validate the response structure\n              if (!languageResult || typeof languageResult.is_english !== 'boolean') {\n                throw new Error('Invalid response structure');\n              }\n            } catch (error) {\n              core.error(`Failed to parse AI response: ${error.message}`);\n              console.log('Raw AI response:', aiResponse);\n\n              // Log more details for debugging\n              core.warning('Defaulting to English due to parsing error');\n\n              // Default to English if we can't parse the response\n              return;\n            }\n\n            if (languageResult.is_english) {\n              console.log('Issue is in English, no action needed');\n              return;\n            }\n\n            // If language is undefined or not detected, skip processing\n            if (!languageResult.detected_language || languageResult.detected_language === 'undefined') {\n              console.log('Language could not be determined, skipping processing');\n              return;\n            }\n\n            console.log(`Issue detected as non-English: ${languageResult.detected_language}`);\n\n            // Post comment explaining the language requirement\n            const commentBody = [\n              '<!-- workflow: detect-non-english-issues -->',\n              '### üåê Non-English issue detected',\n              '',\n              `This issue appears to be written in **${languageResult.detected_language}** rather than English.`,\n              '',\n              'The Home Assistant project uses English as the primary language for issues to ensure that everyone in our international community can participate and help resolve issues. This allows any of our thousands of contributors to jump in and provide assistance.',\n              '',\n              '**What to do:**',\n              '1. Re-create the issue using the English language',\n              '2. If you need help with translation, consider using:',\n              '   - Translation tools like Google Translate',\n              '   - AI assistants like ChatGPT or Claude',\n              '',\n              'This helps our community provide the best possible support and ensures your issue gets the attention it deserves from our global contributor base.',\n              '',\n              'Thank you for your understanding! üôè'\n            ].join('\\n');\n\n            try {\n              // Add comment\n              await github.rest.issues.createComment({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: issueNumber,\n                body: commentBody\n              });\n\n              console.log('Posted language requirement comment');\n\n              // Add non-english label\n              await github.rest.issues.addLabels({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: issueNumber,\n                labels: ['non-english']\n              });\n\n              console.log('Added non-english label');\n\n              // Close the issue\n              await github.rest.issues.update({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: issueNumber,\n                state: 'closed',\n                state_reason: 'not_planned'\n              });\n\n              console.log('Closed the issue');\n\n            } catch (error) {\n              core.error('Failed to process non-English issue:', error.message);\n              if (error.status === 403) {\n                core.error('Permission denied or rate limit exceeded');\n              }\n            }\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".github/workflows/detect-non-english-issues.yml", "language": "yaml", "complexity": "complex", "star_count": 80370, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 194}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "{\n  \"problemMatcher\": [\n    {\n      \"owner\": \"python\",\n      \"pattern\": [\n        {\n          \"regexp\": \"^=+ slowest durations =+$\"\n        },\n        {\n          \"regexp\": \"^((.*s)\\\\s(call|setup|teardown)\\\\s+(.*)::(.*))$\",\n          \"message\": 1,\n          \"file\": 2,\n          \"loop\": true\n        }\n      ]\n    }\n  ]\n}\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".github/workflows/matchers/pytest-slow.json", "language": "json", "complexity": "simple", "star_count": 80370, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Restrict task creation\n\n# yamllint disable-line rule:truthy\non:\n  issues:\n    types: [opened]\n\njobs:\n  check-authorization:\n    runs-on: ubuntu-latest\n    # Only run if this is a Task issue type (from the issue form)\n    if: github.event.issue.issue_type == 'Task'\n    steps:\n      - name: Check if user is authorized\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const issueAuthor = context.payload.issue.user.login;\n\n            // First check if user is an organization member\n            try {\n              await github.rest.orgs.checkMembershipForUser({\n                org: 'home-assistant',\n                username: issueAuthor\n              });\n              console.log(`‚úÖ ${issueAuthor} is an organization member`);\n              return; // Authorized, no need to check further\n            } catch (error) {\n              console.log(`‚ÑπÔ∏è ${issueAuthor} is not an organization member, checking codeowners...`);\n            }\n\n            // If not an org member, check if they're a codeowner\n            try {\n              // Fetch CODEOWNERS file from the repository\n              const { data: codeownersFile } = await github.rest.repos.getContent({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                path: 'CODEOWNERS',\n                ref: 'dev'\n              });\n\n              // Decode the content (it's base64 encoded)\n              const codeownersContent = Buffer.from(codeownersFile.content, 'base64').toString('utf-8');\n\n              // Check if the issue author is mentioned in CODEOWNERS\n              // GitHub usernames in CODEOWNERS are prefixed with @\n              if (codeownersContent.includes(`@${issueAuthor}`)) {\n                console.log(`‚úÖ ${issueAuthor} is a integration code owner`);\n                return; // Authorized\n              }\n            } catch (error) {\n              console.error('Error checking CODEOWNERS:', error);\n            }\n\n            // If we reach here, user is not authorized\n            console.log(`‚ùå ${issueAuthor} is not authorized to create Task issues`);\n\n            // Close the issue with a comment\n            await github.rest.issues.createComment({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              issue_number: context.issue.number,\n              body: `Hi @${issueAuthor}, thank you for your contribution!\\n\\n` +\n                    `Task issues are restricted to Open Home Foundation staff, authorized contributors, and integration code owners.\\n\\n` +\n                    `If you would like to:\\n` +\n                    `- Report a bug: Please use the [bug report form](https://github.com/home-assistant/core/issues/new?template=bug_report.yml)\\n` +\n                    `- Request a feature: Please submit to [Feature Requests](https://github.com/orgs/home-assistant/discussions)\\n\\n` +\n                    `If you believe you should have access to create Task issues, please contact the maintainers.`\n            });\n\n            await github.rest.issues.update({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              issue_number: context.issue.number,\n              state: 'closed'\n            });\n\n            // Add a label to indicate this was auto-closed\n            await github.rest.issues.addLabels({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              issue_number: context.issue.number,\n              labels: ['auto-closed']\n            });\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".github/workflows/restrict-task-creation.yml", "language": "yaml", "complexity": "medium", "star_count": 80370, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 85}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Translations\n\n# yamllint disable-line rule:truthy\non:\n  workflow_dispatch:\n  push:\n    branches:\n      - dev\n    paths:\n      - \"**strings.json\"\n\nenv:\n  DEFAULT_PYTHON: \"3.13\"\n\njobs:\n  upload:\n    name: Upload\n    if: github.repository_owner == 'home-assistant'\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout the repository\n        uses: actions/checkout@v4.2.2\n\n      - name: Set up Python ${{ env.DEFAULT_PYTHON }}\n        uses: actions/setup-python@v5.6.0\n        with:\n          python-version: ${{ env.DEFAULT_PYTHON }}\n\n      - name: Upload Translations\n        run: |\n          export LOKALISE_TOKEN=\"${{ secrets.LOKALISE_TOKEN }}\"\n          python3 -m script.translations upload\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".github/workflows/translations.yml", "language": "yaml", "complexity": "simple", "star_count": 80370, "quality_score": 0.8, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Build wheels\n\n# yamllint disable-line rule:truthy\non:\n  workflow_dispatch:\n  schedule:\n    - cron: \"0 4 * * *\"\n  push:\n    branches:\n      - dev\n      - rc\n    paths:\n      - \".github/workflows/wheels.yml\"\n      - \"homeassistant/package_constraints.txt\"\n      - \"requirements_all.txt\"\n      - \"requirements.txt\"\n      - \"script/gen_requirements_all.py\"\n\nenv:\n  DEFAULT_PYTHON: \"3.13\"\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref_name}}\n  cancel-in-progress: true\n\njobs:\n  init:\n    name: Initialize wheels builder\n    if: github.repository_owner == 'home-assistant'\n    runs-on: ubuntu-latest\n    outputs:\n      architectures: ${{ steps.info.outputs.architectures }}\n    steps:\n      - name: Checkout the repository\n        uses: actions/checkout@v4.2.2\n\n      - name: Set up Python ${{ env.DEFAULT_PYTHON }}\n        id: python\n        uses: actions/setup-python@v5.6.0\n        with:\n          python-version: ${{ env.DEFAULT_PYTHON }}\n          check-latest: true\n\n      - name: Create Python virtual environment\n        run: |\n          python -m venv venv\n          . venv/bin/activate\n          python --version\n          pip install \"$(grep '^uv' < requirements.txt)\"\n          uv pip install -r requirements.txt\n\n      - name: Get information\n        id: info\n        uses: home-assistant/actions/helpers/info@master\n\n      - name: Create requirements_diff file\n        run: |\n          if [[ ${{ github.event_name }} =~ (schedule|workflow_dispatch) ]]; then\n            touch requirements_diff.txt\n          else\n            curl -s -o requirements_diff.txt https://raw.githubusercontent.com/home-assistant/core/master/requirements.txt\n          fi\n\n      - name: Write env-file\n        run: |\n          (\n            echo \"GRPC_PYTHON_BUILD_SYSTEM_OPENSSL=true\"\n            echo \"GRPC_PYTHON_BUILD_WITH_CYTHON=true\"\n\n            # Fix out of memory issues with rust\n            echo \"CARGO_NET_GIT_FETCH_WITH_CLI=true\"\n\n            # OpenCV headless installation\n            echo \"CI_BUILD=1\"\n            echo \"ENABLE_HEADLESS=1\"\n\n            # Use C-Extension for SQLAlchemy\n            echo \"REQUIRE_SQLALCHEMY_CEXT=1\"\n\n            # Add additional pip wheel build constraints\n            echo \"PIP_CONSTRAINT=build_constraints.txt\"\n          ) > .env_file\n\n      - name: Write pip wheel build constraints\n        run: |\n          (\n            # ninja 1.11.1.2 + 1.11.1.3 seem to be broken on at least armhf\n            # this caused the numpy builds to fail\n            # https://github.com/scikit-build/ninja-python-distributions/issues/274\n            echo \"ninja==1.11.1.1\"\n          ) > build_constraints.txt\n\n      - name: Upload env_file\n        uses: actions/upload-artifact@v4.6.2\n        with:\n          name: env_file\n          path: ./.env_file\n          include-hidden-files: true\n          overwrite: true\n\n      - name: Upload build_constraints\n        uses: actions/upload-artifact@v4.6.2\n        with:\n          name: build_constraints\n          path: ./build_constraints.txt\n          overwrite: true\n\n      - name: Upload requirements_diff\n        uses: actions/upload-artifact@v4.6.2\n        with:\n          name: requirements_diff\n          path: ./requirements_diff.txt\n          overwrite: true\n\n      - name: Generate requirements\n        run: |\n          . venv/bin/activate\n          python -m script.gen_requirements_all ci\n\n      - name: Upload requirements_all_wheels\n        uses: actions/upload-artifact@v4.6.2\n        with:\n          name: requirements_all_wheels\n          path: ./requirements_all_wheels_*.txt\n\n  core:\n    name: Build Core wheels ${{ matrix.abi }} for ${{ matrix.arch }} (musllinux_1_2)\n    if: github.repository_owner == 'home-assistant'\n    needs: init\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        abi: [\"cp313\"]\n        arch: ${{ fromJson(needs.init.outputs.architectures) }}\n    steps:\n      - name: Checkout the repository\n        uses: actions/checkout@v4.2.2\n\n      - name: Download env_file\n        uses: actions/download-artifact@v4.3.0\n        with:\n          name: env_file\n\n      - name: Download build_constraints\n        uses: actions/download-artifact@v4.3.0\n        with:\n          name: build_constraints\n\n      - name: Download requirements_diff\n        uses: actions/download-artifact@v4.3.0\n        with:\n          name: requirements_diff\n\n      - name: Adjust build env\n        run: |\n          # Don't build wheels for uv as uv requires a greater version of rust as currently available on alpine\n          sed -i \"/uv/d\" requirements.txt\n          sed -i \"/uv/d\" requirements_diff.txt\n\n      - name: Build wheels\n        uses: home-assistant/wheels@2025.03.0\n        with:\n          abi: ${{ matrix.abi }}\n          tag: musllinux_1_2\n          arch: ${{ matrix.arch }}\n          wheels-key: ${{ secrets.WHEELS_KEY }}\n          env-file: true\n          apk: \"libffi-dev;openssl-dev;yaml-dev;nasm;zlib-ng-dev\"\n          skip-binary: aiohttp;multidict;propcache;yarl;SQLAlchemy\n          constraints: \"homeassistant/package_constraints.txt\"\n          requirements-diff: \"requirements_diff.txt\"\n          requirements: \"requirements.txt\"\n\n  integrations:\n    name: Build wheels ${{ matrix.abi }} for ${{ matrix.arch }}\n    if: github.repository_owner == 'home-assistant'\n    needs: init\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        abi: [\"cp313\"]\n        arch: ${{ fromJson(needs.init.outputs.architectures) }}\n    steps:\n      - name: Checkout the repository\n        uses: actions/checkout@v4.2.2\n\n      - name: Download env_file\n        uses: actions/download-artifact@v4.3.0\n        with:\n          name: env_file\n\n      - name: Download build_constraints\n        uses: actions/download-artifact@v4.3.0\n        with:\n          name: build_constraints\n\n      - name: Download requirements_diff\n        uses: actions/download-artifact@v4.3.0\n        with:\n          name: requirements_diff\n\n      - name: Download requirements_all_wheels\n        uses: actions/download-artifact@v4.3.0\n        with:\n          name: requirements_all_wheels\n\n      - name: Adjust build env\n        run: |\n          if [ \"${{ matrix.arch }}\" = \"i386\" ]; then\n            echo \"NPY_DISABLE_SVML=1\" >> .env_file\n          fi\n\n          # Do not pin numpy in wheels building\n          sed -i \"/numpy/d\" homeassistant/package_constraints.txt\n          # Don't build wheels for uv as uv requires a greater version of rust as currently available on alpine\n          sed -i \"/uv/d\" requirements.txt\n          sed -i \"/uv/d\" requirements_diff.txt\n\n      - name: Build wheels\n        uses: home-assistant/wheels@2025.03.0\n        with:\n          abi: ${{ matrix.abi }}\n          tag: musllinux_1_2\n          arch: ${{ matrix.arch }}\n          wheels-key: ${{ secrets.WHEELS_KEY }}\n          env-file: true\n          apk: \"bluez-dev;libffi-dev;openssl-dev;glib-dev;eudev-dev;libxml2-dev;libxslt-dev;libpng-dev;libjpeg-turbo-dev;tiff-dev;cups-dev;gmp-dev;mpfr-dev;mpc1-dev;ffmpeg-dev;gammu-dev;yaml-dev;openblas-dev;fftw-dev;lapack-dev;gfortran;blas-dev;eigen-dev;freetype-dev;glew-dev;harfbuzz-dev;hdf5-dev;libdc1394-dev;libtbb-dev;mesa-dev;openexr-dev;openjpeg-dev;uchardet-dev;nasm;zlib-ng-dev\"\n          skip-binary: aiohttp;charset-normalizer;grpcio;multidict;SQLAlchemy;propcache;protobuf;pymicro-vad;yarl\n          constraints: \"homeassistant/package_constraints.txt\"\n          requirements-diff: \"requirements_diff.txt\"\n          requirements: \"requirements_all.txt\"\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".github/workflows/wheels.yml", "language": "yaml", "complexity": "complex", "star_count": 80370, "quality_score": 0.8, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 234}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "repos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.12.1\n    hooks:\n      - id: ruff-check\n        args:\n          - --fix\n      - id: ruff-format\n        files: ^((homeassistant|pylint|script|tests)/.+)?[^/]+\\.(py|pyi)$\n  - repo: https://github.com/codespell-project/codespell\n    rev: v2.4.1\n    hooks:\n      - id: codespell\n        args:\n          - --ignore-words-list=aiport,astroid,checkin,currenty,hass,iif,incomfort,lookin,nam,NotIn\n          - --skip=\"./.*,*.csv,*.json,*.ambr\"\n          - --quiet-level=2\n        exclude_types: [csv, json, html]\n        exclude: ^tests/fixtures/|homeassistant/generated/|tests/components/.*/snapshots/\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n      - id: check-executables-have-shebangs\n        stages: [manual]\n      - id: check-json\n        exclude: (.vscode|.devcontainer)\n      - id: no-commit-to-branch\n        args:\n          - --branch=dev\n          - --branch=master\n          - --branch=rc\n  - repo: https://github.com/adrienverge/yamllint.git\n    rev: v1.37.1\n    hooks:\n      - id: yamllint\n  - repo: https://github.com/pre-commit/mirrors-prettier\n    rev: v3.0.3\n    hooks:\n      - id: prettier\n  - repo: https://github.com/cdce8p/python-typing-update\n    rev: v0.6.0\n    hooks:\n      # Run `python-typing-update` hook manually from time to time\n      # to update python typing syntax.\n      # Will require manual work, before submitting changes!\n      # pre-commit run --hook-stage manual python-typing-update --all-files\n      - id: python-typing-update\n        stages: [manual]\n        args:\n          - --py311-plus\n          - --force\n          - --keep-updates\n        files: ^(homeassistant|tests|script)/.+\\.py$\n  - repo: local\n    hooks:\n      # Run mypy through our wrapper script in order to get the possible\n      # pyenv and/or virtualenv activated; it may not have been e.g. if\n      # committing from a GUI tool that was not launched from an activated\n      # shell.\n      - id: mypy\n        name: mypy\n        entry: script/run-in-env.sh mypy\n        language: script\n        require_serial: true\n        types_or: [python, pyi]\n        files: ^(homeassistant|pylint)/.+\\.(py|pyi)$\n      - id: pylint\n        name: pylint\n        entry: script/run-in-env.sh pylint --ignore-missing-annotations=y\n        language: script\n        require_serial: true\n        types_or: [python, pyi]\n        files: ^(homeassistant|tests)/.+\\.(py|pyi)$\n      - id: gen_requirements_all\n        name: gen_requirements_all\n        entry: script/run-in-env.sh python3 -m script.gen_requirements_all\n        pass_filenames: false\n        language: script\n        types: [text]\n        files: ^(homeassistant/.+/manifest\\.json|homeassistant/brands/.+\\.json|pyproject\\.toml|\\.pre-commit-config\\.yaml|script/gen_requirements_all\\.py)$\n      - id: hassfest\n        name: hassfest\n        entry: script/run-in-env.sh python3 -m script.hassfest\n        pass_filenames: false\n        language: script\n        types: [text]\n        files: ^(homeassistant/.+/(icons|manifest|strings)\\.json|homeassistant/.+/(quality_scale)\\.yaml|homeassistant/brands/.*\\.json|homeassistant/.+/services\\.yaml|script/hassfest/(?!metadata|mypy_config).+\\.py|requirements.+\\.txt)$\n      - id: hassfest-metadata\n        name: hassfest-metadata\n        entry: script/run-in-env.sh python3 -m script.hassfest -p metadata,docker\n        pass_filenames: false\n        language: script\n        types: [text]\n        files: ^(script/hassfest/metadata\\.py|homeassistant/const\\.py$|pyproject\\.toml|homeassistant/components/go2rtc/const\\.py)$\n      - id: hassfest-mypy-config\n        name: hassfest-mypy-config\n        entry: script/run-in-env.sh python3 -m script.hassfest -p mypy_config\n        pass_filenames: false\n        language: script\n        types: [text]\n        files: ^(script/hassfest/mypy_config\\.py|\\.strict-typing|mypy\\.ini)$\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": ".pre-commit-config.yaml", "language": "yaml", "complexity": "medium", "star_count": 80370, "quality_score": 0.89, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 102}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\nplatform=\"${args[0]}\"\nversion=\"${args[1]}\"\npyver=default\n\n# check for explicit python version like 8.3@3.8\ndeclare -a splitversion\nIFS='@' read -ra splitversion <<< \"$version\"\n\nif [ \"${#splitversion[@]}\" -gt 1 ]; then\n    version=\"${splitversion[0]}\"\n    pyver=\"${splitversion[1]}\"\nfi\n\nif [ \"${#args[@]}\" -gt 2 ]; then\n    target=\"shippable/posix/group${args[2]}/\"\nelse\n    target=\"shippable/posix/\"\nfi\n\nstage=\"${S:-prod}\"\nprovider=\"${P:-default}\"\n\n# shellcheck disable=SC2086\nansible-test integration --color -v --retry-on-error \"${target}\" ${COVERAGE:+\"$COVERAGE\"} ${CHANGED:+\"$CHANGED\"} ${UNSTABLE:+\"$UNSTABLE\"} \\\n    --python \"${pyver}\" --remote \"${platform}/${version}\" --remote-terminate always --remote-stage \"${stage}\" --remote-provider \"${provider}\" \\\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/alpine.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\ncloud=\"${args[0]}\"\npython=\"${args[1]}\"\ngroup=\"${args[2]}\"\n\ntarget=\"shippable/${cloud}/group${group}/\"\n\nstage=\"${S:-prod}\"\n\nchanged_all_target=\"shippable/${cloud}/smoketest/\"\n\nif ! ansible-test integration \"${changed_all_target}\" --list-targets > /dev/null 2>&1; then\n    # no smoketest tests are available for this cloud\n    changed_all_target=\"none\"\nfi\n\nif [ \"${group}\" == \"1\" ]; then\n    # only run smoketest tests for group1\n    changed_all_mode=\"include\"\nelse\n    # smoketest tests already covered by group1\n    changed_all_mode=\"exclude\"\nfi\n\n# shellcheck disable=SC2086\nansible-test integration --color -v --retry-on-error \"${target}\" ${COVERAGE:+\"$COVERAGE\"} ${CHANGED:+\"$CHANGED\"} ${UNSTABLE:+\"$UNSTABLE\"} \\\n    --remote-terminate always --remote-stage \"${stage}\" \\\n    --docker --python \"${python}\" --changed-all-target \"${changed_all_target}\" --changed-all-mode \"${changed_all_mode}\"\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/cloud.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 35}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\nscript=\"${args[0]}\"\n\ntest=\"$1\"\n\ndocker images\ndocker ps\n\nfor container in $(docker ps --format '{{.Image}} {{.ID}}' | grep -v -e '^quay.io/ansible/azure-pipelines-test-container:' | sed 's/^.* //'); do\n    docker rm -f \"${container}\" || true  # ignore errors\ndone\n\ndocker ps\n\nexport PATH=\"${PWD}/bin:${PATH}\"\nexport PYTHONIOENCODING='utf-8'\n\nif [ -n \"${COVERAGE:-}\" ]; then\n    # on-demand coverage reporting triggered by setting the COVERAGE environment variable to a non-empty value\n    export COVERAGE=\"--coverage\"\nelif [[ \"${COMMIT_MESSAGE}\" =~ ci_coverage ]]; then\n    # on-demand coverage reporting triggered by having 'ci_coverage' in the latest commit message\n    export COVERAGE=\"--coverage\"\nelse\n    # on-demand coverage reporting disabled (default behavior, always-on coverage reporting remains enabled)\n    export COVERAGE=\"--coverage-check\"\nfi\n\nif [ -n \"${COMPLETE:-}\" ]; then\n    # disable change detection triggered by setting the COMPLETE environment variable to a non-empty value\n    export CHANGED=\"\"\nelif [[ \"${COMMIT_MESSAGE}\" =~ ci_complete ]]; then\n    # disable change detection triggered by having 'ci_complete' in the latest commit message\n    export CHANGED=\"\"\nelse\n    # enable change detection (default behavior)\n    export CHANGED=\"--changed\"\nfi\n\nif [ \"${IS_PULL_REQUEST:-}\" == \"true\" ]; then\n    # run unstable tests which are targeted by focused changes on PRs\n    export UNSTABLE=\"--allow-unstable-changed\"\nelse\n    # do not run unstable tests outside PRs\n    export UNSTABLE=\"\"\nfi\n\nif [[ \"${COVERAGE:-}\" == \"--coverage\" ]]; then\n    timeout=60\nelse\n    timeout=50\nfi\n\nansible-test env --dump --show --timeout \"${timeout}\" --color -v\n\n\".azure-pipelines/commands/${script}.sh\" \"${test}\"\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/entry-point.sh", "language": "bash", "complexity": "medium", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 63}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\nplatform=\"${args[0]}\"\nversion=\"${args[1]}\"\npyver=default\n\n# check for explicit python version like 8.3@3.8\ndeclare -a splitversion\nIFS='@' read -ra splitversion <<< \"$version\"\n\nif [ \"${#splitversion[@]}\" -gt 1 ]; then\n    version=\"${splitversion[0]}\"\n    pyver=\"${splitversion[1]}\"\nfi\n\nif [ \"${#args[@]}\" -gt 2 ]; then\n    target=\"shippable/posix/group${args[2]}/\"\nelse\n    target=\"shippable/posix/\"\nfi\n\nstage=\"${S:-prod}\"\nprovider=\"${P:-default}\"\n\n# shellcheck disable=SC2086\nansible-test integration --color -v --retry-on-error \"${target}\" ${COVERAGE:+\"$COVERAGE\"} ${CHANGED:+\"$CHANGED\"} ${UNSTABLE:+\"$UNSTABLE\"} \\\n    --python \"${pyver}\" --remote \"${platform}/${version}\" --remote-terminate always --remote-stage \"${stage}\" --remote-provider \"${provider}\" \\\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/fedora.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\nplatform=\"${args[0]}\"\nversion=\"${args[1]}\"\npyver=default\n\n# check for explicit python version like 8.3@3.8\ndeclare -a splitversion\nIFS='@' read -ra splitversion <<< \"$version\"\n\nif [ \"${#splitversion[@]}\" -gt 1 ]; then\n    version=\"${splitversion[0]}\"\n    pyver=\"${splitversion[1]}\"\nfi\n\nif [ \"${#args[@]}\" -gt 2 ]; then\n    target=\"shippable/posix/group${args[2]}/\"\nelse\n    target=\"shippable/posix/\"\nfi\n\nstage=\"${S:-prod}\"\nprovider=\"${P:-default}\"\n\n# shellcheck disable=SC2086\nansible-test integration --color -v --retry-on-error \"${target}\" ${COVERAGE:+\"$COVERAGE\"} ${CHANGED:+\"$CHANGED\"} ${UNSTABLE:+\"$UNSTABLE\"} \\\n    --python \"${pyver}\" --remote \"${platform}/${version}\" --remote-terminate always --remote-stage \"${stage}\" --remote-provider \"${provider}\" \\\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/freebsd.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\ncloud=\"${args[0]}\"\npython=\"${args[1]}\"\ngroup=\"${args[2]}\"\n\ntarget=\"shippable/${cloud}/group${group}/\"\n\nstage=\"${S:-prod}\"\n\nchanged_all_target=\"shippable/${cloud}/smoketest/\"\n\nif ! ansible-test integration \"${changed_all_target}\" --list-targets > /dev/null 2>&1; then\n    # no smoketest tests are available for this cloud\n    changed_all_target=\"none\"\nfi\n\nif [ \"${group}\" == \"1\" ]; then\n    # only run smoketest tests for group1\n    changed_all_mode=\"include\"\nelse\n    # smoketest tests already covered by group1\n    changed_all_mode=\"exclude\"\nfi\n\n# shellcheck disable=SC2086\nansible-test integration --color -v --retry-on-error \"${target}\" ${COVERAGE:+\"$COVERAGE\"} ${CHANGED:+\"$CHANGED\"} ${UNSTABLE:+\"$UNSTABLE\"} \\\n    --remote-terminate always --remote-stage \"${stage}\" \\\n    --docker --python \"${python}\" --changed-all-target \"${changed_all_target}\" --changed-all-mode \"${changed_all_mode}\"\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/galaxy.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 35}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\npython=\"${args[1]}\"\n\nif [ \"${#args[@]}\" -gt 2 ]; then\n    target=\"shippable/generic/group${args[2]}/\"\nelse\n    target=\"shippable/generic/\"\nfi\n\n# shellcheck disable=SC2086\nansible-test integration --color -v --retry-on-error \"${target}\" ${COVERAGE:+\"$COVERAGE\"} ${CHANGED:+\"$CHANGED\"} ${UNSTABLE:+\"$UNSTABLE\"} \\\n    --docker default --python \"${python}\"\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/generic.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n# Temporary script to support side-by-side testing for incidental code coverage.\n# Once collection migration has been completed the incidental tests can be combined with the regular tests.\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\nscript=\"${args[1]}\"\n\nIFS='/' test=\"${args[*]:1}\"\n\n\".azure-pipelines/commands/incidental/${script}.sh\" \"${test}\"\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/i.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\nversion=\"${args[1]}\"\nconnection=\"${args[2]}\"\nconnection_setting=\"${args[3]}\"\n\ntarget=\"shippable/windows/incidental/\"\n\nstage=\"${S:-prod}\"\nprovider=\"${P:-default}\"\n\n# python version to run full tests on while other versions run minimal tests\npython_default=\"$(PYTHONPATH=\"${PWD}/test/lib\" python -c 'from ansible_test._internal import constants; print(constants.CONTROLLER_MIN_PYTHON_VERSION)')\"\n\n# version to test when only testing a single version\nsingle_version=2022\n\n# shellcheck disable=SC2086\nansible-test windows-integration --list-targets -v ${CHANGED:+\"$CHANGED\"} ${UNSTABLE:+\"$UNSTABLE\"} > /tmp/explain.txt 2>&1 || { cat /tmp/explain.txt && false; }\n{ grep ' windows-integration: .* (targeted)$' /tmp/explain.txt || true; } > /tmp/windows.txt\n\nif [ -s /tmp/windows.txt ] || [ \"${CHANGED:+$CHANGED}\" == \"\" ]; then\n    echo \"Detected changes requiring integration tests specific to Windows:\"\n    cat /tmp/windows.txt\n\n    echo \"Running Windows integration tests for the version ${version}.\"\nelse\n    echo \"No changes requiring integration tests specific to Windows were detected.\"\n    echo \"Running Windows integration tests for a single version only: ${single_version}\"\n\n    if [ \"${version}\" != \"${single_version}\" ]; then\n        echo \"Skipping this job since it is for: ${version}\"\n        exit 0\n    fi\nfi\n\n# shellcheck disable=SC2086\nansible-test windows-integration --color -v --retry-on-error \"${target}\" ${COVERAGE:+\"$COVERAGE\"} ${CHANGED:+\"$CHANGED\"} ${UNSTABLE:+\"$UNSTABLE\"} \\\n    --controller \"docker:default,python=${python_default}\" \\\n    --target \"remote:windows/${version},connection=${connection}+${connection_setting},provider=${provider}\" \\\n    --remote-terminate always --remote-stage \"${stage}\"\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/incidental/windows.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8600000000000001, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 47}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\nimage=\"${args[1]}\"\n\nif [ \"${#args[@]}\" -gt 2 ]; then\n    target=\"shippable/posix/group${args[2]}/\"\nelse\n    target=\"shippable/posix/\"\nfi\n\n# shellcheck disable=SC2086\nansible-test integration --color -v --retry-on-error \"${target}\" ${COVERAGE:+\"$COVERAGE\"} ${CHANGED:+\"$CHANGED\"} ${UNSTABLE:+\"$UNSTABLE\"} \\\n    --docker \"${image}\"\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/linux.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\nplatform=\"${args[0]}\"\nversion=\"${args[1]}\"\npyver=default\n\n# check for explicit python version like 8.3@3.8\ndeclare -a splitversion\nIFS='@' read -ra splitversion <<< \"$version\"\n\nif [ \"${#splitversion[@]}\" -gt 1 ]; then\n    version=\"${splitversion[0]}\"\n    pyver=\"${splitversion[1]}\"\nfi\n\nif [ \"${#args[@]}\" -gt 2 ]; then\n    target=\"shippable/posix/group${args[2]}/\"\nelse\n    target=\"shippable/posix/\"\nfi\n\nstage=\"${S:-prod}\"\nprovider=\"${P:-default}\"\n\n# shellcheck disable=SC2086\nansible-test integration --color -v --retry-on-error \"${target}\" ${COVERAGE:+\"$COVERAGE\"} ${CHANGED:+\"$CHANGED\"} ${UNSTABLE:+\"$UNSTABLE\"} \\\n    --python \"${pyver}\" --remote \"${platform}/${version}\" --remote-terminate always --remote-stage \"${stage}\" --remote-provider \"${provider}\" \\\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/macos.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\nplatform=\"${args[0]}\"\nversion=\"${args[1]}\"\npyver=default\n\n# check for explicit python version like 8.3@3.8\ndeclare -a splitversion\nIFS='@' read -ra splitversion <<< \"$version\"\n\nif [ \"${#splitversion[@]}\" -gt 1 ]; then\n    version=\"${splitversion[0]}\"\n    pyver=\"${splitversion[1]}\"\nfi\n\nif [ \"${#args[@]}\" -gt 2 ]; then\n    target=\"shippable/posix/group${args[2]}/\"\nelse\n    target=\"shippable/posix/\"\nfi\n\nstage=\"${S:-prod}\"\nprovider=\"${P:-default}\"\n\n# shellcheck disable=SC2086\nansible-test integration --color -v --retry-on-error \"${target}\" ${COVERAGE:+\"$COVERAGE\"} ${CHANGED:+\"$CHANGED\"} ${UNSTABLE:+\"$UNSTABLE\"} \\\n    --python \"${pyver}\" --remote \"${platform}/${version}\" --remote-terminate always --remote-stage \"${stage}\" --remote-provider \"${provider}\" \\\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/remote.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\nplatform=\"${args[0]}\"\nversion=\"${args[1]}\"\npyver=default\n\n# check for explicit python version like 8.3@3.8\ndeclare -a splitversion\nIFS='@' read -ra splitversion <<< \"$version\"\n\nif [ \"${#splitversion[@]}\" -gt 1 ]; then\n    version=\"${splitversion[0]}\"\n    pyver=\"${splitversion[1]}\"\nfi\n\nif [ \"${#args[@]}\" -gt 2 ]; then\n    target=\"shippable/posix/group${args[2]}/\"\nelse\n    target=\"shippable/posix/\"\nfi\n\nstage=\"${S:-prod}\"\nprovider=\"${P:-default}\"\n\n# shellcheck disable=SC2086\nansible-test integration --color -v --retry-on-error \"${target}\" ${COVERAGE:+\"$COVERAGE\"} ${CHANGED:+\"$CHANGED\"} ${UNSTABLE:+\"$UNSTABLE\"} \\\n    --python \"${pyver}\" --remote \"${platform}/${version}\" --remote-terminate always --remote-stage \"${stage}\" --remote-provider \"${provider}\" \\\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/rhel.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\ngroup=\"${args[1]}\"\n\ngroup2_include=(\n    ansible-doc\n    changelog\n    package-data\n    pep8\n    pylint\n    validate-modules\n)\n\ngroup1_exclude=(\n    \"${group2_include[@]}\"\n)\n\noptions=()\n\ncase \"${group}\" in\n    1)\n        for name in \"${group1_exclude[@]}\"; do\n            options+=(--skip-test \"${name}\")\n        done\n        ;;\n    2)\n        for name in \"${group2_include[@]}\"; do\n            options+=(--test \"${name}\")\n        done\n        ;;\nesac\n\n# shellcheck disable=SC2086\nansible-test sanity --color -v --junit ${COVERAGE:+\"$COVERAGE\"} ${CHANGED:+\"$CHANGED\"} \\\n    --docker \\\n    \"${options[@]}\" --allow-disabled\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/sanity.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 42}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env bash\n\nset -o pipefail -eux\n\ndeclare -a args\nIFS='/:' read -ra args <<< \"$1\"\n\nplatform=\"${args[0]}\"\nversion=\"${args[1]}\"\npyver=default\n\n# check for explicit python version like 8.3@3.8\ndeclare -a splitversion\nIFS='@' read -ra splitversion <<< \"$version\"\n\nif [ \"${#splitversion[@]}\" -gt 1 ]; then\n    version=\"${splitversion[0]}\"\n    pyver=\"${splitversion[1]}\"\nfi\n\nif [ \"${#args[@]}\" -gt 2 ]; then\n    target=\"shippable/posix/group${args[2]}/\"\nelse\n    target=\"shippable/posix/\"\nfi\n\nstage=\"${S:-prod}\"\nprovider=\"${P:-default}\"\n\n# shellcheck disable=SC2086\nansible-test integration --color -v --retry-on-error \"${target}\" ${COVERAGE:+\"$COVERAGE\"} ${CHANGED:+\"$CHANGED\"} ${UNSTABLE:+\"$UNSTABLE\"} \\\n    --python \"${pyver}\" --remote \"${platform}/${version}\" --remote-terminate always --remote-stage \"${stage}\" --remote-provider \"${provider}\" \\\n", "metadata": {"source": "github.com/ansible/ansible", "file_path": ".azure-pipelines/commands/ubuntu.sh", "language": "bash", "complexity": "simple", "star_count": 65718, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": true, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management using GitHub Actions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# https://cwiki.apache.org/confluence/display/INFRA/git+-+.asf.yaml+features\n---\ngithub:\n  description: \"Apache Airflow - A platform to programmatically author, schedule, and monitor workflows\"\n  homepage: https://airflow.apache.org/\n  # Social media preview image is not supported by GitHub API/asf.yaml, need to be uploaded\n  # manually in GitHub repository --> Settings --> click \"Edit\" in \"Social preview\"\n  # See also:\n  # https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/customizing-your-repositorys-social-media-preview\n  # social_media_preview: docs/apache-airflow/img/logos/github_repository_social_image.png\n  labels:\n    # Note that GitHub only supports <=20 labels/topics per repo! Pipeline will fail if you add more.\n    - airflow\n    - apache\n    - apache-airflow\n    - automation\n    - dag\n    - data-engineering\n    - data-integration\n    - data-orchestrator\n    - data-pipelines\n    - data-science\n    - elt\n    - etl\n    - machine-learning\n    - mlops\n    - orchestration\n    - python\n    - scheduler\n    - workflow\n    - workflow-engine\n    - workflow-orchestration\n  features:\n    # Enable issues management\n    issues: true\n    # Enable projects for project management boards\n    projects: true\n    # Enable wiki for documentation\n    wiki: false\n    # Enable discussions\n    discussions: true\n\n  enabled_merge_buttons:\n    squash: true\n    merge: false\n    rebase: false\n\n  pull_requests:\n    # allow auto-merge\n    allow_auto_merge: false\n    # auto-delete head branches after being merged\n    del_branch_on_merge: true\n\n  protected_branches:\n    main:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_conversation_resolution: true\n      required_signatures: false\n    v1-10-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_signatures: false\n    v2-0-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-1-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-2-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-3-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-4-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-5-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-6-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-7-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-8-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-9-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-10-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-11-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v3-0-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    providers-fab/v1-5:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_conversation_resolution: true\n      required_signatures: false\n  collaborators:\n    # Max 10 collaborators allowed\n    # https://github.com/apache/infrastructure-asfyaml/blob/main/README.md#assigning-the-github-triage-role-to-external-collaborators\n    # Procedure for adding new member is listed in\n    # https://github.com/apache/airflow/blob/main/ISSUE_TRIAGE_PROCESS.rst#L97\n    - aritra24\n    - omkar-foss\n    - nathadfield\n    - sunank200\n    - cmarteepants\n    - karenbraganz\n    - gyli\n    - jroachgolf84\n\nnotifications:\n  jobs: jobs@airflow.apache.org\n  commits: commits@airflow.apache.org\n  issues: commits@airflow.apache.org\n  pullrequests: commits@airflow.apache.org\n  discussions: commits@airflow.apache.org\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".asf.yaml", "language": "yaml", "complexity": "complex", "star_count": 41241, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": true, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 173}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Airflow Bug report\ndescription: Problems and issues with code in Apache Airflow core\nlabels: [\"kind:bug\", \"area:core\", \"needs-triage\"]\nbody:\n  - type: markdown\n    attributes:\n      # yamllint disable rule:line-length\n      value: \"\n        <img src='https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/logos/airflow_64x64_emoji_transparent.png' align='left' width='80' height='80'>\n        Thank you for finding the time to report the problem!\n\n        We really appreciate the community's efforts to improve Airflow.\n\n        Note, you do not need to create an issue if you have a change ready to submit!\n\n        You can open a [pull request](https://github.com/apache/airflow/pulls) immediately instead.\n        <br clear='left'/>\"\n      # yamllint enable rule:line-length\n  - type: dropdown\n    attributes:\n      label: Apache Airflow version\n      description: >\n        What Apache Airflow version are you using? If you do not see your version, please (ideally) test on\n        the latest release or main to see if the issue is fixed before reporting it.\n      multiple: false\n      options:\n        - \"3.0.3\"\n        - \"2.11.0\"\n        - \"main (development)\"\n        - \"Other Airflow 2 version (please specify below)\"\n    validations:\n      required: true\n  - type: input\n    attributes:\n      label: If \"Other Airflow 2 version\" selected, which one?\n      # yamllint disable rule:line-length\n      description: >\n        On what 2.X version of Airflow are you currently experiencing the issue? Remember, you are encouraged to\n        test with the latest release or on the main branch to verify your issue still exists, especially if\n        your version is at least a minor version older than the [current stable release](https://airflow.apache.org/docs/apache-airflow/stable/installation/supported-versions.html#version-life-cycle).\n      # yamllint enable rule:line-length\n  - type: textarea\n    attributes:\n      label: What happened?\n      description: Describe what happened.\n      placeholder: >\n        Please provide the context in which the problem occurred and explain what happened\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: What you think should happen instead?\n      description: What do you think went wrong?\n      placeholder: >\n        Please explain why you think the behaviour is erroneous. It is extremely helpful if you copy&paste\n        the fragment of logs showing the exact error messages or wrong behaviour and screenshots for\n        UI problems or YouTube link to a video of you demonstrating the problem. You can include files by\n        dragging and dropping them here.\n  - type: textarea\n    attributes:\n      label: How to reproduce\n      description: >\n        What should we do to reproduce the problem? If you are not able to provide a reproducible case,\n        please open a [discussion](https://github.com/apache/airflow/discussions) instead.\n      placeholder: >\n        Please make sure you provide a reproducible step-by-step case of how to reproduce the problem\n        as minimally and precisely as possible. Keep in mind we do not have access to your cluster or DAGs.\n        Remember that non-reproducible issues will be closed! Opening a discussion is recommended as a\n        first step.\n    validations:\n      required: true\n  - type: input\n    attributes:\n      label: Operating System\n      description: What Operating System are you using?\n      placeholder: \"You can get it via `cat /etc/os-release` for example\"\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Versions of Apache Airflow Providers\n      description: What Apache Airflow Providers versions are you using?\n      placeholder: You can use `pip freeze | grep apache-airflow-providers` (you can leave only relevant ones)\n  - type: dropdown\n    attributes:\n      label: Deployment\n      description: >\n        What kind of deployment do you have? If you use a Managed Service, consider first using regular\n        channels of reporting issues for the service.\n      multiple: false\n      options:\n        - \"Official Apache Airflow Helm Chart\"\n        - \"Other 3rd-party Helm chart\"\n        - \"Docker-Compose\"\n        - \"Other Docker-based deployment\"\n        - \"Virtualenv installation\"\n        - \"Astronomer\"\n        - \"Google Cloud Composer\"\n        - \"Amazon (AWS) MWAA\"\n        - \"Microsoft ADF Managed Airflow\"\n        - \"Other\"\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Deployment details\n      description: Additional description of your deployment.\n      placeholder: >\n        Enter any relevant details of your deployment. Especially version of your tools,\n        software (docker-compose, helm, k8s, etc.), any customisation and configuration you added.\n  - type: textarea\n    attributes:\n      label: Anything else?\n      description: Anything else we need to know?\n      placeholder: >\n        How often does this problem occur? (Once? Every time? Only when certain conditions are met?)\n        Any relevant logs to include? Put them here inside fenced\n        ``` ``` blocks or inside a foldable details tag if it's long:\n        <details><summary>x.log</summary> lots of stuff </details>\n  - type: checkboxes\n    attributes:\n      label: Are you willing to submit PR?\n      description: >\n        This is absolutely not required, but we are happy to guide you in the contribution process\n        especially if you already have a good understanding of how to implement the fix.\n        Airflow is a community-managed project and we love to bring new contributors in.\n        Find us in #new-contributors on Slack!\n      options:\n        - label: Yes I am willing to submit a PR!\n  - type: checkboxes\n    attributes:\n      label: Code of Conduct\n      description: >\n        The Code of Conduct helps create a safe space for everyone. We require\n        that everyone agrees to it.\n      options:\n        - label: >\n            I agree to follow this project's\n            [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n          required: true\n  - type: markdown\n    attributes:\n      value: \"Thanks for completing our form!\"\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/ISSUE_TEMPLATE/1-airflow_bug_report.yml", "language": "yaml", "complexity": "medium", "star_count": 41241, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 145}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Airflow feature request\ndescription: Suggest an idea for this project\nlabels: [\"kind:feature\", \"needs-triage\"]\nbody:\n  - type: markdown\n    attributes:\n      # yamllint disable rule:line-length\n      value: \"\n        <img src='https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/logos/airflow_64x64_emoji_transparent.png' align='left' width='80' height='80'>\n        Thank you for finding the time to propose new feature!\n\n        We really appreciate the community efforts to improve Airflow.\n\n        Features should be small improvements that do not dramatically change Airflow assumptions.\n        Note, that in this case you do not even need to create an issue if you have a code change ready to submit!\n        You can open [Pull Request](https://github.com/apache/airflow/pulls) immediately instead.\n\n        For bigger features, those that are impacting Airflow's architecture, security assumptions,\n        backwards compatibility etc. should be discussed in the [airflow devlist](https://lists.apache.org/list.html?dev@airflow.apache.org).\n        Such features will need initial discussion - possibly in [discussion](https://github.com/apache/airflow/discussions), followed by\n        [Airflow Improvement Proposal](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals) and formal voting.\n        If you want to introduce such feature, you need to be prepared to lead a discussion, get consensus\n        among the community and eventually conduct a successful\n        [vote](https://www.apache.org/foundation/voting.html) in the community.\n\n        If unsure - open a [discussion](https://github.com/apache/airflow/discussions) first to gather\n        an initial feedback on your idea.\n\n        <br clear='left'/>\"\n\n      # yamllint enable rule:line-length\n  - type: textarea\n    attributes:\n      label: Description\n      description: A short description of your feature\n  - type: textarea\n    attributes:\n      label: Use case/motivation\n      description: What would you like to happen?\n      placeholder: >\n          Rather than telling us how you might implement this feature, try to take a\n          step back and describe what you are trying to achieve.\n  - type: textarea\n    attributes:\n      label: Related issues\n      description: Is there currently another issue associated with this?\n  - type: checkboxes\n    attributes:\n      label: Are you willing to submit a PR?\n      description: >\n        If want to submit a PR you do not need to open feature request, <b>just create the PR!</b>.\n        Especially if you already have a good understanding of how to implement the feature.\n        Airflow is a community-managed project and we love to bring new contributors in.\n        Find us in #new-contributors on Slack!\n        It's optional though - if you have good idea for small feature,\n        others might implement it if they pick an interest in it, so feel free to leave that\n        checkbox unchecked.\n      options:\n        - label: Yes I am willing to submit a PR!\n  - type: checkboxes\n    attributes:\n      label: Code of Conduct\n      description: The Code of Conduct helps create a safe space for everyone. We require\n        that everyone agrees to it.\n      options:\n        - label: >\n            I agree to follow this project's\n            [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n          required: true\n  - type: markdown\n    attributes:\n      value: \"Thanks for completing our form!\"\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/ISSUE_TEMPLATE/2-feature_request.yml", "language": "yaml", "complexity": "simple", "star_count": 41241, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 74}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Airflow Providers Bug report\ndescription: Problems and issues with code in Apache Airflow Providers\nlabels: [\"kind:bug\", \"area:providers\", \"needs-triage\"]\nbody:\n  - type: markdown\n    attributes:\n      # yamllint disable rule:line-length\n      value: \"\n        <img src='https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/logos/airflow_64x64_emoji_transparent.png' align='left' width='80' height='80'>\n        Thank you for finding the time to report a problem!\n\n        We really appreciate the community's efforts to improve Airflow.\n\n        Note, you do not need to create an issue if you have a change ready to submit!\n\n        You can open a [pull request](https://github.com/apache/airflow/pulls) immediately instead.\n        <br clear='left'/>\"\n      # yamllint enable rule:line-length\n  - type: dropdown\n    attributes:\n      label: Apache Airflow Provider(s)\n      description: Provider(s) that the issue report is about (you can choose more than one)\n      multiple: true\n      options:\n        - airbyte\n        - alibaba\n        - amazon\n        - apache-beam\n        - apache-cassandra\n        - apache-drill\n        - apache-druid\n        - apache-flink\n        - apache-hdfs\n        - apache-hive\n        - apache-iceberg\n        - apache-impala\n        - apache-kafka\n        - apache-kylin\n        - apache-livy\n        - apache-pig\n        - apache-pinot\n        - apache-spark\n        - apache-tinkerpop\n        - apprise\n        - arangodb\n        - asana\n        - atlassian-jira\n        - celery\n        - cloudant\n        - cncf-kubernetes\n        - cohere\n        - common-compat\n        - common-io\n        - common-messaging\n        - common-sql\n        - databricks\n        - datadog\n        - dbt-cloud\n        - dingding\n        - discord\n        - docker\n        - edge3\n        - elasticsearch\n        - exasol\n        - fab\n        - facebook\n        - ftp\n        - git\n        - github\n        - google\n        - grpc\n        - hashicorp\n        - http\n        - imap\n        - influxdb\n        - jdbc\n        - jenkins\n        - keycloak\n        - microsoft-azure\n        - microsoft-mssql\n        - microsoft-psrp\n        - microsoft-winrm\n        - mongo\n        - mysql\n        - neo4j\n        - odbc\n        - openai\n        - openfaas\n        - openlineage\n        - opensearch\n        - opsgenie\n        - oracle\n        - pagerduty\n        - papermill\n        - pgvector\n        - pinecone\n        - postgres\n        - presto\n        - qdrant\n        - redis\n        - salesforce\n        - samba\n        - segment\n        - sendgrid\n        - sftp\n        - singularity\n        - slack\n        - smtp\n        - snowflake\n        - sqlite\n        - ssh\n        - standard\n        - tableau\n        - telegram\n        - teradata\n        - trino\n        - vertica\n        - weaviate\n        - yandex\n        - ydb\n        - zendesk\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Versions of Apache Airflow Providers\n      description: What Apache Airflow Providers versions are you using?\n      placeholder: You can use `pip freeze | grep apache-airflow-providers` (you can leave only relevant ones)\n  - type: input\n    attributes:\n      label: Apache Airflow version\n      description: >\n        What Apache Airflow version are you using?\n        [Only Airflow 2 is supported](https://github.com/apache/airflow#version-life-cycle) for bugs.\n    validations:\n      required: true\n  - type: input\n    attributes:\n      label: Operating System\n      description: What Operating System are you using?\n      placeholder: \"You can get it via `cat /etc/os-release` for example\"\n    validations:\n      required: true\n  - type: dropdown\n    attributes:\n      label: Deployment\n      description: >\n        What kind of deployment do you have? If you use a Managed Service, consider first using regular\n        channels of reporting issues for the service.\n      multiple: false\n      options:\n        - \"Official Apache Airflow Helm Chart\"\n        - \"Other 3rd-party Helm chart\"\n        - \"Docker-Compose\"\n        - \"Other Docker-based deployment\"\n        - \"Virtualenv installation\"\n        - \"Astronomer\"\n        - \"Google Cloud Composer\"\n        - \"Amazon (AWS) MWAA\"\n        - \"Microsoft ADF Managed Airflow\"\n        - \"Other\"\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Deployment details\n      description: Additional description of your deployment.\n      placeholder: >\n        Enter any relevant details of your deployment. Especially version of your tools,\n        software (docker-compose, helm, k8s, etc.), any customisation and configuration you added.\n  - type: textarea\n    attributes:\n      label: What happened\n      description: Describe what happened.\n      placeholder: >\n        Please provide the context in which the problem occurred and explain what happened\n  - type: textarea\n    attributes:\n      label: What you think should happen instead\n      description: What do you think went wrong?\n      placeholder: >\n        Please explain why you think the behaviour is erroneous. It is extremely helpful if you copy&paste\n        the fragment of logs showing the exact error messages or wrong behaviour and screenshots for\n        UI problems or YouTube link to a video of you demonstrating the problem. You can include files by\n        dragging and dropping them here.\n  - type: textarea\n    attributes:\n      label: How to reproduce\n      description: >\n        What should we do to reproduce the problem? If you are not able to provide a reproducible case,\n        please open a [Discussion](https://github.com/apache/airflow/discussions) instead.\n      placeholder: >\n        Please make sure you provide a reproducible step-by-step case of how to reproduce the problem\n        as minimally and precisely as possible. Keep in mind we do not have access to your cluster or\n        DAGs. Remember that non-reproducible issues will be closed! Opening a discussion is\n        recommended as a first step.\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Anything else\n      description: Anything else we need to know?\n      placeholder: >\n        How often does this problem occur? (Once? Every time? Only when certain conditions are met?)\n        Any relevant logs to include? Put them here inside fenced\n        ``` ``` blocks or inside a foldable details tag if it's long:\n        <details><summary>x.log</summary> lots of stuff </details>\n  - type: checkboxes\n    attributes:\n      label: Are you willing to submit PR?\n      description: >\n        This is absolutely not required, but we are happy to guide you in the contribution process\n        especially if you already have a good understanding of how to implement the fix.\n        Airflow is a community-managed project and we love to bring new contributors in.\n        Find us in #new-contributors on Slack!\n      options:\n        - label: Yes I am willing to submit a PR!\n  - type: checkboxes\n    attributes:\n      label: Code of Conduct\n      description: >\n        The Code of Conduct helps create a safe space for everyone. We require\n        that everyone agrees to it.\n      options:\n        - label: >\n            I agree to follow this project's\n            [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n          required: true\n  - type: markdown\n    attributes:\n      value: \"Thanks for completing our form!\"\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/ISSUE_TEMPLATE/3-airflow_providers_bug_report.yml", "language": "yaml", "complexity": "complex", "star_count": 41241, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": true, "file_count": 1, "line_count": 233}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Airflow Helm Chart Bug report\ndescription: Problems and issues with the Apache Airflow Official Helm Chart\nlabels: [\"kind:bug\", \"area:helm-chart\", \"needs-triage\"]\nbody:\n  - type: markdown\n    attributes:\n      # yamllint disable rule:line-length\n      value: \"\n        <img src='https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/logos/airflow_64x64_emoji_transparent.png' align='left' width='120' height='120'>\n        Thank you for finding the time to report the problem!\n\n        We really appreciate the community's efforts to improve Airflow.\n\n        Note that this issue is only for the\n        [Official Apache Airflow Helm Chart](https://airflow.apache.org/docs/helm-chart/stable/index.html).\n        If you use another 3rd-party Chart, you should report your issue in the repo of that chart instead.\n\n        Note, you do not need to create an issue if you have a change ready to submit!\n\n        You can open a [pull request](https://github.com/apache/airflow/pulls) immediately instead.\n        <br clear='left'/>\"\n      # yamllint enable rule:line-length\n  - type: dropdown\n    attributes:\n      label: Official Helm Chart version\n      description: >\n        What Apache Airflow Helm Chart version are you using?\n      multiple: false\n      options:\n        - \"1.18.0 (latest released)\"\n        - \"1.17.0\"\n        - \"1.16.0\"\n        - \"1.15.0\"\n        - \"1.14.0\"\n        - \"1.13.1\"\n        - \"1.13.0\"\n        - \"1.12.0\"\n        - \"1.11.0\"\n        - \"1.10.0\"\n        - \"1.9.0\"\n        - \"1.8.0\"\n        - \"1.7.0\"\n        - \"1.6.0\"\n        - \"1.5.0\"\n        - \"1.4.0\"\n        - \"1.3.0\"\n        - \"1.2.0\"\n        - \"1.1.0\"\n        - \"1.0.0\"\n        - \"main (development)\"\n    validations:\n      required: true\n  - type: input\n    attributes:\n      label: Apache Airflow version\n      description: >\n        What Apache Airflow version are you using?\n        [Only Airflow 2 is supported](https://github.com/apache/airflow#version-life-cycle) for bugs.\n    validations:\n      required: true\n  - type: input\n    attributes:\n      label: Kubernetes Version\n      description: Which Kubernetes Version do you use?\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Helm Chart configuration\n      description: Additional description of your Helm Chart configuration.\n      placeholder: >\n        Enter any relevant details of your Helm Chart configuration. Maybe you can\n        paste your `values.yaml` or important parts of it here? Make sure to surround the code\n        you paste with ``` ```.\n  - type: textarea\n    attributes:\n      label: Docker Image customizations\n      description: What are the specific modification you've made in your image?\n      placeholder: >\n        Did you extend or customise the official Airflow image? Did you add any packages? Maybe\n        you can share a link to your image, or copy the Dockerfile and `docker build` commands\n        you used to build the image? Make sure to surround the code you paste with ``` ```.\n  - type: textarea\n    attributes:\n      label: What happened\n      description: Describe what happened.\n      placeholder: >\n        Please provide the context in which the problem occurred and explain what happened\n  - type: textarea\n    attributes:\n      label: What you think should happen instead\n      description: What do you think went wrong?\n      placeholder: >\n        Please explain why you think the behaviour is erroneous. It is extremely helpful if you copy&paste\n        the fragment of logs showing the exact error messages or wrong behaviour and screenshots for\n        UI problems or YouTube link to a video of you demonstrating the problem. You can include files by\n        dragging and dropping them here.\n  - type: textarea\n    attributes:\n      label: How to reproduce\n      description: >\n        What should we do to reproduce the problem? If you are not able to provide a reproducible case,\n        please open a [Discussion](https://github.com/apache/airflow/discussions) instead.\n      placeholder: >\n        Please make sure you provide a reproducible step-by-step case of how to reproduce the problem\n        as minimally and precisely as possible. Keep in mind we do not have access to your cluster or DAGs.\n        Remember that non-reproducible issues will be closed! Opening a discussion is recommended as a\n        first step.\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Anything else\n      description: Anything else we need to know?\n      placeholder: >\n        How often does this problem occur? (Once? Every time? Only when certain conditions are met?)\n        Any relevant logs to include? Put them here inside fenced\n        ``` ``` blocks or inside a foldable details tag if it's long:\n        <details><summary>x.log</summary> lots of stuff </details>\n  - type: checkboxes\n    attributes:\n      label: Are you willing to submit PR?\n      description: >\n        This is absolutely not required, but we are happy to guide you in the contribution process\n        especially if you already have a good understanding of how to implement the fix.\n        Airflow is a community-managed project and we love to bring new contributors in.\n        Find us in #new-contributors on Slack!\n      options:\n        - label: Yes I am willing to submit a PR!\n  - type: checkboxes\n    attributes:\n      label: Code of Conduct\n      description: >\n        The Code of Conduct helps create a safe space for everyone. We require\n        that everyone agrees to it.\n      options:\n        - label: >\n            I agree to follow this project's\n            [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n          required: true\n  - type: markdown\n    attributes:\n      value: \"Thanks for completing our form!\"\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/ISSUE_TEMPLATE/4-airflow_helmchart_bug_report.yml", "language": "yaml", "complexity": "medium", "star_count": 41241, "quality_score": 0.8300000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": true, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 145}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Airflow Doc issue report\ndescription: Problems and issues with Apache Airflow documentation\nlabels: [\"kind:bug\", \"kind:documentation\", \"needs-triage\"]\nbody:\n  - type: markdown\n    attributes:\n      # yamllint disable rule:line-length\n      value: \"\n        <img src='https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/logos/airflow_64x64_emoji_transparent.png' align='left' width='80' height='80'>\n        Thank you for finding the time to report the problem!\n\n        We really appreciate the community's efforts to improve Airflow.\n\n        Note, you do not need to create an issue if you have a change ready to submit!\n\n        You can open a [pull request](https://github.com/apache/airflow/pulls) immediately instead.\n        <br clear='left'/>\"\n      # yamllint enable rule:line-length\n  - type: textarea\n    attributes:\n      label: What do you see as an issue?\n      description: Please describe the issue with documentation you have.\n      placeholder: >\n        Please include links to the documentation that has the problem and possibly screenshots showing\n        the problem. Explain why do you think it is an issue. Make sure you include view of the target\n        audience of the documentation. Please explain why you think the docs are wrong.\n  - type: textarea\n    attributes:\n      label: Solving the problem\n      description: How do you think the problem can be solved?\n      placeholder: >\n        Please explain how you think the documentation could be fixed. Ideally specify where a new or missing\n        documentation should be added and what kind of information should be included. Sometimes people\n        writing the documentation do not realise that some assumptions the have might not be in the heads\n        of the reader, so try to explain exactly what you would like to see in the docs and why.\n  - type: textarea\n    attributes:\n      label: Anything else\n      description: Anything else we need to know?\n      placeholder: >\n        How often does this problem occur? (Once? Every time? Only when certain conditions are met?)\n        Any relevant logs to include? Put them here inside fenced\n        ``` ``` blocks or inside a foldable details tag if it's long:\n        <details><summary>x.log</summary> lots of stuff </details>\n  - type: checkboxes\n    attributes:\n      label: Are you willing to submit PR?\n      description: >\n        This is absolutely not required, but we are happy to guide you in the contribution process\n        especially if you already have a good understanding of how to implement the fix.\n        Airflow is a community-managed project and we love to bring new contributors in.\n        Find us in #new-contributors on Slack!\n      options:\n        - label: Yes I am willing to submit a PR!\n  - type: checkboxes\n    attributes:\n      label: Code of Conduct\n      description: >\n        The Code of Conduct helps create a safe space for everyone. We require\n        that everyone agrees to it.\n      options:\n        - label: >\n            I agree to follow this project's\n            [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n          required: true\n  - type: markdown\n    attributes:\n      value: \"Thanks for completing our form!\"\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/ISSUE_TEMPLATE/5-airflow_doc_issue_report.yml", "language": "yaml", "complexity": "simple", "star_count": 41241, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 70}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Task and Meta\ndescription: (Maintainers only) Tasks and Meta issues for the project\nlabels: [\"kind:meta\"]\nbody:\n  - type: textarea\n    attributes:\n      label: Body\n      description: Free form issue for committers to be able to add tasks/meta issues\n  - type: checkboxes\n    attributes:\n      label: Committer\n      description: Committer acknowledgment.\n      options:\n        - label: I acknowledge that I am a maintainer/committer of the Apache Airflow project.\n          required: true\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/ISSUE_TEMPLATE/6-free_form.yml", "language": "yaml", "complexity": "simple", "star_count": 41241, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": false, "has_automation": false, "file_count": 1, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n\n<!--\nThank you for contributing! Please make sure that your code changes\nare covered with tests. And in case of new features or big changes\nremember to adjust the documentation.\n\nFeel free to ping committers for the review!\n\nIn case of an existing issue, reference it using one of the following:\n\ncloses: #ISSUE\nrelated: #ISSUE\n\nHow to write a good git commit message:\nhttp://chris.beams.io/posts/git-commit/\n-->\n\n\n\n<!-- Please keep an empty line above the dashes. -->\n---\n**^ Add meaningful description above**\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 41241, "quality_score": 0.8, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 45}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate class-based implementation for Create automation scripts for deployment and system management\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n-->\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n- [What should be and should NOT be reported ?](#what-should-be-and-should-not-be-reported-)\n- [How to report the issue ?](#how-to-report-the-issue-)\n- [Is this really a security vulnerability ?](#is-this-really-a-security-vulnerability-)\n- [How do we assess severity of the issue ?](#how-do-we-assess-severity-of-the-issue-)\n- [What happens after you report the issue ?](#what-happens-after-you-report-the-issue-)\n- [Does CVE in Airflow Providers impact Airflow core package ?](#does-cve-in-airflow-providers-impact-airflow-core-package-)\n- [Where do I find more information about Airflow Security ?](#where-do-i-find-more-information-about-airflow-security-)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\nThis document contains information on how to report security vulnerabilities in Apache Airflow and\nhow security issues reported to the Apache Airflow security team are handled. If you would like\nto learn more, head to the\n[Airflow security](https://airflow.apache.org/docs/apache-airflow/stable/security/) documentation.\n\n**‚ö†Ô∏è Please do not file GitHub issues for security vulnerabilities as they are public! ‚ö†Ô∏è**\n\nThe Apache Software Foundation takes security issues very seriously. Apache\nAirflow specifically offers security features and is responsive to issues\naround its features. If you have any concern around Airflow Security or believe\nyou have uncovered a vulnerability, we suggest that you get in touch via the\ne-mail address [security@airflow.apache.org](mailto:security@airflow.apache.org).\n\nBefore sending the report, however, please read the following guidelines first. The guidelines should\nanswer the most common questions you might have about reporting vulnerabilities.\n\n### What should be and should NOT be reported ?\n\n**Only** use the security e-mail address to report undisclosed security vulnerabilities in Apache\nAirflow and to manage the process of fixing such vulnerabilities. We do not accept regular\nbug reports or other security-related queries at this address. We will ignore mail\nsent to this address that does not relate to an undisclosed security problem\nin the Apache Airflow project. Please follow regular communication channels described in\nthe [Airflow Community](https://airflow.apache.org/community/index.html) page for inquiries, questions and other discussions related\nto the process or issues.\n\nSpecifically, we will ignore results of security scans that contain a list of dependencies of Airflow\nwith dependencies in Airflow Docker reference image - there is a page that describes how the\n[Airflow reference Image is fixed at release time](https://airflow.apache.org/docs/docker-stack/index.html#fixing-images-at-release-time) and providing helpful instructions explaining\nhow you can build your own image and manage dependencies of Airflow in your own image.\n\n### How to report the issue ?\n\nPlease send one plain-text email for each vulnerability you are reporting including an explanation\nof how it affects Airflow security. We may ask that you resubmit your report if you send it as an image,\nmovie, HTML, or PDF attachment when you could as easily describe it with plain text.\n\n### Is this really a security vulnerability ?\n\nBefore reporting vulnerabilities, please make sure to read and understand the [security model](https://airflow.apache.org/docs/apache-airflow/stable/security/security_model.html)\nof Airflow, because some of the potential security vulnerabilities that are valid for projects that are\npublicly accessible from the Internet, are not valid for Airflow.\n\nAirflow is not designed to be used by untrusted users, and some trusted users are trusted enough to do a\nvariety of operations that could be considered as vulnerabilities in other products/circumstances.\n\n\nAgain. Please make sure to read and understand the [security model](https://airflow.apache.org/docs/apache-airflow/stable/security/security_model.html)\nof Airflow - this might help to save your time for reporting and our time for assessing the issues when\nthey are clearly expected, according to the Security model. You also avoid disappointment when you spend\na lot of time on preparing the issue report to follow the guidelines above and what you get as a response is\n\"this is not a security issue\" or \"this is invalid security issue\" or \"this is expected behaviour\". You\nsave time for yourself and for the Airflow Security team by reading and understanding the security model\nbefore reporting the issue.\n\n### How do we assess severity of the issue ?\n\nSeverity of the issue is determined based on the criteria described in\nthe [Severity Rating blog post](https://security.apache.org/blog/severityrating/) by the Apache Software Foundation Security team.\n\nDue to reasons explained in the previous chapter, some potential security vulnerabilities\ndo not apply to Airflow, or have a different severity than some generic scoring systems\n(for example `CVSS`) calculation suggests. So we are not using any generic scoring system.\n\n\n### What happens after you report the issue ?\n\nThe Airflow Security Team will get back to you after assessing the report. You will usually get\nconfirmation that the issue is being worked (or that we quickly assessed it as invalid) within several\nbusiness days. Note that this is an Open-Source projects and members of the security team are volunteers,\nso please make sure to be patient. If you do not get a response within a week, please send a kind reminder\nto the security team about a lack of response; however, reminders should only be for the initial response\nand not for updates on the assessment or remediation. We will usually let you know the CVE number that will\nbe assigned to the issue and the severity of the issue as well as release the issue is scheduled to be fixed\nafter we assess the issue (which might take longer or shorter time depending on the issue complexity and\npotential impact, severity, whether we want to address a whole class issues in a single fix and a number\nof other factors). You should subscribe  and monitor the `users@airflow.apache.org` mailing\nlist or `announce@apache.org` to see the announcement of the security issue - including the CVE number\nand the severity of the issue once the issue is fixed and release is public. Note that the\n`announce@apache.org` mailing list is moderated, and it might take a few hours for the announcement to\nappear there, so `users@airflow.apache.org` is the best place to monitor for the announcement.\n\nSecurity issues in Airflow are handled by the Airflow Security Team. Details about the Airflow Security\nTeam and how members of it are chosen can be found in the\n[Contributing documentation](https://github.com/apache/airflow/blob/main/contributing-docs/01_roles_in_airflow_project.rst#security-team).\n\n### Does CVE in Airflow Providers impact Airflow core package ?\n\nAirflow core package is released separately from provider distributions. While Airflow comes with ``constraints``\nwhich describe which version of providers have been tested when the version of Airflow was released, the\nusers of Airflow are advised to install providers independently of Airflow core when they want to apply\nsecurity fixes found and released in providers. Therefore, the issues found and fixed in providers do\nnot apply to the Airflow core package. There are also Airflow providers released by 3rd-parties, but the\nAirflow community is not responsible for releasing and announcing security vulnerabilities in them, this\nis handled entirely by the 3rd-parties that release their own providers.\n\n### Where do I find more information about Airflow Security ?\n\nIf you wish to know more about the ASF security process,\nthe [ASF Security team's page](https://www.apache.org/security/) describes\nhow vulnerability reports are handled in general by all ASF projects, and includes PGP keys if\nyou wish to use them when you report the issues.\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/SECURITY.md", "language": "markdown", "complexity": "medium", "star_count": 41241, "quality_score": 0.8600000000000001, "features": {"has_ci_cd": true, "has_docker": true, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": false, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 135}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate module integration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n---\nname: 'Setup Breeze'\ndescription: 'Sets up Python and Breeze'\ninputs:\n  python-version:\n    description: 'Python version to use'\n    default: \"3.10\"\noutputs:\n  host-python-version:\n    description: Python version used in host\n    value: ${{ steps.host-python-version.outputs.host-python-version }}\nruns:\n  using: \"composite\"\n  steps:\n    - name: \"Setup python\"\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ inputs.python-version }}\n    # NOTE! Installing Breeze without using cache is FASTER than when using cache - uv is so fast and has\n    # so low overhead, that just running upload cache/restore cache is slower than installing it from scratch\n    - name: \"Install Breeze\"\n      shell: bash\n      run: ./scripts/ci/install_breeze.sh\n      env:\n        PYTHON_VERSION: \"${{ inputs.python-version }}\"\n    - name: \"Free space\"\n      shell: bash\n      run: breeze ci free-space\n      env:\n        AIRFLOW_ROOT_PATH: \"${{ github.workspace }}\"\n    - name: \"Get Python version\"\n      shell: bash\n      run: >\n        echo \"host-python-version=$(python -c 'import platform; print(platform.python_version())')\"\n        >> ${GITHUB_OUTPUT}\n      id: host-python-version\n    - name: \"Disable cheatsheet\"\n      shell: bash\n      run: breeze setup config --no-cheatsheet --no-asciiart\n      env:\n        AIRFLOW_ROOT_PATH: \"${{ github.workspace }}\"\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/actions/breeze/action.yml", "language": "yaml", "complexity": "simple", "star_count": 41241, "quality_score": 0.89, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": false, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 59}, "authority_level": "high", "authentic": true}}
{"domain": "devops_automation", "category": "automation_scripts", "prompt": "### Devops_Automation Request (CLAUDE.md):\nCreate configuration for Create automation scripts for deployment and system management with bash automation scripts\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n---\nname: 'Install pre-commit'\ndescription: 'Installs pre-commit and related packages'\ninputs:\n  python-version:\n    description: 'Python version to use'\n    default: \"3.10\"\n  uv-version:\n    description: 'uv version to use'\n    default: \"0.8.0\"  # Keep this comment to allow automatic replacement of uv version\n  pre-commit-version:\n    description: 'pre-commit version to use'\n    default: \"4.2.0\"  # Keep this comment to allow automatic replacement of pre-commit version\n  pre-commit-uv-version:\n    description: 'pre-commit-uv version to use'\n    default: \"4.1.4\"  # Keep this comment to allow automatic replacement of pre-commit-uv version\n  skip-pre-commits:\n    description: \"Skip some pre-commits from installation\"\n    default: \"\"\nruns:\n  using: \"composite\"\n  steps:\n    - name: Install pre-commit, uv, and pre-commit-uv\n      shell: bash\n      env:\n        UV_VERSION: ${{inputs.uv-version}}\n        PRE_COMMIT_VERSION: ${{inputs.pre-commit-version}}\n        PRE_COMMIT_UV_VERSION: ${{inputs.pre-commit-uv-version}}\n        SKIP: ${{ inputs.skip-pre-commits }}\n      run: |\n        pip install uv==${UV_VERSION} || true\n        uv tool install pre-commit==${PRE_COMMIT_VERSION} --with uv==${UV_VERSION} \\\n        --with pre-commit-uv==${PRE_COMMIT_UV_VERSION}\n      working-directory: ${{ github.workspace }}\n    # We need to use tar file with archive to restore all the permissions and symlinks\n    - name: \"Delete ~.cache\"\n      run: |\n        du ~/ --max-depth=2\n        echo\n        echo Deleting ~/.cache\n        echo\n        rm -rf ~/.cache\n        echo\n      shell: bash\n    - name: \"Restore pre-commit cache\"\n      uses: apache/infrastructure-actions/stash/restore@1c35b5ccf8fba5d4c3fdf25a045ca91aa0cbc468\n      with:\n        key: cache-pre-commit-v4-${{ inputs.python-version }}-${{ hashFiles('.pre-commit-config.yaml') }}\n        path: /tmp/\n      id: restore-pre-commit-cache\n    - name: \"Check if pre-commit cache tarball exists\"\n      shell: bash\n      run: |\n        if [ -f /tmp/cache-pre-commit.tar.gz ]; then\n          echo \"‚úÖ Cache tarball found: /tmp/cache-pre-commit.tar.gz\"\n        else\n          echo \"‚ùå Cache tarball missing. Expected /tmp/cache-pre-commit.tar.gz\"\n          exit 1\n        fi\n      if: steps.restore-pre-commit-cache.outputs.stash-hit == 'true'\n    - name: \"Restore .cache from the tar file\"\n      run: tar -C ~ -xzf /tmp/cache-pre-commit.tar.gz\n      shell: bash\n      if: steps.restore-pre-commit-cache.outputs.stash-hit == 'true'\n    - name: \"Show restored files\"\n      run: |\n        echo \"Restored files\"\n        du ~/ --max-depth=2\n        echo\n      shell: bash\n      if: steps.restore-pre-commit-cache.outputs.stash-hit == 'true'\n    - name: Install pre-commit hooks\n      shell: bash\n      run: pre-commit install-hooks || (cat ~/.cache/pre-commit/pre-commit.log && exit 1)\n      working-directory: ${{ github.workspace }}\n      env:\n        SKIP: ${{ inputs.skip-pre-commits }}\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/actions/install-pre-commit/action.yml", "language": "yaml", "complexity": "medium", "star_count": 41241, "quality_score": 0.8600000000000001, "features": {"has_ci_cd": false, "has_docker": false, "has_kubernetes": false, "has_terraform": false, "has_ansible": false, "has_monitoring": false, "has_secrets": true, "has_environment_vars": true, "has_scripts": true, "has_automation": false, "file_count": 1, "line_count": 95}, "authority_level": "high", "authentic": true}}
