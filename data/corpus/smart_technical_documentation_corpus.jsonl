{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "labels: [question]\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Thanks for your interest in FastAPI! üöÄ\n\n        Please follow these instructions, fill every question, and do every step. üôè\n\n        I'm asking this because answering questions and solving problems in GitHub is what consumes most of the time.\n\n        I end up not being able to add new features, fix bugs, review pull requests, etc. as fast as I wish because I have to spend too much time handling questions.\n\n        All that, on top of all the incredible help provided by a bunch of community members, the [FastAPI Experts](https://fastapi.tiangolo.com/fastapi-people/#experts), that give a lot of their time to come here and help others.\n\n        That's a lot of work they are doing, but if more FastAPI users came to help others like them just a little bit more, it would be much less effort for them (and you and me üòÖ).\n\n        By asking questions in a structured way (following this) it will be much easier to help you.\n\n        And there's a high chance that you will find the solution along the way and you won't even have to submit it and wait for an answer. üòé\n\n        As there are too many questions, I'll have to discard and close the incomplete ones. That will allow me (and others) to focus on helping people like you that follow the whole process and help us help you. ü§ì\n  - type: checkboxes\n    id: checks\n    attributes:\n      label: First Check\n      description: Please confirm and check all the following options.\n      options:\n        - label: I added a very descriptive title here.\n          required: true\n        - label: I used the GitHub search to find a similar question and didn't find it.\n          required: true\n        - label: I searched the FastAPI documentation, with the integrated search.\n          required: true\n        - label: I already searched in Google \"How to X in FastAPI\" and didn't find any information.\n          required: true\n        - label: I already read and followed all the tutorial in the docs and didn't find an answer.\n          required: true\n        - label: I already checked if it is not related to FastAPI but to [Pydantic](https://github.com/pydantic/pydantic).\n          required: true\n        - label: I already checked if it is not related to FastAPI but to [Swagger UI](https://github.com/swagger-api/swagger-ui).\n          required: true\n        - label: I already checked if it is not related to FastAPI but to [ReDoc](https://github.com/Redocly/redoc).\n          required: true\n  - type: checkboxes\n    id: help\n    attributes:\n      label: Commit to Help\n      description: |\n        After submitting this, I commit to one of:\n\n          * Read open questions until I find 2 where I can help someone and add a comment to help there.\n          * I already hit the \"watch\" button in this repository to receive notifications and I commit to help at least 2 people that ask questions in the future.\n          * Review one Pull Request by downloading the code and following [all the review process](https://fastapi.tiangolo.com/help-fastapi/#review-pull-requests).\n\n      options:\n        - label: I commit to help with one of those options üëÜ\n          required: true\n  - type: textarea\n    id: example\n    attributes:\n      label: Example Code\n      description: |\n        Please add a self-contained, [minimal, reproducible, example](https://stackoverflow.com/help/minimal-reproducible-example) with your use case.\n\n        If I (or someone) can copy it, run it, and see it right away, there's a much higher chance I (or someone) will be able to help you.\n\n      placeholder: |\n        from fastapi import FastAPI\n\n        app = FastAPI()\n\n\n        @app.get(\"/\")\n        def read_root():\n            return {\"Hello\": \"World\"}\n      render: python\n    validations:\n      required: true\n  - type: textarea\n    id: description\n    attributes:\n      label: Description\n      description: |\n        What is the problem, question, or error?\n\n        Write a short description telling me what you are doing, what you expect to happen, and what is currently happening.\n      placeholder: |\n        * Open the browser and call the endpoint `/`.\n        * It returns a JSON with `{\"Hello\": \"World\"}`.\n        * But I expected it to return `{\"Hello\": \"Sara\"}`.\n    validations:\n      required: true\n  - type: dropdown\n    id: os\n    attributes:\n      label: Operating System\n      description: What operating system are you on?\n      multiple: true\n      options:\n        - Linux\n        - Windows\n        - macOS\n        - Other\n    validations:\n      required: true\n  - type: textarea\n    id: os-details\n    attributes:\n      label: Operating System Details\n      description: You can add more details about your operating system here, in particular if you chose \"Other\".\n  - type: input\n    id: fastapi-version\n    attributes:\n      label: FastAPI Version\n      description: |\n        What FastAPI version are you using?\n\n        You can find the FastAPI version with:\n\n        ```bash\n        python -c \"import fastapi; print(fastapi.__version__)\"\n        ```\n    validations:\n      required: true\n  - type: input\n    id: pydantic-version\n    attributes:\n      label: Pydantic Version\n      description: |\n        What Pydantic version are you using?\n\n        You can find the Pydantic version with:\n\n        ```bash\n        python -c \"import pydantic; print(pydantic.version.VERSION)\"\n        ```\n    validations:\n      required: true\n  - type: input\n    id: python-version\n    attributes:\n      label: Python Version\n      description: |\n        What Python version are you using?\n\n        You can find the Python version with:\n\n        ```bash\n        python --version\n        ```\n    validations:\n      required: true\n  - type: textarea\n    id: context\n    attributes:\n      label: Additional Context\n      description: Add any additional context information or screenshots you think are useful.\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/DISCUSSION_TEMPLATE/questions.yml", "language": "yaml", "complexity": "medium", "star_count": 87642, "quality_score": 0.89, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 159, "word_count": 795}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "blank_issues_enabled: false\ncontact_links:\n  - name: Security Contact\n    about: Please report security vulnerabilities to security@tiangolo.com\n  - name: Question or Problem\n    about: Ask a question or ask about a problem in GitHub Discussions.\n    url: https://github.com/fastapi/fastapi/discussions/categories/questions\n  - name: Feature Request\n    about: To suggest an idea or ask about a feature, please start with a question saying what you would like to achieve. There might be a way to do it already.\n    url: https://github.com/fastapi/fastapi/discussions/categories/questions\n  - name: Show and tell\n    about: Show what you built with FastAPI or to be used with FastAPI.\n    url: https://github.com/fastapi/fastapi/discussions/categories/show-and-tell\n  - name: Translations\n    about: Coordinate translations in GitHub Discussions.\n    url: https://github.com/fastapi/fastapi/discussions/categories/translations\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/ISSUE_TEMPLATE/config.yml", "language": "yaml", "complexity": "simple", "star_count": 87642, "quality_score": 0.8, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 17, "word_count": 101}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Privileged\ndescription: You are @tiangolo or he asked you directly to create an issue here. If not, check the other options. üëá\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Thanks for your interest in FastAPI! üöÄ\n\n        If you are not @tiangolo or he didn't ask you directly to create an issue here, please start the conversation in a [Question in GitHub Discussions](https://github.com/fastapi/fastapi/discussions/categories/questions) instead.\n  - type: checkboxes\n    id: privileged\n    attributes:\n      label: Privileged issue\n      description: Confirm that you are allowed to create an issue here.\n      options:\n        - label: I'm @tiangolo or he asked me directly to create an issue here.\n          required: true\n  - type: textarea\n    id: content\n    attributes:\n      label: Issue Content\n      description: Add the content of the issue here.\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/ISSUE_TEMPLATE/privileged.yml", "language": "yaml", "complexity": "simple", "star_count": 87642, "quality_score": 0.8, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": false, "line_count": 23, "word_count": 118}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "docs:\n  - all:\n    - changed-files:\n      - any-glob-to-any-file:\n        - docs/en/docs/**\n        - docs_src/**\n      - all-globs-to-all-files:\n        - '!fastapi/**'\n        - '!pyproject.toml'\n        - '!docs/en/data/sponsors.yml'\n        - '!docs/en/overrides/main.html'\n\nlang-all:\n  - all:\n    - changed-files:\n      - any-glob-to-any-file:\n        - docs/*/docs/**\n      - all-globs-to-all-files:\n        - '!docs/en/docs/**'\n        - '!fastapi/**'\n        - '!pyproject.toml'\n\ninternal:\n  - all:\n    - changed-files:\n      - any-glob-to-any-file:\n        - .github/**\n        - scripts/**\n        - .gitignore\n        - .pre-commit-config.yaml\n        - pdm_build.py\n        - requirements*.txt\n        - docs/en/data/sponsors.yml\n        - docs/en/overrides/main.html\n      - all-globs-to-all-files:\n        - '!docs/*/docs/**'\n        - '!fastapi/**'\n        - '!pyproject.toml'\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/labeler.yml", "language": "yaml", "complexity": "simple", "star_count": 87642, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 39, "word_count": 69}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Add to Project\n\non:\n  pull_request_target:\n  issues:\n    types:\n      - opened\n      - reopened\n\njobs:\n  add-to-project:\n    name: Add to project\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/add-to-project@v1.0.2\n        with:\n          project-url: https://github.com/orgs/fastapi/projects/2\n          github-token: ${{ secrets.PROJECTS_TOKEN }}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/add-to-project.yml", "language": "yaml", "complexity": "simple", "star_count": 87642, "quality_score": 0.8, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 19, "word_count": 31}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Build Docs\non:\n  push:\n    branches:\n      - master\n  pull_request:\n    types:\n      - opened\n      - synchronize\n\nenv:\n  UV_SYSTEM_PYTHON: 1\n\njobs:\n  changes:\n    runs-on: ubuntu-latest\n    # Required permissions\n    permissions:\n      pull-requests: read\n    # Set job outputs to values from filter step\n    outputs:\n      docs: ${{ steps.filter.outputs.docs }}\n    steps:\n    - uses: actions/checkout@v4\n    # For pull requests it's not necessary to checkout the code but for the main branch it is\n    - uses: dorny/paths-filter@v3\n      id: filter\n      with:\n        filters: |\n          docs:\n            - README.md\n            - docs/**\n            - docs_src/**\n            - requirements-docs.txt\n            - requirements-docs-insiders.txt\n            - pyproject.toml\n            - mkdocs.yml\n            - mkdocs.insiders.yml\n            - mkdocs.maybe-insiders.yml\n            - mkdocs.no-insiders.yml\n            - .github/workflows/build-docs.yml\n            - .github/workflows/deploy-docs.yml\n  langs:\n    needs:\n      - changes\n    runs-on: ubuntu-latest\n    outputs:\n      langs: ${{ steps.show-langs.outputs.langs }}\n    steps:\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.4.15\"\n          enable-cache: true\n          cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n      - name: Install docs extras\n        run: uv pip install -r requirements-docs.txt\n      # Install MkDocs Material Insiders here just to put it in the cache for the rest of the steps\n      - name: Install Material for MkDocs Insiders\n        if: ( github.event_name != 'pull_request' || github.secret_source == 'Actions' )\n        run: uv pip install -r requirements-docs-insiders.txt\n        env:\n          TOKEN: ${{ secrets.FASTAPI_MKDOCS_MATERIAL_INSIDERS }}\n      - name: Verify Docs\n        run: python ./scripts/docs.py verify-docs\n      - name: Export Language Codes\n        id: show-langs\n        run: |\n          echo \"langs=$(python ./scripts/docs.py langs-json)\" >> $GITHUB_OUTPUT\n\n  build-docs:\n    needs:\n      - changes\n      - langs\n    if: ${{ needs.changes.outputs.docs == 'true' }}\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        lang: ${{ fromJson(needs.langs.outputs.langs) }}\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.4.15\"\n          enable-cache: true\n          cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n      - name: Install docs extras\n        run: uv pip install -r requirements-docs.txt\n      - name: Install Material for MkDocs Insiders\n        if: ( github.event_name != 'pull_request' || github.secret_source == 'Actions' )\n        run: uv pip install -r requirements-docs-insiders.txt\n        env:\n          TOKEN: ${{ secrets.FASTAPI_MKDOCS_MATERIAL_INSIDERS }}\n      - name: Update Languages\n        run: python ./scripts/docs.py update-languages\n      - uses: actions/cache@v4\n        with:\n          key: mkdocs-cards-${{ matrix.lang }}-${{ github.ref }}\n          path: docs/${{ matrix.lang }}/.cache\n      - name: Build Docs\n        run: python ./scripts/docs.py build-lang ${{ matrix.lang }}\n      - uses: actions/upload-artifact@v4\n        with:\n          name: docs-site-${{ matrix.lang }}\n          path: ./site/**\n          include-hidden-files: true\n\n  # https://github.com/marketplace/actions/alls-green#why\n  docs-all-green:  # This job does nothing and is only used for the branch protection\n    if: always()\n    needs:\n      - build-docs\n    runs-on: ubuntu-latest\n    steps:\n      - name: Decide whether the needed jobs succeeded or failed\n        uses: re-actors/alls-green@release/v1\n        with:\n          jobs: ${{ toJSON(needs) }}\n          allowed-skips: build-docs\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/build-docs.yml", "language": "yaml", "complexity": "medium", "star_count": 87642, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 138, "word_count": 406}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: FastAPI People Contributors\n\non:\n  schedule:\n    - cron: \"0 3 1 * *\"\n  workflow_dispatch:\n    inputs:\n      debug_enabled:\n        description: \"Run the build with tmate debugging enabled (https://github.com/marketplace/actions/debugging-with-tmate)\"\n        required: false\n        default: \"false\"\n\nenv:\n  UV_SYSTEM_PYTHON: 1\n\njobs:\n  job:\n    if: github.repository_owner == 'fastapi'\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.4.15\"\n          enable-cache: true\n          cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n      - name: Install Dependencies\n        run: uv pip install -r requirements-github-actions.txt\n      # Allow debugging with tmate\n      - name: Setup tmate session\n        uses: mxschmitt/action-tmate@v3\n        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.debug_enabled == 'true' }}\n        with:\n          limit-access-to-actor: true\n        env:\n          GITHUB_TOKEN: ${{ secrets.FASTAPI_PR_TOKEN }}\n      - name: FastAPI People Contributors\n        run: python ./scripts/contributors.py\n        env:\n          GITHUB_TOKEN: ${{ secrets.FASTAPI_PR_TOKEN }}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/contributors.yml", "language": "yaml", "complexity": "simple", "star_count": 87642, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 54, "word_count": 138}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Deploy Docs\non:\n  workflow_run:\n    workflows:\n      - Build Docs\n    types:\n      - completed\n\npermissions:\n  deployments: write\n  issues: write\n  pull-requests: write\n  statuses: write\n\nenv:\n  UV_SYSTEM_PYTHON: 1\n\njobs:\n  deploy-docs:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.4.15\"\n          enable-cache: true\n          cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n      - name: Install GitHub Actions dependencies\n        run: uv pip install -r requirements-github-actions.txt\n      - name: Deploy Docs Status Pending\n        run: python ./scripts/deploy_docs_status.py\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          COMMIT_SHA: ${{ github.event.workflow_run.head_sha }}\n          RUN_ID: ${{ github.run_id }}\n      - name: Clean site\n        run: |\n          rm -rf ./site\n          mkdir ./site\n      - uses: actions/download-artifact@v4\n        with:\n          path: ./site/\n          pattern: docs-site-*\n          merge-multiple: true\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n          run-id: ${{ github.event.workflow_run.id }}\n      - name: Deploy to Cloudflare Pages\n        # hashFiles returns an empty string if there are no files\n        if: hashFiles('./site/*')\n        id: deploy\n        env:\n          PROJECT_NAME: fastapitiangolo\n          BRANCH: ${{ ( github.event.workflow_run.head_repository.full_name == github.repository && github.event.workflow_run.head_branch == 'master' && 'main' ) || ( github.event.workflow_run.head_sha ) }}\n        uses: cloudflare/wrangler-action@v3\n        with:\n          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}\n          accountId: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}\n          command: pages deploy ./site --project-name=${{ env.PROJECT_NAME }} --branch=${{ env.BRANCH }}\n      - name: Comment Deploy\n        run: python ./scripts/deploy_docs_status.py\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          DEPLOY_URL: ${{ steps.deploy.outputs.deployment-url }}\n          COMMIT_SHA: ${{ github.event.workflow_run.head_sha }}\n          RUN_ID: ${{ github.run_id }}\n          IS_DONE: \"true\"\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/deploy-docs.yml", "language": "yaml", "complexity": "simple", "star_count": 87642, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 78, "word_count": 222}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Issue Manager\n\non:\n  schedule:\n    - cron: \"13 22 * * *\"\n  issue_comment:\n    types:\n      - created\n  issues:\n    types:\n      - labeled\n  pull_request_target:\n    types:\n      - labeled\n  workflow_dispatch:\n\npermissions:\n  issues: write\n  pull-requests: write\n\njobs:\n  issue-manager:\n    if: github.repository_owner == 'fastapi'\n    runs-on: ubuntu-latest\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: tiangolo/issue-manager@0.5.1\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          config: >\n            {\n              \"answered\": {\n                \"delay\": 864000,\n                \"message\": \"Assuming the original need was handled, this will be automatically closed now. But feel free to add more comments or create new issues or PRs.\"\n              },\n              \"waiting\": {\n                \"delay\": 2628000,\n                \"message\": \"As this PR has been waiting for the original user for a while but seems to be inactive, it's now going to be closed. But if there's anyone interested, feel free to create a new PR.\"\n              },\n              \"invalid\": {\n                \"delay\": 0,\n                \"message\": \"This was marked as invalid and will be closed now. If this is an error, please provide additional details.\"\n              }\n            }\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/issue-manager.yml", "language": "yaml", "complexity": "simple", "star_count": 87642, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 48, "word_count": 162}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Label Approved\n\non:\n  schedule:\n    - cron: \"0 12 * * *\"\n  workflow_dispatch:\n\npermissions:\n  pull-requests: write\n\nenv:\n  UV_SYSTEM_PYTHON: 1\n\njobs:\n  label-approved:\n    if: github.repository_owner == 'fastapi'\n    runs-on: ubuntu-latest\n    steps:\n    - name: Dump GitHub context\n      env:\n        GITHUB_CONTEXT: ${{ toJson(github) }}\n      run: echo \"$GITHUB_CONTEXT\"\n    - uses: actions/checkout@v4\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: \"3.11\"\n    - name: Setup uv\n      uses: astral-sh/setup-uv@v6\n      with:\n        version: \"0.4.15\"\n        enable-cache: true\n        cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n    - name: Install GitHub Actions dependencies\n      run: uv pip install -r requirements-github-actions.txt\n    - name: Label Approved\n      run: python ./scripts/label_approved.py\n      env:\n        TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        CONFIG: >\n          {\n            \"approved-1\":\n              {\n                \"number\": 1,\n                \"await_label\": \"awaiting-review\"\n              }\n          }\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/label-approved.yml", "language": "yaml", "complexity": "simple", "star_count": 87642, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": true, "has_headers": false, "line_count": 50, "word_count": 104}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Labels\non:\n  pull_request_target:\n    types:\n      - opened\n      - synchronize\n      - reopened\n      # For label-checker\n      - labeled\n      - unlabeled\n\njobs:\n  labeler:\n    permissions:\n      contents: read\n      pull-requests: write\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/labeler@v5\n      if: ${{ github.event.action != 'labeled' && github.event.action != 'unlabeled' }}\n    - run: echo \"Done adding labels\"\n  # Run this after labeler applied labels\n  check-labels:\n    needs:\n      - labeler\n    permissions:\n      pull-requests: read\n    runs-on: ubuntu-latest\n    steps:\n      - uses: docker://agilepathway/pull-request-label-checker:latest\n        with:\n          one_of: breaking,security,feature,bug,refactor,upgrade,docs,lang-all,internal\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/labeler.yml", "language": "yaml", "complexity": "simple", "star_count": 87642, "quality_score": 0.8, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 34, "word_count": 74}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Latest Changes\n\non:\n  pull_request_target:\n    branches:\n      - master\n    types:\n      - closed\n  workflow_dispatch:\n    inputs:\n      number:\n        description: PR number\n        required: true\n      debug_enabled:\n        description: 'Run the build with tmate debugging enabled (https://github.com/marketplace/actions/debugging-with-tmate)'\n        required: false\n        default: 'false'\n\njobs:\n  latest-changes:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n        with:\n          # To allow latest-changes to commit to the main branch\n          token: ${{ secrets.FASTAPI_LATEST_CHANGES }}\n      # Allow debugging with tmate\n      - name: Setup tmate session\n        uses: mxschmitt/action-tmate@v3\n        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.debug_enabled == 'true' }}\n        with:\n          limit-access-to-actor: true\n      - uses: tiangolo/latest-changes@0.3.2\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          latest_changes_file: docs/en/docs/release-notes.md\n          latest_changes_header: '## Latest Changes'\n          end_regex: '^## '\n          debug_logs: true\n          label_header_prefix: '### '\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/latest-changes.yml", "language": "yaml", "complexity": "simple", "star_count": 87642, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 45, "word_count": 116}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Notify Translations\n\non:\n  pull_request_target:\n    types:\n      - labeled\n      - closed\n  workflow_dispatch:\n    inputs:\n      number:\n        description: PR number\n        required: true\n      debug_enabled:\n        description: 'Run the build with tmate debugging enabled (https://github.com/marketplace/actions/debugging-with-tmate)'\n        required: false\n        default: 'false'\n\nenv:\n  UV_SYSTEM_PYTHON: 1\n\njobs:\n  job:\n    runs-on: ubuntu-latest\n    permissions:\n      discussions: write\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.4.15\"\n          enable-cache: true\n          cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n      - name: Install Dependencies\n        run: uv pip install -r requirements-github-actions.txt\n      # Allow debugging with tmate\n      - name: Setup tmate session\n        uses: mxschmitt/action-tmate@v3\n        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.debug_enabled == 'true' }}\n        with:\n          limit-access-to-actor: true\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: Notify Translations\n        run: python ./scripts/notify_translations.py\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          NUMBER: ${{ github.event.inputs.number || null }}\n          DEBUG: ${{ github.event.inputs.debug_enabled || 'false' }}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/notify-translations.yml", "language": "yaml", "complexity": "simple", "star_count": 87642, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 60, "word_count": 148}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: FastAPI People\n\non:\n  schedule:\n    - cron: \"0 14 1 * *\"\n  workflow_dispatch:\n    inputs:\n      debug_enabled:\n        description: Run the build with tmate debugging enabled (https://github.com/marketplace/actions/debugging-with-tmate)\n        required: false\n        default: \"false\"\n\nenv:\n  UV_SYSTEM_PYTHON: 1\n\njobs:\n  job:\n    if: github.repository_owner == 'fastapi'\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n      - name: Setup uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.4.15\"\n          enable-cache: true\n          cache-dependency-glob: |\n            requirements**.txt\n            pyproject.toml\n      - name: Install Dependencies\n        run: uv pip install -r requirements-github-actions.txt\n      # Allow debugging with tmate\n      - name: Setup tmate session\n        uses: mxschmitt/action-tmate@v3\n        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.debug_enabled == 'true' }}\n        with:\n          limit-access-to-actor: true\n        env:\n          GITHUB_TOKEN: ${{ secrets.FASTAPI_PEOPLE }}\n      - name: FastAPI People Experts\n        run: python ./scripts/people.py\n        env:\n          GITHUB_TOKEN: ${{ secrets.FASTAPI_PEOPLE }}\n          SLEEP_INTERVAL: ${{ vars.PEOPLE_SLEEP_INTERVAL }}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/people.yml", "language": "yaml", "complexity": "simple", "star_count": 87642, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 55, "word_count": 141}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Publish\n\non:\n  release:\n    types:\n      - created\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        package:\n          - fastapi\n          - fastapi-slim\n    permissions:\n      id-token: write\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.10\"\n          # Issue ref: https://github.com/actions/setup-python/issues/436\n          # cache: \"pip\"\n          # cache-dependency-path: pyproject.toml\n      - name: Install build dependencies\n        run: pip install build\n      - name: Build distribution\n        env:\n          TIANGOLO_BUILD_PACKAGE: ${{ matrix.package }}\n        run: python -m build\n      - name: Publish\n        uses: pypa/gh-action-pypi-publish@v1.12.4\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": ".github/workflows/publish.yml", "language": "yaml", "complexity": "simple", "star_count": 87642, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 43, "word_count": 98}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Feature request\nabout: Suggest an idea for this project\ntitle: ''\nlabels: ''\nassignees: ''\n\n---\n\n**Is your feature request related to a problem? Please describe.**\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": ".github/ISSUE_TEMPLATE/feature_request.md", "language": "markdown", "complexity": "simple", "star_count": 8401, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 15, "word_count": 60}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Docs\n\non:\n  workflow_dispatch:\n\njobs:\n  docs:\n    runs-on: ubuntu-latest\n    \n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n      - name: Docs update\n        run: git push origin master:docs\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": ".github/workflows/docs.yml", "language": "yaml", "complexity": "simple", "star_count": 8401, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 16, "word_count": 24}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Full Test\n\non:\n  push:\n  workflow_dispatch:\n  pull_request:\n    types: [assigned, opened, synchronize, reopened]\n\njobs:\n  test:\n    runs-on: ubuntu-22.04\n    strategy:\n      matrix:\n        python-version: ['3.7', '3.8', '3.9', '3.10', '3.11', '3.12', '3.13']\n        django-version: ['<3.2', '<3.3', '<4.2', '<4.3', '<5.1', '<5.2', '<5.3']\n        exclude:\n          - python-version: '3.7'\n            django-version: '<5.1'\n          - python-version: '3.8'\n            django-version: '<5.1'\n          - python-version: '3.9'\n            django-version: '<5.1'\n          - python-version: '3.12'\n            django-version: '<3.2'\n          - python-version: '3.12'\n            django-version: '<3.3'\n          - python-version: '3.13'\n            django-version: '<3.2'\n          - python-version: '3.13'\n            django-version: '<3.3'\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Install core\n        run: pip install --pre \"Django${{ matrix.django-version }}\" \"pydantic<3\"\n      - name: Install tests\n        run: pip install pytest pytest-asyncio pytest-django psycopg2-binary\n      - name: Test\n        run: pytest\n\n  coverage:\n    runs-on: ubuntu-latest\n    \n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.10'\n      - name: Install Flit\n        run: pip install flit \"django>=5.2\"\n      - name: Install Dependencies\n        run: flit install --symlink\n      - name: Test\n        run: pytest --cov=ninja\n\n  codestyle:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: 3.9\n      - name: Install Flit\n        run: pip install flit\n      - name: Install Dependencies\n        run: flit install --symlink\n      - name: Ruff format\n        run: ruff format --preview --check ninja tests\n      - name: Ruff lint\n        run: ruff check --preview ninja tests\n      - name: mypy\n        run: mypy ninja tests/mypy_test.py\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": ".github/workflows/test_full.yml", "language": "yaml", "complexity": "simple", "star_count": 8401, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 78, "word_count": 215}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.2.0\n    hooks:\n      - id: check-yaml\n      # - id: end-of-file-fixer\n      # - id: trailing-whitespace\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.7.1\n    hooks:\n      - id: mypy\n        additional_dependencies: [\"django-stubs\", \"pydantic\"]\n        exclude: (tests|docs)/\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.4.2\n    hooks:\n      - id: ruff-format\n      - id: ruff\n        args: [--fix, --exit-non-zero-on-fix]\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": ".pre-commit-config.yaml", "language": "yaml", "complexity": "simple", "star_count": 8401, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 20, "word_count": 47}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributing\n\nDjango Ninja uses Flit to build, package and publish the project.\n\nto install it use:\n\n```\npip install flit\n```\n\nOnce you have it - to install all dependencies required for development and testing  use this command:\n\n\n```\nflit install --deps develop --symlink\n```\n\nOnce done you can check if all works with \n\n```\npytest .\n```\n\nor using Makefile:\n\n```\nmake test\n```\n\nNow you are ready to make your contribution\n\n\nWhen you're done please make sure you to test your functionality \nand check the coverage of your contribution.\n\n```\npytest --cov=ninja --cov-report term-missing tests\n```\n\nor using Makefile:\n\n```\nmake test-cov\n```\n \n## Code style\n\nDjango Ninja uses `ruff`, and `mypy` for style checks.\n\nRun `pre-commit install` to create a git hook to fix your styles before you commit.\n\nAlternatively, manually check your code with:\n\n```\nruff format --check ninja tests\nruff check ninja tests\nmypy ninja\n```\n\nor using Makefile:\n\n```\nmake lint\n```\n\nOr reformat your code with:\n\n```\nruff format ninja tests\nruff check ninja tests --fix\n```\n\nor using Makefile:\n\n```\nmake fmt\n```\n \n## Docs\nPlease do not forget to document your contribution\n\nDjango Ninja uses `mkdocs`:\n\n```\ncd docs/\nmkdocs serve\n```\nand go to browser to see changes in real time\n\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "simple", "star_count": 8401, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 90, "word_count": 212}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<a href=\"https://github.com/vitalik/django-ninja/issues/383\"><img width=\"814\" alt=\"SCR-20230123-m1t\" src=\"https://user-images.githubusercontent.com/95222/214056666-585c0479-c122-4cb3-add4-b8844088ccdd.png\"></a>\n\n\n\n<a href=\"https://github.com/vitalik/django-ninja/issues/383\">^ Please read ^</a>\n\n\n\n\n<p align=\"center\">\n  <a href=\"https://django-ninja.dev/\"><img src=\"https://django-ninja.dev/img/logo-big.png\"></a>\n</p>\n<p align=\"center\">\n    <em>Fast to learn, fast to code, fast to run</em>\n</p>\n\n\n![Test](https://github.com/vitalik/django-ninja/actions/workflows/test_full.yml/badge.svg)\n![Coverage](https://img.shields.io/codecov/c/github/vitalik/django-ninja)\n[![PyPI version](https://badge.fury.io/py/django-ninja.svg)](https://badge.fury.io/py/django-ninja)\n[![Downloads](https://static.pepy.tech/personalized-badge/django-ninja?period=month&units=international_system&left_color=black&right_color=brightgreen&left_text=downloads/month)](https://pepy.tech/project/django-ninja)\n\n# Django Ninja - Fast Django REST Framework\n\n**Django Ninja** is a web framework for building APIs with **Django** and Python 3.6+ **type hints**.\n\n\n **Key features:**\n\n  - **Easy**: Designed to be easy to use and intuitive.\n  - **FAST execution**: Very high performance thanks to **<a href=\"https://pydantic-docs.helpmanual.io\" target=\"_blank\">Pydantic</a>** and **<a href=\"/docs/docs/guides/async-support.md\">async support</a>**.\n  - **Fast to code**: Type hints and automatic docs lets you focus only on business logic.\n  - **Standards-based**: Based on the open standards for APIs: **OpenAPI** (previously known as Swagger) and **JSON Schema**.\n  - **Django friendly**: (obviously) has good integration with the Django core and ORM.\n  - **Production ready**: Used by multiple companies on live projects (If you use django-ninja and would like to publish your feedback, please email ppr.vitaly@gmail.com).\n\n\n\n![Django Ninja REST Framework](docs/docs/img/benchmark.png)\n\n**Documentation**: https://django-ninja.dev\n\n---\n\n## Installation\n\n```\npip install django-ninja\n```\n\n\n\n## Usage\n\n\nIn your django project next to urls.py create new `api.py` file:\n\n```Python\nfrom ninja import NinjaAPI\n\napi = NinjaAPI()\n\n\n@api.get(\"/add\")\ndef add(request, a: int, b: int):\n    return {\"result\": a + b}\n```\n\n\nNow go to `urls.py` and add the following:\n\n\n```Python hl_lines=\"3 7\"\n...\nfrom .api import api\n\nurlpatterns = [\n    path(\"admin/\", admin.site.urls),\n    path(\"api/\", api.urls),  # <---------- !\n]\n```\n\n**That's it !**\n\nNow you've just created an API that:\n\n - receives an HTTP GET request at `/api/add`\n - takes, validates and type-casts GET parameters `a` and `b`\n - decodes the result to JSON\n - generates an OpenAPI schema for defined operation\n\n### Interactive API docs\n\nNow go to <a href=\"http://127.0.0.1:8000/api/docs\" target=\"_blank\">http://127.0.0.1:8000/api/docs</a>\n\nYou will see the automatic interactive API documentation (provided by <a href=\"https://github.com/swagger-api/swagger-ui\" target=\"_blank\">Swagger UI</a> or <a href=\"https://github.com/Redocly/redoc\" target=\"_blank\">Redoc</a>):\n\n\n![Swagger UI](docs/docs/img/index-swagger-ui.png)\n\n## What next?\n\n - Read the full documentation here - https://django-ninja.dev\n - To support this project, please give star it on Github. ![github star](docs/docs/img/github-star.png)\n - Share it [via Twitter](https://twitter.com/intent/tweet?text=Check%20out%20Django%20Ninja%20-%20Fast%20Django%20REST%20Framework%20-%20https%3A%2F%2Fdjango-ninja.dev)\n - If you already using django-ninja, please share your feedback to ppr.vitaly@gmail.com\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 8401, "quality_score": 0.82, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 107, "word_count": 343}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# API Docs\n\n## OpenAPI docs\n\nOnce you configured your Ninja API and started runserver -  go to <a href=\"http://127.0.0.1:8000/api/docs\" target=\"_blank\">http://127.0.0.1:8000/api/docs</a>\n\nYou will see the automatic, interactive API documentation (provided by the <a href=\"https://github.com/swagger-api/swagger-ui\" target=\"_blank\">OpenAPI / Swagger UI</a>\n\n\n## CDN vs staticfiles\n\nYou are not required to put django ninja to `INSTALLED_APPS`. In that case the interactive UI is hosted by CDN.\n\nTo host docs (Js/css) from your own server - just put \"ninja\" to INSTALLED_APPS - in that case standard django staticfiles mechanics will host it.\n\n## Switch to Redoc\n\n\n```python\nfrom ninja import Redoc\n\napi = NinjaAPI(docs=Redoc())\n\n```\n\nThen you will see the alternative automatic documentation (provided by <a href=\"https://github.com/Redocly/redoc\" target=\"_blank\">Redoc</a>).\n\n## Changing docs display settings\n\nTo set some custom settings for Swagger or Redocs you can use `settings` param on the docs class\n\n```python\nfrom ninja import Redoc, Swagger\n\napi = NinjaAPI(docs=Swagger(settings={\"persistAuthorization\": True}))\n...\napi = NinjaAPI(docs=Redoc(settings={\"disableSearch\": True}))\n\n```\n\nSettings reference:\n\n - [Swagger configuration](https://swagger.io/docs/open-source-tools/swagger-ui/usage/configuration/)\n - [Redoc configuration](https://redocly.com/docs/api-reference-docs/configuration/functionality/)\n\n\n\n## Hiding docs\n\nIn case you do not need to display interactive documentation - set `docs_url` argument to `None`\n\n```python\napi = NinjaAPI(docs_url=None)\n```\n\n## Protecting docs\n\nTo protect docs with authentication (or decorate for some other use case) use `docs_decorator` argument:\n\n```python\nfrom django.contrib.admin.views.decorators import staff_member_required\n\napi = NinjaAPI(docs_decorator=staff_member_required)\n```\n\n## Extending OpenAPI Spec with custom attributes\n\nYou can extend OpenAPI spec with custom attributes, for example to add `termsOfService`\n\n```python\napi = NinjaAPI(\n   openapi_extra={\n       \"info\": {\n           \"termsOfService\": \"https://example.com/terms/\",\n       }\n   },\n   title=\"Demo API\",\n   description=\"This is a demo API with dynamic OpenAPI info section\"\n)\n```\n\n## Resolving the doc's url\n\nThe url for the api's documentation view can be reversed by referencing the view's name `openapi-view`.\n\nIn Python code, for example:\n```python\nfrom django.urls import reverse\n\nreverse('api-1.0.0:openapi-view')\n\n>>> '/api/docs'\n```\n\nIn a Django template, for example:\n```Html\n<a href=\"{% url 'api-1.0.0:openapi-view' %}\">API Docs</a>\n\n<a href=\"/api/docs\">API Docs</a>\n```\n\n## Creating custom docs viewer\n\nTo create your own view for OpenAPI - create a class inherited from DocsBase and overwrite `render_page` method:\n\n```python\nform ninja.openapi.docs import DocsBase\n\nclass MyDocsViewer(DocsBase)\n    def render_page(self, request, api):\n        ... # return http response\n\n...\n\napi = NinjaAPI(docs=MyDocsViewer())\n\n```\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": "docs/docs/guides/api-docs.md", "language": "markdown", "complexity": "medium", "star_count": 8401, "quality_score": 0.8799999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 118, "word_count": 352}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## Intro\n\nSince **version 3.1**, Django comes with **async views support**. This allows you run efficient concurrent views that are network and/or IO bound.\n\n```\npip install Django>=3.1 django-ninja\n```\n\nAsync views work more efficiently when it comes to:\n\n- calling external APIs over the network\n- executing/waiting for database queries\n- reading/writing from/to disk drives\n\n**Django Ninja** takes full advantage of async views and makes it very easy to work with them.\n\n## Quick example\n\n### Code\n\nLet's take an example.  We have an API operation that does some work (currently just sleeps for provided number of seconds) and returns a word:\n\n```python hl_lines=\"5\"\nimport time\n\n@api.get(\"/say-after\")\ndef say_after(request, delay: int, word: str):\n    time.sleep(delay)\n    return {\"saying\": word}\n```\n\nTo make this code asynchronous, all you have to do is add the **`async`** keyword to a function (and use async aware libraries for work processing - in our case we will replace the stdlib `sleep` with `asyncio.sleep`):\n\n```python hl_lines=\"1 4 5\"\nimport asyncio\n\n@api.get(\"/say-after\")\nasync def say_after(request, delay: int, word: str):\n    await asyncio.sleep(delay)\n    return {\"saying\": word}\n```\n\n### Run\n\nTo run this code you need an ASGI server like <a href=\"https://www.uvicorn.org/\" target=\"_blank\">Uvicorn</a> or <a href=\"https://github.com/django/daphne\" target=\"_blank\">Daphne</a>. Let's use Uvicorn for, example:\n\nTo install Uvicorn, use:\n\n```\npip install uvicorn\n```\n\nThen start the server:\n\n```\nuvicorn your_project.asgi:application --reload\n```\n\n> <small>\n> *Note: replace `your_project` with your project package name*<br>\n> *`--reload` flag used to automatically reload server if you do any changes to the code (do not use on production)*\n> </small>\n\n!!! note\n    You can run async views with `manage.py runserver`, but it does not work well with some libraries, so at this time (July 2020) it is recommended to use ASGI servers like Uvicorn or Daphne.\n\n### Test\n\nGo to your browser and open <a href=\"http://127.0.0.1:8000/api/say-after?delay=3&word=hello\" target=\"_blank\">http://127.0.0.1:8000/api/say-after?delay=3&word=hello</a> (**delay=3**)\nAfter a 3-second wait you should see the \"hello\" message.\n\nNow let's flood this operation with **100 parallel requests**:\n\n```\nab -c 100 -n 100 \"http://127.0.0.1:8000/api/say-after?delay=3&word=hello\"\n```\n\nwhich will result in something like this:\n\n```\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0    1   1.1      1       4\nProcessing:  3008 3063  16.2   3069    3082\nWaiting:     3008 3062  15.7   3068    3079\nTotal:       3008 3065  16.3   3070    3083\n\nPercentage of the requests served within a certain time (ms)\n  50%   3070\n  66%   3072\n  75%   3075\n  80%   3076\n  90%   3081\n  95%   3082\n  98%   3083\n  99%   3083\n 100%   3083 (longest request)\n```\n\nBased on the numbers, our service was able to handle each of the 100 concurrent requests with just a little overhead.\n\nTo achieve the same concurrency with WSGI and sync operations you would need to spin up about 10 workers with 10 threads each!\n\n## Mixing sync and async operations\n\nKeep in mind that you can use **both sync and async operations** in your project, and **Django Ninja** will route it automatically:\n\n```python hl_lines=\"2 7\"\n\n@api.get(\"/say-sync\")\ndef say_after_sync(request, delay: int, word: str):\n    time.sleep(delay)\n    return {\"saying\": word}\n\n@api.get(\"/say-async\")\nasync def say_after_async(request, delay: int, word: str):\n    await asyncio.sleep(delay)\n    return {\"saying\": word}\n```\n\n## Elasticsearch example\n\nLet's take a real world use case. For this example, let's use the latest version of Elasticsearch that now comes with async support:\n\n```\npip install elasticsearch>=7.8.0\n```\n\nAnd now instead of the `Elasticsearch` class, use the `AsyncElasticsearch` class and `await` the results:\n\n```python hl_lines=\"2 7 11 12\"\nfrom ninja import NinjaAPI\nfrom elasticsearch import AsyncElasticsearch\n\n\napi = NinjaAPI()\n\nes = AsyncElasticsearch()\n\n\n@api.get(\"/search\")\nasync def search(request, q: str):\n    resp = await es.search(\n        index=\"documents\", \n        body={\"query\": {\"query_string\": {\"query\": q}}},\n        size=20,\n    )\n    return resp[\"hits\"]\n```\n\n## Using ORM\n\nCurrently, certain key parts of Django are not able to operate safely in an async environment, as they have global state that is not coroutine-aware. These parts of Django are classified as ‚Äúasync-unsafe‚Äù, and are protected from execution in an async environment. **The ORM** is the main example, but there are other parts that are also protected in this way.\n\nLearn more about async safety here in the <a href=\"https://docs.djangoproject.com/en/stable/topics/async/#async-safety\" target=\"_blank\">official Django docs</a>.\n\nSo, if you do this:\n\n```python hl_lines=\"3\"\n@api.get(\"/blog/{post_id}\")\nasync def search(request, post_id: int):\n    blog = Blog.objects.get(pk=post_id)\n    ...\n```\n\nit throws an error. Until the async ORM is implemented, you can use the `sync_to_async()` adapter:\n\n```python hl_lines=\"1 3 9\"\nfrom asgiref.sync import sync_to_async\n\n@sync_to_async\ndef get_blog(post_id):\n    return Blog.objects.get(pk=post_id)\n\n@api.get(\"/blog/{post_id}\")\nasync def search(request, post_id: int):\n    blog = await get_blog(post_id)\n    ...\n```\n\nor even shorter:\n\n```python hl_lines=\"3\"\n@api.get(\"/blog/{post_id}\")\nasync def search(request, post_id: int):\n    blog = await sync_to_async(Blog.objects.get)(pk=post_id)\n    ...\n```\n\nThere is a common **GOTCHA**: Django querysets are lazily evaluated (database query happens only when you start iterating), so this will **not** work:\n\n```python\nall_blogs = await sync_to_async(Blog.objects.all)()\n# it will throw an error later when you try to iterate over all_blogs\n...\n```\n\nInstead, use evaluation (with `list`):\n\n```python\nall_blogs = await sync_to_async(list)(Blog.objects.all())\n...\n```\n\nSince Django **version 4.1**, Django comes with asynchronous versions of ORM operations.\nThese eliminate the need to use `sync_to_async` in most cases.\nThe async operations have the same names as their sync counterparts but are prepended with *a*. So using\nthe example above, you can rewrite it as:\n\n```python hl_lines=\"3\"\n@api.get(\"/blog/{post_id}\")\nasync def search(request, post_id: int):\n    blog = await Blog.objects.aget(pk=post_id)\n    ...\n```\n\nWhen working with querysets, use `async for` paired with list comprehension:\n\n```python\nall_blogs = [blog async for blog in Blog.objects.all()]\n...\n```\n\nLearn more about the async ORM interface in the <a href=\"https://docs.djangoproject.com/en/4.1/releases/4.1/#asynchronous-orm-interface\" target=\"_blank\">official Django docs</a>.\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": "docs/docs/guides/async-support.md", "language": "markdown", "complexity": "complex", "star_count": 8401, "quality_score": 0.82, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 225, "word_count": 889}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Authentication\n\n## Intro\n\n**Django Ninja** provides several tools to help you deal with authentication and authorization easily, rapidly, in a standard way, and without having to study and learn <a href=\"https://swagger.io/docs/specification/authentication/\" target=\"_blank\">all the security specifications</a>.\n\nThe core concept is that when you describe an API operation, you can define an authentication object.\n\n```python hl_lines=\"2 7\"\n{!./src/tutorial/authentication/code001.py!}\n```\n\nIn this example, the client will only be able to call the `pets` method if it uses Django session authentication (the default is cookie based), otherwise an HTTP-401 error will be returned.\n\nIf you need to authorize only a superuser, you can use `from ninja.security import django_auth_superuser` instead.\n\n## Automatic OpenAPI schema\n\nHere's an example where the client, in order to authenticate, needs to pass a header:\n\n`Authorization: Bearer supersecret`\n\n```python hl_lines=\"4 5 6 7 10\"\n{!./src/tutorial/authentication/bearer01.py!}\n```\n\nNow go to the docs at <a href=\"http://localhost:8000/api/docs\" target=\"_blank\">http://localhost:8000/api/docs</a>.\n\n\n![Swagger UI Auth](../img/auth-swagger-ui.png)\n\nNow, when you click the **Authorize** button, you will get a prompt to input your authentication token.\n\n![Swagger UI Auth](../img/auth-swagger-ui-prompt.png)\n\nWhen you do test calls, the Authorization header will be passed for every request.\n\n\n## Global authentication\n\nIn case you need to secure **all** methods of your API, you can pass the `auth` argument to the `NinjaAPI` constructor:\n\n\n```python hl_lines=\"11 19\"\nfrom ninja import NinjaAPI, Form\nfrom ninja.security import HttpBearer\n\n\nclass GlobalAuth(HttpBearer):\n    def authenticate(self, request, token):\n        if token == \"supersecret\":\n            return token\n\n\napi = NinjaAPI(auth=GlobalAuth())\n\n# @api.get(...)\n# def ...\n\n# @api.post(...)\n# def ...\n```\n\nAnd, if you need to overrule some of those methods, you can do that on the operation level again by passing the `auth` argument. In this example, authentication will be disabled for the `/token` operation:\n\n```python hl_lines=\"19\"\n{!./src/tutorial/authentication/global01.py!}\n```\n\n## Available auth options\n\n### Custom function\n\n\nThe \"`auth=`\" argument accepts any Callable object. **NinjaAPI** passes authentication only if the callable object returns a value that can be **converted to boolean `True`**. This return value will be assigned to the `request.auth` attribute.\n\n```python hl_lines=\"1 2 3 6\"\n{!./src/tutorial/authentication/code002.py!}\n```\n\n\n### API Key\n\nSome API's use API keys for authorization. An API key is a token that a client provides when making API calls to identify itself. The key can be sent in the query string:\n```\nGET /something?api_key=abcdef12345\n```\n\nor as a request header:\n\n```\nGET /something HTTP/1.1\nX-API-Key: abcdef12345\n```\n\nor as a cookie:\n\n```\nGET /something HTTP/1.1\nCookie: X-API-KEY=abcdef12345\n```\n\n**Django Ninja** comes with built-in classes to help you handle these cases.\n\n\n#### in Query\n\n```python hl_lines=\"1 2 5 6 7 8 9 10 11 12\"\n{!./src/tutorial/authentication/apikey01.py!}\n```\n\nIn this example we take a token from `GET['api_key']` and find a `Client` in the database that corresponds to this key. The Client instance will be set to the `request.auth` attribute.\n\nNote: **`param_name`** is the name of the GET parameter that will be checked for. If not set, the default of \"`key`\" will be used.\n\n\n#### in Header\n\n```python hl_lines=\"1 4\"\n{!./src/tutorial/authentication/apikey02.py!}\n```\n\n#### in Cookie\n\n```python hl_lines=\"1 4\"\n{!./src/tutorial/authentication/apikey03.py!}\n```\n\n### Django Session Authentication\n\n**Django Ninja** provides built-in session authentication classes that leverage Django's existing session framework:\n\n#### SessionAuth\n\nUses Django's default session authentication - authenticates any logged-in user:\n\n```python\nfrom ninja.security import SessionAuth\n\n@api.get(\"/protected\", auth=SessionAuth())\ndef protected_view(request):\n    return {\"user\": request.auth.username}\n```\n\n#### SessionAuthSuperUser\n\nAuthenticates only users with superuser privileges:\n\n```python\nfrom ninja.security import SessionAuthSuperUser\n\n@api.get(\"/admin-only\", auth=SessionAuthSuperUser())\ndef admin_view(request):\n    return {\"message\": \"Hello superuser!\"}\n```\n\n#### SessionAuthIsStaff\n\nAuthenticates users who are either superusers or staff members:\n\n```python\nfrom ninja.security import SessionAuthIsStaff\n\n@api.get(\"/staff-area\", auth=SessionAuthIsStaff())\ndef staff_view(request):\n    return {\"message\": \"Hello staff member!\"}\n```\n\nThese authentication classes automatically use Django's `SESSION_COOKIE_NAME` setting and check the user's authentication status through the standard Django session framework.\n\n\n\n### HTTP Bearer\n\n```python hl_lines=\"1 4 5 6 7\"\n{!./src/tutorial/authentication/bearer01.py!}\n```\n\n### HTTP Basic Auth\n\n```python hl_lines=\"1 4 5 6 7\"\n{!./src/tutorial/authentication/basic01.py!}\n```\n\n\n## Multiple authenticators\n\nThe **`auth`** argument also allows you to pass multiple authenticators:\n\n```python hl_lines=\"18\"\n{!./src/tutorial/authentication/multiple01.py!}\n```\n\nIn this case **Django Ninja** will first check the API key `GET`, and if not set or invalid will check the `header` key.\nIf both are invalid, it will raise an authentication error to the response.\n\n\n## Router authentication\n\nUse `auth` argument on Router to apply authenticator to all operations declared in it:\n\n```python\napi.add_router(\"/events/\", events_router, auth=BasicAuth())\n```\n\nor using router constructor\n```python\nrouter = Router(auth=BasicAuth())\n```\n\nThis overrides any API level authentication. To allow router operations to not use the API-level authentication by default, you can explicitly set the router's `auth=None`.\n\n\n## Custom exceptions\n\nRaising an exception that has an exception handler will return the response from that handler in\nthe same way an operation would:\n\n```python hl_lines=\"1 4\"\n{!./src/tutorial/authentication/bearer02.py!}\n```\n\n\n## Async authentication\n\n**Django Ninja** has basic support for asynchronous authentication. While the default authentication classes are not async-compatible, you can still define your custom asynchronous authentication callables and pass them in using `auth`.\n\n```python\nasync def async_auth(request):\n    ...\n\n\n@api.get(\"/pets\", auth=async_auth)\ndef pets(request):\n    ...\n```\n\n\nSee [Handling errors](errors.md) for more information.\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": "docs/docs/guides/authentication.md", "language": "markdown", "complexity": "complex", "star_count": 8401, "quality_score": 0.82, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 240, "word_count": 817}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Handling errors\n\n**Django Ninja** allows you to install custom exception handlers to deal with how you return responses when errors or handled exceptions occur.\n\n## Custom exception handlers\n\nLet's say you are making API that depends on some external service that is designed to be unavailable at some moments. Instead of throwing default 500 error upon exception - you can handle the error and give some friendly response back to the client (to come back later)\n\nTo achieve that you need:\n\n1. create some exception (or use existing one)\n2. use api.exception_handler decorator\n\n\nExample:\n\n\n```python hl_lines=\"9 10\"\napi = NinjaAPI()\n\nclass ServiceUnavailableError(Exception):\n    pass\n\n\n# initializing handler\n\n@api.exception_handler(ServiceUnavailableError)\ndef service_unavailable(request, exc):\n    return api.create_response(\n        request,\n        {\"message\": \"Please retry later\"},\n        status=503,\n    )\n\n\n# some logic that throws exception\n\n@api.get(\"/service\")\ndef some_operation(request):\n    if random.choice([True, False]):\n        raise ServiceUnavailableError()\n    return {\"message\": \"Hello\"}\n\n```\n\nException handler function takes 2 arguments:\n\n - **request** - Django http request\n - **exc** - actual exception\n\nfunction must return http response\n\n## Override the default exception handlers\n\n**Django Ninja** registers default exception handlers for the types shown below.\nYou can register your own handlers with `@api.exception_handler` to override the default handlers.\n\n#### `ninja.errors.AuthenticationError`\n\nRaised when authentication data is not valid\n\n#### `ninja.errors.AuthorizationError`\n\nRaised when authentication data is valid, but doesn't allow you to access the resource\n\n#### `ninja.errors.ValidationError`\n\nRaised when request data does not validate\n\n#### `ninja.errors.HttpError`\n\nUsed to throw http error with status code from any place of the code\n\n#### `django.http.Http404`\n \n Django's default 404 exception (can be returned f.e. with `get_object_or_404`)\n\n#### `Exception`\n \nAny other unhandled exception by application.\n\nDefault behavior \n \n  - **if `settings.DEBUG` is `True`** - returns a traceback in plain text (useful when debugging in console or swagger UI)\n  - **else** - default django exception handler mechanism is used (error logging, email to ADMINS)\n\n\n## Customizing request validation errors\n\nRequests that fail validation raise `ninja.errors.ValidationError` (not to be confused with `pydantic.ValidationError`).\n`ValidationError`s have a default exception handler that returns a 422 (Unprocessable Content) JSON response of the form:\n```json\n{\n    \"detail\": [ ... ]\n}\n```\n\nYou can change this behavior by overriding the default handler for `ValidationError`s:\n\n```python hl_lines=\"1 4\"\nfrom ninja.errors import ValidationError\n...\n\n@api.exception_handler(ValidationError)\ndef validation_errors(request, exc):\n    return HttpResponse(\"Invalid input\", status=422)\n```\n\nIf you need even more control over validation errors (for example, if you need to reference the schema associated with\nthe model that failed validation), you can supply your own `validation_error_from_error_contexts` in a `NinjaAPI` subclass:\n\n```python hl_lines=\"4\"\nfrom ninja.errors import ValidationError, ValidationErrorContext\nfrom typing import Any, Dict, List\n\nclass CustomNinjaAPI(NinjaAPI):\n    def validation_error_from_error_contexts(\n        self, error_contexts: List[ValidationErrorContext],\n    ) -> ValidationError:\n        custom_error_infos: List[Dict[str, Any]] = []\n        for context in error_contexts:\n            model = context.model\n            pydantic_schema = model.__pydantic_core_schema__\n            param_source = model.__ninja_param_source__\n            for e in context.pydantic_validation_error.errors(\n                include_url=False, include_context=False, include_input=False\n            ):\n                custom_error_info = {\n                # TODO: use `e`, `param_source`, and `pydantic_schema` as desired\n                }\n                custom_error_infos.append(custom_error_info)\n        return ValidationError(custom_error_infos)\n\napi = CustomNinjaAPI()\n```\n\nNow each `ValidationError` raised during request validation will contain data from your `validation_error_from_error_contexts`.\n\n\n## Throwing HTTP responses with exceptions\n\nAs an alternative to custom exceptions and writing handlers for it - you can as well throw http exception that will lead to returning a http response with desired code\n\n\n```python\nfrom ninja.errors import HttpError\n\n@api.get(\"/some/resource\")\ndef some_operation(request):\n    if True:\n        raise HttpError(503, \"Service Unavailable. Please retry later.\")\n\n```\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": "docs/docs/guides/errors.md", "language": "markdown", "complexity": "medium", "star_count": 8401, "quality_score": 0.8799999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 154, "word_count": 539}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Request Body\n\nRequest bodies are typically used with ‚Äúcreate‚Äù and ‚Äúupdate‚Äù operations (POST, PUT, PATCH).\nFor example, when creating a resource using POST or PUT, the request body usually contains the representation of the resource to be created.\n\nTo declare a **request body**, you need to use **Django Ninja `Schema`**.\n\n!!! info\n    Under the hood **Django Ninja** uses <a href=\"https://pydantic-docs.helpmanual.io/\" class=\"external-link\" target=\"_blank\">Pydantic</a> models with all their power and benefits.\n    The alias `Schema` was chosen to avoid confusion in code when using Django models, as Pydantic's model class is called Model by default, and conflicts with Django's Model class.\n\n## Import Schema\n\nFirst, you need to import `Schema` from `ninja`:\n\n```python hl_lines=\"2\"\n{!./src/tutorial/body/code01.py!}\n```\n\n## Create your data model\n\nThen you declare your data model as a class that inherits from `Schema`.\n\nUse standard Python types for all the attributes:\n\n```python hl_lines=\"5 6 7 8 9\"\n{!./src/tutorial/body/code01.py!}\n```\n\nNote: if you use **`None`** as the default value for an attribute, it will become optional in the request body.\nFor example, this model above declares a JSON \"`object`\" (or Python `dict`) like:\n\n```JSON\n{\n    \"name\": \"Katana\",\n    \"description\": \"An optional description\",\n    \"price\": 299.00,\n    \"quantity\": 10\n}\n```\n\n...as `description` is optional (with a default value of `None`), this JSON \"`object`\" would also be valid:\n\n```JSON\n{\n    \"name\": \"Katana\",\n    \"price\": 299.00,\n    \"quantity\": 10\n}\n```\n\n## Declare it as a parameter\n\nTo add it to your *path operation*, declare it the same way you declared the path and query parameters:\n\n```python hl_lines=\"13\"\n{!./src/tutorial/body/code01.py!}\n```\n\n... and declare its type as the model you created, `Item`.\n\n## Results\n\nWith just that Python type declaration, **Django Ninja** will:\n\n* Read the body of the request as JSON.\n* Convert the corresponding types (if needed).\n* Validate the data.\n    * If the data is invalid, it will return a nice and meaningful error, indicating exactly where and what the incorrect data was.\n* Give you the received data in the parameter `item`.\n    * Because you declared it in the function to be of type `Item`, you will also have all the editor support\n      (completion, etc.) for all the attributes and their types.\n* Generate <a href=\"https://json-schema.org\" class=\"external-link\" target=\"_blank\">JSON Schema</a> definitions for\n  your models, and you can also use them anywhere else you like if it makes sense for your project.\n* Those schemas will be part of the generated OpenAPI schema, and used by the automatic documentation <abbr title=\"User Interfaces\">UI's</abbr>.\n\n## Automatic docs\n\nThe JSON Schemas of your models will be part of your OpenAPI generated schema, and will be shown in the interactive API docs:\n\n![Openapi schema](../../img/body-schema-doc.png)\n\n... and they will be also used in the API docs inside each *path operation* that needs them:\n\n![Openapi schema](../../img/body-schema-doc2.png)\n\n## Editor support\n\nIn your editor, inside your function you will get type hints and completion everywhere (this wouldn't happen if you received a `dict` instead of a Schema object):\n\n![Type hints](../../img/body-editor.gif)\n\n\nThe previous screenshots were taken with <a href=\"https://code.visualstudio.com\" class=\"external-link\" target=\"_blank\">Visual Studio Code</a>.\n\nYou would get the same editor support with <a href=\"https://www.jetbrains.com/pycharm/\" class=\"external-link\" target=\"_blank\">PyCharm</a> and most of the other Python editors.\n\n\n## Request body + path parameters\n\nYou can declare path parameters **and** body requests at the same time.\n\n**Django Ninja** will recognize that the function parameters that match path parameters should be **taken from the path**, and that function parameters that are declared with `Schema` should be **taken from the request body**.\n\n```python hl_lines=\"11 12\"\n{!./src/tutorial/body/code02.py!}\n```\n\n## Request body + path + query parameters\n\nYou can also declare **body**, **path** and **query** parameters, all at the same time.\n\n**Django Ninja** will recognize each of them and take the data from the correct place.\n\n```python hl_lines=\"11 12\"\n{!./src/tutorial/body/code03.py!}\n```\n\nThe function parameters will be recognized as follows:\n\n* If the parameter is also declared in the **path**, it will be used as a path parameter.\n* If the parameter is of a **singular type** (like `int`, `float`, `str`, `bool`, etc.), it will be interpreted as a **query** parameter.\n* If the parameter is declared to be of the type of **Schema** (or Pydantic `BaseModel`), it will be interpreted as a request **body**.\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": "docs/docs/guides/input/body.md", "language": "markdown", "complexity": "medium", "star_count": 8401, "quality_score": 0.7899999999999999, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 124, "word_count": 687}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# File uploads\n\nHandling files are no different from other parameters.\n\n```python hl_lines=\"1 2 5\"\nfrom ninja import NinjaAPI, File\nfrom ninja.files import UploadedFile\n\n@api.post(\"/upload\")\ndef upload(request, file: File[UploadedFile]):\n    data = file.read()\n    return {'name': file.name, 'len': len(data)}\n```\n\n\n`UploadedFile` is an alias to [Django's UploadFile](https://docs.djangoproject.com/en/stable/ref/files/uploads/#django.core.files.uploadedfile.UploadedFile) and has all the methods and attributes to access the uploaded file:\n\n - read()\n - multiple_chunks(chunk_size=None)\n - chunks(chunk_size=None)\n - name\n - size\n - content_type\n - content_type_extra\n - charset\n - etc.\n\n## Uploading array of files\n\nTo **upload several files** at the same time, just declare a `List` of `UploadedFile`:\n\n\n```python hl_lines=\"1 6\"\nfrom typing import List\nfrom ninja import NinjaAPI, File\nfrom ninja.files import UploadedFile\n\n@api.post(\"/upload-many\")\ndef upload_many(request, files: File[List[UploadedFile]]):\n    return [f.name for f in files]\n```\n\n## Uploading files with extra fields\n\nNote: The HTTP protocol does not allow you to send files in `application/json` format by default (unless you encode it somehow to JSON on client side)\n\nTo send files along with some extra attributes, you need to send bodies with `multipart/form-data` encoding. You can do it by simply marking fields with `Form`:\n\n```python hl_lines=\"14\"\nfrom ninja import NinjaAPI, Schema, UploadedFile, Form, File\nfrom datetime import date\n\napi = NinjaAPI()\n\n\nclass UserDetails(Schema):\n    first_name: str\n    last_name: str\n    birthdate: date\n\n\n@api.post('/users')\ndef create_user(request, details: Form[UserDetails], file: File[UploadedFile]):\n    return [details.dict(), file.name]\n\n```\n\nNote: in this case all fields should be send as form fields\n\nYou can as well send payload in single field as JSON - just remove the Form mark from:\n\n```python\n@api.post('/users')\ndef create_user(request, details: UserDetails, file: File[UploadedFile]):\n    return [details.dict(), file.name]\n\n```\n\nthis will expect from the client side to send data as `multipart/form-data with 2 fields:\n  \n  - details: JSON as string\n  - file: file\n\n\n### List of files with extra info\n\n```python\n@api.post('/users')\ndef create_user(request, details: Form[UserDetails], files: File[list[UploadedFile]]):\n    return [details.dict(), [f.name for f in files]]\n```\n\n### Optional file input\n\nIf you would like the file input to be optional, all that you have to do is to pass `None` to the `File` type, like so:\n\n```python\n@api.post('/users')\ndef create_user(request, details: Form[UserDetails], avatar: File[UploadedFile] = None):\n    user = add_user_to_database(details)\n    if avatar is not None:\n        set_user_avatar(user)\n```\n\n\n## Handling request.FILES in PUT/PATCH Requests\n\n**Problem**\n\n```python\n@api.put(\"/upload\") # !!!!\ndef upload(request, file: File[UploadedFile]):\n   ...\n```\n\nFor some [historical reasosns Django‚Äôs](https://groups.google.com/g/django-users/c/BeBKj_6qNsc) `request.FILES` is populated only for POST requests by default. When using HTTP PUT or PATCH methods with file uploads (e.g., multipart/form-data), request.FILES will not contain uploaded files. This is a known Django behavior, not specific to Django Ninja.\n\nAs a result, views expecting files in PUT or PATCH requests may not behave correctly, since request.FILES will be empty.\n\n**Solution**\n\nDjango Ninja provides a built-in middleware to automatically fix this behavior:\n`ninja.compatibility.files.fix_request_files_middleware`\n\nThis middleware will manually parse multipart/form-data for PUT and PATCH requests and populate request.FILES, making file uploads work as expected across all HTTP methods.\n\n**Usage**\n\nTo enable the middleware, add the following to your Django settings:\n\n```python\nMIDDLEWARE = [\n    # ... your existing middleware ...\n    \"ninja.compatibility.files.fix_request_files_middleware\",\n]\n```\n\n**Auto-detection**\n\nWhen Django Ninja detects a PUT or PATCH  etc methods with multipart/form-data and expected FILES  - it will throw an error message suggesting you install the compatibility middleware:\n\n\nNote: This middleware does not interfere with normal POST behavior or any other methods.\n\n\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": "docs/docs/guides/input/file-params.md", "language": "markdown", "complexity": "medium", "star_count": 8401, "quality_score": 0.85, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 146, "word_count": 539}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Filtering\n\nIf you want to allow the user to filter your querysets by a number of different attributes, it makes sense\nto encapsulate your filters into a `FilterSchema` class. `FilterSchema` is a regular `Schema`, so it's using all the\nnecessary features of Pydantic, but it also adds some bells and whistles that ease the translation of the user-facing filtering\nparameters into database queries. \n\nStart off with defining a subclass of `FilterSchema`:\n\n```python hl_lines=\"6 7 8\"\nfrom ninja import FilterSchema, Field\nfrom typing import Optional\n\n\nclass BookFilterSchema(FilterSchema):\n    name: Optional[str] = None\n    author: Optional[str] = None\n    created_after: Optional[datetime] = None\n```\n\n\nNext, use this schema in conjunction with `Query` in your API handler:\n```python hl_lines=\"2\"\n@api.get(\"/books\")\ndef list_books(request, filters: BookFilterSchema = Query(...)):\n    books = Book.objects.all()\n    books = filters.filter(books)\n    return books\n```\n\nJust like described in [defining query params using schema](./query-params.md#using-schema), Django Ninja converts the fields\ndefined in `BookFilterSchema` into query parameters.\n\nYou can use a shorthand one-liner `.filter()` to apply those filters to your queryset:\n```python hl_lines=\"4\"\n@api.get(\"/books\")\ndef list_books(request, filters: BookFilterSchema = Query(...)):\n    books = Book.objects.all()\n    books = filters.filter(books)\n    return books\n```\n\nUnder the hood, `FilterSchema` converts its fields into [Q expressions](https://docs.djangoproject.com/en/3.1/topics/db/queries/#complex-lookups-with-q-objects) which it then combines and uses to filter your queryset.\n\n\nAlternatively to using the `.filter` method, you can get the prepared `Q`-expression and perform the filtering yourself.\nThat can be useful, when you have some additional queryset filtering on top of what you expose to the user through the API:\n```python hl_lines=\"5 8\"\n@api.get(\"/books\")\ndef list_books(request, filters: BookFilterSchema = Query(...)):\n\n    # Never serve books from inactive publishers and authors\n    q = Q(author__is_active=True) | Q(publisher__is_active=True)\n    \n    # But allow filtering the rest of the books\n    q &= filters.get_filter_expression()\n    return Book.objects.filter(q)\n```\n\nBy default, the filters will behave the following way:\n\n* `None` values will be ignored and not filtered against;\n* Every non-`None` field will be converted into a `Q`-expression based on the `Field` definition of each field;\n* All `Q`-expressions will be merged into one using `AND` logical operator;\n* The resulting `Q`-expression is used to filter the queryset and return you a queryset with a `.filter` clause applied.\n\n## Customizing Fields\nBy default, `FilterSet` will use the field names to generate Q expressions:\n```python\nclass BookFilterSchema(FilterSchema):\n    name: Optional[str] = None\n```\nThe `name` field will be converted into `Q(name=...)` expression.\n\nWhen your database lookups are more complicated than that, you can explicitly specify them in the field definition using a `\"q\"` kwarg:\n```python hl_lines=\"2\"\nclass BookFilterSchema(FilterSchema):\n    name: Optional[str] = Field(None, q='name__icontains')\n```\nYou can even specify multiple lookup keyword argument names as a list:\n```python hl_lines=\"2 3 4\"\nclass BookFilterSchema(FilterSchema):\n    search: Optional[str] = Field(None, q=['name__icontains',\n                                     'author__name__icontains',\n                                     'publisher__name__icontains'])\n```\nAnd to make generic fields, you can make the field name implicit by skipping it:\n```python hl_lines=\"2\"\nIContainsField = Annotated[Optional[str], Field(None, q='__icontains')]\n\nclass BookFilterSchema(FilterSchema):\n    name: IContainsField\n```\nBy default, field-level expressions are combined using `\"OR\"` connector, so with the above setup, a query parameter `?search=foobar` will search for books that have \"foobar\" in either of their name, author or publisher.\n\n\n## Combining expressions\nBy default,\n\n* Field-level expressions are joined together using `OR` operator.\n* The fields themselves are joined together using `AND` operator.\n\nSo, with the following `FilterSchema`...\n```python\nclass BookFilterSchema(FilterSchema):\n    search: Optional[str] = Field(None, q=['name__icontains', 'author__name__icontains'])\n    popular: Optional[bool] = None\n```\n...and the following query parameters from the user\n```\nhttp://localhost:8000/api/books?search=harry&popular=true\n```\nthe `FilterSchema` instance will look for popular books that have `harry` in the book's _or_ author's name. \n\n\nYou can customize this behavior using an `expression_connector` argument in field-level and class-level definition:\n```python hl_lines=\"3 7\"\nclass BookFilterSchema(FilterSchema):\n    active: Optional[bool] = Field(None, q=['is_active', 'publisher__is_active'],\n                                   expression_connector='AND')\n    name: Optional[str] = Field(None, q='name__icontains')\n    \n    class Config:\n        expression_connector = 'OR'\n```\n\nAn expression connector can take the values of `\"OR\"`, `\"AND\"` and `\"XOR\"`, but the latter is only [supported](https://docs.djangoproject.com/en/4.1/ref/models/querysets/#xor) in Django starting with 4.1.\n\nNow, a request with these query parameters \n```\nhttp://localhost:8000/api/books?name=harry&active=true\n```\n...shall search for books that have `harry` in their name _or_ are active themselves _and_ are published by active publishers.\n\n\n## Filtering by Nones\nYou can make the `FilterSchema` treat `None` as a valid value that should be filtered against.\n\nThis can be done on a field level with a `ignore_none` kwarg:\n```python hl_lines=\"3\"\nclass BookFilterSchema(FilterSchema):\n    name: Optional[str] = Field(None, q='name__icontains')\n    tag: Optional[str] = Field(None, q='tag', ignore_none=False)\n```\n\nThis way when no other value for `\"tag\"` is provided by the user, the filtering will always include a condition `tag=None`.\n\nYou can also specify this settings for all fields at the same time in the Config:\n```python hl_lines=\"6\"\nclass BookFilterSchema(FilterSchema):\n    name: Optional[str] = Field(None, q='name__icontains')\n    tag: Optional[str] = Field(None, q='tag', ignore_none=False)\n    \n    class Config:\n        ignore_none = False\n```\n\n\n## Custom expressions\nSometimes you might want to have complex filtering scenarios that cannot be handled by individual Field annotations.\nFor such cases you can implement your field filtering logic as a custom method. Simply define a method called `filter_<fieldname>` which takes a filter value and returns a Q expression:\n\n```python hl_lines=\"5\"\nclass BookFilterSchema(FilterSchema):\n    tag: Optional[str] = None\n    popular: Optional[bool] = None\n    \n    def filter_popular(self, value: bool) -> Q:\n        return Q(view_count__gt=1000) | Q(download_count__gt=100) if value else Q()\n```\nSuch field methods take precedence over what is specified in the `Field()` definition of the corresponding fields.\n\nIf that is not enough, you can implement your own custom filtering logic for the entire `FilterSet` class in a `custom_expression` method:\n\n```python hl_lines=\"5\"\nclass BookFilterSchema(FilterSchema):\n    name: Optional[str] = None\n    popular: Optional[bool] = None\n\n    def custom_expression(self) -> Q:\n        q = Q()\n        if self.name:\n            q &= Q(name__icontains=self.name)\n        if self.popular:\n            q &= (\n                Q(view_count__gt=1000) |\n                Q(downloads__gt=100) |\n                Q(tag='popular')\n            )\n        return q\n```\nThe `custom_expression` method takes precedence over any other definitions described earlier, including `filter_<fieldname>` methods.\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": "docs/docs/guides/input/filtering.md", "language": "markdown", "complexity": "medium", "star_count": 8401, "quality_score": 0.85, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 193, "word_count": 935}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Form data\n\n**Django Ninja** also allows you to parse and validate `request.POST` data\n(aka `application/x-www-form-urlencoded` or `multipart/form-data`).\n\n## Form Data as params \n\n```python hl_lines=\"1 4\"\nfrom ninja import NinjaAPI, Form\n\n@api.post(\"/login\")\ndef login(request, username: Form[str], password: Form[str]):\n    return {'username': username, 'password': '*****'}\n```\n\nNote the following:\n\n1) You need to import the `Form` class from `ninja`\n```python\nfrom ninja import Form\n```\n\n2) Use `Form` as default value for your parameter:\n```python\nusername: Form[str]\n```\n\n## Using a Schema\n\nIn a similar manner to [Body](body.md#declare-it-as-a-parameter), you can use\na Schema to organize your parameters.\n\n```python hl_lines=\"12\"\n{!./src/tutorial/form/code01.py!}\n```\n\n## Request form + path + query parameters\n\nIn a similar manner to [Body](body.md#request-body-path-query-parameters), you can use\nForm data in combination with other parameter sources.\n\nYou can declare query **and** path **and** form field, **and** etc... parameters at the same time.\n\n**Django Ninja** will recognize that the function parameters that match path\nparameters should be **taken from the path**, and that function parameters that\nare declared with `Form(...)` should be **taken from the request form fields**, etc.\n\n```python hl_lines=\"12\"\n{!./src/tutorial/form/code02.py!}\n```\n## Mapping Empty Form Field to Default\n\nForm fields that are optional, are often sent with an empty value. This value is\ninterpreted as an empty string, and thus may fail validation for fields such as `int` or `bool`.\n\nThis can be fixed, as described in the Pydantic docs, by using\n[Generic Classes as Types](https://pydantic-docs.helpmanual.io/usage/types/#generic-classes-as-types).\n\n```python hl_lines=\"15 16 23-25\"\n{!./src/tutorial/form/code03.py!}\n```\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": "docs/docs/guides/input/form-params.md", "language": "markdown", "complexity": "medium", "star_count": 8401, "quality_score": 0.82, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 62, "word_count": 241}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# HTTP Methods\n\n## Defining operations\n\nAn `operation` can be one of the following [HTTP methods](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods):\n\n- GET\n- POST\n- PUT\n- DELETE\n- PATCH\n\n**Django Ninja** comes with a decorator for each operation:\n\n```python hl_lines=\"1 5 9 13 17\"\n@api.get(\"/path\")\ndef get_operation(request):\n    ...\n\n@api.post(\"/path\")\ndef post_operation(request):\n    ...\n\n@api.put(\"/path\")\ndef put_operation(request):\n    ...\n\n@api.delete(\"/path\")\ndef delete_operation(request):\n    ...\n\n@api.patch(\"/path\")\ndef patch_operation(request):\n    ...\n```\n\nSee the [operations parameters](../../reference/operations-parameters.md)\nreference docs for information on what you can pass to any of these decorators.\n\n## Handling multiple methods\n\nIf you need to handle multiple methods with a single function for a given path,\nyou can use the `api_operation` decorator:\n\n```python hl_lines=\"1\"\n@api.api_operation([\"POST\", \"PATCH\"], \"/path\")\ndef mixed_operation(request):\n    ...\n```\n\nThis feature can also be used to implement other HTTP methods that don't have\ncorresponding **Django Ninja** methods, such as `HEAD` or `OPTIONS`.\n\n```python hl_lines=\"1\"\n@api.api_operation([\"HEAD\", \"OPTIONS\"], \"/path\")\ndef mixed_operation(request):\n    ...\n```\n", "metadata": {"source": "github.com/vitalik/django-ninja", "file_path": "docs/docs/guides/input/operations.md", "language": "markdown", "complexity": "simple", "star_count": 8401, "quality_score": 0.73, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 59, "word_count": 146}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: \"Documentation Update\"\ndescription: Create an issue for documentation changes\ntitle: \"Docs: <title>\"\nlabels:\n  - \"Documentation :books:\"\nprojects:\n  - \"litestar-org/16\"\nbody:\n  - type: textarea\n    id: summary\n    attributes:\n      label: \"Summary\"\n      description: Provide a brief summary of your feature request\n      placeholder: Describe in a few lines your feature request\n    validations:\n      required: true\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".github/ISSUE_TEMPLATE/DOCS.yml", "language": "yaml", "complexity": "simple", "star_count": 6976, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 17, "word_count": 50}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: \"Feature Request\"\ndescription: Create an issue for a new feature request\ntitle: \"Enhancement: <title>\"\nlabels:\n  - \"Enhancement\"\nprojects:\n  - \"litestar-org/16\"\nbody:\n  - type: textarea\n    id: summary\n    attributes:\n      label: \"Summary\"\n      description: Provide a brief summary of your feature request\n      placeholder: Describe in a few lines your feature request\n    validations:\n      required: true\n  - type: textarea\n    id: basic_example\n    attributes:\n      label: \"Basic Example\"\n      description: Indicate here some basic examples of your feature.\n      placeholder: Provide some basic example of your feature request\n    validations:\n      required: false\n  - type: textarea\n    id: drawbacks\n    attributes:\n      label: \"Drawbacks and Impact\"\n      description: What are the drawbacks or impacts of your feature request?\n      placeholder: Describe any the drawbacks or impacts of your feature request\n    validations:\n      required: false\n  - type: textarea\n    id: unresolved_question\n    attributes:\n      label: \"Unresolved questions\"\n      description: What, if any, unresolved questions do you have about your feature request?\n      placeholder: Identify any unresolved issues.\n    validations:\n      required: false\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".github/ISSUE_TEMPLATE/REQUEST.yml", "language": "yaml", "complexity": "simple", "star_count": 6976, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 41, "word_count": 146}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "blank_issues_enabled: false\ncontact_links:\n  - name: Litestar Documentation\n    url: https://docs.litestar.dev/\n    about: Official Litestar documentation - please check here before opening an issue.\n  - name: Litestar Website\n    url: https://litestar.dev/\n    about: Main Litestar website - for details about Litestar's projects.\n  - name: Discord\n    url: https://discord.gg/litestar\n    about: Join our Discord community to chat or get in touch with the maintainers.\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".github/ISSUE_TEMPLATE/config.yml", "language": "yaml", "complexity": "simple", "star_count": 6976, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 12, "word_count": 56}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!--\nBy submitting this pull request, you agree to:\n- follow [Litestar's Code of Conduct](https://github.com/litestar-org/.github/blob/main/CODE_OF_CONDUCT.md)\n- follow [Litestar's contribution guidelines](https://github.com/litestar-org/.github/blob/main/CONTRIBUTING.md)\n- follow the [PSFs's Code of Conduct](https://www.python.org/psf/conduct/)\n-->\n## Description\n\n-\n\n<!--\nPlease add in issue numbers this pull request will close, if applicable\nExamples: Fixes #4321 or Closes #1234\n\nEnsure you are using a supported keyword to properly link an issue:\nhttps://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword\n-->\n## Closes\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".github/PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "medium", "star_count": 6976, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 19, "word_count": 66}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: v1\n\nlabels:\n  # -- types -------------------------------------------------------------------\n  - label: 'type/feat'\n    sync: true\n    matcher:\n      title: '^feat(\\([^)]+\\))?!?:'\n\n  - label: 'type/bug'\n    sync: true\n    matcher:\n      title: '^fix(\\([^)]+\\))?!?:'\n\n  - label: 'type/docs'\n    sync: true\n    matcher:\n      title: '^docs(\\([^)]+\\))?:'\n\n  - label: 'Breaking üî®'\n    sync: true\n    matcher:\n      title: '^(feat|fix)(\\([^)]+\\))?!:'\n\n  # -- distinct areas ----------------------------------------------------------\n  - label: '3.x'\n    sync: true\n    matcher:\n      baseBranch: '^v3$'\n\n  - label: 'area/docs'\n    sync: true\n    matcher:\n      files:\n        any: ['docs/*', 'docs/**/*', '**/*.rst', '**/*.md']\n\n  - label: 'area/unit-tests'\n    sync: true\n    matcher:\n      files:\n        any: ['test/unit/*', 'test/unit/**/*']\n\n  - label: 'area/end-to-end-tests'\n    sync: true\n    matcher:\n      files:\n        any: ['test/e2e/*', 'test/e2e/**/*']\n\n  - label: 'area/test-apps'\n    sync: true\n    matcher:\n      files:\n        any: ['test/test_apps/*', 'test/test_apps/**/*']\n\n  - label: 'area/docs'\n    sync: true\n    matcher:\n      files:\n        any: ['docs/*', 'docs/**/*', '**/*.rst', '**/*.md']\n\n  - label: 'area/unit-tests'\n    sync: true\n    matcher:\n      files:\n        any: ['test/unit/*', 'test/unit/**/*']\n\n  - label: 'area/end-to-end-tests'\n    sync: true\n    matcher:\n      files:\n        any: ['test/e2e/*', 'test/e2e/**/*']\n\n  - label: 'area/test-apps'\n    sync: true\n    matcher:\n      files:\n        any: ['test/test_apps/*', 'test/test_apps/**/*']\n\n  - label: 'area/ci'\n    sync: true\n    matcher:\n      files:\n        any: ['.github/**/*', 'codecov.yml', 'pre-commit-config.yaml', 'sonar-project.properties']\n\n  - label: 'area/dependencies'\n    sync: true\n    matcher:\n      files:\n        any: ['pyproject.toml', '*.lock']\n\n  - label: 'area/enums'\n    sync: true\n    matcher:\n      files: ['litestar/enums.py']\n\n  - label: 'area/background-tasks'\n    sync: true\n    matcher:\n      files: ['litestar/background_tasks.py']\n\n  - label: 'area/constants'\n    sync: true\n    matcher:\n      files: ['litestar/constants.py']\n\n  - label: 'area/concurrency'\n    sync: true\n    matcher:\n      files: ['litestar/concurrency.py']\n\n  - label: 'area/parsers'\n    sync: true\n    matcher:\n      files: ['litestar/_parsers.py']\n\n  - label: 'area/layers'\n    sync: true\n    matcher:\n      files: ['litestar/_layers/*']\n\n  - label: 'area/multipart'\n    sync: true\n    matcher:\n      files: ['litestar/_multipart.py']\n\n  - label: 'area/di'\n    sync: true\n    matcher:\n      files: ['litestar/di.py']\n\n  - label: 'area/file-system'\n    sync: true\n    matcher:\n      files: ['litestar/file_system.py']\n\n  - label: 'area/controller'\n    sync: true\n    matcher:\n      files: ['litestar/controller.py']\n\n  - label: 'area/serialization'\n    sync: true\n    matcher:\n      files: ['litestar/serialization/*']\n\n  - label: 'area/params'\n    sync: true\n    matcher:\n      files: ['litestar/params.py']\n\n  - label: 'area/template'\n    sync: true\n    matcher:\n      files: ['litestar/template/*']\n\n  - label: 'area/events'\n    sync: true\n    matcher:\n      files: ['litestar/events/*']\n\n  - label: 'area/router'\n    sync: true\n    matcher:\n      files: ['litestar/router.py']\n\n  - label: 'area/exceptions'\n    sync: true\n    matcher:\n      files: ['litestar/exceptions/*']\n\n  - label: 'area/static-files'\n    sync: true\n    matcher:\n      files: ['litestar/static_files/*']\n\n  - label: 'area/signature'\n    sync: true\n    matcher:\n      files: ['litestar/_signature/*']\n\n  - label: 'area/plugins'\n    sync: true\n    matcher:\n      files: ['litestar/plugins/*']\n\n  - label: 'area/stores'\n    sync: true\n    matcher:\n      files: ['litestar/stores/*']\n\n  - label: 'area/logging'\n    sync: true\n    matcher:\n      files: ['litestar/logging/*']\n\n  - label: 'area/connection'\n    sync: true\n    matcher:\n      files: ['litestar/connection/*']\n\n  - label: 'area/asgi'\n    sync: true\n    matcher:\n      files: ['litestar/_asgi/*']\n\n  - label: 'area/types'\n    sync: true\n    matcher:\n      files: ['litestar/types/*']\n\n  - label: 'area/kwargs'\n    sync: true\n    matcher:\n      files: ['litestar/_kwargs/*']\n\n  - label: 'area/datastructures'\n    sync: true\n    matcher:\n      files: ['litestar/datastructures/*']\n\n  - label: 'area/channels'\n    sync: true\n    matcher:\n      files: ['litestar/channels/*']\n\n  - label: 'area/response'\n    sync: true\n    matcher:\n      files: ['litestar/response/*']\n\n  - label: 'area/repository'\n    sync: true\n    matcher:\n      files: ['litestar/repository/*']\n\n  - label: 'area/security'\n    sync: true\n    matcher:\n      files: ['litestar/security/*']\n\n  - label: 'area/dto'\n    sync: true\n    matcher:\n      files: ['litestar/dto/*']\n\n  - label: 'area/testing'\n    sync: true\n    matcher:\n      files: ['litestar/testing/*']\n\n  - label: 'area/openapi'\n    sync: true\n    matcher:\n      files: ['litestar/_openapi/*']\n\n  - label: 'area/middleware'\n    sync: true\n    matcher:\n      files: ['litestar/middleware/*']\n\n  - label: 'area/handlers'\n    sync: true\n    matcher:\n      files: ['litestar/handlers/*']\n\n  - label: 'area/contrib'\n    sync: true\n    matcher:\n      files: ['litestar/contrib/*']\n\n  - label: 'area/private-api'\n    sync: true\n    matcher:\n      files:\n        any: ['litestar/_*.py', 'litestar/*/_*.py', 'litestar/_*/**/*.py']\n\n  # -- Size Based Labels -------------------------------------------------------\n  - label: 'size: small'\n    sync: true\n    matcher:\n      files:\n        count:\n          gte: 1\n          lte: 10\n\n  - label: 'size: medium'\n    sync: true\n    matcher:\n      files:\n        count:\n          gte: 10\n          lte: 25\n\n  - label: 'size: large'\n    sync: true\n    matcher:\n      files:\n        count:\n          gte: 26\n\n# -- Merge Checks --------------------------------------------------------------\nchecks:\n  - context: 'No Merge check'\n    description: \"Disable merging when 'do not merge' label is set\"\n    labels:\n      none: ['do not merge']\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".github/labeler.yml", "language": "yaml", "complexity": "complex", "star_count": 6976, "quality_score": 0.7899999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 307, "word_count": 527}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Tests And Linting\n\non:\n  pull_request:\n  merge_group:\n  push:\n    branches:\n      - main\n      - v1.51\n\nenv:\n  UV_LOCKED: 1\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.8\"\n\n      - name: Install Pre-Commit\n        run: python -m pip install pre-commit\n\n      - name: Load cached Pre-Commit Dependencies\n        id: cached-pre-commit-dependencies\n        uses: actions/cache@v4\n        with:\n          path: ~/.cache/pre-commit/\n          key: pre-commit-4|${{ env.pythonLocation }}|${{ hashFiles('.pre-commit-config.yaml') }}\n\n      - name: Execute Pre-Commit\n        run: pre-commit run --show-diff-on-failure --color=always --all-files\n\n  mypy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.8\"\n          allow-prereleases: true\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.5.4\"\n          enable-cache: true\n\n      - name: Run mypy\n        run: uv run mypy\n\n  pyright:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.8\"\n          allow-prereleases: true\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.5.4\"\n          enable-cache: true\n\n      - name: Run pyright\n        run: uv run pyright\n\n  slotscheck:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.8\"\n          allow-prereleases: false\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.5.4\"\n          enable-cache: true\n\n      - name: Run slotscheck\n        run: uv run slotscheck litestar\n\n  test:\n    name: \"test (${{ matrix.python-version }})\"\n    strategy:\n      fail-fast: true\n      matrix:\n        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\", \"3.13\"]\n    uses: ./.github/workflows/test.yml\n    with:\n      coverage: ${{ (matrix.python-version == '3.12' || matrix.python-version == '3.8') }}\n      python-version: ${{ matrix.python-version }}\n\n  # add an aggregate step here to check if any of the steps of the matrix 'test' job\n  # failed. this allows us to have dynamic or diverging steps in the matrix, while still\n  # being able to mark the 'test' step as a required check for a PR to be considered\n  # mergeable, without having to specify each individual matrix item.\n  test_success:\n    needs: test\n    # ensure this step always runs\n    if: always()\n    runs-on: ubuntu-latest\n    steps:\n      - name: Report success or fail\n        run: exit ${{ needs.test.result == 'success' && '0' || '1' }}\n\n  test_typing_extensions:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        typing-extensions: [\"4.12.1\", \"4.13.1\", \"latest\"]\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up python\n        uses: actions/setup-python@v5\n        with:\n          python-version: 3.13\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.6.12\"\n          enable-cache: true\n\n      - name: Install dependencies\n        run: uv sync\n\n      - name: Install typing-extensions\n        if: ${{ matrix.typing-extensions != 'latest' }}\n        run: uv pip install typing-extensions==\"${{ matrix.typing-extensions }}\"\n\n      - name: Install typing-extensions\n        if: ${{ matrix.typing-extensions == 'latest' }}\n        run: uv pip install typing-extensions --upgrade --prerelease=allow\n\n      - name: Test\n        run: uv run --no-sync -- python -m pytest tests/unit/test_typing.py\n\n\n  test_integration:\n    name: Test server integration\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        uvicorn-version: [\"uvicorn<0.27.0\", \"uvicorn>=0.27.0\"]\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up python 3.11\n        uses: actions/setup-python@v5\n        with:\n          python-version: 3.11\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.5.4\"\n          enable-cache: true\n\n      - name: Install Build Dependencies\n        run: sudo apt-get install build-essential libpq-dev python3-dev -y\n\n      - name: Install dependencies\n        run: |\n          uv sync\n          uv pip install -U \"${{ matrix.uvicorn-version }}\"\n\n      - name: Set PYTHONPATH\n        run: echo \"PYTHONPATH=$PWD\" >> $GITHUB_ENV\n\n      - name: Test\n        run: uv run --no-sync pytest tests -m server_integration\n\n  test-platform-compat:\n    if: github.event_name == 'push' || contains(github.event.pull_request.labels.*.name, 'test platform compat')\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [\"macos-latest\", \"windows-latest\"]\n    uses: ./.github/workflows/test.yml\n    with:\n      python-version: \"3.13\"\n      os: ${{ matrix.os }}\n      timeout: 30\n\n  codeql:\n    needs:\n      - test\n      - validate\n    runs-on: ubuntu-latest\n    permissions:\n      security-events: write\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Initialize CodeQL Without Dependencies\n        uses: github/codeql-action/init@v3\n        with:\n          setup-python-dependencies: false\n\n      - name: Perform CodeQL Analysis\n        uses: github/codeql-action/analyze@v3\n\n  build-docs:\n    needs:\n      - validate\n    if: github.event_name == 'pull_request'\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Install Build Dependencies\n        run: sudo apt-get install build-essential libpq-dev python3-dev -y\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n          allow-prereleases: true\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.5.4\"\n          enable-cache: true\n\n      - name: Build docs\n        run: uv run make docs\n\n      - name: Check docs links\n        env:\n          LITESTAR_DOCS_IGNORE_MISSING_EXAMPLE_OUTPUT: 1\n        run: uv run make docs-linkcheck\n\n      - name: Save PR number\n        run: |\n          echo \"${{ github.event.number }}\" > .pr_number\n\n      - name: Upload artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: docs-preview\n          path: |\n            docs/_build/html\n            .pr_number\n          include-hidden-files: true\n\n  test_minimal_app:\n    name: Test Minimal Application with Base Dependencies\n    runs-on: ubuntu-latest\n    env:\n      python_version: \"3.12\"\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.5.4\"\n          enable-cache: true\n\n      - name: Set pythonpath\n        run: echo \"PYTHONPATH=$PWD\" >> $GITHUB_ENV\n\n      - name: Test\n        run: |\n          mv tests/examples/test_hello_world.py test_hello_world.py\n          uv run pytest test_hello_world.py\n\n  test_pydantic_1_app:\n    name: Test Minimal Pydantic 1 application\n    runs-on: ubuntu-latest\n    env:\n      python_version: \"3.12\"\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Install Build Dependencies\n        run: sudo apt-get install build-essential libpq-dev python3-dev -y\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.5.4\"\n          enable-cache: true\n\n      - name: Install dependencies\n        run: |\n          uv sync\n          uv pip install \"pydantic==1.*\"\n\n      - name: Set pythonpath\n        run: echo \"PYTHONPATH=$PWD\" >> $GITHUB_ENV\n\n      - name: Test\n        run: uv run --no-sync coverage run --branch -m unittest test_apps/pydantic_1_app.py\n\n      - name: Rename coverage file\n        run: mv .coverage* .coverage.pydantic_v1\n\n      - uses: actions/upload-artifact@v4\n        with:\n          name: coverage-data-pydantic_v1-${{ inputs.python-version }}\n          path: .coverage.pydantic_v1\n          include-hidden-files: true\n\n  upload-test-coverage:\n    runs-on: ubuntu-latest\n    needs:\n      - test\n      - test_pydantic_1_app\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n\n      - name: Download Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          pattern: coverage-data*\n          merge-multiple: true\n\n      - name: Combine coverage files\n        run: |\n          python -Im pip install coverage covdefaults\n          python -Im coverage combine\n          python -Im coverage xml -i\n\n      - name: Fix coverage file name\n        run: sed -i \"s/home\\/runner\\/work\\/litestar\\/litestar/github\\/workspace/g\" coverage.xml\n\n      - name: Upload coverage reports to Codecov\n        uses: codecov/codecov-action@v5\n        with:\n          files: coverage.xml\n          token: ${{ secrets.CODECOV_TOKEN }}\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".github/workflows/ci.yml", "language": "yaml", "complexity": "complex", "star_count": 6976, "quality_score": 0.76, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 366, "word_count": 898}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Deploy documentation preview\non:\n  workflow_run:\n    workflows: [Tests And Linting]\n    types: [completed]\n\njobs:\n  deploy:\n    if: ${{ github.event.workflow_run.conclusion == 'success' &&  github.event.workflow_run.event == 'pull_request' }}\n    runs-on: ubuntu-latest\n    permissions:\n      issues: write\n      pull-requests: write\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Download artifact\n        uses: dawidd6/action-download-artifact@v11\n        with:\n          workflow_conclusion: success\n          run_id: ${{ github.event.workflow_run.id }}\n          path: docs-preview\n          name: docs-preview\n\n      - name: Validate and set PR number\n        run: |\n          PR_NUMBER=$(cat docs-preview/.pr_number)\n          if ! [[ \"$PR_NUMBER\" =~ ^[0-9]+$ ]]; then\n            echo \"Invalid PR number: $PR_NUMBER\"\n            exit 1\n          fi\n          echo \"PR_NUMBER=$PR_NUMBER\" >> $GITHUB_ENV\n\n      - name: Deploy docs preview\n        uses: JamesIves/github-pages-deploy-action@v4\n        with:\n          folder: docs-preview/docs/_build/html\n          token: ${{ secrets.DOCS_PREVIEW_DEPLOY_TOKEN }}\n          repository-name: litestar-org/litestar-docs-preview\n          clean: false\n          target-folder: ${{ env.PR_NUMBER }}\n          branch: gh-pages\n\n      - uses: actions/github-script@v7\n        env:\n          PR_NUMBER: ${{ env.PR_NUMBER }}\n        with:\n          script: |\n            const issue_number = process.env.PR_NUMBER\n            const body = \"Documentation preview will be available shortly at https://litestar-org.github.io/litestar-docs-preview/\" + issue_number\n\n            const opts = github.rest.issues.listComments.endpoint.merge({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              issue_number: issue_number,\n            });\n\n            const comments = await github.paginate(opts)\n\n            for (const comment of comments) {\n              if (comment.user.id === 41898282 && comment.body === body) {\n                await github.rest.issues.deleteComment({\n                  owner: context.repo.owner,\n                  repo: context.repo.repo,\n                  comment_id: comment.id\n                })\n              }\n            }\n\n            await github.rest.issues.createComment({\n                  owner: context.repo.owner,\n                  repo: context.repo.repo,\n                  issue_number: issue_number,\n                  body: body,\n            })\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".github/workflows/docs-preview.yml", "language": "yaml", "complexity": "simple", "star_count": 6976, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": false, "line_count": 78, "word_count": 192}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Documentation Building\n\non:\n  release:\n    types: [published]\n  push:\n    branches:\n      - main\n      - v3.0\n\nenv:\n  UV_LOCKED: 1\n\njobs:\n  docs:\n    permissions:\n      contents: write\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.5.4\"\n          enable-cache: true\n\n      - name: Install dependencies\n        run: uv sync\n\n      - name: Fetch gh pages\n        run: git fetch origin gh-pages --depth=1\n\n      - name: Build release docs\n        run: uv run python tools/build_docs.py docs-build\n        if: github.event_name == 'release'\n\n      - name: Build docs (main branch)\n        run: uv run python tools/build_docs.py docs-build --version main\n        if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n\n      - name: Build docs (v3.0 branch)\n        run: uv run python tools/build_docs.py docs-build --version 3-dev\n        if: github.event_name == 'push' && github.ref == 'refs/heads/v3.0'\n\n      - name: Deploy\n        uses: JamesIves/github-pages-deploy-action@v4\n        with:\n          folder: docs-build\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".github/workflows/docs.yml", "language": "yaml", "complexity": "simple", "star_count": 6976, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 54, "word_count": 129}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport itertools\nimport json\nimport pathlib\nimport re\nimport sys\n\n__all__ = (\n    \"find_resolved_issues\",\n    \"main\",\n)\n\n\ndef find_resolved_issues(source: str, tag: str) -> list[str]:\n    version = tag.split(\"v\", maxsplit=1)[-1]\n    changelog_line = f\".. changelog:: {version}\"\n    stop_line = \".. changelog::\"\n    return list(\n        {\n            issue\n            for line in itertools.takewhile(\n                lambda l: stop_line not in l,  # noqa: E741\n                source.split(changelog_line, maxsplit=1)[1].splitlines(),\n            )\n            if re.match(r\"\\s+:issue: [\\d ,]+\", line)\n            for issue in re.findall(r\"\\d+\", line)\n        }\n    )\n\n\ndef main(tag: str) -> str:\n    source = pathlib.Path(\"docs/release-notes/changelog.rst\").read_text()\n    return json.dumps(find_resolved_issues(source, tag))\n\n\nif __name__ == \"__main__\":\n    print(main(sys.argv[1]))  # noqa: T201\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".github/workflows/notify_released_issues/get_closed_issues.py", "language": "python", "complexity": "simple", "star_count": 6976, "quality_score": 0.7899999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 39, "word_count": 91}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: \"Pull Request Labeler\"\n\non:\n  pull_request_target:\n\njobs:\n  apply-labels:\n    permissions:\n      contents: read\n      pull-requests: write\n      checks: write\n      statuses: write\n    runs-on: ubuntu-latest\n    steps:\n    - uses: fuxingloh/multi-labeler@v4\n      with:\n        github-token: \"${{ secrets.GITHUB_TOKEN }}\"\n\n  distinguish-pr-origin:\n    needs: apply-labels\n    if: ${{ always() }}\n    permissions:\n      pull-requests: write\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/github-script@v7\n        with:\n          github-token: ${{secrets.GITHUB_TOKEN}}\n          script: |\n            const maintainers = [\n              'JacobCoffee', 'provinzkraut', 'cofin',\n              'peterschutt', 'Alc-Alc', 'guacs',\n              'dependabot[bot]', 'all-contributors[bot]'\n            ]\n            if (maintainers.includes(context.payload.sender.login)) {\n              github.rest.issues.addLabels({\n                issue_number: context.issue.number,\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                labels: ['pr/internal']\n              })\n            } else {\n              github.rest.issues.addLabels({\n                issue_number: context.issue.number,\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                labels: ['pr/external', 'Triage Required :hospital:']\n              })\n            }\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".github/workflows/pr-labeler.yml", "language": "yaml", "complexity": "simple", "star_count": 6976, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": false, "line_count": 50, "word_count": 92}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: \"PR merged\"\n\non:\n  pull_request:\n    types:\n      - closed\n    branches:\n      - main\n      - v3.0\n\njobs:\n  close_and_notify:\n    name: Close issues and notify\n    if: github.event.pull_request.merged == true\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/github-script@v7\n        with:\n          script: |\n            const prNumber = context.payload.number\n            const branch = context.baseRef\n            // TODO: use semantic commits to specify the exact version, when it will be released\n            const commentBody = `<!--closing-comment-->\\nThis issue has been closed in #${prNumber}. The change will be included in upcoming releases.`\n\n            const query = `query($number: Int!, $owner: String!, $name: String!) { repository(owner: $owner, name: $name) {\n                pullRequest(number: $number) {\n                      id\n                      closingIssuesReferences (first: 10) { edges { node { number } } }\n                      }\n                  }\n              }`\n            const res = await github.graphql(query, {number: prNumber, owner: context.repo.owner, name: context.repo.repo})\n            const linkedIssues = res.repository.pullRequest.closingIssuesReferences.edges.map(\n              edge => edge.node.number\n            )\n\n            for (const issueNumber of linkedIssues) {\n              const res = await github.rest.issues.update({\n                  owner: context.repo.owner,\n                  repo: context.repo.repo,\n                  issue_number: issueNumber,\n                  state: \"closed\",\n                  state_reason: \"completed\"\n              })\n              if (res.status === 200) {\n                await github.rest.issues.createComment({\n                      owner: context.repo.owner,\n                      repo: context.repo.repo,\n                      issue_number: issueNumber,\n                      body: commentBody,\n                })\n              }\n            }\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".github/workflows/pr-merged.yml", "language": "yaml", "complexity": "simple", "star_count": 6976, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 54, "word_count": 167}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: \"Lint PR Title\"\n\non:\n  pull_request_target:\n    types:\n      - opened\n      - edited\n      - synchronize\n\npermissions:\n  pull-requests: read\n\njobs:\n  main:\n    name: Validate PR title\n    runs-on: ubuntu-latest\n    steps:\n      - uses: amannn/action-semantic-pull-request@v5\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".github/workflows/pr-title.yml", "language": "yaml", "complexity": "simple", "star_count": 6976, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 21, "word_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Test\n\non:\n  workflow_call:\n    inputs:\n      python-version:\n        required: true\n        type: string\n      coverage:\n        required: false\n        type: boolean\n        default: false\n      os:\n        required: false\n        type: string\n        default: \"ubuntu-latest\"\n      timeout:\n        required: false\n        type: number\n        default: 10\n\nenv:\n  UV_LOCKED: 1\n\njobs:\n  test:\n    runs-on: ${{ inputs.os }}\n    timeout-minutes: ${{ inputs.timeout }}\n    defaults:\n      run:\n        shell: bash\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up python ${{ inputs.python-version }}\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ inputs.python-version }}\n\n      # Linux Source\n      - name: Install Build Dependencies\n        run: sudo apt-get install build-essential libpq-dev python3-dev -y\n        if: startsWith(inputs.os, 'ubuntu')\n\n      # MacOS Source\n      - name: Install Build Dependencies\n        run: brew install libpq && brew link --force libpq\n        if: startsWith(inputs.os, 'macos')\n\n      # Windows Source\n      - name: Install Build Dependencies\n        uses: ikalnytskyi/action-setup-postgres@v7\n        if: startsWith(inputs.os, 'windows')\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v6\n        with:\n          version: \"0.6.12\"\n          enable-cache: true\n\n      - name: Install dependencies\n        run: uv sync\n\n      - name: Set PYTHONPATH\n        run: echo \"PYTHONPATH=$PWD\" >> $GITHUB_ENV\n\n      - name: Test\n        if: ${{ !inputs.coverage }}\n        run: uv run pytest docs/examples tests -n auto\n\n      - name: Test with coverage\n        if: inputs.coverage\n        run: uv run pytest docs/examples tests -n auto --cov\n\n      - name: Rename coverage file\n        if: inputs.coverage\n        run: mv .coverage .coverage.${{ inputs.python-version }}\n\n      - uses: actions/upload-artifact@v4\n        if: inputs.coverage\n        with:\n          name: coverage-data-${{ inputs.python-version }}\n          path: .coverage.${{ inputs.python-version }}\n          include-hidden-files: true\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".github/workflows/test.yml", "language": "yaml", "complexity": "simple", "star_count": 6976, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 86, "word_count": 210}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "default_language_version:\n  python: \"3.12\"\nrepos:\n  - repo: https://github.com/compilerla/conventional-pre-commit\n    rev: v3.6.0\n    hooks:\n      - id: conventional-pre-commit\n        stages: [commit-msg]\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n      - id: check-ast\n      - id: check-case-conflict\n      - id: check-toml\n      - id: debug-statements\n        exclude: ^(litestar/config/app\\.py|litestar/app\\.py|test_apps/debugging/main\\.py)$\n      - id: end-of-file-fixer\n      - id: mixed-line-ending\n      - id: trailing-whitespace\n        exclude: \"tests/unit/test_openapi/test_typescript_converter/test_converter.py\"\n  - repo: https://github.com/provinzkraut/unasyncd\n    rev: \"v0.8.1\"\n    hooks:\n      - id: unasyncd\n        additional_dependencies: [\"ruff\"]\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: \"v0.8.1\"\n    hooks:\n      - id: ruff\n        args: [\"--fix\"]\n      - id: ruff-format\n  - repo: https://github.com/crate-ci/typos\n    rev: v1.30.3\n    hooks:\n      - id: typos\n  - repo: https://github.com/python-formate/flake8-dunder-all\n    rev: v0.4.1\n    hooks:\n      - id: ensure-dunder-all\n        exclude: \"test*|examples*|tools\"\n        args: [\"--use-tuple\"]\n  - repo: https://github.com/sphinx-contrib/sphinx-lint\n    rev: \"v1.0.0\"\n    hooks:\n      - id: sphinx-lint\n  - repo: local\n    hooks:\n      - id: pypi-readme\n        name: pypi-readme\n        language: python\n        entry: python tools/pypi_readme.py\n        types: [markdown]\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": ".pre-commit-config.yaml", "language": "yaml", "complexity": "simple", "star_count": 6976, "quality_score": 0.7899999999999999, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": false, "line_count": 53, "word_count": 118}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas with installation and setup instructions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Contribution Guide\n==================\n\n.. _pipx: https://pypa.github.io/pipx/\n.. |pipx| replace:: `pipx <https://pypa.github.io/pipx/>`__\n\n.. _homebrew: https://brew.sh/\n.. |homebrew| replace:: `Homebrew <https://brew.sh/>`__\n\n\nGetting Started\n---------------\n\nSupported Python Versions\n+++++++++++++++++++++++++\n\nThe lowest currently supported version is Python 3.8.\nAt a minimum you will need Python 3.8 for code changes and 3.12 if you plan on doing documentation building / changes.\n\nYou can use various tools to manage multiple Python versions on your system including:\n\n* `pyenv <https://github.com/pyenv/pyenv>`_ and `pyenv-win for Windows <https://github.com/pyenv-win/pyenv-win>`_\n* `rtx / mise <https://mise.jdx.dev/>`_\n* `asdf <https://asdf-vm.com/>`_\n* `Building each version manually from source <https://www.build-python-from-source.com/>`_\n* Utilizing `GitHub Codespaces <https://codespaces.new/litestar-org/litestar?quickstart=1>`_\n\nWe use the lowest supported version in our type-checking CI,\nthis ensures that the changes you made are backward compatible.\n\nSetting up the environment\n++++++++++++++++++++++++++\n\n.. tip:: We maintain a Makefile with several commands to help with common tasks.\n   You can run ``make help`` to see a list of available commands.\n\nIf you are utilizing `GitHub Codespaces <https://codespaces.new/litestar-org/litestar?quickstart=1>`_,\nthe environment will bootstrap itself automatically. The steps below are for local development.\n\n#. Install `uv <https://docs.astral.sh/uv/getting-started/installation/>`_:\n\n#. Run ``make install`` to create a `virtual environment <https://docs.python.org/3/tutorial/venv.html>`_\n   and install the required development dependencies or run the ``uv sync`` command manually:\n\n   .. code-block:: shell\n        :caption: Installing the development dependencies\n\n        uv sync\n\n\n.. tip:: Many modern IDEs like PyCharm or VS Code will enable the uv-managed virtualenv that is created in step 2\n   for you automatically.\n   If your IDE / editor does not offer this functionality, then you will need to manually activate the virtualenv\n   yourself. Otherwise you may encounter errors or unexpected behaviour when trying to run the commands referenced\n   within this document.\n\n   To activate the virtualenv manually, please consult uv's documentation on\n   `working with virtual environments <https://docs.astral.sh/uv/pip/environments/>`_.\n\n   The rest of this document will assume this environment is active wherever commands are referenced.\n\nCode contributions\n------------------\n\nWorkflow\n++++++++\n\n#. `Fork <https://github.com/litestar-org/litestar/fork>`_ the `Litestar repository <https://github.com/litestar-org/litestar>`_\n#. Clone your fork locally with git\n#. `Set up the environment <#setting-up-the-environment>`_\n#. Make your changes\n#. (Optional) Run ``pre-commit run --all-files`` to run linters and formatters. This step is optional and will be executed\n   automatically by git before you make a commit, but you may want to run it manually in order to apply fixes\n#. Commit your changes to git. We follow `conventional commits <https://www.conventionalcommits.org/>`_\n   which are enforced using a ``pre-commit`` hook.\n#. Push the changes to your fork\n#. Open a `pull request <https://docs.github.com/en/pull-requests>`_. Give the pull request a descriptive title\n   indicating what it changes. The style of the PR title should also follow\n   `conventional commits <https://www.conventionalcommits.org/>`_, and this is enforced using a GitHub action.\n#. Add yourself as a contributor using the `all-contributors bot <https://allcontributors.org/docs/en/bot/usage>`_\n\nGuidelines for writing code\n----------------------------\n\n- Code should be `Pythonic and zen <https://peps.python.org/pep-0020/>`_\n- All code should be fully `typed <https://peps.python.org/pep-0484/>`_. This is enforced via\n  `mypy <https://mypy.readthedocs.io/en/stable/>`_ and `Pyright <https://github.com/microsoft/pyright/>`_\n\n  * When requiring complex types, use a `type alias <https://docs.python.org/3/library/typing.html#type-aliases>`_.\n    Check :doc:`reference/types` if a type alias for your use case already exists\n  * If something cannot be typed correctly due to a limitation of the type checkers, you may use :func:`typing.cast`\n    to rectify the situation. However, you should only use this as a last resort if you've exhausted all other options\n    of `type narrowing <https://mypy.readthedocs.io/en/stable/type_narrowing.html>`_, such as :func:`isinstance` checks\n    and `type guards <https://docs.python.org/3/library/typing.html#typing.TypeGuard>`_.\n  * You may use a properly scoped ``type: ignore`` if you ensured that a line is correct, but mypy / pyright has\n    issues with it.\n\n    Properly scoped meaning do not use blank ``type: ignore``, instead supply the specific error code, e.g.,\n    ``type: ignore[attr-defined]``\n\n- If you are adding or modifying existing code, ensure that it's fully tested. 100% test coverage is mandatory, and will\n  be checked on the PR using `SonarCloud <https://www.sonarsource.com/products/sonarcloud/>`_ and\n  `Codecov <https://codecov.io/>`_\n- All functions, methods, classes, and attributes should be documented with a docstring. We use the\n  `Google docstring style <https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html>`_. If you come\n  across a function or method that doesn't conform to this standard, please update it as you go\n- When adding a new public interface, it has to be  included in the reference documentation located in\n  ``docs/reference``. If applicable, add or modify examples in the docs related to the new functionality implemented,\n  following the guidelines established in `Adding examples`_.\n\nWriting and running tests\n+++++++++++++++++++++++++\n\nTests are contained within the ``tests`` directory, and follow the same directory structure as the ``litestar`` module.\nIf you are adding a test case, it should be located within the correct submodule of ``tests``. E.g., tests for\n``litestar/utils/sync.py`` reside in ``tests/utils/test_sync.py``.\n\nThe ``Makefile`` includes several commands for running tests:\n\n- ``make test`` to run tests located in ``tests``\n- ``make test-examples`` to run tests located in ``docs/examples/tests``\n- ``make test-all`` to run all tests\n- ``make coverage`` to run tests with coverage and generate an html report\n\nThe tests make use of `pytest-xdist <https://pytest-xdist.readthedocs.io>`_ to speed up\ntest runs. These are enabled by default when running ``make test``, ``make test-all``\nor ``make coverage``. Due to the nature of pytest-xdist, attaching a debugger isn't as\nstraightforward. For debugging, it's recommended to run the tests individually with\n``pytest <test name>`` or via an IDE, which will skip ``pytest-xdist``.\n\nRunning type checkers\n+++++++++++++++++++++\n\nWe use `mypy <https://mypy.readthedocs.io/en/stable/>`_ and `pyright <https://github.com/microsoft/pyright/>`_ to\nenforce type safety. You can run them with:\n\n- ``make mypy``\n- ``make pyright``\n- ``make type-check`` to run both\n- ``make lint`` to run pre-commit hooks and type checkers.\n\nOur type checkers are run on Python 3.8 in CI, so you should make sure to run them on the same version locally as well.\n\nProject documentation\n---------------------\n\nThe documentation is located in the ``/docs`` directory and is written  in\n`reStructuredText <https://docutils.sourceforge.io/rst.html>`_ with the `Sphinx <https://www.sphinx-doc.org/en/master/>`_.\nlibrary. If you're unfamiliar with any of those,\n`reStructuredText primer <https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html>`_ and\n`Sphinx quickstart <https://www.sphinx-doc.org/en/master/usage/quickstart.html>`_ are recommended reads.\n\nDocs theme and appearance\n+++++++++++++++++++++++++\n\nWe welcome contributions that enhance / improve the appearance and usability of the docs. We use the excellent\n`PyData Sphinx Theme <https://pydata-sphinx-theme.readthedocs.io/>`_ theme, which comes with a lot of options out of the box.\nIf you wish to contribute to the docs style / setup, or static site generation, you should consult the theme docs\nas a first step.\n\nRunning the docs locally\n++++++++++++++++++++++++\n\nYou can serve the documentation locally with\n\n.. code-block:: shell\n    :caption: Serving the documentation locally\n\n    make docs-serve\n\nor build it with\n\n.. code-block:: shell\n    :caption: Serving the documentation locally\n\n    make docs\n\n\nWriting and editing docs\n++++++++++++++++++++++++\n\nWe welcome contributions that enhance / improve the content of the docs. Feel free to add examples, clarify text,\nrestructure the docs, etc., but make sure to follow these guidelines:\n\n- Write text in idiomatic English, using simple language\n- Do not use contractions for ease of reading for non-native English speakers\n- Opt for `Oxford commas <https://en.wikipedia.org/wiki/Serial_comma>`_ when listing a series of terms\n- Keep examples simple and self contained (see `Adding examples`_). This is to ensure they are tested\n  alongside the rest of the test suite and properly type checked and linted.\n- Provide links where applicable.\n- Use `intersphinx <https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html>`_ wherever possible when\n  referencing external libraries\n- Provide diagrams using `Mermaid <https://mermaid.js.org/>`_ where applicable and possible\n\nAdding examples\n~~~~~~~~~~~~~~~\n\nThe examples from the docs are located in their own modules inside the ``/docs/examples`` folder. This makes it easier\nto test them alongside the rest of the test suite, ensuring they do not become stale as Litestar evolves.\n\nPlease follow the next guidelines when adding a new example:\n\n- Add the example in the corresponding module directory in ``/docs/examples`` or create a new one if necessary\n- Create a suite for the module in ``/tests/examples`` that tests the aspects of the example that it demonstrates\n- Reference the example in the rst file with an external reference code block, e.g.\n\n.. code-block:: rst\n   :caption: An example of how to use literal includes of external files\n\n   .. literalinclude:: /examples/test_thing.py\n      :language: python\n      :caption: All includes should have a descriptive caption\n\nAutomatically execute examples\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nOur docs include a Sphinx extension that can automatically run requests against example apps\nand include their result in the documentation page when its being built. This only requires 2 steps:\n\n1. Create an example file with an ``app`` object in it, which is an instance of ``Litestar``\n2. Add a comment in the form of ``# run: /hello`` to the example file\n\nWhen building the docs (or serving them locally), a process serving the ``app`` instance\nwill be launched, and the requests specified in the comments will be run against it. The\ncomments will be stripped from the result, and the output of the ``curl`` invocation inserted\nafter the example code-block.\n\nThe ``# run:`` syntax is nothing special; everything after the colon will be passed to\nthe ``curl`` command that's being invoked. The URL is built automatically, so the\nspecified path can just be a path relative to the app.\n\nIn practice, this looks like the following:\n\n.. code-block:: python\n   :caption: An example of how to use the automatic example runner\n   :no-upgrade:\n\n   from typing import Dict\n\n   from litestar import Litestar, get\n\n\n   @get(\"/\")\n   def hello_world() -> Dict[str, str]:\n       \"\"\"Handler function that returns a greeting dictionary.\"\"\"\n       return {\"hello\": \"world\"}\n\n\n   app = Litestar(route_handlers=[hello_world])\n\n   # run: /\n\nThis is equivalent to:\n\n.. code-block:: python\n   :caption: An example of how to use the automatic example runner\n\n      from typing import Dict\n\n      from litestar import Litestar, get\n\n\n      @get(\"/\")\n      def hello_world() -> Dict[str, str]:\n          \"\"\"Handler function that returns a greeting dictionary.\"\"\"\n          return {\"hello\": \"world\"}\n\n\n      app = Litestar(route_handlers=[hello_world])\n\n\n.. admonition:: Run it\n\n      .. code-block:: bash\n\n         > curl http://127.0.0.1:8000/\n         {\"hello\": \"world\"}\n\n\nCreating a New Release\n----------------------\n\n#. Checkout the ``main`` branch:\n\n   .. code-block:: shell\n        :caption: Checking out the main branch of the ``litestar`` repository\n\n        git checkout main\n\n#. Run the release preparation script:\n\n   .. code-block:: shell\n        :caption: Preparing  a new release\n\n        python tools/prepare_release.py <new version number> --update-version --create-draft-release\n\n   Replace ``<new version number>`` with the desired version number following the\n   `versioning scheme <https://litestar.dev/about/litestar-releases#version-numbering>`_.\n\n   This script will:\n\n   - Update the version in ``pyproject.toml``\n   - Generate a changelog entry in :doc:`/release-notes/changelog`\n   - Create a draft release on GitHub\n\n#. Review the generated changelog entry in :doc:`/release-notes/changelog` to ensure it looks correct.\n\n#. Commit the changes to ``main``:\n\n   .. code-block:: shell\n        :caption: Committing the changes to the main branch\n\n        git commit -am \"chore(release): prepare release vX.Y.Z\"\n\n   Replace ``vX.Y.Z`` with the actual version number.\n\n#. Create a new branch for the release:\n\n   .. code-block:: shell\n        :caption: Creating a new branch for the release\n\n        git checkout -b vX.Y.Z\n\n#. Push the changes to a ``vX.Y.Z`` branch:\n\n   .. code-block:: shell\n        :caption: Pushing the changes to the ``vX.Y.Z`` branch\n\n        git push origin vX.Y.Z\n\n#. Open a pull request from the ``vX.Y.Z`` branch to ``main``.\n#. Once the pull request is approved, go to the draft release on GitHub\n   (the release preparation script will provide a link).\n#. Review the release notes in the draft release to ensure they look correct.\n#. If everything looks good, click \"Publish release\" to make the release official.\n#. Go to the `Release Action <https://github.com/litestar-org/litestar/actions/workflows/publish.yml>`_ and approve\n   the release workflow if necessary.\n#. Check that the release workflow runs successfully.\n\n.. note:: The version number should follow `semantic versioning <https://semver.org/>`_ and\n   `PEP 440 <https://peps.python.org/pep-0440/>`_.\n", "metadata": {"source": "github.com/litestar-org/litestar", "file_path": "CONTRIBUTING.rst", "language": "restructuredtext", "complexity": "complex", "star_count": 6976, "quality_score": 0.85, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 342, "word_count": 1848}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Documentation\nabout: Report an issue related to the fastapi-mcp documentation/examples\ntitle: ''\nlabels: documentation\nassignees: ''\n\n---\n\n\n", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": ".github/ISSUE_TEMPLATE/documentation.md", "language": "markdown", "complexity": "simple", "star_count": 6708, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 11, "word_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Feature request\nabout: Suggest an idea for this project\ntitle: ''\nlabels: enhancement\nassignees: ''\n\n---\n\n**Is your feature request related to a problem? Please describe.**\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": ".github/ISSUE_TEMPLATE/feature_request.md", "language": "markdown", "complexity": "simple", "star_count": 6708, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 21, "word_count": 90}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: 2\nupdates:\n  - package-ecosystem: \"github-actions\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n    open-pull-requests-limit: 10\n    labels:\n      - \"dependencies\"\n      - \"github-actions\"\n\n  - package-ecosystem: \"pip\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n    open-pull-requests-limit: 10\n    labels:\n      - \"dependencies\"\n      - \"python\"", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": ".github/dependabot.yml", "language": "yaml", "complexity": "simple", "star_count": 6708, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 19, "word_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## Describe your changes\n\n## Issue ticket number and link (if applicable)\n\n## Screenshots of the feature / bugfix\n\n## Checklist before requesting a review\n- [ ] Added relevant tests\n- [ ] Run ruff & mypy\n- [ ] All tests pass\n", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": ".github/pull_request_template.md", "language": "markdown", "complexity": "simple", "star_count": 6708, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 11, "word_count": 44}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  ruff:\n    name: Ruff\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v5\n        with:\n          version: \"0.6.12\"\n          enable-cache: true\n          cache-dependency-glob: \"uv.lock\"\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version-file: \".python-version\"\n\n      - name: Install dependencies\n        run: uv sync --all-extras --dev\n\n      - name: Lint with Ruff\n        run: uv run ruff check .\n\n  mypy:\n    name: MyPy\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v5\n        with:\n          version: \"0.6.12\"\n          enable-cache: true\n          cache-dependency-glob: \"uv.lock\"\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version-file: \".python-version\"\n\n      - name: Install dependencies\n        run: uv sync --all-extras --dev\n\n      - name: Type check with MyPy\n        run: uv run mypy .\n\n  test:\n    name: Test Python ${{ matrix.python-version }}\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.10\", \"3.11\", \"3.12\"]\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v5\n        with:\n          version: \"0.6.12\"\n          python-version: ${{ matrix.python-version }}\n          enable-cache: true\n          cache-dependency-glob: \"uv.lock\"\n\n      - name: Install dependencies\n        run: uv sync --all-extras --dev\n\n      - name: Run tests\n        run: uv run pytest --cov=fastapi_mcp --cov-report=xml\n\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v5\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n          fail_ci_if_error: false\n", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": ".github/workflows/ci.yml", "language": "yaml", "complexity": "simple", "star_count": 6708, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 88, "word_count": 186}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Release\n\non:\n  release:\n    types: [created]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v5\n        with:\n          version: \"0.6.12\"\n          enable-cache: true\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version-file: \".python-version\"\n\n      - name: Install build dependencies\n        run: |\n          uv sync --all-extras --dev\n          uv pip install build twine\n\n      - name: Build and publish\n        env:\n          TWINE_USERNAME: __token__\n          TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}\n        run: |\n          uv run python -m build\n          uv run twine check dist/*\n          uv run twine upload dist/*\n", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": ".github/workflows/release.yml", "language": "yaml", "complexity": "simple", "star_count": 6708, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": false, "line_count": 39, "word_count": 83}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [0.3.7]\n\n### Fixed\n- üêõ Fix a bug with OAuth default_scope (#123)\n\n## [0.3.6]\n\nSkipped 0.3.5 due to a broken release attempt.\n\n### Added\n- üöÄ Add configurable HTTP header forwarding (#181)\n\n### Fixed\n- üêõ Fix a bug with handling FastAPI `root_path` parameter (#163)\n\n## [0.3.4]\n\n### Fixed\n- üêõ Update the `mcp` dependency to `1.8.1`. Fixes [Issue #134](https://github.com/tadata-org/fastapi_mcp/issues/134) that was caused after a breaking change in mcp sdk 1.8.0.\n\n## [0.3.3]\n\nFixes the broken release from 0.3.2.\n\n### Fixed\n- üêõ Fix critical bug in openapi conversion (missing `param_desc` definition) (#107, #99)\n- üêõ Fix non-ascii support (#66)\n\n## [0.3.2] - Broken\n\nThis is a broken release and should not be used.\n\n### Fixed\n- üêõ Fix a bug preventing simple setup of [basic token passthrough](docs/03_authentication_and_authorization.md#basic-token-passthrough)\n\n## [0.3.1]\n\nüöÄ FastApiMCP now supports MCP Authorization!\n\nYou can now add MCP-compliant OAuth configuration in a FastAPI-native way, using your existing FastAPI `Depends()` that we all know and love.\n\n### Added\n- üéâ Support for Authentication / Authorization compliant to [MCP 2025-03-26 Specification](https://modelcontextprotocol.io/specification/2025-03-26/basic/authorization), using OAuth 2.1. (#10)\n- üéâ Support passing http headers to tool calls (#82)\n\n## [0.3.0]\n\nüöÄ FastApiMCP now works with ASGI-transport by default.\n\nThis means the `base_url` argument is redundant, and thus has been removed.\n\nYou can still set up an explicit base URL using the `http_client` argument, and injecting your own `httpx.AsyncClient` if necessary.\n\n### Removed\n- ‚ö†Ô∏è Breaking Change: Removed `base_url` argument since it's not used anymore by the MCP transport.\n\n### Fixed\n- üêõ Fix short timeout issue (#71), increasing the default timeout to 10\n\n\n## [0.2.0]\n\n### Changed\n- Complete refactor from function-based API to a new class-based API with `FastApiMCP`\n- Explicit separation between MCP instance creation and mounting with `mcp = FastApiMCP(app)` followed by `mcp.mount()`\n- FastAPI-native approach for transport providing more flexible routing options\n- Updated minimum MCP dependency to v1.6.0\n\n### Added\n- Support for deploying MCP servers separately from API service\n- Support for \"refreshing\" with `setup_server()` when dynamically adding FastAPI routes. Fixes [Issue #19](https://github.com/tadata-org/fastapi_mcp/issues/19)\n- Endpoint filtering capabilities through new parameters:\n  - `include_operations`: Expose only specific operations by their operation IDs\n  - `exclude_operations`: Expose all operations except those with specified operation IDs\n  - `include_tags`: Expose only operations with specific tags\n  - `exclude_tags`: Expose all operations except those with specific tags\n\n### Fixed\n- FastAPI-native approach for transport. Fixes [Issue #28](https://github.com/tadata-org/fastapi_mcp/issues/28)\n- Numerous bugs in OpenAPI schema to tool conversion, addressing [Issue #40](https://github.com/tadata-org/fastapi_mcp/issues/40) and [Issue #45](https://github.com/tadata-org/fastapi_mcp/issues/45)\n\n### Removed\n- Function-based API (`add_mcp_server`, `create_mcp_server`, etc.)\n- Custom tool support via `@mcp.tool()` decorator\n\n## [0.1.8]\n\n### Fixed\n- Remove unneeded dependency.\n\n## [0.1.7]\n\n### Fixed\n- [Issue #34](https://github.com/tadata-org/fastapi_mcp/issues/34): Fix syntax error.\n\n## [0.1.6]\n\n### Fixed\n- [Issue #23](https://github.com/tadata-org/fastapi_mcp/issues/23): Hide handle_mcp_connection tool.\n\n## [0.1.5]\n\n### Fixed\n- [Issue #25](https://github.com/tadata-org/fastapi_mcp/issues/25): Dynamically creating tools function so tools are useable.\n\n## [0.1.4]\n\n### Fixed\n- [Issue #8](https://github.com/tadata-org/fastapi_mcp/issues/8): Converted tools unuseable due to wrong passing of arguments.\n\n## [0.1.3]\n\n### Fixed\n- Dependency resolution issue with `mcp` package and `pydantic-settings`\n\n## [0.1.2]\n\n### Changed\n- Complete refactor: transformed from a code generator to a direct integration library\n- Replaced the CLI-based approach with a direct API for adding MCP servers to FastAPI applications\n- Integrated MCP servers now mount directly to FastAPI apps at runtime instead of generating separate code\n- Simplified the API with a single `add_mcp_server` function for quick integration\n- Removed code generation entirely in favor of runtime integration\n\n### Added\n- Main `add_mcp_server` function for simple MCP server integration\n- Support for adding custom MCP tools alongside API-derived tools\n- Improved test suite\n- Manage with uv\n\n### Removed\n- CLI interface and all associated commands (generate, run, install, etc.)\n- Code generation functionality\n\n## [0.1.1] - 2024-07-03\n\n### Fixed\n- Added support for PEP 604 union type syntax (e.g., `str | None`) in FastAPI endpoints\n- Improved type handling in model field generation for newer Python versions (3.10+)\n- Fixed compatibility issues with modern type annotations in path parameters, query parameters, and Pydantic models\n\n## [0.1.0] - 2024-03-08\n\n### Added\n- Initial release of FastAPI-MCP\n- Core functionality for converting FastAPI applications to MCP servers\n- CLI tool for generating, running, and installing MCP servers\n- Automatic discovery of FastAPI endpoints\n- Type-safe conversion from FastAPI endpoints to MCP tools\n- Documentation preservation from FastAPI to MCP\n- Claude integration for easy installation and use\n- API integration that automatically makes HTTP requests to FastAPI endpoints\n- Examples directory with sample FastAPI application\n- Basic test suite", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": "CHANGELOG.md", "language": "markdown", "complexity": "medium", "star_count": 6708, "quality_score": 0.7899999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 161, "word_count": 777}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas with installation and setup instructions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributing to FastAPI-MCP\n\nFirst off, thank you for considering contributing to FastAPI-MCP!\n\n## Development Setup\n\n1.  Make sure you have Python 3.10+ installed\n2.  Install [uv](https://docs.astral.sh/uv/getting-started/installation/) package manager\n3.  Fork the repository\n4.  Clone your fork\n\n    ```bash\n    git clone https://github.com/YOUR-USERNAME/fastapi_mcp.git\n    cd fastapi-mcp\n\n    # Add the upstream remote\n    git remote add upstream https://github.com/tadata-org/fastapi_mcp.git\n    ```\n\n5.  Set up the development environment:\n\n    ```bash\n    uv sync\n    ```\n\n    That's it! The `uv sync` command will automatically create and use a virtual environment.\n\n6.  Install pre-commit hooks:\n\n    ```bash\n    uv run pre-commit install\n    uv run pre-commit run\n    ```\n\n    Pre-commit hooks will automatically run checks (like ruff, formatting, etc.) when you make a commit, ensuring your code follows our style guidelines.\n\n### Running Commands\n\nYou have two options for running commands:\n\n1.  **With the virtual environment activated**:\n    ```bash\n    source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n    # Then run commands directly\n    pytest\n    mypy .\n    ruff check .\n    ```\n\n2.  **Without activating the virtual environment**:\n    ```bash\n    # Use uv run prefix for all commands\n    uv run pytest\n    uv run mypy .\n    uv run ruff check .\n    ```\n\nBoth approaches work - use whichever is more convenient for you.\n\n> **Note:** For simplicity, commands in this guide are mostly written **without** the `uv run` prefix. If you haven't activated your virtual environment, remember to prepend `uv run` to all python-related commands and tools.\n\n### Adding Dependencies\n\nWhen adding new dependencies to the library:\n\n1.  **Runtime dependencies** - packages needed to run the application:\n    ```bash\n    uv add new-package\n    ```\n\n2.  **Development dependencies** - packages needed for development, testing, or CI:\n    ```bash\n    uv add --group dev new-package\n    ```\n\nAfter adding dependencies, make sure to:\n1.  Test that everything works with the new package\n2.  Commit both `pyproject.toml` and `uv.lock` files:\n    ```bash\n    git add pyproject.toml uv.lock\n    git commit -m \"Add new-package dependency\"\n    ```\n\n## Development Process\n\n1. Fork the repository and set the upstream remote\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Run type checking (`mypy .`)\n5. Run the tests (`pytest`)\n6. Format your code (`ruff check .` and `ruff format .`). Not needed if pre-commit is installed, as it will run it for you.\n7. Commit your changes (`git commit -m 'Add some amazing feature'`)\n8. Push to the branch (`git push origin feature/amazing-feature`)\n9. Open a Pull Request. Make sure the Pull Request's base branch is [the original repository's](https://github.com/tadata-org/fastapi_mcp/) `main` branch.\n\n## Code Style\n\nWe use the following tools to ensure code quality:\n\n- **ruff** for linting and formatting\n- **mypy** for type checking\n\nPlease make sure your code passes all checks before submitting a pull request:\n\n```bash\n# Check code formatting and style\nruff check .\nruff format .\n\n# Check types\nmypy .\n```\n\n## Testing\n\nWe use pytest for testing. Please write tests for any new features and ensure all tests pass:\n\n```bash\n# Run all tests\npytest\n```\n\n## Pull Request Process\n\n1. Ensure your code follows the style guidelines of the project\n2. Update the README.md with details of changes if applicable\n3. The versioning scheme we use is [SemVer](http://semver.org/)\n4. Include a descriptive commit message\n5. Your pull request will be merged once it's reviewed and approved\n\n## Code of Conduct\n\nPlease note we have a code of conduct, please follow it in all your interactions with the project.\n\n- Be respectful and inclusive\n- Be collaborative\n- When disagreeing, try to understand why\n- A diverse community is a strong community\n\n## Questions?\n\nDon't hesitate to open an issue if you have any questions about contributing to FastAPI-MCP.", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "medium", "star_count": 6708, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 143, "word_count": 591}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<p align=\"center\"><a href=\"https://github.com/tadata-org/fastapi_mcp\"><img src=\"https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c\" alt=\"fastapi-to-mcp\" height=100/></a></p>\n\n<div align=\"center\">\n  <span style=\"font-size: 0.85em; font-weight: normal;\">Built by <a href=\"https://tadata.com\">Tadata</a></span>\n</div>\n\n<h1 align=\"center\">\n  FastAPI-MCP\n</h1>\n\n<p align=\"center\">Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!</p>\n<div align=\"center\">\n\n[![PyPI version](https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&label=pypi%20package)](https://pypi.org/project/fastapi-mcp/)\n[![Python Versions](https://img.shields.io/pypi/pyversions/fastapi-mcp.svg)](https://pypi.org/project/fastapi-mcp/)\n[![FastAPI](https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&logoColor=white)](#)\n[![CI](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg)](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml)\n[![Coverage](https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg)](https://codecov.io/gh/tadata-org/fastapi_mcp)\n\n</div>\n\n\n<p align=\"center\"><a href=\"https://github.com/tadata-org/fastapi_mcp\"><img src=\"https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c\" alt=\"fastapi-mcp-usage\" height=\"400\"/></a></p>\n\n\n## Features\n\n- **Authentication** built in, using your existing FastAPI dependencies!\n\n- **FastAPI-native:** Not just another OpenAPI -> MCP converter\n\n- **Zero/Minimal configuration** required - just point it at your FastAPI app and it works\n\n- **Preserving schemas** of your request models and response models\n\n- **Preserve documentation** of all your endpoints, just as it is in Swagger\n\n- **Flexible deployment** - Mount your MCP server to the same app, or deploy separately\n\n- **ASGI transport** - Uses FastAPI's ASGI interface directly for efficient communication\n\n\n## Hosted Solution\n\nIf you prefer a managed hosted solution check out [tadata.com](https://tadata.com).\n\n## Installation\n\nWe recommend using [uv](https://docs.astral.sh/uv/), a fast Python package installer:\n\n```bash\nuv add fastapi-mcp\n```\n\nAlternatively, you can install with pip:\n\n```bash\npip install fastapi-mcp\n```\n\n## Basic Usage\n\nThe simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:\n\n```python\nfrom fastapi import FastAPI\nfrom fastapi_mcp import FastApiMCP\n\napp = FastAPI()\n\nmcp = FastApiMCP(app)\n\n# Mount the MCP server directly to your FastAPI app\nmcp.mount()\n```\n\nThat's it! Your auto-generated MCP server is now available at `https://app.base.url/mcp`.\n\n## Documentation, Examples and Advanced Usage\n\nFastAPI-MCP provides [comprehensive documentation](https://fastapi-mcp.tadata.com/). Additionaly, check out the [examples directory](examples) for code samples demonstrating these features in action.\n\n## FastAPI-first Approach\n\nFastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:\n\n- **Native dependencies**: Secure your MCP endpoints using familiar FastAPI `Depends()` for authentication and authorization\n\n- **ASGI transport**: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API\n\n- **Unified infrastructure**: Your FastAPI app doesn't need to run separately from the MCP server (though [separate deployment](https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app) is also supported)\n\nThis design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.\n\n\n## Development and Contributing\n\nThank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.\n\nBefore you get started, please see our [Contribution Guide](CONTRIBUTING.md).\n\n## Community\n\nJoin [MCParty Slack community](https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg) to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.\n\n## Requirements\n\n- Python 3.10+ (Recommended 3.12)\n- uv\n\n## License\n\nMIT License. Copyright (c) 2025 Tadata Inc.\n", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 6708, "quality_score": 0.7899999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 114, "word_count": 431}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<p align=\"center\"><a href=\"https://github.com/tadata-org/fastapi_mcp\"><img src=\"https://github.com/user-attachments/assets/609d5b8b-37a1-42c4-87e2-f045b60026b1\" alt=\"fastapi-to-mcp\" height=\"100\"/></a></p>\n<h1 align=\"center\">FastAPI-MCP</h1>\n<p align=\"center\">‰∏Ä‰∏™Èõ∂ÈÖçÁΩÆÂ∑•ÂÖ∑ÔºåÁî®‰∫éËá™Âä®Â∞Ü FastAPI Á´ØÁÇπÂÖ¨ÂºÄ‰∏∫Ê®°Âûã‰∏ä‰∏ãÊñáÂçèËÆÆÔºàMCPÔºâÂ∑•ÂÖ∑„ÄÇ</p>\n<div align=\"center\">\n\n[![PyPI version](https://badge.fury.io/py/fastapi-mcp.svg)](https://pypi.org/project/fastapi-mcp/)\n[![Python Versions](https://img.shields.io/pypi/pyversions/fastapi-mcp.svg)](https://pypi.org/project/fastapi-mcp/)\n[![FastAPI](https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&logoColor=white)](#)\n![](https://badge.mcpx.dev?type=dev 'MCP Dev')\n[![CI](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg)](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg)](https://codecov.io/gh/tadata-org/fastapi_mcp)\n\n</div>\n\n<p align=\"center\"><a href=\"https://github.com/tadata-org/fastapi_mcp\"><img src=\"https://github.com/user-attachments/assets/1cba1bf2-2fa4-46c7-93ac-1e9bb1a95257\" alt=\"fastapi-mcp-usage\" height=\"400\"/></a></p>\n\n> Ê≥®ÊÑèÔºöÊúÄÊñ∞ÁâàÊú¨ËØ∑ÂèÇÈòÖ [README.md](README.md).\n\n## ÁâπÁÇπ\n\n- **Áõ¥Êé•ÈõÜÊàê** - Áõ¥Êé•Â∞Ü MCP ÊúçÂä°Âô®ÊåÇËΩΩÂà∞ÊÇ®ÁöÑ FastAPI Â∫îÁî®\n- **Èõ∂ÈÖçÁΩÆ** - Âè™ÈúÄÊåáÂêëÊÇ®ÁöÑ FastAPI Â∫îÁî®Âç≥ÂèØÂ∑•‰Ωú\n- **Ëá™Âä®ÂèëÁé∞** - ÊâÄÊúâ FastAPI Á´ØÁÇπÂπ∂ËΩ¨Êç¢‰∏∫ MCP Â∑•ÂÖ∑\n- **‰øùÁïôÊ®°Âºè** - ‰øùÁïôÊÇ®ÁöÑËØ∑Ê±ÇÊ®°ÂûãÂíåÂìçÂ∫îÊ®°ÂûãÁöÑÊ®°Âºè\n- **‰øùÁïôÊñáÊ°£** - ‰øùÁïôÊâÄÊúâÁ´ØÁÇπÁöÑÊñáÊ°£ÔºåÂ∞±ÂÉèÂú® Swagger ‰∏≠‰∏ÄÊ†∑\n- **ÁÅµÊ¥ªÈÉ®ÁΩ≤** - Â∞Ü MCP ÊúçÂä°Âô®ÊåÇËΩΩÂà∞Âêå‰∏ÄÂ∫îÁî®ÔºåÊàñÂçïÁã¨ÈÉ®ÁΩ≤\n- **ASGI ‰º†Ëæì** - ÈªòËÆ§‰ΩøÁî® FastAPI ÁöÑ ASGI Êé•Âè£Áõ¥Êé•ÈÄö‰ø°ÔºåÊèêÈ´òÊïàÁéá\n\n## ÂÆâË£Ö\n\nÊàë‰ª¨Êé®Ëçê‰ΩøÁî® [uv](https://docs.astral.sh/uv/)Ôºå‰∏Ä‰∏™Âø´ÈÄüÁöÑ Python ÂåÖÂÆâË£ÖÂô®Ôºö\n\n```bash\nuv add fastapi-mcp\n```\n\nÊàñËÄÖÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî® pip ÂÆâË£ÖÔºö\n\n```bash\npip install fastapi-mcp\n```\n\n## Âü∫Êú¨Áî®Ê≥ï\n\n‰ΩøÁî® FastAPI-MCP ÁöÑÊúÄÁÆÄÂçïÊñπÊ≥ïÊòØÁõ¥Êé•Â∞Ü MCP ÊúçÂä°Âô®Ê∑ªÂä†Âà∞ÊÇ®ÁöÑ FastAPI Â∫îÁî®‰∏≠Ôºö\n\n```python\nfrom fastapi import FastAPI\nfrom fastapi_mcp import FastApiMCP\n\napp = FastAPI()\n\nmcp = FastApiMCP(app)\n\n# Áõ¥Êé•Â∞Ü MCP ÊúçÂä°Âô®ÊåÇËΩΩÂà∞ÊÇ®ÁöÑ FastAPI Â∫îÁî®\nmcp.mount()\n```\n\nÂ∞±ÊòØËøôÊ†∑ÔºÅÊÇ®ÁöÑËá™Âä®ÁîüÊàêÁöÑ MCP ÊúçÂä°Âô®Áé∞Âú®ÂèØ‰ª•Âú® `https://app.base.url/mcp` ËÆøÈóÆ„ÄÇ\n\n## Â∑•ÂÖ∑ÂëΩÂêç\n\nFastAPI-MCP ‰ΩøÁî® FastAPI Ë∑ØÁî±‰∏≠ÁöÑ`operation_id`‰Ωú‰∏∫ MCP Â∑•ÂÖ∑ÁöÑÂêçÁß∞„ÄÇÂ¶ÇÊûúÊÇ®‰∏çÊåáÂÆö`operation_id`ÔºåFastAPI ‰ºöËá™Âä®ÁîüÊàê‰∏Ä‰∏™Ôºå‰ΩÜËøô‰∫õÂêçÁß∞ÂèØËÉΩÊØîËæÉÊô¶Ê∂©„ÄÇ\n\nÊØîËæÉ‰ª•‰∏ã‰∏§‰∏™Á´ØÁÇπÂÆö‰πâÔºö\n\n```python\n# Ëá™Âä®ÁîüÊàêÁöÑ operation_idÔºàÁ±ª‰ºº‰∫é \"read_user_users__user_id__get\"Ôºâ\n@app.get(\"/users/{user_id}\")\nasync def read_user(user_id: int):\n    return {\"user_id\": user_id}\n\n# ÊòæÂºè operation_idÔºàÂ∑•ÂÖ∑Â∞ÜË¢´ÂëΩÂêç‰∏∫ \"get_user_info\"Ôºâ\n@app.get(\"/users/{user_id}\", operation_id=\"get_user_info\")\nasync def read_user(user_id: int):\n    return {\"user_id\": user_id}\n```\n\n‰∏∫‰∫ÜËé∑ÂæóÊõ¥Ê∏ÖÊô∞„ÄÅÊõ¥Áõ¥ËßÇÁöÑÂ∑•ÂÖ∑ÂêçÁß∞ÔºåÊàë‰ª¨Âª∫ËÆÆÂú® FastAPI Ë∑ØÁî±ÂÆö‰πâ‰∏≠Ê∑ªÂä†ÊòæÂºèÁöÑ`operation_id`ÂèÇÊï∞„ÄÇ\n\nË¶Å‰∫ÜËß£Êõ¥Â§ö‰ø°ÊÅØÔºåËØ∑ÈòÖËØª FastAPI ÂÆòÊñπÊñáÊ°£‰∏≠ÂÖ≥‰∫é [Ë∑ØÂæÑÊìç‰ΩúÁöÑÈ´òÁ∫ßÈÖçÁΩÆ](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/) ÁöÑÈÉ®ÂàÜ„ÄÇ\n\n## È´òÁ∫ßÁî®Ê≥ï\n\nFastAPI-MCP Êèê‰æõ‰∫ÜÂ§öÁßçÊñπÂºèÊù•Ëá™ÂÆö‰πâÂíåÊéßÂà∂ MCP ÊúçÂä°Âô®ÁöÑÂàõÂª∫ÂíåÈÖçÁΩÆ„ÄÇ‰ª•‰∏ãÊòØ‰∏Ä‰∫õÈ´òÁ∫ßÁî®Ê≥ïÊ®°ÂºèÔºö\n\n### Ëá™ÂÆö‰πâÊ®°ÂºèÊèèËø∞\n\n```python\nfrom fastapi import FastAPI\nfrom fastapi_mcp import FastApiMCP\n\napp = FastAPI()\n\nmcp = FastApiMCP(\n    app,\n    name=\"ÊàëÁöÑ API MCP\",\n    describe_all_responses=True,     # Âú®Â∑•ÂÖ∑ÊèèËø∞‰∏≠ÂåÖÂê´ÊâÄÊúâÂèØËÉΩÁöÑÂìçÂ∫îÊ®°Âºè\n    describe_full_response_schema=True  # Âú®Â∑•ÂÖ∑ÊèèËø∞‰∏≠ÂåÖÂê´ÂÆåÊï¥ÁöÑ JSON Ê®°Âºè\n)\n\nmcp.mount()\n```\n\n### Ëá™ÂÆö‰πâÂÖ¨ÂºÄÁöÑÁ´ØÁÇπ\n\nÊÇ®ÂèØ‰ª•‰ΩøÁî® Open API Êìç‰Ωú ID ÊàñÊ†áÁ≠æÊù•ÊéßÂà∂Âì™‰∫õ FastAPI Á´ØÁÇπÊö¥Èú≤‰∏∫ MCP Â∑•ÂÖ∑Ôºö\n\n```python\nfrom fastapi import FastAPI\nfrom fastapi_mcp import FastApiMCP\n\napp = FastAPI()\n\n# ‰ªÖÂåÖÂê´ÁâπÂÆöÊìç‰Ωú\nmcp = FastApiMCP(\n    app,\n    include_operations=[\"get_user\", \"create_user\"]\n)\n\n# ÊéíÈô§ÁâπÂÆöÊìç‰Ωú\nmcp = FastApiMCP(\n    app,\n    exclude_operations=[\"delete_user\"]\n)\n\n# ‰ªÖÂåÖÂê´ÂÖ∑ÊúâÁâπÂÆöÊ†áÁ≠æÁöÑÊìç‰Ωú\nmcp = FastApiMCP(\n    app,\n    include_tags=[\"users\", \"public\"]\n)\n\n# ÊéíÈô§ÂÖ∑ÊúâÁâπÂÆöÊ†áÁ≠æÁöÑÊìç‰Ωú\nmcp = FastApiMCP(\n    app,\n    exclude_tags=[\"admin\", \"internal\"]\n)\n\n# ÁªìÂêàÊìç‰Ωú ID ÂíåÊ†áÁ≠æÔºàÂåÖÂê´Ê®°ÂºèÔºâ\nmcp = FastApiMCP(\n    app,\n    include_operations=[\"user_login\"],\n    include_tags=[\"public\"]\n)\n\nmcp.mount()\n```\n\nÂÖ≥‰∫éËøáÊª§ÁöÑÊ≥®ÊÑè‰∫ãÈ°πÔºö\n- ÊÇ®‰∏çËÉΩÂêåÊó∂‰ΩøÁî®`include_operations`Âíå`exclude_operations`\n- ÊÇ®‰∏çËÉΩÂêåÊó∂‰ΩøÁî®`include_tags`Âíå`exclude_tags`\n- ÊÇ®ÂèØ‰ª•Â∞ÜÊìç‰ΩúËøáÊª§‰∏éÊ†áÁ≠æËøáÊª§ÁªìÂêà‰ΩøÁî®Ôºà‰æãÂ¶ÇÔºå‰ΩøÁî®`include_operations`Âíå`include_tags`Ôºâ\n- ÂΩìÁªìÂêàËøáÊª§Âô®Êó∂ÔºåÂ∞ÜÈááÂèñË¥™Â©™ÊñπÊ≥ï„ÄÇÂåπÈÖç‰ªª‰∏ÄÊ†áÂáÜÁöÑÁ´ØÁÇπÈÉΩÂ∞ÜË¢´ÂåÖÂê´\n\n### ‰∏éÂéüÂßã FastAPI Â∫îÁî®ÂàÜÂºÄÈÉ®ÁΩ≤\n\nÊÇ®‰∏çÈôê‰∫éÂú®ÂàõÂª∫ MCP ÁöÑÂêå‰∏Ä‰∏™ FastAPI Â∫îÁî®‰∏äÊèê‰æõ MCP ÊúçÂä°„ÄÇ\n\nÊÇ®ÂèØ‰ª•‰ªé‰∏Ä‰∏™ FastAPI Â∫îÁî®ÂàõÂª∫ MCP ÊúçÂä°Âô®ÔºåÂπ∂Â∞ÜÂÖ∂ÊåÇËΩΩÂà∞Âè¶‰∏Ä‰∏™Â∫îÁî®‰∏äÔºö\n\n```python\nfrom fastapi import FastAPI\nfrom fastapi_mcp import FastApiMCP\n\n# ÊÇ®ÁöÑ API Â∫îÁî®\napi_app = FastAPI()\n# ... Âú® api_app ‰∏äÂÆö‰πâÊÇ®ÁöÑ API Á´ØÁÇπ ...\n\n# ‰∏Ä‰∏™ÂçïÁã¨ÁöÑ MCP ÊúçÂä°Âô®Â∫îÁî®\nmcp_app = FastAPI()\n\n# ‰ªé API Â∫îÁî®ÂàõÂª∫ MCP ÊúçÂä°Âô®\nmcp = FastApiMCP(api_app)\n\n# Â∞Ü MCP ÊúçÂä°Âô®ÊåÇËΩΩÂà∞ÂçïÁã¨ÁöÑÂ∫îÁî®\nmcp.mount(mcp_app)\n\n# Áé∞Âú®ÊÇ®ÂèØ‰ª•ÂàÜÂà´ËøêË°å‰∏§‰∏™Â∫îÁî®Ôºö\n# uvicorn main:api_app --host api-host --port 8001\n# uvicorn main:mcp_app --host mcp-host --port 8000\n```\n\n### Âú® MCP ÊúçÂä°Âô®ÂàõÂª∫ÂêéÊ∑ªÂä†Á´ØÁÇπ\n\nÂ¶ÇÊûúÊÇ®Âú®ÂàõÂª∫ MCP ÊúçÂä°Âô®ÂêéÂêë FastAPI Â∫îÁî®Ê∑ªÂä†Á´ØÁÇπÔºåÊÇ®ÈúÄË¶ÅÂà∑Êñ∞ÊúçÂä°Âô®‰ª•ÂåÖÂê´ÂÆÉ‰ª¨Ôºö\n\n```python\nfrom fastapi import FastAPI\nfrom fastapi_mcp import FastApiMCP\n\napp = FastAPI()\n# ... ÂÆö‰πâÂàùÂßãÁ´ØÁÇπ ...\n\n# ÂàõÂª∫ MCP ÊúçÂä°Âô®\nmcp = FastApiMCP(app)\nmcp.mount()\n\n# Âú® MCP ÊúçÂä°Âô®ÂàõÂª∫ÂêéÊ∑ªÂä†Êñ∞Á´ØÁÇπ\n@app.get(\"/new/endpoint/\", operation_id=\"new_endpoint\")\nasync def new_endpoint():\n    return {\"message\": \"Hello, world!\"}\n\n# Âà∑Êñ∞ MCP ÊúçÂä°Âô®‰ª•ÂåÖÂê´Êñ∞Á´ØÁÇπ\nmcp.setup_server()\n```\n\n### ‰∏é FastAPI Â∫îÁî®ÁöÑÈÄö‰ø°\n\nFastAPI-MCP ÈªòËÆ§‰ΩøÁî® ASGI ‰º†ËæìÔºåËøôÊÑèÂë≥ÁùÄÂÆÉÁõ¥Êé•‰∏éÊÇ®ÁöÑ FastAPI Â∫îÁî®ÈÄö‰ø°ÔºåËÄå‰∏çÈúÄË¶ÅÂèëÈÄÅ HTTP ËØ∑Ê±Ç„ÄÇËøôÊ†∑Êõ¥È´òÊïàÔºå‰πü‰∏çÈúÄË¶ÅÂü∫Á°Ä URL„ÄÇ\n\nÂ¶ÇÊûúÊÇ®ÈúÄË¶ÅÊåáÂÆöËá™ÂÆö‰πâÂü∫Á°Ä URL Êàñ‰ΩøÁî®‰∏çÂêåÁöÑ‰º†ËæìÊñπÊ≥ïÔºåÊÇ®ÂèØ‰ª•Êèê‰æõËá™Â∑±ÁöÑ `httpx.AsyncClient`Ôºö\n\n```python\nimport httpx\nfrom fastapi import FastAPI\nfrom fastapi_mcp import FastApiMCP\n\napp = FastAPI()\n\n# ‰ΩøÁî®Â∏¶ÊúâÁâπÂÆöÂü∫Á°Ä URL ÁöÑËá™ÂÆö‰πâ HTTP ÂÆ¢Êà∑Á´Ø\ncustom_client = httpx.AsyncClient(\n    base_url=\"https://api.example.com\",\n    timeout=30.0\n)\n\nmcp = FastApiMCP(\n    app,\n    http_client=custom_client\n)\n\nmcp.mount()\n```\n\n## Á§∫‰æã\n\nËØ∑ÂèÇÈòÖ [examples](examples) ÁõÆÂΩï‰ª•Ëé∑ÂèñÂÆåÊï¥Á§∫‰æã„ÄÇ\n\n## ‰ΩøÁî® SSE ËøûÊé•Âà∞ MCP ÊúçÂä°Âô®\n\n‰∏ÄÊó¶ÊÇ®ÁöÑÈõÜÊàê‰∫Ü MCP ÁöÑ FastAPI Â∫îÁî®ËøêË°åÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî®‰ªª‰ΩïÊîØÊåÅ SSE ÁöÑ MCP ÂÆ¢Êà∑Á´ØËøûÊé•Âà∞ÂÆÉÔºå‰æãÂ¶Ç CursorÔºö\n\n1. ËøêË°åÊÇ®ÁöÑÂ∫îÁî®„ÄÇ\n\n2. Âú® Cursor -> ËÆæÁΩÆ -> MCP ‰∏≠Ôºå‰ΩøÁî®ÊÇ®ÁöÑ MCP ÊúçÂä°Âô®Á´ØÁÇπÁöÑURLÔºà‰æãÂ¶ÇÔºå`http://localhost:8000/mcp`Ôºâ‰Ωú‰∏∫ sse„ÄÇ\n\n3. Cursor Â∞ÜËá™Âä®ÂèëÁé∞ÊâÄÊúâÂèØÁî®ÁöÑÂ∑•ÂÖ∑ÂíåËµÑÊ∫ê„ÄÇ\n\n## ‰ΩøÁî® [mcp-proxy stdio](https://github.com/sparfenyuk/mcp-proxy?tab=readme-ov-file#1-stdio-to-sse) ËøûÊé•Âà∞ MCP ÊúçÂä°Âô®\n\nÂ¶ÇÊûúÊÇ®ÁöÑ MCP ÂÆ¢Êà∑Á´Ø‰∏çÊîØÊåÅ SSEÔºå‰æãÂ¶Ç Claude DesktopÔºö\n\n1. ËøêË°åÊÇ®ÁöÑÂ∫îÁî®„ÄÇ\n\n2. ÂÆâË£Ö [mcp-proxy](https://github.com/sparfenyuk/mcp-proxy?tab=readme-ov-file#installing-via-pypi)Ôºå‰æãÂ¶ÇÔºö`uv tool install mcp-proxy`„ÄÇ\n\n3. Âú® Claude Desktop ÁöÑ MCP ÈÖçÁΩÆÊñá‰ª∂Ôºà`claude_desktop_config.json`Ôºâ‰∏≠Ê∑ªÂä†Ôºö\n\nÂú® Windows ‰∏äÔºö\n```json\n{\n  \"mcpServers\": {\n    \"my-api-mcp-proxy\": {\n        \"command\": \"mcp-proxy\",\n        \"args\": [\"http://127.0.0.1:8000/mcp\"]\n    }\n  }\n}\n```\nÂú® MacOS ‰∏äÔºö\n```json\n{\n  \"mcpServers\": {\n    \"my-api-mcp-proxy\": {\n        \"command\": \"/Full/Path/To/Your/Executable/mcp-proxy\",\n        \"args\": [\"http://127.0.0.1:8000/mcp\"]\n    }\n  }\n}\n```\nÈÄöËøáÂú®ÁªàÁ´ØËøêË°å`which mcp-proxy`Êù•ÊâæÂà∞ mcp-proxy ÁöÑË∑ØÂæÑ„ÄÇ\n\n4. Claude Desktop Â∞ÜËá™Âä®ÂèëÁé∞ÊâÄÊúâÂèØÁî®ÁöÑÂ∑•ÂÖ∑ÂíåËµÑÊ∫ê\n\n## ÂºÄÂèëÂíåË¥°ÁåÆ\n\nÊÑüË∞¢ÊÇ®ËÄÉËôë‰∏∫ FastAPI-MCP ÂÅöÂá∫Ë¥°ÁåÆÔºÅÊàë‰ª¨ÈºìÂä±Á§æÂå∫ÂèëÂ∏ÉÈóÆÈ¢òÂíåÊãâÂèñËØ∑Ê±Ç„ÄÇ\n\nÂú®ÂºÄÂßã‰πãÂâçÔºåËØ∑ÂèÇÈòÖÊàë‰ª¨ÁöÑ [Ë¥°ÁåÆÊåáÂçó](CONTRIBUTING.md)„ÄÇ\n\n## Á§æÂå∫\n\nÂä†ÂÖ• [MCParty Slack Á§æÂå∫](https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg)Ôºå‰∏éÂÖ∂‰ªñ MCP Áà±Â•ΩËÄÖËÅîÁ≥ªÔºåÊèêÈóÆÔºåÂπ∂ÂàÜ‰∫´ÊÇ®‰ΩøÁî® FastAPI-MCP ÁöÑÁªèÈ™å„ÄÇ\n\n## Ë¶ÅÊ±Ç\n\n- Python 3.10+ÔºàÊé®Ëçê3.12Ôºâ\n- uv\n\n## ËÆ∏ÂèØËØÅ\n\nMIT License. Copyright (c) 2024 Tadata Inc.\n", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": "README_zh-CN.md", "language": "markdown", "complexity": "complex", "star_count": 6708, "quality_score": 0.82, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 302, "word_count": 624}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from examples.shared.apps.items import app # The FastAPI app\nfrom examples.shared.setup import setup_logging\n\nfrom fastapi_mcp import FastApiMCP\n\nsetup_logging()\n\n# Add MCP server to the FastAPI app\nmcp = FastApiMCP(app)\n\n# Mount the MCP server to the FastAPI app\nmcp.mount()\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": "examples/01_basic_usage_example.py", "language": "python", "complexity": "simple", "star_count": 6708, "quality_score": 0.7899999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 18, "word_count": 47}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\n\"\"\"\nThis example shows how to describe the full response schema instead of just a response example.\n\"\"\"\nfrom examples.shared.apps.items import app # The FastAPI app\nfrom examples.shared.setup import setup_logging\n\nfrom fastapi_mcp import FastApiMCP\n\nsetup_logging()\n\n# Add MCP server to the FastAPI app\nmcp = FastApiMCP(\n    app,\n    name=\"Item API MCP\",\n    description=\"MCP server for the Item API\",\n    describe_full_response_schema=True,  # Describe the full response JSON-schema instead of just a response example\n    describe_all_responses=True,  # Describe all the possible responses instead of just the success (2XX) response\n)\n\nmcp.mount()\n\nif __name__ == \"__main__\":\n    import uvicorn\n    \n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": "examples/02_full_schema_description_example.py", "language": "python", "complexity": "medium", "star_count": 6708, "quality_score": 0.82, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 27, "word_count": 94}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis example shows how to customize exposing endpoints by filtering operation IDs and tags.\nNotes on filtering:\n- You cannot use both `include_operations` and `exclude_operations` at the same time\n- You cannot use both `include_tags` and `exclude_tags` at the same time\n- You can combine operation filtering with tag filtering (e.g., use `include_operations` with `include_tags`)\n- When combining filters, a greedy approach will be taken. Endpoints matching either criteria will be included\n\"\"\"\nfrom examples.shared.apps.items import app # The FastAPI app\nfrom examples.shared.setup import setup_logging\n\nfrom fastapi_mcp import FastApiMCP\n\nsetup_logging()\n\n# Examples demonstrating how to filter MCP tools by operation IDs and tags\n\n# Filter by including specific operation IDs\ninclude_operations_mcp = FastApiMCP(\n    app,\n    name=\"Item API MCP - Included Operations\",\n    include_operations=[\"get_item\", \"list_items\"],\n)\n\n# Filter by excluding specific operation IDs\nexclude_operations_mcp = FastApiMCP(\n    app,    \n    name=\"Item API MCP - Excluded Operations\",\n    exclude_operations=[\"create_item\", \"update_item\", \"delete_item\"],\n)\n\n# Filter by including specific tags\ninclude_tags_mcp = FastApiMCP(\n    app,\n    name=\"Item API MCP - Included Tags\",\n    include_tags=[\"items\"],\n)\n\n# Filter by excluding specific tags\nexclude_tags_mcp = FastApiMCP(\n    app,\n    name=\"Item API MCP - Excluded Tags\",\n    exclude_tags=[\"search\"],\n)\n\n# Combine operation IDs and tags (include mode)\ncombined_include_mcp = FastApiMCP(\n    app,\n    name=\"Item API MCP - Combined Include\",\n    include_operations=[\"delete_item\"],\n    include_tags=[\"search\"],\n)\n\n# Mount all MCP servers with different paths\ninclude_operations_mcp.mount(mount_path=\"/include-operations-mcp\")\nexclude_operations_mcp.mount(mount_path=\"/exclude-operations-mcp\")\ninclude_tags_mcp.mount(mount_path=\"/include-tags-mcp\")\nexclude_tags_mcp.mount(mount_path=\"/exclude-tags-mcp\")\ncombined_include_mcp.mount(mount_path=\"/combined-include-mcp\")\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    print(\"Server is running with multiple MCP endpoints:\")\n    print(\" - /include-operations-mcp: Only get_item and list_items operations\")\n    print(\" - /exclude-operations-mcp: All operations except create_item, update_item, and delete_item\")\n    print(\" - /include-tags-mcp: Only operations with the 'items' tag\")\n    print(\" - /exclude-tags-mcp: All operations except those with the 'search' tag\")\n    print(\" - /combined-include-mcp: Operations with 'search' tag or delete_item operation\")\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": "examples/03_custom_exposed_endpoints_example.py", "language": "python", "complexity": "simple", "star_count": 6708, "quality_score": 0.82, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 71, "word_count": 279}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis example shows how to run the MCP server and the FastAPI app separately.\nYou can create an MCP server from one FastAPI app, and mount it to a different app.\n\"\"\"\nfrom fastapi import FastAPI\n\nfrom examples.shared.apps.items import app\nfrom examples.shared.setup import setup_logging\n\nfrom fastapi_mcp import FastApiMCP\n\nsetup_logging()\n\nMCP_SERVER_HOST = \"localhost\"\nMCP_SERVER_PORT = 8000\nITEMS_API_HOST = \"localhost\"\nITEMS_API_PORT = 8001\n\n\n# Take the FastAPI app only as a source for MCP server generation\nmcp = FastApiMCP(app)\n\n# Mount the MCP server to a separate FastAPI app\nmcp_app = FastAPI()\nmcp.mount(mcp_app)\n\n# Run the MCP server separately from the original FastAPI app.\n# It still works üöÄ\n# Your original API is **not exposed**, only via the MCP server.\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(mcp_app, host=\"0.0.0.0\", port=8000)", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": "examples/04_separate_server_example.py", "language": "python", "complexity": "simple", "star_count": 6708, "quality_score": 0.82, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 33, "word_count": 129}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis example shows how to re-register tools if you add endpoints after the MCP server was created.\n\"\"\"\nfrom examples.shared.apps.items import app # The FastAPI app\nfrom examples.shared.setup import setup_logging\n\nfrom fastapi_mcp import FastApiMCP\n\nsetup_logging()\n\nmcp = FastApiMCP(app) # Add MCP server to the FastAPI app\nmcp.mount() # MCP server\n\n\n# This endpoint will not be registered as a tool, since it was added after the MCP instance was created\n@app.get(\"/new/endpoint/\", operation_id=\"new_endpoint\", response_model=dict[str, str])\nasync def new_endpoint():\n    return {\"message\": \"Hello, world!\"}\n\n\n# But if you re-run the setup, the new endpoints will now be exposed.\nmcp.setup_server()\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    \n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n", "metadata": {"source": "github.com/tadata-org/fastapi_mcp", "file_path": "examples/05_reregister_tools_example.py", "language": "python", "complexity": "medium", "star_count": 6708, "quality_score": 0.85, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 29, "word_count": 106}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Test pipeline\non:\n  push:\n    branches:\n      - main\n  pull_request:\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: ['3.8', '3.9', '3.10', '3.11', '3.12']\n    steps:\n    - uses: actions/checkout@v2\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v2\n      with:\n        python-version: ${{ matrix.python-version }}\n    - name: Install dependencies\n      run: |\n        pip install --upgrade pip\n        pip install \"poetry<2\" \"tox<4\" \"tox-gh-actions<3\" \"coveralls<4\"\n    - name: Test with tox\n      run: tox\n    - name: Coveralls\n      run: coveralls --service github\n      env:\n        COVERALLS_PARALLEL: true\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        COVERALLS_FLAG_NAME: test-${{ matrix.python-version }}\n\n  finish-coveralls:\n    needs: test\n    runs-on: ubuntu-latest\n    steps:\n    - name: Install \"coveralls<4\"\n      run: pip install coveralls\n    - name: Coveralls Finished\n      run: coveralls --service github --finish\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": ".github/workflows/pipeline.yml", "language": "yaml", "complexity": "simple", "star_count": 4553, "quality_score": 0.6, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": false, "line_count": 42, "word_count": 109}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Publish Python üêç distributions üì¶ to PyPI and TestPyPI\non:\n  push:\n    tags:\n      - '[0-9]+.[0-9]+.[0-9]+*'\n  release:\n    types:\n      - published\njobs:\n  build-n-publish:\n    name: Build and publish Python üêç distributions üì¶ to PyPI and TestPyPI\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@master\n\n    - name: Set up Python 3.9\n      uses: actions/setup-python@v1\n      with:\n        python-version: 3.9\n\n    - name: Update version\n      run: sed -i \"s/^version = .*/version = '${{github.ref_name}}'/\" pyproject.toml\n\n    - name: Build a binary wheel and a source tarball\n      run: |\n        pip install poetry\n        poetry build\n\n    - name: Publish distribution üì¶ to Test PyPI\n      if: github.event_name == 'push'\n      uses: pypa/gh-action-pypi-publish@v1.5.0\n      with:\n        user: __token__\n        password: ${{ secrets.TEST_PYPI_API_TOKEN }}\n        repository_url: https://test.pypi.org/legacy/\n\n    - name: Publish distribution üì¶ to PyPI if triggered by release\n      if: github.event_name == 'release'\n      uses: pypa/gh-action-pypi-publish@v1.5.0\n      with:\n        user: __token__\n        password: ${{ secrets.PYPI_API_TOKEN }}\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": ".github/workflows/release.yml", "language": "yaml", "complexity": "simple", "star_count": 4553, "quality_score": 0.6, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": false, "line_count": 43, "word_count": 127}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "ci:\n  autoupdate_branch: \"main\"\n  autoupdate_schedule: monthly\nrepos:\n  - repo: https://github.com/pycqa/flake8\n    rev: 7.1.1\n    hooks:\n      - id: flake8\n        files: \"^connexion/\"\n        additional_dependencies:\n          - flake8-rst-docstrings==0.2.3\n\n  - repo: https://github.com/PyCQA/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n        name: isort\n        files: \"^connexion/\"\n        args: [\"--project\", \"connexion\", \"--check-only\", \"--diff\"]\n      - id: isort\n        name: isort examples\n        files: \"^examples/\"\n        args: [\"--thirdparty\", \"connexion\", \"--check-only\", \"--diff\"]\n      - id: isort\n        name: isort tests\n        files: \"^tests/\"\n        args: [\"--project\", \"conftest\", \"--thirdparty\", \"connexion\", \"--check-only\", \"--diff\"]\n\n  - repo: https://github.com/psf/black\n    rev: 22.3.0\n    hooks:\n      - id: black\n        name: black\n        files: \"^connexion/\"\n        args: [\"connexion\"]\n      - id: black\n        name: black examples\n        files: \"^examples/\"\n        args: [\"examples\"]\n      - id: black\n        name: black tests\n        files: \"^tests/\"\n        args: [\"tests\"]\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v0.981\n    hooks:\n      - id: mypy\n        files: \"^connexion/\"\n        args: [\"--ignore-missing-imports\", \"connexion\"]\n        additional_dependencies:\n          - types-jsonschema\n          - types-PyYAML\n          - types-requests\n        pass_filenames: false\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": ".pre-commit-config.yaml", "language": "yaml", "complexity": "simple", "star_count": 4553, "quality_score": 0.6, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 56, "word_count": 124}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: 2\n\nsphinx:\n  configuration: docs/conf.py\n\nbuild:\n  os: \"ubuntu-22.04\"\n  tools:\n    python: \"3.8\"\n  jobs:\n    post_create_environment:\n      # Install poetry\n      - python -m pip install poetry\n      # Tell poetry to not use a virtual environment\n      - poetry config virtualenvs.create false\n    post_install:\n      # Install dependencies with 'docs' dependency group\n      - VIRTUAL_ENV=$READTHEDOCS_VIRTUALENV_PATH python -m poetry install --with docs --all-extras\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": ".readthedocs.yaml", "language": "yaml", "complexity": "simple", "star_count": 4553, "quality_score": 0.63, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 19, "word_count": 53}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": " <a id=\"top\"></a>\n <p align=\"center\">\n     <img src=\"https://raw.githubusercontent.com/spec-first/connexion/main/docs/images/logo_banner.svg\" width=\"100%\"/>\n </p>\n <p align=\"center\">\n     <a href=\"https://pypi.org/project/connexion\"><img alt=\"coveralls\" src=\"https://img.shields.io/pypi/status/connexion.svg?style=flat-square&color=brightgreen\"></a>\n     <a href=\"https://pypi.org/project/connexion\"><img alt=\"PyPI version\" src=\"https://img.shields.io/pypi/v/connexion?color=brightgreen&style=flat-square\"></a>\n     <a href=\"https://pypistats.org/packages/connexion\"><img alt=\"PyPI\" src=\"https://img.shields.io/pypi/dm/connexion?style=flat-square&color=brightgreen\"></a>\n     <a href=\"https://github.com/spec-first/connexion/blob/main/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/pypi/l/connexion?style=flat-square&color=brightgreen\"></a>\n     <a href=\"https://github.com/spec-first/connexion/actions/workflows/pipeline.yml\"><img alt=\"GitHub Workflow Status\" src=\"https://img.shields.io/github/actions/workflow/status/spec-first/connexion/pipeline.yml?style=flat-square\"></a>\n     <a href=\"https://coveralls.io/github/spec-first/connexion?branch=main\"><img alt=\"Coveralls\" src=\"https://img.shields.io/coverallsCoverage/github/spec-first/connexion?style=flat-square\"></a>\n     <a href=\"https://gurubase.io/g/connexion\"><img alt=\"Gurubase\" src=\"https://img.shields.io/badge/Gurubase-Ask%20Connexion%20Guru-brightgreen?style=flat-square\"></a>\n     <br>\n     <br>\n     <a href=\"https://connexion.readthedocs.io/en/stable/\"><strong>Explore the docs ¬ª</strong></a>\n </p>\n\n---\n\nConnexion is a modern Python web framework that makes spec-first and api-first development easy.\nYou describe your API in an [OpenAPI][OpenAPI] (or [Swagger][Swagger]) specification with as much \ndetail as you want and Connexion will guarantee that it works as you specified.\n\nIt works either standalone, or in combination with any ASGI or WSGI-compatible framework!\n\n<p align=\"center\">\n   <br>\n   <a href=\"https://connexion.readthedocs.io/en/latest/v3.html\"><strong>üì¢ Connexion 3 was recently released! Read about the changes here ¬ª</strong></a>\n   <br>\n   <br>\n</p>\n\n## ‚ú® Features\n\nConnexion provides the following functionality **based on your specification**:\n\n- üöè **Automatic route registration**, no ``@route`` decorators needed\n- üîí **Authentication**, split from your application logic\n- üîé **Request and response validation** of headers, parameters, and body\n- üì¨ **Parameter parsing and injection**, no request object needed\n- üì® **Response serialization**, you can return regular Python objects\n- üì∫ **A Swagger UI console** with live documentation and ‚Äòtry it out‚Äô feature\n- üß© **Pluggability**, in all dimensions\n\nConnexion also **helps you write your OpenAPI specification** and develop against it by providing a command line interface which lets you test and mock your specification.\n\n```shell\n   connexion run openapi.yaml\n```\n\n <p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n\n## ü´∂ Sponsors\n\n<a href=\"https://www.ml6.eu\"><img src=\"https://raw.githubusercontent.com/spec-first/connexion/main/docs/images/sponsors/ML6.png\" title=ML6 height=\"100\"></a>\n<a href=\"https://www.devmark.ai/fern/?utm_source=connexion&utm_loc=readme&utm_type=logo\"><img src=\"https://raw.githubusercontent.com/spec-first/connexion/main/docs/images/sponsors/Fern.png\" title=Fern height=\"100\"></a>\n\nSponsors help us dedicate time to maintain Connexion. Want to help?\n\n<a href=\"https://github.com/sponsors/spec-first\"><strong>Explore the options ¬ª</strong></a>\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## ü™§ Why Connexion\n\nWith Connexion, you write the spec first. Connexion then calls your Python\ncode, handling the mapping from the specification to the code. This\nincentivizes you to write the specification so that all of your\ndevelopers can understand what your API does, even before you write a\nsingle line of code.\n\nIf multiple teams depend on your APIs, you can use Connexion to easily\nsend them the documentation of your API. This guarantees that your API will\nfollow the specification that you wrote. This is a different process from\nthe one offered by most frameworks, which generate a specification\n*after* you've written the code.\nSome disadvantages of generating specifications based on code is that\nthey often end up lacking details or mix your documentation with the implementation\nlogic of your application.\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## ‚öíÔ∏è How to Use\n\n### Installation\n\nYou can install connexion using pip:\n\n```shell\n    $ pip install connexion\n```\n\nConnexion provides 'extras' with optional dependencies to unlock additional features:\n\n- `swagger-ui`: Enables a Swagger UI console for your application.\n- `uvicorn`: Enables to run the your application using `app.run()` for\n  development instead of using an external ASGI server.\n- `flask`: Enables the `FlaskApp` to build applications compatible with the Flask\n  ecosystem.\n\nYou can install them as follows:\n\n```shell\n    $ pip install connexion[swagger-ui]\n    $ pip install connexion[swagger-ui,uvicorn]\n```\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n### Creating your application\n\nConnexion can be used either as a standalone application or as a middleware wrapping an existing\nASGI (or WSGI) application written using a different framework. The standalone application can be\nbuilt using either the `AsyncApp` or `FlaskApp`.\n\n- The `AsyncApp` is a lightweight application with native asynchronous support. Use it if you\n  are starting a new project and have no specific reason to use one of the other options.\n\n  ```Python\n      from connexion import AsyncApp\n\n      app = AsyncApp(__name__)\n  ```\n\n- The `FlaskApp` leverages the `Flask` framework, which is useful if you're migrating from\n  connexion 2.X or you want to leverage the `Flask` ecosystem.\n\n  ```python\n      from connexion import FlaskApp\n\n      app = FlaskApp(__name__)\n  ```\n\n- The `ConnexionMiddleware` can be wrapped around any existing ASGI or WSGI application.\n  Use it if you already have an application written in a different framework and want to add\n  functionality provided by connexion\n\n  ```python\n      from asgi_framework import App\n      from connexion import ConnexionMiddleware\n\n      app = App(__name__)\n      app = ConnexionMiddleware(app)\n  ```\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n### Registering an API\n\nWhile you can register individual routes on your application, Connexion really shines when you\nregister an API defined by an OpenAPI (or Swagger) specification.\nThe operation described in your specification is automatically linked to your Python view function via the ``operationId``\n\n**run.py**\n\n```python\n   def post_greeting(name: str, greeting: str):  # Paramaeters are automatically unpacked\n       return f\"{greeting} {name}\", 200          # Responses are automatically serialized\n\n   app.add_api(\"openapi.yaml\")\n```\n\n**openapi.yaml**\n\n```yaml\n   ...\n   paths:\n     /greeting/{name}:\n       post:\n         operationId: run.post_greeting\n         responses:\n           '200':\n             content:\n               text/plain:\n                 schema:\n                   type: string\n         parameters:\n           - name: name\n             in: path\n             required: true\n             schema:\n               type: string\n           - name: greeting\n             in: query\n             required: true\n             schema:\n               type: string\n```\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n### Running your application\n\nIf you installed connexion using `connexion[uvicorn]`, you can run it using the\n`run` method. This is only recommended for development:\n\n```python\n    app.run()\n```\n\nIn production, run your application using an ASGI server such as `uvicorn`. If you defined your\n`app` in a python module called `run.py`, you can run it as follows:\n\n```shell\n    $ uvicorn run:app\n```\n\nOr with gunicorn:\n\n```shell\n    $ gunicorn -k uvicorn.workers.UvicornWorker run:app\n```\n\n----\n\nNow you're able to run and use Connexion!\n\nSee the [examples][examples] folder for more examples.\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## üìú Changes\n\nA full changelog is maintained on the [GitHub releases page][Releases].\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## ü§≤ Contributing\n\nWe welcome your ideas, issues, and pull requests. Just follow the\nusual/standard GitHub practices.\n\nFor easy development, install connexion using poetry with all extras, and\ninstall the pre-commit hooks to automatically run black formatting and static analysis checks.\n\n```shell\n    pip install poetry\n    poetry install --all-extras\n    pre-commit install\n```\n\nYou can find out more about how Connexion works and where to apply your changes by having a look\nat our [architecture][Architecture].\n\nUnless you explicitly state otherwise in advance, any non trivial\ncontribution intentionally submitted for inclusion in this project by you\nto the steward of this repository shall be under the\nterms and conditions of Apache License 2.0 written below, without any\nadditional copyright information, terms or conditions.\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## üôè Thanks\n\nWe'd like to thank all of Connexion's contributors for working on this\nproject, Swagger/OpenAPI for their support, and Zalando for originally developing and releasing Connexion.\n\n## üìö Recommended Resources\n\nAbout the advantages of working spec-first:\n\n* [Blog Atlassian][Blog Atlassian]\n* [API guidelines Zalando][API guidelines Zalando]\n* [Blog ML6][Blog ML6]\n* [Blog Zalando][Blog Zalando]\n\nTools to help you work spec-first:\n\n* [Online swagger editor][Online swagger editor]\n* [VS Code plugin][VS Code plugin]\n* [Pycharm plugin][Pycharm plugin]\n\n[OpenAPI]: https://openapis.org/\n[Swagger]: http://swagger.io/open-source-integrations/\n[Blog atlassian]: https://www.atlassian.com/blog/technology/spec-first-api-development\n[Blog ML6]: https://blog.ml6.eu/why-we-decided-to-help-maintain-connexion-c9f449877083\n[Blog Zalando]: https://engineering.zalando.com/posts/2016/12/crafting-effective-microservices-in-python.html\n[API guidelines Zalando]: https://opensource.zalando.com/restful-api-guidelines/#api-first\n[Online swagger editor]: https://editor.swagger.io/\n[VS Code plugin]: https://marketplace.visualstudio.com/items?itemName=42Crunch.vscode-openapi\n[Pycharm plugin]: https://plugins.jetbrains.com/plugin/14837-openapi-swagger-editor\n[examples]: https://github.com/spec-first/connexion/blob/main/examples\n[Releases]: https://github.com/spec-first/connexion/releases\n[Architecture]: https://github.com/spec-first/connexion/blob/main/docs/images/architecture.png\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 4553, "quality_score": 0.69, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 286, "word_count": 1131}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nConnexion is a framework that automagically handles HTTP requests based on OpenAPI Specification\n(formerly known as Swagger Spec) of your API described in YAML format. Connexion allows you to\nwrite an OpenAPI specification, then maps the endpoints to your Python functions; this makes it\nunique, as many tools generate the specification based on your Python code. You can describe your\nREST API in as much detail as you want; then Connexion guarantees that it will work as you\nspecified.\n\"\"\"\n\nfrom .apps import AbstractApp  # NOQA\nfrom .apps.asynchronous import AsyncApp\nfrom .datastructures import NoContent  # NOQA\nfrom .exceptions import ProblemException  # NOQA\nfrom .problem import problem  # NOQA\nfrom .resolver import Resolution, Resolver, RestyResolver  # NOQA\nfrom .utils import not_installed_error  # NOQA\n\ntry:\n    from connexion.apps.flask import FlaskApi, FlaskApp\nexcept ImportError as e:  # pragma: no cover\n    _flask_not_installed_error = not_installed_error(\n        e, msg=\"Please install connexion using the 'flask' extra\"\n    )\n    FlaskApi = _flask_not_installed_error  # type: ignore\n    FlaskApp = _flask_not_installed_error  # type: ignore\n\nfrom connexion.apps.asynchronous import AsyncApi, AsyncApp\nfrom connexion.context import request\nfrom connexion.middleware import ConnexionMiddleware\n\nApp = FlaskApp\nApi = FlaskApi\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "connexion/__init__.py", "language": "python", "complexity": "medium", "star_count": 4553, "quality_score": 0.72, "features": {"has_code_examples": false, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 33, "word_count": 180}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis module defines an AbstractApp, which defines a standardized user interface for a Connexion\napplication.\n\"\"\"\nimport abc\nimport pathlib\nimport typing as t\n\nfrom starlette.testclient import TestClient\nfrom starlette.types import ASGIApp, Receive, Scope, Send\n\nfrom connexion.jsonifier import Jsonifier\nfrom connexion.lifecycle import ConnexionRequest, ConnexionResponse\nfrom connexion.middleware import ConnexionMiddleware, MiddlewarePosition, SpecMiddleware\nfrom connexion.middleware.lifespan import Lifespan\nfrom connexion.options import SwaggerUIOptions\nfrom connexion.resolver import Resolver\nfrom connexion.types import MaybeAwaitable\nfrom connexion.uri_parsing import AbstractURIParser\n\n\nclass AbstractApp:\n    \"\"\"\n    Abstract class for a Connexion Application. A Connexion Application provides an interface for a\n    framework application wrapped by Connexion Middleware. Since its main function is to provide an\n    interface, it delegates most of the work to the middleware and framework application.\n    \"\"\"\n\n    _middleware_app: SpecMiddleware\n    \"\"\"\n    The application wrapped by the ConnexionMiddleware, which in its turn wraps the framework\n    application.\n    \"\"\"\n\n    def __init__(\n        self,\n        import_name: str,\n        *,\n        lifespan: t.Optional[Lifespan] = None,\n        middlewares: t.Optional[list] = None,\n        specification_dir: t.Union[pathlib.Path, str] = \"\",\n        arguments: t.Optional[dict] = None,\n        auth_all_paths: t.Optional[bool] = None,\n        jsonifier: t.Optional[Jsonifier] = None,\n        pythonic_params: t.Optional[bool] = None,\n        resolver: t.Optional[t.Union[Resolver, t.Callable]] = None,\n        resolver_error: t.Optional[int] = None,\n        strict_validation: t.Optional[bool] = None,\n        swagger_ui_options: t.Optional[SwaggerUIOptions] = None,\n        uri_parser_class: t.Optional[AbstractURIParser] = None,\n        validate_responses: t.Optional[bool] = None,\n        validator_map: t.Optional[dict] = None,\n        security_map: t.Optional[dict] = None,\n    ) -> None:\n        \"\"\"\n        :param import_name: The name of the package or module that this object belongs to. If you\n            are using a single module, __name__ is always the correct value. If you however are\n            using a package, it‚Äôs usually recommended to hardcode the name of your package there.\n        :param lifespan: A lifespan context function, which can be used to perform startup and\n        :param middlewares: The list of middlewares to wrap around the application. Defaults to\n            :obj:`middleware.main.ConnexionMiddleware.default_middlewares`\n        :param specification_dir: The directory holding the specification(s). The provided path\n            should either be absolute or relative to the root path of the application. Defaults to\n            the root path.\n        :param arguments: Arguments to substitute the specification using Jinja.\n        :param auth_all_paths: whether to authenticate not paths not defined in the specification.\n            Defaults to False.\n        :param jsonifier: Custom jsonifier to overwrite json encoding for json responses.\n        :param pythonic_params: When True, CamelCase parameters are converted to snake_case and an\n            underscore is appended to any shadowed built-ins. Defaults to False.\n        :param resolver: Callable that maps operationId to a function or instance of\n            :class:`resolver.Resolver`.\n        :param resolver_error: Error code to return for operations for which the operationId could\n            not be resolved. If no error code is provided, the application will fail when trying to\n            start.\n        :param strict_validation: When True, extra form or query parameters not defined in the\n            specification result in a validation error. Defaults to False.\n        :param swagger_ui_options: Instance of :class:`options.SwaggerUIOptions` with\n            configuration options for the swagger ui.\n        :param uri_parser_class: Class to use for uri parsing. See :mod:`uri_parsing`.\n        :param validate_responses: Whether to validate responses against the specification. This has\n            an impact on performance. Defaults to False.\n        :param validator_map: A dictionary of validators to use. Defaults to\n            :obj:`validators.VALIDATOR_MAP`.\n        :param security_map: A dictionary of security handlers to use. Defaults to\n            :obj:`security.SECURITY_HANDLERS`\n        \"\"\"\n        self.middleware = ConnexionMiddleware(\n            self._middleware_app,\n            import_name=import_name,\n            lifespan=lifespan,\n            middlewares=middlewares,\n            specification_dir=specification_dir,\n            arguments=arguments,\n            auth_all_paths=auth_all_paths,\n            jsonifier=jsonifier,\n            swagger_ui_options=swagger_ui_options,\n            pythonic_params=pythonic_params,\n            resolver=resolver,\n            resolver_error=resolver_error,\n            strict_validation=strict_validation,\n            uri_parser_class=uri_parser_class,\n            validate_responses=validate_responses,\n            validator_map=validator_map,\n            security_map=security_map,\n        )\n\n    def add_middleware(\n        self,\n        middleware_class: t.Type[ASGIApp],\n        position: MiddlewarePosition = MiddlewarePosition.BEFORE_CONTEXT,\n        **options: t.Any,\n    ) -> None:\n        \"\"\"Add a middleware to the stack on the specified position.\n\n        :param middleware_class: Middleware class to add\n        :param position: Position to add the middleware, one of the MiddlewarePosition Enum\n        :param options: Options to pass to the middleware_class on initialization\n        \"\"\"\n        self.middleware.add_middleware(middleware_class, position=position, **options)\n\n    def add_api(\n        self,\n        specification: t.Union[pathlib.Path, str, dict],\n        *,\n        base_path: t.Optional[str] = None,\n        name: t.Optional[str] = None,\n        arguments: t.Optional[dict] = None,\n        auth_all_paths: t.Optional[bool] = None,\n        jsonifier: t.Optional[Jsonifier] = None,\n        pythonic_params: t.Optional[bool] = None,\n        resolver: t.Optional[t.Union[Resolver, t.Callable]] = None,\n        resolver_error: t.Optional[int] = None,\n        strict_validation: t.Optional[bool] = None,\n        swagger_ui_options: t.Optional[SwaggerUIOptions] = None,\n        uri_parser_class: t.Optional[AbstractURIParser] = None,\n        validate_responses: t.Optional[bool] = None,\n        validator_map: t.Optional[dict] = None,\n        security_map: t.Optional[dict] = None,\n        **kwargs,\n    ) -> t.Any:\n        \"\"\"\n        Register an API represented by a single OpenAPI specification on this application.\n        Multiple APIs can be registered on a single application.\n\n        :param specification: OpenAPI specification. Can be provided either as dict, a path\n            to file, or a URL.\n        :param base_path: Base path to host the API. This overrides the basePath / servers in the\n            specification.\n        :param name: Name to register the API with. If no name is passed, the base_path is used\n            as name instead.\n        :param arguments: Arguments to substitute the specification using Jinja.\n        :param auth_all_paths: whether to authenticate not paths not defined in the specification.\n            Defaults to False.\n        :param jsonifier: Custom jsonifier to overwrite json encoding for json responses.\n        :param pythonic_params: When True, CamelCase parameters are converted to snake_case and an\n            underscore is appended to any shadowed built-ins. Defaults to False.\n        :param resolver: Callable that maps operationId to a function or instance of\n            :class:`resolver.Resolver`.\n        :param resolver_error: Error code to return for operations for which the operationId could\n            not be resolved. If no error code is provided, the application will fail when trying to\n            start.\n        :param strict_validation: When True, extra form or query parameters not defined in the\n            specification result in a validation error. Defaults to False.\n        :param swagger_ui_options: A :class:`options.SwaggerUIOptions` instance with configuration\n            options for the swagger ui.\n        :param uri_parser_class: Class to use for uri parsing. See :mod:`uri_parsing`.\n        :param validate_responses: Whether to validate responses against the specification. This has\n            an impact on performance. Defaults to False.\n        :param validator_map: A dictionary of validators to use. Defaults to\n            :obj:`validators.VALIDATOR_MAP`\n        :param security_map: A dictionary of security handlers to use. Defaults to\n            :obj:`security.SECURITY_HANDLERS`\n        :param kwargs: Additional keyword arguments to pass to the `add_api` method of the managed\n            middlewares. This can be used to pass arguments to middlewares added beyond the default\n            ones.\n\n        :return: The Api registered on the middleware application wrapping the framework.\n        \"\"\"\n        return self.middleware.add_api(\n            specification,\n            base_path=base_path,\n            name=name,\n            arguments=arguments,\n            auth_all_paths=auth_all_paths,\n            jsonifier=jsonifier,\n            pythonic_params=pythonic_params,\n            resolver=resolver,\n            resolver_error=resolver_error,\n            strict_validation=strict_validation,\n            swagger_ui_options=swagger_ui_options,\n            uri_parser_class=uri_parser_class,\n            validate_responses=validate_responses,\n            validator_map=validator_map,\n            security_map=security_map,\n            **kwargs,\n        )\n\n    def add_url_rule(\n        self,\n        rule,\n        endpoint: t.Optional[str] = None,\n        view_func: t.Optional[t.Callable] = None,\n        **options,\n    ):\n        \"\"\"\n        Connects a URL rule.  Works exactly like the `route` decorator.\n\n        Basically this example::\n\n            @app.route('/')\n            def index():\n                pass\n\n        Is equivalent to the following::\n\n            def index():\n                pass\n            app.add_url_rule('/', 'index', index)\n\n        Internally`route` invokes `add_url_rule` so if you want to customize the behavior via\n        subclassing you only need to change this method.\n\n        :param rule: the URL rule as string.\n        :param endpoint: the name of the endpoint for the registered URL rule, which is used for\n            reverse lookup. Flask defaults to the name of the view function.\n        :param view_func: the function to call when serving a request to the provided endpoint.\n        :param options: the options to be forwarded to the underlying ``werkzeug.routing.Rule``\n            object.  A change to Werkzeug is handling of method options. methods is a list of\n            methods this rule should be limited to (`GET`, `POST` etc.).  By default a rule just\n            listens for `GET` (and implicitly `HEAD`).\n        \"\"\"\n\n    def route(self, rule: str, **options):\n        \"\"\"\n        A decorator that is used to register a view function for a\n        given URL rule.  This does the same thing as `add_url_rule`\n        but is intended for decorator usage::\n\n            @app.route('/')\n            def index():\n                return 'Hello World'\n\n        :param rule: the URL rule as string\n        :param options: the options to be forwarded to the underlying ``werkzeug.routing.Rule``\n                        object. A change to Werkzeug is handling of method options. methods is a\n                        list of methods this rule should be limited to (`GET`, `POST` etc.).\n                        By default a rule just listens for `GET` (and implicitly `HEAD`).\n        \"\"\"\n\n        def decorator(func: t.Callable) -> t.Callable:\n            self.add_url_rule(rule, view_func=func, **options)\n            return func\n\n        return decorator\n\n    @abc.abstractmethod\n    def add_error_handler(\n        self,\n        code_or_exception: t.Union[int, t.Type[Exception]],\n        function: t.Callable[\n            [ConnexionRequest, Exception], MaybeAwaitable[ConnexionResponse]\n        ],\n    ) -> None:\n        \"\"\"\n        Register a callable to handle application errors.\n\n        :param code_or_exception: An exception class or the status code of HTTP exceptions to\n            handle.\n        :param function: Callable that will handle exception, may be async.\n        \"\"\"\n\n    def test_client(self, **kwargs):\n        \"\"\"Creates a test client for this application. The keywords arguments passed in are\n        passed to the ``StarletteClient``.\"\"\"\n        return TestClient(self, **kwargs)\n\n    def run(self, import_string: t.Optional[str] = None, **kwargs):\n        \"\"\"Run the application using uvicorn.\n\n        :param import_string: application as import string (eg. \"main:app\"). This is needed to run\n                              using reload.\n        :param kwargs: kwargs to pass to `uvicorn.run`.\n        \"\"\"\n        self.middleware.run(import_string, **kwargs)\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        return await self.middleware(scope, receive, send)\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "connexion/apps/abstract.py", "language": "python", "complexity": "complex", "star_count": 4553, "quality_score": 0.8099999999999999, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 289, "word_count": 1351}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis module defines a native connexion asynchronous application.\n\"\"\"\n\nimport functools\nimport logging\nimport pathlib\nimport typing as t\n\nfrom starlette.responses import Response as StarletteResponse\nfrom starlette.routing import Router\nfrom starlette.types import Receive, Scope, Send\n\nfrom connexion.apps.abstract import AbstractApp\nfrom connexion.decorators import StarletteDecorator\nfrom connexion.jsonifier import Jsonifier\nfrom connexion.lifecycle import ConnexionRequest, ConnexionResponse\nfrom connexion.middleware.abstract import RoutedAPI, RoutedMiddleware\nfrom connexion.middleware.lifespan import Lifespan\nfrom connexion.operations import AbstractOperation\nfrom connexion.options import SwaggerUIOptions\nfrom connexion.resolver import Resolver\nfrom connexion.types import MaybeAwaitable\nfrom connexion.uri_parsing import AbstractURIParser\n\nlogger = logging.getLogger(__name__)\n\n\nclass AsyncOperation:\n    def __init__(\n        self,\n        fn: t.Callable,\n        jsonifier: Jsonifier,\n        operation_id: str,\n        pythonic_params: bool,\n    ) -> None:\n        self._fn = fn\n        self.jsonifier = jsonifier\n        self.operation_id = operation_id\n        self.pythonic_params = pythonic_params\n        functools.update_wrapper(self, fn)\n\n    @classmethod\n    def from_operation(\n        cls,\n        operation: AbstractOperation,\n        *,\n        pythonic_params: bool,\n        jsonifier: Jsonifier,\n    ) -> \"AsyncOperation\":\n        return cls(\n            operation.function,\n            jsonifier=jsonifier,\n            operation_id=operation.operation_id,\n            pythonic_params=pythonic_params,\n        )\n\n    @property\n    def fn(self) -> t.Callable:\n        decorator = StarletteDecorator(\n            pythonic_params=self.pythonic_params,\n            jsonifier=self.jsonifier,\n        )\n        return decorator(self._fn)\n\n    async def __call__(\n        self, scope: Scope, receive: Receive, send: Send\n    ) -> StarletteResponse:\n        response = await self.fn()\n        return await response(scope, receive, send)\n\n\nclass AsyncApi(RoutedAPI[AsyncOperation]):\n    def __init__(\n        self,\n        *args,\n        pythonic_params: bool,\n        jsonifier: t.Optional[Jsonifier] = None,\n        **kwargs,\n    ) -> None:\n        super().__init__(*args, **kwargs)\n        self.pythonic_params = pythonic_params\n        self.jsonifier = jsonifier or Jsonifier()\n        self.router = Router()\n        self.add_paths()\n\n    def make_operation(self, operation: AbstractOperation) -> AsyncOperation:\n        return AsyncOperation.from_operation(\n            operation, pythonic_params=self.pythonic_params, jsonifier=self.jsonifier\n        )\n\n\nclass AsyncASGIApp(RoutedMiddleware[AsyncApi]):\n\n    api_cls = AsyncApi\n\n    def __init__(self) -> None:\n        self.apis: t.Dict[str, t.List[AsyncApi]] = {}\n        self.operations: t.Dict[str, AsyncOperation] = {}\n        self.router = Router()\n        super().__init__(self.router)\n\n    def add_api(self, *args, name: t.Optional[str] = None, **kwargs):\n        api = super().add_api(*args, **kwargs)\n\n        if name is not None:\n            self.router.mount(api.base_path, api.router, name=name)\n        else:\n            self.router.mount(api.base_path, api.router)\n        return api\n\n    def add_url_rule(\n        self,\n        rule,\n        endpoint: t.Optional[str] = None,\n        view_func: t.Optional[t.Callable] = None,\n        methods: t.List[str] = None,\n        **options,\n    ):\n        self.router.add_route(rule, endpoint=view_func, name=endpoint, methods=methods)\n\n\nclass AsyncApp(AbstractApp):\n    \"\"\"Connexion Application based on ConnexionMiddleware wrapping a async Connexion application\n    based on starlette tools.\"\"\"\n\n    def __init__(\n        self,\n        import_name: str,\n        *,\n        lifespan: t.Optional[Lifespan] = None,\n        middlewares: t.Optional[list] = None,\n        specification_dir: t.Union[pathlib.Path, str] = \"\",\n        arguments: t.Optional[dict] = None,\n        auth_all_paths: t.Optional[bool] = None,\n        jsonifier: t.Optional[Jsonifier] = None,\n        pythonic_params: t.Optional[bool] = None,\n        resolver: t.Optional[t.Union[Resolver, t.Callable]] = None,\n        resolver_error: t.Optional[int] = None,\n        strict_validation: t.Optional[bool] = None,\n        swagger_ui_options: t.Optional[SwaggerUIOptions] = None,\n        uri_parser_class: t.Optional[AbstractURIParser] = None,\n        validate_responses: t.Optional[bool] = None,\n        validator_map: t.Optional[dict] = None,\n        security_map: t.Optional[dict] = None,\n    ) -> None:\n        \"\"\"\n        :param import_name: The name of the package or module that this object belongs to. If you\n            are using a single module, __name__ is always the correct value. If you however are\n            using a package, it‚Äôs usually recommended to hardcode the name of your package there.\n        :param lifespan: A lifespan context function, which can be used to perform startup and\n            shutdown tasks.\n        :param middlewares: The list of middlewares to wrap around the application. Defaults to\n            :obj:`middleware.main.ConnexionMiddleware.default_middlewares`\n        :param specification_dir: The directory holding the specification(s). The provided path\n            should either be absolute or relative to the root path of the application. Defaults to\n            the root path.\n        :param arguments: Arguments to substitute the specification using Jinja.\n        :param auth_all_paths: whether to authenticate not paths not defined in the specification.\n            Defaults to False.\n        :param jsonifier: Custom jsonifier to overwrite json encoding for json responses.\n        :param pythonic_params: When True, CamelCase parameters are converted to snake_case and an\n            underscore is appended to any shadowed built-ins. Defaults to False.\n        :param resolver: Callable that maps operationId to a function or instance of\n            :class:`resolver.Resolver`.\n        :param resolver_error: Error code to return for operations for which the operationId could\n            not be resolved. If no error code is provided, the application will fail when trying to\n            start.\n        :param strict_validation: When True, extra form or query parameters not defined in the\n            specification result in a validation error. Defaults to False.\n        :param swagger_ui_options: Instance of :class:`options.SwaggerUIOptions` with\n            configuration options for the swagger ui.\n        :param uri_parser_class: Class to use for uri parsing. See :mod:`uri_parsing`.\n        :param validate_responses: Whether to validate responses against the specification. This has\n            an impact on performance. Defaults to False.\n        :param validator_map: A dictionary of validators to use. Defaults to\n            :obj:`validators.VALIDATOR_MAP`.\n        :param security_map: A dictionary of security handlers to use. Defaults to\n            :obj:`security.SECURITY_HANDLERS`\n        \"\"\"\n        self._middleware_app: AsyncASGIApp = AsyncASGIApp()\n\n        super().__init__(\n            import_name,\n            lifespan=lifespan,\n            middlewares=middlewares,\n            specification_dir=specification_dir,\n            arguments=arguments,\n            auth_all_paths=auth_all_paths,\n            jsonifier=jsonifier,\n            pythonic_params=pythonic_params,\n            resolver=resolver,\n            resolver_error=resolver_error,\n            strict_validation=strict_validation,\n            swagger_ui_options=swagger_ui_options,\n            uri_parser_class=uri_parser_class,\n            validate_responses=validate_responses,\n            validator_map=validator_map,\n            security_map=security_map,\n        )\n\n    def add_url_rule(\n        self,\n        rule,\n        endpoint: t.Optional[str] = None,\n        view_func: t.Optional[t.Callable] = None,\n        **options,\n    ):\n        self._middleware_app.add_url_rule(\n            rule, endpoint=endpoint, view_func=view_func, **options\n        )\n\n    def add_error_handler(\n        self,\n        code_or_exception: t.Union[int, t.Type[Exception]],\n        function: t.Callable[\n            [ConnexionRequest, Exception], MaybeAwaitable[ConnexionResponse]\n        ],\n    ) -> None:\n        self.middleware.add_error_handler(code_or_exception, function)\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "connexion/apps/asynchronous.py", "language": "python", "complexity": "complex", "star_count": 4553, "quality_score": 0.84, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 221, "word_count": 719}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis module defines a FlaskApp, a Connexion application to wrap a Flask application.\n\"\"\"\nimport functools\nimport pathlib\nimport typing as t\n\nimport flask\nimport starlette.exceptions\nimport werkzeug.exceptions\nfrom a2wsgi import WSGIMiddleware\nfrom flask import Response as FlaskResponse\nfrom starlette.types import Receive, Scope, Send\n\nfrom connexion.apps.abstract import AbstractApp\nfrom connexion.decorators import FlaskDecorator\nfrom connexion.exceptions import ResolverError\nfrom connexion.frameworks import flask as flask_utils\nfrom connexion.jsonifier import Jsonifier\nfrom connexion.lifecycle import ConnexionRequest, ConnexionResponse\nfrom connexion.middleware.abstract import AbstractRoutingAPI, SpecMiddleware\nfrom connexion.middleware.lifespan import Lifespan\nfrom connexion.operations import AbstractOperation\nfrom connexion.options import SwaggerUIOptions\nfrom connexion.resolver import Resolver\nfrom connexion.types import MaybeAwaitable, WSGIApp\nfrom connexion.uri_parsing import AbstractURIParser\n\n\nclass FlaskOperation:\n    def __init__(\n        self,\n        fn: t.Callable,\n        jsonifier: Jsonifier,\n        operation_id: str,\n        pythonic_params: bool,\n    ) -> None:\n        self._fn = fn\n        self.jsonifier = jsonifier\n        self.operation_id = operation_id\n        self.pythonic_params = pythonic_params\n        functools.update_wrapper(self, fn)\n\n    @classmethod\n    def from_operation(\n        cls,\n        operation: AbstractOperation,\n        *,\n        pythonic_params: bool,\n        jsonifier: Jsonifier,\n    ) -> \"FlaskOperation\":\n        return cls(\n            fn=operation.function,\n            jsonifier=jsonifier,\n            operation_id=operation.operation_id,\n            pythonic_params=pythonic_params,\n        )\n\n    @property\n    def fn(self) -> t.Callable:\n        decorator = FlaskDecorator(\n            pythonic_params=self.pythonic_params,\n            jsonifier=self.jsonifier,\n        )\n        return decorator(self._fn)\n\n    def __call__(self, *args, **kwargs) -> FlaskResponse:\n        return self.fn(*args, **kwargs)\n\n\nclass FlaskApi(AbstractRoutingAPI):\n    def __init__(\n        self, *args, jsonifier: t.Optional[Jsonifier] = None, **kwargs\n    ) -> None:\n        self.jsonifier = jsonifier or Jsonifier(flask.json, indent=2)\n        super().__init__(*args, **kwargs)\n\n    def _set_base_path(self, base_path: t.Optional[str] = None) -> None:\n        super()._set_base_path(base_path)\n        self._set_blueprint()\n\n    def _set_blueprint(self):\n        endpoint = flask_utils.flaskify_endpoint(self.base_path) or \"/\"\n        self.blueprint = flask.Blueprint(\n            endpoint,\n            __name__,\n            url_prefix=self.base_path,\n        )\n\n    def _add_resolver_error_handler(self, method: str, path: str, err: ResolverError):\n        pass\n\n    def make_operation(self, operation):\n        return FlaskOperation.from_operation(\n            operation, pythonic_params=self.pythonic_params, jsonifier=self.jsonifier\n        )\n\n    @staticmethod\n    def _framework_path_and_name(\n        operation: AbstractOperation, path: str\n    ) -> t.Tuple[str, str]:\n        flask_path = flask_utils.flaskify_path(\n            path, operation.get_path_parameter_types()\n        )\n        endpoint_name = flask_utils.flaskify_endpoint(\n            operation.operation_id, operation.randomize_endpoint\n        )\n        return flask_path, endpoint_name\n\n    def _add_operation_internal(\n        self,\n        method: str,\n        path: str,\n        operation: t.Callable,\n        name: t.Optional[str] = None,\n    ) -> None:\n        self.blueprint.add_url_rule(path, name, operation, methods=[method])\n\n    def add_url_rule(\n        self,\n        rule,\n        endpoint: t.Optional[str] = None,\n        view_func: t.Optional[t.Callable] = None,\n        **options,\n    ):\n        return self.blueprint.add_url_rule(rule, endpoint, view_func, **options)\n\n\nclass FlaskASGIApp(SpecMiddleware):\n    def __init__(self, import_name, server_args: dict, **kwargs):\n        self.app = flask.Flask(import_name, **server_args)\n        self.app.json = flask_utils.FlaskJSONProvider(self.app)\n        self.app.url_map.converters[\"float\"] = flask_utils.NumberConverter\n        self.app.url_map.converters[\"int\"] = flask_utils.IntegerConverter\n\n        # Propagate Errors so we can handle them in the middleware\n        self.app.config[\"PROPAGATE_EXCEPTIONS\"] = True\n        self.app.config[\"TRAP_BAD_REQUEST_ERRORS\"] = True\n        self.app.config[\"TRAP_HTTP_EXCEPTIONS\"] = True\n\n        self.asgi_app = WSGIMiddleware(self.app.wsgi_app)\n\n    def add_api(self, specification, *, name: t.Optional[str] = None, **kwargs):\n        api = FlaskApi(specification, **kwargs)\n\n        if name is not None:\n            self.app.register_blueprint(api.blueprint, name=name)\n        else:\n            self.app.register_blueprint(api.blueprint)\n\n        return api\n\n    def add_url_rule(\n        self,\n        rule,\n        endpoint: t.Optional[str] = None,\n        view_func: t.Optional[t.Callable] = None,\n        **options,\n    ):\n        return self.app.add_url_rule(rule, endpoint, view_func, **options)\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        return await self.asgi_app(scope, receive, send)\n\n\nclass FlaskApp(AbstractApp):\n    \"\"\"Connexion Application based on ConnexionMiddleware wrapping a Flask application.\"\"\"\n\n    _middleware_app: FlaskASGIApp\n\n    def __init__(\n        self,\n        import_name: str,\n        *,\n        lifespan: t.Optional[Lifespan] = None,\n        middlewares: t.Optional[list] = None,\n        server_args: t.Optional[dict] = None,\n        specification_dir: t.Union[pathlib.Path, str] = \"\",\n        arguments: t.Optional[dict] = None,\n        auth_all_paths: t.Optional[bool] = None,\n        jsonifier: t.Optional[Jsonifier] = None,\n        pythonic_params: t.Optional[bool] = None,\n        resolver: t.Optional[t.Union[Resolver, t.Callable]] = None,\n        resolver_error: t.Optional[int] = None,\n        strict_validation: t.Optional[bool] = None,\n        swagger_ui_options: t.Optional[SwaggerUIOptions] = None,\n        uri_parser_class: t.Optional[AbstractURIParser] = None,\n        validate_responses: t.Optional[bool] = None,\n        validator_map: t.Optional[dict] = None,\n        security_map: t.Optional[dict] = None,\n    ):\n        \"\"\"\n        :param import_name: The name of the package or module that this object belongs to. If you\n            are using a single module, __name__ is always the correct value. If you however are\n            using a package, it‚Äôs usually recommended to hardcode the name of your package there.\n        :param lifespan: A lifespan context function, which can be used to perform startup and\n            shutdown tasks.\n        :param middlewares: The list of middlewares to wrap around the application. Defaults to\n            :obj:`middleware.main.ConnexionMiddleware.default_middlewares`\n        :param server_args: Arguments to pass to the Flask application.\n        :param specification_dir: The directory holding the specification(s). The provided path\n            should either be absolute or relative to the root path of the application. Defaults to\n            the root path.\n        :param arguments: Arguments to substitute the specification using Jinja.\n        :param auth_all_paths: whether to authenticate all paths not defined in the specification.\n            Defaults to False.\n        :param jsonifier: Custom jsonifier to overwrite json encoding for json responses.\n        :param pythonic_params: When True, CamelCase parameters are converted to snake_case and an\n            underscore is appended to any shadowed built-ins. Defaults to False.\n        :param resolver: Callable that maps operationId to a function or instance of\n            :class:`resolver.Resolver`.\n        :param resolver_error: Error code to return for operations for which the operationId could\n            not be resolved. If no error code is provided, the application will fail when trying to\n            start.\n        :param strict_validation: When True, extra form or query parameters not defined in the\n            specification result in a validation error. Defaults to False.\n        :param swagger_ui_options: Instance of :class:`options.SwaggerUIOptions` with\n            configuration options for the swagger ui.\n        :param uri_parser_class: Class to use for uri parsing. See :mod:`uri_parsing`.\n        :param validate_responses: Whether to validate responses against the specification. This has\n            an impact on performance. Defaults to False.\n        :param validator_map: A dictionary of validators to use. Defaults to\n            :obj:`validators.VALIDATOR_MAP`.\n        :param security_map: A dictionary of security handlers to use. Defaults to\n            :obj:`security.SECURITY_HANDLERS`\n        \"\"\"\n        self._middleware_app = FlaskASGIApp(import_name, server_args or {})\n\n        super().__init__(\n            import_name,\n            lifespan=lifespan,\n            middlewares=middlewares,\n            specification_dir=specification_dir,\n            arguments=arguments,\n            auth_all_paths=auth_all_paths,\n            jsonifier=jsonifier,\n            pythonic_params=pythonic_params,\n            resolver=resolver,\n            resolver_error=resolver_error,\n            strict_validation=strict_validation,\n            swagger_ui_options=swagger_ui_options,\n            uri_parser_class=uri_parser_class,\n            validate_responses=validate_responses,\n            validator_map=validator_map,\n            security_map=security_map,\n        )\n\n        self.app = self._middleware_app.app\n        self.app.register_error_handler(\n            werkzeug.exceptions.HTTPException, self._http_exception\n        )\n\n    def _http_exception(self, exc: werkzeug.exceptions.HTTPException):\n        \"\"\"Reraise werkzeug HTTPExceptions as starlette HTTPExceptions\"\"\"\n        raise starlette.exceptions.HTTPException(exc.code, detail=exc.description)\n\n    def add_url_rule(\n        self,\n        rule,\n        endpoint: t.Optional[str] = None,\n        view_func: t.Optional[t.Callable] = None,\n        **options,\n    ):\n        self._middleware_app.add_url_rule(\n            rule, endpoint=endpoint, view_func=view_func, **options\n        )\n\n    def add_error_handler(\n        self,\n        code_or_exception: t.Union[int, t.Type[Exception]],\n        function: t.Callable[\n            [ConnexionRequest, Exception], MaybeAwaitable[ConnexionResponse]\n        ],\n    ) -> None:\n        self.middleware.add_error_handler(code_or_exception, function)\n\n    def add_wsgi_middleware(\n        self, middleware: t.Type[WSGIApp], **options: t.Any\n    ) -> None:\n        \"\"\"Wrap the underlying Flask application with a WSGI middleware. Note that it will only be\n        called at the end of the middleware stack. Middleware that needs to act sooner, needs to\n        be added as ASGI middleware instead.\n\n        Adding multiple middleware using this method wraps each middleware around the previous one.\n\n        :param middleware: Middleware class to add\n        :param options: Options to pass to the middleware_class on initialization\n        \"\"\"\n        self._middleware_app.asgi_app.app = middleware(\n            self._middleware_app.asgi_app.app, **options  # type: ignore\n        )\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "connexion/apps/flask.py", "language": "python", "complexity": "complex", "star_count": 4553, "quality_score": 0.78, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 292, "word_count": 959}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis module defines a command-line interface (CLI) that runs an OpenAPI specification to be a\nstarting point for developing your API with Connexion.\n\"\"\"\n\nimport argparse\nimport importlib.metadata\nimport logging\nimport os\nimport sys\nimport typing as t\n\nimport connexion\nfrom connexion.apps import AbstractApp\nfrom connexion.mock import MockResolver\nfrom connexion.options import SwaggerUIOptions\n\nlogger = logging.getLogger(__name__)\n\nFLASK_APP = \"flask\"\nASYNC_APP = \"async\"\nAVAILABLE_APPS = {\n    FLASK_APP: \"connexion.apps.flask.FlaskApp\",\n    ASYNC_APP: \"connexion.apps.asynchronous.AsyncApp\",\n}\n\n\ndef run(app: AbstractApp, args: argparse.Namespace):\n    app.run(\"connexion.cli:create_app\", port=args.port, host=args.host, factory=True)\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--version\",\n    action=\"version\",\n    version=f\"Connexion {importlib.metadata.version('connexion')}\",\n)\n\nsubparsers = parser.add_subparsers()\nrun_parser = subparsers.add_parser(\"run\")\nrun_parser.set_defaults(func=run)\n\nrun_parser.add_argument(\"spec_file\", help=\"Path to OpenAPI specification.\")\nrun_parser.add_argument(\n    \"base_module_path\", nargs=\"?\", help=\"Root directory of handler code.\"\n)\nrun_parser.add_argument(\n    \"-p\", \"--port\", default=5000, type=int, help=\"Port to listen on.\"\n)\nrun_parser.add_argument(\n    \"-H\", \"--host\", default=\"127.0.0.1\", type=str, help=\"Host interface to bind on.\"\n)\nrun_parser.add_argument(\n    \"--stub\",\n    action=\"store_true\",\n    help=\"Returns status code 501, and `Not Implemented Yet` payload, for the endpoints which \"\n    \"handlers are not found.\",\n)\nrun_parser.add_argument(\n    \"--mock\",\n    choices=[\"all\", \"notimplemented\"],\n    help=\"Returns example data for all endpoints or for which handlers are not found.\",\n)\nrun_parser.add_argument(\n    \"--swagger-ui-path\",\n    help=\"Personalize what URL path the API console UI will be mounted.\",\n    default=\"/ui\",\n)\nrun_parser.add_argument(\n    \"--swagger-ui-template-dir\",\n    help=\"Path to a customized API console UI dashboard.\",\n)\nrun_parser.add_argument(\n    \"--auth-all-paths\",\n    help=\"Enable authentication to paths not defined in the spec.\",\n    action=\"store_true\",\n)\nrun_parser.add_argument(\n    \"--validate-responses\",\n    help=\"Enable validation of response values from operation handlers.\",\n    action=\"store_true\",\n)\nrun_parser.add_argument(\n    \"--strict-validation\",\n    help=\"Enable strict validation of request payloads.\",\n    action=\"store_true\",\n)\nrun_parser.add_argument(\n    \"-v\",\n    \"--verbose\",\n    help=\"Show verbose information.\",\n    action=\"count\",\n    default=0,\n)\nrun_parser.add_argument(\"--base-path\", help=\"Override the basePath in the API spec.\")\nrun_parser.add_argument(\n    \"--app-framework\",\n    \"-f\",\n    choices=list(AVAILABLE_APPS),\n    default=ASYNC_APP,\n    help=\"The app framework used to run the server\",\n)\n\n\ndef create_app(args: t.Optional[argparse.Namespace] = None) -> AbstractApp:\n    \"\"\"Runs a server compliant with a OpenAPI/Swagger Specification file.\"\"\"\n    if args is None:\n        args = parser.parse_args()\n\n    if args.verbose == 1:\n        logging_level = logging.INFO\n    elif args.verbose >= 2:\n        logging_level = logging.DEBUG\n    else:\n        logging_level = logging.WARN\n\n    logging.basicConfig(level=logging_level)\n\n    if args.spec_file.startswith(\"http\") or args.spec_file.startswith(\"https\"):\n        spec_file_full_path = args.spec_file\n    else:\n        spec_file_full_path = os.path.abspath(args.spec_file)\n        py_module_path = args.base_module_path or os.path.dirname(spec_file_full_path)\n        sys.path.insert(1, os.path.abspath(py_module_path))\n        logger.debug(f\"Added {py_module_path} to system path.\")\n\n    resolver_error = None\n    if args.stub:\n        resolver_error = 501\n\n    api_extra_args = {}\n    if args.mock:\n        resolver = MockResolver(mock_all=args.mock == \"all\")\n        api_extra_args[\"resolver\"] = resolver\n\n    app_cls = connexion.utils.get_function_from_name(AVAILABLE_APPS[args.app_framework])\n\n    swagger_ui_options = SwaggerUIOptions(\n        swagger_ui_path=args.swagger_ui_path,\n        swagger_ui_template_dir=args.swagger_ui_template_dir,\n    )\n\n    app = app_cls(\n        __name__,\n        auth_all_paths=args.auth_all_paths,\n        swagger_ui_options=swagger_ui_options,\n    )\n\n    app.add_api(\n        spec_file_full_path,\n        base_path=args.base_path,\n        resolver_error=resolver_error,\n        validate_responses=args.validate_responses,\n        strict_validation=args.strict_validation,\n        **api_extra_args,\n    )\n\n    return app\n\n\ndef main(argv: t.Optional[t.List[str]] = None) -> None:\n    if argv is None:\n        argv = sys.argv[1:]\n    if not argv:\n        argv = [\"--help\"]\n\n    args = parser.parse_args(argv)\n    app = create_app(args)\n    args.func(app, args)\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "connexion/cli.py", "language": "python", "complexity": "complex", "star_count": 4553, "quality_score": 0.78, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 171, "word_count": 402}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from contextvars import ContextVar\n\nfrom starlette.types import Receive, Scope\nfrom werkzeug.local import LocalProxy\n\nfrom connexion.lifecycle import ConnexionRequest\nfrom connexion.operations import AbstractOperation\n\nUNBOUND_MESSAGE = (\n    \"Working outside of operation context. Make sure your app is wrapped in a \"\n    \"ContextMiddleware and you're processing a request while accessing the context.\"\n)\n\n\n_context: ContextVar[dict] = ContextVar(\"CONTEXT\")\ncontext = LocalProxy(_context, unbound_message=UNBOUND_MESSAGE)\n\n_operation: ContextVar[AbstractOperation] = ContextVar(\"OPERATION\")\noperation = LocalProxy(_operation, unbound_message=UNBOUND_MESSAGE)\n\n_receive: ContextVar[Receive] = ContextVar(\"RECEIVE\")\nreceive = LocalProxy(_receive, unbound_message=UNBOUND_MESSAGE)\n\n_scope: ContextVar[Scope] = ContextVar(\"SCOPE\")\nscope = LocalProxy(_scope, unbound_message=UNBOUND_MESSAGE)\n\nrequest = LocalProxy(\n    lambda: ConnexionRequest(scope, receive), unbound_message=UNBOUND_MESSAGE\n)\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "connexion/context.py", "language": "python", "complexity": "simple", "star_count": 4553, "quality_score": 0.6599999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 30, "word_count": 89}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fnmatch import fnmatch\n\n# special marker object to return empty content for any status code\n# e.g. in app method do \"return NoContent, 201\"\nNoContent = object()\n\n\nclass MediaTypeDict(dict):\n    \"\"\"\n    A dictionary where keys can be either media types or media type ranges. When fetching a\n    value from the dictionary, the provided key is checked against the ranges. The most specific\n    key is chosen as prescribed by the OpenAPI spec, with `type/*` being preferred above\n    `*/subtype`.\n    \"\"\"\n\n    def __getitem__(self, item):\n        # Sort keys in order of specificity\n        for key in sorted(self, key=lambda k: (\"*\" not in k, k), reverse=True):\n            if fnmatch(item, key):\n                return super().__getitem__(key)\n        raise super().__getitem__(item)\n\n    def get(self, item, default=None):\n        try:\n            return self[item]\n        except KeyError:\n            return default\n\n    def __contains__(self, item):\n        try:\n            self[item]\n        except KeyError:\n            return False\n        else:\n            return True\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "connexion/datastructures.py", "language": "python", "complexity": "simple", "star_count": 4553, "quality_score": 0.78, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 36, "word_count": 130}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import abc\nimport asyncio\nimport functools\nimport json\nimport typing as t\n\nfrom asgiref.sync import async_to_sync\nfrom starlette.concurrency import run_in_threadpool\n\nfrom connexion.context import operation, receive, scope\nfrom connexion.decorators.parameter import (\n    AsyncParameterDecorator,\n    BaseParameterDecorator,\n    SyncParameterDecorator,\n)\nfrom connexion.decorators.response import (\n    AsyncResponseDecorator,\n    BaseResponseDecorator,\n    NoResponseDecorator,\n    SyncResponseDecorator,\n)\nfrom connexion.frameworks.abstract import Framework\nfrom connexion.frameworks.starlette import Starlette as StarletteFramework\nfrom connexion.uri_parsing import AbstractURIParser\nfrom connexion.utils import not_installed_error\n\ntry:\n    from connexion.frameworks.flask import Flask as FlaskFramework\nexcept ImportError as e:\n    _flask_not_installed_error = not_installed_error(\n        e, msg=\"Please install connexion using the 'flask' extra\"\n    )\n    FlaskFramework = _flask_not_installed_error  # type: ignore\n\n\nclass BaseDecorator:\n    \"\"\"Base class for connexion decorators.\"\"\"\n\n    framework: t.Type[Framework]\n\n    def __init__(\n        self,\n        *,\n        pythonic_params: bool = False,\n        uri_parser_class: AbstractURIParser = None,\n        jsonifier=json,\n    ) -> None:\n        self.pythonic_params = pythonic_params\n        self.uri_parser_class = uri_parser_class\n        self.jsonifier = jsonifier\n\n        self.arguments, self.has_kwargs = None, None\n\n    @property\n    @abc.abstractmethod\n    def _parameter_decorator_cls(self) -> t.Type[BaseParameterDecorator]:\n        raise NotImplementedError\n\n    @property\n    @abc.abstractmethod\n    def _response_decorator_cls(self) -> t.Type[BaseResponseDecorator]:\n        raise NotImplementedError\n\n    @property\n    @abc.abstractmethod\n    def _sync_async_decorator(self) -> t.Callable[[t.Callable], t.Callable]:\n        \"\"\"Decorator to translate between sync and async functions.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def uri_parser(self):\n        uri_parser_class = self.uri_parser_class or operation.uri_parser_class\n        return uri_parser_class(operation.parameters, operation.body_definition())\n\n    def decorate(self, function: t.Callable) -> t.Callable:\n        \"\"\"Decorate a function with decorators based on the operation.\"\"\"\n        function = self._sync_async_decorator(function)\n\n        parameter_decorator = self._parameter_decorator_cls(\n            framework=self.framework,\n            pythonic_params=self.pythonic_params,\n        )\n        function = parameter_decorator(function)\n\n        response_decorator = self._response_decorator_cls(\n            framework=self.framework,\n            jsonifier=self.jsonifier,\n        )\n        function = response_decorator(function)\n\n        return function\n\n    @abc.abstractmethod\n    def __call__(self, function: t.Callable) -> t.Callable:\n        raise NotImplementedError\n\n\nclass WSGIDecorator(BaseDecorator):\n    \"\"\"Decorator for usage with WSGI apps. The parameter decorator works with a Flask request,\n    and provides Flask datastructures to the view function. This works for any WSGI app, since\n    we get the request via the connexion context provided by WSGI middleware.\n\n    This decorator does not parse responses, but passes them directly to the WSGI App.\"\"\"\n\n    framework = FlaskFramework\n\n    @property\n    def _parameter_decorator_cls(self) -> t.Type[SyncParameterDecorator]:\n        return SyncParameterDecorator\n\n    @property\n    def _response_decorator_cls(self) -> t.Type[BaseResponseDecorator]:\n        return NoResponseDecorator\n\n    @property\n    def _sync_async_decorator(self) -> t.Callable[[t.Callable], t.Callable]:\n        def decorator(function: t.Callable) -> t.Callable:\n            @functools.wraps(function)\n            def wrapper(*args, **kwargs) -> t.Callable:\n                if asyncio.iscoroutinefunction(function):\n                    return async_to_sync(function)(*args, **kwargs)\n                else:\n                    return function(*args, **kwargs)\n\n            return wrapper\n\n        return decorator\n\n    def __call__(self, function: t.Callable) -> t.Callable:\n        @functools.wraps(function)\n        def wrapper(*args, **kwargs):\n            request = self.framework.get_request(uri_parser=self.uri_parser)\n            decorated_function = self.decorate(function)\n            return decorated_function(request)\n\n        return wrapper\n\n\nclass FlaskDecorator(WSGIDecorator):\n    \"\"\"Decorator for usage with Connexion or Flask apps. The parameter decorator works with a\n    Flask request, and provides Flask datastructures to the view function.\n\n    The response decorator returns Flask responses.\"\"\"\n\n    @property\n    def _response_decorator_cls(self) -> t.Type[SyncResponseDecorator]:\n        return SyncResponseDecorator\n\n\nclass ASGIDecorator(BaseDecorator):\n    \"\"\"Decorator for usage with ASGI apps. The parameter decorator works with a Starlette request,\n    and provides Starlette datastructures to the view function. This works for any ASGI app, since\n    we get the request via the connexion context provided by ASGI middleware.\n\n    This decorator does not parse responses, but passes them directly to the ASGI App.\"\"\"\n\n    framework = StarletteFramework\n\n    @property\n    def _parameter_decorator_cls(self) -> t.Type[AsyncParameterDecorator]:\n        return AsyncParameterDecorator\n\n    @property\n    def _response_decorator_cls(self) -> t.Type[BaseResponseDecorator]:\n        return NoResponseDecorator\n\n    @property\n    def _sync_async_decorator(self) -> t.Callable[[t.Callable], t.Callable]:\n        def decorator(function: t.Callable) -> t.Callable:\n            @functools.wraps(function)\n            async def wrapper(*args, **kwargs):\n                if asyncio.iscoroutinefunction(function):\n                    return await function(*args, **kwargs)\n                else:\n                    return await run_in_threadpool(function, *args, **kwargs)\n\n            return wrapper\n\n        return decorator\n\n    def __call__(self, function: t.Callable) -> t.Callable:\n        @functools.wraps(function)\n        async def wrapper(*args, **kwargs):\n            request = self.framework.get_request(\n                uri_parser=self.uri_parser, scope=scope, receive=receive\n            )\n            decorated_function = self.decorate(function)\n            return await decorated_function(request)\n\n        return wrapper\n\n\nclass StarletteDecorator(ASGIDecorator):\n    \"\"\"Decorator for usage with Connexion or Starlette apps. The parameter decorator works with a\n    Starlette request, and provides Starlette datastructures to the view function.\n\n    The response decorator returns Starlette responses.\"\"\"\n\n    @property\n    def _response_decorator_cls(self) -> t.Type[AsyncResponseDecorator]:\n        return AsyncResponseDecorator\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "connexion/decorators/main.py", "language": "python", "complexity": "complex", "star_count": 4553, "quality_score": 0.78, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 202, "word_count": 554}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis module defines a decorator to convert request parameters to arguments for the view function.\n\"\"\"\nimport abc\nimport asyncio\nimport builtins\nimport functools\nimport keyword\nimport logging\nimport re\nimport typing as t\nfrom copy import copy, deepcopy\n\nimport inflection\n\nfrom connexion.context import context, operation\nfrom connexion.frameworks.abstract import Framework\nfrom connexion.http_facts import FORM_CONTENT_TYPES\nfrom connexion.lifecycle import ConnexionRequest, WSGIRequest\nfrom connexion.operations import AbstractOperation, Swagger2Operation\nfrom connexion.utils import (\n    deep_merge,\n    inspect_function_arguments,\n    is_null,\n    is_nullable,\n    make_type,\n)\n\nlogger = logging.getLogger(__name__)\n\nCONTEXT_NAME = \"context_\"\n\n\nclass BaseParameterDecorator:\n    def __init__(\n        self,\n        *,\n        framework: t.Type[Framework],\n        pythonic_params: bool = False,\n    ) -> None:\n        self.framework = framework\n        self.sanitize_fn = pythonic if pythonic_params else sanitized\n\n    def _maybe_get_body(\n        self,\n        request: t.Union[WSGIRequest, ConnexionRequest],\n        *,\n        arguments: t.List[str],\n        has_kwargs: bool,\n    ) -> t.Any:\n        body_name = self.sanitize_fn(operation.body_name(request.content_type))\n        # Pass form contents separately for Swagger2 for backward compatibility with\n        # Connexion 2 Checking for body_name is not enough\n        if (body_name in arguments or has_kwargs) or (\n            request.mimetype in FORM_CONTENT_TYPES\n            and isinstance(operation, Swagger2Operation)\n        ):\n            return request.get_body()\n        else:\n            return None\n\n    @abc.abstractmethod\n    def __call__(self, function: t.Callable) -> t.Callable:\n        raise NotImplementedError\n\n\nclass SyncParameterDecorator(BaseParameterDecorator):\n    def __call__(self, function: t.Callable) -> t.Callable:\n        unwrapped_function = unwrap_decorators(function)\n        arguments, has_kwargs = inspect_function_arguments(unwrapped_function)\n\n        @functools.wraps(function)\n        def wrapper(request: WSGIRequest) -> t.Any:\n            request_body = self._maybe_get_body(\n                request, arguments=arguments, has_kwargs=has_kwargs\n            )\n\n            kwargs = prep_kwargs(\n                request,\n                request_body=request_body,\n                files=request.files(),\n                arguments=arguments,\n                has_kwargs=has_kwargs,\n                sanitize=self.sanitize_fn,\n            )\n\n            return function(**kwargs)\n\n        return wrapper\n\n\nclass AsyncParameterDecorator(BaseParameterDecorator):\n    def __call__(self, function: t.Callable) -> t.Callable:\n        unwrapped_function = unwrap_decorators(function)\n        arguments, has_kwargs = inspect_function_arguments(unwrapped_function)\n\n        @functools.wraps(function)\n        async def wrapper(request: ConnexionRequest) -> t.Any:\n            request_body = self._maybe_get_body(\n                request, arguments=arguments, has_kwargs=has_kwargs\n            )\n\n            while asyncio.iscoroutine(request_body):\n                request_body = await request_body\n\n            kwargs = prep_kwargs(\n                request,\n                request_body=request_body,\n                files=await request.files(),\n                arguments=arguments,\n                has_kwargs=has_kwargs,\n                sanitize=self.sanitize_fn,\n            )\n\n            return await function(**kwargs)\n\n        return wrapper\n\n\ndef prep_kwargs(\n    request: t.Union[WSGIRequest, ConnexionRequest],\n    *,\n    request_body: t.Any,\n    files: t.Dict[str, t.Any],\n    arguments: t.List[str],\n    has_kwargs: bool,\n    sanitize: t.Callable,\n) -> dict:\n    kwargs = get_arguments(\n        operation,\n        path_params=request.path_params,\n        query_params=request.query_params,\n        body=request_body,\n        files=files,\n        arguments=arguments,\n        has_kwargs=has_kwargs,\n        sanitize=sanitize,\n        content_type=request.mimetype,\n    )\n\n    # optionally convert parameter variable names to un-shadowed, snake_case form\n    kwargs = {sanitize(k): v for k, v in kwargs.items()}\n\n    # add context info (e.g. from security decorator)\n    for key, value in context.items():\n        if has_kwargs or key in arguments:\n            kwargs[key] = value\n        else:\n            logger.debug(\"Context parameter '%s' not in function arguments\", key)\n    # attempt to provide the request context to the function\n    if CONTEXT_NAME in arguments:\n        kwargs[CONTEXT_NAME] = context\n\n    return kwargs\n\n\ndef unwrap_decorators(function: t.Callable) -> t.Callable:\n    \"\"\"Unwrap decorators to return the original function.\"\"\"\n    while hasattr(function, \"__wrapped__\"):\n        function = function.__wrapped__  # type: ignore\n    return function\n\n\ndef snake_and_shadow(name: str) -> str:\n    \"\"\"\n    Converts the given name into Pythonic form. Firstly it converts CamelCase names to snake_case. Secondly it looks to\n    see if the name matches a known built-in and if it does it appends an underscore to the name.\n    :param name: The parameter name\n    \"\"\"\n    snake = inflection.underscore(name)\n    if snake in builtins.__dict__ or keyword.iskeyword(snake):\n        return f\"{snake}_\"\n    return snake\n\n\ndef sanitized(name: str) -> str:\n    return name and re.sub(\n        \"^[^a-zA-Z_]+\", \"\", re.sub(\"[^0-9a-zA-Z_]\", \"\", re.sub(r\"\\[(?!])\", \"_\", name))\n    )\n\n\ndef pythonic(name: str) -> str:\n    name = name and snake_and_shadow(name)\n    return sanitized(name)\n\n\ndef get_arguments(\n    operation: AbstractOperation,\n    *,\n    path_params: dict,\n    query_params: dict,\n    body: t.Any,\n    files: dict,\n    arguments: t.List[str],\n    has_kwargs: bool,\n    sanitize: t.Callable,\n    content_type: str,\n) -> t.Dict[str, t.Any]:\n    \"\"\"\n    get arguments for handler function\n    \"\"\"\n    ret = {}\n    ret.update(_get_path_arguments(path_params, operation=operation, sanitize=sanitize))\n    ret.update(\n        _get_query_arguments(\n            query_params,\n            operation=operation,\n            arguments=arguments,\n            has_kwargs=has_kwargs,\n            sanitize=sanitize,\n        )\n    )\n\n    if operation.method.upper() == \"TRACE\":\n        # TRACE requests MUST NOT include a body (RFC7231 section 4.3.8)\n        return ret\n\n    ret.update(\n        _get_body_argument(\n            body,\n            operation=operation,\n            arguments=arguments,\n            has_kwargs=has_kwargs,\n            sanitize=sanitize,\n            content_type=content_type,\n        )\n    )\n    body_schema = operation.body_schema(content_type)\n    ret.update(_get_file_arguments(files, arguments, body_schema, has_kwargs))\n    return ret\n\n\ndef _get_path_arguments(\n    path_params: dict, *, operation: AbstractOperation, sanitize: t.Callable\n) -> dict:\n    \"\"\"\n    Extract handler function arguments from path parameters\n    \"\"\"\n    kwargs = {}\n\n    path_definitions = {\n        parameter[\"name\"]: parameter\n        for parameter in operation.parameters\n        if parameter[\"in\"] == \"path\"\n    }\n\n    for name, value in path_params.items():\n        sanitized_key = sanitize(name)\n        if name in path_definitions:\n            kwargs[sanitized_key] = _get_val_from_param(value, path_definitions[name])\n        else:  # Assume path params mechanism used for injection\n            kwargs[sanitized_key] = value\n    return kwargs\n\n\ndef _get_val_from_param(value: t.Any, param_definitions: t.Dict[str, dict]) -> t.Any:\n    \"\"\"Cast a value according to its definition in the specification.\"\"\"\n    param_schema = param_definitions.get(\"schema\", param_definitions)\n\n    if is_nullable(param_schema) and is_null(value):\n        return None\n\n    if param_schema[\"type\"] == \"array\":\n        type_ = param_schema[\"items\"][\"type\"]\n        format_ = param_schema[\"items\"].get(\"format\")\n        return [make_type(part, type_, format_) for part in value]\n    else:\n        type_ = param_schema[\"type\"]\n        format_ = param_schema.get(\"format\")\n        return make_type(value, type_, format_)\n\n\ndef _get_query_arguments(\n    query_params: dict,\n    *,\n    operation: AbstractOperation,\n    arguments: t.List[str],\n    has_kwargs: bool,\n    sanitize: t.Callable,\n) -> dict:\n    \"\"\"\n    extract handler function arguments from the query parameters\n    \"\"\"\n    query_definitions = {\n        parameter[\"name\"]: parameter\n        for parameter in operation.parameters\n        if parameter[\"in\"] == \"query\"\n    }\n\n    default_query_params = _get_query_defaults(query_definitions)\n\n    query_arguments = deepcopy(default_query_params)\n    query_arguments = deep_merge(query_arguments, query_params)\n    return _query_args_helper(\n        query_definitions, query_arguments, arguments, has_kwargs, sanitize\n    )\n\n\ndef _get_query_defaults(query_definitions: t.Dict[str, dict]) -> t.Dict[str, t.Any]:\n    \"\"\"Get the default values for the query parameter from the parameter definition.\"\"\"\n    defaults = {}\n    for k, v in query_definitions.items():\n        try:\n            if \"default\" in v:\n                defaults[k] = v[\"default\"]\n            elif v[\"schema\"][\"type\"] == \"object\":\n                defaults[k] = _get_default_obj(v[\"schema\"])\n            else:\n                defaults[k] = v[\"schema\"][\"default\"]\n        except KeyError:\n            pass\n    return defaults\n\n\ndef _get_default_obj(schema: dict) -> dict:\n    try:\n        return deepcopy(schema[\"default\"])\n    except KeyError:\n        properties = schema.get(\"properties\", {})\n        return _build_default_obj_recursive(properties, {})\n\n\ndef _build_default_obj_recursive(properties: dict, default_object: dict) -> dict:\n    \"\"\"takes disparate and nested default keys, and builds up a default object\"\"\"\n    for name, property_ in properties.items():\n        if \"default\" in property_ and name not in default_object:\n            default_object[name] = copy(property_[\"default\"])\n        elif property_.get(\"type\") == \"object\" and \"properties\" in property_:\n            default_object.setdefault(name, {})\n            default_object[name] = _build_default_obj_recursive(\n                property_[\"properties\"], default_object[name]\n            )\n    return default_object\n\n\ndef _query_args_helper(\n    query_definitions: dict,\n    query_arguments: dict,\n    function_arguments: t.List[str],\n    has_kwargs: bool,\n    sanitize: t.Callable,\n) -> dict:\n    result = {}\n    for key, value in query_arguments.items():\n        sanitized_key = sanitize(key)\n        if not has_kwargs and sanitized_key not in function_arguments:\n            logger.debug(\n                \"Query Parameter '%s' (sanitized: '%s') not in function arguments\",\n                key,\n                sanitized_key,\n            )\n        else:\n            logger.debug(\n                \"Query Parameter '%s' (sanitized: '%s') in function arguments\",\n                key,\n                sanitized_key,\n            )\n            try:\n                query_defn = query_definitions[key]\n            except KeyError:  # pragma: no cover\n                logger.error(\n                    \"Function argument '%s' (non-sanitized: %s) not defined in specification\",\n                    sanitized_key,\n                    key,\n                )\n            else:\n                logger.debug(\"%s is a %s\", key, query_defn)\n                result.update({sanitized_key: _get_val_from_param(value, query_defn)})\n    return result\n\n\ndef _get_body_argument(\n    body: t.Any,\n    *,\n    operation: AbstractOperation,\n    arguments: t.List[str],\n    has_kwargs: bool,\n    sanitize: t.Callable,\n    content_type: str,\n) -> dict:\n    if len(arguments) <= 0 and not has_kwargs:\n        return {}\n\n    if not operation.is_request_body_defined:\n        return {}\n\n    body_name = sanitize(operation.body_name(content_type))\n\n    if content_type in FORM_CONTENT_TYPES:\n        result = _get_body_argument_form(\n            body, operation=operation, content_type=content_type\n        )\n\n        # Unpack form values for Swagger for compatibility with Connexion 2 behavior\n        if content_type in FORM_CONTENT_TYPES and isinstance(\n            operation, Swagger2Operation\n        ):\n            if has_kwargs:\n                return result\n            else:\n                return {\n                    sanitize(name): value\n                    for name, value in result.items()\n                    if sanitize(name) in arguments\n                }\n    else:\n        result = _get_body_argument_json(\n            body, operation=operation, content_type=content_type\n        )\n\n    if body_name in arguments or has_kwargs:\n        return {body_name: result}\n\n    return {}\n\n\ndef _get_body_argument_json(\n    body: t.Any, *, operation: AbstractOperation, content_type: str\n) -> t.Any:\n    # if the body came in null, and the schema says it can be null, we decide\n    # to include no value for the body argument, rather than the default body\n    if is_nullable(operation.body_schema(content_type)) and is_null(body):\n        return None\n\n    if body is None:\n        default_body = operation.body_schema(content_type).get(\"default\", {})\n        return deepcopy(default_body)\n\n    return body\n\n\ndef _get_body_argument_form(\n    body: dict, *, operation: AbstractOperation, content_type: str\n) -> dict:\n    # now determine the actual value for the body (whether it came in or is default)\n    default_body = operation.body_schema(content_type).get(\"default\", {})\n    body_props = {\n        k: {\"schema\": v}\n        for k, v in operation.body_schema(content_type).get(\"properties\", {}).items()\n    }\n\n    # by OpenAPI specification `additionalProperties` defaults to `true`\n    # see: https://github.com/OAI/OpenAPI-Specification/blame/3.0.2/versions/3.0.2.md#L2305\n    additional_props = operation.body_schema().get(\"additionalProperties\", True)\n\n    body_arg = deepcopy(default_body)\n    body_arg.update(body or {})\n\n    if body_props or additional_props:\n        return _get_typed_body_values(body_arg, body_props, additional_props)\n\n    return {}\n\n\ndef _get_typed_body_values(body_arg, body_props, additional_props):\n    \"\"\"\n    Return a copy of the provided body_arg dictionary\n    whose values will have the appropriate types\n    as defined in the provided schemas.\n\n    :type body_arg: type dict\n    :type body_props: dict\n    :type additional_props: dict|bool\n    :rtype: dict\n    \"\"\"\n    additional_props_defn = (\n        {\"schema\": additional_props} if isinstance(additional_props, dict) else None\n    )\n    res = {}\n\n    for key, value in body_arg.items():\n        try:\n            prop_defn = body_props[key]\n            res[key] = _get_val_from_param(value, prop_defn)\n        except KeyError:  # pragma: no cover\n            if not additional_props:\n                logger.error(f\"Body property '{key}' not defined in body schema\")\n                continue\n            if additional_props_defn is not None:\n                value = _get_val_from_param(value, additional_props_defn)\n            res[key] = value\n\n    return res\n\n\ndef _get_file_arguments(files, arguments, body_schema: dict, has_kwargs=False):\n    results = {}\n    for k, v in files.items():\n        if not (k in arguments or has_kwargs):\n            continue\n        if body_schema.get(\"properties\", {}).get(k, {}).get(\"type\") != \"array\":\n            v = v[0]\n        results[k] = v\n\n    return results\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "connexion/decorators/parameter.py", "language": "python", "complexity": "complex", "star_count": 4553, "quality_score": 0.84, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 496, "word_count": 1340}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import abc\nimport collections.abc\nimport functools\nimport logging\nimport types\nimport typing as t\nfrom enum import Enum\n\nfrom connexion import utils\nfrom connexion.context import operation\nfrom connexion.datastructures import NoContent\nfrom connexion.exceptions import NonConformingResponseHeaders\nfrom connexion.frameworks.abstract import Framework\nfrom connexion.lifecycle import ConnexionResponse\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseResponseDecorator:\n    def __init__(self, *, framework: t.Type[Framework], jsonifier):\n        self.framework = framework\n        self.jsonifier = jsonifier\n\n    @abc.abstractmethod\n    def __call__(self, function: t.Callable) -> t.Callable:\n        raise NotImplementedError\n\n    def build_framework_response(self, handler_response):\n        data, status_code, headers = self._unpack_handler_response(handler_response)\n        content_type = self._infer_content_type(data, headers)\n        if not self.framework.is_framework_response(data):\n            data = self._serialize_data(data, content_type=content_type)\n            status_code = status_code or self._infer_status_code(data)\n            headers = self._update_headers(headers, content_type=content_type)\n        return self.framework.build_response(\n            data, content_type=content_type, status_code=status_code, headers=headers\n        )\n\n    @staticmethod\n    def _infer_content_type(data: t.Any, headers: dict) -> t.Optional[str]:\n        \"\"\"Infer the response content type from the returned data, headers and operation spec.\n\n        :param data: Response data\n        :param headers: Headers returned by the handler.\n\n        :return: Inferred content type\n\n        :raises: NonConformingResponseHeaders if content type cannot be deducted.\n        \"\"\"\n        content_type = utils.extract_content_type(headers)\n\n        # TODO: don't default\n        produces = list(set(operation.produces))\n        if data is not None and not produces:\n            produces = [\"application/json\"]\n\n        if content_type:\n            if content_type not in produces:\n                raise NonConformingResponseHeaders(\n                    f\"Returned content type ({content_type}) is not defined in operation spec \"\n                    f\"({operation.produces}).\"\n                )\n        else:\n            if not produces:\n                # Produces can be empty/ for empty responses\n                pass\n            elif len(produces) == 1:\n                content_type = produces[0]\n            else:\n                if isinstance(data, str):\n                    for produced_content_type in produces:\n                        if \"text/plain\" in produced_content_type:\n                            content_type = produced_content_type\n                elif isinstance(data, bytes) or isinstance(\n                    data, (types.GeneratorType, collections.abc.Iterator)\n                ):\n                    for produced_content_type in produces:\n                        if \"application/octet-stream\" in produced_content_type:\n                            content_type = produced_content_type\n\n                if content_type is None:\n                    raise NonConformingResponseHeaders(\n                        \"Multiple response content types are defined in the operation spec, but \"\n                        \"the handler response did not specify which one to return.\"\n                    )\n\n        return content_type\n\n    def _serialize_data(self, data: t.Any, *, content_type: str) -> t.Any:\n        \"\"\"Serialize the data based on the content type.\"\"\"\n        if data is None or data is NoContent:\n            return None\n        # TODO: encode responses\n        mime_type, _ = utils.split_content_type(content_type)\n        if utils.is_json_mimetype(mime_type):\n            return self.jsonifier.dumps(data)\n        return data\n\n    @staticmethod\n    def _infer_status_code(data: t.Any) -> int:\n        \"\"\"Infer the status code from the returned data.\"\"\"\n        if data is None:\n            return 204\n        return 200\n\n    @staticmethod\n    def _update_headers(\n        headers: t.Dict[str, str], *, content_type: str\n    ) -> t.Dict[str, str]:\n        # Check if Content-Type is in headers, taking into account case-insensitivity\n        for key, value in headers.items():\n            if key.lower() == \"content-type\":\n                return headers\n\n        if content_type:\n            headers[\"Content-Type\"] = content_type\n        return headers\n\n    @staticmethod\n    def _unpack_handler_response(\n        handler_response: t.Union[str, bytes, dict, list, tuple]\n    ) -> t.Tuple[t.Union[str, bytes, dict, list, None], t.Optional[int], dict]:\n        \"\"\"Unpack the handler response into data, status_code and headers.\n\n        :param handler_response: The response returned from the handler function if it was not a\n            response class.\n\n        :return: A tuple of data, status_code and headers\n        \"\"\"\n        data, status_code, headers = None, None, {}\n\n        if not isinstance(handler_response, tuple):\n            data = handler_response\n\n        elif len(handler_response) == 1:\n            (data,) = handler_response\n\n        elif len(handler_response) == 2:\n            data, status_code_or_headers = handler_response\n            if isinstance(status_code_or_headers, int):\n                # Extra int call because of int subclasses such as http.HTTPStatus (IntEnum)\n                status_code = int(status_code_or_headers)\n            elif isinstance(status_code_or_headers, Enum) and isinstance(\n                status_code_or_headers.value, int\n            ):\n                status_code = status_code_or_headers.value\n            else:\n                headers = status_code_or_headers\n\n        elif len(handler_response) == 3:\n            data, status_code, headers = handler_response\n\n        else:\n            raise TypeError(\n                \"The view function did not return a valid response tuple.\"\n                \" The tuple must have the form (body), (body, status, headers),\"\n                \" (body, status), or (body, headers).\"\n            )\n\n        return data, status_code, headers\n\n\nclass SyncResponseDecorator(BaseResponseDecorator):\n    def __call__(self, function: t.Callable) -> t.Callable:\n        @functools.wraps(function)\n        def wrapper(*args, **kwargs):\n            \"\"\"\n            This method converts a handler response to a framework response.\n            The handler response can be a ConnexionResponse, a framework response, a tuple or an\n            object.\n            \"\"\"\n            handler_response = function(*args, **kwargs)\n            if self.framework.is_framework_response(handler_response):\n                return handler_response\n            elif isinstance(handler_response, ConnexionResponse):\n                return self.framework.connexion_to_framework_response(handler_response)\n            else:\n                return self.build_framework_response(handler_response)\n\n        return wrapper\n\n\nclass AsyncResponseDecorator(BaseResponseDecorator):\n    def __call__(self, function: t.Callable) -> t.Callable:\n        @functools.wraps(function)\n        async def wrapper(*args, **kwargs):\n            \"\"\"\n            This method converts a handler response to a framework response.\n            The handler response can be a ConnexionResponse, a framework response, a tuple or an\n            object.\n            \"\"\"\n            handler_response = await function(*args, **kwargs)\n            if self.framework.is_framework_response(handler_response):\n                return handler_response\n            elif isinstance(handler_response, ConnexionResponse):\n                return self.framework.connexion_to_framework_response(handler_response)\n            else:\n                return self.build_framework_response(handler_response)\n\n        return wrapper\n\n\nclass NoResponseDecorator(BaseResponseDecorator):\n    \"\"\"Dummy decorator to skip response serialization.\"\"\"\n\n    def __call__(self, function: t.Callable) -> t.Callable:\n        return lambda request: function(request)\n", "metadata": {"source": "github.com/spec-first/connexion", "file_path": "connexion/decorators/response.py", "language": "python", "complexity": "complex", "star_count": 4553, "quality_score": 0.8099999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 208, "word_count": 674}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CI\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: 3.12\n\n      - name: Install dependencies\n        run: |\n          curl -sSL https://install.python-poetry.org | python3 -\n          poetry install\n\n      - name: Run tests\n        run: poetry run pytest\n", "metadata": {"source": "github.com/Integuru-AI/Integuru", "file_path": ".github/workflows/ci.yml", "language": "yaml", "complexity": "simple", "star_count": 4406, "quality_score": 0.6, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": false, "line_count": 31, "word_count": 54}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Integuru\n\nAn AI agent that generates integration code by reverse-engineering platforms' internal APIs.\n\n## Integuru in Action\n\n![Integuru in action](./integuru_demo.gif)\n\n## What Integuru Does\n\nYou use ```create_har.py``` to generate a file containing all browser network requests, a file with the cookies, and write a prompt describing the action triggered in the browser. The agent outputs runnable Python code that hits the platform's internal endpoints to perform the desired action.\n\n## How It Works\n\nLet's assume we want to download utility bills:\n\n1. The agent identifies the request that downloads the utility bills.\n   For example, the request URL might look like this:\n   ```\n   https://www.example.com/utility-bills?accountId=123&userId=456\n   ```\n2. It identifies parts of the request that depend on other requests.\n   The above URL contains dynamic parts (accountId and userId) that need to be obtained from other requests.\n   ```\n   accountId=123 userId=456\n   ```\n3. It finds the requests that provide these parts and makes the download request dependent on them. It also attaches these requests to the original request to build out a dependency graph.\n   ```\n   GET https://www.example.com/get_account_id\n   GET https://www.example.com/get_user_id\n   ```\n4. This process repeats until the request being checked depends on no other request and only requires the authentication cookies.\n5. The agent traverses up the graph, starting from nodes (requests) with no outgoing edges until it reaches the master node while converting each node to a runnable function.\n\n## Features\n\n- Generate a dependency graph of requests to make the final request that performs the desired action.\n- Allow input variables (for example, choosing the YEAR to download a document from). This is currently only supported for graph generation. Input variables for code generation coming soon!\n- Generate code to hit all requests in the graph to perform the desired action.\n\n## Setup\n\n1. Set up your OpenAI [API Keys](https://platform.openai.com/account/api-keys) and add the `OPENAI_API_KEY` environment variable. (We recommend using an account with access to models that are at least as capable as OpenAI o1-mini. Models on par with OpenAI o1-preview are ideal.)\n2. Install Python requirements via poetry:\n   ```\n   poetry install\n   ```\n3. Open a poetry shell:\n   ```\n   poetry shell\n   ```\n4. Register the Poetry virtual environment with Jupyter:\n   ```\n   poetry run ipython kernel install --user --name=integuru\n   ```\n5. Run the following command to spawn a browser:\n   ```\n   poetry run python create_har.py\n   ```\n   Log into your platform and perform the desired action (such as downloading a utility bill).\n6. Run Integuru:\n   ```\n   poetry run integuru --prompt \"download utility bills\" --model <gpt-4o|o3-mini|o1|o1-mini>\n   ```\n   You can also run it via Jupyter Notebook `main.ipynb`\n\n   **Recommended to use gpt-4o as the model for graph generation as it supports function calling. Integuru will automatically switch to o1-preview for code generation if available in the user's OpenAI account.** \n\n## Usage\n\nAfter setting up the project, you can use Integuru to analyze and reverse-engineer API requests for external platforms. Simply provide the appropriate .har file and a prompt describing the action that you want to trigger.\n\n```\npoetry run integuru --help\nUsage: integuru [OPTIONS]\n\nOptions:\n  --model TEXT                    The LLM model to use (default is gpt-4o)\n  --prompt TEXT                   The prompt for the model  [required]\n  --har-path TEXT                 The HAR file path (default is\n                                  ./network_requests.har)\n  --cookie-path TEXT              The cookie file path (default is\n                                  ./cookies.json)\n  --max_steps INTEGER             The max_steps (default is 20)\n  --input_variables <TEXT TEXT>...\n                                  Input variables in the format key value\n  --generate-code                 Whether to generate the full integration\n                                  code\n  --help                          Show this message and exit.\n```\n\n\n## Running Unit Tests\n\nTo run unit tests using `pytest`, use the following command:\n\n```\npoetry run pytest\n```\n\n## Continuous Integration (CI) Workflow\n\nThis repository includes a CI workflow using GitHub Actions. The workflow is defined in the `.github/workflows/ci.yml` file and is triggered on each push and pull request to the `main` branch. The workflow performs the following steps:\n\n1. Checks out the code.\n2. Sets up Python 3.12.\n3. Installs dependencies using `poetry`.\n4. Runs tests using `pytest`.\n\n## Note on 2FA\n\nWhen the destination site uses two-factor authentication (2FA), the workflow remains the same. Ensure that you complete the 2FA process and obtain the cookies/auth tokens/session tokens after 2FA. These tokens will be used in the workflow.\n\n\n## Demo\n\n[![Demo Video](http://markdown-videos-api.jorgenkh.no/youtube/7OJ4w5BCpQ0)](https://www.youtube.com/watch?v=7OJ4w5BCpQ0)\n\n## Contributing\n\nContributions to improve Integuru are welcome. Please feel free to submit issues or pull requests on the project's repository.\n\n## Info\n\nInteguru is built by Integuru.ai. Besides our work on the agent, we take custom requests for new integrations or additional features for existing supported platforms. We also offer hosting and authentication services. If you have requests or want to work with us, reach out at richard@integuru.ai.\n\nWe open-source unofficial APIs that we've built already. You can find them [here](https://github.com/Integuru-AI/APIs-by-Integuru).\n\n## Privacy Policy\n\n### Data Storage\nCollected data is stored locally in the `network_requests.har` and `cookies.json` files.\n\n### LLM Usage\nThe tool uses a cloud-based LLM (OpenAI's GPT-4o and o1-preview models).\n\n### LLM Training\nThe LLM is not trained or improved by the usage of this tool.\n", "metadata": {"source": "github.com/Integuru-AI/Integuru", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 4406, "quality_score": 0.6599999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 139, "word_count": 816}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nimport json\nfrom playwright.async_api import async_playwright\n\n\nasync def open_browser_and_wait():\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=False)\n\n        context = await browser.new_context(\n            record_har_path=\"network_requests.har\",  # Path to save the HAR file\n            record_har_content=\"embed\",  # Omit content to make the HAR file smaller\n            # TODO record_har_url_filter=\"*\",  # Optional URL filter\n        )\n\n        page = await context.new_page()\n\n        print(\n            \"Browser is open. Press Enter in the terminal when you're ready to close the browser and save cookies...\"\n        )\n\n        input(\"Press Enter to continue and close the browser...\")\n\n        # Ensure 2FA is completed before saving cookies\n        cookies = await context.cookies()\n\n        with open(\"cookies.json\", \"w\") as f:\n            json.dump(cookies, f, indent=4)\n\n        await context.close()\n\n        await browser.close()\n\nasyncio.run(open_browser_and_wait())\n", "metadata": {"source": "github.com/Integuru-AI/Integuru", "file_path": "create_har.py", "language": "python", "complexity": "simple", "star_count": 4406, "quality_score": 0.69, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 35, "word_count": 107}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from dotenv import load_dotenv\nimport time  # Add this import\n\nload_dotenv()\n\nfrom integuru.main import call_agent\nimport asyncio\nimport click\n\n@click.command()\n@click.option(\n    \"--model\", default=\"gpt-4o\", help=\"The LLM model to use (default is gpt-4o)\"\n)\n@click.option(\"--prompt\", required=True, help=\"The prompt for the model\")\n@click.option(\n    \"--har-path\",\n    default=\"./network_requests.har\",\n    help=\"The HAR file path (default is ./network_requests.har)\",\n)\n@click.option(\n    \"--cookie-path\",\n    default=\"./cookies.json\",\n    help=\"The cookie file path (default is ./cookies.json)\",\n)\n@click.option(\n    \"--max_steps\", default=20, type=int, help=\"The max_steps (default is 20)\"\n)\n@click.option(\n    \"--input_variables\",\n    multiple=True,\n    type=(str, str),\n    help=\"Input variables in the format key value\",\n)\n@click.option(\n    \"--generate-code\",\n    is_flag=True,\n    default=False,\n    help=\"Whether to generate the full integration code\",\n)\ndef cli(\n    model, prompt, har_path, cookie_path, max_steps, input_variables, generate_code\n):\n    input_vars = dict(input_variables)\n    asyncio.run(\n        call_agent(\n            model,\n            prompt,\n            har_path,\n            cookie_path,\n            input_variables=input_vars,\n            max_steps=max_steps,\n            to_generate_code=generate_code,\n        )\n    )\n\nif __name__ == \"__main__\":\n    cli()\n", "metadata": {"source": "github.com/Integuru-AI/Integuru", "file_path": "integuru/__main__.py", "language": "python", "complexity": "simple", "star_count": 4406, "quality_score": 0.69, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 58, "word_count": 125}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nimport urllib\nimport os\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional, Set\n\nfrom integuru.util.LLM import llm\nfrom integuru.models.DAGManager import DAGManager\nfrom integuru.util.har_processing import *\nfrom integuru.models.request import Request\nfrom integuru.models.agent_state import AgentState\n\nclass IntegrationAgent:\n    ACTION_URL_KEY: str = \"action_url\"\n    IN_PROCESS_NODE_KEY: str = \"in_process_node\"\n    TO_BE_PROCESSED_NODES_KEY: str = \"to_be_processed_nodes\"\n    IN_PROCESS_NODE_DYNAMIC_PARTS_KEY: str = \"in_process_node_dynamic_parts\"\n    MASTER_NODE_KEY: str = \"master_node\"\n    INPUT_VARIABLES_KEY: str = \"input_variables\"\n\n    def __init__(\n        self,\n        prompt: str,\n        har_file_path: str,\n        cookie_path: str,\n    ):  \n        self.prompt: str = prompt\n        self.duplicate_part_set: Set[str] = set()\n        self.global_master_node: Optional[str] = None\n        self.req_to_res_map: Dict[Request, str] = parse_har_file(har_file_path)\n        self.url_to_res_req_dict: Dict[str, Dict[str, Any]] = build_url_to_req_res_map(self.req_to_res_map)\n        self.har_urls: List[Tuple[str, str, str, str]] = get_har_urls(har_file_path)\n        self.cookie_dict: Dict[str, Dict[str, Any]] = parse_cookie_file_to_dict(cookie_path)\n        self.curl_to_id_dict: Dict[str, str] = {}\n        self.cookie_to_id_dict: Dict[str, str] = {}\n        self.dag_manager: DAGManager = DAGManager()\n\n    def end_url_identify_agent(self, state: AgentState) -> AgentState:\n        \"\"\"\n        Identify the URL responsible for a specific action\n        \"\"\"\n        function_def = {\n            \"name\": \"identify_end_url\",\n            \"description\": \"Identify the URL responsible for a specific action\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"url\": {\n                        \"type\": \"string\",\n                        \"description\": f\"The URL responsible for {self.prompt}\"\n                    }\n                },\n                \"required\": [\"url\"]\n            }\n        }\n\n        prompt = f\"\"\"\n        {self.har_urls}\n        Task:\n        Given the above list of URLs, request types, and response formats, find the URL responsible for the action below:\n        {self.prompt}\n        \"\"\"\n\n        response = llm.get_instance().invoke(\n            prompt,\n            functions=[function_def],\n            function_call={\"name\": \"identify_end_url\"}\n        )\n        \n        function_call = response.additional_kwargs['function_call']\n        end_url = json.loads(function_call['arguments'])['url']\n\n        state[self.ACTION_URL_KEY] = end_url\n        return state\n\n    def input_variables_identifying_agent(self, state: AgentState) -> AgentState:\n        \"\"\"\n        Identify input variables present in the cURL command\n        \"\"\"\n        in_process_node_id = state[self.IN_PROCESS_NODE_KEY]\n        curl = self.dag_manager.graph.nodes[in_process_node_id][\"content\"][\"key\"].to_curl_command()\n        input_variables = state[self.INPUT_VARIABLES_KEY]\n        if not input_variables:\n            return state\n        \n        function_def = {\n            \"name\": \"identify_input_variables\",\n            \"description\": \"Identify input variables present in the cURL command.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"identified_variables\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"variable_name\": {\"type\": \"string\", \"description\": \"The original key of the variable\"},\n                                \"variable_value\": {\"type\": \"string\", \"description\": \"The exact version of the variable that is present in the cURL command. This should closely match the value in the provided Input Variables.\"}\n                            },\n                            \"required\": [\"variable_name\", \"variable_value\"]\n                        },\n                        \"description\": \"A list of identified variables and their values.\"\n                    }\n                },\n                \"required\": [\"identified_variables\"]\n            }\n        }\n\n\n        prompt = f\"\"\"\n        cURL: {curl}\n        Input Variables: {input_variables}\n\n        Task:\n        Identify which input variables (the value in the key-value pair) from the Input Variables provided above are present in the cURL command.\n\n        Important:\n        - If an input variable is found in the cURL, include it in the output.\n        - Do not include variables that are not provided above.\n        - The key of the input variable is a description of the variable.\n        - The value is the value that should closely match the value in the cURL command. No substitutions.\n\n        \"\"\"\n\n\n        response = llm.get_instance().invoke(\n            prompt,\n            functions=[function_def],\n            function_call={\"name\": \"identify_input_variables\"}\n        )\n\n        function_call = response.additional_kwargs.get('function_call', {})\n        arguments = json.loads(function_call.get('arguments', '{}'))\n        identified_variables = arguments.get('identified_variables', [])\n        \n        if identified_variables:\n            # Convert the identified_variables format\n            converted_variables = {item['variable_name']: item['variable_value'] for item in identified_variables}\n            \n            current_dynamic_parts = self.dag_manager.graph.nodes[in_process_node_id].get(\"dynamic_parts\", [])\n            updated_dynamic_parts = [part for part in current_dynamic_parts if part not in converted_variables.values()]\n            self.dag_manager.update_node(in_process_node_id, dynamic_parts=updated_dynamic_parts, input_variables=converted_variables)\n\n        return state\n\n    def dynamic_part_identifying_agent(self, state: AgentState) -> AgentState:\n        \"\"\"\n        Identify dynamic parts present in the cURL command\n        \"\"\"\n        in_process_node_id = state[self.TO_BE_PROCESSED_NODES_KEY].pop()\n        request = self.dag_manager.graph.nodes[in_process_node_id][\"content\"][\"key\"]\n        curl = request.to_minified_curl_command()\n        if curl.endswith(\".js'\"):\n            self.dag_manager.update_node(in_process_node_id, dynamic_parts=[])\n            state[self.IN_PROCESS_NODE_DYNAMIC_PARTS_KEY] = [] \n            state[self.IN_PROCESS_NODE_KEY] = in_process_node_id\n            return state\n        \n\n        input_variables = state[self.INPUT_VARIABLES_KEY]            \n\n        function_def = {\n            \"name\": \"identify_dynamic_parts\",\n            \"description\": (\n                \"Given the above cURL command, identify which parts are dynamic and validated by the server \"\n                \"for correctness (e.g., IDs, tokens, session variables). Exclude any parameters that represent \"\n                \"arbitrary user input or general data that can be hardcoded (e.g., amounts, notes, messages).\"\n            ),\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"dynamic_parts\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": (\n                            \"List of dynamic parts identified in the cURL command. Do not include duplicates. \"\n                            \"Only strictly include the dynamic values (not the keys or any not extra part in front and after the value) of parts that are unique to a user or session \"\n                            \"and, if incorrect, will cause the request to fail.\"\n                            \"Do not include the keys, only the values.\"\n                        ),\n                    }\n                },\n                \"required\": [\"dynamic_parts\"],\n            },\n        }\n\n        prompt = f\"\"\"\n        URL: {curl}\n\n        Task:\n\n        Use your best judgment to identify which parts of the cURL command are dynamic, specific to a user or session, and are checked by the server for validity. These include tokens, IDs, session variables, or any other values that are unique to a user or session and, if incorrect, will cause the request to fail.\n\n        Important:\n            - IGNORE THE COOKIE HEADER\n            - Ignore common headers like user-agent, sec-ch-ua, accept-encoding, referer, etc.\n            - Exclude parameters that represent arbitrary user input or general data that can be hardcoded, such as amounts, notes, messages, actions, etc.\n            - Only output the variable values and not the keys.\n            - Only include dynamic parts that are unique identifiers, tokens, or session variables.\n\n        \"\"\"\n\n        response = llm.get_instance().invoke(\n            prompt,\n            functions=[function_def],\n            function_call={\"name\": \"identify_dynamic_parts\"}\n        )\n\n        function_call = response.additional_kwargs['function_call']\n        dynamic_parts = json.loads(function_call['arguments'])['dynamic_parts']\n\n        self.dag_manager.update_node(in_process_node_id, dynamic_parts=dynamic_parts)\n\n        # to detect if input_variables are in the request\n        present_variables = [variable for variable in input_variables if variable in curl]\n        if present_variables:\n            for variable in present_variables:\n                if variable in dynamic_parts:\n                    dynamic_parts.remove(variable)\n            self.dag_manager.update_node(in_process_node_id, input_variables=present_variables)\n\n\n        state[self.IN_PROCESS_NODE_DYNAMIC_PARTS_KEY] = dynamic_parts\n        state[self.IN_PROCESS_NODE_KEY] = in_process_node_id\n        return state\n\n    def url_to_curl(self, state: AgentState) -> AgentState:\n        \"\"\"\n        Identify the master cURL command responsible for the action\n        \"\"\"\n        request = self.url_to_res_req_dict[state[\"action_url\"]][\"request\"]\n        curl = request.to_curl_command()\n        if curl in self.curl_to_id_dict:\n            master_node_id = self.curl_to_id_dict[curl]\n        else:\n            master_node_id = self.dag_manager.add_node(\n                node_type=\"master_curl\",  # Specify node type\n                content={\n                    \"key\": request,\n                    \"value\": self.req_to_res_map[request]\n                },\n                dynamic_parts=[\"None\"],\n                extracted_parts=[\"None\"]\n            )\n            self.curl_to_id_dict[curl] = master_node_id\n        state[self.MASTER_NODE_KEY] = master_node_id\n        state[self.TO_BE_PROCESSED_NODES_KEY].append(master_node_id)\n        self.global_master_node_id = master_node_id\n        return state\n\n    def get_simplest_request(self, request_list: List[Request]) -> Request:\n        \"\"\"\n        Find the index of the simplest cURL command from a list\n        \"\"\"\n        function_def = {\n            \"name\": \"get_simplest_curl_index\",\n            \"description\": \"Find the index of the simplest cURL command from a list\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"index\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The index of the simplest cURL command in the list\"\n                    }\n                },\n                \"required\": [\"index\"]\n            }\n        }\n        # convert request objects to strings\n        serializable_list = [str(req) for req in request_list]\n\n        prompt = f\"\"\"\n        {json.dumps(serializable_list)}\n        Task:\n        Given the above list of cURL commands, find the index of the curl that has the least number of dependencies and variables.\n        The index should be 0-based (i.e., the first item has index 0).\n        \"\"\"\n\n        response = llm.get_instance().invoke(\n            prompt,\n            functions=[function_def],\n            function_call={\"name\": \"get_simplest_curl_index\"}\n        )\n\n        function_call = response.additional_kwargs['function_call']\n        simplest_curl_index = json.loads(function_call['arguments'])['index']\n        \n        # Retrieve the actual cURL command using the index\n        simplest_curl = request_list[simplest_curl_index]\n        return simplest_curl\n\n    def find_curl_from_content(self, state: AgentState) -> AgentState:\n        \"\"\"\n        Find the cURL command that contains the dynamic parts\n        \"\"\"\n        search_string_list = state[self.IN_PROCESS_NODE_DYNAMIC_PARTS_KEY]\n        search_string_list_leftovers = search_string_list.copy()\n\n        in_process_node_id = state[self.IN_PROCESS_NODE_KEY]\n        new_to_be_processed_nodes = []\n\n        # Handle cookies\n        for search_string in search_string_list_leftovers[:]:\n            cookie_key = self.find_key_by_string_in_value(\n                self.cookie_dict, search_string\n            )\n            if cookie_key:\n                search_string_list_leftovers.remove(search_string)\n                if cookie_key in self.cookie_to_id_dict:\n                    cookie_node_id = self.cookie_to_id_dict[cookie_key]\n                else:\n                    cookie_node_id = self.dag_manager.add_node(\n                        node_type=\"cookie\",  # Specify node type\n                        content={\n                            \"key\": cookie_key,\n                            \"value\": search_string\n                        }, \n                        extracted_parts=[search_string]\n                    )\n                    self.cookie_to_id_dict[cookie_key] = cookie_node_id\n                    #dont need to add node to to_be_processed_nodes because cookies dont need further processing\n                self.dag_manager.add_edge(in_process_node_id, cookie_node_id)\n\n        # Handle curls\n        if search_string_list_leftovers:\n            for search_string in search_string_list_leftovers[:]:\n                requests_with_search_string = []\n\n                for request, response in self.req_to_res_map.items():\n                    curl = str(request)\n                    if (\n                        (\n                            isinstance(curl, str)\n                            and search_string.lower() in response[\"text\"].lower()\n                        )\n                        and (search_string.lower() not in curl.lower())\n                    ) or (\n                        urllib.parse.unquote(search_string) in curl\n                        and (urllib.parse.unquote(search_string) not in curl)\n                    ):\n                        requests_with_search_string.append(request)\n                simplest_request = \"\"\n\n                # Get simplest curl to reduce number of dependencies\n                if len(requests_with_search_string) > 1:\n                    simplest_request = self.get_simplest_request(requests_with_search_string)\n                elif len(requests_with_search_string) == 1:\n                    simplest_request = requests_with_search_string[0]\n                else:\n                    print(f\"Could not find curl with search string: {search_string} in response\")\n                    not_found_node_id = self.dag_manager.add_node(\n                        node_type=\"not found\",\n                        content={\n                            \"key\": search_string\n                        },\n                    )\n                    self.dag_manager.add_edge(in_process_node_id, not_found_node_id)\n                    search_string_list_leftovers.remove(search_string)\n\n                    continue\n        \n                        \n                if simplest_request.url.endswith(\".js\") or \"text/html\" in self.req_to_res_map[simplest_request][\"type\"]:\n                    current_dynamic_parts = self.dag_manager.graph.nodes[in_process_node_id].get(\"dynamic_parts\", [])\n                    updated_dynamic_parts = [part for part in current_dynamic_parts if part != search_string]\n                    self.dag_manager.update_node(in_process_node_id, dynamic_parts=updated_dynamic_parts)\n                    search_string_list_leftovers.remove(search_string)\n                    continue    \n                \n      \n\n\n                if simplest_request not in self.curl_to_id_dict:\n                    if simplest_request.url.endswith(\".js\"):\n                        self.dag_manager.update_node(in_process_node_id, dynamic_parts=[])\n                        continue    \n\n                    curl_node_id = self.dag_manager.add_node(\n                        node_type=\"curl\",  # Specify node type\n                    content={\n                        \"key\": simplest_request,\n                        \"value\": self.req_to_res_map[simplest_request]\n                    },\n                    extracted_parts=[search_string]\n                    )\n                    self.curl_to_id_dict[simplest_request] = curl_node_id\n                    new_to_be_processed_nodes.append(curl_node_id)\n                else:\n                    # append new extracted part to existing curl node\n                    curl_node_id = self.curl_to_id_dict[simplest_request]\n                    node = self.dag_manager.get_node(curl_node_id)\n                    new_extracted_parts = node.get(\"extracted_parts\", [])\n                    new_extracted_parts.append(search_string)\n                    # Remove duplicates from new_extracted_parts\n                    new_extracted_parts = list(dict.fromkeys(new_extracted_parts))\n\n                    self.dag_manager.update_node(curl_node_id, extracted_parts=new_extracted_parts)\n\n                self.dag_manager.add_edge(in_process_node_id, curl_node_id)\n                \n        state[self.TO_BE_PROCESSED_NODES_KEY].extend(new_to_be_processed_nodes)\n        state[self.IN_PROCESS_NODE_DYNAMIC_PARTS_KEY] = []\n        return state\n\n    @staticmethod\n    def find_key_by_string_in_value(dictionary: Dict[str, Dict[str, Any]], search_string: str) -> Optional[str]:\n        for key, value in dictionary.items():\n            if search_string in value.get(\"value\", \"\"):\n                return key\n        return None\n    \n", "metadata": {"source": "github.com/Integuru-AI/Integuru", "file_path": "integuru/agent.py", "language": "python", "complexity": "complex", "star_count": 4406, "quality_score": 0.75, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 412, "word_count": 1347}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from langgraph.graph import END, StateGraph\nfrom integuru.models.agent_state import AgentState\nfrom integuru.agent import IntegrationAgent\nfrom functools import partial  # To pass extra arguments to functions\nfrom integuru.util.print import print_dag, visualize_dag, print_dag_in_reverse\n\ndef check_end_condition(state, agent, to_generate_code):\n    agent.dag_manager.detect_cycles()\n\n    if len(state.get(\"to_be_processed_nodes\", [])) == 0:\n        print(\"------------------------Successfully analyzed!!!-------------------------------\", flush=True)\n        print_dag(agent.dag_manager.graph, agent.global_master_node_id)\n        visualize_dag(agent.dag_manager.graph)\n        print_dag_in_reverse(agent.dag_manager.graph, to_generate_code=to_generate_code)\n        return \"end\"\n    else:\n        print(\"Continuing execution\", flush=True)\n        print(f\"Generated graph at current step: {print_dag(agent.dag_manager.graph, agent.global_master_node_id)}\", flush=True)\n        return \"continue\"\n\n\ndef build_graph(prompt, har_file_path=\"network_requests.har\", cookie_path=\"cookies.json\", to_generate_code=False):\n    agent = IntegrationAgent(prompt, har_file_path, cookie_path)\n\n    graph_builder = StateGraph(AgentState)\n\n    # Add nodes using the agent's methods\n    graph_builder.add_node(\"IntegrationAgent\", agent.end_url_identify_agent)\n    graph_builder.set_entry_point(\"IntegrationAgent\")\n\n    graph_builder.add_node(\"urlTocurl\", agent.url_to_curl)\n    graph_builder.add_edge(\"IntegrationAgent\", \"urlTocurl\")\n\n    graph_builder.add_node(\n        \"dynamicurlDataIdentifyingAgent\", agent.dynamic_part_identifying_agent\n    )\n    graph_builder.add_edge(\"urlTocurl\", \"dynamicurlDataIdentifyingAgent\")\n\n    graph_builder.add_node(\"inputVariablesIdentifyingAgent\", agent.input_variables_identifying_agent)\n    graph_builder.add_edge(\"dynamicurlDataIdentifyingAgent\", \"inputVariablesIdentifyingAgent\")\n\n    graph_builder.add_node(\"findcurlFromContent\", agent.find_curl_from_content)\n    graph_builder.add_edge(\"inputVariablesIdentifyingAgent\", \"findcurlFromContent\")\n\n    # Add conditional edges \n    graph_builder.add_conditional_edges(                \n        \"findcurlFromContent\",\n        partial(check_end_condition, agent=agent, to_generate_code=to_generate_code),\n        {\"end\": END, \"continue\": \"dynamicurlDataIdentifyingAgent\"},\n    )\n\n    graph = graph_builder.compile()\n    return graph, agent \n", "metadata": {"source": "github.com/Integuru-AI/Integuru", "file_path": "integuru/graph_builder.py", "language": "python", "complexity": "simple", "star_count": 4406, "quality_score": 0.69, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 54, "word_count": 125}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import List, Dict, Optional, Any\nimport json\n\nclass Request:\n    def __init__(self, method: str, url: str, headers: Dict[str, str], \n                 query_params: Optional[Dict[str, str]] = None, body: Optional[Any] = None):\n        self.method = method\n        self.url = url\n        self.headers = headers  \n        self.query_params = query_params\n        self.body = body\n\n    def to_curl_command(self) -> str:\n        curl_parts = [f\"curl -X {self.method}\"]\n\n        for name, value in self.headers.items():\n            curl_parts.append(f\"-H '{name}: {value}'\")\n\n        if self.query_params:\n            query_string = \"&\".join([f\"{k}={v}\" for k, v in self.query_params.items()])\n            self.url += f\"?{query_string}\"\n\n        if self.body:\n            content_type = None\n            for k in self.headers:\n                if k.lower() == 'content-type':\n                    content_type = self.headers[k]\n                    break\n\n            if isinstance(self.body, dict):\n                # Add Content-Type header if not present\n                if not content_type:\n                    curl_parts.append(f\"-H 'Content-Type: application/json'\")\n                curl_parts.append(f\"--data '{json.dumps(self.body)}'\")\n            elif isinstance(self.body, str):\n                curl_parts.append(f\"--data '{self.body}'\")\n\n        curl_parts.append(f\"'{self.url}'\")\n\n        return \" \".join(curl_parts)\n\n    def to_minified_curl_command(self) -> str:\n        \"\"\"\n        Minifies the curl command by removing referer and cookie headers.\n        This is done to reduce LLM hallucinations.\n        \"\"\"\n        curl_parts = [f\"curl -X {self.method}\"]\n\n        for name, value in self.headers.items():\n            if name.lower() not in ['referer', 'cookie']:\n                curl_parts.append(f\"-H '{name}: {value}'\")\n\n        if self.query_params:\n            query_string = \"&\".join([f\"{k}={v}\" for k, v in self.query_params.items()])\n            self.url += f\"?{query_string}\"\n\n        if self.body:\n            content_type = None\n            for k in self.headers:\n                if k.lower() == 'content-type':\n                    content_type = self.headers[k]\n                    break\n\n            if isinstance(self.body, dict):\n                if not content_type:\n                    curl_parts.append(f\"-H 'Content-Type: application/json'\")\n                curl_parts.append(f\"--data '{json.dumps(self.body)}'\")\n            elif isinstance(self.body, str):\n                curl_parts.append(f\"--data '{self.body}'\")\n\n        curl_parts.append(f\"'{self.url}'\")\n\n        return \" \".join(curl_parts)\n\n    def __str__(self) -> str:\n        return self.to_curl_command()\n", "metadata": {"source": "github.com/Integuru-AI/Integuru", "file_path": "integuru/models/request.py", "language": "python", "complexity": "simple", "star_count": 4406, "quality_score": 0.75, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 77, "word_count": 216}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from langchain_openai import ChatOpenAI\n\nclass LLMSingleton:\n    _instance = None\n    _default_model = \"gpt-4o\"  \n    _alternate_model = \"o1-preview\"\n\n    @classmethod\n    def get_instance(cls, model: str = None):\n        if model is None:\n            model = cls._default_model\n            \n        if cls._instance is None:\n            cls._instance = ChatOpenAI(model=model, temperature=1)\n        return cls._instance\n\n    @classmethod\n    def set_default_model(cls, model: str):\n        \"\"\"Set the default model to use when no specific model is requested\"\"\"\n        cls._default_model = model\n        cls._instance = None  # Reset instance to force recreation with new model\n\n    @classmethod\n    def revert_to_default_model(cls):\n        \"\"\"Set the default model to use when no specific model is requested\"\"\"\n        print(\"Reverting to default model: \", cls._default_model, \"Performance will be degraded as Integuru is using non O1 model\")\n        cls._alternate_model = cls._default_model\n\n    @classmethod\n    def switch_to_alternate_model(cls):\n        \"\"\"Returns a ChatOpenAI instance configured for o1-miniss\"\"\"\n        # Create a new instance only if we don't have one yet\n        cls._instance = ChatOpenAI(model=cls._alternate_model, temperature=1)\n\n        return cls._instance\n\nllm = LLMSingleton()\n\n", "metadata": {"source": "github.com/Integuru-AI/Integuru", "file_path": "integuru/util/LLM.py", "language": "python", "complexity": "simple", "star_count": 4406, "quality_score": 0.78, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 39, "word_count": 137}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nimport os\nfrom urllib.parse import urlparse\nfrom integuru.models.request import Request\nfrom typing import Tuple, Dict, Optional, Any, List\n\nexcluded_keywords = (\n    \"google\",\n    \"taboola\",\n    \"datadog\",\n    \"sentry\",\n    # \"relic\"\n)\n\nexcluded_header_keywords = (\n    \"cookie\",\n    \"sec-\",\n    \"accept\",\n    \"user-agent\",\n    \"referer\",\n    \"relic\",\n    \"sentry\",\n    \"datadog\",\n    \"amplitude\",\n    \"mixpanel\",\n    \"segment\",\n    \"heap\",\n    \"hotjar\",\n    \"fullstory\",\n    \"pendo\",\n    \"optimizely\",\n    \"adobe\",\n    \"analytics\",\n    \"tracking\",\n    \"telemetry\",\n    \"clarity\",  # Microsoft Clarity\n    \"matomo\",\n    \"plausible\",\n)\n\ndef format_request(har_request: Dict[str, Any]) -> Request:\n    \"\"\"\n    Formats a HAR request into a Request object.\n    \"\"\"\n    method = har_request.get(\"method\", \"GET\")\n    url = har_request.get(\"url\", \"\")\n\n    # Store headers as a dictionary, excluding headers containing excluded keywords\n    headers = {\n        header.get(\"name\", \"\"): header.get(\"value\", \"\")\n        for header in har_request.get(\"headers\", [])\n        if not any(keyword.lower() in header.get(\"name\", \"\").lower() \n                  for keyword in excluded_header_keywords)\n    }\n\n    query_params_list = har_request.get(\"queryString\", [])\n    query_params = {param[\"name\"]: param[\"value\"] for param in query_params_list} if query_params_list else None\n\n    post_data = har_request.get(\"postData\", {})\n    body = post_data.get(\"text\") if post_data else None\n\n    # Try to parse body as JSON if Content-Type is application/json\n    if body:\n        headers_lower = {k.lower(): v for k, v in headers.items()}\n        content_type = headers_lower.get('content-type')\n        if content_type and 'application/json' in content_type.lower():\n            try:\n                body = json.loads(body)\n            except json.JSONDecodeError:\n                pass  # Keep body as is if not valid JSON\n\n    return Request(\n        method=method,\n        url=url,\n        headers=headers, \n        query_params=query_params,\n        body=body\n    )\n\n\ndef format_response(har_response: Dict[str, Any]) -> Dict[str, str]:\n    \"\"\"\n    Extracts and returns the content text and content type from a HAR response.\n    \"\"\"\n    content = har_response.get(\"content\", {})\n    return {\n        \"text\": content.get(\"text\", \"\"),\n        \"type\": content.get(\"mimeType\", \"\")\n    }\n\n\ndef parse_har_file(har_file_path: str) -> Dict[Request, Dict[str, str]]:\n    \"\"\"\n    Parses the HAR file and returns a dictionary mapping Request objects to response dictionaries.\n    \"\"\"\n    req_res_dict = {}\n\n    with open(har_file_path, 'r', encoding='utf-8') as file:\n        har_data = json.load(file)\n\n    entries = har_data.get(\"log\", {}).get(\"entries\", [])\n\n    for entry in entries:\n        request_data = entry.get(\"request\", {})\n        response_data = entry.get(\"response\", {})\n\n        formatted_request = format_request(request_data)\n        response_dict = format_response(response_data)\n\n        req_res_dict[formatted_request] = response_dict\n\n    return req_res_dict\n\n\ndef build_url_to_req_res_map(req_res_dict: Dict[Request, Dict[str, str]]) -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Builds a dictionary mapping URLs to {'request': formatted_request, 'response': response_dict}\n    \"\"\"\n    url_to_req_res_dict = {}\n\n    for request, response in req_res_dict.items():\n        url = request.url\n        # If multiple requests to the same URL, you can choose to overwrite or store all\n        url_to_req_res_dict[url] = {\n            'request': request,\n            'response': response\n        }\n\n    return url_to_req_res_dict\n\n\ndef get_har_urls(har_file_path: str) -> List[Tuple[str, str, str, str]]:\n    \"\"\"\n    Extracts and returns a list of tuples containing method, URL, response format, and response preview\n    from a HAR file, excluding certain file types and keywords.\n    \"\"\"\n    # List to store tuples of URLs, request methods, response file formats, and response preview\n    urls_with_details = []\n\n    # Define a tuple of file extensions to exclude\n    excluded_extensions = (\n        \".png\",\n        \".jpg\",\n        \".jpeg\",\n        \".gif\",\n        \".webp\",\n        \".svg\",\n        \".ico\",  # Image files\n        \".css\",  # Stylesheets\n        # \".js\",\n        # \".map\",  # JavaScript files\n        \".woff\",\n        \".woff2\",\n        \".ttf\",\n        \".otf\",\n        \".eot\",  # Font files\n        \".mp3\",\n        \".mp4\",\n        \".wav\",\n        \".avi\",\n        \".mov\",\n        \".flv\",\n        \".wmv\",\n        \".webm\",  # Media files\n        # \".pdf\",\n        # \".zip\",\n        \".rar\",\n        \".7z\",\n        \".tar\",\n        \".gz\",\n        \".exe\",\n        \".dmg\",  # Other non-text files\n    )\n\n    # Read the HAR file\n    with open(har_file_path, \"r\", encoding=\"utf-8\") as file:\n        har_data = json.load(file)\n\n    # Extract entries from the HAR data\n    entries = har_data.get(\"log\", {}).get(\"entries\", [])   \n    for entry in entries:\n        request = entry.get(\"request\", {})\n        response = entry.get(\"response\", {})\n        url = request.get(\"url\")\n        method = request.get(\"method\", \"GET\")  # Default to 'GET' if method is missing\n        response_format = response.get(\"content\", {}).get(\"mimeType\", \"\")\n        response_text = response.get(\"content\", {}).get(\"text\", \"\")\n        response_preview = response_text[:30] if response_text else \"\"\n\n        if url:\n            parsed_url = urlparse(url)\n            path = parsed_url.path.lower()\n\n            _, extension = os.path.splitext(path)\n\n            request_text = url.lower()\n\n            headers = request.get(\"headers\", [])\n            for header in headers:\n                request_text += header.get(\"name\", \"\").lower()\n                request_text += header.get(\"value\", \"\").lower()\n\n            postData = request.get(\"postData\", {}).get(\"text\", \"\").lower()\n            request_text += postData\n\n            # Exclude URLs with the specified extensions or if keywords are in the request\n            # this is done to reduce the number of requests we send to the LLM\n            if extension not in excluded_extensions and not any(\n                keyword.lower() in request_text for keyword in excluded_keywords\n            ):\n                urls_with_details.append((method, url, response_format, response_preview))\n\n    return urls_with_details\n    \n\ndef parse_cookie_file_to_dict(cookie_file_path: str) -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Parses a JSON cookie file and returns a dictionary of cookie data.\n    \"\"\"\n    parsed_data = {}\n\n    with open(cookie_file_path, \"r\") as file:\n        cookies = json.load(file)  \n\n    for cookie in cookies:\n        name = cookie.get(\"name\")\n        value = cookie.get(\"value\")\n        domain = cookie.get(\"domain\")\n        path = cookie.get(\"path\")\n\n        if name:\n            parsed_data[name] = {\n                \"value\": value,\n                \"domain\": domain,\n                \"path\": path,\n                \"expires\": cookie.get(\"expires\"),\n                \"httpOnly\": cookie.get(\"httpOnly\"),\n                \"secure\": cookie.get(\"secure\"),\n                \"sameSite\": cookie.get(\"sameSite\"),\n            }\n\n    return parsed_data\n", "metadata": {"source": "github.com/Integuru-AI/Integuru", "file_path": "integuru/util/har_processing.py", "language": "python", "complexity": "complex", "star_count": 4406, "quality_score": 0.75, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 243, "word_count": 705}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from platform import node\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom typing import Dict, Set, Optional, Any\nfrom integuru.util.LLM import llm\nimport json\nfrom langchain_openai import ChatOpenAI\nfrom typing import List\nfrom openai import NotFoundError  # Add this import\n\ndef print_dag(\n    graph: nx.DiGraph,\n    current_node_id: str,\n    prefix: str = \"\",\n    is_last: bool = True,\n    visited: Optional[Set[str]] = None,\n    depth: int = 0,\n    max_depth: Optional[int] = None,\n) -> None:\n    \"\"\"\n    Recursively prints the DAG structure with visual connectors and cUrl.\n    \"\"\"\n    if visited is None:\n        visited = set()\n\n    connector = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n    new_prefix = prefix + (\"    \" if is_last else \"‚îÇ   \")\n\n    node_attrs = graph.nodes[current_node_id]\n    dynamic_parts = node_attrs.get(\"dynamic_parts\", [])\n    key = node_attrs.get(\"content\", \"\").get(\"key\", \"\")\n    extracted_parts = node_attrs.get(\"extracted_parts\", [])\n    input_variables = node_attrs.get(\"input_variables\", [])\n    node_type = node_attrs.get(\"node_type\", \"\")  # Get node type\n    \n    node_label = f\"[{node_type}] [node_id: {current_node_id}]\"\n    if input_variables:\n        node_label += f\"\\n{new_prefix}    [input_variables: {input_variables}]\"\n    node_label += f\"\\n{new_prefix}    [dynamic_parts: {dynamic_parts}]\"\n    node_label += f\"\\n{new_prefix}    [extracted_parts: {extracted_parts}]\"\n    node_label += f\"\\n{new_prefix}    [{key}]\"\n\n    print(f\"{prefix}{connector}{node_label}\")\n\n    visited.add(current_node_id)\n\n    if max_depth is not None and depth >= max_depth:\n        return\n\n    children = list(graph.successors(current_node_id))\n    child_count = len(children)\n\n    for i, child_id in enumerate(children):\n        is_last_child = i == child_count - 1\n\n        if child_id in visited:\n            loop_connector = \"‚îî‚îÄ‚îÄ \" if is_last_child else \"‚îú‚îÄ‚îÄ \"\n            print(f\"{new_prefix}{loop_connector}(Already visited) [node_id: {child_id}]\")\n        else:\n            print_dag(\n                graph,\n                child_id,\n                prefix=new_prefix,\n                is_last=is_last_child,\n                visited=visited,\n                depth=depth + 1,\n                max_depth=max_depth,\n            )\n\n\ndef visualize_dag(graph: nx.DiGraph) -> None:\n    \"\"\"\n    Visualizes the DAG using Matplotlib with arrows indicating direction.\n    \"\"\"\n    plt.switch_backend(\"Agg\")\n\n    pos = nx.spring_layout(graph) \n\n    nx.draw_networkx_nodes(graph, pos, node_size=700, node_color=\"lightblue\")\n\n    nx.draw_networkx_edges(\n        graph, pos, edgelist=graph.edges, arrowstyle=\"->\", arrowsize=20\n    )\n\n    labels = {node: f\"{node}\" for node in graph.nodes()}\n    nx.draw_networkx_labels(graph, pos, labels, font_size=10)\n\n    edge_labels = nx.get_edge_attributes(graph, \"cUrl\") \n    nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels)\n\n    plt.title(\"Directed Acyclic Graph (DAG)\")\n    plt.savefig(\"dag_visualization.png\")\n    plt.close()\n\n\ndef find_json_path(json_obj, target_value, current_path=None):\n    \"\"\"\n    Finds the path(s) to a given value in a JSON object.\n    \n    Args:\n    json_obj (dict or list): The JSON object to search.\n    target_value: The value to find in the JSON object.\n    current_path (list): The current path being explored (used for recursion).\n\n    Returns:\n    list: A list of dictionaries, each containing 'key_path' and 'value' for each occurrence of the target value.\n    \"\"\"\n    if current_path is None:\n        current_path = []\n\n    results = []\n\n    if isinstance(json_obj, dict):\n        for key, value in json_obj.items():\n            new_path = current_path + [key]\n            if value == target_value:\n                results.append({\n                    'key_path': new_path,\n                    'value': value\n                })\n            if isinstance(value, (dict, list)):\n                results.extend(find_json_path(value, target_value, new_path))\n    elif isinstance(json_obj, list):\n        for i, item in enumerate(json_obj):\n            new_path = current_path + [i]\n            if item == target_value:\n                results.append({\n                    'key_path': new_path,\n                    'value': item\n                })\n            if isinstance(item, (dict, list)):\n                results.extend(find_json_path(item, target_value, new_path))\n    \n    return results\n\n\n\ndef generate_code(node_id: str, graph: nx.DiGraph) -> str:\n    \"\"\"\n    Generates Python code for a given node in the graph based on its attributes.\n    \"\"\"\n\n    node_attrs = graph.nodes[node_id]\n\n    if node_attrs.get(\"node_type\", \"\") == \"cookie\":\n        cookie_value = node_attrs.get('content', {}).get('value', '')\n        cookie_key = node_attrs.get('content', {}).get('key', '')\n        return f\"{cookie_value} = cookie_dict['{cookie_key}']\"\n\n    content = node_attrs.get(\"content\", {})\n    curl = content.get(\"key\", \"\")\n    response = content.get(\"value\", {})\n    response_type = response.get(\"type\", \"\")\n    response_text = response.get(\"text\", \"\")\n\n    dynamic_parts = node_attrs.get(\"dynamic_parts\", \"\")\n    extracted_parts = node_attrs.get(\"extracted_parts\", \"\")\n    input_variables = node_attrs.get(\"input_variables\", \"\")\n    to_parse_response = True\n\n    parse_response_prompt = \"\"\n\n    if response_type in [\"application/octet-stream\", \"application/pdf\", \"application/zip\", \"image/jpeg\", \"image/png\"]:\n        parse_response_prompt = f\"\"\"\n            The response is a downloadable file of type {response_type}.\n            Include code to save the response content to a file with an appropriate extension.\n        \"\"\"\n\n    if \"application/json\" in response_type:\n        key_paths = []\n        for extracted_part in extracted_parts:\n            key_path = find_json_path(json.loads(response_text), extracted_part)\n            key_paths.append(key_path)\n\n        parse_response_prompt = f\"\"\"\n            Response:\n            {response_text}\n\n            Parse out the following variables from the response using JSON keys:\n            {key_paths}\n\n            Through your judgement from analyzing the response, if polling is required to retrieve the variables above from the response. If so, implement polling else dont.\n        \"\"\"\n\n    if \"text/html\" in response_type or \"application/javascript\" in response_type:\n        if len(response_text) > 100000:\n            context_snippets = []\n            for part in extracted_parts:\n                index = response_text.find(part)\n                if index != -1:\n                    start = max(0, index - 50)\n                    end = min(len(response_text), index + len(part) + 50)\n                    snippet = response_text[start:end]\n                    context_snippets.append(f\"{part}: {snippet}\")\n            \n            parse_response_prompt = f\"\"\"\n                The HTML response is too long to process entirely. \n                Here are the relevant sections for each variable to be extracted:\n\n                {chr(10).join(context_snippets)}\n\n            \"\"\"\n        else:\n            parse_response_prompt = f\"\"\"\n                Response:\n                {response_text}\n            \"\"\"\n        parse_response_prompt += f\"\"\"\n            Parse out the variables following variables locations from the response using regex using locational context: \n\n            {extracted_parts}\n            Do not include the variable in the regex filter as the variable will change. And do not be too specific with the regex.\n\n        \"\"\"\n\n    dynamic_parts_prompt = \"\"\n    if dynamic_parts:\n        dynamic_parts_prompt = f\"\"\"\n    Instead of hard coding, pass the following variables into the function as parameters in a dict. The dict should have keys thats the same as the value itself\n    {dynamic_parts} \n\n    Keep everything else in the header hardcoded.\n    \"\"\"\n\n    prompt = f\"\"\"\n    Task:\n    Write a Python function with a descriptive name that makes a request like the cURL below:\n    {curl}\n\n\n    Assume cookies are in a variable as parameter called \"cookie_string\".\n\n    The parameters should be {\"1. a dict of all the parameters and 2. Just the cookie string\" if dynamic_parts else \"only the cookie string\"}.\n\n    {dynamic_parts_prompt}\n\n    {parse_response_prompt}\n    \n    Return a dictionary with the keys as the original parsed values content (needs to be hardcoded) and the values as the parsed values.\n\n    Do not include pseudo-headers or any headers that start with a colon in the request.\n\n    IMPORTANT! Do not include any backticks or markdown syntax AT ALL\n\n    \"\"\"\n\n    # Make the API call using o1_llm\n\n    llm_model = llm.switch_to_alternate_model()\n    try:\n        response = llm_model.invoke(prompt)\n    except Exception as e:\n        print(\"Switching to default model\")\n        llm.revert_to_default_model()\n        response = llm.switch_to_alternate_model().invoke(prompt)\n\n    # Extract the generated code from the response\n    code = response.content.strip()\n\n    # cannot get chatgpt to not return backticks\n    if code.startswith(\"```python\"):\n        code = code[10:]\n    if code.endswith(\"```\"):\n        code = code[:-3]\n\n    return code\n\ndef aggregate_functions(txt_path, output_path):\n    # Read the content of the file\n    with open(txt_path, 'r') as file:\n        content = file.read()\n\n    # Initialize ChatGPT\n\n    # Prepare the prompt for ChatGPT\n    prompt = f\"\"\"\n    The following text contains multiple Python functions:\n\n    {content}\n\n    Please generate Python code that does the following:    \n    1. Fix up the functions if needed in the order they appear in the text.\n    2. Leave everything that is hardcoded as is.\n    3. Call each function in the order they appear in the text.\n    4. The cookies will be hard coded in the file in a string format of key=value;key=value. You will need to convert them to a dict to retrieve values from them.\n    5. Pass the return value of each function as an argument to the next function, if applicable.\n    6. Ensure that the last function in the text is called last.\n    7. Output the entire directly runnable code\n\n\n\n    Only provide the Python code, without any explanations or markdown formatting.\n    DO NOT include any backticks or markdown syntax AT ALL\n    \"\"\"\n\n    # Get the response from ChatGPT\n\n    llm_model = llm.switch_to_alternate_model()\n    try:\n        response = llm_model.invoke(prompt)\n    except Exception as e:\n        print(\"Switching to default model\")\n        llm.revert_to_default_model()\n        response = llm.switch_to_alternate_model().invoke(prompt)\n    # Extract the generated code\n    generated_code = response.content.strip()\n\n    # Save the generated code to the specified output file\n    with open(output_path, 'w') as file:\n        file.write(generated_code)\n\n    print(f\"Aggregated function calls have been saved to '{output_path}'\")\n\n    return output_path\n\ndef generate_obfuscation_map(dynamic_parts_list: List[str]) -> Dict[str, str]:\n    obfuscation_map = {}\n    for part in dynamic_parts_list:\n        # Replace invalid characters with underscores and prepend with 'var_' to ensure it starts with a letter\n        safe_key = f\"var_{hash(part)}\".replace('-', '_').replace('.', '_')\n        obfuscation_map[part] = safe_key\n    return obfuscation_map\n\ndef swap_string_using_obfuscation_map(input_string: str, obfuscation_map: Dict[str, str]) -> str:\n    \"\"\"\n    Swaps all parts in the input string that match keys in the obfuscation map with their corresponding values.\n\n    Args:\n    input_string (str): The string to perform replacements on.\n    obfuscation_map (Dict[str, str]): The obfuscation map with keys to be replaced by their values.\n\n    Returns:\n    str: The modified string with replacements made.\n    \"\"\"\n    for key, value in obfuscation_map.items():\n        input_string = input_string.replace(key, value)\n    return input_string\n\ndef print_dag_in_reverse(graph: nx.DiGraph, max_depth: Optional[int] = None, to_generate_code: bool = False) -> None:\n    \"\"\"\n    Generates the order of requests to be made based on the DAG.\n    Prints the DAG starting from source nodes and ending at sink nodes, traversing successors.\n    \"\"\"\n    if to_generate_code:\n        print(\"--------------Generating code------------\")\n\n    generated_code = \"\"\n\n    dynamic_parts_list = []\n\n    def _print_dag_recursive(\n        current_node_id: str,\n        prefix: str = \"\",\n        is_last: bool = True,\n        visited: Optional[Set[str]] = None,\n        fully_processed: Optional[Set[str]] = None,\n        depth: int = 0,\n    ) -> None:\n        \"\"\"\n        Helper function to recursively print the DAG in reverse order.\n        \"\"\"\n        nonlocal generated_code, dynamic_parts_list\n        if visited is None:\n            visited = set()\n        if fully_processed is None:\n            fully_processed = set()\n    \n        if current_node_id in fully_processed:\n            return\n    \n        if current_node_id in visited:\n            # Avoid infinite recursion in case of cycles\n            return\n    \n        visited.add(current_node_id)\n    \n        if max_depth is not None and depth >= max_depth:\n            visited.remove(current_node_id)\n            return\n    \n        # Get child nodes (successors)\n        children = list(graph.successors(current_node_id))\n        child_count = len(children)\n    \n        # Recursively process child nodes first\n        for i, child_id in enumerate(children):\n            is_last_child = i == child_count - 1\n            new_prefix = prefix + (\"    \" if is_last else \"‚îÇ   \")\n            _print_dag_recursive(\n                child_id,\n                prefix=new_prefix,\n                is_last=is_last_child,  # Ensure this argument is passed correctly\n                visited=visited,\n                fully_processed=fully_processed,\n                depth=depth + 1,\n            )\n    \n        # After all children have been processed, print the current node\n        connector = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n        print(f\"{prefix}{connector}{get_node_label(graph, current_node_id)}\")\n        if to_generate_code:\n            generated_code += generate_code(current_node_id, graph) + \"\\n\\n\"\n        fully_processed.add(current_node_id)\n        visited.remove(current_node_id)\n    \n    def get_node_label(graph: nx.DiGraph, node_id: str) -> str:\n        \"\"\"\n        Generates a label for a node in the graph based on its attributes.\n        \"\"\"\n        # Get node attributes\n        node_attrs = graph.nodes[node_id]\n        dynamic_parts = node_attrs.get(\"dynamic_parts\", [])\n        extracted_parts = node_attrs.get(\"extracted_parts\", \"\")\n        content = node_attrs.get(\"content\", \"\")\n        key = content.get(\"key\", \"\")\n        input_variables = node_attrs.get(\"input_variables\", \"\")\n\n        if dynamic_parts:\n            dynamic_parts_list.extend(dynamic_parts)\n        node_type = node_attrs.get(\"node_type\", \"\")\n        node_label = f\"[{node_type}] \"\n        node_label += f\"[node_id: {node_id}]\"\n        node_label += f\" [dynamic_parts: {dynamic_parts}]\"\n        node_label += f\" [extracted_parts: {extracted_parts}]\"\n        node_label += f\" [input_variables: {input_variables}]\"\n        node_label += f\" [{key}]\"\n        return node_label\n    \n    # Start from source nodes (nodes with no incoming edges)\n    source_nodes = [n for n in graph.nodes() if graph.in_degree(n) == 0]\n    \n    fully_processed = set()\n    for idx, source_node in enumerate(source_nodes):\n        is_last_source = idx == len(source_nodes) - 1\n        _print_dag_recursive(\n            source_node,\n            prefix=\"\",\n            is_last=is_last_source,\n            visited=set(),\n            fully_processed=fully_processed,\n            depth=0,\n        )\n    \n    if to_generate_code:\n        obfuscation_map = generate_obfuscation_map(dynamic_parts_list)\n        generated_code = swap_string_using_obfuscation_map(generated_code, obfuscation_map)\n        with open(\"generated_code.txt\", \"w\") as f:\n            f.write(generated_code)\n        \n        aggregate_functions(\"generated_code.txt\", \"generated_code.py\")\n        print(\"--------------Generated integration code in generated_code.py!!------------\")\n\n\n\n", "metadata": {"source": "github.com/Integuru-AI/Integuru", "file_path": "integuru/util/print.py", "language": "python", "complexity": "complex", "star_count": 4406, "quality_score": 0.75, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 455, "word_count": 1635}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import unittest\nfrom integuru.agent import IntegrationAgent\nfrom integuru.models.agent_state import AgentState\nfrom unittest.mock import patch, MagicMock\n\nclass TestIntegrationAgent(unittest.TestCase):\n\n    def setUp(self):\n        self.prompt = \"Test prompt\"\n        self.har_file_path = \"test.har\"\n        self.cookie_path = \"test_cookies.json\"\n        self.agent = IntegrationAgent(self.prompt, self.har_file_path, self.cookie_path)\n        self.state = AgentState(\n            master_node=None,\n            in_process_node=None,\n            to_be_processed_nodes=[],\n            in_process_node_dynamic_parts=[],\n            action_url=\"\",\n            input_variables={}\n        )\n\n    @patch('integuru.agent.llm.get_instance')\n    def test_end_url_identify_agent(self, mock_llm_instance):\n        mock_response = MagicMock()\n        mock_response.additional_kwargs = {\n            'function_call': {\n                'arguments': '{\"url\": \"http://example.com/action\"}'\n            }\n        }\n        mock_llm_instance.return_value.invoke.return_value = mock_response\n\n        updated_state = self.agent.end_url_identify_agent(self.state)\n        self.assertEqual(updated_state[self.agent.ACTION_URL_KEY], \"http://example.com/action\")\n\n    @patch('integuru.agent.llm.get_instance')\n    def test_input_variables_identifying_agent(self, mock_llm_instance):\n        self.state[self.agent.IN_PROCESS_NODE_KEY] = \"node_1\"\n        self.state[self.agent.INPUT_VARIABLES_KEY] = {\"var1\": \"value1\"}\n        self.agent.dag_manager.graph.add_node(\"node_1\", content={\"key\": MagicMock()})\n        self.agent.dag_manager.graph.nodes[\"node_1\"][\"content\"][\"key\"].to_curl_command.return_value = \"curl command\"\n\n        mock_response = MagicMock()\n        mock_response.additional_kwargs = {\n            'function_call': {\n                'arguments': '{\"identified_variables\": [{\"variable_name\": \"var1\", \"variable_value\": \"value1\"}]}'\n            }\n        }\n        mock_llm_instance.return_value.invoke.return_value = mock_response\n\n        updated_state = self.agent.input_variables_identifying_agent(self.state)\n        self.assertEqual(updated_state[self.agent.INPUT_VARIABLES_KEY], {\"var1\": \"value1\"})\n\n    @patch('integuru.agent.llm.get_instance')\n    def test_dynamic_part_identifying_agent(self, mock_llm_instance):\n        self.state[self.agent.TO_BE_PROCESSED_NODES_KEY] = [\"node_1\"]\n        self.agent.dag_manager.graph.add_node(\"node_1\", content={\"key\": MagicMock()})\n        self.agent.dag_manager.graph.nodes[\"node_1\"][\"content\"][\"key\"].to_minified_curl_command.return_value = \"curl command\"\n\n        mock_response = MagicMock()\n        mock_response.additional_kwargs = {\n            'function_call': {\n                'arguments': '{\"dynamic_parts\": [\"dynamic_part1\"]}'\n            }\n        }\n        mock_llm_instance.return_value.invoke.return_value = mock_response\n\n        updated_state = self.agent.dynamic_part_identifying_agent(self.state)\n        self.assertEqual(updated_state[self.agent.IN_PROCESS_NODE_DYNAMIC_PARTS_KEY], [\"dynamic_part1\"])\n\nif __name__ == '__main__':\n    unittest.main()\n", "metadata": {"source": "github.com/Integuru-AI/Integuru", "file_path": "tests/test_integration_agent.py", "language": "python", "complexity": "simple", "star_count": 4406, "quality_score": 0.78, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 72, "word_count": 152}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: 2\nupdates:\n- package-ecosystem: pip\n  directory: \"/\"\n  schedule:\n    interval: monthly\n    time: '08:00'\n    timezone: US/Pacific\n  open-pull-requests-limit: 10\n  target-branch: flask_mongorest\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": ".github/dependabot.yml", "language": "yaml", "complexity": "simple", "star_count": 3704, "quality_score": 0.6, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 11, "word_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "language: python\n\nmatrix:\n  include:\n    - python: 3.6\n      env: TOXENV=py36-flask010\n    - python: 3.6\n      env: TOXENV=py36-flask10\n    - python: 3.6\n      env: TOXENV=py36-flask104\n#    - python: 3.7\n#      env: TOXENV=py37-flask010\n#      dist: xenial\n#      sudo: true\n#    - python: 3.7\n#      env: TOXENV=py37-flask10\n#      dist: xenial\n#      sudo: true\n    - python: 3.7\n      env: TOXENV=py37-flask104\n      dist: xenial\n      sudo: true\n    - python: 3.7\n      env: TOXENV=py37-flask104-marshmallow-apispec\n    - python: 3.8-dev\n      env: TOXENV=py38-dev-flasklatest\n      dist: xenial\n      sudo: true\n    - python: 3.9\n      env: TOXENV=py39-flask104\n      dist: jammy\n    - python: 3.11\n      env: TOXENV=py311-flasklatest\n      dist: jammy\n    - python: 3.11\n      env: TOXENV=py311-marshmallow-apispec\n      dist: jammy\n  allow_failures:\n    - env: TOXENV=py38-dev-flasklatest\n    - env: TOXENV=py36-flask010\n    - env: TOXENV=py36-flask10\n\nbefore_install:\n  - pip install tox\n\nscript:\n  - tox\n\nafter_success:\n  - coveralls\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": ".travis.yml", "language": "yaml", "complexity": "simple", "star_count": 3704, "quality_score": 0.6, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 51, "word_count": 110}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment include:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at rochacbruno@gmail.com. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at [http://contributor-covenant.org/version/1/4][version]\n\n[homepage]: http://contributor-covenant.org\n[version]: http://contributor-covenant.org/version/1/4/\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": "CODE_OF_CONDUCT.md", "language": "markdown", "complexity": "medium", "star_count": 3704, "quality_score": 0.63, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 47, "word_count": 446}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Flasgger\n## Easy Swagger UI for your Flask API\n\n[![Build Status](https://travis-ci.com/flasgger/flasgger.svg?branch=master)](https://travis-ci.com/flasgger/flasgger)\n[![Code Health](https://landscape.io/github/rochacbruno/flasgger/master/landscape.svg?style=flat)](https://landscape.io/github/rochacbruno/flasgger/master)\n[![Coverage Status](https://coveralls.io/repos/github/rochacbruno/flasgger/badge.svg?branch=master)](https://coveralls.io/github/rochacbruno/flasgger?branch=master)\n[![PyPI](https://img.shields.io/pypi/v/flasgger.svg)](https://pypi.python.org/pypi/flasgger)\n <a target=\"_blank\" href=\"https://www.paypal.com/cgi-bin/webscr?cmd=_donations&amp;business=rochacbruno%40gmail%2ecom&amp;lc=BR&amp;item_name=Flasgger&amp;no_note=0&amp;currency_code=USD&amp;bn=PP%2dDonationsBF%3abtn_donate_SM%2egif%3aNonHostedGuest\"><img alt='Donate with Paypal' src='http://www.paypalobjects.com/en_US/i/btn/btn_donate_SM.gif' /></a>\n\n\n![flasgger](docs/flasgger.png)\n\nFlasgger is a Flask extension to **extract [OpenAPI-Specification](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#operation-object)** from all Flask views registered in your API.\n\nFlasgger also comes with **[SwaggerUI](http://swagger.io/swagger-ui/) embedded** so you can access [http://localhost:5000/apidocs](localhost:5000/apidocs) and visualize and interact with your API resources.\n\nFlasgger also **provides validation** of the incoming data, using the same specification it can validates if the data received as a POST, PUT, PATCH is valid against the schema defined using **YAML**, **Python dictionaries** or **Marshmallow Schemas**.\n\nFlasgger can work with simple function views or MethodViews using docstring as specification, or using `@swag_from` decorator to get specification from **YAML** or **dict** and also provides **SwaggerView** which can use **Marshmallow Schemas**  as specification.\n\nFlasgger is compatible with `Flask-RESTful` so you can use `Resources` and `swag` specifications together, take a look at [restful example.](examples/restful.py)\n\nFlasgger also supports `Marshmallow APISpec` as base template for specification, if you are using APISPec from Marshmallow take a look at [apispec example.](examples/apispec_example.py)\n\nTable of Contents\n=================\n\n* [Top Contributors](#top-contributors)\n* [Examples and demo app](#examples-and-demo-app)\n  * [Docker](#docker)\n* [Installation](#installation)\n* [Getting started](#getting-started)\n  * [Using docstrings as specification](#using-docstrings-as-specification)\n  * [Using external YAML files](#using-external-yaml-files)\n  * [Using dictionaries as raw specs](#using-dictionaries-as-raw-specs)\n  * [Using Marshmallow Schemas](#using-marshmallow-schemas)\n  * [Using <strong>Flask RESTful</strong> Resources](#using-flask-restful-resources)\n  * [Auto-parsing external YAML docs and MethodViews](#auto-parsing-external-yaml-docs-and-methodviews)\n  * [Handling multiple http methods and routes for a single function](#handling-multiple-http-methods-and-routes-for-a-single-function)\n* [Use the same data to validate your API POST body.](#use-the-same-data-to-validate-your-api-post-body)\n     * [Custom validation](#custom-validation)\n     * [Validation Error handling](#validation-error-handling)\n* [Get defined schemas as python dictionaries](#get-defined-schemas-as-python-dictionaries)\n* [HTML sanitizer](#html-sanitizer)\n* [Swagger UI and templates](#swagger-ui-and-templates)\n* [OpenAPI 3.0 Support](#openapi-30-support)\n  * [Externally loading Swagger UI and jQuery JS/CSS](#externally-loading-swagger-ui-and-jquery-jscss)\n* [Initializing Flasgger with default data.](#initializing-flasgger-with-default-data)\n  * [Getting default data at runtime](#getting-default-data-at-runtime)\n  * [Behind a reverse proxy](#behind-a-reverse-proxy)\n* [Customize default configurations](#customize-default-configurations)\n  * [Extracting Definitions](#extracting-definitions)\n  * [Python2 Compatibility](#python2-compatibility)\n\nCreated by [gh-md-toc](https://github.com/ekalinin/github-markdown-toc)\n\n# Top Contributors\n\n[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/0)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/0)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/1)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/1)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/2)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/2)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/3)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/3)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/4)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/4)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/5)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/5)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/6)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/6)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/7)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/7)\n\n# Examples and demo app\n\nThere are some [example applications](examples/) and you can also play with examples in [Flasgger demo app](http://flasgger.pythonanywhere.com/)\n\n> NOTE: all the examples apps are also test cases and run automatically in Travis CI to ensure quality and coverage.\n\n## Docker\n\nThe examples and demo app can also be built and run as a Docker image/container:\n\n```\ndocker build -t flasgger .\ndocker run -it --rm -p 5000:5000 --name flasgger flasgger\n```\nThen access the Flasgger demo app at http://localhost:5000 .\n\n# Installation\n\n> under your virtualenv do:\n\nEnsure you have latest setuptools\n```\npip install -U setuptools\n```\n\nthen install beta version (recommended)\n\n```\npip install flasgger==0.9.7b2\n```\n\nor (latest stable for legacy apps)\n\n```\npip install flasgger==0.9.5\n```\n\nor (dev version)\n\n```\npip install https://github.com/flasgger/flasgger/tarball/master\n```\n\n> NOTE: If you want to use **Marshmallow Schemas** you also need to run `pip install marshmallow apispec`\n\n## How to run tests\n\n(You may see the command in [.travis.yml](./.travis.yml) for `before_install` part)\n\nIn your virtualenv:\n\n```\npip install -r requirements.txt\npip install -r requirements-dev.txt\nmake test\n```\n\n# Getting started\n\n## Using docstrings as specification\n\nCreate a file called for example `colors.py`\n\n```python\nfrom flask import Flask, jsonify\nfrom flasgger import Swagger\n\napp = Flask(__name__)\nswagger = Swagger(app)\n\n@app.route('/colors/<palette>/')\ndef colors(palette):\n    \"\"\"Example endpoint returning a list of colors by palette\n    This is using docstrings for specifications.\n    ---\n    parameters:\n      - name: palette\n        in: path\n        type: string\n        enum: ['all', 'rgb', 'cmyk']\n        required: true\n        default: all\n    definitions:\n      Palette:\n        type: object\n        properties:\n          palette_name:\n            type: array\n            items:\n              $ref: '#/definitions/Color'\n      Color:\n        type: string\n    responses:\n      200:\n        description: A list of colors (may be filtered by palette)\n        schema:\n          $ref: '#/definitions/Palette'\n        examples:\n          rgb: ['red', 'green', 'blue']\n    \"\"\"\n    all_colors = {\n        'cmyk': ['cyan', 'magenta', 'yellow', 'black'],\n        'rgb': ['red', 'green', 'blue']\n    }\n    if palette == 'all':\n        result = all_colors\n    else:\n        result = {palette: all_colors.get(palette)}\n\n    return jsonify(result)\n\napp.run(debug=True)\n```\n\nNow run:\n\n```\npython colors.py\n```\n\nAnd go to: [http://localhost:5000/apidocs/](http://localhost:5000/apidocs/)\n\nYou should get:\n\n![colors](docs/colors.png)\n\n## Using external YAML files\n\nSave a new file `colors.yml`\n\n```yaml\nExample endpoint returning a list of colors by palette\nIn this example the specification is taken from external YAML file\n---\nparameters:\n  - name: palette\n    in: path\n    type: string\n    enum: ['all', 'rgb', 'cmyk']\n    required: true\n    default: all\ndefinitions:\n  Palette:\n    type: object\n    properties:\n      palette_name:\n        type: array\n        items:\n          $ref: '#/definitions/Color'\n  Color:\n    type: string\nresponses:\n  200:\n    description: A list of colors (may be filtered by palette)\n    schema:\n      $ref: '#/definitions/Palette'\n    examples:\n      rgb: ['red', 'green', 'blue']\n```\n\n\nlets use the same example changing only the view function.\n\n```python\nfrom flasgger import swag_from\n\n@app.route('/colors/<palette>/')\n@swag_from('colors.yml')\ndef colors(palette):\n    ...\n```\n\nIf you do not want to use the decorator you can use the docstring `file:` shortcut.\n\n```python\n@app.route('/colors/<palette>/')\ndef colors(palette):\n    \"\"\"\n    file: colors.yml\n    \"\"\"\n    ...\n```\n\n\n## Using dictionaries as raw specs\n\nCreate a Python dictionary as:\n\n```python\nspecs_dict = {\n  \"parameters\": [\n    {\n      \"name\": \"palette\",\n      \"in\": \"path\",\n      \"type\": \"string\",\n      \"enum\": [\n        \"all\",\n        \"rgb\",\n        \"cmyk\"\n      ],\n      \"required\": \"true\",\n      \"default\": \"all\"\n    }\n  ],\n  \"definitions\": {\n    \"Palette\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"palette_name\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"$ref\": \"#/definitions/Color\"\n          }\n        }\n      }\n    },\n    \"Color\": {\n      \"type\": \"string\"\n    }\n  },\n  \"responses\": {\n    \"200\": {\n      \"description\": \"A list of colors (may be filtered by palette)\",\n      \"schema\": {\n        \"$ref\": \"#/definitions/Palette\"\n      },\n      \"examples\": {\n        \"rgb\": [\n          \"red\",\n          \"green\",\n          \"blue\"\n        ]\n      }\n    }\n  }\n}\n```\n\nNow take the same function and use the dict in the place of YAML file.\n\n```python\n@app.route('/colors/<palette>/')\n@swag_from(specs_dict)\ndef colors(palette):\n    \"\"\"Example endpoint returning a list of colors by palette\n    In this example the specification is taken from specs_dict\n    \"\"\"\n    ...\n```\n\n## Using Marshmallow Schemas\n\n> FIRST: `pip install marshmallow apispec`\n\n> USAGE #1: `SwaggerView`\n\n```python\nfrom flask import Flask, jsonify\nfrom flasgger import Swagger, SwaggerView, Schema, fields\n\n\nclass Color(Schema):\n    name = fields.Str()\n\nclass Palette(Schema):\n    pallete_name = fields.Str()\n    colors = fields.Nested(Color, many=True)\n\nclass PaletteView(SwaggerView):\n    parameters = [\n        {\n            \"name\": \"palette\",\n            \"in\": \"path\",\n            \"type\": \"string\",\n            \"enum\": [\"all\", \"rgb\", \"cmyk\"],\n            \"required\": True,\n            \"default\": \"all\"\n        }\n    ]\n    responses = {\n        200: {\n            \"description\": \"A list of colors (may be filtered by palette)\",\n            \"schema\": Palette\n        }\n    }\n\n    def get(self, palette):\n        \"\"\"\n        Colors API using schema\n        This example is using marshmallow schemas\n        \"\"\"\n        all_colors = {\n            'cmyk': ['cyan', 'magenta', 'yellow', 'black'],\n            'rgb': ['red', 'green', 'blue']\n        }\n        if palette == 'all':\n            result = all_colors\n        else:\n            result = {palette: all_colors.get(palette)}\n        return jsonify(result)\n\napp = Flask(__name__)\nswagger = Swagger(app)\n\napp.add_url_rule(\n    '/colors/<palette>',\n    view_func=PaletteView.as_view('colors'),\n    methods=['GET']\n)\n\napp.run(debug=True)\n\n```\n\n> USAGE #2: `Custom Schema from flasgger`\n\n- `Body` - support all fields in marshmallow\n- `Query` - support simple fields in marshmallow (Int, String and etc)\n- `Path` - support only int and str\n\n```python\nfrom flask import Flask, abort\nfrom flasgger import Swagger, Schema, fields\nfrom marshmallow.validate import Length, OneOf\n\napp = Flask(__name__)\nSwagger(app)\n\nswag = {\"swag\": True,\n        \"tags\": [\"demo\"],\n        \"responses\": {200: {\"description\": \"Success request\"},\n                      400: {\"description\": \"Validation error\"}}}\n\n\nclass Body(Schema):\n    color = fields.List(fields.String(), required=True, validate=Length(max=5), example=[\"white\", \"blue\", \"red\"])\n\n    def swag_validation_function(self, data, main_def):\n        self.load(data)\n\n    def swag_validation_error_handler(self, err, data, main_def):\n        abort(400, err)\n\n\nclass Query(Schema):\n    color = fields.String(required=True, validate=OneOf([\"white\", \"blue\", \"red\"]))\n\n    def swag_validation_function(self, data, main_def):\n        self.load(data)\n\n    def swag_validation_error_handler(self, err, data, main_def):\n        abort(400, err)\n\n    swag_in = \"query\"\n\n\n@app.route(\"/color/<id>/<name>\", methods=[\"POST\"], **swag)\ndef index(body: Body, query: Query, id: int, name: str):\n    return {\"body\": body, \"query\": query, \"id\": id, \"name\": name}\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n```\n\n\n> NOTE: take a look at `examples/validation.py` for a more complete example.\n\n\n> NOTE: when catching arguments in path rule always use explicit types, bad: ``/api/<username>`` good: ``/api/<string:username>``\n\n\n## Using **Flask RESTful** Resources\n\nFlasgger is compatible with Flask-RESTful you only need to install `pip install flask-restful` and then:\n\n```python\nfrom flask import Flask\nfrom flasgger import Swagger\nfrom flask_restful import Api, Resource\n\napp = Flask(__name__)\napi = Api(app)\nswagger = Swagger(app)\n\nclass Username(Resource):\n    def get(self, username):\n        \"\"\"\n        This examples uses FlaskRESTful Resource\n        It works also with swag_from, schemas and spec_dict\n        ---\n        parameters:\n          - in: path\n            name: username\n            type: string\n            required: true\n        responses:\n          200:\n            description: A single user item\n            schema:\n              id: User\n              properties:\n                username:\n                  type: string\n                  description: The name of the user\n                  default: Steven Wilson\n        \"\"\"\n        return {'username': username}, 200\n\n\napi.add_resource(Username, '/username/<username>')\n\napp.run(debug=True)\n```\n\n## Auto-parsing external YAML docs and `MethodView`s\n\nFlasgger can be configured to auto-parse external YAML API docs.  [Set a `doc_dir`](https://github.com/rochacbruno/flasgger/blob/aaef05c17cc559d01b7436211093463642eb6ae2/examples/parsed_view_func.py#L16) in your `app.config['SWAGGER']` and Swagger will load API docs by looking in `doc_dir` for YAML files stored by endpoint-name and method-name.  For example, `'doc_dir': './examples/docs/'` and a file `./examples/docs/items/get.yml` will provide a Swagger doc for `ItemsView` method `get`.\n\nAdditionally, when using **Flask RESTful** per above, by passing `parse=True` when constructing `Swagger`, Flasgger will use  `flask_restful.reqparse.RequestParser`, locate all `MethodView`s and parsed and validated data will be stored in `flask.request.parsed_data`.\n\n## Handling multiple http methods and routes for a single function\n\nYou can separate specifications by endpoint or methods\n\n```python\nfrom flasgger.utils import swag_from\n\n@app.route('/api/<string:username>', endpoint='with_user_name', methods=['PUT', 'GET'])\n@app.route('/api/', endpoint='without_user_name')\n@swag_from('path/to/external_file.yml', endpoint='with_user_name')\n@swag_from('path/to/external_file_no_user_get.yml', endpoint='without_user_name', methods=['GET'])\n@swag_from('path/to/external_file_no_user_put.yml', endpoint='without_user_name', methods=['PUT'])\ndef fromfile_decorated(username=None):\n    if not username:\n        return \"No user!\"\n    return jsonify({'username': username})\n```\n\nAnd the same can be achieved with multiple methods in a `MethodView` or `SwaggerView` by\nregistering the `url_rule` many times. Take a look at `examples/example_app`\n\n\n# Use the same data to validate your API POST body.\n\nSetting `swag_from`'s _validation_ parameter to `True` will validate incoming data automatically:\n\n```python\nfrom flasgger import swag_from\n\n@swag_from('defs.yml', validation=True)\ndef post():\n    # if not validate returns ValidationError response with status 400\n    # also returns the validation message.\n```\n\nUsing `swagger.validate` annotation is also possible:\n\n```python\nfrom flasgger import Swagger\n\nswagger = Swagger(app)\n\n@swagger.validate('UserSchema')\ndef post():\n    '''\n    file: defs.yml\n    '''\n    # if not validate returns ValidationError response with status 400\n    # also returns the validation message.\n```\n\nYet you can call `validate` manually:\n\n```python\nfrom flasgger import swag_from, validate\n\n@swag_from('defs.yml')\ndef post():\n    validate(request.json, 'UserSchema', 'defs.yml')\n    # if not validate returns ValidationError response with status 400\n    # also returns the validation message.\n```\n\nIt is also possible to define `validation=True` in `SwaggerView` and also use\n`specs_dict` for validation.\n\nTake a look at `examples/validation.py` for more information.\n\nAll validation options can be found at http://json-schema.org/latest/json-schema-validation.html\n\n### Custom validation\n\nBy default Flasgger will use [python-jsonschema](https://python-jsonschema.readthedocs.io/en/latest/)\nto perform validation.\n\nCustom validation functions are supported as long as they meet the requirements:\n - take two, and only two, positional arguments:\n    - the data to be validated as the first; and\n    - the schema to validate against as the second argument\n - raise any kind of exception when validation fails.\n\nAny return value is discarded.\n\n\nProviding the function to the Swagger instance will make it the default:\n\n```python\nfrom flasgger import Swagger\n\nswagger = Swagger(app, validation_function=my_validation_function)\n```\n\nProviding the function as parameter of `swag_from` or `swagger.validate`\nannotations or directly to the `validate` function will force it's use\nover the default validation function for Swagger:\n\n```python\nfrom flasgger import swag_from\n\n@swag_from('spec.yml', validation=True, validation_function=my_function)\n...\n```\n\n```python\nfrom flasgger import Swagger\n\nswagger = Swagger(app)\n\n@swagger.validate('Pet', validation_function=my_function)\n...\n```\n\n```python\nfrom flasgger import validate\n\n...\n\n    validate(\n        request.json, 'Pet', 'defs.yml', validation_function=my_function)\n```\n\n### Validation Error handling\n\nBy default Flasgger will handle validation errors by aborting the\nrequest with a 400 BAD REQUEST response with the error message.\n\nA custom validation error handling function can be provided to\nsupersede default behavior as long as it meets the requirements:\n - take three, and only three, positional arguments:\n    - the error raised as the first;\n    - the data which failed validation as the second; and\n    - the schema used in to validate as the third argument\n\n\nProviding the function to the Swagger instance will make it the default:\n\n```python\nfrom flasgger import Swagger\n\nswagger = Swagger(app, validation_error_handler=my_handler)\n```\n\nProviding the function as parameter of `swag_from` or `swagger.validate`\nannotations or directly to the `validate` function will force it's use\nover the default validation function for Swagger:\n\n```python\nfrom flasgger import swag_from\n\n@swag_from(\n    'spec.yml', validation=True, validation_error_handler=my_handler)\n...\n```\n\n```python\nfrom flasgger import Swagger\n\nswagger = Swagger(app)\n\n@swagger.validate('Pet', validation_error_handler=my_handler)\n...\n```\n\n```python\nfrom flasgger import validate\n\n...\n\n    validate(\n        request.json, 'Pet', 'defs.yml',\n        validation_error_handler=my_handler)\n```\n\nExamples of use of a custom validation error handler function can be\nfound at [example validation_error_handler.py](examples/validation_error_handler.py)\n\n# Get defined schemas as python dictionaries\n\nYou may wish to use schemas you defined in your Swagger specs as dictionaries\nwithout replicating the specification. For that you can use the `get_schema`\nmethod:\n\n```python\nfrom flask import Flask, jsonify\nfrom flasgger import Swagger, swag_from\n\napp = Flask(__name__)\nswagger = Swagger(app)\n\n@swagger.validate('Product')\ndef post():\n    \"\"\"\n    post endpoint\n    ---\n    tags:\n      - products\n    parameters:\n      - name: body\n        in: body\n        required: true\n        schema:\n          id: Product\n          required:\n            - name\n          properties:\n            name:\n              type: string\n              description: The product's name.\n              default: \"Guarana\"\n    responses:\n      200:\n        description: The product inserted in the database\n        schema:\n          $ref: '#/definitions/Product'\n    \"\"\"\n    rv = db.insert(request.json)\n    return jsonify(rv)\n\n...\n\nproduct_schema = swagger.get_schema('product')\n```\n\nThis method returns a dictionary which contains the Flasgger schema id,\nall defined parameters and a list of required parameters.\n\n# HTML sanitizer\n\nBy default Flasgger will try to sanitize the content in YAML definitions\nreplacing every ```\\n``` with ```<br>``` but you can change this behaviour\nsetting another kind of sanitizer.\n\n```python\nfrom flasgger import Swagger, NO_SANITIZER\n\napp =Flask()\nswagger = Swagger(app, sanitizer=NO_SANITIZER)\n```\n\nYou can write your own sanitizer\n\n```python\nswagger = Swagger(app, sanitizer=lambda text: do_anything_with(text))\n```\n\nThere is also a Markdown parser available, if you want to be able to render\nMarkdown in your specs description use **MK_SANITIZER**\n\n\n# Swagger UI and templates\n\nYou can override the `templates/flasgger/index.html` in your application and\nthis template will be the `index.html` for SwaggerUI. Use `flasgger/ui2/templates/index.html`\nas base for your customization.\n\nFlasgger supports Swagger UI versions 2 and 3, The version 3 is still experimental but you\ncan try setting `app.config['SWAGGER']['uiversion']`.\n\n```python\napp = Flask(__name__)\napp.config['SWAGGER'] = {\n    'title': 'My API',\n    'uiversion': 3\n}\nswagger = Swagger(app)\n\n```\n\n# OpenAPI 3.0 Support\n\nThere is experimental support for OpenAPI 3.0 that should work when using SwaggerUI 3. To use OpenAPI 3.0, set `app.config['SWAGGER']['openapi']` to a version that the current SwaggerUI 3 supports such as `'3.0.2'`.\n\nFor an example of this that uses `callbacks` and `requestBody`, see the [callbacks example](examples/callbacks.py).\n\n## Externally loading Swagger UI and jQuery JS/CSS\n\nStarting with Flasgger 0.9.2 you can specify external URL locations for loading the JavaScript and CSS for the Swagger and jQuery libraries loaded in the Flasgger default templates.  If the configuration properties below are omitted, Flasgger will serve static versions it includes - these versions may be older than the current Swagger UI v2 or v3 releases.\n\nThe following example loads Swagger UI and jQuery versions from unpkg.com:\n\n```\nswagger_config = Swagger.DEFAULT_CONFIG\nswagger_config['swagger_ui_bundle_js'] = '//unpkg.com/swagger-ui-dist@3/swagger-ui-bundle.js'\nswagger_config['swagger_ui_standalone_preset_js'] = '//unpkg.com/swagger-ui-dist@3/swagger-ui-standalone-preset.js'\nswagger_config['jquery_js'] = '//unpkg.com/jquery@2.2.4/dist/jquery.min.js'\nswagger_config['swagger_ui_css'] = '//unpkg.com/swagger-ui-dist@3/swagger-ui.css'\nSwagger(app, config=swagger_config)\n```\n\n# Initializing Flasgger with default data.\n\nYou can start your Swagger spec with any default data providing a template:\n\n```python\ntemplate = {\n  \"swagger\": \"2.0\",\n  \"info\": {\n    \"title\": \"My API\",\n    \"description\": \"API for my data\",\n    \"contact\": {\n      \"responsibleOrganization\": \"ME\",\n      \"responsibleDeveloper\": \"Me\",\n      \"email\": \"me@me.com\",\n      \"url\": \"www.me.com\",\n    },\n    \"termsOfService\": \"http://me.com/terms\",\n    \"version\": \"0.0.1\"\n  },\n  \"host\": \"mysite.com\",  # overrides localhost:500\n  \"basePath\": \"/api\",  # base bash for blueprint registration\n  \"schemes\": [\n    \"http\",\n    \"https\"\n  ],\n  \"operationId\": \"getmyData\"\n}\n\nswagger = Swagger(app, template=template)\n\n```\n\nAnd then the template is the default data unless some view changes it. You\ncan also provide all your specs as template and have no views. Or views in\nexternal APP.\n\n## Getting default data at runtime\n\nSometimes you need to get some data at runtime depending on dynamic values ex: you want to check `request.is_secure` to decide if `schemes` will be `https` you can do that by using `LazyString`.\n\n```py\nfrom flask import Flask\nfrom flasgger import, Swagger, LazyString, LazyJSONEncoder\n\napp = Flask(__init__)\n\n# Set the custom Encoder (Inherit it if you need to customize)\napp.json_encoder = LazyJSONEncoder\n\n\ntemplate = dict(\n    info={\n        'title': LazyString(lambda: 'Lazy Title'),\n        'version': LazyString(lambda: '99.9.9'),\n        'description': LazyString(lambda: 'Hello Lazy World'),\n        'termsOfService': LazyString(lambda: '/there_is_no_tos')\n    },\n    host=LazyString(lambda: request.host),\n    schemes=[LazyString(lambda: 'https' if request.is_secure else 'http')],\n    foo=LazyString(lambda: \"Bar\")\n)\nSwagger(app, template=template)\n\n```\n\nThe `LazyString` values will be evaluated only when `jsonify` encodes the value at runtime, so you have access to Flask `request, session, g, etc..` and also may want to access a database.\n\n## Behind a reverse proxy\n\nSometimes you're serving your swagger docs behind an reverse proxy (e.g. NGINX).  When following the [Flask guidance](http://flask.pocoo.org/snippets/35/),\nthe swagger docs will load correctly, but the \"Try it Out\" button points to the wrong place.  This can be fixed with the following code:\n\n```python\nfrom flask import Flask, request\nfrom flasgger import Swagger, LazyString, LazyJSONEncoder\n\napp = Flask(__name__)\napp.json_encoder = LazyJSONEncoder\n\ntemplate = dict(swaggerUiPrefix=LazyString(lambda : request.environ.get('HTTP_X_SCRIPT_NAME', '')))\nswagger = Swagger(app, template=template)\n\n```\n\n# Customize default configurations\n\nCustom configurations such as a different specs route or disabling Swagger UI can be provided to Flasgger:\n\n```python\nswagger_config = {\n    \"headers\": [\n    ],\n    \"specs\": [\n        {\n            \"endpoint\": 'apispec_1',\n            \"route\": '/apispec_1.json',\n            \"rule_filter\": lambda rule: True,  # all in\n            \"model_filter\": lambda tag: True,  # all in\n        }\n    ],\n    \"static_url_path\": \"/flasgger_static\",\n    # \"static_folder\": \"static\",  # must be set by user\n    \"swagger_ui\": True,\n    \"specs_route\": \"/apidocs/\"\n}\n\nswagger = Swagger(app, config=swagger_config)\n\n```\n\n## Extracting Definitions\n\nDefinitions can be extracted when `id` is found in spec, example:\n\n```python\nfrom flask import Flask, jsonify\nfrom flasgger import Swagger\n\napp = Flask(__name__)\nswagger = Swagger(app)\n\n@app.route('/colors/<palette>/')\ndef colors(palette):\n    \"\"\"Example endpoint returning a list of colors by palette\n    ---\n    parameters:\n      - name: palette\n        in: path\n        type: string\n        enum: ['all', 'rgb', 'cmyk']\n        required: true\n        default: all\n    responses:\n      200:\n        description: A list of colors (may be filtered by palette)\n        schema:\n          id: Palette\n          type: object\n          properties:\n            palette_name:\n              type: array\n              items:\n                schema:\n                  id: Color\n                  type: string\n        examples:\n          rgb: ['red', 'green', 'blue']\n    \"\"\"\n    all_colors = {\n        'cmyk': ['cyan', 'magenta', 'yellow', 'black'],\n        'rgb': ['red', 'green', 'blue']\n    }\n    if palette == 'all':\n        result = all_colors\n    else:\n        result = {palette: all_colors.get(palette)}\n\n    return jsonify(result)\n\napp.run(debug=True)\n```\n\nIn this example you do not have to pass `definitions` but need to add `id` to\nyour schemas.\n\n## Python2 Compatibility\n\nVersion `0.9.5.*` will be the last version that supports Python2. \nPlease direct discussions to [#399](https://github.com/flasgger/flasgger/issues/399). \n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 3704, "quality_score": 0.8099999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 939, "word_count": 2928}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Flasgger\n## Flask APIÁöÑEasy Swagger UI\n\n[![ÊûÑÂª∫Áä∂ÊÄÅ](https://travis-ci.com/flasgger/flasgger.svg?branch=master)](https://travis-ci.com/flasgger/flasgger)\n[![‰ª£Á†ÅÂÅ•Â∫∑](https://landscape.io/github/rochacbruno/flasgger/master/landscape.svg?style=flat)](https://landscape.io/github/rochacbruno/flasgger/master)\n[![ÊµãËØïË¶ÜÁõñÁä∂ÊÄÅ](https://coveralls.io/repos/github/rochacbruno/flasgger/badge.svg?branch=master)](https://coveralls.io/github/rochacbruno/flasgger?branch=master)\n[![PyPI](https://img.shields.io/pypi/v/flasgger.svg)](https://pypi.python.org/pypi/flasgger)\n <a target=\"_blank\" href=\"https://www.paypal.com/cgi-bin/webscr?cmd=_donations&amp;business=rochacbruno%40gmail%2ecom&amp;lc=BR&amp;item_name=Flasgger&amp;no_note=0&amp;currency_code=USD&amp;bn=PP%2dDonationsBF%3abtn_donate_SM%2egif%3aNonHostedGuest\"><img alt='ÈÄöËøáPaypalÊçêÂä©' src='http://www.paypalobjects.com/en_US/i/btn/btn_donate_SM.gif' /></a>\n\n\n![flasgger](docs/flasgger.png)\n\nFlasggerÊòØ‰∏Ä‰∏™FlaskÊâ©Â±ïÔºåÂèØ‰ªéÊÇ®ÁöÑAPI‰∏≠ÊâÄÊúâÂ∑≤Ê≥®ÂÜåÁöÑFlaskËßÜÂõæ‰∏≠**ÊèêÂèñ[OpenAPI-Specification](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#operation-object)**Ôºà‰ª•‰∏ãÁÆÄÁß∞\"spec\"Ôºâ„ÄÇ\n\nFlasggerËøò**ÈõÜÊàê [SwaggerUI](http://swagger.io/swagger-ui/)**ÔºåÂõ†Ê≠§ÊÇ®ÂèØ‰ª•ËÆøÈóÆ[http://localhost:5000/apidocs](localhost:5000/apidocs)Âπ∂ÂèØËßÜÂåñÂπ∂‰∏éÊÇ®ÁöÑAPIËµÑÊ∫êËøõË°å‰∫§‰∫í„ÄÇ\n\nFlasggerËøò‰ΩøÁî®‰∏éÂèØ‰ª•È™åËØÅ‰ª•POSTÔºåPUTÔºåPATCHÂΩ¢ÂºèÊé•Êî∂ÁöÑÊï∞ÊçÆÊòØÂê¶‰∏é**YAML**Ôºå**PythonÂ≠óÂÖ∏**Ôºå**Marshmallow Schema**ÊâÄÂÆö‰πâÁöÑspec‰∏ÄËá¥„ÄÇ\n\nFlasggerÂèØ‰ª•‰ΩøÁî®ÁÆÄÂçïÁöÑÂáΩÊï∞ËßÜÂõæÊàñÊñπÊ≥ïËßÜÂõæÔºà‰ΩøÁî®docstring‰Ωú‰∏∫ËßÑËåÉÔºâÔºåÊàñ‰ΩøÁî®@swag_fromË£ÖÈ•∞Âô®‰ªé**YAML**Êàñ**dict**Ëé∑ÂèñspecÔºåËøòÊèê‰æõÂèØ‰ª•‰ΩøÁî®**SwaggerView**Ë∞ÉÁî®**Marshmallow Schemas**‰Ωú‰∏∫spec„ÄÇ\n\nFlasgger‰∏é`Flask-RESTful`ÂÖºÂÆπÔºåÂõ†Ê≠§ÊÇ®ÂèØ‰ª•ÂêåÊó∂‰ΩøÁî®`Resources`Âíå`swag` specÔºåÁúãÁúã[restfulÁ§∫‰æã](examples/restful.py)„ÄÇ\n\nFlasggerËøòÊîØÊåÅÂ∞Ü`Marshmallow APISpec`‰Ωú‰∏∫ËßÑËåÉÁöÑspecÊ®°ÊùøÔºåÂ¶ÇÊûúÊÇ®‰ΩøÁî®ÁöÑÊòØMarshmallowÁöÑAPISPecÔºåËØ∑Êü•Áúã[apispecÁ§∫‰æã](examples/apispec_example.py)„ÄÇ\n\n# È´òÂ∫¶ÂèÇ‰∏éÁöÑË¥°ÁåÆËÄÖ\n\n[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/0)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/0)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/1)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/1)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/2)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/2)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/3)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/3)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/4)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/4)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/5)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/5)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/6)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/6)[![](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/images/7)](https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/7)\n\n# Á§∫‰æãÂíåÊºîÁ§∫Â∫îÁî®\n\nÊúâ‰∏Ä‰∫õ[Á§∫‰æãÂ∫îÁî®Á®ãÂ∫è](examples/) ÔºåÊÇ®‰πüÂèØ‰ª•Âú®[FlasggerÊºîÁ§∫Â∫îÁî®Á®ãÂ∫è](http://flasgger.pythonanywhere.com/)‰∏≠‰ΩøÁî®Á§∫‰æã„ÄÇ\n\n> Ê≥®ÊÑèÔºöÊâÄÊúâÁ§∫‰æãÂ∫îÁî®Á®ãÂ∫è‰πüÊòØÊµãËØïÁî®‰æãÔºåÂπ∂Âú®Travis CI‰∏≠Ëá™Âä®ËøêË°å‰ª•Á°Æ‰øùË¥®ÈáèÂíåÊµãËØïË¶ÜÁõñËåÉÂõ¥„ÄÇ\n\n## Docker\n\nÁ§∫‰æãÂíåÊºîÁ§∫Â∫îÁî®Á®ãÂ∫è‰πüÂèØ‰ª•‰Ωú‰∏∫DockerÊò†ÂÉè/ÂÆπÂô®ÊûÑÂª∫ÂíåËøêË°åÔºö\n\n```\ndocker build -t flasgger .\ndocker run -it --rm -p 5000:5000 --name flasgger flasgger\n```\nÁÑ∂ÂêéËÆøÈóÆ‰Ωç‰∫é http://localhost:5000 ÁöÑFlasggerÊºîÁ§∫Â∫îÁî®Á®ãÂ∫è„ÄÇ\n\n# ÂÆâË£Ö\n\n> Âú®ÊÇ®ÁöÑvirtualenv‰∏ãÊâßË°å‰ª•‰∏ãÊìç‰ΩúÔºö\n\nÁ°Æ‰øùÊÇ®Êã•ÊúâÊúÄÊñ∞ÁöÑsetuptools\n```\npip install -U setuptools\n```\n\nÁÑ∂Âêé\n\n```\npip install flasgger\n```\n\nÊàñÔºàÂºÄÂèëÁâàÊú¨Ôºâ\n\n```\npip install https://github.com/flasgger/flasgger/tarball/master\n```\n\n> Ê≥®ÊÑèÔºöÂ¶ÇÊûúË¶Å‰ΩøÁî®**Marshmallow Schemas**ÔºåÂàôËøòÈúÄË¶ÅËøêË°å`pip install marshmallow apispec`„ÄÇ\n\n# ÂÖ•Èó®\n\n## ‰ΩøÁî®ÊñáÊ°£Â≠óÁ¨¶‰∏≤‰Ωú‰∏∫spec\n\n\nÂàõÂª∫‰∏Ä‰∏™Âêç‰∏∫`colors.py`ÁöÑÊñá‰ª∂\n\n```python\nfrom flask import Flask, jsonify\nfrom flasgger import Swagger\n\napp = Flask(__name__)\nswagger = Swagger(app)\n\n@app.route('/colors/<palette>/')\ndef colors(palette):\n    \"\"\"Á§∫‰æãÁ´ØÁÇπÊåâË∞ÉËâ≤ÊùøËøîÂõûÈ¢úËâ≤ÂàóË°®\n    ËøôÊòØ‰ΩøÁî®ÊñáÊ°£Â≠óÁ¨¶‰∏≤‰Ωú‰∏∫spec„ÄÇ\n    ---\n    parameters:\n      - name: palette\n        in: path\n        type: string\n        enum: ['all', 'rgb', 'cmyk']\n        required: true\n        default: all\n    definitions:\n      Palette:\n        type: object\n        properties:\n          palette_name:\n            type: array\n            items:\n              $ref: '#/definitions/Color'\n      Color:\n        type: string\n    responses:\n      200:\n        description: È¢úËâ≤ÂàóË°®ÔºàÂèØË¢´Ë∞ÉËâ≤ÊùøËøáÊª§Ôºâ\n        schema:\n          $ref: '#/definitions/Palette'\n        examples:\n          rgb: ['red', 'green', 'blue']\n    \"\"\"\n    all_colors = {\n        'cmyk': ['cyan', 'magenta', 'yellow', 'black'],\n        'rgb': ['red', 'green', 'blue']\n    }\n    if palette == 'all':\n        result = all_colors\n    else:\n        result = {palette: all_colors.get(palette)}\n\n    return jsonify(result)\n\napp.run(debug=True)\n```\n\nÁé∞Âú®ËøêË°åÔºö\n\n```\npython colors.py\n```\n\nÂπ∂ËΩ¨Âà∞Ôºö[http://localhost:5000/apidocs/](http://localhost:5000/apidocs/)\n\nÊÇ®Â∫îËØ•ÂæóÂà∞Ôºö\n\n![colors](docs/colors.png)\n\n## ‰ΩøÁî®Â§ñÈÉ®YAMLÊñá‰ª∂\n\n‰øùÂ≠ò‰∏Ä‰∏™Êñ∞Êñá‰ª∂`colors.yml`\n\n```yaml\nÁ§∫‰æãÁ´ØÁÇπÊåâË∞ÉËâ≤ÊùøËøîÂõûÈ¢úËâ≤ÂàóË°®\nÂú®Ê≠§Á§∫‰æã‰∏≠ÔºåËßÑËåÉÂèñËá™Â§ñÈÉ®YAMLÊñá‰ª∂\n---\nparameters:\n  - name: palette\n    in: path\n    type: string\n    enum: ['all', 'rgb', 'cmyk']\n    required: true\n    default: all\ndefinitions:\n  Palette:\n    type: object\n    properties:\n      palette_name:\n        type: array\n        items:\n          $ref: '#/definitions/Color'\n  Color:\n    type: string\nresponses:\n  200:\n    description: È¢úËâ≤ÂàóË°®ÔºàÂèØË¢´Ë∞ÉËâ≤ÊùøËøáÊª§Ôºâ\n    schema:\n      $ref: '#/definitions/Palette'\n    examples:\n      rgb: ['red', 'green', 'blue']\n```\n\n\nËÆ©Êàë‰ª¨‰ΩøÁî®Áõ∏ÂêåÁöÑÁ§∫‰æãÔºå‰ªÖÊõ¥ÊîπËßÜÂõæÂäüËÉΩ„ÄÇ\n\n```python\nfrom flasgger import swag_from\n\n@app.route('/colors/<palette>/')\n@swag_from('colors.yml')\ndef colors(palette):\n    ...\n```\n\nÂ¶ÇÊûúÊÇ®‰∏çÊÉ≥‰ΩøÁî®Ë£ÖÈ•∞Âô®ÔºåÂàôÂèØ‰ª•‰ΩøÁî®docstring`file:`Âø´Êç∑ÊñπÂºè„ÄÇ\n\n```python\n@app.route('/colors/<palette>/')\ndef colors(palette):\n    \"\"\"\n    file: colors.yml\n    \"\"\"\n    ...\n```\n\n\n## ‰ΩøÁî®Â≠óÂÖ∏‰Ωú‰∏∫raw spec\n\nÂàõÂª∫‰∏Ä‰∏™PythonÂ≠óÂÖ∏‰∏∫Ôºö\n\n```python\nspecs_dict = {\n  \"parameters\": [\n    {\n      \"name\": \"palette\",\n      \"in\": \"path\",\n      \"type\": \"string\",\n      \"enum\": [\n        \"all\",\n        \"rgb\",\n        \"cmyk\"\n      ],\n      \"required\": \"true\",\n      \"default\": \"all\"\n    }\n  ],\n  \"definitions\": {\n    \"Palette\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"palette_name\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"$ref\": \"#/definitions/Color\"\n          }\n        }\n      }\n    },\n    \"Color\": {\n      \"type\": \"string\"\n    }\n  },\n  \"responses\": {\n    \"200\": {\n      \"description\": \"È¢úËâ≤ÂàóË°®ÔºàÂèØË¢´Ë∞ÉËâ≤ÊùøËøáÊª§Ôºâ\",\n      \"schema\": {\n        \"$ref\": \"#/definitions/Palette\"\n      },\n      \"examples\": {\n        \"rgb\": [\n          \"red\",\n          \"green\",\n          \"blue\"\n        ]\n      }\n    }\n  }\n}\n```\n\nÁé∞Âú®‰∏∫Âêå‰∏Ä‰∏™ÂáΩÊï∞‰ΩøÁî®dictÊõø‰ª£YAMLÊñá‰ª∂„ÄÇ\n\n```python\n@app.route('/colors/<palette>/')\n@swag_from(specs_dict)\ndef colors(palette):\n    \"\"\"Example endpoint returning a list of colors by palette\n    In this example the specification is taken from specs_dict\n    \"\"\"\n    ...\n```\n\n## ‰ΩøÁî® Marshmallow Schemas\n\n> È¶ñÂÖàÔºö `pip install marshmallow apispec`\n\n```python\nfrom flask import Flask, jsonify\nfrom flasgger import Swagger, SwaggerView, Schema, fields\n\n\nclass Color(Schema):\n    name = fields.Str()\n\nclass Palette(Schema):\n    pallete_name = fields.Str()\n    colors = fields.Nested(Color, many=True)\n\nclass PaletteView(SwaggerView):\n    parameters = [\n        {\n            \"name\": \"palette\",\n            \"in\": \"path\",\n            \"type\": \"string\",\n            \"enum\": [\"all\", \"rgb\", \"cmyk\"],\n            \"required\": True,\n            \"default\": \"all\"\n        }\n    ]\n    responses = {\n        200: {\n            \"description\": \"È¢úËâ≤ÂàóË°®ÔºàÂèØË¢´Ë∞ÉËâ≤ÊùøËøáÊª§Ôºâ\",\n            \"schema\": Palette\n        }\n    }\n\n    def get(self, palette):\n        \"\"\"\n        Colors API using schema\n        This example is using marshmallow schemas\n        \"\"\"\n        all_colors = {\n            'cmyk': ['cyan', 'magenta', 'yellow', 'black'],\n            'rgb': ['red', 'green', 'blue']\n        }\n        if palette == 'all':\n            result = all_colors\n        else:\n            result = {palette: all_colors.get(palette)}\n        return jsonify(result)\n\napp = Flask(__name__)\nswagger = Swagger(app)\n\napp.add_url_rule(\n    '/colors/<palette>',\n    view_func=PaletteView.as_view('colors'),\n    methods=['GET']\n)\n\napp.run(debug=True)\n\n```\n\n> Ê≥®ÊÑèÔºöËØ∑Êü•Áúã`examples/validation.py`Ôºå‰ª•Ëé∑ÂæóÊõ¥ÂÆåÊï¥ÁöÑÁ§∫‰æã„ÄÇ\n\n\n> Ê≥®ÊÑèÔºöÂú®Ë∑ØÂæÑËßÑÂàô(path rule)‰∏≠ÊçïËé∑ÂèÇÊï∞Êó∂ÔºåËØ∑ÂßãÁªà‰ΩøÁî®ÊòæÂºèÁ±ªÂûãÔºå‰∏çÂèØ‰ª•Ôºö``/api/<username>`` ÂèØ‰ª•Ôºö``/api/<string:username>``\n\n\n## ‰ΩøÁî® **Flask RESTful** ËµÑÊ∫ê ÔºàResourcesÔºâ\n\nFlasgger‰∏éFlask-RESTfulÂÖºÂÆπÔºåÊÇ®Âè™ÈúÄË¶ÅÂÆâË£Ö`pip install flask-restful`ÔºåÁÑ∂ÂêéÔºö\n\n```python\n\nfrom flask import Flask\nfrom flasgger import Swagger\nfrom flask_restful import Api, Resource\n\napp = Flask(__name__)\napi = Api(app)\nswagger = Swagger(app)\n\nclass Username(Resource):\n    def get(self, username):\n       \"\"\"\n       Êú¨Á§∫‰æã‰ΩøÁî®FlaskRESTfulËµÑÊ∫ê\n       ÂÆÉ‰πüÈÄÇÁî®‰∫éswag_fromÔºåschemaÂíåspec_dict\n       ---\n       parameters:\n         - in: path\n           name: username\n           type: string\n           required: true\n       responses:\n         200:\n           description: Âçï‰∏™Áî®Êà∑È°π\n           schema:\n             id: User\n             properties:\n               username:\n                 type: string\n                 description: Áî®Êà∑Âêç\n                 default: ÂåøÂêçÁî®Êà∑\n        \"\"\"\n        return {'username': username}, 200\n\n\napi.add_resource(Username, '/username/<username>')\n\napp.run(debug=True)\n\n```\n\n## Ëá™Âä®Ëß£ÊûêÂ§ñÈÉ®YAMLÊñáÊ°£Âíå `MethodView`s\n\nÂèØ‰ª•Â∞ÜFlasggerÈÖçÁΩÆ‰∏∫Ëá™Âä®Ëß£ÊûêÂ§ñÈÉ®YAML APIÊñáÊ°£„ÄÇÂú®ÊÇ®ÁöÑ `app.config['SWAGGER']`‰∏≠ËÆæÁΩÆ[Set a `doc_dir`](https://github.com/rochacbruno/flasgger/blob/aaef05c17cc559d01b7436211093463642eb6ae2/examples/parsed_view_func.py#L16)ÔºåÁÑ∂ÂêéSwaggerÂ∞ÜÂä†ËΩΩAPIÊñáÊ°£ÈÄöËøáÂú®doc_dir‰∏≠Êü•ÊâæÁî±Á´ØÁÇπÂêçÁß∞ÂíåÂáΩÊï∞ÂêçÁß∞Â≠òÂÇ®ÁöÑYAMLÊñá‰ª∂„ÄÇ‰æãÂ¶ÇÔºå`'doc_dir': './examples/docs/'`ÂíåÊñá‰ª∂`./examples/docs/items/get.yml`Â∞Ü‰∏∫`ItemsView`ÊñπÊ≥ï`get`Êèê‰æõSwaggerÊñáÊ°£„ÄÇ\n\nÂè¶Â§ñÔºåÂΩìÂ¶Ç‰∏ä‰ΩøÁî®**Flask RESTful**Êó∂ÔºåÈÄöËøáÂú®ÊûÑÈÄ†`Swagger`Êó∂‰º†ÈÄí`parse=True`ÔºåFlasggerÂ∞Ü‰ΩøÁî®`flask_restful.reqparse.RequestParser`ÔºåÊâæÂà∞ÊâÄÊúâ`MethodView`sÔºåÁÑ∂ÂêéÂ∞ÜËß£ÊûêÂíåÈ™åËØÅÁöÑÊï∞ÊçÆÂ≠òÂÇ®Âú®flask.request.parsed_data‰∏≠„ÄÇ\n\n## ‰∏∫Âçï‰∏™ÂáΩÊï∞Â§ÑÁêÜÂ§ö‰∏™httpÊñπÊ≥ïÂíåË∑ØÁî±\n\nÊÇ®ÂèØ‰ª•ÊåâÁ´ØÁÇπÊàñÂáΩÊï∞ÂàÜÈöîspec\n\n```python\nfrom flasgger.utils import swag_from\n\n@app.route('/api/<string:username>', endpoint='with_user_name', methods=['PUT', 'GET'])\n@app.route('/api/', endpoint='without_user_name')\n@swag_from('path/to/external_file.yml', endpoint='with_user_name')\n@swag_from('path/to/external_file_no_user_get.yml', endpoint='without_user_name', methods=['GET'])\n@swag_from('path/to/external_file_no_user_put.yml', endpoint='without_user_name', methods=['PUT'])\ndef fromfile_decorated(username=None):\n    if not username:\n        return \"No user!\"\n    return jsonify({'username': username})\n```\n\nÂêåÊ†∑ÔºåÂèØ‰ª•ÈÄöËøáÂ§öÊ¨°Ê≥®ÂÜåurl_ruleÂÆûÁé∞Âú®Âêå‰∏Ä‰∏™`MethodView`Êàñ`SwaggerView`‰∏≠‰ΩøÁî®Â§ö‰∏™ÂáΩÊï∞„ÄÇ`examples/example_app`\n\n\n# ‰ΩøÁî®Áõ∏ÂêåÁöÑÊï∞ÊçÆÊù•È™åËØÅAPI POST‰∏ª‰Ωì(body)„ÄÇ\n\nÂ∞Ü`swag_from`ÁöÑ`_validation_`ÂèÇÊï∞ËÆæÁΩÆ‰∏∫`True`‰ºöËá™Âä®È™åËØÅ‰º†ÂÖ•ÁöÑÊï∞ÊçÆÔºö\n\n```python\nfrom flasgger import swag_from\n\n@swag_from('defs.yml', validation=True)\ndef post():\n    # Â¶ÇÊûúÊú™ÈÄöËøáÈ™åËØÅÔºåÂàôËøîÂõûÁä∂ÊÄÅ‰∏∫400ÁöÑValidationErrorÂìçÂ∫îÂπ∂ËøòËøîÂõûÈ™åËØÅÊ∂àÊÅØ„ÄÇ\n```\n\n‰πüÂèØ‰ª•‰ΩøÁî®`swagger.validate`Ê≥®ÈáäÔºö\n\n```python\nfrom flasgger import Swagger\n\nswagger = Swagger(app)\n\n@swagger.validate('UserSchema')\ndef post():\n    '''\n    file: defs.yml\n    '''\n    # Â¶ÇÊûúÊú™ÈÄöËøáÈ™åËØÅÔºåÂàôËøîÂõûÁä∂ÊÄÅ‰∏∫400ÁöÑValidationErrorÂìçÂ∫îÂπ∂ËøòËøîÂõûÈ™åËØÅÊ∂àÊÅØ„ÄÇ\n```\n\nYet you can call `validate` manually:\n\n```python\nfrom flasgger import swag_from, validate\n\n@swag_from('defs.yml')\ndef post():\n    validate(request.json, 'UserSchema', 'defs.yml')\n    # Â¶ÇÊûúÊú™ÈÄöËøáÈ™åËØÅÔºåÂàôËøîÂõûÁä∂ÊÄÅ‰∏∫400ÁöÑValidationErrorÂìçÂ∫îÂπ∂ËøòËøîÂõûÈ™åËØÅÊ∂àÊÅØ„ÄÇ\n```\n\n‰πüÂèØ‰ª•Âú®`SwaggerView`‰∏≠ÂÆö‰πâ`validation=True`Âπ∂‰ΩøÁî®\n`specs_dict`Áî®‰∫éÈ™åËØÅ„ÄÇ\n\nËØ∑Êü•Áúã`examples/validation.py`‰∫ÜËß£Êõ¥Â§ö‰ø°ÊÅØ„ÄÇ\n\nÊâÄÊúâÈ™åËØÅÈÄâÈ°πÂùáÂèØÂú® http://json-schema.org/latest/json-schema-validation.html ÊâæÂà∞\n\n### Ëá™ÂÆö‰πâÈ™åËØÅ\n\nÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåFlasggerÂ∞Ü‰ΩøÁî®[python-jsonschema](https://python-jsonschema.readthedocs.io/en/latest/)\nÊâßË°åÈ™åËØÅ„ÄÇ\n\nÂè™Ë¶ÅÊª°Ë∂≥Ë¶ÅÊ±ÇÔºåÂ∞±ÊîØÊåÅËá™ÂÆö‰πâÈ™åËØÅÂäüËÉΩÔºö\n - ‰ªÖÊé•Âèó‰∏§‰∏™‰ΩçÁΩÆÂèÇÊï∞Ôºö\n    - È¶ñÂÖàË¶ÅÈ™åËØÅÁöÑÊï∞ÊçÆÔºõÂíå\n    - ‰Ωú‰∏∫Á¨¨‰∫å‰∏™ÂèÇÊï∞È™åËØÅÁöÑschema \n - È™åËØÅÂ§±Ë¥•Êó∂ÂºïÂèë‰ªª‰ΩïÂΩ¢ÂºèÁöÑÂºÇÂ∏∏„ÄÇ\n\n‰ªª‰ΩïËøîÂõûÂÄºÈÉΩÂ∞ÜË¢´‰∏¢ÂºÉ„ÄÇ\n\n\nÂêëSwaggerÂÆû‰æãÊèê‰æõÂáΩÊï∞Â∞Ü‰ΩøÂÖ∂Êàê‰∏∫ÈªòËÆ§ÂÄºÔºö\n\n```python\nfrom flasgger import Swagger\n\nswagger = Swagger(app, validation_function=my_validation_function)\n```\n\nÊèê‰æõÂáΩÊï∞‰Ωú‰∏∫`swag_from`Êàñ`swagger.validate`ÁöÑÂèÇÊï∞\nÊâπÊ≥®ÊàñÁõ¥Êé•Ê∑ªÂä†Âà∞`validate`ÂáΩÊï∞Â∞ÜÂº∫Âà∂‰ΩøÁî®ÂÆÉ\nËÄå‰∏çÊòØSwaggerÁöÑÈªòËÆ§È™åËØÅÂäüËÉΩÔºö\n```python\nfrom flasgger import swag_from\n\n@swag_from('spec.yml', validation=True, validation_function=my_function)\n...\n```\n\n```python\nfrom flasgger import Swagger\n\nswagger = Swagger(app)\n\n@swagger.validate('Pet', validation_function=my_function)\n...\n```\n\n```python\nfrom flasgger import validate\n\n...\n\n    validate(\n        request.json, 'Pet', 'defs.yml', validation_function=my_function)\n```\n\n### È™åËØÅÈîôËØØÂ§ÑÁêÜ\n\nÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåFlasggerÂ§ÑÁêÜÈ™åËØÅÈîôËØØÁöÑÊñπÂºèÊòØ‰∏≠Ê≠¢ËØ∑Ê±Ç(abort)Âπ∂ËøîÂõû\nÂ∏¶ÊúâÈîôËØØÊ∂àÊÅØÁöÑ400 BAD REQUESTÂìçÂ∫î„ÄÇ\n\nÂèØ‰ª•Êèê‰æõËá™ÂÆö‰πâÈ™åËØÅÈîôËØØÂ§ÑÁêÜÂáΩÊï∞(custom validation error handling function) \nÂè™Ë¶ÅÁ¨¶ÂêàË¶ÅÊ±ÇÔºåÂ∞±‰ºöÂèñ‰ª£ÈªòËÆ§Ë°å‰∏∫Ôºö\n - Êé•Âèó‰∏â‰∏™‰∏î‰ªÖ‰∏â‰∏™‰ΩçÁΩÆÂèÇÊï∞Ôºö\n    - ÊäõÂá∫ÁöÑerror \n    - ÈÄ†ÊàêÈ™åËØÅÂ§±Ë¥•ÁöÑÊï∞ÊçÆÔºõÂíå\n    - Áî®‰∫éÈ™åËØÅÁöÑschema \n\n\nÂêëSwaggerÂÆû‰æãÊèê‰æõÂáΩÊï∞Â∞Ü‰ΩøÂÖ∂Êàê‰∏∫ÈªòËÆ§ÂÄºÔºö\n\n```python\nfrom flasgger import Swagger\n\nswagger = Swagger(app, validation_error_handler=my_handler)\n```\nÊèê‰æõËøô‰∏™ÂáΩÊï∞‰Ωú‰∏∫`swag_from`Êàñ`swagger.validate`ÊâπÊ≥®ÁöÑÂèÇÊï∞\nÊàñÁõ¥Êé•Ê∑ªÂä†Âà∞`validate`ÂáΩÊï∞ÈÉΩÂ∞ÜÂº∫Âà∂‰ΩøÁî®ÂÆÉ\nËÄå‰∏çÊòØSwaggerÁöÑÈªòËÆ§È™åËØÅÂäüËÉΩÔºö\n```python\nfrom flasgger import swag_from\n\n@swag_from(\n    'spec.yml', validation=True, validation_error_handler=my_handler)\n...\n```\n\n```python\nfrom flasgger import Swagger\n\nswagger = Swagger(app)\n\n@swagger.validate('Pet', validation_error_handler=my_handler)\n...\n```\n\n```python\nfrom flasgger import validate\n\n...\n\n    validate(\n        request.json, 'Pet', 'defs.yml',\n        validation_error_handler=my_handler)\n```\n\n‰ΩøÁî®Ëá™ÂÆö‰πâÈ™åËØÅÈîôËØØÂ§ÑÁêÜÂáΩÊï∞ÁöÑÂ∫îÁî®Á§∫‰æãÔºö\nËßÅ [example validation_error_handler.py](examples/validation_error_handler.py)\n\n# Ëé∑ÂèñÂ∑≤ÂÆö‰πâÁöÑschema‰Ωú‰∏∫pythonËØç\n\nÊÇ®ÂèØËÉΩÂ∏åÊúõ‰∏çÂ§çÂà∂specËÄåÂ∞ÜÂú®Swagger spec‰∏≠ÂÆö‰πâschemasÁî®‰ΩúÂ≠óÂÖ∏„ÄÇ\n‰∏∫Ê≠§ÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî®`get_schema`\nÂáΩÊï∞Ôºö\n\n```python\nfrom flask import Flask, jsonify\nfrom flasgger import Swagger, swag_from\n\napp = Flask(__name__)\nswagger = Swagger(app)\n\n@swagger.validate('Product')\ndef post():\n    \"\"\"\n    post endpoint\n    ---\n    tags:\n      - products\n    parameters:\n      - name: body\n        in: body\n        required: true\n        schema:\n          id: Product\n          required:\n            - name\n          properties:\n            name:\n              type: string\n              description: The product's name.\n              default: \"Guarana\"\n    responses:\n      200:\n        description: The product inserted in the database\n        schema:\n          $ref: '#/definitions/Product'\n    \"\"\"\n    rv = db.insert(request.json)\n    return jsonify(rv)\n\n...\n\nproduct_schema = swagger.get_schema('product')\n```\n\nÊ≠§ÊñπÊ≥ïËøîÂõûÂåÖÂê´Flasgger schema idÁöÑÂ≠óÂÖ∏Ôºå\nÊâÄÊúâÂÆö‰πâÁöÑÂèÇÊï∞ÂíåÊâÄÈúÄÂèÇÊï∞ÁöÑÂàóË°®„ÄÇ\n\n# HTMLÊ∏ÖÁêÜÂô®(HTML sanitizer)\n\nÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåFlasggerÂ∞ÜÂ∞ùËØïÊ∏ÖÁêÜYAMLÂÆö‰πâ‰∏≠ÁöÑÂÜÖÂÆπ\nÁî®```<br>```ÊõøÊç¢ÊØè‰∏™`\\n`Ôºå‰ΩÜÊòØÊÇ®ÂèØ‰ª•Êõ¥ÊîπÊ≠§Ë°å‰∏∫\nËÆæÁΩÆÂè¶‰∏ÄÁßçÊ∏ÖÁêÜÂô®„ÄÇ\n\n```python\nfrom flasgger import Swagger, NO_SANITIZER\n\napp =Flask()\nswagger = Swagger(app, sanitizer=NO_SANITIZER)\n```\n\nÊÇ®ÂèØ‰ª•Ëá™Ë°åÁºñÂÜôÊ∏ÖÁêÜÂô®\n\n```python\nswagger = Swagger(app, sanitizer=lambda text: do_anything_with(text))\n```\n\nÂ¶ÇÊûúÊÇ®Â∏åÊúõËÉΩÂ§üÊ∏≤ÊüìÊÇ®ÁöÑspecs description‰∏≠ÁöÑMarkdownÔºåËøòÂèØ‰ª•‰ΩøÁî®MarkdownËß£ÊûêÂô®**MK_SANITIZER**\n\n\n# Swagger UIÂíåÊ®°Êùø\n\nÊÇ®ÂèØ‰ª•Âú®Â∫îÁî®Á®ãÂ∫è‰∏≠ÈáçÂÜô(override) `templates/flasgger/index.html`Ôºå\nÈáçÂÜôÂêéÁöÑÊ®°ÊùøÂ∞Ü‰∏∫SwaggerUIÁöÑ`index.html`„ÄÇ‰ΩøÁî®`flasgger/ui2/templates/index.html`\n‰Ωú‰∏∫ÊÇ®Ëá™ÂÆö‰πâ`index.html`Êñá‰ª∂ÁöÑÂü∫Á°Ä„ÄÇ\n\nFlasggerÊîØÊåÅSwagger UIÁâàÊú¨2Âíå3„ÄÇÁâàÊú¨3‰ªçÂ§Ñ‰∫éËØïÈ™åÈò∂ÊÆµÔºå‰ΩÜÊòØÊÇ®\nÂèØ‰ª•Â∞ùËØïËÆæÁΩÆ`app.config['SWAGGER']['uiversion']`.\n\n```python\napp = Flask(__name__)\napp.config['SWAGGER'] = {\n    'title': 'My API',\n    'uiversion': 3\n}\nswagger = Swagger(app)\n\n```\n\n# OpenAPI 3.0ÊîØÊåÅ\n\nÂØπOpenAPI 3.0ÁöÑÂÆûÈ™åÊÄßÊîØÊåÅÂ∫îËØ•Âú®‰ΩøÁî®SwaggerUI 3Êó∂Ëµ∑‰ΩúÁî®„ÄÇË¶Å‰ΩøÁî®OpenAPI 3.0ÔºåËØ∑Â∞Ü`app.config['SWAGGER']['openapi']` ËÆæÁΩÆ‰∏∫ÂΩìÂâçSwaggerUI 3ÊîØÊåÅÁöÑÁâàÊú¨Ôºå‰æãÂ¶Ç`'3.0.2'`„ÄÇ\n\nÊúâÂÖ≥‰ΩøÁî®`callbacks`Âíå`requestBody`ÁöÑÁ§∫‰æãÔºåËØ∑ÂèÇËßÅ[callbacks example](examples/callbacks.py)„ÄÇ\n\n## ‰ªéÂ§ñÈÉ®Âä†ËΩΩSwagger UIÂíåjQuery JS/CSS\n\n‰ªéFlasgger 0.9.2ÂºÄÂßãÔºåÊÇ®ÂèØ‰ª•ÊåáÂÆöÂ§ñÈÉ®URL‰ΩçÁΩÆÔºå‰ª•‰∏∫FlasggerÈªòËÆ§Ê®°Êùø‰∏≠Âä†ËΩΩÁöÑSwaggerÂíåjQueryÂ∫ìÂä†ËΩΩJavaScriptÂíåCSS„ÄÇÂ¶ÇÊûúÁúÅÁï•‰ª•‰∏ãÈÖçÁΩÆÂ±ûÊÄßÔºåÂàôFlasggerÂ∞ÜÊèê‰æõÂÆÉÊâÄÂåÖÂê´ÁöÑÈùôÊÄÅÁâàÊú¨-Ëøô‰∫õÁâàÊú¨ÂèØËÉΩÊØîÂΩìÂâçÁöÑSwagger UI v2Êàñv3ÁâàÊú¨Ë¶ÅÊóß„ÄÇ\n\n‰ª•‰∏ãÁ§∫‰æã‰ªéunpkg.comÂä†ËΩΩSwagger UIÂíåjQueryÁâàÊú¨Ôºö\n\n```\nswagger_config = Swagger.DEFAULT_CONFIG\nswagger_config['swagger_ui_bundle_js'] = '//unpkg.com/swagger-ui-dist@3/swagger-ui-bundle.js'\nswagger_config['swagger_ui_standalone_preset_js'] = '//unpkg.com/swagger-ui-dist@3/swagger-ui-standalone-preset.js'\nswagger_config['jquery_js'] = '//unpkg.com/jquery@2.2.4/dist/jquery.min.js'\nswagger_config['swagger_ui_css'] = '//unpkg.com/swagger-ui-dist@3/swagger-ui.css'\nSwagger(app, config=swagger_config)\n```\n\n# Initializing Flasgger with default data.\n\nÊÇ®ÂèØ‰ª•‰ΩøÁî®ÈªòËÆ§Êï∞ÊçÆÊù•ÂêØÂä®Swagger specÊàñÊèê‰æõÊ®°ÊùøÔºö\n\n```python\ntemplate = {\n  \"swagger\": \"2.0\",\n  \"info\": {\n    \"title\": \"My API\",\n    \"description\": \"API for my data\",\n    \"contact\": {\n      \"responsibleOrganization\": \"ME\",\n      \"responsibleDeveloper\": \"Me\",\n      \"email\": \"me@me.com\",\n      \"url\": \"www.me.com\",\n    },\n    \"termsOfService\": \"http://me.com/terms\",\n    \"version\": \"0.0.1\"\n  },\n  \"host\": \"mysite.com\",  # overrides localhost:500\n  \"basePath\": \"/api\",  # base bash for blueprint registration\n  \"schemes\": [\n    \"http\",\n    \"https\"\n  ],\n  \"operationId\": \"getmyData\"\n}\n\nswagger = Swagger(app, template=template)\n\n```\n\nAnd then the template is the default data unless some view changes it. You\ncan also provide all your specs as template and have no views. Or views in\nexternal APP.\n\nÁÑ∂ÂêéÔºåÈô§ÈùûÊüê‰∫õËßÜÂõæÊõ¥Êîπ‰∫ÜÊ®°ÊùøÔºåÂê¶ÂàôÊ®°ÊùøÊòØÈªòËÆ§Êï∞ÊçÆ„ÄÇÊÇ®\n‰πüÂèØ‰ª•Êèê‰æõÊâÄÊúâspec‰Ωú‰∏∫Ê®°ÊùøÔºåËÄåÊó†ÈúÄÂåÖÊã¨ËßÜÂõæ„ÄÇÊàñ\nÂ§ñÈÉ®APP‰∏≠ÁöÑËßÜÂõæ„ÄÇ\n\n## #Âú®ËøêË°åÊó∂Ëé∑ÂèñÈªòËÆ§Êï∞ÊçÆ\n\nÊúâÊó∂ÊÇ®ÈúÄË¶ÅÂú®ËøêË°åÊó∂Ëé∑Âèñ‰∏Ä‰∫õÊï∞ÊçÆÔºåÂÖ∑‰ΩìÂèñÂÜ≥‰∫éÂä®ÊÄÅÂÄºÔºå‰æãÂ¶ÇÔºöË¶ÅÊ£ÄÊü•`request.is_secure`‰ª•Á°ÆÂÆö`schemes`ÊòØÂê¶‰∏∫`https`ÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî®`LazyString`ÂÅöÂà∞Ëøô‰∏ÄÁÇπ„ÄÇ\n\n```py\nfrom flask import Flask\nfrom flasgger import, Swagger, LazyString, LazyJSONEncoder\n\napp = Flask(__init__)\n\n# ËÆæÁΩÆËá™ÂÆö‰πâÁºñÁ†ÅÂô®ÔºàÂ¶ÇÊûúÈúÄË¶ÅËá™ÂÆö‰πâÔºåÂàôÁªßÊâøÂÆÉÔºâ\napp.json_encoder = LazyJSONEncoder\n\n\ntemplate = dict(\n    info={\n        'title': LazyString(lambda: 'Lazy Title'),\n        'version': LazyString(lambda: '99.9.9'),\n        'description': LazyString(lambda: 'Hello Lazy World'),\n        'termsOfService': LazyString(lambda: '/there_is_no_tos')\n    },\n    host=LazyString(lambda: request.host),\n    schemes=[LazyString(lambda: 'https' if request.is_secure else 'http')],\n    foo=LazyString(lambda: \"Bar\")\n)\nSwagger(app, template=template)\n\n```\n\nThe `LazyString` values will be evaluated only when `jsonify` encodes the value at runtime, so you have access to Flask `request, session, g, etc..` and also may want to access a database.\n‰ªÖÂΩì`jsonify`Âú®ËøêË°åÊó∂ÂØπËØ•ÂÄºËøõË°åÁºñÁ†ÅÊó∂ÔºåÊâç‰ºöËØÑ‰º∞`LazyString`ÁöÑÂÄºÔºåÂõ†Ê≠§ÊÇ®ÂèØ‰ª•ËÆøÈóÆFlaskÁöÑ`requestÔºåsessionÔºåg Á≠â`ÔºåÊàñËÄÖÊòØËÆøÈóÆÊï∞ÊçÆÂ∫ì„ÄÇ\n\n## ÁªèËøáÂèçÂêë‰ª£ÁêÜ\n\nÊúâÊó∂ÔºåÊÇ®ÊòØÂú®ÂèçÂêë‰ª£ÁêÜÔºà‰æãÂ¶ÇNGINXÔºâÂêéÈù¢Êèê‰æõÂ∫ûÂ§ßÁöÑÊñáÊ°£„ÄÇÂΩìÈÅµÂæ™[FlaskÊåáÂçó](http://flask.pocoo.org/snippets/35/)Êó∂Ôºå\nSwaggerÁöÑÊñáÊ°£Â∞ÜÊ≠£Á°ÆÂä†ËΩΩÔºå‰ΩÜÊòØ‚ÄúÂ∞ùËØï‰∏Ä‰∏ã‚ÄùÊåâÈíÆÊåáÂêëÈîôËØØÁöÑ‰ΩçÁΩÆ„ÄÇÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ã‰ª£Á†ÅËß£ÂÜ≥Ê≠§ÈóÆÈ¢òÔºö\n\n```python\nfrom flask import Flask, request\nfrom flasgger import Swagger, LazyString, LazyJSONEncoder\n\napp = Flask(__name__)\napp.json_encoder = LazyJSONEncoder\n\ntemplate = dict(swaggerUiPrefix=LazyString(lambda : request.environ.get('HTTP_X_SCRIPT_NAME', '')))\nswagger = Swagger(app, template=template)\n\n``` \n\n# Ëá™ÂÆö‰πâÈªòËÆ§ÈÖçÁΩÆ\n\nÂèØ‰ª•ÂêëFlasggerÊèê‰æõËá™ÂÆö‰πâÈÖçÁΩÆÔºå‰æãÂ¶Ç‰∏çÂêåÁöÑspec routeÊàñÁ¶ÅÁî®Swagger UIÔºö\n\n```python\nswagger_config = {\n    \"headers\": [\n    ],\n    \"specs\": [\n        {\n            \"endpoint\": 'apispec_1',\n            \"route\": '/apispec_1.json',\n            \"rule_filter\": lambda rule: True,  # all in\n            \"model_filter\": lambda tag: True,  # all in\n        }\n    ],\n    \"static_url_path\": \"/flasgger_static\",\n    # \"static_folder\": \"static\",  # must be set by user\n    \"swagger_ui\": True,\n    \"specs_route\": \"/apidocs/\"\n}\n\nswagger = Swagger(app, config=swagger_config)\n\n```\n\n## ÊèêÂèñÂÆö‰πâ\n\nÂΩìÂú®spec‰∏≠ÊâæÂà∞`id`Êó∂ÔºåÂèØ‰ª•ÊèêÂèñÂÆö‰πâÔºå‰æãÂ¶ÇÔºö\n\n```python\nfrom flask import Flask, jsonify\nfrom flasgger import Swagger\n\napp = Flask(__name__)\nswagger = Swagger(app)\n\n@app.route('/colors/<palette>/')\ndef colors(palette):\n    \"\"\"Example endpoint returning a list of colors by palette\n    ---\n    parameters:\n      - name: palette\n        in: path\n        type: string\n        enum: ['all', 'rgb', 'cmyk']\n        required: true\n        default: all\n    responses:\n      200:\n        description: È¢úËâ≤ÂàóË°®ÔºàÂèØË¢´Ë∞ÉËâ≤ÊùøËøáÊª§Ôºâ\n        schema:\n          id: Palette\n          type: object\n          properties:\n            palette_name:\n              type: array\n              items:\n                schema:\n                  id: Color\n                  type: string\n        examples:\n          rgb: ['red', 'green', 'blue']\n    \"\"\"\n    all_colors = {\n        'cmyk': ['cyan', 'magenta', 'yellow', 'black'],\n        'rgb': ['red', 'green', 'blue']\n    }\n    if palette == 'all':\n        result = all_colors\n    else:\n        result = {palette: all_colors.get(palette)}\n\n    return jsonify(result)\n\napp.run(debug=True)\n```\n\nÂú®Ê≠§Á§∫‰æã‰∏≠ÔºåÊÇ®‰∏çÂøÖ‰º†ÈÄí`definitions`ÔºåËÄåÊòØÈúÄË¶ÅÂ∞Ü`id`Ê∑ªÂä†Âà∞ÊÇ®ÁöÑschema„ÄÇ\n\n## Python2ÂÖºÂÆπÊÄß\n\n\nÁâàÊú¨`0.9.5.*`Â∞ÜÊòØÊúÄÂêé‰∏Ä‰∏™ÊîØÊåÅPython2ÁöÑÁâàÊú¨„ÄÇ\nËØ∑Âú®ËøôÈáåËÆ®ËÆ∫[#399](https://github.com/flasgger/flasgger/issues/399)„ÄÇ\n\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": "README.zh.md", "language": "markdown", "complexity": "complex", "star_count": 3704, "quality_score": 0.8099999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 837, "word_count": 1392}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "flasgger\nmarshmallow\napispec\nflask-restful\ndecorator\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": "demo_app/requirements.txt", "language": "text", "complexity": "simple", "star_count": 3704, "quality_score": 0.39999999999999997, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 6, "word_count": 5}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "This is the summary defined in yaml file\nFirst line is the summary\nAll following lines until the hyphens is added to description\nthe format of the first lines until 3 hyphens will be not yaml compliant\nbut everything below the 3 hyphens should be.\n---\ntags:\n  - users\nparameters:\n  - in: path\n    name: username\n    type: string\n    required: true\nresponses:\n  200:\n    description: A single user item\n    schema:\n      id: rec_username\n      properties:\n        username:\n          type: string\n          description: The name of the user\n          default: 'steve-harris'\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": "etc/flasgger_package/flasgger_package/package_specs.yml", "language": "yaml", "complexity": "simple", "star_count": 3704, "quality_score": 0.6, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 24, "word_count": 81}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nExample using marshmallow APISpec as base template for Flasgger specs\n\"\"\"\n# coding: utf-8\nfrom flask import Flask, jsonify\n\nfrom flasgger import APISpec, Schema, Swagger, fields\nfrom apispec.ext.marshmallow import MarshmallowPlugin\nfrom apispec_webframeworks.flask import FlaskPlugin\n\n# Create an APISpec\nspec = APISpec(\n    title='Flasger Petstore',\n    version='1.0.10',\n    openapi_version='2.0',\n    plugins=[\n        FlaskPlugin(),\n        MarshmallowPlugin(),\n    ],\n)\n\napp = Flask(__name__)\n\n\n# Optional marshmallow support\nclass CategorySchema(Schema):\n    id = fields.Int()\n    name = fields.Str(required=True)\n\n\nclass PetSchema(Schema):\n    category = fields.Nested(CategorySchema, many=True)\n    name = fields.Str()\n\n\n@app.route('/random')\ndef random_pet():\n    \"\"\"\n    A cute furry animal endpoint.\n    Get a random pet\n    ---\n    description: Get a random pet\n    responses:\n        200:\n            description: A pet to be returned\n            schema:\n                $ref: '#/definitions/Pet'\n    \"\"\"\n    pet = {'category': [{'id': 1, 'name': 'rodent'}], 'name': 'Mickey'}\n    return jsonify(PetSchema().dump(pet).data)\n\n@app.route(\"/add\", methods=[\"POST\"])\ndef create_pet(body: PetSchema) :\n    \"\"\"Create a cute furry animal endpoint.\n    ---\n    post:\n      description: Create a random pet\n      parameters: \n        - in: body\n          name: body\n          required: True\n          schema:\n                $ref: '#/definitions/Pet'\n      security:\n        - ApiKeyAuth: []\n      responses:\n        201:\n          description: If pet is created\n          content:\n            application/json:\n              status: string\n    \"\"\"\n    return jsonify(\n        {\"status\": \"New user created\"}\n    ), 201\n\n\ntemplate = spec.to_flasgger(\n    app,\n    definitions=[CategorySchema, PetSchema],\n    paths=[random_pet, create_pet]\n)\n\n\"\"\"\noptionally if using apispec.APISpec from original module\nyou can do:\n\nfrom flasgger.utils import apispec_to_template\ntemplate = apispec_to_template(\n    app=app,\n    spec=spec,\n    definitions=[CategorySchema, PetSchema],\n    paths=[random_pet, create_pet]\n)\n\n\"\"\"\n\n# start Flasgger using a template from apispec\nswag = Swagger(app, template=template)\n\n\nif __name__ == '__main__':\n    app.run(debug=True)\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": "examples/apispec_example.py", "language": "python", "complexity": "complex", "star_count": 3704, "quality_score": 0.75, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 104, "word_count": 223}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nExample using marshmallow APISpec as base template for Flasgger specs\nand using the Swagger UI new style layout version 3\n\"\"\"\n# coding: utf-8\nfrom flask import Flask, jsonify\n\nfrom flasgger import APISpec, Schema, Swagger, fields\nfrom apispec.ext.marshmallow import MarshmallowPlugin\nfrom apispec_webframeworks.flask import FlaskPlugin\n\n# Create an APISpec\nspec = APISpec(\n    title='Flasger Petstore',\n    version='1.0.10',\n    openapi_version='2.0',\n    plugins=[\n        FlaskPlugin(),\n        MarshmallowPlugin(),\n    ],\n)\n\napp = Flask(__name__)\n\n\n# Optional marshmallow support\nclass CategorySchema(Schema):\n    id = fields.Int()\n    name = fields.Str(required=True)\n\n\nclass PetSchema(Schema):\n    category = fields.Nested(CategorySchema, many=True)\n    name = fields.Str()\n\n\n@app.route('/random')\ndef random_pet():\n    \"\"\"\n    A cute furry animal endpoint.\n    Get a random pet\n    ---\n    description: Get a random pet\n    responses:\n        200:\n            description: A pet to be returned\n            schema:\n                $ref: '#/definitions/Pet'\n    \"\"\"\n    pet = {'category': [{'id': 1, 'name': 'rodent'}], 'name': 'Mickey'}\n    return jsonify(PetSchema().dump(pet).data)\n\n\ntemplate = spec.to_flasgger(\n    app,\n    definitions=[CategorySchema, PetSchema],\n    paths=[random_pet]\n)\n\n\"\"\"\noptionally if using apispec.APISpec from original module\nyou can do:\n\nfrom flasgger.utils import apispec_to_template\ntemplate = apispec_to_template(\n    app=app,\n    spec=spec,\n    definitions=[CategorySchema, PetSchema],\n    paths=[random_pet]\n)\n\n\"\"\"\n\n# set the UIVERSION to 3\napp.config['SWAGGER'] = {'uiversion': 3}\n\n# start Flasgger using a template from apispec\nswag = Swagger(app, template=template)\n\n\nif __name__ == '__main__':\n    app.run(debug=True)\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": "examples/apispec_example_uiversion3.py", "language": "python", "complexity": "complex", "star_count": 3704, "quality_score": 0.78, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 83, "word_count": 187}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nA test to ensure that MethodView inheritance works as expected\n\"\"\"\nfrom flask import Flask, jsonify\nfrom flask.views import MethodView\n\nfrom flasgger import Swagger\n\n\nclass BaseAPIView(MethodView):\n    \"\"\"BAse view\"\"\"\n\n\nclass ModelAPIView(BaseAPIView):\n    \"\"\"Model api view\"\"\"\n\n\nclass PostAPIView(ModelAPIView):\n\n    def get(self, team_id):\n        \"\"\"\n        Get a list of users\n        First line is the summary\n        All following lines until the hyphens is added to description\n        ---\n        tags:\n          - users\n        parameters:\n          - name: team_id\n            in: path\n            description: ID of team (type any number)\n            required: true\n            type: integer\n        definitions:\n          User:\n            type: object\n            properties:\n              name:\n                type: string\n              team:\n                type: integer\n        responses:\n          200:\n            description: Returns a list of users\n            schema:\n              id: Users\n              type: object\n              properties:\n                users:\n                  type: array\n                  items:\n                    $ref: '#/definitions/User'\n            examples:\n              users: [{'name': 'Russel Allen', 'team': 66}]\n        \"\"\"\n        data = {\n            \"users\": [\n                {\"name\": \"Steven Wilson\", \"team\": team_id},\n                {\"name\": \"Mikael Akerfeldt\", \"team\": team_id},\n                {\"name\": \"Daniel Gildenlow\", \"team\": team_id}\n            ]\n        }\n        return jsonify(data)\n\n\napp = Flask(__name__)\nswag = Swagger(app)\n\napp.add_url_rule(\n    '/user/<team_id>',\n    view_func=PostAPIView.as_view('user'),\n    methods=['GET']\n)\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": "examples/base_model_view.py", "language": "python", "complexity": "medium", "star_count": 3704, "quality_score": 0.75, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 77, "word_count": 160}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nAn example that uses HTTP Basic Auth to protect the /apidocs and\n/apispec_1.json endpoints.\n\nIn this example the login credentials to view the Swagger docs are as follows:\n\n    Username: guest\n    Password: secret\n\nFailing to provide the right credentials results in a 401 Unauthorized\nresponse from all the Flasgger endpoints.\n\"\"\"\nfrom flask import Flask, request, redirect, Response, jsonify\nfrom functools import wraps\n\nfrom flasgger import Swagger\n\ndef requires_basic_auth(f):\n    \"\"\"Decorator to require HTTP Basic Auth for your endpoint.\"\"\"\n\n    def check_auth(username, password):\n        return username == \"guest\" and password == \"secret\"\n\n    def authenticate():\n        return Response(\n            \"Authentication required.\", 401,\n            {\"WWW-Authenticate\": \"Basic realm='Login Required'\"},\n        )\n\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        # NOTE: This example will require Basic Auth only when you run the\n        # app directly. For unit tests, we can't block it from getting the\n        # Swagger specs so we just allow it to go thru without auth.\n        # The following two lines of code wouldn't be needed in a normal\n        # production environment.\n        if __name__ != \"__main__\":\n            return f(*args, **kwargs)\n\n        auth = request.authorization\n        if not auth or not check_auth(auth.username, auth.password):\n            return authenticate()\n        return f(*args, **kwargs)\n\n    return decorated\n\napp = Flask(__name__)\napp.config[\"SWAGGER\"] = {\n    \"title\": \"Swagger Basic Auth App\",\n    \"uiversion\": 2,\n}\nswag = Swagger(app,\n    decorators=[ requires_basic_auth ],\n    template={\n        \"swagger\": \"2.0\",\n        \"info\": {\n            \"title\": \"Swagger Basic Auth App\",\n            \"version\": \"1.0\",\n        },\n        \"consumes\": [\n            \"application/json\",\n        ],\n        \"produces\": [\n            \"application/json\",\n        ],\n    },\n)\n\n@app.route(\"/echo/<name>\", methods=[\"GET\", \"POST\"])\ndef echo(name):\n    \"\"\"\n    Echo back the name and any posted parameters.\n    ---\n    tags:\n      - echo\n    parameters:\n      - in: path\n        name: name\n        type: string\n        required: true\n      - in: body\n        name: body\n        description: JSON parameters.\n        schema:\n          properties:\n            first_name:\n              type: string\n              description: First name.\n              example: Alice\n            last_name:\n              type: string\n              description: Last name.\n              example: Smith\n            dob:\n              type: string\n              format: date\n              description: Date of birth.\n              example: 1990-01-01\n            comment:\n              type: string\n              description: Something arbitrary.\n              example: Hello world\n    responses:\n      200:\n        description: OK.\n    \"\"\"\n    data = {\n        \"url_name\": name,\n        \"json\": request.json,\n    }\n    return jsonify(data)\n\n@app.route(\"/\")\ndef index():\n    return redirect(\"/apidocs\")\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": "examples/basic_auth.py", "language": "python", "complexity": "complex", "star_count": 3704, "quality_score": 0.75, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 119, "word_count": 326}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThe simple example using OpenAPI 3.0 callbacks\n\"\"\"\n\nfrom flask import Flask, jsonify\n\nfrom flasgger import Swagger\n\napp = Flask(__name__)\napp.config['SWAGGER'] = {\n    'title': 'OA3 Callbacks',\n    'openapi': '3.0.2'\n}\nSwagger(app)\n\n\n@app.route('/run_callback/',  methods=['POST'])\ndef run_callback():\n    \"\"\"Example endpoint that specifies OA3 callbacks\n    This is using docstring for specifications\n    ---\n    tags:\n      - callbacks\n    requestBody:\n      description: Test\n      required: true\n      content:\n        application/json:\n          schema:\n            properties:\n              callback_url:\n                type: string\n                format: uri\n                description: Callback URL for request\n    callbacks:\n        onSomeEvent:\n          '{$request.body.callback_url}':\n            post:\n              requestBody:\n                description: status payload\n                content:\n                  application/json:\n                    schema:\n                      properties:\n                        status:\n                          type: string\n    \"\"\"\n\n    return jsonify({'result': 'ok'})\n\n\ndef test_swag(client, specs_data):\n    \"\"\"\n    This test is runs automatically in Travis CI\n\n    :param client: Flask app test client\n    :param specs_data: {'url': {swag_specs}} for every spec in app\n    \"\"\"\n    for url, spec in specs_data.items():\n        assert 'openapi' in spec\n        assert spec['openapi'] == '3.0.2'\n\n        assert 'callbacks' in spec['paths']['/run_callback/']['post']\n        assert 'onSomeEvent' in \\\n               spec['paths']['/run_callback/']['post']['callbacks']\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": "examples/callbacks.py", "language": "python", "complexity": "medium", "star_count": 3704, "quality_score": 0.8099999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 70, "word_count": 144}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"0.9.0 News and Changes\n\n- Updated Swagger UI 3.x\n- uiversion now defaults to 3\n- 'hide_top_bar' config option to remove the green top bar\n- 'fotter_text' added to customize the footer text (allows html and <script>)\n- templates/flasgger/footer.html added and can be replaced to customization\n  this footer.html is the right place for custom <script>\n- 'top_text' added to customize the header text (allows html)\n- templates/flasgger/top.html added and can be replaced to customization\n- 'head_text' added to customize the <head> (allows html)\n- templates/flasgger/head.html added and can be replaced to customization\n- added 'doc_expansion' config to control the collapse\n- added 'ui_params' to allow override of any swagger.ui values\n\"\"\"\n\nfrom flask import Flask, request\nfrom flasgger import Swagger\n\napp = Flask(__name__)\napp.config['TEMPLATES_AUTO_RELOAD'] = True\napp.config['SWAGGER'] = {\n    'title': 'MyAPI',\n\n    # UI version 3 is now the default\n    # 'uiversion': '3',\n\n    # This setup hides the green top bar\n    'hide_top_bar': True,\n\n    # this text is rendered in the footer\n    # (optionally you can replace flasgger/footer.html template)\n    'footer_text': '<b>Hello World </b><script>alert(\"Hello World!\")</script>',\n\n    # this text is rendered in the header\n    # (optionally you can replace flasgger/header.html template)\n    'top_text': '<b><span class=\"top_text\">Welcome to my api </span></b>',\n\n    # this text is rendered in the <head>\n    # (optionally you can replace flasgger/head.html template)\n    'head_text': '<style>.top_text{color: red;}</style>',\n\n    # Control the collapse of each tag, '\"none\"' means all tags default closed\n    # \"none\" - It'll Hide everything.\n    # \"list\"- It'll expand/List all the operations only. (default)\n    # \"full\" - It'll expand everything(Full expand as the name says).\n    'doc_expansion': \"list\",\n\n    # Allows overriding any of the uiparams\n    # This is useful to override other stuff not provided by the above aliases\n    'ui_params': {\n        'apisSorter': 'alpha',\n        'operationsSorter': 'alpha',\n    },\n\n    # Allows overriding any of the uiparams with Javascript expressions\n    # This is useful to override other stuff not provided by the above aliases which cannot be serialized to a JSON string\n    'ui_params_text': '''{\n    \"operationsSorter\" : (a, b) => a.get(\"path\").localeCompare(b.get(\"path\"))\n}'''\n}\n\nSwagger(app)\n\n\n@app.route('/', methods=['GET', 'POST'])\ndef main_route():\n    \"\"\"\n    Test Endpoint\n    ---\n    tags:\n        -   Test\n    parameters:\n        -   name: data\n            in: formData\n            required: True\n            type: string\n            description: data to send\n    responses:\n        200:\n            description: data received successfully\n        404:\n            description: data not found in request form\n    \"\"\"\n    if 'data' not in request.form.keys():\n        return 'data not found in request form', 404\n    return 'data received: ' + str(request.form['data'])\n\n\nif __name__ == '__main__':\n    app.run()\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": "examples/changelog_090.py", "language": "python", "complexity": "medium", "star_count": 3704, "quality_score": 0.78, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 92, "word_count": 389}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThe simple example using declared definitions.\n\"\"\"\n\nfrom flask import Flask, jsonify\n\nfrom flasgger import Swagger, utils\n\napp = Flask(__name__)\napp.config['SWAGGER'] = {\n    'title': 'Colors API'\n}\nSwagger(app)\n\n\n@app.route('/colors/<palette>/')\ndef colors(palette):\n    \"\"\"Example endpoint return a list of colors by palette\n    This is using docstring for specifications\n    ---\n    tags:\n      - colors\n    parameters:\n      - name: palette\n        in: path\n        type: string\n        enum: ['all', 'rgb', 'cmyk']\n        required: true\n        default: all\n        description: Which palette to filter?\n    operationId: get_colors\n    consumes:\n      - application/json\n    produces:\n      - application/json\n    security:\n      colors_auth:\n        - 'write:colors'\n        - 'read:colors'\n    schemes: ['http', 'https']\n    deprecated: false\n    externalDocs:\n      description: Project repository\n      url: http://github.com/rochacbruno/flasgger\n    definitions:\n      Palette:\n        type: object\n        properties:\n          palette_name:\n            type: array\n            items:\n              $ref: '#/definitions/Color'\n      Color:\n        type: string\n    responses:\n      200:\n        description: A list of colors (may be filtered by palette)\n        schema:\n          $ref: '#/definitions/Palette'\n        examples:\n          rgb: ['red', 'green', 'blue']\n    \"\"\"\n    all_colors = {\n        'cmyk': ['cyan', 'magenta', 'yellow', 'black'],\n        'rgb': ['red', 'green', 'blue']\n    }\n    if palette == 'all':\n        result = all_colors\n    else:\n        result = {palette: all_colors.get(palette)}\n\n    return jsonify(result)\n\n\ndef test_swag(client, specs_data):\n    \"\"\"\n    This test is runs automatically in Travis CI\n\n    :param client: Flask app test client\n    :param specs_data: {'url': {swag_specs}} for every spec in app\n    \"\"\"\n    for url, spec in specs_data.items():\n        assert 'Palette' in spec['definitions']\n        assert 'Color' in spec['definitions']\n        assert 'colors' in spec['paths']['/colors/{palette}/']['get']['tags']\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": "examples/colors.py", "language": "python", "complexity": "medium", "star_count": 3704, "quality_score": 0.8099999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 90, "word_count": 209}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThe simple example using declared definitions, and external static js/css.\n\"\"\"\n\nfrom flask import Flask, jsonify\n\nfrom flasgger import Swagger\n\napp = Flask(__name__)\napp.config['SWAGGER'] = {\n    'title': 'Colors API'\n}\nswagger_config = Swagger.DEFAULT_CONFIG\nswagger_config['swagger_ui_bundle_js'] = '//unpkg.com/swagger-ui-dist@3/swagger-ui-bundle.js'\nswagger_config['swagger_ui_standalone_preset_js'] = '//unpkg.com/swagger-ui-dist@3/swagger-ui-standalone-preset.js'\nswagger_config['jquery_js'] = '//unpkg.com/jquery@2.2.4/dist/jquery.min.js'\nswagger_config['swagger_ui_css'] = '//unpkg.com/swagger-ui-dist@3/swagger-ui.css'\nSwagger(app, config=swagger_config)\n\n@app.route('/colors/<palette>/')\ndef colors(palette):\n    \"\"\"Example endpoint return a list of colors by palette\n    This is using docstring for specifications\n    ---\n    tags:\n      - colors\n    parameters:\n      - name: palette\n        in: path\n        type: string\n        enum: ['all', 'rgb', 'cmyk']\n        required: true\n        default: all\n        description: Which palette to filter?\n    operationId: get_colors\n    consumes:\n      - application/json\n    produces:\n      - application/json\n    security:\n      colors_auth:\n        - 'write:colors'\n        - 'read:colors'\n    schemes: ['http', 'https']\n    deprecated: false\n    externalDocs:\n      description: Project repository\n      url: http://github.com/rochacbruno/flasgger\n    definitions:\n      Palette:\n        type: object\n        properties:\n          palette_name:\n            type: array\n            items:\n              $ref: '#/definitions/Color'\n      Color:\n        type: string\n    responses:\n      200:\n        description: A list of colors (may be filtered by palette)\n        schema:\n          $ref: '#/definitions/Palette'\n        examples:\n          rgb: ['red', 'green', 'blue']\n    \"\"\"\n    all_colors = {\n        'cmyk': ['cyan', 'magenta', 'yellow', 'black'],\n        'rgb': ['red', 'green', 'blue']\n    }\n    if palette == 'all':\n        result = all_colors\n    else:\n        result = {palette: all_colors.get(palette)}\n\n    return jsonify(result)\n\n\ndef test_swag(client, specs_data):\n    \"\"\"\n    This test is runs automatically in Travis CI\n\n    :param client: Flask app test client\n    :param specs_data: {'url': {swag_specs}} for every spec in app\n    \"\"\"\n    for url, spec in specs_data.items():\n        assert 'Palette' in spec['definitions']\n        assert 'Color' in spec['definitions']\n        assert 'colors' in spec['paths']['/colors/{palette}/']['get']['tags']\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n", "metadata": {"source": "github.com/flasgger/flasgger", "file_path": "examples/colors_external_js.py", "language": "python", "complexity": "medium", "star_count": 3704, "quality_score": 0.8099999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 94, "word_count": 228}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: 'Feature Request'\nabout: Suggest a feature\n---\n\n# Feature Request\n\n## Description\n\n<!-- edit: --> A clear and concise description of the problem or missing capability...\n\n## Describe the solution you'd like\n\n<!-- edit: --> If you have a solution in mind, please describe it.\n\n## Describe alternatives you've considered\n\n<!-- edit: --> Have you considered any alternative solutions or workarounds?\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": ".github/ISSUE_TEMPLATE/feature.md", "language": "markdown", "complexity": "simple", "star_count": 3515, "quality_score": 0.6, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 19, "word_count": 63}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# PR Checklist\n\nPlease check if your PR fulfills the following requirements:\n\n- [ ] The commit message follows our contributing guidelines\n- [ ] Tests for the changes have been added (for bug fixes / features)\n- [ ] Docs have been added / updated (for bug fixes / features)\n\n## PR Type\n\nWhat kind of change does this PR introduce?\n\n- [ ] Bugfix\n- [ ] Feature\n- [ ] Code style update\n- [ ] Refactoring (no functional changes)\n- [ ] Build related changes\n- [ ] CI related changes\n- [ ] Documentation content changes\n- [ ] Other\n\n## What is the current behavior?\n\n## What is the new behavior?\n\n## Does this PR introduce a breaking change?\n\n- [ ] Yes\n- [ ] No\n\n## Other information\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": ".github/PULL_REQUEST_TEMPLATE/general.md", "language": "markdown", "complexity": "simple", "star_count": 3515, "quality_score": 0.6, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 32, "word_count": 136}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Keep GitHub Actions up to date with GitHub's Dependabot...\n# https://docs.github.com/en/code-security/dependabot/working-with-dependabot/keeping-your-actions-up-to-date-with-dependabot\n# https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/configuration-options-for-the-dependabot.yml-file#package-ecosystem\nversion: 2\nupdates:\n  - package-ecosystem: github-actions\n    directory: /\n    groups:\n      github-actions:\n        patterns:\n          - \"*\"  # Group all Actions updates into a single larger pull request\n    schedule:\n      interval: weekly\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": ".github/dependabot.yml", "language": "yaml", "complexity": "simple", "star_count": 3515, "quality_score": 0.63, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 14, "word_count": 41}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Publish\n\non:\n  push:\n    tags:\n      - \"*.*.*\"\n\njobs:\n  publish:\n    name: Publish the package on pypi\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout the source code\n        uses: actions/checkout@v4\n\n      - name: Set the python version\n        uses: actions/setup-python@v5\n        with:\n          python-version: 3.13\n\n      - name: Install dependencies\n        uses: ./.github/actions/install\n        with:\n          python-version: 3.13\n\n      - name: Install builders for publishing\n        run: pip install -r requirements/publish.txt\n\n      - name: Build the distributions\n        run: python -m build --sdist --wheel\n\n      - name: Publish the package\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          user: __token__\n          password: ${{ secrets.PYPI_API_TOKEN }}\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": ".github/workflows/publish.yml", "language": "yaml", "complexity": "simple", "star_count": 3515, "quality_score": 0.6, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 37, "word_count": 82}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Review\n\non: [push, pull_request]\n\njobs:\n  test:\n    name: Run linters and tests\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        python: [\"3.9\", \"3.10\", \"3.11\", \"3.12\", \"3.13\"]\n\n    steps:\n      - name: Checkout the source code\n        uses: actions/checkout@v4\n\n      - name: Set the python version\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python }}\n\n      - name: Install dependencies\n        uses: ./.github/actions/install\n        with:\n          python-version: ${{ matrix.python }}\n\n      - name: Run tests\n        run: tox -e $(tox -l | grep py${PYTHON_VERSION//.} | paste -sd \",\" -)\n        env:\n          PYTHON_VERSION: ${{ matrix.python }}\n\n      - name: Report coverage\n        run: |\n          pip install coverage\n          coverage report\n\n      - name: Check for incompatibilities with publishing to PyPi\n        if: ${{ matrix.python == 3.13 }}\n        run: |\n          pip install -r requirements/publish.txt\n          python -m build --sdist\n          twine check dist/*\n\n  # Return successful statuses \"Run linters and tests\" for python {3.6...8}\n  # These are required for branch protection rules that cannot be removed yet\n  # https://github.com/axnsan12/drf-yasg/pull/922#issuecomment-2739945046\n  review:\n    name: Run linters and tests\n    runs-on: ubuntu-latest\n    needs: test\n    strategy:\n      fail-fast: false\n      matrix:\n        python: [\"3.6\", \"3.7\", \"3.8\", \"3.9\"]\n\n    steps:\n      - run: echo \"Deprecated tests skipped üß™\"\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": ".github/workflows/review.yml", "language": "yaml", "complexity": "simple", "star_count": 3515, "quality_score": 0.6, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 59, "word_count": 171}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "version: 2\n\nsphinx:\n  configuration: docs/conf.py\n\nformats:\n  - pdf\n\nbuild:\n  image: latest\n\npython:\n  version: 3.8\n  install:\n    - requirements: requirements/docs.txt\n    - method: pip\n      path: .\n      extra_requirements:\n        - validation\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": ".readthedocs.yml", "language": "yaml", "complexity": "simple", "star_count": 3515, "quality_score": 0.63, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 20, "word_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": ".. |br| raw:: html\n\n   <br />\n\n############\nContributing\n############\n\nContributions are always welcome and appreciated! Here are some ways you can contribute.\n\n******\nIssues\n******\n\nYou can and should open an issue for any of the following reasons:\n\n* you found a bug; steps for reproducing, or a pull request with a failing test case will be greatly appreciated\n* you wanted to do something but did not find a way to do it after reading the documentation\n* you believe the current way of doing something is more complicated or less elegant than it can be\n* a related feature that you want is missing from the package\n\nPlease always check for existing issues before opening a new issue.\n\n*************\nPull requests\n*************\n\nYou want to contribute some code? Great! Here are a few steps to get you started:\n\n#. **Fork the repository on GitHub**\n#. **Clone your fork and create a branch for the code you want to add**\n#. **Create a new virtualenv and install the package in development mode**\n\n   .. code:: console\n\n      $ python -m venv venv\n      $ source venv/bin/activate\n      (venv) $ python -m pip install -U pip setuptools\n      (venv) $ pip install -U -r requirements/dev.txt -e '.[validation]'\n\n#. **Make your changes and check them against the test project**\n\n   .. code:: console\n\n      (venv) $ cd testproj\n      (venv) $ python manage.py migrate\n      (venv) $ python manage.py runserver\n      (venv) $ firefox localhost:8000/swagger/\n\n#. **Update the tests if necessary**\n\n   You can find them in the ``tests`` directory.\n\n   If your change modifies the expected schema output, you should regenerate the reference schema at\n   ``tests/reference.yaml``:\n\n   .. code:: console\n\n      (venv) $ python testproj/manage.py generate_swagger tests/reference.yaml --overwrite --user admin --url http://test.local:8002/\n\n   After checking the git diff to verify that no unexpected changes appeared, you should commit the new\n   ``reference.yaml`` together with your changes.\n\n#. **Run tests. The project is setup to use tox and pytest for testing**\n\n   .. code:: console\n\n      # install test dependencies\n      (venv) $ pip install -U -r requirements/test.txt\n      # run tests in the current environment, faster than tox\n      (venv) $ pytest -n auto --cov\n      # (optional) sort imports with isort and check flake8 linting\n      (venv) $ isort --apply\n      (venv) $ flake8 src/drf_yasg testproj tests setup.py\n      # (optional) run tests for other python versions in separate environments\n      (venv) $ tox\n\n#. **Update documentation**\n\n   If the change modifies behavior or adds new features, you should update the documentation and ``README.rst``\n   accordingly. Documentation is written in reStructuredText and built using Sphinx. You can find the sources in the\n   ``docs`` directory.\n\n   To build and check the docs, run\n\n   .. code:: console\n\n      (venv) $ tox -e docs\n\n#. **Push your branch and submit a pull request to the master branch on GitHub**\n\n   Incomplete/Work In Progress pull requests are encouraged, because they allow you to get feedback and help more\n   easily.\n\n#. **Your code must pass all the required CI jobs before it is merged**\n\n   As of now, this consists of running on the supported Python, Django, DRF version matrix (see README),\n   and building the docs successfully.\n\n******************\nMaintainer's notes\n******************\n\nRelease checklist\n=================\n\n* update ``docs/changelog.rst`` with changes since the last tagged version\n* commit & tag the release - ``git tag x.x.x -m \"Release version x.x.x\"``\n* push using ``git push --follow-tags``\n* verify that `Actions`_ has built the tag and successfully published the release to `PyPI`_\n* publish release notes `on GitHub`_\n* start the `ReadTheDocs build`_ if it has not already started\n* deploy the live demo `on Heroku`_\n\n\n.. _Actions: https://github.com/axnsan12/drf-yasg/actions\n.. _PyPI: https://pypi.org/project/drf-yasg/\n.. _on GitHub: https://github.com/axnsan12/drf-yasg/releases\n.. _ReadTheDocs build: https://readthedocs.org/projects/drf-yasg/builds/\n.. _on Heroku: https://dashboard.heroku.com/pipelines/412d1cae-6a95-4f5e-810b-94869133f36a\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": "CONTRIBUTING.rst", "language": "restructuredtext", "complexity": "medium", "star_count": 3515, "quality_score": 0.63, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 121, "word_count": 595}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": ".. |br| raw:: html\n\n   <br />\n\n#######\nLicense\n#######\n\n********************\nBSD 3-Clause License\n********************\n\nCopyright (c) 2017 - 2019, Cristian V. <cristi@cvjd.me> |br|\\ All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": "LICENSE.rst", "language": "restructuredtext", "complexity": "simple", "star_count": 3515, "quality_score": 0.6, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 39, "word_count": 242}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": ".. role:: python(code)\n   :language: python\n\n########################################\ndrf-yasg - Yet another Swagger generator\n########################################\n\n|actions| |nbsp| |codecov| |nbsp| |rtd-badge| |nbsp| |pypi-version| |nbsp| |gitter|\n\nGenerate **real** Swagger/OpenAPI 2.0 specifications from a Django Rest Framework API.\n\nCompatible with\n\n- **Django Rest Framework**: 3.13, 3.14, 3.15\n- **Django**: 4.0, 4.1, 4.2, 5.0, 5.1, 5.2\n- **Python**: 3.9, 3.10, 3.11, 3.12, 3.13\n\nOnly the latest patch version of each ``major.minor`` series of Python, Django and Django REST Framework is supported.\n\n**Only the latest version of drf-yasg is supported.** Support of old versions is dropped immediately with the release\nof a new version. Please do not create issues before upgrading to the latest release available at the time. Regression\nreports are accepted and will be resolved with a new release as quickly as possible. Removed features will usually go\nthrough a deprecation cycle of a few minor releases.\n\nResources:\n\n* `Sources <https://github.com/axnsan12/drf-yasg>`_\n* `Documentation <https://drf-yasg.readthedocs.io>`_\n* `Changelog <https://drf-yasg.readthedocs.io/en/stable/changelog.html>`_\n* `Live demo <https://drf-yasg-demo.herokuapp.com>`_\n* `Discussion <https://app.gitter.im/#/room/#drf-yasg:gitter.im>`_\n\n|heroku-button|\n\n****************\nOpenAPI 3.0 note\n****************\n\nIf you are looking to add Swagger/OpenAPI support to a new project you might want to take a look at\n`drf-spectacular <https://github.com/tfranzel/drf-spectacular>`_, which is an actively maintained new library that\nshares most of the goals of this project, while working with OpenAPI 3.0 schemas.\n\nOpenAPI 3.0 provides a lot more flexibility than 2.0 in the types of API that can be described.\n``drf-yasg`` is unlikely to soon, if ever, get support for OpenAPI 3.0.\n\n\n********\nFeatures\n********\n\n- full support for nested Serializers and Schemas\n- response schemas and descriptions\n- model definitions compatible with codegen tools\n- customization hooks at all points in the spec generation process\n- JSON and YAML format for spec\n- bundles latest version of\n  `swagger-ui <https://github.com/swagger-api/swagger-ui>`_ and\n  `redoc <https://github.com/Rebilly/ReDoc>`_ for viewing the generated documentation\n- schema view is cacheable out of the box\n- generated Swagger schema can be automatically validated by\n  `swagger-spec-validator <https://github.com/Yelp/swagger_spec_validator>`_\n- supports Django REST Framework API versioning with ``URLPathVersioning`` and ``NamespaceVersioning``; other DRF\n  or custom versioning schemes are not currently supported\n\n.. figure:: https://raw.githubusercontent.com/axnsan12/drf-yasg/1.0.2/screenshots/redoc-nested-response.png\n   :width: 100%\n   :figwidth: image\n   :alt: redoc screenshot\n\n   **Fully nested request and response schemas.**\n\n.. figure:: https://raw.githubusercontent.com/axnsan12/drf-yasg/1.0.2/screenshots/swagger-ui-list.png\n   :width: 100%\n   :figwidth: image\n   :alt: swagger-ui screenshot\n\n   **Choose between redoc and swagger-ui.**\n\n.. figure:: https://raw.githubusercontent.com/axnsan12/drf-yasg/1.0.2/screenshots/swagger-ui-models.png\n   :width: 100%\n   :figwidth: image\n   :alt: model definitions screenshot\n\n   **Real Model definitions.**\n\n\n*****************\nTable of contents\n*****************\n\n.. contents::\n   :depth: 4\n\n*****\nUsage\n*****\n\n0. Installation\n===============\n\nThe preferred installation method is directly from pypi:\n\n.. code:: console\n\n   pip install --upgrade drf-yasg\n\nAdditionally, if you want to use the built-in validation mechanisms (see `4. Validation`_), you need to install\nsome extra requirements:\n\n.. code:: console\n\n   pip install --upgrade drf-yasg[validation]\n\n.. _readme-quickstart:\n\n1. Quickstart\n=============\n\nIn ``settings.py``:\n\n.. code:: python\n\n   INSTALLED_APPS = [\n      ...\n      'django.contrib.staticfiles',  # required for serving swagger ui's css/js files\n      'drf_yasg',\n      ...\n   ]\n\nIn ``urls.py``:\n\n.. code:: python\n\n   ...\n   from django.urls import re_path\n   from rest_framework import permissions\n   from drf_yasg.views import get_schema_view\n   from drf_yasg import openapi\n\n   ...\n\n   schema_view = get_schema_view(\n      openapi.Info(\n         title=\"Snippets API\",\n         default_version='v1',\n         description=\"Test description\",\n         terms_of_service=\"https://www.google.com/policies/terms/\",\n         contact=openapi.Contact(email=\"contact@snippets.local\"),\n         license=openapi.License(name=\"BSD License\"),\n      ),\n      public=True,\n      permission_classes=(permissions.AllowAny,),\n   )\n\n   urlpatterns = [\n      path('swagger.<format>/', schema_view.without_ui(cache_timeout=0), name='schema-json'),\n      path('swagger/', schema_view.with_ui('swagger', cache_timeout=0), name='schema-swagger-ui'),\n      path('redoc/', schema_view.with_ui('redoc', cache_timeout=0), name='schema-redoc'),\n      ...\n   ]\n\nThis exposes 4 endpoints:\n\n* A JSON view of your API specification at ``/swagger.json``\n* A YAML view of your API specification at ``/swagger.yaml``\n* A swagger-ui view of your API specification at ``/swagger/``\n* A ReDoc view of your API specification at ``/redoc/``\n\n2. Configuration\n================\n\n---------------------------------\na. ``get_schema_view`` parameters\n---------------------------------\n\n- ``info`` - Swagger API Info object; if omitted, defaults to ``DEFAULT_INFO``\n- ``url`` - API base url; if left blank will be deduced from the location the view is served at\n- ``patterns`` - passed to SchemaGenerator\n- ``urlconf`` - passed to SchemaGenerator\n- ``public`` - if False, includes only endpoints the current user has access to\n- ``validators`` - a list of validator names to apply on the generated schema; only ``ssv`` is currently supported\n- ``generator_class`` - schema generator class to use; should be a subclass of ``OpenAPISchemaGenerator``\n- ``authentication_classes`` - authentication classes for the schema view itself\n- ``permission_classes`` - permission classes for the schema view itself\n\n-------------------------------\nb. ``SchemaView`` options\n-------------------------------\n\n-  :python:`SchemaView.with_ui(renderer, cache_timeout, cache_kwargs)` - get a view instance using the\n   specified UI renderer; one of ``swagger``, ``redoc``\n-  :python:`SchemaView.without_ui(cache_timeout, cache_kwargs)` - get a view instance with no UI renderer;\n   same as ``as_cached_view`` with no kwargs\n-  :python:`SchemaView.as_cached_view(cache_timeout, cache_kwargs, **initkwargs)` - same as ``as_view``,\n   but with optional caching\n-  you can, of course, call :python:`as_view` as usual\n\nAll of the first 3 methods take two optional arguments, ``cache_timeout`` and ``cache_kwargs``; if present,\nthese are passed on to Django‚Äôs :python:`cached_page` decorator in order to enable caching on the resulting view.\nSee `3. Caching`_.\n\n----------------------------------------------\nc. ``SWAGGER_SETTINGS`` and ``REDOC_SETTINGS``\n----------------------------------------------\n\nAdditionally, you can include some more settings in your ``settings.py`` file.\nSee https://drf-yasg.readthedocs.io/en/stable/settings.html for details.\n\n\n3. Caching\n==========\n\nSince the schema does not usually change during the lifetime of the django process, there is out of the box support for\ncaching the schema view in-memory, with some sane defaults:\n\n* caching is enabled by the `cache_page <https://docs.djangoproject.com/en/1.11/topics/cache/#the-per-view-cache>`__\n  decorator, using the default Django cache backend, can be changed using the ``cache_kwargs`` argument\n* HTTP caching of the response is blocked to avoid confusing situations caused by being shown stale schemas\n* the cached schema varies on the ``Cookie`` and ``Authorization`` HTTP headers to enable filtering of visible endpoints\n  according to the authentication credentials of each user; note that this means that every user accessing the schema\n  will have a separate schema cached in memory.\n\n4. Validation\n=============\n\nGiven the numerous methods to manually customize the generated schema, it makes sense to validate the result to ensure\nit still conforms to OpenAPI 2.0. To this end, validation is provided at the generation point using python swagger\nlibraries, and can be activated by passing :python:`validators=['ssv']` to ``get_schema_view``; if the generated\nschema is not valid, a :python:`SwaggerValidationError` is raised by the handling codec.\n\n**Warning:** This internal validation can slow down your server.\nCaching can mitigate the speed impact of validation.\n\nThe provided validation will catch syntactic errors, but more subtle violations of the spec might slip by them. To\nensure compatibility with code generation tools, it is recommended to also employ one or more of the following methods:\n\n-------------------------------\n``swagger-ui`` validation badge\n-------------------------------\n\nOnline\n^^^^^^\n\nIf your schema is publicly accessible, `swagger-ui` will automatically validate it against the official swagger\nonline validator and display the result in the bottom-right validation badge.\n\nOffline\n^^^^^^^\n\nIf your schema is not accessible from the internet, you can run a local copy of\n`swagger-validator <https://hub.docker.com/r/swaggerapi/swagger-validator/>`_ and set the ``VALIDATOR_URL`` accordingly:\n\n.. code:: python\n\n    SWAGGER_SETTINGS = {\n        ...\n        'VALIDATOR_URL': 'http://localhost:8189',\n        ...\n    }\n\n.. code:: console\n\n    $ docker run --name swagger-validator -d -p 8189:8080 --add-host test.local:10.0.75.1 swaggerapi/swagger-validator\n    84dabd52ba967c32ae6b660934fa6a429ca6bc9e594d56e822a858b57039c8a2\n    $ curl http://localhost:8189/debug?url=http://test.local:8002/swagger/?format=openapi\n    {}\n\n---------------------\nUsing ``swagger-cli``\n---------------------\n\nhttps://www.npmjs.com/package/swagger-cli\n\n.. code:: console\n\n    $ npm install -g swagger-cli\n    [...]\n    $ swagger-cli validate http://test.local:8002/swagger.yaml\n    http://test.local:8002/swagger.yaml is valid\n\n--------------------------------------------------------------\nManually on `editor.swagger.io <https://editor.swagger.io/>`__\n--------------------------------------------------------------\n\nImporting the generated spec into https://editor.swagger.io/ will automatically trigger validation on it.\nThis method is currently the only way to get both syntactic and semantic validation on your specification.\nThe other validators only provide JSON schema-level validation, but miss things like duplicate operation names,\nimproper content types, etc\n\n5. Code generation\n==================\n\nYou can use the specification outputted by this library together with\n`swagger-codegen <https://github.com/swagger-api/swagger-codegen>`_ to generate client code in your language of choice:\n\n.. code:: console\n\n   $ docker run --rm -v ${PWD}:/local swaggerapi/swagger-codegen-cli generate -i /local/tests/reference.yaml -l javascript -o /local/.codegen/js\n\nSee the GitHub page linked above for more details.\n\n.. _readme-testproj:\n\n6. Example project\n==================\n\nFor additional usage examples, you can take a look at the test project in the ``testproj`` directory:\n\n.. code:: console\n\n   $ git clone https://github.com/axnsan12/drf-yasg.git\n   $ cd drf-yasg\n   $ virtualenv venv\n   $ source venv/bin/activate\n   (venv) $ cd testproj\n   (venv) $ python -m pip install --upgrade pip setuptools\n   (venv) $ pip install --upgrade -r requirements.txt\n   (venv) $ python manage.py migrate\n   (venv) $ python manage.py runserver\n   (venv) $ firefox localhost:8000/swagger/\n\n************************\nThird-party integrations\n************************\n\ndjangorestframework-camel-case\n===============================\n\nIntegration with `djangorestframework-camel-case <https://github.com/vbabiy/djangorestframework-camel-case>`_ is\nprovided out of the box - if you have ``djangorestframework-camel-case`` installed and your ``APIView`` uses\n``CamelCaseJSONParser`` or ``CamelCaseJSONRenderer``, all property names will be converted to *camelCase* by default.\n\ndjangorestframework-recursive\n===============================\n\nIntegration with `djangorestframework-recursive <https://github.com/heywbj/django-rest-framework-recursive>`_ is\nprovided out of the box - if you have ``djangorestframework-recursive`` installed.\n\n.. |actions| image:: https://img.shields.io/github/actions/workflow/status/axnsan12/drf-yasg/review.yml?branch=master\n   :target: https://github.com/axnsan12/drf-yasg/actions\n   :alt: GitHub Workflow Status\n\n.. |codecov| image:: https://img.shields.io/codecov/c/github/axnsan12/drf-yasg/master.svg\n   :target: https://codecov.io/gh/axnsan12/drf-yasg\n   :alt: Codecov\n\n.. |pypi-version| image:: https://img.shields.io/pypi/v/drf-yasg.svg\n   :target: https://pypi.org/project/drf-yasg/\n   :alt: PyPI\n\n.. |gitter| image:: https://badges.gitter.im/drf-yasg.svg\n    :target: https://app.gitter.im/#/room/#drf-yasg:gitter.im\n    :alt: Gitter\n\n.. |rtd-badge| image:: https://img.shields.io/readthedocs/drf-yasg.svg\n   :target: https://drf-yasg.readthedocs.io/\n   :alt: ReadTheDocs\n\n.. |heroku-button| image:: https://www.herokucdn.com/deploy/button.svg\n   :target: https://heroku.com/deploy?template=https://github.com/axnsan12/drf-yasg\n   :alt: Heroku deploy button\n\n.. |nbsp| unicode:: 0xA0\n   :trim:\n\ndrf-extra-fields\n=================\n\nIntegration with `drf-extra-fields <https://github.com/Hipo/drf-extra-fields>`_ has a problem with Base64 fields.\nThe drf-yasg will generate Base64 file or image fields as Readonly and not required. Here is a workaround code\nfor display the Base64 fields correctly.\n\n.. code:: python\n\n  class PDFBase64FileField(Base64FileField):\n      ALLOWED_TYPES = ['pdf']\n\n      class Meta:\n          swagger_schema_fields = {\n              'type': 'string',\n              'title': 'File Content',\n              'description': 'Content of the file base64 encoded',\n              'read_only': False  # <-- FIX\n          }\n\n      def get_file_extension(self, filename, decoded_file):\n          try:\n              PyPDF2.PdfFileReader(io.BytesIO(decoded_file))\n          except PyPDF2.utils.PdfReadError as e:\n              logger.warning(e)\n          else:\n              return 'pdf'\n\n************\nContributing\n************\n\nSee https://drf-yasg.readthedocs.io/en/stable/contributing.html for details.\n\nThis repository adheres to semantic versioning standards. For more\ninformation on semantic versioning visit `SemVer <https://semver.org>`_.\n\nTo keep our process simple we merge pull requests into the master branch we use\ngit tags for releases. We use labels to mark which issues are intended for each\nversion. For example:\n\n.. figure:: ./docs/images/flow.png\n   :width: 70%\n   :figwidth: image\n   :alt: Git flow\n   :align: center\n\nLabels\n======\n\n- New issues without a version are given a ``triage`` label.\n\n- Issues are labeled ``bug``, ``enhancement`` or ``question`` to describe their\n  content\n\n- Once given a version, an issue will either have an assignee or be given a\n  ``help wanted`` label\n\n- A question that hasn't been answered will be given an ``unanswered`` label\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": "README.rst", "language": "restructuredtext", "complexity": "complex", "star_count": 3515, "quality_score": 0.8099999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 425, "word_count": 1649}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#########\nChangelog\n#########\n\n***********\n**1.21.10**\n***********\n\n**FIXED:** Fix type hints when using postponed evaluation of annotations (PEP-563) (:pr:`840`)\n**IMPROVED:** Update JSON & YAML renderers to not use a \".\" in their format string (:pr:`911`)\n**FIXED:** Fix lint errors when comparing types with == instead of is (:pr:`868`)\n**IMPROVED:** Update swagger-ui-dist to address CVE-2021-46708 (:pr:`904`)\n\n**********\n**1.21.9**\n**********\n\n**ADDED:** Added support for zoneinfo object fields (:pr:`908`)\n\n**********\n**1.21.8**\n**********\n\n**ADDED:** Python 3.11 and 3.12 support (:pr:`891`)\n**FIXED:** Fix pkg_resources version lookups for Python 3.9+ (:pr:`891`)\n\n**********\n**1.21.7**\n**********\n\n*Release date: Jul 20, 2023*\n\n**ADDED:** Added ``drf_yasg.inspectors.query.DrfAPICompatInspector`` (:pr:`857`)\n**ADDED:** Added ``DrfAPICompatInspector`` to serve as a replacement ``CoreAPICompatInspector`` (:pr:`857`)\n**ADDED:** Allow ``DEFAULT_SPEC_RENDERERS`` default renderers to be overriden in the settings (:pr:`857`)\n**FIXED:** Fixed redoc source mapping (:pr:`859`)\n\n**********\n**1.21.6**\n**********\n\n*Release date: Jun 15, 2023*\n\n**IMPROVED:** Remove required coreapi dependency (:pr:`854`)\n**IMPROVED:** Feature: Migrate to PyYAML for yaml generator (:pr:`845`)\n**FIXED:** Keep path parameters in their given order (:pr:`841`)\n**FIXED:** Provide support for enums in codecs (:pr:`837`)\n\n**********\n**1.21.5**\n**********\n\n*Release date: Feb 09, 2023*\n\n**ADDED:** Python 3.10 support  (:pr:`818`)\n**DEPRECATED:** Python 3.6 support as it's been deprecated by swagger-spec-validator\n**FIXED:** RecursiveField resolver (:pr:`822`)\n\n**********\n**1.21.4**\n**********\n\n*Release date: Sep 26, 2022*\n\n**FIXED:** Remove NullBooleanFields if the django-rest-framework version >= 3.14.0 (:pr:`814`)\n\n**********\n**1.21.3**\n**********\n\n*Release date: Jul 18, 2022*\n\n**FIXED:** Set generator url for swagger_settings.DEFAULT_API_URL (:pr:`682`)\n**FIXED:** Check fields for allow_null attribute (:pr:`688`)\n**FIXED:** Encode pytz object field as a string by default (:pr:`717`)\n**FIXED:** Handle errors rendering with TemplateHTMLRenderer (:pr:`742`)\n\n**********\n**1.21.2**\n**********\n\n*Release date: Jul 18, 2022*\n\n**FIXED:** Fixed code block rst syntax in ``README.rst``\n\n**********\n**1.21.1**\n**********\n\n*Release date: Jul 17, 2022*\n\n**FIXED:** Refer to permission_classes as a tuple (:pr:`678`)\n**IMPROVED:** Document drf-extra-fields base64 integration (:pr:`445`)\n**ADDED:** Added many support to example code (:pr:`695`)\n**ADDED:** Allow specifying response as a reference (:pr:`757`)\n**FIXED:** Fix old spelling errors and add a cspell configuration (:pr:`796`)\n**FIXED:** Remove universal wheel, python 2 is unsupported (:pr:`782`)\n**FIXED:** Fix duration field inspector (:pr:`549`)\n\n**********\n**1.21.0**\n**********\n\n*Release date: Jul 14, 2022*\n\n- **IMPROVED:** Add utf-8 support to yaml loaders (:pr:`692`)\n\n**********\n**1.20.3**\n**********\n\n*Release date: Jul 14, 2022*\n\n- **FIXED:** Source mapping in ``redoc.min.js`` (:pr:`778`)\n- **FIXED:** Publish action tag pattern in ``publish.yml`` (:pr:`794`)\n\n**********\n**1.20.0**\n**********\n\n*Release date: Oct 25, 2020*\n\n- **IMPROVED:** updated ``swagger-ui`` to version 3.36.0\n- **IMPROVED:** updated ``ReDoc`` to version 2.0.0-rc.40\n- **FIXED:** fixed compatibility with Django Rest Framework 3.12\n- **FIXED:** fixed compatibility with Python 3.9 typing generics\n- **FIXED:** dropped support for obsolete ``django.conf.settings.LOGOUT_URL`` (:pr:`646`)\n\n| **Support was dropped for Python 2.7, DRF 3.8, DRF 3.9.**\n| **Requirements are now: Python>=3.6, Django>=2.2, DRF>=3.10**\n\nThe 1.18 and 1.19 series was skipped to avoid confusion with the drf-yasg2 fork. I would also like to take this\nopportunity to extend my apologies to the community at large for the large gap in the maintenance of drf-yasg\nand the problems it has caused.\n\n**********\n**1.17.1**\n**********\n\n*Release date: Feb 17, 2020*\n\n- **FIXED:** fixed compatibility issue with CurrentUserDefault in Django Rest Framework 3.11\n- **FIXED:** respect `USERNAME_FIELD` in `generate_swagger` command (:pr:`486`)\n\n**Support was dropped for Python 3.5, Django 2.0, Django 2.1, DRF 3.7**\n\n**********\n**1.17.0**\n**********\n\n*Release date: Oct 03, 2019*\n\n- **ADDED:** added `JSONFieldInspector` for `JSONField` support (:pr:`417`)\n- **IMPROVED:** updated ``swagger-ui`` to version 3.23.11\n- **IMPROVED:** updated ``ReDoc`` to version 2.0.0-rc.14 (:issue:`398`)\n- **FIXED:** fixed a type hint support issue (:pr:`428`, :issue:`450`)\n- **FIXED:** fixed packaging issue caused by a missing requirement (:issue:`412`)\n\n**********\n**1.16.1**\n**********\n\n*Release date: Jul 16, 2019*\n\n- **IMPROVED:** better enum type detection for nested `ChoiceField`\\ s (:pr:`400`)\n- **FIXED:** fixed DRF 3.10 compatibility (:pr:`408`, :issue:`410`, :issue:`411`)\n\n**********\n**1.16.0**\n**********\n\n*Release date: Jun 13, 2019*\n\n- **ADDED:** added `reference_resolver_class` attribute hook to `SwaggerAutoSchema` (:pr:`350`)\n- **ADDED:** added `operation_keys` attribute to `SwaggerAutoSchema`, along with `__init__` parameter (:pr:`355`)\n- **FIXED:** fixed potential crash on `issubclass` check without `isclass` check\n\n**********\n**1.15.1**\n**********\n\n*Release date: Jun 13, 2019*\n\n- **IMPROVED:** updated ``swagger-ui`` to version 3.22.3\n- **IMPROVED:** updated ``ReDoc`` to version 2.0.0-rc.8-1\n- **FIXED:** fixed an issue with inspection of typing hints on Python 2.7 (:issue:`363`)\n- **FIXED:** fixed an issue with inspection of typing hints on Python 3.7 (:issue:`371`)\n\n**Python 3.4 support has been dropped!**\n\n**********\n**1.15.0**\n**********\n\n*Release date: Apr 01, 2019*\n\n- **ADDED:** added ``is_list_view`` and ``has_list_response`` extension points to ``SwaggerAutoSchema`` (:issue:`331`)\n- **IMPROVED:** updated ``swagger-ui`` to version 3.22.0\n- **IMPROVED:** updated ``ReDoc`` to version 2.0.0-rc.4\n- **FIXED:** ``ListModelMixin`` will now always be treated as a list view (:issue:`306`)\n- **FIXED:** non-primitive values in field ``choices`` will now be handled properly (:issue:`340`)\n\n**********\n**1.14.0**\n**********\n\n*Release date: Mar 04, 2019*\n\n- **IMPROVED:** updated ``swagger-ui`` to version 3.21.0\n- **FIXED:** implicit ``ref_name`` collisions will now throw an exception\n- **FIXED:** ``RecursiveField`` will now also work as a child of ``ListSerializer`` (:pr:`321`)\n- **FIXED:** fixed ``minLength`` and ``maxLength`` for ``ListSerializer`` and ``ListField``\n- **FIXED:** the ``items`` property of ``Schema``, ``Parameter`` and ``Items`` objects was renamed to ``items_``; this\n  is a *mildly breaking change* and was needed to fix the collision with the ``items`` method of ``dict`` (:pr:`308`)\n- **REMOVED:** the ``get_summary`` and ``get_description`` methods have been removed (previously deprecated in 1.12.0)\n\n**********\n**1.13.0**\n**********\n\n*Release date: Jan 29, 2019*\n\n- **IMPROVED:** type hint inspection is now supported for collections and ``Optional`` (:pr:`272`)\n- **IMPROVED:** updated ``swagger-ui`` to version 3.20.5\n- **IMPROVED:** updated ``ReDoc`` to version 2.0.0-rc.2\n- **DEPRECATED:** quietly dropped support for the ``flex`` validator; it will still work if the library is installed,\n  but the setup.py requirement was removed and the validator will be silently skipped if not installed (:issue:`285`)\n\n**********\n**1.12.1**\n**********\n\n*Release date: Dec 28, 2018*\n\n- **IMPROVED:** updated ``ReDoc`` to version 2.0.0-rc.0\n- **FIXED:** management command will now correctly fall back to ``DEFAULT_VERSION`` for mock request\n- **FIXED:** fixed bad \"raised exception during schema generation\" warnings caused by missing ``self`` parameter\n\n**********\n**1.12.0**\n**********\n\n*Release date: Dec 23, 2018*\n\n- **ADDED:** ``get_security_definitions`` and ``get_security_requirements`` hooks to ``OpenAPISchemaGenerator``\n- **ADDED:** added ``get_summary_and_description`` and ``split_summary_from_description`` extension points to\n  ``SwaggerAutoSchema`` to allow for better customization\n- **IMPROVED:** updated ``swagger-ui`` to version 3.20.4\n- **IMPROVED:** paginator ``next`` and ``previous`` fields are now marked as ``x-nullable`` (:issue:`263`)\n- **IMPROVED:** added the ``tags`` argument to ``swagger_auto_schema`` (:pr:`259`)\n- **IMPROVED:** type of ``enum`` will now be automatically detected from ``ChoiceField`` if all ``choices`` values\n  are objects of the same Python class (:pr:`264`)\n- **IMPROVED:** ``SwaggerValidationError`` details will now be logged and shown in the exception message\n- **FIXED:** user implementations of ``get_queryset``, ``get_parsers`` and ``get_renderers`` will no longer be bypassed\n- **FIXED:** fixed handling of lazy objects in user-supplied values\n- **FIXED:** ``read_only`` serializer fields will be correctly ignored when generating form parameters (:issue:`261`)\n- **FIXED:** fixed incorrect return type from ``UIRenderer`` (:pr:`268`)\n- **FIXED:** fixed inconsistent ordering of global ``securityDefinitions`` and ``security`` objects\n- **DEPRECATED:** the ``get_summary`` and ``get_description`` extension points have been deprecated in favor of the\n  new ``get_summary_and_description``, and will be removed in a future release\n\n**IMPORTANT PACKAGING NOTE**\n\nStarting with this version, the ``setup_requires`` argument was dropped from ``setup.py`` in favor of\n``build-system.requires`` in ``pyproject.toml`` . This means that for correctly building or installing from sdist,\nyou will need to use a PEP517/PEP518 compliant tool (tox>=3.3.0, setuptools>=40, pip>=10.0, pep517.build) or manually\ninstall the build requirements yourself (just ``setuptools`` and ``setuptools-scm``, for now).\n\nAdditionally, for correct package version detection, a full git checkout is required when building (this was always the\ncase). Building without ``.git`` or without ``setuptools-scm`` will result in a distribution with a version like\n``drf-yasg-1!0.0.0.dev0+noscm.00000167d19bd859``.\n\n**********\n**1.11.1**\n**********\n\n*Release date: Nov 29, 2018*\n\n- **IMPROVED:** updated ``swagger-ui`` to version 3.20.1\n- **IMPROVED:** updated ``ReDoc`` to version 2.0.0-alpha.41\n- **FIXED:** ``minLength`` and ``maxLength`` will now also work for ``ListSerializer`` in addition to ``ListField``\n- **FIXED:** ``MultipleChoiceField`` will now use the ``multi`` ``collectionFormat`` where appropriate (:issue:`257`)\n- **FIXED:** the ``format``, ``pattern``, ``enum``, ``min_length`` and ``max_length`` attributes of\n  ``coreschema.Schema`` will now be persisted into the converted ``openapi.Parameter`` (:issue:`212`, :pr:`233`)\n\n**********\n**1.11.0**\n**********\n\n*Release date: Oct 14, 2018*\n\n- **ADDED:** ``PERSIST_AUTH``, ``REFETCH_SCHEMA_WITH_AUTH``, ``REFETCH_SCHEMA_ON_LOGOUT``\n  settings and related javascript implementation for persisting authentication data to swagger-ui localStorage\n- **IMPROVED:** UI-enabled views will now no longer generate the full specification document twice; the HTML part\n  of the view will only generate a barebones ``Swagger`` object with no ``paths`` and ``definitions``\n- **IMPROVED:** added the ``FETCH_SCHEMA_WITH_QUERY`` setting to enable fetching of the schema document using\n  query parameters passed to the UI view (:issue:`208`)\n- **IMPROVED:** added support for the very common ``x-nullable`` extension (:issue:`217`)\n- **IMPROVED:** extensibility of some classes was improved by adding more extension points, together with more blocks\n  for ``swagger-ui.html``/``redoc.html`` and some JavaScript hooks in ``swagger-ui-init.js``\n- **FIXED:** removed usage of ``inspect.signature`` on python 2.7 (:issue:`222`)\n\n**********\n**1.10.2**\n**********\n\n*Release date: Sep 13, 2018*\n\n- **ADDED:** added the ``DISPLAY_OPERATION_ID`` ``swagger-ui`` setting\n- **IMPROVED:** updated ``ReDoc`` to version 2.0.0-alpha.38\n- **IMPROVED:** Operation summary will now be parsed from multi-line view method docstrings (:issue:`205`)\n- **IMPROVED:** ``pattern`` will now work on any field with a ``RegexValidator``\n  (would previously not appear on fields with special formats such as ``EmailField``)\n- **FIXED:** fixed an issue with ``RelatedFieldInspector`` handling of nested serializers\n- **FIXED:** fixed handling of ``reverse_lazy`` in URL settings (:issue:`209`)\n\n**********\n**1.10.1**\n**********\n\n*Release date: Sep 10, 2018*\n\n- **ADDED:** added the ``SPEC_URL`` setting for controlling the download link in ``swagger-ui`` and ``ReDoc``\n- **ADDED:** updated ``ReDoc`` settings (added ``NATIVE_SCROLLBARS`` and ``REQUIRED_PROPS_FIRST``)\n- **ADDED:** added ``extra_styles`` and ``extra_scripts`` blocks to ui templates (:issue:`178`)\n- **IMPROVED:** updated ``swagger-ui`` to version 3.18.2\n- **IMPROVED:** updated ``ReDoc`` to version 2.0.0-alpha.37\n- **FIXED:** stopped generating invalid OpenAPI by improper placement of ``readOnly`` Schemas\n- **FIXED:** fixed broken CSS when ``USE_SESSION_AUTH=False``\n- **FIXED:** fixed implementation of ``operation_summary`` and ``deprecated`` (:pr:`194`, :pr:`198`)\n- **FIXED:** fixed a bug related to nested ``typing`` hints (:pr:`195`)\n- **FIXED:** removed dependency on ``future`` (:issue:`196`)\n- **FIXED:** fixed exceptions logged for fields with ``default=None`` (:issue:`203`)\n- **FIXED:** fixed ``request_body=no_body`` handling and related tests (:issue:`188`, :issue:`199`)\n\n\n**********\n**1.10.0**\n**********\n\n*Release date: Aug 08, 2018*\n\n- **ADDED:** added ``EXCLUDED_MEDIA_TYPES`` setting for controlling ``produces`` MIME type filtering (:issue:`158`)\n- **ADDED:** added support for ``SerializerMethodField``, via the ``swagger_serializer_method`` decorator for the\n  method field, and support for Python 3.5 style type hinting of the method field return type\n  (:issue:`137`, :pr:`175`, :pr:`179`)\n\n  *NOTE:* in order for this to work, you will have to add the new ``drf_yasg.inspectors.SerializerMethodFieldInspector``\n  to your ``DEFAULT_FIELD_INSPECTORS`` array if you changed it from the default value\n\n- **IMPROVED:** updated ``swagger-ui`` to version 3.18.0\n- **IMPROVED:** added support for Python 3.7 and Django 2.1 (:pr:`176`)\n- **IMPROVED:** ``swagger_schema_fields`` will now also work on serializer ``Field``\\ s (:issue:`167`)\n- **IMPROVED:** ``ref_name`` collisions will now log a warning message (:issue:`156`)\n- **IMPROVED:** added ``operation_summary`` and ``deprecated`` arguments to ``swagger_auto_schema``\n  (:issue:`149`, :issue:`173`)\n- **FIXED:** made ``swagger_auto_schema`` work with DRF 3.9 ``@action`` mappings (:issue:`177`)\n\n*********\n**1.9.2**\n*********\n\n*Release date: Aug 03, 2018*\n\n- **IMPROVED:** updated ``swagger-ui`` to version 3.17.6\n- **IMPROVED:** updated ``ReDoc`` to version 2.0.0-alpha.32\n- **IMPROVED:** added ``--api-version`` argument to the ``generate_swagger`` management command (:pr:`170`)\n- **FIXED:** corrected various documentation typos (:pr:`160`, :pr:`162`, :issue:`171`, :pr:`172`)\n- **FIXED:** made ``generate_swagger`` work for projects without authentication (:pr:`161`)\n- **FIXED:** fixed ``SafeText`` interaction with YAML codec (:issue:`159`)\n\n*********\n**1.9.1**\n*********\n\n*Release date: Jun 30, 2018*\n\n- **IMPROVED:** added a ``swagger_fake_view`` marker to more easily detect mock views in view methods;\n  ``getattr(self, 'swagger_fake_view', False)`` inside a view method like ``get_serializer_class`` will tell you if the\n  view instance is being used for swagger schema introspection (:issue:`154`)\n- **IMPROVED:** updated ``swagger-ui`` to version 3.17.1\n- **IMPROVED:** updated ``ReDoc`` to version 2.0.0-alpha.25\n- **FIXED:** fixed wrong handling of duplicate urls in urlconf (:pr:`155`)\n- **FIXED:** fixed crash when passing ``None`` as a response override (:issue:`148`)\n\n*********\n**1.9.0**\n*********\n\n*Release date: Jun 16, 2018*\n\n- **ADDED:** added ``DEFAULT_GENERATOR_CLASS`` setting and ``--generator-class`` argument to the ``generate_swagger``\n  management command (:issue:`140`)\n- **FIXED:** fixed wrongly required ``'count'`` response field on ``CursorPagination`` (:issue:`141`)\n- **FIXED:** fixed some cases where ``swagger_schema_fields`` would not be handled (:pr:`142`)\n- **FIXED:** fixed crash when encountering ``coreapi.Fields``\\ s without a ``schema`` (:issue:`143`)\n\n*********\n**1.8.0**\n*********\n\n*Release date: Jun 01, 2018*\n\n- **ADDED:** added a :ref:`swagger_schema_fields <swagger_schema_fields>` field on serializer ``Meta`` classes for\n  customizing schema generation (:issue:`132`, :pr:`134`)\n- **FIXED:** error responses from schema views are now rendered with ``JSONRenderer`` instead of throwing\n  confusing errors (:pr:`130`, :issue:`58`)\n- **FIXED:** ``readOnly`` schema fields will now no longer be marked as ``required`` (:pr:`133`)\n\n*********\n**1.7.4**\n*********\n\n*Release date: May 14, 2018*\n\n- **IMPROVED:** updated ``swagger-ui`` to version 3.14.2\n- **IMPROVED:** updated ``ReDoc`` to version 2.0.0-alpha.20\n- **FIXED:** ignore ``None`` return from ``get_operation`` to avoid empty ``Path`` objects in output\n- **FIXED:** request body is now allowed on ``DELETE`` endpoints (:issue:`118`)\n\n*********\n**1.7.3**\n*********\n\n*Release date: May 12, 2018*\n\n- **FIXED:** views whose ``__init__`` methods throw exceptions will now be ignored during endpoint enumeration\n\n*********\n**1.7.2**\n*********\n\n*Release date: May 12, 2018*\n\n- **FIXED:** fixed generation of default ``SECURITY_REQUIREMENTS`` to match documented behavior\n- **FIXED:** ordering of ``SECURITY_REQUIREMENTS`` and ``SECURITY_DEFINITIONS`` is now stable\n\n*********\n**1.7.1**\n*********\n\n*Release date: May 05, 2018*\n\n- **IMPROVED:** updated ``swagger-ui`` to version 3.14.1\n- **IMPROVED:** set ``swagger-ui`` ``showCommonExtensions`` to ``True`` by default and add\n  ``SHOW_COMMON_EXTENSIONS`` setting key\n- **IMPROVED:** set ``min_length=1`` when ``allow_blank=False`` (:pr:`112`, thanks to :ghuser:`elnappo`)\n- **FIXED:** made documentation ordering of ``SwaggerDict`` extra attributes stable\n\n*********\n**1.7.0**\n*********\n\n*Release date: Apr 27, 2018*\n\n- **ADDED:** added integration with `djangorestframework-recursive <https://github.com/heywbj/django-rest-framework-recursive>`_\n  (:issue:`109`, :pr:`110`, thanks to :ghuser:`rsichny`)\n\n  *NOTE:* in order for this to work, you will have to add the new ``drf_yasg.inspectors.RecursiveFieldInspector`` to\n  your ``DEFAULT_FIELD_INSPECTORS`` array if you changed it from the default value\n\n- **FIXED:** ``SchemaRef`` now supports cyclical references via the ``ignore_unresolved`` argument\n\n*********\n**1.6.2**\n*********\n\n*Release date: Apr 25, 2018*\n\n- **IMPROVED:** updated ``swagger-ui`` to version 3.13.6\n- **IMPROVED:** switched ``ReDoc`` to version 2.0.0-alpha.17 (was 1.21.2); fixes :issue:`107`\n- **FIXED:** made documentation ordering of parameters stable for urls with multiple parameters (:issue:`105`, :pr:`106`)\n- **FIXED:** fixed crash when using a model ``ChoiceField`` of unknown child type\n\n*********\n**1.6.1**\n*********\n\n*Release date: Apr 01, 2018*\n\n- **ADDED:** added ``SUPPORTED_SUBMIT_METHODS`` ``swagger-ui`` setting\n\n*********\n**1.6.0**\n*********\n\n*Release date: Mar 24, 2018*\n\n- **IMPROVED:** ``OAUTH2_REDIRECT_URL`` will now default to the built in ``oauth2-redirect.html`` file\n\n*********\n**1.5.1**\n*********\n\n*Release date: Mar 18, 2018*\n\n- **IMPROVED:** updated ``swagger-ui`` to version 3.13.0\n- **FIXED:** fixed a crash caused by ``serializers.OneToOneRel`` (:pr:`81`, thanks to :ghuser:`ko-pp`)\n\n*********\n**1.5.0**\n*********\n\n*Release date: Mar 12, 2018*\n\n- **IMPROVED:** ``serializers.HiddenField`` are now hidden (:issue:`78`, :pr:`79`, thanks to :ghuser:`therefromhere`)\n\n  *NOTE:* in order for this to work, you will have to add the new ``drf_yasg.inspectors.HiddenFieldInspector`` to your\n  ``DEFAULT_FIELD_INSPECTORS`` array if you changed it from the default value\n\n- **IMPROVED:** type of model field is now detected for ``serializers.SlugRelatedField`` with ``read_only=True``\n  (:issue:`82`, :pr:`83`, thanks to :ghuser:`therefromhere`)\n\n*********\n**1.4.7**\n*********\n\n*Release date: Mar 05, 2018*\n\n- **FIXED:** prevent crashes caused by attempting to delete object attributes which do not exist in the first place\n  (:issue:`76`)\n\n*********\n**1.4.6**\n*********\n\n*Release date: Mar 05, 2018*\n\n- **IMPROVED:** updated ``swagger-ui`` to version 3.12.0\n- **IMPROVED:** updated ``ReDoc`` to version 1.21.2\n\n*********\n**1.4.5**\n*********\n\n*Release date: Mar 05, 2018*\n\n- **FIXED:** fixed an issue with modification of ``swagger_auto_schema`` arguments in-place during introspection, which\n  would sometimes cause an incomplete Swagger document to be generated after the first pass (:issue:`74`, :pr:`75`)\n\n*********\n**1.4.4**\n*********\n\n*Release date: Feb 26, 2018*\n\n- **IMPROVED:** ``type`` for ``ChoiceField`` generated by a ``ModelSerializer`` from a model field with ``choices=...``\n  will now be set according to the associated model field (:issue:`69`)\n- **FIXED:** ``lookup_field`` and ``lookup_value_regex`` on the same ``ViewSet``  will no longer trigger an exception\n  (:issue:`68`)\n\n*********\n**1.4.3**\n*********\n\n*Release date: Feb 22, 2018*\n\n- **FIXED:** added a missing assignment that would cause the ``default`` argument to ``openapi.Parameter.__init__`` to\n  be ignored\n\n*********\n**1.4.2**\n*********\n\n*Release date: Feb 22, 2018*\n\n- **FIXED:** fixed a bug that causes a ``ModelViewSet`` generated from models with nested ``ForeignKey`` to output\n  models named ``Nested`` into the ``definitions`` section (:issue:`59`, :pr:`65`)\n- **FIXED:** ``Response`` objects without a ``schema`` are now properly handled when passed through\n  ``swagger_auto_schema`` (:issue:`66`)\n\n*********\n**1.4.1**\n*********\n\n*Release date: Feb 21, 2018*\n\n- **FIXED:** the ``coerce_to_string`` is now respected when setting the type, default value and min/max values of\n  ``DecimalField`` in the OpenAPI schema (:issue:`62`)\n- **FIXED:** error responses from web UI views are now rendered with ``TemplateHTMLRenderer`` instead of throwing\n  confusing errors (:issue:`58`)\n- **IMPROVED:** updated ``swagger-ui`` to version 3.10.0\n- **IMPROVED:** updated ``ReDoc`` to version 1.21.0\n\n*********\n**1.4.0**\n*********\n\n*Release date: Feb 04, 2018*\n\n- **ADDED:** added settings for OAuth2 client configuration in ``swagger-ui`` (:issue:`53`)\n- **IMPROVED:** updated ``swagger-ui`` to version 3.9.3\n\n*********\n**1.3.1**\n*********\n\n*Release date: Jan 24, 2018*\n\n- **FIXED:** fixed a bug that would sometimes cause endpoints to wrongly be output as form operations (:issue:`50`)\n- **IMPROVED:** added generation of ``produces`` based on renderer classes\n- **IMPROVED:** added generation of top-level ``consumes`` and ``produces`` based on\n  ``DEFAULT_PARSER_CLASSES`` and ``DEFAULT_RENDERER_CLASSES`` (:issue:`48`)\n\n*********\n**1.3.0**\n*********\n\n*Release date: Jan 23, 2018*\n\n- **ADDED:** security requirements are now correctly set and can be customized; this should fix problems related\n  to authentication in ``swagger-ui`` Try it out!  (:issue:`50`, :pr:`54`)\n- **IMPROVED:** updated ``swagger-ui`` to version 3.9.2\n- **IMPROVED:** updated ``ReDoc`` to version 1.20.0\n- **FIXED:** fixed an exception caused by a warning in get_path_from_regex (:pr:`49`, thanks to :ghuser:`blueyed`)\n\n*********\n**1.2.2**\n*********\n\n*Release date: Jan 12, 2018*\n\n- **FIXED:** djangorestframework>=3.7.7 is now required because of breaking changes\n  (:issue:`44`, :pr:`45`, thanks to :ghuser:`h-hirokawa`)\n\n*********\n**1.2.1**\n*********\n\n*Release date: Jan 12, 2018*\n\n- Fixed deployment issues\n\n*********\n**1.2.0**\n*********\n\n*Release date: Jan 12, 2018 (missing from PyPI due to deployment issues)*\n\n- **ADDED:** ``basePath`` is now generated by taking into account the ``SCRIPT_NAME`` variable and the\n  longest common prefix of API urls (:issue:`37`, :pr:`42`)\n- **IMPROVED:** removed inline scripts and styles from bundled HTML templates to increase CSP compatibility\n- **IMPROVED:** improved validation errors and added more assertion sanity checks (:issue:`37`, :issue:`40`)\n- **IMPROVED:** improved handling of NamespaceVersioning by excluding endpoints of differing versions\n  (i.e. when accessing the schema view for v1, v2 endpoints will not be included in swagger)\n\n*********\n**1.1.3**\n*********\n\n*Release date: Jan 02, 2018*\n\n- **FIXED:** schema view cache will now always ``Vary`` on the ``Cookie`` and ``Authentication`` (the\n  ``Vary`` header was previously only added if ``public`` was set to ``True``) - this fixes issues related to Django\n  authentication in ``swagger-ui`` and ``CurrentUserDefault`` values in the schema\n\n*********\n**1.1.2**\n*********\n\n*Release date: Jan 01, 2018*\n\n- **IMPROVED:** updated ``swagger-ui`` to version 3.8.1\n- **IMPROVED:** removed some unneeded static files\n\n*********\n**1.1.1**\n*********\n\n*Release date: Dec 27, 2017*\n\n- **ADDED:** :ref:`generate_swagger management command <management-command>`\n  (:issue:`29`, :pr:`31`, thanks to :ghuser:`beaugunderson`)\n- **FIXED:** fixed improper generation of ``\\Z`` regex tokens - will now be replaced by ``$``\n\n*********\n**1.1.0**\n*********\n\n*Release date: Dec 27, 2017*\n\n- **ADDED:** added support for APIs versioned with ``URLPathVersioning`` or ``NamespaceVersioning``\n- **ADDED:** added ability to recursively customize schema generation\n  :ref:`using plugable inspector classes <custom-spec-inspectors>`\n- **ADDED:** added ``operation_id`` parameter to :func:`@swagger_auto_schema <.swagger_auto_schema>`\n- **ADDED:** integration with `djangorestframework-camel-case\n  <https://github.com/vbabiy/djangorestframework-camel-case>`_ (:issue:`28`)\n- **IMPROVED:** strings, arrays and integers will now have min/max validation attributes inferred from the\n  field-level validators\n- **FIXED:** fixed a bug that caused ``title`` to never be generated for Schemas; ``title`` is now correctly\n  populated from the field's ``label`` property\n\n*********\n**1.0.6**\n*********\n\n*Release date: Dec 23, 2017*\n\n- **FIXED:** Swagger UI \"Try it out!\" should now work with Django login\n- **FIXED:** callable ``default`` values on serializer fields will now be properly called (:pr:`24`, :issue:`25`)\n- **IMPROVED:** updated ``swagger-ui`` to version 3.8.0\n- **IMPROVED:** ``PrimaryKeyRelatedField`` and ``SlugRelatedField`` will now have\n  appropriate types based on the related model (:pr:`26`)\n- **IMPROVED:** mock views will now have a bound request even with ``public=False`` (:pr:`23`)\n\n*********\n**1.0.5**\n*********\n\n*Release date: Dec 18, 2017*\n\n- **FIXED:** fixed a crash caused by having read-only Serializers nested by reference\n- **FIXED:** removed erroneous backslashes in paths when routes are generated using Django 2\n  `path() <https://docs.djangoproject.com/en/2.0/ref/urls/#django.urls.path>`_\n- **IMPROVED:** updated ``swagger-ui`` to version 3.7.0\n- **IMPROVED:** ``FileField`` is now generated as an URL or file name in response Schemas\n  (:pr:`21`, thanks to :ghuser:`h-hirokawa`)\n\n*********\n**1.0.4**\n*********\n\n*Release date: Dec 16, 2017*\n\n- **FIXED:** fixed improper generation of YAML references\n- **ADDED:** added ``query_serializer`` parameter to\n  :func:`@swagger_auto_schema <.swagger_auto_schema>` (:issue:`16`, :pr:`17`)\n\n*********\n**1.0.3**\n*********\n\n*Release date: Dec 15, 2017*\n\n- **FIXED:** fixed bug that caused schema views returned from cache to fail (:issue:`14`)\n- **FIXED:** disabled automatic generation of response schemas for form operations to avoid confusing errors caused by\n  attempting to shove file parameters into Schema objects\n\n*********\n**1.0.2**\n*********\n\n*Release date: Dec 13, 2017*\n\n- First published version\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": "docs/changelog.rst", "language": "restructuredtext", "complexity": "complex", "star_count": 3515, "quality_score": 0.75, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 746, "word_count": 3393}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# drf-yasg documentation build configuration file, created by\n# sphinx-quickstart on Sun Dec 10 15:20:34 2017.\nimport inspect\nimport os\nimport re\nimport shutil\nimport sys\n\nfrom docutils import nodes, utils\nfrom docutils.parsers.rst import roles\nfrom docutils.parsers.rst.roles import set_classes\n\ntry:\n    from importlib import metadata\nexcept ImportError:  # Python < 3.8\n    import importlib_metadata as metadata\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\"sphinx.ext.autodoc\", \"sphinx.ext.viewcode\"]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = ['.rst', '.md']\nsource_suffix = \".rst\"\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# General information about the project.\nproject = \"drf-yasg\"\ncopyright = \"2018, Cristi V.\"\nauthor = \"Cristi V.\"\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n\n# The full version, including alpha/beta/rc tags.\nrelease = metadata.version(\"drf_yasg\")\nif \"noscm\" in release:\n    raise AssertionError(\n        \"Invalid package version string: %s. \\n\"\n        \"The documentation must be built with drf_yasg installed from a distribution \"\n        \"package, which must have been built with a proper version number (i.e. from \"\n        \"a full source checkout).\" % (release,)\n    )\n\n# The short X.Y.Z version.\nversion = \".\".join(release.split(\".\")[:3])\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = \"en\"\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\"]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"sphinx\"\n\nmodindex_common_prefix = [\"drf_yasg.\"]\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n# html_theme = 'default'\n\nhtml_theme = \"sphinx_rtd_theme\"\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    \"**\": [\n        \"relations.html\",  # needs 'show_related': True theme option to display\n        \"searchbox.html\",\n    ]\n}\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \"drf-yasgdoc\"\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \"drf-yasg.tex\", \"drf-yasg Documentation\", \"Cristi V.\", \"manual\"),\n]\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, \"drf-yasg\", \"drf-yasg Documentation\", [author], 1)]\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        \"drf-yasg\",\n        \"drf-yasg Documentation\",\n        author,\n        \"drf-yasg\",\n        \"One line description of project.\",\n        \"Miscellaneous\",\n    ),\n]\n\nautodoc_default_options = {\"private-members\": None}\nautodoc_member_order = \"bysource\"\nautoclass_content = \"both\"\nautodoc_mock_imports = []\n\nnitpick_ignore = [\n    (\"py:class\", \"object\"),\n    (\"py:class\", \"bool\"),\n    (\"py:class\", \"dict\"),\n    (\"py:class\", \"list\"),\n    (\"py:class\", \"str\"),\n    (\"py:class\", \"int\"),\n    (\"py:class\", \"bytes\"),\n    (\"py:class\", \"tuple\"),\n    (\"py:class\", \"function\"),\n    (\"py:class\", \"type\"),\n    (\"py:class\", \"OrderedDict\"),\n    (\"py:class\", \"None\"),\n    (\"py:obj\", \"None\"),\n    (\"py:class\", \"Exception\"),\n    (\"py:class\", \"collections.OrderedDict\"),\n    (\"py:class\", \"yaml.CSafeDumper\"),\n    (\"py:class\", \"rest_framework.serializers.Serializer\"),\n    (\"py:class\", \"rest_framework.renderers.BaseRenderer\"),\n    (\"py:class\", \"rest_framework.parsers.BaseParser\"),\n    (\"py:class\", \"rest_framework.schemas.generators.EndpointEnumerator\"),\n    (\"py:class\", \"rest_framework.views.APIView\"),\n    (\"py:class\", \"OpenAPICodecYaml\"),\n    (\"py:class\", \"OpenAPICodecJson\"),\n    (\"py:class\", \"OpenAPISchemaGenerator\"),\n    (\"py:class\", \"coreapi.Field\"),\n    (\"py:class\", \"BaseFilterBackend\"),\n    (\"py:class\", \"BasePagination\"),\n    (\"py:class\", \"Request\"),\n    (\"py:class\", \"rest_framework.request.Request\"),\n    (\"py:class\", \"rest_framework.serializers.Field\"),\n    (\"py:class\", \"serializers.Field\"),\n    (\"py:class\", \"serializers.BaseSerializer\"),\n    (\"py:class\", \"Serializer\"),\n    (\"py:class\", \"BaseSerializer\"),\n    (\"py:class\", \"APIView\"),\n]\n\n# even though the package should be already installed, the sphinx build on RTD\n# for some reason needs the sources dir to be in the path in order for viewcode to work\nsys.path.insert(0, os.path.abspath(\"../src\"))\n\n# activate the Django testproj to be able to successfully import drf_yasg\nsys.path.insert(0, os.path.abspath(\"../testproj\"))\nos.putenv(\"DJANGO_SETTINGS_MODULE\", \"testproj.settings.local\")\n\nfrom django.conf import settings  # noqa: E402\n\nsettings.configure()\n\n# instantiate a SchemaView in the views module to make it available to autodoc\nimport drf_yasg.views  # noqa: E402\n\ndrf_yasg.views.SchemaView = drf_yasg.views.get_schema_view(None)\n\n# monkey patch to stop sphinx from trying to find classes by their real location instead\n# of the top-level __init__ alias; this allows us to document only `drf_yasg.inspectors`\n# and avoid broken references or double documenting\n\nimport drf_yasg.inspectors  # noqa: E402\n\n\ndef redirect_cls(cls):\n    if cls.__module__.startswith(\"drf_yasg.inspectors\"):\n        return getattr(drf_yasg.inspectors, cls.__name__)\n    return cls\n\n\nfor cls_name in drf_yasg.inspectors.__all__:\n    # first pass - replace all classes' module with the top level module\n    real_cls = getattr(drf_yasg.inspectors, cls_name)\n    if not inspect.isclass(real_cls):\n        continue\n\n    patched_dict = dict(real_cls.__dict__)\n    patched_dict.update({\"__module__\": \"drf_yasg.inspectors\"})\n    patched_cls = type(cls_name, real_cls.__bases__, patched_dict)\n    setattr(drf_yasg.inspectors, cls_name, patched_cls)\n\nfor cls_name in drf_yasg.inspectors.__all__:\n    # second pass - replace the inheritance bases for all classes to point to the new\n    # clean classes\n    real_cls = getattr(drf_yasg.inspectors, cls_name)\n    if not inspect.isclass(real_cls):\n        continue\n\n    patched_bases = tuple(redirect_cls(base) for base in real_cls.__bases__)\n    patched_cls = type(cls_name, patched_bases, dict(real_cls.__dict__))\n    setattr(drf_yasg.inspectors, cls_name, patched_cls)\n\n# custom interpreted role for linking to GitHub issues and pull requests\n# use as :issue:`14` or :pr:`17`\ngh_issue_uri = \"https://github.com/axnsan12/drf-yasg/issues/{}\"\ngh_pr_uri = \"https://github.com/axnsan12/drf-yasg/pull/{}\"\ngh_user_uri = \"https://github.com/{}\"\n\nsuppress_warnings = [\"image.not_readable\"]\n\n\ndef sphinx_err(inliner, lineno, rawtext, msg):\n    msg = inliner.reporter.error(msg, line=lineno)\n    prb = inliner.problematic(rawtext, rawtext, msg)\n    return [prb], [msg]\n\n\ndef sphinx_ref(options, rawtext, text, ref):\n    set_classes(options)\n    node = nodes.reference(rawtext, text, refuri=ref, **options)\n    return [node], []\n\n\ndef role_github_user(name, rawtext, text, lineno, inliner, options=None, content=None):\n    options = options or {}\n    content = content or []\n\n    if not re.match(r\"^[a-z\\d](?:[a-z\\d]|-(?=[a-z\\d])){0,38}$\", text, re.IGNORECASE):\n        return sphinx_err(\n            inliner, lineno, rawtext, '\"%s\" is not a valid GitHub username.' % text\n        )\n\n    ref = gh_user_uri.format(text)\n    text = \"@\" + utils.unescape(text)\n    return sphinx_ref(options, rawtext, text, ref)\n\n\ndef role_github_pull_request_or_issue(\n    name, rawtext, text, lineno, inliner, options=None, content=None\n):\n    options = options or {}\n    content = content or []\n    try:\n        if int(text) <= 0:\n            raise ValueError\n    except ValueError:\n        return sphinx_err(\n            inliner,\n            lineno,\n            rawtext,\n            \"GitHub pull request or issue number must be a number greater than or equal\"\n            ' to 1; \"%s\" is invalid.' % text,\n        )\n\n    if name == \"pr\":\n        ref = gh_pr_uri\n    elif name == \"issue\":\n        ref = gh_issue_uri\n    else:\n        return sphinx_err(\n            inliner,\n            lineno,\n            rawtext,\n            'unknown role name for GitHub reference - \"%s\"' % name,\n        )\n\n    ref = ref.format(text)\n    text = \"#\" + utils.unescape(text)\n    return sphinx_ref(options, rawtext, text, ref)\n\n\ndef copy_static(app, build):\n    shutil.copytree(\"docs/images\", \"docs/_build/html/docs/images\", dirs_exist_ok=True)\n\n\nroles.register_local_role(\"pr\", role_github_pull_request_or_issue)\nroles.register_local_role(\"issue\", role_github_pull_request_or_issue)\nroles.register_local_role(\"ghuser\", role_github_user)\n\n\ndef setup(app):\n    app.add_css_file(\"css/style.css\")\n    app.connect(\"build-finished\", copy_static)\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": "docs/conf.py", "language": "python", "complexity": "complex", "star_count": 3515, "quality_score": 0.8099999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 343, "word_count": 1254}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": ".. |br| raw:: html\n\n   <br />\n\n########################\nCustom schema generation\n########################\n\nIf the default spec generation does not quite match what you were hoping to achieve, ``drf-yasg`` provides some\ncustom behavior hooks by default.\n\n.. _custom-spec-excluding-endpoints:\n\n*******************\nExcluding endpoints\n*******************\n\nYou can prevent a view from being included in the Swagger view by setting its class-level ``swagger_schema``\nattribute to ``None``, or you can prevent an operation from being included by setting its ``auto_schema`` override\nto none in :ref:`@swagger_auto_schema <custom-spec-swagger-auto-schema>`:\n\n.. code-block:: python\n\n   class UserList(APIView):\n      swagger_schema = None\n\n      # all methods of the UserList class will be excluded\n      ...\n\n   # only the GET method will be shown in Swagger\n   @swagger_auto_schema(method='put', auto_schema=None)\n   @swagger_auto_schema(methods=['get'], ...)\n   @api_view(['GET', 'PUT'])\n   def user_detail(request, pk):\n      pass\n\n.. _custom-spec-swagger-auto-schema:\n\n**************************************\nThe ``@swagger_auto_schema`` decorator\n**************************************\n\nYou can use the :func:`@swagger_auto_schema <.swagger_auto_schema>` decorator on view functions to override\nsome properties of the generated :class:`.Operation`. For example, in a ``ViewSet``,\n\n.. code-block:: python\n\n   from drf_yasg.utils import swagger_auto_schema\n\n   @swagger_auto_schema(operation_description=\"partial_update description override\", responses={404: 'slug not found'})\n   def partial_update(self, request, *args, **kwargs):\n      \"\"\"partial_update method docstring\"\"\"\n      ...\n\nwill override the description of the ``PATCH /article/{id}/`` operation, and document a 404 response with no body and\nthe given description.\n\nWhere you can use the :func:`@swagger_auto_schema <.swagger_auto_schema>` decorator depends on the type of your view:\n\n* for function based ``@api_view``\\ s, because the same view can handle multiple methods, and thus represent multiple\n  operations, you have to add the decorator multiple times if you want to override different operations:\n\n   .. code-block:: python\n   \n      from drf_yasg import openapi\n      from drf_yasg.utils import swagger_auto_schema\n      from rest_framework.decorators import api_view\n\n      from drf_yasg import openapi\n\n      test_param = openapi.Parameter('test', openapi.IN_QUERY, description=\"test manual param\", type=openapi.TYPE_BOOLEAN)\n      user_response = openapi.Response('response description', UserSerializer)\n\n      # 'method' can be used to customize a single HTTP method of a view\n      @swagger_auto_schema(method='get', manual_parameters=[test_param], responses={200: user_response})\n      # 'methods' can be used to apply the same modification to multiple methods\n      @swagger_auto_schema(methods=['put', 'post'], request_body=UserSerializer)\n      @api_view(['GET', 'PUT', 'POST'])\n      def user_detail(request, pk):\n          ...\n\n* for class based ``APIView``, ``GenericAPIView`` and non-``ViewSet`` derivatives, you have to decorate the respective\n  method of each operation:\n\n   .. code-block:: python\n\n      class UserList(APIView):\n         @swagger_auto_schema(responses={200: UserSerializer(many=True)})\n         def get(self, request):\n            ...\n\n         @swagger_auto_schema(operation_description=\"description\")\n         def post(self, request):\n            ...\n\n* for ``ViewSet``, ``GenericViewSet``, ``ModelViewSet``, because each viewset corresponds to multiple **paths**, you have\n  to decorate the *action methods*, i.e. ``list``, ``create``, ``retrieve``, etc. |br|\n  Additionally, ``@action``\\ s defined on the viewset, like function based api views, can respond to multiple HTTP\n  methods and thus have multiple operations that must be decorated separately:\n\n\n   .. code-block:: python\n\n      class ArticleViewSet(viewsets.ModelViewSet):\n         # method or 'methods' can be skipped because the action only handles a single method (GET)\n         @swagger_auto_schema(operation_description='GET /articles/today/')\n         @action(detail=False, methods=['get'])\n         def today(self, request):\n            ...\n\n         @swagger_auto_schema(method='get', operation_description=\"GET /articles/{id}/image/\")\n         @swagger_auto_schema(method='post', operation_description=\"POST /articles/{id}/image/\")\n         @action(detail=True, methods=['get', 'post'], parser_classes=(MultiPartParser,))\n         def image(self, request, id=None):\n            ...\n\n         @swagger_auto_schema(operation_description=\"PUT /articles/{id}/\")\n         def update(self, request, *args, **kwargs):\n            ...\n\n         @swagger_auto_schema(operation_description=\"PATCH /articles/{id}/\")\n         def partial_update(self, request, *args, **kwargs):\n            ...\n\n.. Tip::\n\n   If you want to customize the generation of a method you are not implementing yourself, you can use\n   ``swagger_auto_schema`` in combination with Django's ``method_decorator``:\n\n   .. code-block:: python\n\n      @method_decorator(name='list', decorator=swagger_auto_schema(\n          operation_description=\"description from swagger_auto_schema via method_decorator\"\n      ))\n      class ArticleViewSet(viewsets.ModelViewSet):\n         ...\n\n   This allows you to avoid unnecessarily overriding the method.\n\n.. Tip::\n\n   You can go even further and directly decorate the result of ``as_view``, in the same manner you would\n   override an ``@api_view`` as described above:\n\n   .. code-block:: python\n\n      decorated_login_view = \\\n         swagger_auto_schema(\n            method='post',\n            responses={status.HTTP_200_OK: LoginResponseSerializer}\n         )(LoginView.as_view())\n\n      urlpatterns = [\n         ...\n         url(r'^login/$', decorated_login_view, name='login')\n      ]\n\n   This can allow you to avoid skipping an unnecessary *subclass* altogether.\n\n.. Warning::\n\n   However, do note that both of the methods above can lead to unexpected (and maybe surprising) results by\n   replacing/decorating methods on the base class itself.\n\n\n*********************************\nSupport for SerializerMethodField\n*********************************\n\nSchema generation of ``serializers.SerializerMethodField`` is supported in two ways:\n\n1) The :func:`swagger_serializer_method <.swagger_serializer_method>` decorator for the use case where the serializer\n   method is using a serializer. e.g.:\n\n   .. code-block:: python\n\n      from drf_yasg.utils import swagger_serializer_method\n\n      class OtherStuffSerializer(serializers.Serializer):\n          foo = serializers.CharField()\n\n      class ParentSerializer(serializers.Serializer):\n          other_stuff = serializers.SerializerMethodField()\n          many_other_stuff = serializers.SerializerMethodField()\n          \n          @swagger_serializer_method(serializer_or_field=OtherStuffSerializer)\n          def get_other_stuff(self, obj):\n              return OtherStuffSerializer().data\n          \n          @swagger_serializer_method(serializer_or_field=OtherStuffSerializer(many=True))\n          def get_many_other_stuff(self, obj):\n              return OtherStuffSerializer().data\n\n\n   Note that the ``serializer_or_field`` parameter can accept either a subclass or an instance of ``serializers.Field``.\n   \n   \n\n2) For simple cases where the method is returning one of the supported types, `Python 3 type hinting`_ of the\n   serializer method return value can be used. e.g.:\n\n   .. code-block:: python\n\n      class SomeSerializer(serializers.Serializer):\n          some_number = serializers.SerializerMethodField()\n\n          def get_some_number(self, obj) -> float:\n              return 1.0\n\n   When return type hinting is not supported, the equivalent ``serializers.Field`` subclass can be used with\n   :func:`swagger_serializer_method <.swagger_serializer_method>`:\n\n   .. code-block:: python\n\n      class SomeSerializer(serializers.Serializer):\n          some_number = serializers.SerializerMethodField()\n\n          @swagger_serializer_method(serializer_or_field=serializers.FloatField)\n          def get_some_number(self, obj):\n              return 1.0\n\n\n********************************\nSerializer ``Meta`` nested class\n********************************\n\nYou can define some per-serializer or per-field options by adding a ``Meta`` class to your ``Serializer`` or\nserializer ``Field``, e.g.:\n\n.. code-block:: python\n\n   class WhateverSerializer(Serializer):\n      ...\n\n      class Meta:\n         ... options here ...\n\n.. _swagger_schema_fields:\n\nThe available options are:\n\n   * ``ref_name`` - a string which will be used as the model definition name for this serializer class; setting it to\n     ``None`` will force the serializer to be generated as an inline model everywhere it is used. If two serializers\n     have the same ``ref_name``, both their usages will be replaced with a reference to the same definition.\n     If this option is not specified, all serializers have an implicit name derived from their class name, minus any\n     ``Serializer`` suffix (e.g. ``UserSerializer`` -> ``User``, ``SerializerWithSuffix`` -> ``SerializerWithSuffix``)\n   * ``swagger_schema_fields`` - a dictionary mapping :class:`.Schema` field names to values. These attributes\n     will be set on the :class:`.Schema` object generated from the ``Serializer``. Field names must be python values,\n     which are converted to Swagger ``Schema`` attribute names according to :func:`.make_swagger_name`.\n     Attribute names and values must conform to the `OpenAPI 2.0 specification <https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#schemaObject>`_.\n\nSuppose you wanted to model an email using a `JSONField` to store the subject and body for performance reasons:\n\n.. code-block:: python\n\n   from django.contrib.postgres.fields import JSONField\n\n   class Email(models.Model):\n       # Store data as JSON, but the data should be made up of\n       # an object that has two properties, \"subject\" and \"body\"\n       # Example:\n       # {\n       #   \"subject\": \"My Title\",\n       #   \"body\": \"The body of the message.\",\n       # }\n       message = JSONField()\n\nTo instruct ``drf-yasg`` to output an OpenAPI schema that matches this, create a custom ``JSONField``:\n\n.. code-block:: python\n\n   class EmailMessageField(serializers.JSONField):\n       class Meta:\n           swagger_schema_fields = {\n               \"type\": openapi.TYPE_OBJECT,\n               \"title\": \"Email\",\n               \"properties\": {\n                   \"subject\": openapi.Schema(\n                       title=\"Email subject\",\n                       type=openapi.TYPE_STRING,\n                   ),\n                   \"body\": openapi.Schema(\n                       title=\"Email body\",\n                       type=openapi.TYPE_STRING,\n                   ),\n               },\n               \"required\": [\"subject\", \"body\"],\n            }\n\n   class EmailSerializer(ModelSerializer):\n       class Meta:\n           model = Email\n           fields = \"__all__\"\n\n       message = EmailMessageField()\n\n.. Warning::\n\n   Overriding a default ``Field`` generated by a ``ModelSerializer`` will also override automatically\n   generated validators for that ``Field``.  To add ``Serializer`` validation back in manually, see the relevant\n   `DRF Validators`_ and `DRF Fields`_ documentation.\n\n   One example way to do this is to set the ``default_validators`` attribute on a field.\n\n   .. code-block:: python\n\n      class EmailMessageField(serializers.JSONField):\n         default_validators = [my_custom_email_validator]\n         ...\n\n*************************\nSubclassing and extending\n*************************\n\n\n---------------------\n``SwaggerAutoSchema``\n---------------------\n\nFor more advanced control you can subclass :class:`~.inspectors.SwaggerAutoSchema` - see the documentation page\nfor a list of methods you can override.\n\nYou can put your custom subclass to use by setting it on a view method using the\n:ref:`@swagger_auto_schema <custom-spec-swagger-auto-schema>` decorator described above, by setting it as a\nclass-level attribute named ``swagger_schema`` on the view class, or\n:ref:`globally via settings <default-class-settings>`.\n\nFor example, to generate all operation IDs as camel case, you could do:\n\n.. code-block:: python\n\n   from inflection import camelize\n\n   class CamelCaseOperationIDAutoSchema(SwaggerAutoSchema):\n      def get_operation_id(self, operation_keys):\n         operation_id = super(CamelCaseOperationIDAutoSchema, self).get_operation_id(operation_keys)\n         return camelize(operation_id, uppercase_first_letter=False)\n\n\n   SWAGGER_SETTINGS = {\n      'DEFAULT_AUTO_SCHEMA_CLASS': 'path.to.CamelCaseOperationIDAutoSchema',\n      ...\n   }\n\n--------------------------\n``OpenAPISchemaGenerator``\n--------------------------\n\nIf you need to control things at a higher level than :class:`.Operation` objects (e.g. overall document structure,\nvendor extensions in metadata) you can also subclass :class:`.OpenAPISchemaGenerator` - again, see the documentation\npage for a list of its methods.\n\nThis custom generator can be put to use by setting it as the :attr:`.generator_class` of a :class:`.SchemaView` using\n:func:`.get_schema_view`.\n\n.. _custom-spec-inspectors:\n\n---------------------\n``Inspector`` classes\n---------------------\n\nFor customizing behavior related to specific field, serializer, filter or paginator classes you can implement the\n:class:`~.inspectors.FieldInspector`, :class:`~.inspectors.SerializerInspector`, :class:`~.inspectors.FilterInspector`,\n:class:`~.inspectors.PaginatorInspector` classes and use them with\n:ref:`@swagger_auto_schema <custom-spec-swagger-auto-schema>` or one of the\n:ref:`related settings <default-class-settings>`.\n\nA :class:`~.inspectors.FilterInspector` that adds a description to all ``DjangoFilterBackend`` parameters could be\nimplemented like so:\n\n.. code-block:: python\n\n   class DjangoFilterDescriptionInspector(CoreAPICompatInspector):\n      def get_filter_parameters(self, filter_backend):\n         if isinstance(filter_backend, DjangoFilterBackend):\n            result = super(DjangoFilterDescriptionInspector, self).get_filter_parameters(filter_backend)\n            for param in result:\n               if not param.get('description', ''):\n                  param.description = \"Filter the returned list by {field_name}\".format(field_name=param.name)\n\n            return result\n\n         return NotHandled\n\n   @method_decorator(name='list', decorator=swagger_auto_schema(\n      filter_inspectors=[DjangoFilterDescriptionInspector]\n   ))\n   class ArticleViewSet(viewsets.ModelViewSet):\n      filter_backends = (DjangoFilterBackend,)\n      filterset_fields = ('title',)\n      ...\n\n\nA second example, of a :class:`~.inspectors.FieldInspector` that removes the ``title`` attribute from all generated\n:class:`.Schema` objects:\n\n.. code-block:: python\n\n   from drf_yasg.inspectors import FieldInspector\n\n   class NoSchemaTitleInspector(FieldInspector):\n      def process_result(self, result, method_name, obj, **kwargs):\n         # remove the `title` attribute of all Schema objects\n         if isinstance(result, openapi.Schema.OR_REF):\n            # traverse any references and alter the Schema object in place\n            schema = openapi.resolve_ref(result, self.components)\n            schema.pop('title', None)\n\n            # no ``return schema`` here, because it would mean we always generate\n            # an inline `object` instead of a definition reference\n\n         # return back the same object that we got - i.e. a reference if we got a reference\n         return result\n\n\n   class NoTitleAutoSchema(SwaggerAutoSchema):\n      field_inspectors = [NoSchemaTitleInspector] + swagger_settings.DEFAULT_FIELD_INSPECTORS\n\n   class ArticleViewSet(viewsets.ModelViewSet):\n      swagger_schema = NoTitleAutoSchema\n      ...\n\n\n.. Note::\n\n   A note on references - :class:`.Schema` objects are sometimes output by reference (:class:`.SchemaRef`); in fact,\n   that is how named models are implemented in OpenAPI:\n\n      - in the output swagger document there is a ``definitions`` section containing :class:`.Schema` objects for all\n        models\n      - every usage of a model refers to that single :class:`.Schema` object - for example, in the ArticleViewSet\n        above, all requests and responses containing an ``Article`` model would refer to the same schema definition by a\n        ``'$ref': '#/definitions/Article'``\n\n   This is implemented by only generating **one** :class:`.Schema` object for every serializer **class** encountered.\n\n   This means that you should generally avoid view or method-specific ``FieldInspector``\\ s if you are dealing with\n   references (a.k.a named models), because you can never know which view will be the first to generate the schema\n   for a given serializer.\n\n   **IMPORTANT:** nested fields on ``ModelSerializer``\\ s that are generated from model ``ForeignKeys`` will always be\n   output by value. If you want the by-reference behavior you have to explicitly set the serializer class of nested\n   fields instead of letting ``ModelSerializer`` generate one automatically; for example:\n\n   .. code-block:: python\n\n      class OneSerializer(serializers.ModelSerializer):\n         class Meta:\n            model = SomeModel\n            fields = ('id',)\n\n\n      class AnotherSerializer(serializers.ModelSerializer):\n         child = OneSerializer()\n\n         class Meta:\n            model = SomeParentModel\n            fields = ('id', 'child')\n\n   Another caveat that stems from this is that any serializer named \"``NestedSerializer``\" will be forced inline\n   unless it has a ``ref_name`` set explicitly.\n\n\n.. _Python 3 type hinting: https://docs.python.org/3/library/typing.html\n.. _DRF Validators: https://www.django-rest-framework.org/api-guide/validators/\n.. _DRF Fields: https://www.django-rest-framework.org/api-guide/fields/#validators\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": "docs/custom_spec.rst", "language": "restructuredtext", "complexity": "complex", "star_count": 3515, "quality_score": 0.8099999999999999, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 467, "word_count": 1748}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "######################\nCustomizing the web UI\n######################\n\nThe web UI can be customized using the settings available in :ref:`swagger-ui-settings` and :ref:`redoc-ui-settings`.\n\nYou can also extend one of the `drf-yasg/swagger-ui.html`_ or `drf-yasg/redoc.html`_ templates that are used for\nrendering. See the template source code (linked above) for a complete list of customizable blocks.\n\nThe ``swagger-ui`` view has some quite involved JavaScript hooks used for some functionality, which you might also\nwant to review at `drf-yasg/swagger-ui-init.js`_.\n\n.. _drf-yasg/swagger-ui.html: https://github.com/axnsan12/drf-yasg/blob/master/src/drf_yasg/templates/drf-yasg/swagger-ui.html\n.. _drf-yasg/swagger-ui-init.js: https://github.com/axnsan12/drf-yasg/blob/master/src/drf_yasg/static/drf-yasg/swagger-ui-init.js\n.. _drf-yasg/redoc.html: https://github.com/axnsan12/drf-yasg/blob/master/src/drf_yasg/templates/drf-yasg/redoc.html\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": "docs/custom_ui.rst", "language": "restructuredtext", "complexity": "simple", "star_count": 3515, "quality_score": 0.63, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 16, "word_count": 81}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "drf\\_yasg package\n====================\n\ndrf\\_yasg\\.codecs\n---------------------------\n\n.. automodule:: drf_yasg.codecs\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :exclude-members: SaneYamlDumper,SaneYamlLoader\n\ndrf\\_yasg\\.errors\n---------------------------\n\n.. automodule:: drf_yasg.errors\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\ndrf\\_yasg\\.generators\n-------------------------------\n\n.. automodule:: drf_yasg.generators\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\ndrf\\_yasg\\.inspectors\n-------------------------------\n\n.. autodata:: drf_yasg.inspectors.NotHandled\n\n.. automodule:: drf_yasg.inspectors\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\ndrf\\_yasg\\.middleware\n-------------------------------\n\n.. automodule:: drf_yasg.middleware\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\ndrf\\_yasg\\.openapi\n----------------------------\n\n.. automodule:: drf_yasg.openapi\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :exclude-members: _bare_SwaggerDict\n\ndrf\\_yasg\\.renderers\n------------------------------\n\n.. automodule:: drf_yasg.renderers\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\ndrf\\_yasg\\.utils\n--------------------------\n\n.. automodule:: drf_yasg.utils\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\ndrf\\_yasg\\.views\n--------------------------\n\n.. automodule:: drf_yasg.views\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. |br| raw:: html\n\n   <br />\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": "docs/drf_yasg.rst", "language": "restructuredtext", "complexity": "medium", "star_count": 3515, "quality_score": 0.6, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": false, "has_images": false, "has_tables": true, "has_headers": false, "line_count": 83, "word_count": 88}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas using OpenAPI/Swagger specification\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": ".. drf-yasg documentation master file, created by\n   sphinx-quickstart on Sun Dec 10 15:20:34 2017.\n   You can adapt this file completely to your liking, but it should at least\n   contain the root `toctree` directive.\n\ndrf-yasg\n===========\n\n.. toctree::\n   :maxdepth: 2\n   :caption: Table of contents:\n\n   readme.rst\n   rendering.rst\n   openapi.rst\n   security.rst\n   custom_spec.rst\n   custom_ui.rst\n   settings.rst\n   contributing.rst\n   license.rst\n   changelog.rst\n\nSource code documentation\n=========================\n\n* :ref:`genindex`\n* :ref:`modindex`\n* :ref:`search`\n\n.. toctree::\n   :maxdepth: 2\n\n   drf_yasg.rst\n", "metadata": {"source": "github.com/axnsan12/drf-yasg", "file_path": "docs/index.rst", "language": "restructuredtext", "complexity": "simple", "star_count": 3515, "quality_score": 0.63, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 35, "word_count": 68}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n## Table of Contents\n\n- [Antipatterns](#antipatterns)\n  - [Strict email validation](#strict-email-validation)\n  - [Late returns](#late-returns)\n  - [Hacks comment](#hacks-comment)\n  - [Repeating arguments in function name](#repeating-arguments-in-function-name)\n  - [Repeating class name in method name](#repeating-class-name-in-method-name)\n  - [Repeating function name in docstring](#repeating-function-name-in-docstring)\n  - [Unreadable response construction](#unreadable-response-construction)\n  - [Undeterministic tests](#undeterministic-tests)\n  - [Unbalanced boilerplate](#unbalanced-boilerplate)\n  - [Inconsistent use of verbs in functions](#inconsistent-use-of-verbs-in-functions)\n  - [Opaque function arguments](#opaque-function-arguments)\n  - [Hiding formatting](#hiding-formatting)\n  - [Returning nothing instead of raising NotFound exception](#returning-nothing-instead-of-raising-notfound-exception)\n  - [Having a library that contains all utils](#having-a-library-that-contains-all-utils)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n# Antipatterns\n\nMost of those are antipatterns in the Python programming language, but some of\nthem might be more generic.\n\n## Strict email validation\n\nIt is almost impossible to strictly validate an email. Even if you were writing\nor using a regex that follows\n[RFC5322](http://tools.ietf.org/html/rfc5322#section-3.4), you would have false\npositives when trying to validate actual emails that don't follow the RFC.\n\nWhat's more, validating an email provides very weak guarantees. A stronger,\nmore meaningful validation would be to send an email and validate that the user\nreceived it.\n\nTo sum up, don't waste your time trying to validate an email if you don't need\nto (or just check that there's a `@` in it). If you need to, send an email with\na token and validate that the user received it.\n\n## Late returns\n\nReturning early reduces cognitive overhead, and improve readability by killing\nindentation levels.\n\nBad:\n\n```python\ndef toast(bread):\n    if bread.kind != 'brioche':\n        if not bread.is_stale:\n            toaster.toast(bread)\n```\n\nGood:\n\n```python\ndef toast(bread):\n    if bread.kind == 'brioche' or bread.is_stale:\n        return\n\n    toaster.toast(bread)\n```\n\n## Hacks comment\n\nBad:\n\n```python\n# Gigantic hack (written by Louis de Funes) 04-01-2015\ntoaster.restart()\n```\n\nThere's multiple things wrong with this comment:\n\n- Even if it is actually a hack, no need to say it in a comment. It lowers the\n  perceived quality of a codebase and impacts developer motivation.\n- Putting the author and the date is totally useless when using source control\n  (`git blame`).\n- This does not explain why it's temporary.\n- It's impossible to easily grep for temporary fixes.\n- [Louis de Fun√®s](https://en.wikipedia.org/wiki/Louis_de_Fun%C3%A8s) would never\n  write a hack.\n\nGood:\n\n```python\n# Need to restart toaster to prevent burning bread\n# TODO: replace with proper fix\ntoaster.restart()\n```\n\n- This clearly explains the nature of the temporary fix.\n- Using `TODO` is an ubiquitous pattern that allows easy grepping and plays\n  nice with most text editors.\n- The perceived quality of this temporary fix is much higher.\n\n## Repeating arguments in function name\n\nBad:\n\n```python\ndef get_by_color(color):\n    return Toasters.filter_by(color=color)\n```\n\nPutting the argument name in both the function name and in arguments is, in\nmost cases and for most interpreted languages, redundant.\n\nGood:\n\n```python\ndef get(color=None):\n    if color:\n        return Toasters.filter_by(color=color)\n```\n\n## Repeating class name in method name\n\nBad:\n\n```python\nclass Toasters(object):\n    def get_toaster(self, toaster_id):\n        pass\n```\n\nThis is bad because it's unnecessarily redundant (`Toasters.get_toaster(1)`). According to the single responsibility principle, a class should focus on one area of responsibility. So the `Toasters` class should only focus on toasters object.\n\nGood:\n\n```python\nclass Toasters(object):\n    def get(self, toaster_id):\n        pass\n```\n\nWhich produces much more concise code:\n\n```\ntoaster = Toasters.get(1)\n```\n\n## Repeating function name in docstring\n\nBad:\n\n```python\ndef test_return_true_if_toast_is_valid():\n    \"\"\"Verify that we return true if toast is valid.\"\"\"\n    assert is_valid(Toast('brioche')) is true\n```\n\nWhy is it bad?\n\n- The docstring and function name are not DRY.\n- There's no actual explanation of what valid means.\n\nGood:\n\n```python\ndef test_valid_toast():\n    \"\"\"Verify that 'brioche' are valid toasts.\"\"\"\n    assert is_valid(Toast('brioche')) is true\n```\n\nOr, another variation:\n\n```python\ndef test_brioche_are_valid_toast():\n    assert is_valid(Toast('brioche')) is true\n```\n\n## Unreadable response construction\n\nTODO\n\nBad:\n\n```python\ndef get_data():\n    returned = {}\n    if stuff:\n        returned['toaster'] = 'toaster'\n    if other_stuff:\n        if the_other_stuff:\n            returned['toast'] = 'brioche'\n    else:\n        returned['toast'] = 'bread'\n    return returned\n```\n\nGood:\n\n```python\ndef get_data():\n    returned = {\n        'toaster': '',\n        'toast': '',\n    }\n```\n\n## Undeterministic tests\n\nWhen testing function that don't behave deterministically, it can be tempting\nto run them multiple time and average their results.\n\nBad:\n\n```python\ndef function():\n    if random.random() > .4:\n        return True\n    else:\n        return False\n\n\ndef test_function():\n    number_of_true = 0\n    for _ in xrange(1000):\n        returned = function()\n        if returned:\n            number_of_true += 1\n\n    assert 30 < number_of_true < 50\n```\n\nThere are multiple things that are wrong with this approach:\n\n- This is a flaky test. Theoretically, this test could still fail.\n- This example is simple enough, but `function` might be doing some\n  computationally expensive task, which would make this test severely\n  inefficient.\n- The test is quite difficult to understand.\n\nGood:\n\n```python\n@mock.patch('random.random')\ndef test_function(mock_random):\n    mock_random.return_value = 0.7\n    assert function() is True\n```\n\nThis is a deterministic test that clearly tells what's going on.\n\n## Unbalanced boilerplate\n\nOne thing to strive for in libraries is have as little boilerplate as possible,\nbut not less.\n\nNot enough boilerplate: you'll spend hours trying to understand specific\nbehaviors that are too magical/implicit. You will need flexibility and you\nwon't be able to get it. Boilerplate is useful insofar as it increases\n[transparency](http://www.catb.org/esr/writings/taoup/html/ch01s06.html).\n\nToo much boilerplate: users of your library will be stuck using outdated\npatterns. Users will write library to generate the boilerplate required by your\nlibrary.\n\nI think Flask and SQLAlchemy do a very good job at keeping this under control.\n\n## Inconsistent use of verbs in functions\n\nBad:\n\n```python\ndef get_toasters(color):\n    \"\"\"Get a bunch of toasters.\"\"\"\n    return filter(lambda toaster: toaster.color == color, TOASTERS)\n\n\ndef find_toaster(id_):\n    \"\"\"Return a single toaster.\"\"\"\n    toasters = filter(lambda toaster: toaster.id == id_, TOASTERS)\n    assert len(toasters) == 1\n    return toasters[1]\n\n\ndef find_toasts(color):\n    \"\"\"Find a bunch of toasts.\"\"\"\n    return filter(lambda toast: toast.color == color, TOASTS)\n```\n\nThe use of verb is inconsistent in this example. `get` is used to return\na possibly empty list of toasters, and `find` is used to return a single\ntoaster (or raise an exception) in the second function or a possibly empty list\nof toasts in the third function.\n\nThis is based on personal taste but I have the following rule:\n\n- `get` prefixes function that return at most one object (they either return\n  none or raise an exception depending on the cases)\n- `find` prefixes function that return a possibly empty list (or iterable) of\n  objects.\n\nGood:\n\n```python\ndef find_toasters(color):\n    \"\"\"Find a bunch of toasters.\"\"\"\n    return filter(lambda toaster: toaster.color == color, TOASTERS)\n\n\ndef get_toaster(id_):\n    \"\"\"Return a single toaster.\"\"\"\n    toasters = filter(lambda toaster: toaster.id == id_, TOASTERS)\n    assert len(toasters) == 1\n    return toasters[1]\n\n\ndef find_toasts(color):\n    \"\"\"Find a bunch of toasts.\"\"\"\n    return filter(lambda toast: toast.color == color, TOASTS)\n```\n\n## Opaque function arguments\n\nA few variants of what I consider code that is difficult to debug:\n\n```python\ndef create(toaster_params):\n    name = toaster_params['name']\n    color = toaster_params.get('color', 'red')\n\n\nclass Toaster(object):\n\n    def __init__(self, params):\n        self.name = params['name']\n\n\n# Probably the worst of all\ndef create2(*args, **kwargs):\n    name = kwargs['name']\n```\n\nWhy is this bad?\n\n- It's really easy to make a mistake, especially in interpreted languages such\n  as Python. For instance, if I call `create({'name': 'hello', 'ccolor': 'blue'})`, I won't get any error, but the color won't be the one I expect.\n- It increases cognitive load, as I have to understand where the object is\n  coming from to introspect its content.\n- It makes the job of static analyzer harder or impossible.\n\nGranted, this pattern is sometimes required (for instance when the number of\nparams is too large, or when dealing with pure data).\n\nA better way is to be explicit:\n\n```python\ndef create(name, color='red'):\n    pass  # ...\n```\n\n## Hiding formatting\n\nBad:\n\n```python\n# main.py\n\nfrom utils import format_query\n\n\ndef get_user(user_id):\n    url = get_url(user_id)\n    return requests.get(url)\n\n\n# utils.py\n\n\ndef get_url(user_id):\n    return 'http://127.0.0.1/users/%s' % user_id\n```\n\nI consider this an antipattern because it hides the request formatting from the developer, making it more complex to see what `url` look like. In this extreme example, the formatting function is a one-liner which sounds a bit overkill for\na function.\n\nGood:\n\n```python\ndef get_user(user_id):\n    url = 'http://127.0.0.1/users/%s' % user_id\n    return requests.get(url)\n```\n\nEven if you were duplicating the logic once or twice it might still be fine, because:\n\n- You're unlikely to re-use anywhere else outside this file.\n- Putting this inline makes it easier for follow the flow. Code is written to be read primarily by computers.\n\n## Returning nothing instead of raising NotFound exception\n\nBad in certain cases:\n\n```python\ndef get_toaster(toaster_id):\n    try:\n        return do_get_toaster(toaster_id)\n    except NotFound:\n        return None\n\n\ndef toast(toaster_id):\n    toaster = get_toaster(toaster_id)\n    ...\n    toaster.toast(\"brioche\")\n```\n\nIt all depends on the caller, but in this cases I'd argue that it is bad practice to return nothing when the toaster identified by `toaster_id` can't be found, for two main reasons.\n\n**First reason**: when we provide an identifier, we expect it to return something. Once again, this depends on the caller (for instance, we could try to see if a user exists by checking an email for instance). In this simple example it's ok because the `toaster.toast()` will fail immediately, but what if we were never calling it and creating some other unrelated objects? We would be doing things that we should never be doing if the object did not exist:\n\n```python\ndef toast(toaster_id, user):\n    toaster = get_toaster(toaster_id)\n    # We should never do this! The toaster might not even exists.\n    send_welcome_email(user)\n    bill_new_toaster(user)\n```\n\n**Second reason**: `toaster.toast` will fail anyway if `toaster` is none (in Python with `AttributeError: NoneType has no attribute toast`). In this abstract example it's ok because the two lines are next to each other, but the actual `toaster.toast()` call might happen further down the stack - and it will be very difficult for the developer to understand where the error is coming from.\n\n```python\ndef toast(toaster_id, user):\n    toaster = get_toaster(toaster_id)\n    do_stuff_a(toaster)\n\n\ndef do_stuff_a(toaster):\n    ...\n    do_stuff_b(toaster)\n    ...\n\n\ndef do_stuff_b(toaster):\n    # Here is the actual call where toaster is called - but we should\n    # have failed early if it's not there.\n    toaster.toast()\n```\n\nWhat's the correct things to do?\n\n- If you expect the object to be there, make sure to raise if you don't find it.\n- If you're using SQLAlchemy, use `one()` to force raising an exception if the object can't be found. Don't use `first` or `one_or_none()`.\n\n## Having a library that contains all utils\n\nBad:\n\n```python\ndef get_current_date():\n    ...\n\n\ndef create_csv(...):\n    ...\n\n\ndef upload_to_sftp(...):\n    ...\n```\n\n`util` or `tools` or `lib` modules that contain all sorts of utilities have a tendency to become bloated and unmaintainable. Prefer to have small, dedicated files.\n\nThis will keep your imports logical (`lib.date_utils`, `lib.csv_utils`, `lib.sftp`), make it easier for the reader to identify all the utilities around a specific topic, and test files easy to keep organized.\n", "metadata": {"source": "github.com/charlax/professional-programming", "file_path": "antipatterns/code-antipatterns.md", "language": "markdown", "complexity": "complex", "star_count": 47841, "quality_score": 1.0, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 471, "word_count": 1778}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n## Table of Contents\n\n- [Error handling anti-patterns](#error-handling-anti-patterns)\n  - [Hiding exceptions](#hiding-exceptions)\n  - [Raising unrelated/unspecific exception](#raising-unrelatedunspecific-exception)\n  - [Unconstrained defensive programming](#unconstrained-defensive-programming)\n  - [Unnecessarily catching and re-raising exceptions](#unnecessarily-catching-and-re-raising-exceptions)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n# Error handling anti-patterns\n\n## Hiding exceptions\n\nThere are multiple variations of this anti-pattern:\n\n```python\n# Silence all exceptions\ndef toast(bread):\n    try:\n        toaster = Toaster()\n        toaster.insert(bread)\n        toaster.toast()\n    except:\n        pass\n\n\n# Silence some exceptions\ndef toast(bread):\n    try:\n        toaster = Toaster()\n        toaster.insert(bread)\n        toaster.toast()\n    except ValueError:\n        pass\n```\n\nIt depends on the context but in most cases this is a bad pattern:\n\n- **Debugging** those silent errors will be really difficult, because they won't show up in logs and exception reporting tool such as Sentry. Say you have an undefined variable in `Toaster.insert()`: it will raise `NameError`, which will be caught, and ignored, and you will never know about this developer error.\n- **The user experience** will randomly degrade without anybody knowing about it, including the user.\n- **Identifying** those errors will be impossible. Say `do_stuff` does an HTTP request to another service, and that service starts misbehaving. There won't be any exception, any metric that will let you identify it.\n\nAn article even named this [the most diabolical Python antipattern](https://realpython.com/blog/python/the-most-diabolical-python-antipattern/).\n\nThe following full example:\n\n```python\nfrom collections import namedtuple\n\nBread = namedtuple('Bread', 'color')\n\nclass ToastException(Exception):\n    pass\n\ndef toast(bread):\n    try:\n        put_in_toaster(bread)\n    except:\n        raise ToastException('Could not toast bread')\n\n\ndef put_in_toaster(bread):\n    brad.color = 'light_brown'  # Note the typo\n\n\ntoast(Bread('yellow'))\n```\n\nWill raise this cryptic and impossible to debug error:\n\n```\nTraceback (most recent call last):\n  File \"python-examples/reraise_exceptions.py\", line 19, in <module>\n    toast(Bread('yellow'))\n  File \"python-examples/reraise_exceptions.py\", line 12, in toast\n    raise ToastException('Could not toast bread')\n__main__.ToastException: Could not toast bread\n```\n\nSometime it's tempting to think that graceful degradation is about silencing\nexception. It's not.\n\n- Graceful degradation needs to happen at the **highest level** of the code, so that the user can get a very explicit error message (e.g. \"we're having issues with X, please retry in a moment\"). That requires knowing that there was an error, which you can't tell if you're silencing the exception.\n- You need to know when graceful degradation happens. You also need to be\n  alerted if it happens too often. This requires adding monitoring (using\n  something like statsd) and logging (Python's `logger.exception` automatically\n  adds the exception stacktrace to the log message for instance). Silencing an\n  exception won't make the error go away: all things being equal, it's better\n  for something to break hard, than for an error to be silenced.\n- It is tempting to confound silencing the exception and fixing the exception. Say you're getting sporadic timeouts from a service. You might thing: let's ignore those timeouts and just do something else, like return an empty response. But this is very different from (1) actually finding the root cause for those timeouts (e.g. maybe a specific edge cases impacting certain objects) (2) doing proper graceful degradation (e.g. asking users to retry later because the request failed).\n\nIn other words, ask yourself: would it be a problem if every single action was failing? If you're silencing the error, how would you know it's happening for every single action?\n\nHere's a number a better ways to do this:\n\n**Log and create metrics**\n\n```python\nimport statsd\n\n\ndef toast(bread):\n    # Note: no exception handling here.\n    toaster = Toaster()\n    toaster.insert(bread)\n    toaster.toast()\n\n\ndef main():\n    try:\n        toast('brioche')\n    except:\n        logger.exception('Could not toast bread')\n        statsd.count('toast.error', 1)\n```\n\n**Very important**: adding logging and metrics won't not sufficient if it's difficult to consume them. They won't help the developer who's debugging. There needs to be automatic alerting associated to those stats. The logs needs to be surfaced somewhere, ideally next to the higher exception (e.g. let's our `main` function above is used in a web interface - the web interface could say \"additionally, the following logs were generated and display the log). For instance, Sentry can be configured to surface `logger.exception` errors the same way exception are surfaced. Otherwise the developer will still have to read the code to understand what's going on. Also - this won't work with sporadic errors. Those needs to be dealt with properly, and until then, it's better to let them go to your usual alerting tool.\n\nNote that knowing where to catch is very important too. If you're catching\ninside the `toast` function, you might be hiding things a caller would need to\nknow. Since this function is not returning anything, how would you make the\ndifference between a success and a failure? You can't. That's why you want to\nlet it raise, and catch only in the caller, where you have the context to know\nhow you'll handle the exception.\n\n**Re-raise immediately**\n\n```python\nimport statsd\n\n\ndef toast(bread):\n    try:\n        toaster = Toaster()\n        toaster.insert(bread)\n        toaster.toast()\n    except:\n        raise ToastingException('Could not toast bread %r' % bread)\n\n\ndef main():\n    # Note: no exception handling here\n    toast('brioche')\n```\n\n**Be very specific about the exception that are caught**\n\n```python\nimport statsd\n\n\ndef toast(bread):\n    try:\n        toaster = Toaster()\n    except ValueError:\n        # Note that even though we catch, we're still logging + creating\n        # a metric\n        logger.exception('Could not get toaster')\n        statsd.count('toaster.instantiate.error', 1)\n        return\n\n    toaster.insert(bread)\n    toaster.toast()\n\n\ndef main():\n    toast('brioche')\n```\n\n## Raising unrelated/unspecific exception\n\nMost languages have predefined exceptions, including Python. It is important to make sure that the right exception is raised from a semantic standpoint.\n\nBad:\n\n```python\ndef validate(toast):\n    if isinstance(toast, Brioche):\n        # RuntimeError is too broad\n        raise RuntimeError('Invalid toast')\n\n\ndef validate(toast):\n    if isinstance(toast, Brioche):\n        # SystemError should only be used for internal interpreter errors\n        raise SystemError('Invalid toast')\n```\n\nGood:\n\n```python\ndef validate(toast):\n    if isinstance(toast, Brioche):\n        raise TypeError('Invalid toast')\n```\n\n`TypeError` is here perfectly meaningful, and clearly convey the context around the error.\n\n## Unconstrained defensive programming\n\nWhile defensive programming can be a very good technique to make the code more resilient, it can seriously backfire when misused. This is a very similar anti-pattern to carelessly silencing exceptions (see about this anti-pattern in this document).\n\nOne example is to handle an edge case as a generic case at a very low level. Consider the following example:\n\n```python\ndef get_user_name(user_id):\n    url = 'http://127.0.0.1/users/%s' % user_id\n    response = requests.get(url)\n    if response.status == 404:\n        return 'unknown'\n    return response.data\n```\n\nWhile this may look like a very good example of defensive programming (we're returning `unknown` when we can't find the user), this can have terrible repercussions, very similar to the one we have when doing an unrestricted bare `try... except`:\n\n- A new developer might not know about this magical convention, and assume that `get_user_name` is guaranteed to return a true user name.\n- The external service that we're getting user name from might start failing, and returning 404. We would silently return 'unknown' as a user name for all users, which could have terrible repercussions.\n\nA much cleaner way is to raise an exception on 404, and let the caller decide how it wants to handle users that are not found.\n\n## Unnecessarily catching and re-raising exceptions\n\nBad:\n\n```python\ndef toast(bread):\n    try:\n        put_in_toaster(bread)\n    except InvalidBreadException:\n        raise ToastException('Could not toast')\n```\n\nSide note: an unconditional exception catching is considered even worse (see [hiding exceptions](https://github.com/charlax/antipatterns/blob/master/code-antipatterns.md#hiding-exceptions)).\n\nThis is a better pattern because we explicitly state what happened in the exception message:\n\n```python\ndef toast(bread):\n    try:\n        put_in_toaster(bread)\n    except InvalidBreadType as e:\n        raise ToastException('Cannot toast this bread type')\n```\n\nIf we need to do some cleanup or extra logging, it's better to just raise the original exception again. The developer will know exactly what happened.\n\n```python\ndef toast(bread):\n    try:\n        put_in_toaster(bread)\n    except:\n        print 'Got exception while trying to toast'\n        raise  # note the absence of specific exception\n```\n\nHere's what would happen:\n\n```\nGot exception while trying to toast\nTraceback (most recent call last):\n  File \"reraise_exceptions_good.py\", line 20, in <module>\n    toast(Bread('yellow'))\n  File \"reraise_exceptions_good.py\", line 10, in toast\n    put_in_toaster(bread)\n  File \"reraise_exceptions_good.py\", line 17, in put_in_toaster\n    brad.color = 'light_brown'  # Note the typo\nNameError: global name 'brad' is not defined\n```\n\nAnother way to show how absurd the anti-pattern becomes at scale is through an example:\n\n```python\ndef call_1():\n    try:\n        call_2()\n    except Call2Exception:\n        raise Call1Exception()\n\n\ndef call_2():\n    try:\n        call_3()\n    except Call3Exception:\n        raise Call2Exception()\n\n\ndef call_3():\n    ...\n```\n\nAnother problem with this pattern is that you can consider it quite useless to do all of this catch/re-raise, since you will still need to catch at the end. In other words:\n\n> Error handling, and recovery are best done at the outer layers of your code base. This is known as the end-to-end principle. The end-to-end principle argues that it is easier to handle failure at the far ends of a connection than anywhere in the middle. If you have any handling inside, you still have to do the final top level check. If every layer atop must handle errors, so why bother handling them on the inside?\n\n_[Write code that is easy to delete, not easy to extend](http://programmingisterrible.com/post/139222674273/write-code-that-is-easy-to-delete-not-easy-to)_\n\nA better way:\n\n```python\n# This is the highest level function where we have enough\n# context to know how to handle the exceptions\ndef call_1():\n    try:\n        call_2()\n    except Call2Exception:\n        # handle it...\n        pass\n    except Call3Exception:\n        # handle it...\n        pass\n\n\ndef call_2():\n    # Do not handle anything here.\n    call_3()\n\n\ndef call_3():\n    ...\n```\n\nMore resources:\n\n- [Hiding exceptions](https://github.com/charlax/antipatterns/blob/master/code-antipatterns.md#hiding-exceptions)) anti-pattern.\n", "metadata": {"source": "github.com/charlax/professional-programming", "file_path": "antipatterns/error-handling-antipatterns.md", "language": "markdown", "complexity": "complex", "star_count": 47841, "quality_score": 1.0, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 328, "word_count": 1548}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create comprehensive API documentation with examples and schemas with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n## Table of Contents\n\n- [MVCS Antipatterns](#mvcs-antipatterns)\n  - [Creating entities for association tables](#creating-entities-for-association-tables)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n# MVCS Antipatterns\n\nIn simple terms, Model-View-Controller-Services add a few more layers to the\nMVC pattern. The main one is the service, which owns all the core business\nlogic and manipulate the repository layer.\n\n## Creating entities for association tables\n\nYou'll often need association tables, for instance to set up a many to many\nrelationships between users and their toasters. Let's assume that a toaster can\nbe owned by multiple users.\n\nIt might be tempting to create a `UserToaster` entity for this relationship,\nespecially if this relationship has some complex attributes associated with\n(for instance, since when the toaster is owned by the user).\n\nLet me pull a few quotes from the [Domain Driven\nDesign](http://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215) by Eric Evans:\n\n> Design a portion of the software system to reflect the domain model in a very\n> literal way, so that mapping is obvious.\n\n> Object-oriented programming is powerful because it is based on a modeling\n> paradigm, and it provides implementations of the model constructs. As far as\n> the programmer is concerned, objects really exist in memory, they have\n> associations with other objects, they are organized into classes, and they\n> provide behavior available by messaging.\n\n> Does an object represent something with continuity and identity‚Äî something\n> that is tracked through different states or even across different\n> implementations? Or is it an attribute that describes the state of something\n> else? This is the basic distinction between an ENTITY and a VALUE OBJECT.\n> Defining objects that clearly follow one pattern or the other makes the\n> objects less ambiguous and lays out the path toward specific choices for\n> robust design.\n\nEvans, Eric (2003-08-22). Domain-Driven Design: Tackling Complexity in the\nHeart of Software. Pearson Education. Kindle Edition.\n\nEntities should model business processes, not persistence details\n([source](http://blog.sapiensworks.com/post/2013/05/13/7-Biggest-Pitfalls-When-Doing-Domain-Driven-Design.aspx/)).\n\n- In that case, `UserToaster` does not map to anything the business is using.\n  Using plain English, somebody might ask about \"what toasters does user\n  A owns?\" or \"who owns toaster B and since when?\" Nobody would ask \"give me\n  the UserToaster for user A\".\n- The association table can be considered an implementation detail that should\n  not (in most cases) leak in the domain layer. All the code should be dealing\n  with the simpler logic of \"user having toasters\", not UserToaster objects\n  being an association between a user and a toaster. This makes the code more\n  intuitive and natural.\n- It will be easier to handle serializing a \"user having toasters\" than\n  serializing UserToaster association.\n- This will make it very easy to force the calling site to take care of some\n  business logic. For instance, you might be able to get all `UserToaster`, and\n  then filter on whether they were bought. You might be tempted to do that by\n  going through the `UserToaster` object and filtering those that have\n  `were_bought` to be True. At some point, you might be doing the same thing in\n  multiple places, which will decrease maintainability. If you were hiding that\n  logic in the repository, you wouldn't have that issue `find_bought_toasters`.\n\nSo in that case, I would recommend doing the following:\n\n- Create a `User` and `Toaster` entity.\n- Put the association properties on the entity that makes sense, for instance\n  `owned_since` would be on `Toaster`, even though in the database it's stored\n  on the association table.\n- If filtering on association properties is required, put this logic in\n  repositories. In plain English, you would for instance ask \"give all the\n  toasters user A owned in December?\", you wouldn't ask \"give be all the\n  UserToaster for owned by user A in December\".\n\nNote that Domain Driver Design recommends avoiding many-to-many relationships\naltogether:\n\n> In real life, there are lots of many-to-many associations, and a great number\n> are naturally bidirectional. The same tends to be true of early forms of\n> a model as we brainstorm and explore the domain. But these general\n> associations complicate implementation and maintenance. Furthermore, they\n> communicate very little about the nature of the relationship.\n\n> There are at least three ways of making associations more tractable.\n\n> 1. Imposing a traversal direction\n> 2. Adding a qualifier, effectively reducing multiplicity\n> 3. Eliminating nonessential associations\n\nEvans, Eric (2003-08-22). Domain-Driven Design: Tackling Complexity in the\nHeart of Software (Kindle Locations 1642-1647). Pearson Education. Kindle\nEdition.\n\nImposing a traversal direction is probably the best solution. In our example,\nit's not immediately evident which direction is the right one (a toaster being\nowned by a user, or a user owning a toasters), because that depends on what\nthis application is doing. If we're working on an app that lets a connected\nuser see their toasters, then we would force the relationship to be\nunidirectional user->toasters.\n\nSources:\n\n- [7 Biggest Pitfalls When Doing Domain Driven\n  Design](http://blog.sapiensworks.com/post/2013/05/13/7-Biggest-Pitfalls-When-Doing-Domain-Driven-Design.aspx/)\n- [Domain-Driven Design: Tackling Complexity in the Heart of\n  Software](http://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215)\n", "metadata": {"source": "github.com/charlax/professional-programming", "file_path": "antipatterns/mvcs-antipatterns.md", "language": "markdown", "complexity": "medium", "star_count": 47841, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 114, "word_count": 857}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "api_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive API documentation with examples and schemas\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n## Table of Contents\n\n- [SQLAlchemy Anti-Patterns](#sqlalchemy-anti-patterns)\n  - [Abusing lazily loaded relationships](#abusing-lazily-loaded-relationships)\n  - [Explicit session passing](#explicit-session-passing)\n  - [Implicit transaction handling](#implicit-transaction-handling)\n  - [Loading the full object when checking for object existence](#loading-the-full-object-when-checking-for-object-existence)\n  - [Using identity as comparator](#using-identity-as-comparator)\n  - [Returning `None` instead of raising a `NoResultFound` exception](#returning-none-instead-of-raising-a-noresultfound-exception)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n# SQLAlchemy Anti-Patterns\n\nThis is a list of what I consider [SQLAlchemy](http://www.sqlalchemy.org/)\nanti-patterns.\n\n## Abusing lazily loaded relationships\n\nBad:\n\n```python\nclass Customer(Base):\n\n    @property\n    def has_valid_toast(self):\n        \"\"\"Return True if customer has at least one valid toast.\"\"\"\n        return any(toast.kind == 'brioche' for toast in self.toaster.toasts)\n```\n\nThis suffers from severe performance inefficiencies:\n\n- The toaster will be loaded, as well as its toast. This involves creating and\n  issuing the SQL query, waiting for the database to return, and instantiating\n  all those objects.\n- `has_valid_toast` does not actually care about those objects. It just returns\n  a boolean.\n\nA better way would be to issue a SQL `EXISTS` query so that the database\nhandles this check and only returns a boolean.\n\nGood:\n\n```python\nclass Customer(Base):\n\n    @property\n    def has_valid_toast(self):\n        \"\"\"Return True if customer has at least one valid toast.\"\"\"\n        query = (session.query(Toaster)\n                 .join(Toast)\n                 .with_parent(self)\n                 .filter(Toast.kind == 'brioche'))\n        return session.query(query.exists()).scalar()\n```\n\nThis query might not always be the fastest if those relationships are small,\nand eagerly loaded.\n\n## Explicit session passing\n\nTODO\n\nBad:\n\n```python\ndef toaster_exists(toaster_id, session):\n    ...\n```\n\n## Implicit transaction handling\n\nTODO\n\n## Loading the full object when checking for object existence\n\nBad:\n\n```python\ndef toaster_exists(toaster_id):\n    return bool(session.query(Toaster).filter_by(id=toaster_id).first())\n```\n\nThis is inefficient because it:\n\n- Queries all the columns from the database (including any eagerly loaded joins)\n- Instantiates and maps all data on the Toaster model\n\nThe database query would look something like this. You can see that all columns\nare selected to be loaded by the ORM.\n\n```sql\nSELECT toasters.id AS toasters_id, toasters.name AS toasters_name,\ntoasters.color AS toasters_color\nFROM toasters\nWHERE toasters.id = 1\n LIMIT 1 OFFSET 0\n```\n\nAnd then it just checks if the result is truthy.\n\nHere's a better way to do it:\n\n```python\ndef toaster_exists(toaster_id):\n    query = session.query(Toaster).filter_by(id=toaster_id)\n    return session.query(query.exists()).scalar()\n```\n\nIn this case, we just ask the database about whether a record exists with this\nid. This is obviously much more efficient.\n\n```sql\nSELECT EXISTS (SELECT 1\nFROM toasters\nWHERE toasters.id = 1) AS anon_1\n```\n\n## Using identity as comparator\n\nBad:\n\n```python\ntoasters = session.query(Toaster).filter(Toaster.deleted_at is None).all()\n```\n\nUnfortunately this won't work at all. This query will return all toasters,\nincluding the one that were deleted.\n\nThe way sqlalchemy works is that it overrides the magic comparison methods\n(`__eq__`, `__lt__`, etc.). All comparison methods can be overrode except the\nidentity operator (`is`) which checks for objects identity.\n\nWhat this means is that expression `Toaster.deleted_at is None` will be\nimmediately evaluated by the Python interpreter, and since (presumably)\n`Toaster.deleted_at` is a `sqlalchemy.orm.attributes.InstrumentedAttribute`,\nit's not `None` and thus it's equivalent to doing:\n\n```python\ntoasters = session.query(Toaster).filter(True).all()\n```\n\nWhich obviously renders the filter inoperable, and will return all records.\n\nThere's two ways to fix it:\n\n```python\ntoasters = session.query(Toaster).filter(Toaster.deleted_at == None).all()\n```\n\nHere we use the equality operator, which Python allows overriding. Behind the\nscene, Python calls `Toaster.deleted_at.__eq__(None)`, which gives SQLAlchemy\nthe opportunity to return a comparator that when coerced to a string, will\nevaluate to `deleted_at is NULL`.\n\nMost linter will issue a warning for equality comparison against `None`, so you\ncan also do (this is my preferred solution):\n\n```python\ntoasters = session.query(Toaster).filter(Toaster.deleted_at.is_(None)).all()\n```\n\nSee docs for\n[is\\_](http://docs.sqlalchemy.org/en/rel_1_0/core/sqlelement.html#sqlalchemy.sql.operators.ColumnOperators.is_).\n\n## Returning `None` instead of raising a `NoResultFound` exception\n\nSee [Returning nothing instead of raising NotFound exception](https://github.com/charlax/antipatterns/blob/master/code-antipatterns.md#returning-nothing-instead-of-raising-notfound-exception).\n", "metadata": {"source": "github.com/charlax/professional-programming", "file_path": "antipatterns/sqlalchemy-antipatterns.md", "language": "markdown", "complexity": "medium", "star_count": 47841, "quality_score": 0.92, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 171, "word_count": 620}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n---\ntitle: Neural Network Activation Functions\nsummary: >\n A set of PyTorch implementations/tutorials related to neural network activations\n---\n\n# Neural Networks Activations\n\n* [Fuzzy Tiling Activations](fta/index.html)\n* üöß [Swish](swish/index.html)\n\"\"\"\n\nfrom .swish import Swish\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/activations/__init__.py", "language": "python", "complexity": "simple", "star_count": 62130, "quality_score": 0.89, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 15, "word_count": 36}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n---\ntitle: Fuzzy Tiling Activations\nsummary: >\n  PyTorch implementation and tutorial of Fuzzy Tiling Activations from the\n  paper Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online.\n---\n\n# Fuzzy Tiling Activations (FTA)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/activations/fta/experiment.ipynb)\n\nThis is a [PyTorch](https://pytorch.org) implementation/tutorial of\n[Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online](https://arxiv.org/abs/1911.08068).\n\nFuzzy tiling activations are a form of sparse activations based on binning.\n\nBinning is classification of a scalar value into a bin based on intervals.\nOne problem with binning is that it gives zero gradients for most values (except at the boundary of bins).\nThe other is that binning loses precision if the bin intervals are large.\n\nFTA overcomes these disadvantages.\nInstead of hard boundaries like in Tiling Activations, FTA uses soft boundaries\nbetween bins.\nThis gives non-zero gradients for all or a wide range of values.\nAnd also doesn't lose precision since it's captured in partial values.\n\n#### Tiling Activations\n\n$\\mathbf{c}$ is the tiling vector,\n\n$$\\mathbf{c} = (l, l + \\delta, l + 2 \\delta, \\dots, u - 2 \\delta, u - \\delta)$$\n\nwhere $[l, u]$ is the input range, $\\delta$ is the bin size, and $u - l$ is divisible by $\\delta$.\n\nTiling activation is,\n\n$$\\phi(z) = 1 - I_+ \\big( \\max(\\mathbf{c} - z, 0) + \\max(z - \\delta - \\mathbf{c}) \\big)$$\n\nwhere $I_+(\\cdot)$ is the indicator function which gives $1$ if the input is positive and $0$ otherwise.\n\nNote that tiling activation gives zero gradients because it has hard boundaries.\n\n#### Fuzzy Tiling Activations\n\nThe fuzzy indicator function,\n\n$$I_{\\eta,+}(x) = I_+(\\eta - x) x + I_+ (x - \\eta)$$\n\nwhich increases linearly from $0$ to $1$ when $0 \\le x \\lt \\eta$\nand is equal to $1$ for $\\eta \\le x$.\n$\\eta$ is a hyper-parameter.\n\nFTA uses this to create soft boundaries between bins.\n\n$$\\phi_\\eta(z) = 1 - I_{\\eta,+} \\big( \\max(\\mathbf{c} - z, 0) + \\max(z - \\delta - \\mathbf{c}, 0) \\big)$$\n\n[Here's a simple experiment](experiment.html) that uses FTA in a transformer.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\n\nclass FTA(nn.Module):\n    \"\"\"\n    ### Fuzzy Tiling Activations (FTA)\n    \"\"\"\n\n    def __init__(self, lower_limit: float, upper_limit: float, delta: float, eta: float):\n        \"\"\"\n        :param lower_limit: is the lower limit $l$\n        :param upper_limit: is the upper limit $u$\n        :param delta: is the bin size $\\delta$\n        :param eta: is the parameter $\\eta$ that detemines the softness of the boundaries.\n        \"\"\"\n        super().__init__()\n        # Initialize tiling vector\n        # $$\\mathbf{c} = (l, l + \\delta, l + 2 \\delta, \\dots, u - 2 \\delta, u - \\delta)$$\n        self.c = nn.Parameter(torch.arange(lower_limit, upper_limit, delta), requires_grad=False)\n        # The input vector expands by a factor equal to the number of bins $\\frac{u - l}{\\delta}$\n        self.expansion_factor = len(self.c)\n        # $\\delta$\n        self.delta = delta\n        # $\\eta$\n        self.eta = eta\n\n    def fuzzy_i_plus(self, x: torch.Tensor):\n        \"\"\"\n        #### Fuzzy indicator function\n\n        $$I_{\\eta,+}(x) = I_+(\\eta - x) x + I_+ (x - \\eta)$$\n        \"\"\"\n        return (x <= self.eta) * x + (x > self.eta)\n\n    def forward(self, z: torch.Tensor):\n        # Add another dimension of size $1$.\n        # We will expand this into bins.\n        z = z.view(*z.shape, 1)\n\n        # $$\\phi_\\eta(z) = 1 - I_{\\eta,+} \\big( \\max(\\mathbf{c} - z, 0) + \\max(z - \\delta - \\mathbf{c}, 0) \\big)$$\n        z = 1. - self.fuzzy_i_plus(torch.clip(self.c - z, min=0.) + torch.clip(z - self.delta - self.c, min=0.))\n\n        # Reshape back to original number of dimensions.\n        # The last dimension size gets expanded by the number of bins, $\\frac{u - l}{\\delta}$.\n        return z.view(*z.shape[:-2], -1)\n\n\ndef _test():\n    \"\"\"\n    #### Code to test the FTA module\n    \"\"\"\n    from labml.logger import inspect\n\n    # Initialize\n    a = FTA(-10, 10, 2., 0.5)\n    # Print $\\mathbf{c}$\n    inspect(a.c)\n    # Print number of bins $\\frac{u - l}{\\delta}$\n    inspect(a.expansion_factor)\n\n    # Input $z$\n    z = torch.tensor([1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9., 10., 11.])\n    # Print $z$\n    inspect(z)\n    # Print $\\phi_\\eta(z)$\n    inspect(a(z))\n\n\nif __name__ == '__main__':\n    _test()\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/activations/fta/__init__.py", "language": "python", "complexity": "medium", "star_count": 62130, "quality_score": 0.98, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 132, "word_count": 631}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n---\ntitle: Fuzzy Tiling Activation Experiment\nsummary: >\n Training a transformer with FTA in FFN on Tiny Shakespeare.\n---\n\n# [Fuzzy Tiling Activation](index.html) Experiment\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/activations/fta/experiment.ipynb)\n\nHere we train a transformer that uses [Fuzzy Tiling Activation](index.html) in the\n[Feed-Forward Network](../../transformers/feed_forward.html).\nWe use it for a language model and train it on Tiny Shakespeare dataset\nfor demonstration.\n\nHowever, this is probably not the ideal task for FTA, and we\nbelieve FTA is more suitable for modeling data with continuous variables.\n\"\"\"\n\nimport copy\n\nimport torch\nimport torch.nn as nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.activations.fta import FTA\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.transformers import MultiHeadAttention, TransformerLayer\nfrom labml_nn.transformers.utils import subsequent_mask\n\n\nclass FeedForwardFTA(nn.Module):\n    \"\"\"\n    ## FFN module with [FTA](index.html) activation\n    \"\"\"\n\n    def __init__(self, d_model: int, d_ff: int,\n                 activation: FTA,\n                 dropout: float = 0.1):\n        \"\"\"\n        * `d_model` is the number of features in a token embedding\n        * `d_ff` is the number of features in the hidden layer of the FFN\n        * `activation` is FTA activation module\n        * `dropout` is dropout probability for the hidden layer\n        \"\"\"\n        super().__init__()\n        # Layer one parameterized by weight $W_1$ and bias $b_1$\n        self.layer1 = nn.Linear(d_model, d_ff)\n        # Layer two parameterized by weight $W_1$ and bias $b_1$\n        self.layer2 = nn.Linear(d_ff * activation.expansion_factor, d_model)\n        # Hidden layer dropout\n        self.dropout = nn.Dropout(dropout)\n        # Activation function $f$\n        self.activation = activation\n\n    def forward(self, x: torch.Tensor):\n        # $f(x W_1 + b_1)$\n        x = self.activation(self.layer1(x))\n        # Apply dropout\n        x = self.dropout(x)\n        #\n        return self.layer2(x)\n\n\nclass AutoregressiveTransformer(nn.Module):\n    \"\"\"\n    ## Auto-Regressive model\n\n    This is an autoregressive transformer model that uses Feed-Forward Networks with\n     (Fuzzy Tiling Activations)(index.html).\n    \"\"\"\n\n    def __init__(self, n_tokens: int, d_model: int, n_layers: int, layer: TransformerLayer):\n        \"\"\"\n        :param n_tokens: is the number of tokens in the vocabulary\n        :param d_model: is the embedding size\n        :param n_layers: is the number of transformer layers\n        :param layer: is the layer. We use `n_layers` copies of this for the transformer.\n        \"\"\"\n        super().__init__()\n        # Transformer with `n_layers` layers\n        self.transformer_layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(n_layers)])\n\n        # Token embedding layer\n        self.emb = nn.Embedding(n_tokens, d_model)\n        # Readout layer\n        self.readout = nn.Linear(d_model, n_tokens)\n\n        # The mask will be initialized on the first call\n        self.mask = None\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: are the input tokens of shape `[seq_len, batch_size]`\n        \"\"\"\n        # Create auto-regressive mask\n        if self.mask is None or self.mask.size(0) != len(x):\n            # Subsequent mask, will mask out tokens from seeing future tokens\n            self.mask = subsequent_mask(len(x)).to(x.device)\n\n        # Get the token embeddings\n        x = self.emb(x)\n        # Transformer encoder\n        for layer in self.transformer_layers:\n            x = layer(x=x, mask=self.mask)\n        # Get logits\n        x = self.readout(x)\n\n        # Return results\n        return x, None\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from\n    [`NLPAutoRegressionConfigs`](../../experiments/nlp_autoregression.html#NLPAutoRegressionConfigs)\n    \"\"\"\n\n    # Model\n    model: AutoregressiveTransformer\n\n    # Number of layers\n    n_layers: int = 4\n\n    # $\\alpha$ and $\\beta$ for DeepNorm\n    deep_norm_alpha: float\n    deep_norm_beta: float\n\n    # Number of heads in the attention\n    n_heads: int = 4\n    # Embedding size\n    d_model: int = 256\n    # Size of each attention head\n    d_k: int = 16\n    # Feed forward layer size\n    d_ff: int = 256\n\n    # FTA\n    fta_lower_limit: float = -1.\n    fta_upper_limit: float = +1.\n    fta_delta: float = 0.2\n    fta_eta: float = 0.05\n\n\n@option(Configs.model)\ndef _model(c: Configs):\n    \"\"\"\n    #### Initialize the model\n    \"\"\"\n\n    # Create FTA activation module\n    fta = FTA(c.fta_lower_limit, c.fta_upper_limit, c.fta_delta, c.fta_eta)\n    # Create the transformer.\n    # We re-use [`TransformerLayer`](../../transformers/models.html#TransformerLayer) and\n    # [`MultiHeadAttention`](../../transformers/mha.html) implementations.\n    m = AutoregressiveTransformer(c.n_tokens, c.d_model, c.n_layers,\n                                  TransformerLayer(d_model=c.d_model,\n                                                   feed_forward=FeedForwardFTA(d_model=c.d_model,\n                                                                               d_ff=c.d_ff,\n                                                                               activation=fta,\n                                                                               dropout=0.1),\n                                                   self_attn=MultiHeadAttention(c.n_heads, c.d_model,\n                                                                                dropout_prob=0.0),\n                                                   dropout_prob=0.0))\n\n    # Move to the device\n    return m.to(c.device)\n\n\ndef main():\n    \"\"\"\n    #### Create and run the experiment\n    \"\"\"\n    # Create experiment\n    experiment.create(name=\"fta\", writers={'screen', 'labml'})\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $256$\n        'seq_len': 256,\n        # Train for 32 epochs\n        'epochs': 32,\n        # Batch size $16$\n        'batch_size': 16,\n        # Switch between training and validation for $10$ times per epoch\n        'inner_iterations': 10,\n\n        # Adam optimizer with no warmup\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 3e-4,\n    })\n\n    # Set model(s) for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/activations/fta/experiment.py", "language": "python", "complexity": "complex", "star_count": 62130, "quality_score": 0.98, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 219, "word_count": 699}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n---\ntitle: Neural Networks with Adaptive Computation\nsummary: >\n A set of PyTorch implementations/tutorials related to adaptive computation\n---\n\n# Neural Networks with Adaptive Computation\n\nThese are neural network architectures that change the computation complexity based on the\ncomplexity of the input sample.\n\n* üöß TODO: Adaptive Computation Time for Recurrent Neural Networks\n* [PonderNet: Learning to Ponder](ponder_net/index.html)\n\"\"\"\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/adaptive_computation/__init__.py", "language": "python", "complexity": "simple", "star_count": 62130, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 16, "word_count": 60}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n---\ntitle: \"PonderNet: Learning to Ponder\"\nsummary: >\n A PyTorch implementation/tutorial of PonderNet: Learning to Ponder.\n---\n\n# PonderNet: Learning to Ponder\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[PonderNet: Learning to Ponder](https://arxiv.org/abs/2107.05407).\n\nPonderNet adapts the computation based on the input.\nIt changes the number of steps to take on a recurrent network based on the input.\nPonderNet learns this with end-to-end gradient descent.\n\nPonderNet has a step function of the form\n\n$$\\hat{y}_n, h_{n+1}, \\lambda_n = s(x, h_n)$$\n\nwhere $x$ is the input, $h_n$ is the state, $\\hat{y}_n$ is the prediction at step $n$,\nand $\\lambda_n$ is the probability of halting (stopping) at current step.\n\n$s$ can be any neural network (e.g. LSTM, MLP, GRU, Attention layer).\n\nThe unconditioned probability of halting at step $n$ is then,\n\n$$p_n = \\lambda_n \\prod_{j=1}^{n-1} (1 - \\lambda_j)$$\n\nThat is the probability of not being halted at any of the previous steps and halting at step $n$.\n\nDuring inference, we halt by sampling based on the halting probability $\\lambda_n$\n and get the prediction at the halting layer $\\hat{y}_n$ as the final output.\n\nDuring training, we get the predictions from all the layers and calculate the losses for each of them.\nAnd then take the weighted average of the losses based on the probabilities of getting halted at each layer\n$p_n$.\n\nThe step function is applied to a maximum number of steps donated by $N$.\n\nThe overall loss of PonderNet is\n\n\\begin{align}\nL &= L_{Rec} + \\beta L_{Reg} \\\\\nL_{Rec} &= \\sum_{n=1}^N p_n \\mathcal{L}(y, \\hat{y}_n) \\\\\nL_{Reg} &= \\mathop{KL} \\Big(p_n \\Vert p_G(\\lambda_p) \\Big)\n\\end{align}\n\n$\\mathcal{L}$ is the normal loss function between target $y$ and prediction $\\hat{y}_n$.\n\n$\\mathop{KL}$ is the [Kullback‚ÄìLeibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n\n$p_G$ is the [Geometric distribution](https://en.wikipedia.org/wiki/Geometric_distribution) parameterized by\n$\\lambda_p$. *$\\lambda_p$ has nothing to do with $\\lambda_n$; we are just sticking to same notation as the paper*.\n$$Pr_{p_G(\\lambda_p)}(X = k) = (1 - \\lambda_p)^k \\lambda_p$$.\n\nThe regularization loss biases the network towards taking $\\frac{1}{\\lambda_p}$ steps and incentivizes\n non-zero probabilities for all steps; i.e. promotes exploration.\n\nHere is the [training code `experiment.py`](experiment.html) to train a PonderNet on [Parity Task](../parity.html).\n\"\"\"\n\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn\n\n\n\nclass ParityPonderGRU(nn.Module):\n    \"\"\"\n    ## PonderNet with GRU for Parity Task\n\n    This is a simple model that uses a [GRU Cell](https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html)\n    as the step function.\n\n    This model is for the [Parity Task](../parity.html) where the input is a vector of `n_elems`.\n    Each element of the vector is either `0`, `1` or `-1` and the output is the parity\n    - a binary value that is true if the number of `1`s is odd and false otherwise.\n\n    The prediction of the model is the log probability of the parity being $1$.\n    \"\"\"\n\n    def __init__(self, n_elems: int, n_hidden: int, max_steps: int):\n        \"\"\"\n        * `n_elems` is the number of elements in the input vector\n        * `n_hidden` is the state vector size of the GRU\n        * `max_steps` is the maximum number of steps $N$\n        \"\"\"\n        super().__init__()\n\n        self.max_steps = max_steps\n        self.n_hidden = n_hidden\n\n        # GRU\n        # $$h_{n+1} = s_h(x, h_n)$$\n        self.gru = nn.GRUCell(n_elems, n_hidden)\n        # $$\\hat{y}_n = s_y(h_n)$$\n        # We could use a layer that takes the concatenation of $h$ and $x$ as input\n        # but we went with this for simplicity.\n        self.output_layer = nn.Linear(n_hidden, 1)\n        # $$\\lambda_n = s_\\lambda(h_n)$$\n        self.lambda_layer = nn.Linear(n_hidden, 1)\n        self.lambda_prob = nn.Sigmoid()\n        # An option to set during inference so that computation is actually halted at inference time\n        self.is_halt = False\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        * `x` is the input of shape `[batch_size, n_elems]`\n\n        This outputs a tuple of four tensors:\n\n        1. $p_1 \\dots p_N$ in a tensor of shape `[N, batch_size]`\n        2. $\\hat{y}_1 \\dots \\hat{y}_N$ in a tensor of shape `[N, batch_size]` - the log probabilities of the parity being $1$\n        3. $p_m$ of shape `[batch_size]`\n        4. $\\hat{y}_m$ of shape `[batch_size]` where the computation was halted at step $m$\n        \"\"\"\n\n        #\n        batch_size = x.shape[0]\n\n        # We get initial state $h_1 = s_h(x)$\n        h = x.new_zeros((x.shape[0], self.n_hidden))\n        h = self.gru(x, h)\n\n        # Lists to store $p_1 \\dots p_N$ and $\\hat{y}_1 \\dots \\hat{y}_N$\n        p = []\n        y = []\n        # $\\prod_{j=1}^{n-1} (1 - \\lambda_j)$\n        un_halted_prob = h.new_ones((batch_size,))\n\n        # A vector to maintain which samples has halted computation\n        halted = h.new_zeros((batch_size,))\n        # $p_m$ and $\\hat{y}_m$ where the computation was halted at step $m$\n        p_m = h.new_zeros((batch_size,))\n        y_m = h.new_zeros((batch_size,))\n\n        # Iterate for $N$ steps\n        for n in range(1, self.max_steps + 1):\n            # The halting probability $\\lambda_N = 1$ for the last step\n            if n == self.max_steps:\n                lambda_n = h.new_ones(h.shape[0])\n            # $\\lambda_n = s_\\lambda(h_n)$\n            else:\n                lambda_n = self.lambda_prob(self.lambda_layer(h))[:, 0]\n            # $\\hat{y}_n = s_y(h_n)$\n            y_n = self.output_layer(h)[:, 0]\n\n            # $$p_n = \\lambda_n \\prod_{j=1}^{n-1} (1 - \\lambda_j)$$\n            p_n = un_halted_prob * lambda_n\n            # Update $\\prod_{j=1}^{n-1} (1 - \\lambda_j)$\n            un_halted_prob = un_halted_prob * (1 - lambda_n)\n\n            # Halt based on halting probability $\\lambda_n$\n            halt = torch.bernoulli(lambda_n) * (1 - halted)\n\n            # Collect $p_n$ and $\\hat{y}_n$\n            p.append(p_n)\n            y.append(y_n)\n\n            # Update $p_m$ and $\\hat{y}_m$ based on what was halted at current step $n$\n            p_m = p_m * (1 - halt) + p_n * halt\n            y_m = y_m * (1 - halt) + y_n * halt\n\n            # Update halted samples\n            halted = halted + halt\n            # Get next state $h_{n+1} = s_h(x, h_n)$\n            h = self.gru(x, h)\n\n            # Stop the computation if all samples have halted\n            if self.is_halt and halted.sum() == batch_size:\n                break\n\n        #\n        return torch.stack(p), torch.stack(y), p_m, y_m\n\n\nclass ReconstructionLoss(nn.Module):\n    \"\"\"\n    ## Reconstruction loss\n\n    $$L_{Rec} = \\sum_{n=1}^N p_n \\mathcal{L}(y, \\hat{y}_n)$$\n\n    $\\mathcal{L}$ is the normal loss function between target $y$ and prediction $\\hat{y}_n$.\n    \"\"\"\n\n    def __init__(self, loss_func: nn.Module):\n        \"\"\"\n        * `loss_func` is the loss function $\\mathcal{L}$\n        \"\"\"\n        super().__init__()\n        self.loss_func = loss_func\n\n    def forward(self, p: torch.Tensor, y_hat: torch.Tensor, y: torch.Tensor):\n        \"\"\"\n        * `p` is $p_1 \\dots p_N$ in a tensor of shape `[N, batch_size]`\n        * `y_hat` is $\\hat{y}_1 \\dots \\hat{y}_N$ in a tensor of shape `[N, batch_size, ...]`\n        * `y` is the target of shape `[batch_size, ...]`\n        \"\"\"\n\n        # The total $\\sum_{n=1}^N p_n \\mathcal{L}(y, \\hat{y}_n)$\n        total_loss = p.new_tensor(0.)\n        # Iterate upto $N$\n        for n in range(p.shape[0]):\n            # $p_n \\mathcal{L}(y, \\hat{y}_n)$ for each sample and the mean of them\n            loss = (p[n] * self.loss_func(y_hat[n], y)).mean()\n            # Add to total loss\n            total_loss = total_loss + loss\n\n        #\n        return total_loss\n\n\nclass RegularizationLoss(nn.Module):\n    \"\"\"\n    ## Regularization loss\n\n    $$L_{Reg} = \\mathop{KL} \\Big(p_n \\Vert p_G(\\lambda_p) \\Big)$$\n\n    $\\mathop{KL}$ is the [Kullback‚ÄìLeibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n\n    $p_G$ is the [Geometric distribution](https://en.wikipedia.org/wiki/Geometric_distribution) parameterized by\n    $\\lambda_p$. *$\\lambda_p$ has nothing to do with $\\lambda_n$; we are just sticking to same notation as the paper*.\n    $$Pr_{p_G(\\lambda_p)}(X = k) = (1 - \\lambda_p)^k \\lambda_p$$.\n\n    The regularization loss biases the network towards taking $\\frac{1}{\\lambda_p}$ steps and incentivies non-zero probabilities\n    for all steps; i.e. promotes exploration.\n    \"\"\"\n\n    def __init__(self, lambda_p: float, max_steps: int = 1_000):\n        \"\"\"\n        * `lambda_p` is $\\lambda_p$ - the success probability of geometric distribution\n        * `max_steps` is the highest $N$; we use this to pre-compute $p_G(\\lambda_p)$\n        \"\"\"\n        super().__init__()\n\n        # Empty vector to calculate $p_G(\\lambda_p)$\n        p_g = torch.zeros((max_steps,))\n        # $(1 - \\lambda_p)^k$\n        not_halted = 1.\n        # Iterate upto `max_steps`\n        for k in range(max_steps):\n            # $$Pr_{p_G(\\lambda_p)}(X = k) = (1 - \\lambda_p)^k \\lambda_p$$\n            p_g[k] = not_halted * lambda_p\n            # Update $(1 - \\lambda_p)^k$\n            not_halted = not_halted * (1 - lambda_p)\n\n        # Save $Pr_{p_G(\\lambda_p)}$\n        self.p_g = nn.Parameter(p_g, requires_grad=False)\n\n        # KL-divergence loss\n        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n\n    def forward(self, p: torch.Tensor):\n        \"\"\"\n        * `p` is $p_1 \\dots p_N$ in a tensor of shape `[N, batch_size]`\n        \"\"\"\n        # Transpose `p` to `[batch_size, N]`\n        p = p.transpose(0, 1)\n        # Get $Pr_{p_G(\\lambda_p)}$ upto $N$ and expand it across the batch dimension\n        p_g = self.p_g[None, :p.shape[1]].expand_as(p)\n\n        # Calculate the KL-divergence.\n        # *The [PyTorch KL-divergence](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html)\n        # implementation accepts log probabilities.*\n        return self.kl_div(p.log(), p_g)\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/adaptive_computation/ponder_net/__init__.py", "language": "python", "complexity": "complex", "star_count": 62130, "quality_score": 0.9500000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 268, "word_count": 1255}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n---\ntitle: \"PonderNet Parity Task Experiment\"\nsummary: >\n  This trains is a PonderNet on Parity Task\n---\n\n# [PonderNet](index.html) [Parity Task](../parity.html) Experiment\n\nThis trains a [PonderNet](index.html) on [Parity Task](../parity.html).\n\"\"\"\n\nfrom typing import Any\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nfrom labml import tracker, experiment\nfrom labml_nn.helpers.metrics import AccuracyDirect\nfrom labml_nn.helpers.trainer import SimpleTrainValidConfigs, BatchIndex\nfrom labml_nn.adaptive_computation.parity import ParityDataset\nfrom labml_nn.adaptive_computation.ponder_net import ParityPonderGRU, ReconstructionLoss, RegularizationLoss\n\n\nclass Configs(SimpleTrainValidConfigs):\n    \"\"\"\n    Configurations with a\n     [simple training loop](../../helpers/trainer.html)\n    \"\"\"\n\n    # Number of epochs\n    epochs: int = 100\n    # Number of batches per epoch\n    n_batches: int = 500\n    # Batch size\n    batch_size: int = 128\n\n    # Model\n    model: ParityPonderGRU\n\n    # $L_{Rec}$\n    loss_rec: ReconstructionLoss\n    # $L_{Reg}$\n    loss_reg: RegularizationLoss\n\n    # The number of elements in the input vector.\n    # *We keep it low for demonstration; otherwise, training takes a lot of time.\n    # Although the parity task seems simple, figuring out the pattern by looking at samples\n    # is quite hard.*\n    n_elems: int = 8\n    # Number of units in the hidden layer (state)\n    n_hidden: int = 64\n    # Maximum number of steps $N$\n    max_steps: int = 20\n\n    # $\\lambda_p$ for the geometric distribution $p_G(\\lambda_p)$\n    lambda_p: float = 0.2\n    # Regularization loss $L_{Reg}$ coefficient $\\beta$\n    beta: float = 0.01\n\n    # Gradient clipping by norm\n    grad_norm_clip: float = 1.0\n\n    # Training and validation loaders\n    train_loader: DataLoader\n    valid_loader: DataLoader\n\n    # Accuracy calculator\n    accuracy = AccuracyDirect()\n\n    def init(self):\n        # Print indicators to screen\n        tracker.set_scalar('loss.*', True)\n        tracker.set_scalar('loss_reg.*', True)\n        tracker.set_scalar('accuracy.*', True)\n        tracker.set_scalar('steps.*', True)\n\n        # We need to set the metrics to calculate them for the epoch for training and validation\n        self.state_modules = [self.accuracy]\n\n        # Initialize the model\n        self.model = ParityPonderGRU(self.n_elems, self.n_hidden, self.max_steps).to(self.device)\n        # $L_{Rec}$\n        self.loss_rec = ReconstructionLoss(nn.BCEWithLogitsLoss(reduction='none')).to(self.device)\n        # $L_{Reg}$\n        self.loss_reg = RegularizationLoss(self.lambda_p, self.max_steps).to(self.device)\n\n        # Training and validation loaders\n        self.train_loader = DataLoader(ParityDataset(self.batch_size * self.n_batches, self.n_elems),\n                                       batch_size=self.batch_size)\n        self.valid_loader = DataLoader(ParityDataset(self.batch_size * 32, self.n_elems),\n                                       batch_size=self.batch_size)\n\n    def step(self, batch: Any, batch_idx: BatchIndex):\n        \"\"\"\n        This method gets called by the trainer for each batch\n        \"\"\"\n        # Set the model mode\n        self.model.train(self.mode.is_train)\n\n        # Get the input and labels and move them to the model's device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Increment step in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(len(data))\n\n        # Run the model\n        p, y_hat, p_sampled, y_hat_sampled = self.model(data)\n\n        # Calculate the reconstruction loss\n        loss_rec = self.loss_rec(p, y_hat, target.to(torch.float))\n        tracker.add(\"loss.\", loss_rec)\n\n        # Calculate the regularization loss\n        loss_reg = self.loss_reg(p)\n        tracker.add(\"loss_reg.\", loss_reg)\n\n        # $L = L_{Rec} + \\beta L_{Reg}$\n        loss = loss_rec + self.beta * loss_reg\n\n        # Calculate the expected number of steps taken\n        steps = torch.arange(1, p.shape[0] + 1, device=p.device)\n        expected_steps = (p * steps[:, None]).sum(dim=0)\n        tracker.add(\"steps.\", expected_steps)\n\n        # Call accuracy metric\n        self.accuracy(y_hat_sampled > 0, target)\n\n        if self.mode.is_train:\n            # Compute gradients\n            loss.backward()\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n            # Optimizer\n            self.optimizer.step()\n            # Clear gradients\n            self.optimizer.zero_grad()\n            #\n            tracker.save()\n\n\ndef main():\n    \"\"\"\n    Run the experiment\n    \"\"\"\n    experiment.create(name='ponder_net')\n\n    conf = Configs()\n    experiment.configs(conf, {\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 0.0003,\n    })\n\n    with experiment.start():\n        conf.run()\n\n#\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/adaptive_computation/ponder_net/experiment.py", "language": "python", "complexity": "medium", "star_count": 62130, "quality_score": 0.98, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 162, "word_count": 479}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# [PonderNet: Learning to Ponder](https://nn.labml.ai/adaptive_computation/ponder_net/index.html)\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[PonderNet: Learning to Ponder](https://arxiv.org/abs/2107.05407).\n\nPonderNet adapts the computation based on the input.\nIt changes the number of steps to take on a recurrent network based on the input.\nPonderNet learns this with end-to-end gradient descent.\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/adaptive_computation/ponder_net/readme.md", "language": "markdown", "complexity": "simple", "star_count": 62130, "quality_score": 0.6000000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 9, "word_count": 48}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n---\ntitle: Capsule Networks\nsummary: >\n  PyTorch implementation and tutorial of Capsule Networks.\n  Capsule network is a neural network architecture that embeds features\n  as capsules and routes them with a voting mechanism to next layer of capsules.\n---\n\n# Capsule Networks\n\nThis is a [PyTorch](https://pytorch.org) implementation/tutorial of\n[Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829).\n\nCapsule network is a neural network architecture that embeds features\nas capsules and routes them with a voting mechanism to next layer of capsules.\n\nUnlike in other implementations of models, we've included a sample, because\nit is difficult to understand some concepts with just the modules.\n[This is the annotated code for a model that uses capsules to classify MNIST dataset](mnist.html)\n\nThis file holds the implementations of the core modules of Capsule Networks.\n\nI used [jindongwang/Pytorch-CapsuleNet](https://github.com/jindongwang/Pytorch-CapsuleNet) to clarify some\nconfusions I had with the paper.\n\nHere's a notebook for training a Capsule Network on MNIST dataset.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/capsule_networks/mnist.ipynb)\n\"\"\"\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\n\n\nclass Squash(nn.Module):\n    \"\"\"\n    ## Squash\n\n    This is **squashing** function from paper, given by equation $(1)$.\n\n    $$\\mathbf{v}_j = \\frac{{\\lVert \\mathbf{s}_j \\rVert}^2}{1 + {\\lVert \\mathbf{s}_j \\rVert}^2}\n     \\frac{\\mathbf{s}_j}{\\lVert \\mathbf{s}_j \\rVert}$$\n\n    $\\frac{\\mathbf{s}_j}{\\lVert \\mathbf{s}_j \\rVert}$\n    normalizes the length of all the capsules, whilst\n    $\\frac{{\\lVert \\mathbf{s}_j \\rVert}^2}{1 + {\\lVert \\mathbf{s}_j \\rVert}^2}$\n    shrinks the capsules that have a length smaller than one .\n    \"\"\"\n\n    def __init__(self, epsilon=1e-8):\n        super().__init__()\n        self.epsilon = epsilon\n\n    def forward(self, s: torch.Tensor):\n        \"\"\"\n        The shape of `s` is `[batch_size, n_capsules, n_features]`\n        \"\"\"\n\n        # ${\\lVert \\mathbf{s}_j \\rVert}^2$\n        s2 = (s ** 2).sum(dim=-1, keepdims=True)\n\n        # We add an epsilon when calculating $\\lVert \\mathbf{s}_j \\rVert$ to make sure it doesn't become zero.\n        # If this becomes zero it starts giving out `nan` values and training fails.\n        # $$\\mathbf{v}_j = \\frac{{\\lVert \\mathbf{s}_j \\rVert}^2}{1 + {\\lVert \\mathbf{s}_j \\rVert}^2}\n        # \\frac{\\mathbf{s}_j}{\\sqrt{{\\lVert \\mathbf{s}_j \\rVert}^2 + \\epsilon}}$$\n        return (s2 / (1 + s2)) * (s / torch.sqrt(s2 + self.epsilon))\n\n\nclass Router(nn.Module):\n    \"\"\"\n    ## Routing Algorithm\n\n    This is the routing mechanism described in the paper.\n    You can use multiple routing layers in your models.\n\n    This combines calculating $\\mathbf{s}_j$ for this layer and\n    the routing algorithm described in *Procedure 1*.\n    \"\"\"\n\n    def __init__(self, in_caps: int, out_caps: int, in_d: int, out_d: int, iterations: int):\n        \"\"\"\n        `in_caps` is the number of capsules, and `in_d` is the number of features per capsule from the layer below.\n        `out_caps` and `out_d` are the same for this layer.\n\n        `iterations` is the number of routing iterations, symbolized by $r$ in the paper.\n        \"\"\"\n        super().__init__()\n        self.in_caps = in_caps\n        self.out_caps = out_caps\n        self.iterations = iterations\n        self.softmax = nn.Softmax(dim=1)\n        self.squash = Squash()\n\n        # This is the weight matrix $\\mathbf{W}_{ij}$. It maps each capsule in the\n        # lower layer to each capsule in this layer\n        self.weight = nn.Parameter(torch.randn(in_caps, out_caps, in_d, out_d), requires_grad=True)\n\n    def forward(self, u: torch.Tensor):\n        \"\"\"\n        The shape of `u` is `[batch_size, n_capsules, n_features]`.\n        These are the capsules from the lower layer.\n        \"\"\"\n\n        # $$\\hat{\\mathbf{u}}_{j|i} = \\mathbf{W}_{ij} \\mathbf{u}_i$$\n        # Here $j$ is used to index capsules in this layer, whilst $i$ is\n        # used to index capsules in the layer below (previous).\n        u_hat = torch.einsum('ijnm,bin->bijm', self.weight, u)\n\n        # Initial logits $b_{ij}$ are the log prior probabilities that capsule $i$\n        # should be coupled with $j$.\n        # We initialize these at zero\n        b = u.new_zeros(u.shape[0], self.in_caps, self.out_caps)\n\n        v = None\n\n        # Iterate\n        for i in range(self.iterations):\n            # routing softmax $$c_{ij} = \\frac{\\exp({b_{ij}})}{\\sum_k\\exp({b_{ik}})}$$\n            c = self.softmax(b)\n            # $$\\mathbf{s}_j = \\sum_i{c_{ij} \\hat{\\mathbf{u}}_{j|i}}$$\n            s = torch.einsum('bij,bijm->bjm', c, u_hat)\n            # $$\\mathbf{v}_j = squash(\\mathbf{s}_j)$$\n            v = self.squash(s)\n            # $$a_{ij} = \\mathbf{v}_j \\cdot \\hat{\\mathbf{u}}_{j|i}$$\n            a = torch.einsum('bjm,bijm->bij', v, u_hat)\n            # $$b_{ij} \\gets b_{ij} + \\mathbf{v}_j \\cdot \\hat{\\mathbf{u}}_{j|i}$$\n            b = b + a\n\n        return v\n\n\nclass MarginLoss(nn.Module):\n    \"\"\"\n    ## Margin loss for class existence\n\n    A separate margin loss is used for each output capsule and the total loss is the sum of them.\n    The length of each output capsule is the probability that class is present in the input.\n\n    Loss for each output capsule or class $k$ is,\n    $$\\mathcal{L}_k = T_k \\max(0, m^{+} - \\lVert\\mathbf{v}_k\\rVert)^2 +\n    \\lambda (1 - T_k) \\max(0, \\lVert\\mathbf{v}_k\\rVert - m^{-})^2$$\n\n    $T_k$ is $1$ if the class $k$ is present and $0$ otherwise.\n    The first component of the loss is $0$ when the class is not present,\n    and the second component is $0$ if the class is present.\n    The $\\max(0, x)$ is used to avoid predictions going to extremes.\n    $m^{+}$ is set to be $0.9$ and $m^{-}$ to be $0.1$ in the paper.\n\n    The $\\lambda$ down-weighting is used to stop the length of all capsules from\n    falling during the initial phase of training.\n    \"\"\"\n\n    def __init__(self, *, n_labels: int, lambda_: float = 0.5, m_positive: float = 0.9, m_negative: float = 0.1):\n        super().__init__()\n\n        self.m_negative = m_negative\n        self.m_positive = m_positive\n        self.lambda_ = lambda_\n        self.n_labels = n_labels\n\n    def forward(self, v: torch.Tensor, labels: torch.Tensor):\n        \"\"\"\n        `v`, $\\mathbf{v}_j$ are the squashed output capsules.\n        This has shape `[batch_size, n_labels, n_features]`; that is, there is a capsule for each label.\n\n        `labels` are the labels, and has shape `[batch_size]`.\n        \"\"\"\n        # $$\\lVert \\mathbf{v}_j \\rVert$$\n        v_norm = torch.sqrt((v ** 2).sum(dim=-1))\n\n        # $$\\mathcal{L}$$\n        # `labels` is one-hot encoded labels of shape `[batch_size, n_labels]`\n        labels = torch.eye(self.n_labels, device=labels.device)[labels]\n\n        # $$\\mathcal{L}_k = T_k \\max(0, m^{+} - \\lVert\\mathbf{v}_k\\rVert)^2 +\n        # \\lambda (1 - T_k) \\max(0, \\lVert\\mathbf{v}_k\\rVert - m^{-})^2$$\n        # `loss` has shape `[batch_size, n_labels]`. We have parallelized the computation\n        # of $\\mathcal{L}_k$ for for all $k$.\n        loss = labels * F.relu(self.m_positive - v_norm) + \\\n               self.lambda_ * (1.0 - labels) * F.relu(v_norm - self.m_negative)\n\n        # $$\\sum_k \\mathcal{L}_k$$\n        return loss.sum(dim=-1).mean()\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/capsule_networks/__init__.py", "language": "python", "complexity": "medium", "star_count": 62130, "quality_score": 0.9500000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 186, "word_count": 899}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n---\ntitle: Classify MNIST digits with Capsule Networks\nsummary: Code for training Capsule Networks on MNIST dataset\n---\n\n# Classify MNIST digits with Capsule Networks\n\nThis is an annotated PyTorch code to classify MNIST digits with PyTorch.\n\nThis paper implements the experiment described in paper\n[Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829).\n\"\"\"\nfrom typing import Any\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom labml import experiment, tracker\nfrom labml.configs import option\nfrom labml_nn.capsule_networks import Squash, Router, MarginLoss\nfrom labml_nn.helpers.datasets import MNISTConfigs\nfrom labml_nn.helpers.metrics import AccuracyDirect\nfrom labml_nn.helpers.trainer import SimpleTrainValidConfigs, BatchIndex\n\n\nclass MNISTCapsuleNetworkModel(nn.Module):\n    \"\"\"\n    ## Model for classifying MNIST digits\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # First convolution layer has $256$, $9 \\times 9$ convolution kernels\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9, stride=1)\n        # The second layer (Primary Capsules) s a convolutional capsule layer with $32$ channels\n        # of convolutional $8D$ capsules ($8$ features per capsule).\n        # That is, each primary capsule contains 8 convolutional units with a 9 √ó 9 kernel and a stride of 2.\n        # In order to implement this we create a convolutional layer with $32 \\times 8$ channels and\n        # reshape and permutate its output to get the capsules of $8$ features each.\n        self.conv2 = nn.Conv2d(in_channels=256, out_channels=32 * 8, kernel_size=9, stride=2, padding=0)\n        self.squash = Squash()\n\n        # Routing layer gets the $32 \\times 6 \\times 6$ primary capsules and produces $10$ capsules.\n        # Each of the primary capsules have $8$ features, while output capsules (Digit Capsules)\n        # have $16$ features.\n        # The routing algorithm iterates $3$ times.\n        self.digit_capsules = Router(32 * 6 * 6, 10, 8, 16, 3)\n\n        # This is the decoder mentioned in the paper.\n        # It takes the outputs of the $10$ digit capsules, each with $16$ features to reproduce the\n        # image. It goes through linear layers of sizes $512$ and $1024$ with $ReLU$ activations.\n        self.decoder = nn.Sequential(\n            nn.Linear(16 * 10, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 784),\n            nn.Sigmoid()\n        )\n\n    def forward(self, data: torch.Tensor):\n        \"\"\"\n        `data` are the MNIST images, with shape `[batch_size, 1, 28, 28]`\n        \"\"\"\n        # Pass through the first convolution layer.\n        # Output of this layer has shape `[batch_size, 256, 20, 20]`\n        x = F.relu(self.conv1(data))\n        # Pass through the second convolution layer.\n        # Output of this has shape `[batch_size, 32 * 8, 6, 6]`.\n        # *Note that this layer has a stride length of $2$*.\n        x = self.conv2(x)\n\n        # Resize and permutate to get the capsules\n        caps = x.view(x.shape[0], 8, 32 * 6 * 6).permute(0, 2, 1)\n        # Squash the capsules\n        caps = self.squash(caps)\n        # Take them through the router to get digit capsules.\n        # This has shape `[batch_size, 10, 16]`.\n        caps = self.digit_capsules(caps)\n\n        # Get masks for reconstructioon\n        with torch.no_grad():\n            # The prediction by the capsule network is the capsule with longest length\n            pred = (caps ** 2).sum(-1).argmax(-1)\n            # Create a mask to maskout all the other capsules\n            mask = torch.eye(10, device=data.device)[pred]\n\n        # Mask the digit capsules to get only the capsule that made the prediction and\n        # take it through decoder to get reconstruction\n        reconstructions = self.decoder((caps * mask[:, :, None]).view(x.shape[0], -1))\n        # Reshape the reconstruction to match the image dimensions\n        reconstructions = reconstructions.view(-1, 1, 28, 28)\n\n        return caps, reconstructions, pred\n\n\nclass Configs(MNISTConfigs, SimpleTrainValidConfigs):\n    \"\"\"\n    Configurations with MNIST data and Train & Validation setup\n    \"\"\"\n    epochs: int = 10\n    model: nn.Module = 'capsule_network_model'\n    reconstruction_loss = nn.MSELoss()\n    margin_loss = MarginLoss(n_labels=10)\n    accuracy = AccuracyDirect()\n\n    def init(self):\n        # Print losses and accuracy to screen\n        tracker.set_scalar('loss.*', True)\n        tracker.set_scalar('accuracy.*', True)\n\n        # We need to set the metrics to calculate them for the epoch for training and validation\n        self.state_modules = [self.accuracy]\n\n    def step(self, batch: Any, batch_idx: BatchIndex):\n        \"\"\"\n        This method gets called by the trainer\n        \"\"\"\n        # Set the model mode\n        self.model.train(self.mode.is_train)\n\n        # Get the images and labels and move them to the model's device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Increment step in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(len(data))\n\n        # Run the model\n        caps, reconstructions, pred = self.model(data)\n\n        # Calculate the total loss\n        loss = self.margin_loss(caps, target) + 0.0005 * self.reconstruction_loss(reconstructions, data)\n        tracker.add(\"loss.\", loss)\n\n        # Call accuracy metric\n        self.accuracy(pred, target)\n\n        if self.mode.is_train:\n            loss.backward()\n\n            self.optimizer.step()\n            # Log parameters and gradients\n            if batch_idx.is_last:\n                tracker.add('model', self.model)\n            self.optimizer.zero_grad()\n\n            tracker.save()\n\n\n@option(Configs.model)\ndef capsule_network_model(c: Configs):\n    \"\"\"Set the model\"\"\"\n    return MNISTCapsuleNetworkModel().to(c.device)\n\n\ndef main():\n    \"\"\"\n    Run the experiment\n    \"\"\"\n    experiment.create(name='capsule_network_mnist')\n    conf = Configs()\n    experiment.configs(conf, {'optimizer.optimizer': 'Adam',\n                              'optimizer.learning_rate': 1e-3})\n\n    experiment.add_pytorch_models({'model': conf.model})\n\n    with experiment.start():\n        conf.run()\n\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/capsule_networks/mnist.py", "language": "python", "complexity": "medium", "star_count": 62130, "quality_score": 0.98, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 175, "word_count": 717}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# [Capsule Networks](https://nn.labml.ai/capsule_networks/index.html)\n\nThis is a [PyTorch](https://pytorch.org) implementation/tutorial of\n[Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829).\n\nCapsule network is a neural network architecture that embeds features\nas capsules and routes them with a voting mechanism to next layer of capsules.\n\nUnlike in other implementations of models, we've included a sample, because\nit is difficult to understand some concepts with just the modules.\n[This is the annotated code for a model that uses capsules to classify MNIST dataset](mnist.html)\n\nThis file holds the implementations of the core modules of Capsule Networks.\n\nI used [jindongwang/Pytorch-CapsuleNet](https://github.com/jindongwang/Pytorch-CapsuleNet) to clarify some\nconfusions I had with the paper.\n\nHere's a notebook for training a Capsule Network on MNIST dataset.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/capsule_networks/mnist.ipynb)\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/capsule_networks/readme.md", "language": "markdown", "complexity": "medium", "star_count": 62130, "quality_score": 0.8, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 21, "word_count": 112}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n---\ntitle: Regret Minimization in Games with Incomplete Information (CFR)\nsummary: >\n  This is an annotated implementation/tutorial of Regret Minimization in Games with Incomplete Information\n---\n\n# Regret Minimization in Games with Incomplete Information (CFR)\n\nThe paper\n[Regret Minimization in Games with Incomplete Information](http://martin.zinkevich.org/publications/regretpoker.pdf)\nintroduces counterfactual regret and how minimizing counterfactual regret through self-play\ncan be used to reach Nash equilibrium.\nThe algorithm is called Counterfactual Regret Minimization (**CFR**).\n\nThe paper\n[Monte Carlo Sampling for Regret Minimization in Extensive Games](http://mlanctot.info/files/papers/nips09mccfr.pdf)\nintroduces Monte Carlo Counterfactual Regret Minimization (**MCCFR**),\nwhere we sample from the game tree and estimate the regrets.\n\nWe tried to keep our Python implementation easy-to-understand like a tutorial.\nWe run it on [a very simple imperfect information game called Kuhn poker](kuhn/index.html).\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/cfr/kuhn/experiment.ipynb)\n\n[![Twitter thread](https://img.shields.io/twitter/url?style=social&url=https%3A%2F%2Ftwitter.com%2Flabmlai%2Fstatus%2F1407186002255380484)](https://twitter.com/labmlai/status/1407186002255380484)\nTwitter thread\n\n## Introduction\n\nWe implement Monte Carlo Counterfactual Regret Minimization (MCCFR) with chance sampling (CS).\nIt iteratively, explores part of the game tree by trying all player actions,\nbut sampling chance events.\nChance events are things like dealing cards; they are kept sampled once per iteration.\nThen it calculates, for each action, the *regret* of following the current strategy instead of taking that action.\nThen it updates the strategy based on these regrets for the next iteration, using regret matching.\nFinally, it computes the average of the strategies throughout the iterations,\nwhich is very close to the Nash equilibrium if we ran enough iterations.\n\nWe will first introduce the mathematical notation and theory.\n\n### Player\n\nA player is denoted by $i \\in N$, where $N$ is the set of players.\n\n### [History](#History)\n\nHistory $h \\in H$ is a sequence of actions including chance events,\n and $H$ is the set of all histories.\n\n$Z \\subseteq H$ is the set of terminal histories (game over).\n\n### Action\n\nAction $a$, $A(h) = {a: (h, a) \\in H}$ where $h \\in H$ is a non-terminal [history](#History).\n\n### [Information Set $I_i$](#InfoSet)\n\n**Information set** $I_i \\in \\mathcal{I}_i$ for player $i$\nis similar to a history $h \\in H$\nbut only contains the actions visible to player $i$.\nThat is, the history $h$ will contain actions/events such as cards dealt to the\nopposing player while $I_i$ will not have them.\n\n$\\mathcal{I}_i$ is known as the **information partition** of player $i$.\n\n$h \\in I$ is the set of all histories that belong to a given information set;\ni.e. all those histories look the same in the eye of the player.\n\n<a id=\"Strategy\"></a>\n\n### Strategy\n\n**Strategy of player** $i$, $\\sigma_i \\in \\Sigma_i$ is a distribution over actions $A(I_i)$,\nwhere $\\Sigma_i$ is the set of all strategies for player $i$.\nStrategy on $t$-th iteration is denoted by $\\sigma^t_i$.\n\nStrategy is defined as a probability for taking an action $a$ in for a given information set $I$,\n\n$$\\sigma_i(I)(a)$$\n\n$\\sigma$ is the **strategy profile** which consists of strategies of all players\n $\\sigma_1, \\sigma_2, \\ldots$\n\n$\\sigma_{-i}$ is strategies of all players except $\\sigma_i$\n\n<a id=\"HistoryProbability\"></a>\n\n### Probability of History\n\n$\\pi^\\sigma(h)$ is the probability of reaching the history $h$ with strategy profile $\\sigma$.\n$\\pi^\\sigma(h)_{-i}$ is the probability of reaching $h$ without player $i$'s contribution;\n i.e. player $i$ took the actions to follow $h$ with a probability of $1$.\n\n$\\pi^\\sigma(h)_{i}$ is the probability of reaching $h$ with only player $i$'s contribution.\nThat is,\n$$\\pi^\\sigma(h) = \\pi^\\sigma(h)_{i} \\pi^\\sigma(h)_{-i}$$\n\nProbability of reaching a information set $I$ is,\n$$\\pi^\\sigma(I) = \\sum_{h \\in I} \\pi^\\sigma(h)$$\n\n### Utility (Pay off)\n\nThe [terminal utility](#terminal_utility) is the utility (or pay off)\n of a player $i$ for a terminal history $h$.\n\n$$u_i(h)$$ where $h \\in Z$\n\n$u_i(\\sigma)$ is the expected utility (payoff) for player $i$ with strategy profile $\\sigma$.\n\n$$u_i(\\sigma) = \\sum_{h \\in Z} u_i(h) \\pi^\\sigma(h)$$\n\n<a id=\"NashEquilibrium\"></a>\n\n### Nash Equilibrium\n\nNash equilibrium is a state where none of the players can increase their expected utility (or payoff)\nby changing their strategy alone.\n\nFor two players, Nash equilibrium is a [strategy profile](#Strategy) where\n\n\\begin{align}\nu_1(\\sigma) &\\ge \\max_{\\sigma'_1 \\in \\Sigma_1} u_1(\\sigma'_1, \\sigma_2) \\\\\nu_2(\\sigma) &\\ge \\max_{\\sigma'_2 \\in \\Sigma_2} u_1(\\sigma_1, \\sigma'_2) \\\\\n\\end{align}\n\n$\\epsilon$-Nash equilibrium is,\n\n\\begin{align}\nu_1(\\sigma) + \\epsilon &\\ge \\max_{\\sigma'_1 \\in \\Sigma_1} u_1(\\sigma'_1, \\sigma_2) \\\\\nu_2(\\sigma)  + \\epsilon &\\ge \\max_{\\sigma'_2 \\in \\Sigma_2} u_1(\\sigma_1, \\sigma'_2) \\\\\n\\end{align}\n\n### Regret Minimization\n\nRegret is the utility (or pay off) that the player didn't get because\n she didn't follow the optimal strategy or took the best action.\n\nAverage overall regret for Player $i$ is the average regret of not following the\noptimal strategy in all $T$ rounds of iterations.\n\n$$R^T_i = \\frac{1}{T} \\max_{\\sigma^*_i \\in \\Sigma_i} \\sum_{t=1}^T\n\\Big( u_i(\\sigma^*_i, \\sigma^t_{-i}) - u_i(\\sigma^t) \\Big)$$\n\nwhere $\\sigma^t$ is the strategy profile of all players in iteration $t$,\nand\n\n$$(\\sigma^*_i, \\sigma^t_{-i})$$\n\nis the strategy profile $\\sigma^t$ with player $i$'s strategy\nreplaced with $\\sigma^*_i$.\n\nThe average strategy is the average of strategies followed in each round,\n for all $I \\in \\mathcal{I}, a \\in A(I)$\n\n$$\\textcolor{cyan}{\\bar{\\sigma}^T_i(I)(a)} =\n \\frac{\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}}{\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)}$$\n\nThat is the mean regret of not playing with the optimal strategy.\n\nIf $R^T_i < \\epsilon$ for all players then $\\bar{\\sigma}^T_i(I)(a)$ is a\n$2\\epsilon$-Nash equilibrium.\n\n\\begin{align}\nR^T_i &< \\epsilon \\\\\nR^T_i &= \\frac{1}{T} \\max_{\\sigma^*_i \\in \\Sigma_i} \\sum_{t=1}^T\n\\Big( u_i(\\sigma^*_i, \\sigma^t_{-i}) - u_i(\\sigma^t) \\Big) \\\\\n&= \\frac{1}{T} \\max_{\\sigma^*_i \\in \\Sigma_i} \\sum_{t=1}^T u_i(\\sigma^*_i, \\sigma^t_{-i})\n- \\frac{1}{T} \\sum_{t=1}^T u_i(\\sigma^t) < \\epsilon\n\\end{align}\n\nSince $u_1 = -u_2$ because it's a zero-sum game, we can add $R^T_1$ and $R^T_i$ and the\nsecond term will cancel out.\n\n\\begin{align}\n2\\epsilon &>\n\\frac{1}{T} \\max_{\\sigma^*_1 \\in \\Sigma_1} \\sum_{t=1}^T u_1(\\sigma^*_1, \\sigma^t_{-1}) +\n\\frac{1}{T} \\max_{\\sigma^*_2 \\in \\Sigma_2} \\sum_{t=1}^T u_2(\\sigma^*_2, \\sigma^t_{-2})\n\\end{align}\n\nThe average of utilities over a set of strategies is equal to the utility of the average strategy.\n\n$$\\frac{1}{T} \\sum_{t=1}^T u_i(\\sigma^t) = u_i(\\bar{\\sigma}^T)$$\n\nTherefore,\n\n\\begin{align}\n2\\epsilon &>\n\\max_{\\sigma^*_1 \\in \\Sigma_1} u_1(\\sigma^*_1, \\bar{\\sigma}^T_{-1}) +\n\\max_{\\sigma^*_2 \\in \\Sigma_2} u_2(\\sigma^*_2, \\bar{\\sigma}^T_{-2})\n\\end{align}\n\nFrom the definition of $\\max$,\n$$\\max_{\\sigma^*_2 \\in \\Sigma_2} u_2(\\sigma^*_2, \\bar{\\sigma}^T_{-2}) \\ge u_2(\\bar{\\sigma}^T)\n = -u_1(\\bar{\\sigma}^T)$$\n\nThen,\n\n\\begin{align}\n2\\epsilon &>\n\\max_{\\sigma^*_1 \\in \\Sigma_1} u_1(\\sigma^*_1, \\bar{\\sigma}^T_{-1}) +\n-u_1(\\bar{\\sigma}^T) \\\\\nu_1(\\bar{\\sigma}^T) + 2\\epsilon &> \\max_{\\sigma^*_1 \\in \\Sigma_1} u_1(\\sigma^*_1, \\bar{\\sigma}^T_{-1})\n\\end{align}\n\nThis is $2\\epsilon$-Nash equilibrium.\nYou can similarly prove for games with more than 2 players.\n\nSo we need to minimize $R^T_i$ to get close to a Nash equilibrium.\n\n<a id=\"CounterfactualRegret\"></a>\n\n### Counterfactual regret\n\n**Counterfactual value** $\\textcolor{pink}{v_i(\\sigma, I)}$ is the expected utility for player $i$ if\n if player $i$ tried to reach $I$ (took the actions leading to $I$ with a probability of $1$).\n\n$$\\textcolor{pink}{v_i(\\sigma, I)} = \\sum_{z \\in Z_I} \\pi^\\sigma_{-i}(z[I]) \\pi^\\sigma(z[I], z) u_i(z)$$\n\nwhere $Z_I$ is the set of terminal histories reachable from $I$,\nand $z[I]$ is the prefix of $z$ up to $I$.\n$\\pi^\\sigma(z[I], z)$ is the probability of reaching z from $z[I]$.\n\n**Immediate counterfactual regret** is,\n\n$$R^T_{i,imm}(I) = \\max_{a \\in A{I}} R^T_{i,imm}(I, a)$$\n\nwhere\n\n$$R^T_{i,imm}(I) = \\frac{1}{T} \\sum_{t=1}^T\n\\Big(\n\\textcolor{pink}{v_i(\\sigma^t |_{I \\rightarrow a}, I)} - \\textcolor{pink}{v_i(\\sigma^t, I)}\n\\Big)$$\n\nwhere $\\sigma |_{I \\rightarrow a}$ is the strategy profile $\\sigma$ with the modification\nof always taking action $a$ at information set $I$.\n\nThe [paper](http://martin.zinkevich.org/publications/regretpoker.pdf) proves that (Theorem 3),\n\n$$R^T_i \\le \\sum_{I \\in \\mathcal{I}} R^{T,+}_{i,imm}(I)$$\nwhere $$R^{T,+}_{i,imm}(I) = \\max(R^T_{i,imm}(I), 0)$$\n\n<a id=\"RegretMatching\"></a>\n\n### Regret Matching\n\nThe strategy is calculated using regret matching.\n\nThe regret for each information set and action pair $\\textcolor{orange}{R^T_i(I, a)}$ is maintained,\n\n\\begin{align}\n\\textcolor{coral}{r^t_i(I, a)} &=\n \\textcolor{pink}{v_i(\\sigma^t |_{I \\rightarrow a}, I)} - \\textcolor{pink}{v_i(\\sigma^t, I)}\n \\\\\n\\textcolor{orange}{R^T_i(I, a)} &=\n \\frac{1}{T} \\sum_{t=1}^T \\textcolor{coral}{r^t_i(I, a)}\n\\end{align}\n\nand the strategy is calculated with regret matching,\n\n\\begin{align}\n\\textcolor{lightgreen}{\\sigma_i^{T+1}(I)(a)} =\n\\begin{cases}\n\\frac{\\textcolor{orange}{R^{T,+}_i(I, a)}}{\\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')}},\n  & \\text{if} \\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')} \\gt 0 \\\\\n\\frac{1}{\\lvert A(I) \\rvert},\n & \\text{otherwise}\n\\end{cases}\n\\end{align}\n\nwhere $\\textcolor{orange}{R^{T,+}_i(I, a)} = \\max \\Big(\\textcolor{orange}{R^T_i(I, a)}, 0 \\Big)$\n\nThe paper\nThe paper\n[Regret Minimization in Games with Incomplete Information](http://martin.zinkevich.org/publications/regretpoker.pdf)\nproves that if the strategy is selected according to above equation\n$R^T_i$ gets smaller proportionate to $\\frac{1}{\\sqrt T}$, and\ntherefore reaches $\\epsilon$-[Nash equilibrium](#NashEquilibrium).\n\n<a id=\"MCCFR\"></a>\n\n### Monte Carlo CFR (MCCFR)\n\nComputing $\\textcolor{coral}{r^t_i(I, a)}$ requires expanding the full game tree\non each iteration.\n\nThe paper\n[Monte Carlo Sampling for Regret Minimization in Extensive Games](http://mlanctot.info/files/papers/nips09mccfr.pdf)\nshows we can sample from the game tree and estimate the regrets.\n\n$\\mathcal{Q} = {Q_1, \\ldots, Q_r}$ is a set of subsets of $Z$ ($Q_j \\subseteq Z$) where\nwe look at only a single block $Q_j$ in an iteration.\nUnion of all subsets spans $Z$ ($Q_1 \\cap \\ldots \\cap Q_r = Z$).\n$q_j$ is the probability of picking block $Q_j$.\n\n$q(z)$ is the probability of picking $z$ in current iteration; i.e. $q(z) = \\sum_{j:z \\in Q_j} q_j$ -\nthe sum of $q_j$ where $z \\in Q_j$.\n\nThen we get **sampled counterfactual value** fro block $j$,\n\n$$\\textcolor{pink}{\\tilde{v}(\\sigma, I|j)} =\n \\sum_{z \\in Q_j} \\frac{1}{q(z)}\n \\pi^\\sigma_{-i}(z[I]) \\pi^\\sigma(z[I], z) u_i(z)$$\n\nThe paper shows that\n\n$$\\mathbb{E}_{j \\sim q_j} \\Big[ \\textcolor{pink}{\\tilde{v}(\\sigma, I|j)} \\Big]\n= \\textcolor{pink}{v_i(\\sigma, I)}$$\n\nwith a simple proof.\n\nTherefore we can sample a part of the game tree and calculate the regrets.\nWe calculate an estimate of regrets\n\n$$\n\\textcolor{coral}{\\tilde{r}^t_i(I, a)} =\n \\textcolor{pink}{\\tilde{v}_i(\\sigma^t |_{I \\rightarrow a}, I)} - \\textcolor{pink}{\\tilde{v}_i(\\sigma^t, I)}\n$$\n\nAnd use that to update $\\textcolor{orange}{R^T_i(I, a)}$ and calculate\n the strategy $\\textcolor{lightgreen}{\\sigma_i^{T+1}(I)(a)}$ on each iteration.\nFinally, we calculate the overall average strategy $\\textcolor{cyan}{\\bar{\\sigma}^T_i(I)(a)}$.\n\nHere is a [Kuhn Poker](kuhn/index.html) implementation to try CFR on Kuhn Poker.\n\n*Let's dive into the code!*\n\"\"\"\nfrom typing import NewType, Dict, List, Callable, cast\n\nfrom labml import monit, tracker, logger, experiment\nfrom labml.configs import BaseConfigs, option\n\n# A player $i \\in N$ where $N$ is the set of players\nPlayer = NewType('Player', int)\n# Action $a$, $A(h) = {a: (h, a) \\in H}$ where $h \\in H$ is a non-terminal [history](#History)\nAction = NewType('Action', str)\n\n\nclass History:\n    \"\"\"\n    <a id=\"History\"></a>\n\n    ## History\n\n    History $h \\in H$ is a sequence of actions including chance events,\n     and $H$ is the set of all histories.\n\n    This class should be extended with game specific logic.\n    \"\"\"\n\n    def is_terminal(self):\n        \"\"\"\n        Whether it's a terminal history; i.e. game over.\n        $h \\in Z$\n        \"\"\"\n        raise NotImplementedError()\n\n    def terminal_utility(self, i: Player) -> float:\n        \"\"\"\n        <a id=\"terminal_utility\"></a>\n        Utility of player $i$ for a terminal history.\n        $u_i(h)$ where $h \\in Z$\n        \"\"\"\n        raise NotImplementedError()\n\n    def player(self) -> Player:\n        \"\"\"\n        Get current player, denoted by $P(h)$, where $P$ is known as **Player function**.\n\n        If $P(h) = c$ it means that current event is a chance $c$ event.\n        Something like dealing cards, or opening common cards in poker.\n        \"\"\"\n        raise NotImplementedError()\n\n    def is_chance(self) -> bool:\n        \"\"\"\n        Whether the next step is a chance step; something like dealing a new card.\n        $P(h) = c$\n        \"\"\"\n        raise NotImplementedError()\n\n    def sample_chance(self) -> Action:\n        \"\"\"\n        Sample a chance when $P(h) = c$.\n        \"\"\"\n        raise NotImplementedError()\n\n    def __add__(self, action: Action):\n        \"\"\"\n        Add an action to the history.\n        \"\"\"\n        raise NotImplementedError()\n\n    def info_set_key(self) -> str:\n        \"\"\"\n        Get [information set](#InfoSet) for the current player\n        \"\"\"\n        raise NotImplementedError\n\n    def new_info_set(self) -> 'InfoSet':\n        \"\"\"\n        Create a new [information set](#InfoSet) for the current player\n        \"\"\"\n        raise NotImplementedError()\n\n    def __repr__(self):\n        \"\"\"\n        Human readable representation\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass InfoSet:\n    \"\"\"\n    <a id=\"InfoSet\"></a>\n\n    ## Information Set $I_i$\n    \"\"\"\n\n    # Unique key identifying the information set\n    key: str\n    # $\\sigma_i$, the [strategy](#Strategy) of player $i$\n    strategy: Dict[Action, float]\n    # Total regret of not taking each action $A(I_i)$,\n    #\n    # \\begin{align}\n    # \\textcolor{coral}{\\tilde{r}^t_i(I, a)} &=\n    #  \\textcolor{pink}{\\tilde{v}_i(\\sigma^t |_{I \\rightarrow a}, I)} -\n    #  \\textcolor{pink}{\\tilde{v}_i(\\sigma^t, I)}\n    # \\\\\n    # \\textcolor{orange}{R^T_i(I, a)} &=\n    #  \\frac{1}{T} \\sum_{t=1}^T \\textcolor{coral}{\\tilde{r}^t_i(I, a)}\n    # \\end{align}\n    #\n    # We maintain $T \\textcolor{orange}{R^T_i(I, a)}$ instead of $\\textcolor{orange}{R^T_i(I, a)}$\n    # since $\\frac{1}{T}$ term cancels out anyway when computing strategy\n    # $\\textcolor{lightgreen}{\\sigma_i^{T+1}(I)(a)}$\n    regret: Dict[Action, float]\n    # We maintain the cumulative strategy\n    # $$\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}$$\n    # to compute overall average strategy\n    #\n    # $$\\textcolor{cyan}{\\bar{\\sigma}^T_i(I)(a)} =\n    #  \\frac{\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}}{\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)}$$\n    cumulative_strategy: Dict[Action, float]\n\n    def __init__(self, key: str):\n        \"\"\"\n        Initialize\n        \"\"\"\n        self.key = key\n        self.regret = {a: 0 for a in self.actions()}\n        self.cumulative_strategy = {a: 0 for a in self.actions()}\n        self.calculate_strategy()\n\n    def actions(self) -> List[Action]:\n        \"\"\"\n        Actions $A(I_i)$\n        \"\"\"\n        raise NotImplementedError()\n\n    @staticmethod\n    def from_dict(data: Dict[str, any]) -> 'InfoSet':\n        \"\"\"\n        Load information set from a saved dictionary\n        \"\"\"\n        raise NotImplementedError()\n\n    def to_dict(self):\n        \"\"\"\n        Save the information set to a dictionary\n        \"\"\"\n        return {\n            'key': self.key,\n            'regret': self.regret,\n            'average_strategy': self.cumulative_strategy,\n        }\n\n    def load_dict(self, data: Dict[str, any]):\n        \"\"\"\n        Load data from a saved dictionary\n        \"\"\"\n        self.regret = data['regret']\n        self.cumulative_strategy = data['average_strategy']\n        self.calculate_strategy()\n\n    def calculate_strategy(self):\n        \"\"\"\n        ## Calculate strategy\n\n        Calculate current strategy using [regret matching](#RegretMatching).\n\n        \\begin{align}\n        \\textcolor{lightgreen}{\\sigma_i^{T+1}(I)(a)} =\n        \\begin{cases}\n        \\frac{\\textcolor{orange}{R^{T,+}_i(I, a)}}{\\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')}},\n          & \\text{if} \\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')} \\gt 0 \\\\\n        \\frac{1}{\\lvert A(I) \\rvert},\n         & \\text{otherwise}\n        \\end{cases}\n        \\end{align}\n\n        where $\\textcolor{orange}{R^{T,+}_i(I, a)} = \\max \\Big(\\textcolor{orange}{R^T_i(I, a)}, 0 \\Big)$\n        \"\"\"\n        # $$\\textcolor{orange}{R^{T,+}_i(I, a)} = \\max \\Big(\\textcolor{orange}{R^T_i(I, a)}, 0 \\Big)$$\n        regret = {a: max(r, 0) for a, r in self.regret.items()}\n        # $$\\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')}$$\n        regret_sum = sum(regret.values())\n        # if $\\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')} \\gt 0$,\n        if regret_sum > 0:\n            # $$\\textcolor{lightgreen}{\\sigma_i^{T+1}(I)(a)} =\n            # \\frac{\\textcolor{orange}{R^{T,+}_i(I, a)}}{\\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')}}$$\n            self.strategy = {a: r / regret_sum for a, r in regret.items()}\n        # Otherwise,\n        else:\n            # $\\lvert A(I) \\rvert$\n            count = len(list(a for a in self.regret))\n            # $$\\textcolor{lightgreen}{\\sigma_i^{T+1}(I)(a)} =\n            # \\frac{1}{\\lvert A(I) \\rvert}$$\n            self.strategy = {a: 1 / count for a, r in regret.items()}\n\n    def get_average_strategy(self):\n        \"\"\"\n        ## Get average strategy\n\n        $$\\textcolor{cyan}{\\bar{\\sigma}^T_i(I)(a)} =\n         \\frac{\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}}\n         {\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)}$$\n        \"\"\"\n        # $$\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I) \\textcolor{lightgreen}{\\sigma^t(I)(a)}$$\n        cum_strategy = {a: self.cumulative_strategy.get(a, 0.) for a in self.actions()}\n        # $$\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I) =\n        # \\sum_{a \\in A(I)} \\sum_{t=1}^T\n        # \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}$$\n        strategy_sum = sum(cum_strategy.values())\n        # If $\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I) > 0$,\n        if strategy_sum > 0:\n            # $$\\textcolor{cyan}{\\bar{\\sigma}^T_i(I)(a)} =\n            #  \\frac{\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}}\n            #  {\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)}$$\n            return {a: s / strategy_sum for a, s in cum_strategy.items()}\n        # Otherwise,\n        else:\n            # $\\lvert A(I) \\rvert$\n            count = len(list(a for a in cum_strategy))\n            # $$\\textcolor{cyan}{\\bar{\\sigma}^T_i(I)(a)} =\n            # \\frac{1}{\\lvert A(I) \\rvert}$$\n            return {a: 1 / count for a, r in cum_strategy.items()}\n\n    def __repr__(self):\n        \"\"\"\n        Human readable representation\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass CFR:\n    \"\"\"\n    ## Counterfactual Regret Minimization (CFR) Algorithm\n\n    We do chance sampling (**CS**) where all the chance events (nodes) are sampled and\n    all other events (nodes) are explored.\n\n    We can ignore the term $q(z)$ since it's the same for all terminal histories\n    since we are doing chance sampling and it cancels out when calculating\n    strategy (common in numerator and denominator).\n    \"\"\"\n\n    # $\\mathcal{I}$ set of all information sets.\n    info_sets: Dict[str, InfoSet]\n\n    def __init__(self, *,\n                 create_new_history: Callable[[], History],\n                 epochs: int,\n                 n_players: int = 2):\n        \"\"\"\n        * `create_new_history` creates a new empty history\n        * `epochs` is the number of iterations to train on $T$\n        * `n_players` is the number of players\n        \"\"\"\n        self.n_players = n_players\n        self.epochs = epochs\n        self.create_new_history = create_new_history\n        # A dictionary for $\\mathcal{I}$ set of all information sets\n        self.info_sets = {}\n        # Tracker for analytics\n        self.tracker = InfoSetTracker()\n\n    def _get_info_set(self, h: History):\n        \"\"\"\n        Returns the information set $I$ of the current player for a given history $h$\n        \"\"\"\n        info_set_key = h.info_set_key()\n        if info_set_key not in self.info_sets:\n            self.info_sets[info_set_key] = h.new_info_set()\n        return self.info_sets[info_set_key]\n\n    def walk_tree(self, h: History, i: Player, pi_i: float, pi_neg_i: float) -> float:\n        \"\"\"\n        ### Walk Tree\n\n        This function walks the game tree.\n\n        * `h` is the current history $h$\n        * `i` is the player $i$ that we are computing regrets of\n        * [`pi_i`](#HistoryProbability) is\n         $\\pi^{\\sigma^t}_i(h)$\n        * [`pi_neg_i`](#HistoryProbability) is\n         $\\pi^{\\sigma^t}_{-i}(h)$\n\n        It returns the expected utility, for the history $h$\n        $$\\sum_{z \\in Z_h} \\pi^\\sigma(h, z) u_i(z)$$\n        where $Z_h$ is the set of terminal histories with prefix $h$\n\n        While walking the tee it updates the total regrets $\\textcolor{orange}{R^T_i(I, a)}$.\n        \"\"\"\n\n        # If it's a terminal history $h \\in Z$ return the terminal utility $u_i(h)$.\n        if h.is_terminal():\n            return h.terminal_utility(i)\n        # If it's a chance event $P(h) = c$ sample a and go to next step.\n        elif h.is_chance():\n            a = h.sample_chance()\n            return self.walk_tree(h + a, i, pi_i, pi_neg_i)\n\n        # Get current player's information set for $h$\n        I = self._get_info_set(h)\n        # To store $\\sum_{z \\in Z_h} \\pi^\\sigma(h, z) u_i(z)$\n        v = 0\n        # To store\n        # $$\\sum_{z \\in Z_h} \\pi^{\\sigma^t |_{I \\rightarrow a}}(h, z) u_i(z)$$\n        # for each action $a \\in A(h)$\n        va = {}\n\n        # Iterate through all actions\n        for a in I.actions():\n            # If the current player is $i$,\n            if i == h.player():\n                # \\begin{align}\n                # \\pi^{\\sigma^t}_i(h + a) &= \\pi^{\\sigma^t}_i(h) \\sigma^t_i(I)(a) \\\\\n                # \\pi^{\\sigma^t}_{-i}(h + a) &= \\pi^{\\sigma^t}_{-i}(h)\n                # \\end{align}\n                va[a] = self.walk_tree(h + a, i, pi_i * I.strategy[a], pi_neg_i)\n            # Otherwise,\n            else:\n                # \\begin{align}\n                # \\pi^{\\sigma^t}_i(h + a) &= \\pi^{\\sigma^t}_i(h)  \\\\\n                # \\pi^{\\sigma^t}_{-i}(h + a) &= \\pi^{\\sigma^t}_{-i}(h) * \\sigma^t_i(I)(a)\n                # \\end{align}\n                va[a] = self.walk_tree(h + a, i, pi_i, pi_neg_i * I.strategy[a])\n            # $$\\sum_{z \\in Z_h} \\pi^\\sigma(h, z) u_i(z) =\n            # \\sum_{a \\in A(I)} \\Bigg[ \\sigma^t_i(I)(a)\n            # \\sum_{z \\in Z_h} \\pi^{\\sigma^t |_{I \\rightarrow a}}(h, z) u_i(z)\n            # \\Bigg]$$\n            v = v + I.strategy[a] * va[a]\n\n        # If the current player is $i$,\n        # update the cumulative strategies and total regrets\n        if h.player() == i:\n            # Update cumulative strategies\n            # $$\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}\n            # = \\sum_{t=1}^T \\Big[ \\sum_{h \\in I} \\pi_i^{\\sigma^t}(h)\n            # \\textcolor{lightgreen}{\\sigma^t(I)(a)} \\Big]$$\n            for a in I.actions():\n                I.cumulative_strategy[a] = I.cumulative_strategy[a] + pi_i * I.strategy[a]\n            # \\begin{align}\n            # \\textcolor{coral}{\\tilde{r}^t_i(I, a)} &=\n            #  \\textcolor{pink}{\\tilde{v}_i(\\sigma^t |_{I \\rightarrow a}, I)} -\n            #  \\textcolor{pink}{\\tilde{v}_i(\\sigma^t, I)} \\\\\n            #  &=\n            #  \\pi^{\\sigma^t}_{-i} (h) \\Big(\n            #  \\sum_{z \\in Z_h} \\pi^{\\sigma^t |_{I \\rightarrow a}}(h, z) u_i(z) -\n            #  \\sum_{z \\in Z_h} \\pi^\\sigma(h, z) u_i(z)\n            #  \\Big) \\\\\n            # T \\textcolor{orange}{R^T_i(I, a)} &=\n            #  \\sum_{t=1}^T \\textcolor{coral}{\\tilde{r}^t_i(I, a)}\n            # \\end{align}\n            for a in I.actions():\n                I.regret[a] += pi_neg_i * (va[a] - v)\n\n            # Update the strategy $\\textcolor{lightgreen}{\\sigma^t(I)(a)}$\n            I.calculate_strategy()\n\n        # Return the expected utility for player $i$,\n        # $$\\sum_{z \\in Z_h} \\pi^\\sigma(h, z) u_i(z)$$\n        return v\n\n    def iterate(self):\n        \"\"\"\n        ### Iteratively update $\\textcolor{lightgreen}{\\sigma^t(I)(a)}$\n\n        This updates the strategies for $T$ iterations.\n        \"\"\"\n\n        # Loop for `epochs` times\n        for t in monit.iterate('Train', self.epochs):\n            # Walk tree and update regrets for each player\n            for i in range(self.n_players):\n                self.walk_tree(self.create_new_history(), cast(Player, i), 1, 1)\n\n            # Track data for analytics\n            tracker.add_global_step()\n            self.tracker(self.info_sets)\n            tracker.save()\n\n        # Print the information sets\n        logger.inspect(self.info_sets)\n\n\nclass InfoSetTracker:\n    \"\"\"\n    ### Information set tracker\n\n    This is a small helper class to track data from information sets\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Set tracking indicators\n        \"\"\"\n        tracker.set_histogram(f'strategy.*')\n        tracker.set_histogram(f'average_strategy.*')\n        tracker.set_histogram(f'regret.*')\n\n    def __call__(self, info_sets: Dict[str, InfoSet]):\n        \"\"\"\n        Track the data from all information sets\n        \"\"\"\n        for I in info_sets.values():\n            avg_strategy = I.get_average_strategy()\n            for a in I.actions():\n                tracker.add({\n                    f'strategy.{I.key}.{a}': I.strategy[a],\n                    f'average_strategy.{I.key}.{a}': avg_strategy[a],\n                    f'regret.{I.key}.{a}': I.regret[a],\n                })\n\n\nclass CFRConfigs(BaseConfigs):\n    \"\"\"\n    ### Configurable CFR module\n    \"\"\"\n    create_new_history: Callable[[], History]\n    epochs: int = 1_00_000\n    cfr: CFR = 'simple_cfr'\n\n\n@option(CFRConfigs.cfr)\ndef simple_cfr(c: CFRConfigs):\n    \"\"\"\n    Initialize **CFR** algorithm\n    \"\"\"\n    return CFR(create_new_history=c.create_new_history,\n               epochs=c.epochs)\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/cfr/__init__.py", "language": "python", "complexity": "complex", "star_count": 62130, "quality_score": 1.0, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 754, "word_count": 2977}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import List\n\nimport altair as alt\nimport numpy as np\n\nfrom labml import analytics\nfrom labml.analytics import IndicatorCollection\n\n\ndef calculate_percentages(means: List[np.ndarray], names: List[List[str]]):\n    normalized = []\n\n    for i in range(len(means)):\n        total = np.zeros_like(means[i])\n        for j, n in enumerate(names):\n            if n[-1][:-1] == names[i][-1][:-1]:\n                total += means[j]\n        normalized.append(means[i] / (total + np.finfo(float).eps))\n\n    return normalized\n\n\ndef plot_infosets(indicators: IndicatorCollection, *,\n                  is_normalize: bool = True,\n                  width: int = 600,\n                  height: int = 300):\n    data, names = analytics.indicator_data(indicators)\n    step = [d[:, 0] for d in data]\n    means = [d[:, 5] for d in data]\n\n    if is_normalize:\n        normalized = calculate_percentages(means, names)\n    else:\n        normalized = means\n\n    common = names[0][-1]\n    for i, n in enumerate(names):\n        n = n[-1]\n        if len(n) < len(common):\n            common = common[:len(n)]\n        for j in range(len(common)):\n            if common[j] != n[j]:\n                common = common[:j]\n                break\n\n    table = []\n    for i, n in enumerate(names):\n        for j, v in zip(step[i], normalized[i]):\n            table.append({\n                'series': n[-1][len(common):],\n                'step': j,\n                'value': v\n            })\n\n    table = alt.Data(values=table)\n\n    selection = alt.selection_multi(fields=['series'], bind='legend')\n\n    return alt.Chart(table).mark_line().encode(\n        alt.X('step:Q'),\n        alt.Y('value:Q'),\n        alt.Color('series:N', scale=alt.Scale(scheme='tableau20')),\n        opacity=alt.condition(selection, alt.value(1), alt.value(0.0001))\n    ).add_selection(\n        selection\n    ).properties(width=width, height=height)\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/cfr/analytics.py", "language": "python", "complexity": "simple", "star_count": 62130, "quality_score": 0.89, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 67, "word_count": 172}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n---\ntitle: CFR on Kuhn Poker\nsummary: >\n  This is an annotated implementation/tutorial of CFR on Kuhn Poker\n---\n\n# [Counterfactual Regret Minimization (CFR)](../index.html) on Kuhn Poker\n\nThis applies [Counterfactual Regret Minimization (CFR)](../index.html) to Kuhn poker.\n\n[Kuhn Poker](https://en.wikipedia.org/wiki/Kuhn_poker) is a two player 3-card betting game.\nThe players are dealt one card each out of Ace, King and Queen (no suits).\nThere are only three cards in the pack so one card is left out.\nAce beats King and Queen and King beats Queen - just like in normal ranking of cards.\n\nBoth players ante $1$ chip (blindly bet $1$ chip).\nAfter looking at the cards, the first player can either pass or bet $1$ chip.\nIf first player passes, the the player with higher card wins the pot.\nIf first player bets, the second play can bet (i.e. call) $1$ chip or pass (i.e. fold).\nIf the second player bets and the player with the higher card wins the pot.\nIf the second player passes (i.e. folds) the first player gets the pot.\nThis game is played repeatedly and a good strategy will optimize for the long term utility (or winnings).\n\nHere's some example games:\n\n* `KAp` - Player 1 gets K. Player 2 gets A. Player 1 passes. Player 2 doesn't get a betting chance and Player 2 wins the pot of $2$ chips.\n* `QKbp` - Player 1 gets Q. Player 2 gets K. Player 1 bets a chip. Player 2 passes (folds). Player 1 gets the pot of $4$ because Player 2 folded.\n* `QAbb` - Player 1 gets Q. Player 2 gets A. Player 1 bets a chip. Player 2 also bets (calls). Player 2 wins the pot of $4$.\n\nHe we extend the `InfoSet` class and `History` class defined in [`__init__.py`](../index.html)\nwith Kuhn Poker specifics.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/cfr/kuhn/experiment.ipynb)\n\"\"\"\n\nfrom typing import List, cast, Dict\n\nimport numpy as np\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.cfr import History as _History, InfoSet as _InfoSet, Action, Player, CFRConfigs\n\n# Kuhn poker actions are pass (`p`) or bet (`b`)\nACTIONS = cast(List[Action], ['p', 'b'])\n# The three cards in play are Ace, King and Queen\nCHANCES = cast(List[Action], ['A', 'K', 'Q'])\n# There are two players\nPLAYERS = cast(List[Player], [0, 1])\n\n\nclass InfoSet(_InfoSet):\n    \"\"\"\n    ## [Information set](../index.html#InfoSet)\n    \"\"\"\n\n    @staticmethod\n    def from_dict(data: Dict[str, any]) -> 'InfoSet':\n        \"\"\"Does not support save/load\"\"\"\n        pass\n\n    def actions(self) -> List[Action]:\n        \"\"\"\n        Return the list of actions. Terminal states are handled by `History` class.\n        \"\"\"\n        return ACTIONS\n\n    def __repr__(self):\n        \"\"\"\n        Human readable string representation - it gives the betting probability\n        \"\"\"\n        total = sum(self.cumulative_strategy.values())\n        total = max(total, 1e-6)\n        bet = self.cumulative_strategy[cast(Action, 'b')] / total\n        return f'{bet * 100: .1f}%'\n\n\nclass History(_History):\n    \"\"\"\n    ## [History](../index.html#History)\n\n    This defines when a game ends, calculates the utility and sample chance events (dealing cards).\n\n    The history is stored in a string:\n\n    * First two characters are the cards dealt to player 1 and player 2\n    * The third character is the action by the first player\n    * Fourth character is the action by the second player\n    \"\"\"\n\n    # History\n    history: str\n\n    def __init__(self, history: str = ''):\n        \"\"\"\n        Initialize with a given history string\n        \"\"\"\n        self.history = history\n\n    def is_terminal(self):\n        \"\"\"\n        Whether the history is terminal (game over).\n        \"\"\"\n        # Players are yet to take actions\n        if len(self.history) <= 2:\n            return False\n        # Last player to play passed (game over)\n        elif self.history[-1] == 'p':\n            return True\n        # Both players called (bet) (game over)\n        elif self.history[-2:] == 'bb':\n            return True\n        # Any other combination\n        else:\n            return False\n\n    def _terminal_utility_p1(self) -> float:\n        \"\"\"\n        Calculate the terminal utility for player $1$,  $u_1(z)$\n        \"\"\"\n        # $+1$ if Player 1 has a better card and $-1$ otherwise\n        winner = -1 + 2 * (self.history[0] < self.history[1])\n\n        # Second player passed\n        if self.history[-2:] == 'bp':\n            return 1\n        # Both players called, the player with better card wins $2$ chips\n        elif self.history[-2:] == 'bb':\n            return winner * 2\n        # First player passed, the player with better card wins $1$ chip\n        elif self.history[-1] == 'p':\n            return winner\n        # History is non-terminal\n        else:\n            raise RuntimeError()\n\n    def terminal_utility(self, i: Player) -> float:\n        \"\"\"\n        Get the terminal utility for player $i$\n        \"\"\"\n        # If $i$ is Player 1\n        if i == PLAYERS[0]:\n            return self._terminal_utility_p1()\n        # Otherwise, $u_2(z) = -u_1(z)$\n        else:\n            return -1 * self._terminal_utility_p1()\n\n    def is_chance(self) -> bool:\n        \"\"\"\n        The first two events are card dealing; i.e. chance events\n        \"\"\"\n        return len(self.history) < 2\n\n    def __add__(self, other: Action):\n        \"\"\"\n        Add an action to the history and return a new history\n        \"\"\"\n        return History(self.history + other)\n\n    def player(self) -> Player:\n        \"\"\"\n        Current player\n        \"\"\"\n        return cast(Player, len(self.history) % 2)\n\n    def sample_chance(self) -> Action:\n        \"\"\"\n        Sample a chance action\n        \"\"\"\n        while True:\n            # Randomly pick a card\n            r = np.random.randint(len(CHANCES))\n            chance = CHANCES[r]\n            # See if the card was dealt before\n            for c in self.history:\n                if c == chance:\n                    chance = None\n                    break\n\n            # Return the card if it was not dealt before\n            if chance is not None:\n                return cast(Action, chance)\n\n    def __repr__(self):\n        \"\"\"\n        Human readable representation\n        \"\"\"\n        return repr(self.history)\n\n    def info_set_key(self) -> str:\n        \"\"\"\n        Information set key for the current history.\n        This is a string of actions only visible to the current player.\n        \"\"\"\n        # Get current player\n        i = self.player()\n        # Current player sees her card and the betting actions\n        return self.history[i] + self.history[2:]\n\n    def new_info_set(self) -> InfoSet:\n        # Create a new information set object\n        return InfoSet(self.info_set_key())\n\n\ndef create_new_history():\n    \"\"\"A function to create an empty history object\"\"\"\n    return History()\n\n\nclass Configs(CFRConfigs):\n    \"\"\"\n    Configurations extends the CFR configurations class\n    \"\"\"\n    pass\n\n\n@option(Configs.create_new_history)\ndef _cnh():\n    \"\"\"\n    Set the `create_new_history` method for Kuhn Poker\n    \"\"\"\n    return create_new_history\n\n\ndef main():\n    \"\"\"\n    ### Run the experiment\n    \"\"\"\n\n    # Create an experiment, we only write tracking information to `sqlite` to speed things up.\n    # Since the algorithm iterates fast and we track data on each iteration, writing to\n    # other destinations such as Tensorboard can be relatively time consuming.\n    # SQLite is enough for our analytics.\n    experiment.create(name='kuhn_poker', writers={'sqlite'})\n    # Initialize configuration\n    conf = Configs()\n    # Load configuration\n    experiment.configs(conf)\n    # Start the experiment\n    with experiment.start():\n        # Start iterating\n        conf.cfr.iterate()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/cfr/kuhn/__init__.py", "language": "python", "complexity": "complex", "star_count": 62130, "quality_score": 0.98, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 249, "word_count": 1021}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n---\ntitle: Patches Are All You Need? (ConvMixer)\nsummary: >\n A PyTorch implementation/tutorial of the paper\n \"Patches Are All You Need?\"\n---\n\n#  Patches Are All You Need? (ConvMixer)\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[Patches Are All You Need?](https://arxiv.org/abs/2201.09792).\n\n![ConvMixer diagram from the paper](conv_mixer.png)\n\nConvMixer is Similar to [MLP-Mixer](../transformers/mlp_mixer/index.html).\nMLP-Mixer separates mixing of spatial and channel dimensions, by applying an MLP across spatial dimension\nand then an MLP across the channel dimension\n(spatial MLP replaces the [ViT](../transformers/vit/index.html) attention\nand channel MLP is the [FFN](../transformers/feed_forward.html) of ViT).\n\nConvMixer uses a $1 \\times 1$ convolution for channel mixing and a\ndepth-wise convolution for spatial mixing.\nSince it's a convolution instead of a full MLP across the space, it mixes only the nearby batches in\ncontrast to ViT or MLP-Mixer.\nAlso, the MLP-mixer uses MLPs of two layers for each mixing and ConvMixer uses a single layer for each mixing.\n\nThe paper recommends removing the residual connection across the channel mixing (point-wise convolution)\nand having only a residual connection over the spatial mixing (depth-wise convolution).\nThey also use [Batch normalization](../normalization/batch_norm/index.html) instead\nof [Layer normalization](../normalization/layer_norm/index.html).\n\nHere's [an experiment](experiment.html) that trains ConvMixer on CIFAR-10.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_nn.utils import clone_module_list\n\n\nclass ConvMixerLayer(nn.Module):\n    \"\"\"\n    <a id=\"ConvMixerLayer\"></a>\n\n    ## ConvMixer layer\n\n    This is a single ConvMixer layer. The model will have a series of these.\n    \"\"\"\n\n    def __init__(self, d_model: int, kernel_size: int):\n        \"\"\"\n        * `d_model` is the number of channels in patch embeddings, $h$\n        * `kernel_size` is the size of the kernel of spatial convolution, $k$\n        \"\"\"\n        super().__init__()\n        # Depth-wise convolution is separate convolution for each channel.\n        # We do this with a convolution layer with the number of groups equal to the number of channels.\n        # So that each channel is it's own group.\n        self.depth_wise_conv = nn.Conv2d(d_model, d_model,\n                                         kernel_size=kernel_size,\n                                         groups=d_model,\n                                         padding=(kernel_size - 1) // 2)\n        # Activation after depth-wise convolution\n        self.act1 = nn.GELU()\n        # Normalization after depth-wise convolution\n        self.norm1 = nn.BatchNorm2d(d_model)\n\n        # Point-wise convolution is a $1 \\times 1$ convolution.\n        # i.e. a linear transformation of patch embeddings\n        self.point_wise_conv = nn.Conv2d(d_model, d_model, kernel_size=1)\n        # Activation after point-wise convolution\n        self.act2 = nn.GELU()\n        # Normalization after point-wise convolution\n        self.norm2 = nn.BatchNorm2d(d_model)\n\n    def forward(self, x: torch.Tensor):\n        # For the residual connection around the depth-wise convolution\n        residual = x\n\n        # Depth-wise convolution, activation and normalization\n        x = self.depth_wise_conv(x)\n        x = self.act1(x)\n        x = self.norm1(x)\n\n        # Add residual connection\n        x += residual\n\n        # Point-wise convolution, activation and normalization\n        x = self.point_wise_conv(x)\n        x = self.act2(x)\n        x = self.norm2(x)\n\n        #\n        return x\n\n\nclass PatchEmbeddings(nn.Module):\n    \"\"\"\n    <a id=\"PatchEmbeddings\"></a>\n\n    ## Get patch embeddings\n\n    This splits the image into patches of size $p \\times p$ and gives an embedding for each patch.\n    \"\"\"\n\n    def __init__(self, d_model: int, patch_size: int, in_channels: int):\n        \"\"\"\n        * `d_model` is the number of channels in patch embeddings $h$\n        * `patch_size` is the size of the patch, $p$\n        * `in_channels` is the number of channels in the input image (3 for rgb)\n        \"\"\"\n        super().__init__()\n\n        # We create a convolution layer with a kernel size and and stride length equal to patch size.\n        # This is equivalent to splitting the image into patches and doing a linear\n        # transformation on each patch.\n        self.conv = nn.Conv2d(in_channels, d_model, kernel_size=patch_size, stride=patch_size)\n        # Activation function\n        self.act = nn.GELU()\n        # Batch normalization\n        self.norm = nn.BatchNorm2d(d_model)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the input image of shape `[batch_size, channels, height, width]`\n        \"\"\"\n        # Apply convolution layer\n        x = self.conv(x)\n        # Activation and normalization\n        x = self.act(x)\n        x = self.norm(x)\n\n        #\n        return x\n\n\nclass ClassificationHead(nn.Module):\n    \"\"\"\n    <a id=\"ClassificationHead\"></a>\n\n    ## Classification Head\n\n    They do average pooling (taking the mean of all patch embeddings) and a final linear transformation\n    to predict the log-probabilities of the image classes.\n    \"\"\"\n\n    def __init__(self, d_model: int, n_classes: int):\n        \"\"\"\n        * `d_model` is the number of channels in patch embeddings, $h$\n        * `n_classes` is the number of classes in the classification task\n        \"\"\"\n        super().__init__()\n        # Average Pool\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        # Linear layer\n        self.linear = nn.Linear(d_model, n_classes)\n\n    def forward(self, x: torch.Tensor):\n        # Average pooling\n        x = self.pool(x)\n        # Get the embedding, `x` will have shape `[batch_size, d_model, 1, 1]`\n        x = x[:, :, 0, 0]\n        # Linear layer\n        x = self.linear(x)\n\n        #\n        return x\n\n\nclass ConvMixer(nn.Module):\n    \"\"\"\n    ## ConvMixer\n\n    This combines the patch embeddings block, a number of ConvMixer layers and a classification head.\n    \"\"\"\n\n    def __init__(self, conv_mixer_layer: ConvMixerLayer, n_layers: int,\n                 patch_emb: PatchEmbeddings,\n                 classification: ClassificationHead):\n        \"\"\"\n        * `conv_mixer_layer` is a copy of a single [ConvMixer layer](#ConvMixerLayer).\n         We make copies of it to make ConvMixer with `n_layers`.\n        * `n_layers` is the number of ConvMixer layers (or depth), $d$.\n        * `patch_emb` is the [patch embeddings layer](#PatchEmbeddings).\n        * `classification` is the [classification head](#ClassificationHead).\n        \"\"\"\n        super().__init__()\n        # Patch embeddings\n        self.patch_emb = patch_emb\n        # Classification head\n        self.classification = classification\n        # Make copies of the [ConvMixer layer](#ConvMixerLayer)\n        self.conv_mixer_layers = clone_module_list(conv_mixer_layer, n_layers)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the input image of shape `[batch_size, channels, height, width]`\n        \"\"\"\n        # Get patch embeddings. This gives a tensor of shape `[batch_size, d_model, height / patch_size, width / patch_size]`.\n        x = self.patch_emb(x)\n\n        # Pass through [ConvMixer layers](#ConvMixerLayer)\n        for layer in self.conv_mixer_layers:\n            x = layer(x)\n\n        # Classification head, to get logits\n        x = self.classification(x)\n\n        #\n        return x\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/conv_mixer/__init__.py", "language": "python", "complexity": "complex", "star_count": 62130, "quality_score": 0.9500000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 212, "word_count": 862}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n---\ntitle: Diffusion models\nsummary: >\n A set of PyTorch implementations/tutorials of diffusion models.\n---\n\n# Diffusion models\n\n* [Denoising Diffusion Probabilistic Models (DDPM)](ddpm/index.html)\n* [Stable Diffusion](stable_diffusion/index.html)\n* [Latent Diffusion Model](stable_diffusion/latent_diffusion.html)\n* [Denoising Diffusion Implicit Models (DDIM) Sampling](stable_diffusion/sampler/ddim.html)\n\"\"\"\n", "metadata": {"source": "github.com/labmlai/annotated_deep_learning_paper_implementations", "file_path": "labml_nn/diffusion/__init__.py", "language": "python", "complexity": "simple", "star_count": 62130, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 15, "word_count": 40}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<p align=\"center\"><img width=\"40%\" src=\"logo/pytorch_logo_2018.svg\" /></p>\n\n--------------------------------------------------------------------------------\n\nThis repository provides tutorial code for deep learning researchers to learn [PyTorch](https://github.com/pytorch/pytorch). In the tutorial, most of the models were implemented with less than 30 lines of code. Before starting this tutorial, it is recommended to finish [Official Pytorch Tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n\n\n<br/>\n\n## Table of Contents\n\n#### 1. Basics\n* [PyTorch Basics](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/pytorch_basics/main.py)\n* [Linear Regression](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/linear_regression/main.py#L22-L23)\n* [Logistic Regression](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/logistic_regression/main.py#L33-L34)\n* [Feedforward Neural Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49)\n\n#### 2. Intermediate\n* [Convolutional Neural Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/convolutional_neural_network/main.py#L35-L56)\n* [Deep Residual Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/deep_residual_network/main.py#L76-L113)\n* [Recurrent Neural Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/recurrent_neural_network/main.py#L39-L58)\n* [Bidirectional Recurrent Neural Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py#L39-L58)\n* [Language Model (RNN-LM)](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model/main.py#L30-L50)\n\n#### 3. Advanced\n* [Generative Adversarial Networks](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/generative_adversarial_network/main.py#L41-L57)\n* [Variational Auto-Encoder](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py#L38-L65)\n* [Neural Style Transfer](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/neural_style_transfer)\n* [Image Captioning (CNN-RNN)](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning)\n\n#### 4. Utilities\n* [TensorBoard in PyTorch](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard)\n\n\n<br/>\n\n## Getting Started\n```bash\n$ git clone https://github.com/yunjey/pytorch-tutorial.git\n$ cd pytorch-tutorial/tutorials/PATH_TO_PROJECT\n$ python main.py\n```\n\n<br/>\n\n## Dependencies\n* [Python 2.7 or 3.5+](https://www.continuum.io/downloads)\n* [PyTorch 0.4.0+](http://pytorch.org/)\n\n\n\n\n", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 31552, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 53, "word_count": 143}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters \ninput_size = 784\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.001\n\n# MNIST dataset \ntrain_dataset = torchvision.datasets.MNIST(root='../../data', \n                                           train=True, \n                                           transform=transforms.ToTensor(),  \n                                           download=True)\n\ntest_dataset = torchvision.datasets.MNIST(root='../../data', \n                                          train=False, \n                                          transform=transforms.ToTensor())\n\n# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Fully connected neural network with one hidden layer\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size) \n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\nmodel = NeuralNet(input_size, hidden_size, num_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n\n# Train the model\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Move tensors to the configured device\n        images = images.reshape(-1, 28*28).to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\n# Test the model\n# In test phase, we don't need to compute gradients (for memory efficiency)\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.reshape(-1, 28*28).to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n\n# Save the model checkpoint\ntorch.save(model.state_dict(), 'model.ckpt')", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/01-basics/feedforward_neural_network/main.py", "language": "python", "complexity": "simple", "star_count": 31552, "quality_score": 0.9500000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 94, "word_count": 269}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Hyper-parameters\ninput_size = 1\noutput_size = 1\nnum_epochs = 60\nlearning_rate = 0.001\n\n# Toy dataset\nx_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n                    [9.779], [6.182], [7.59], [2.167], [7.042], \n                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n\ny_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n                    [3.366], [2.596], [2.53], [1.221], [2.827], \n                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n\n# Linear regression model\nmodel = nn.Linear(input_size, output_size)\n\n# Loss and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n# Train the model\nfor epoch in range(num_epochs):\n    # Convert numpy arrays to torch tensors\n    inputs = torch.from_numpy(x_train)\n    targets = torch.from_numpy(y_train)\n\n    # Forward pass\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch+1) % 5 == 0:\n        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n\n# Plot the graph\npredicted = model(torch.from_numpy(x_train)).detach().numpy()\nplt.plot(x_train, y_train, 'ro', label='Original data')\nplt.plot(x_train, predicted, label='Fitted line')\nplt.legend()\nplt.show()\n\n# Save the model checkpoint\ntorch.save(model.state_dict(), 'model.ckpt')", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/01-basics/linear_regression/main.py", "language": "python", "complexity": "simple", "star_count": 31552, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 55, "word_count": 162}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n# Hyper-parameters \ninput_size = 28 * 28    # 784\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.001\n\n# MNIST dataset (images and labels)\ntrain_dataset = torchvision.datasets.MNIST(root='../../data', \n                                           train=True, \n                                           transform=transforms.ToTensor(),\n                                           download=True)\n\ntest_dataset = torchvision.datasets.MNIST(root='../../data', \n                                          train=False, \n                                          transform=transforms.ToTensor())\n\n# Data loader (input pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Logistic regression model\nmodel = nn.Linear(input_size, num_classes)\n\n# Loss and optimizer\n# nn.CrossEntropyLoss() computes softmax internally\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n# Train the model\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Reshape images to (batch_size, input_size)\n        images = images.reshape(-1, input_size)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\n# Test the model\n# In test phase, we don't need to compute gradients (for memory efficiency)\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.reshape(-1, input_size)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum()\n\n    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n\n# Save the model checkpoint\ntorch.save(model.state_dict(), 'model.ckpt')\n", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/01-basics/logistic_regression/main.py", "language": "python", "complexity": "simple", "star_count": 31552, "quality_score": 0.8600000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 77, "word_count": 223}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import torch \nimport torchvision\nimport torch.nn as nn\nimport numpy as np\nimport torchvision.transforms as transforms\n\n\n# ================================================================== #\n#                         Table of Contents                          #\n# ================================================================== #\n\n# 1. Basic autograd example 1               (Line 25 to 39)\n# 2. Basic autograd example 2               (Line 46 to 83)\n# 3. Loading data from numpy                (Line 90 to 97)\n# 4. Input pipline                          (Line 104 to 129)\n# 5. Input pipline for custom dataset       (Line 136 to 156)\n# 6. Pretrained model                       (Line 163 to 176)\n# 7. Save and load model                    (Line 183 to 189) \n\n\n# ================================================================== #\n#                     1. Basic autograd example 1                    #\n# ================================================================== #\n\n# Create tensors.\nx = torch.tensor(1., requires_grad=True)\nw = torch.tensor(2., requires_grad=True)\nb = torch.tensor(3., requires_grad=True)\n\n# Build a computational graph.\ny = w * x + b    # y = 2 * x + 3\n\n# Compute gradients.\ny.backward()\n\n# Print out the gradients.\nprint(x.grad)    # x.grad = 2 \nprint(w.grad)    # w.grad = 1 \nprint(b.grad)    # b.grad = 1 \n\n\n# ================================================================== #\n#                    2. Basic autograd example 2                     #\n# ================================================================== #\n\n# Create tensors of shape (10, 3) and (10, 2).\nx = torch.randn(10, 3)\ny = torch.randn(10, 2)\n\n# Build a fully connected layer.\nlinear = nn.Linear(3, 2)\nprint ('w: ', linear.weight)\nprint ('b: ', linear.bias)\n\n# Build loss function and optimizer.\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n\n# Forward pass.\npred = linear(x)\n\n# Compute loss.\nloss = criterion(pred, y)\nprint('loss: ', loss.item())\n\n# Backward pass.\nloss.backward()\n\n# Print out the gradients.\nprint ('dL/dw: ', linear.weight.grad) \nprint ('dL/db: ', linear.bias.grad)\n\n# 1-step gradient descent.\noptimizer.step()\n\n# You can also perform gradient descent at the low level.\n# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n\n# Print out the loss after 1-step gradient descent.\npred = linear(x)\nloss = criterion(pred, y)\nprint('loss after 1 step optimization: ', loss.item())\n\n\n# ================================================================== #\n#                     3. Loading data from numpy                     #\n# ================================================================== #\n\n# Create a numpy array.\nx = np.array([[1, 2], [3, 4]])\n\n# Convert the numpy array to a torch tensor.\ny = torch.from_numpy(x)\n\n# Convert the torch tensor to a numpy array.\nz = y.numpy()\n\n\n# ================================================================== #\n#                         4. Input pipeline                           #\n# ================================================================== #\n\n# Download and construct CIFAR-10 dataset.\ntrain_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n                                             train=True, \n                                             transform=transforms.ToTensor(),\n                                             download=True)\n\n# Fetch one data pair (read data from disk).\nimage, label = train_dataset[0]\nprint (image.size())\nprint (label)\n\n# Data loader (this provides queues and threads in a very simple way).\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=64, \n                                           shuffle=True)\n\n# When iteration starts, queue and thread start to load data from files.\ndata_iter = iter(train_loader)\n\n# Mini-batch images and labels.\nimages, labels = data_iter.next()\n\n# Actual usage of the data loader is as below.\nfor images, labels in train_loader:\n    # Training code should be written here.\n    pass\n\n\n# ================================================================== #\n#                5. Input pipeline for custom dataset                 #\n# ================================================================== #\n\n# You should build your custom dataset as below.\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self):\n        # TODO\n        # 1. Initialize file paths or a list of file names. \n        pass\n    def __getitem__(self, index):\n        # TODO\n        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n        # 2. Preprocess the data (e.g. torchvision.Transform).\n        # 3. Return a data pair (e.g. image and label).\n        pass\n    def __len__(self):\n        # You should change 0 to the total size of your dataset.\n        return 0 \n\n# You can then use the prebuilt data loader. \ncustom_dataset = CustomDataset()\ntrain_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n                                           batch_size=64, \n                                           shuffle=True)\n\n\n# ================================================================== #\n#                        6. Pretrained model                         #\n# ================================================================== #\n\n# Download and load the pretrained ResNet-18.\nresnet = torchvision.models.resnet18(pretrained=True)\n\n# If you want to finetune only the top layer of the model, set as below.\nfor param in resnet.parameters():\n    param.requires_grad = False\n\n# Replace the top layer for finetuning.\nresnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n\n# Forward pass.\nimages = torch.randn(64, 3, 224, 224)\noutputs = resnet(images)\nprint (outputs.size())     # (64, 100)\n\n\n# ================================================================== #\n#                      7. Save and load the model                    #\n# ================================================================== #\n\n# Save and load the entire model.\ntorch.save(resnet, 'model.ckpt')\nmodel = torch.load('model.ckpt')\n\n# Save and load only the model parameters (recommended).\ntorch.save(resnet.state_dict(), 'params.ckpt')\nresnet.load_state_dict(torch.load('params.ckpt'))\n", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/01-basics/pytorch_basics/main.py", "language": "python", "complexity": "medium", "star_count": 31552, "quality_score": 0.92, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 190, "word_count": 685}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import torch \nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nsequence_length = 28\ninput_size = 28\nhidden_size = 128\nnum_layers = 2\nnum_classes = 10\nbatch_size = 100\nnum_epochs = 2\nlearning_rate = 0.003\n\n# MNIST dataset\ntrain_dataset = torchvision.datasets.MNIST(root='../../data/',\n                                           train=True, \n                                           transform=transforms.ToTensor(),\n                                           download=True)\n\ntest_dataset = torchvision.datasets.MNIST(root='../../data/',\n                                          train=False, \n                                          transform=transforms.ToTensor())\n\n# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Bidirectional recurrent neural network (many-to-one)\nclass BiRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(BiRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n    \n    def forward(self, x):\n        # Set initial states\n        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n        \n        # Forward propagate LSTM\n        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])\n        return out\n\nmodel = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    \n# Train the model\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.reshape(-1, sequence_length, input_size).to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\n# Test the model\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.reshape(-1, sequence_length, input_size).to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n\n# Save the model checkpoint\ntorch.save(model.state_dict(), 'model.ckpt')", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py", "language": "python", "complexity": "medium", "star_count": 31552, "quality_score": 0.9500000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 102, "word_count": 309}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import torch \nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n# Device configuration\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# Hyper parameters\nnum_epochs = 5\nnum_classes = 10\nbatch_size = 100\nlearning_rate = 0.001\n\n# MNIST dataset\ntrain_dataset = torchvision.datasets.MNIST(root='../../data/',\n                                           train=True, \n                                           transform=transforms.ToTensor(),\n                                           download=True)\n\ntest_dataset = torchvision.datasets.MNIST(root='../../data/',\n                                          train=False, \n                                          transform=transforms.ToTensor())\n\n# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Convolutional neural network (two convolutional layers)\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc = nn.Linear(7*7*32, num_classes)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        return out\n\nmodel = ConvNet(num_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train the model\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\n# Test the model\nmodel.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n\n# Save the model checkpoint\ntorch.save(model.state_dict(), 'model.ckpt')", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/02-intermediate/convolutional_neural_network/main.py", "language": "python", "complexity": "simple", "star_count": 31552, "quality_score": 0.9500000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 100, "word_count": 270}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# ---------------------------------------------------------------------------- #\n# An implementation of https://arxiv.org/pdf/1512.03385.pdf                    #\n# See section 4.2 for the model architecture on CIFAR-10                       #\n# Some part of the code was referenced from below                              #\n# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py   #\n# ---------------------------------------------------------------------------- #\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nnum_epochs = 80\nbatch_size = 100\nlearning_rate = 0.001\n\n# Image preprocessing modules\ntransform = transforms.Compose([\n    transforms.Pad(4),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32),\n    transforms.ToTensor()])\n\n# CIFAR-10 dataset\ntrain_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n                                             train=True, \n                                             transform=transform,\n                                             download=True)\n\ntest_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n                                            train=False, \n                                            transform=transforms.ToTensor())\n\n# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size,\n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=False)\n\n# 3x3 convolution\ndef conv3x3(in_channels, out_channels, stride=1):\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n                     stride=stride, padding=1, bias=False)\n\n# Residual block\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = conv3x3(in_channels, out_channels, stride)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(out_channels, out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n        \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\n# ResNet\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_channels = 16\n        self.conv = conv3x3(3, 16)\n        self.bn = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self.make_layer(block, 16, layers[0])\n        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n        self.avg_pool = nn.AvgPool2d(8)\n        self.fc = nn.Linear(64, num_classes)\n        \n    def make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if (stride != 1) or (self.in_channels != out_channels):\n            downsample = nn.Sequential(\n                conv3x3(self.in_channels, out_channels, stride=stride),\n                nn.BatchNorm2d(out_channels))\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for i in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        out = self.conv(x)\n        out = self.bn(out)\n        out = self.relu(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avg_pool(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n    \nmodel = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# For updating learning rate\ndef update_lr(optimizer, lr):    \n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n# Train the model\ntotal_step = len(train_loader)\ncurr_lr = learning_rate\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\n    # Decay learning rate\n    if (epoch+1) % 20 == 0:\n        curr_lr /= 3\n        update_lr(optimizer, curr_lr)\n\n# Test the model\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n\n# Save the model checkpoint\ntorch.save(model.state_dict(), 'resnet.ckpt')\n", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/02-intermediate/deep_residual_network/main.py", "language": "python", "complexity": "medium", "star_count": 31552, "quality_score": 0.98, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 171, "word_count": 483}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Some part of the code was referenced from below.\n# https://github.com/pytorch/examples/tree/master/word_language_model \nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.nn.utils import clip_grad_norm_\nfrom data_utils import Dictionary, Corpus\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nembed_size = 128\nhidden_size = 1024\nnum_layers = 1\nnum_epochs = 5\nnum_samples = 1000     # number of words to be sampled\nbatch_size = 20\nseq_length = 30\nlearning_rate = 0.002\n\n# Load \"Penn Treebank\" dataset\ncorpus = Corpus()\nids = corpus.get_data('data/train.txt', batch_size)\nvocab_size = len(corpus.dictionary)\nnum_batches = ids.size(1) // seq_length\n\n\n# RNN based language model\nclass RNNLM(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n        super(RNNLM, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, x, h):\n        # Embed word ids to vectors\n        x = self.embed(x)\n        \n        # Forward propagate LSTM\n        out, (h, c) = self.lstm(x, h)\n        \n        # Reshape output to (batch_size*sequence_length, hidden_size)\n        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n        \n        # Decode hidden states of all time steps\n        out = self.linear(out)\n        return out, (h, c)\n\nmodel = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Truncated backpropagation\ndef detach(states):\n    return [state.detach() for state in states] \n\n# Train the model\nfor epoch in range(num_epochs):\n    # Set initial hidden and cell states\n    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n    \n    for i in range(0, ids.size(1) - seq_length, seq_length):\n        # Get mini-batch inputs and targets\n        inputs = ids[:, i:i+seq_length].to(device)\n        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n        \n        # Forward pass\n        states = detach(states)\n        outputs, states = model(inputs, states)\n        loss = criterion(outputs, targets.reshape(-1))\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n\n        step = (i+1) // seq_length\n        if step % 100 == 0:\n            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))\n\n# Test the model\nwith torch.no_grad():\n    with open('sample.txt', 'w') as f:\n        # Set intial hidden ane cell states\n        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n                 torch.zeros(num_layers, 1, hidden_size).to(device))\n\n        # Select one word id randomly\n        prob = torch.ones(vocab_size)\n        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n\n        for i in range(num_samples):\n            # Forward propagate RNN \n            output, state = model(input, state)\n\n            # Sample a word id\n            prob = output.exp()\n            word_id = torch.multinomial(prob, num_samples=1).item()\n\n            # Fill input with sampled word id for the next time step\n            input.fill_(word_id)\n\n            # File write\n            word = corpus.dictionary.idx2word[word_id]\n            word = '\\n' if word == '<eos>' else word + ' '\n            f.write(word)\n\n            if (i+1) % 100 == 0:\n                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))\n\n# Save the model checkpoints\ntorch.save(model.state_dict(), 'model.ckpt')", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/02-intermediate/language_model/main.py", "language": "python", "complexity": "medium", "star_count": 31552, "quality_score": 0.9500000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 120, "word_count": 411}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import torch \nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nsequence_length = 28\ninput_size = 28\nhidden_size = 128\nnum_layers = 2\nnum_classes = 10\nbatch_size = 100\nnum_epochs = 2\nlearning_rate = 0.01\n\n# MNIST dataset\ntrain_dataset = torchvision.datasets.MNIST(root='../../data/',\n                                           train=True, \n                                           transform=transforms.ToTensor(),\n                                           download=True)\n\ntest_dataset = torchvision.datasets.MNIST(root='../../data/',\n                                          train=False, \n                                          transform=transforms.ToTensor())\n\n# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Recurrent neural network (many-to-one)\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, x):\n        # Set initial hidden and cell states \n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n        \n        # Forward propagate LSTM\n        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])\n        return out\n\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train the model\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.reshape(-1, sequence_length, input_size).to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\n# Test the model\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.reshape(-1, sequence_length, input_size).to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n\n# Save the model checkpoint\ntorch.save(model.state_dict(), 'model.ckpt')", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/02-intermediate/recurrent_neural_network/main.py", "language": "python", "complexity": "medium", "star_count": 31552, "quality_score": 0.9500000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 103, "word_count": 303}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nlatent_size = 64\nhidden_size = 256\nimage_size = 784\nnum_epochs = 200\nbatch_size = 100\nsample_dir = 'samples'\n\n# Create a directory if not exists\nif not os.path.exists(sample_dir):\n    os.makedirs(sample_dir)\n\n# Image processing\n# transform = transforms.Compose([\n#                 transforms.ToTensor(),\n#                 transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels\n#                                      std=(0.5, 0.5, 0.5))])\ntransform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.5],   # 1 for greyscale channels\n                                     std=[0.5])])\n\n# MNIST dataset\nmnist = torchvision.datasets.MNIST(root='../../data/',\n                                   train=True,\n                                   transform=transform,\n                                   download=True)\n\n# Data loader\ndata_loader = torch.utils.data.DataLoader(dataset=mnist,\n                                          batch_size=batch_size, \n                                          shuffle=True)\n\n# Discriminator\nD = nn.Sequential(\n    nn.Linear(image_size, hidden_size),\n    nn.LeakyReLU(0.2),\n    nn.Linear(hidden_size, hidden_size),\n    nn.LeakyReLU(0.2),\n    nn.Linear(hidden_size, 1),\n    nn.Sigmoid())\n\n# Generator \nG = nn.Sequential(\n    nn.Linear(latent_size, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, image_size),\n    nn.Tanh())\n\n# Device setting\nD = D.to(device)\nG = G.to(device)\n\n# Binary cross entropy loss and optimizer\ncriterion = nn.BCELoss()\nd_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\ng_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)\n\ndef denorm(x):\n    out = (x + 1) / 2\n    return out.clamp(0, 1)\n\ndef reset_grad():\n    d_optimizer.zero_grad()\n    g_optimizer.zero_grad()\n\n# Start training\ntotal_step = len(data_loader)\nfor epoch in range(num_epochs):\n    for i, (images, _) in enumerate(data_loader):\n        images = images.reshape(batch_size, -1).to(device)\n        \n        # Create the labels which are later used as input for the BCE loss\n        real_labels = torch.ones(batch_size, 1).to(device)\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n\n        # ================================================================== #\n        #                      Train the discriminator                       #\n        # ================================================================== #\n\n        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n        # Second term of the loss is always zero since real_labels == 1\n        outputs = D(images)\n        d_loss_real = criterion(outputs, real_labels)\n        real_score = outputs\n        \n        # Compute BCELoss using fake images\n        # First term of the loss is always zero since fake_labels == 0\n        z = torch.randn(batch_size, latent_size).to(device)\n        fake_images = G(z)\n        outputs = D(fake_images)\n        d_loss_fake = criterion(outputs, fake_labels)\n        fake_score = outputs\n        \n        # Backprop and optimize\n        d_loss = d_loss_real + d_loss_fake\n        reset_grad()\n        d_loss.backward()\n        d_optimizer.step()\n        \n        # ================================================================== #\n        #                        Train the generator                         #\n        # ================================================================== #\n\n        # Compute loss with fake images\n        z = torch.randn(batch_size, latent_size).to(device)\n        fake_images = G(z)\n        outputs = D(fake_images)\n        \n        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n        g_loss = criterion(outputs, real_labels)\n        \n        # Backprop and optimize\n        reset_grad()\n        g_loss.backward()\n        g_optimizer.step()\n        \n        if (i+1) % 200 == 0:\n            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n                          real_score.mean().item(), fake_score.mean().item()))\n    \n    # Save real images\n    if (epoch+1) == 1:\n        images = images.reshape(images.size(0), 1, 28, 28)\n        save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))\n    \n    # Save sampled images\n    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n    save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n\n# Save the model checkpoints \ntorch.save(G.state_dict(), 'G.ckpt')\ntorch.save(D.state_dict(), 'D.ckpt')", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/03-advanced/generative_adversarial_network/main.py", "language": "python", "complexity": "medium", "star_count": 31552, "quality_score": 0.92, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 148, "word_count": 449}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Image Captioning\nThe goal of image captioning is to convert a given input image into a natural language description. The encoder-decoder framework is widely used for this task. The image encoder is a convolutional neural network (CNN). In this tutorial, we used [resnet-152](https://arxiv.org/abs/1512.03385) model pretrained on the [ILSVRC-2012-CLS](http://www.image-net.org/challenges/LSVRC/2012/) image classification dataset. The decoder is a long short-term memory (LSTM) network. \n\n![alt text](png/model.png)\n\n#### Training phase\nFor the encoder part, the pretrained CNN extracts the feature vector from a given input image. The feature vector is linearly transformed to have the same dimension as the input dimension of the LSTM network. For the decoder part, source and target texts are predefined. For example, if the image description is **\"Giraffes standing next to each other\"**, the source sequence is a list containing **['\\<start\\>', 'Giraffes', 'standing', 'next', 'to', 'each', 'other']** and the target sequence is a list containing **['Giraffes', 'standing', 'next', 'to', 'each', 'other', '\\<end\\>']**. Using these source and target sequences and the feature vector, the LSTM decoder is trained as a language model conditioned on the feature vector.\n\n#### Test phase\nIn the test phase, the encoder part is almost same as the training phase. The only difference is that batchnorm layer uses moving average and variance instead of mini-batch statistics. This can be easily implemented using [encoder.eval()](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/image_captioning/sample.py#L37). For the decoder part, there is a significant difference between the training phase and the test phase. In the test phase, the LSTM decoder can't see the image description. To deal with this problem, the LSTM decoder feeds back the previosly generated word to the next input. This can be implemented using a [for-loop](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/image_captioning/model.py#L48).\n\n\n\n## Usage \n\n\n#### 1. Clone the repositories\n```bash\ngit clone https://github.com/pdollar/coco.git\ncd coco/PythonAPI/\nmake\npython setup.py build\npython setup.py install\ncd ../../\ngit clone https://github.com/yunjey/pytorch-tutorial.git\ncd pytorch-tutorial/tutorials/03-advanced/image_captioning/\n```\n\n#### 2. Download the dataset\n\n```bash\npip install -r requirements.txt\nchmod +x download.sh\n./download.sh\n```\n\n#### 3. Preprocessing\n\n```bash\npython build_vocab.py   \npython resize.py\n```\n\n#### 4. Train the model\n\n```bash\npython train.py    \n```\n\n#### 5. Test the model \n\n```bash\npython sample.py --image='png/example.png'\n```\n\n<br>\n\n## Pretrained model\nIf you do not want to train the model from scratch, you can use a pretrained model. You can download the pretrained model [here](https://www.dropbox.com/s/ne0ixz5d58ccbbz/pretrained_model.zip?dl=0) and the vocabulary file [here](https://www.dropbox.com/s/26adb7y9m98uisa/vocap.zip?dl=0). You should extract pretrained_model.zip to `./models/` and vocab.pkl to `./data/` using `unzip` command.\n", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/03-advanced/image_captioning/README.md", "language": "markdown", "complexity": "medium", "star_count": 31552, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 60, "word_count": 388}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport os\nimport pickle\nfrom data_loader import get_loader \nfrom build_vocab import Vocabulary\nfrom model import EncoderCNN, DecoderRNN\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torchvision import transforms\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef main(args):\n    # Create model directory\n    if not os.path.exists(args.model_path):\n        os.makedirs(args.model_path)\n    \n    # Image preprocessing, normalization for the pretrained resnet\n    transform = transforms.Compose([ \n        transforms.RandomCrop(args.crop_size),\n        transforms.RandomHorizontalFlip(), \n        transforms.ToTensor(), \n        transforms.Normalize((0.485, 0.456, 0.406), \n                             (0.229, 0.224, 0.225))])\n    \n    # Load vocabulary wrapper\n    with open(args.vocab_path, 'rb') as f:\n        vocab = pickle.load(f)\n    \n    # Build data loader\n    data_loader = get_loader(args.image_dir, args.caption_path, vocab, \n                             transform, args.batch_size,\n                             shuffle=True, num_workers=args.num_workers) \n\n    # Build the models\n    encoder = EncoderCNN(args.embed_size).to(device)\n    decoder = DecoderRNN(args.embed_size, args.hidden_size, len(vocab), args.num_layers).to(device)\n    \n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n    optimizer = torch.optim.Adam(params, lr=args.learning_rate)\n    \n    # Train the models\n    total_step = len(data_loader)\n    for epoch in range(args.num_epochs):\n        for i, (images, captions, lengths) in enumerate(data_loader):\n            \n            # Set mini-batch dataset\n            images = images.to(device)\n            captions = captions.to(device)\n            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n            \n            # Forward, backward and optimize\n            features = encoder(images)\n            outputs = decoder(features, captions, lengths)\n            loss = criterion(outputs, targets)\n            decoder.zero_grad()\n            encoder.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Print log info\n            if i % args.log_step == 0:\n                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n                      .format(epoch, args.num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n                \n            # Save the model checkpoints\n            if (i+1) % args.save_step == 0:\n                torch.save(decoder.state_dict(), os.path.join(\n                    args.model_path, 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n                torch.save(encoder.state_dict(), os.path.join(\n                    args.model_path, 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_path', type=str, default='models/' , help='path for saving trained models')\n    parser.add_argument('--crop_size', type=int, default=224 , help='size for randomly cropping images')\n    parser.add_argument('--vocab_path', type=str, default='data/vocab.pkl', help='path for vocabulary wrapper')\n    parser.add_argument('--image_dir', type=str, default='data/resized2014', help='directory for resized images')\n    parser.add_argument('--caption_path', type=str, default='data/annotations/captions_train2014.json', help='path for train annotation json file')\n    parser.add_argument('--log_step', type=int , default=10, help='step size for prining log info')\n    parser.add_argument('--save_step', type=int , default=1000, help='step size for saving trained models')\n    \n    # Model parameters\n    parser.add_argument('--embed_size', type=int , default=256, help='dimension of word embedding vectors')\n    parser.add_argument('--hidden_size', type=int , default=512, help='dimension of lstm hidden states')\n    parser.add_argument('--num_layers', type=int , default=1, help='number of layers in lstm')\n    \n    parser.add_argument('--num_epochs', type=int, default=5)\n    parser.add_argument('--batch_size', type=int, default=128)\n    parser.add_argument('--num_workers', type=int, default=2)\n    parser.add_argument('--learning_rate', type=float, default=0.001)\n    args = parser.parse_args()\n    print(args)\n    main(args)", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/03-advanced/image_captioning/train.py", "language": "python", "complexity": "medium", "star_count": 31552, "quality_score": 0.92, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 101, "word_count": 347}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Neural Style Transfer\n\n[Neural style transfer](https://arxiv.org/abs/1508.06576) is an algorithm that combines the content of one image with the style of another image using CNN. Given a content image and a style image, the goal is to generate a target image that minimizes the content difference with the content image and the style difference with the style image. \n\n<p align=\"center\"><img width=\"100%\" src=\"png/neural_style2.png\" /></p>\n\n\n#### Content loss\n\nTo minimize the content difference, we forward propagate the content image and the target image to pretrained [VGGNet](https://arxiv.org/abs/1409.1556) respectively, and extract feature maps from multiple convolutional layers. Then, the target image is updated to minimize the [mean-squared error](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/neural_style_transfer/main.py#L81-L82) between the feature maps of the content image and its feature maps. \n\n#### Style loss\n\nAs in computing the content loss, we forward propagate the style image and the target image to the VGGNet and extract convolutional feature maps. To generate a texture that matches the style of the style image, we update the target image by minimizing the mean-squared error between the Gram matrix of the style image and the Gram matrix of the target image (feature correlation minimization). See [here](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/neural_style_transfer/main.py#L84-L94) for how to compute the style loss.\n\n\n\n\n<br>\n\n## Usage \n\n```bash\n$ pip install -r requirements.txt\n$ python main.py --content='png/content.png' --style='png/style.png'\n```\n\n<br>\n\n## Results\nThe following is the result of applying variaous styles of artwork to Anne Hathaway's photograph.\n\n![alt text](png/neural_style.png)\n", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/03-advanced/neural_style_transfer/README.md", "language": "markdown", "complexity": "simple", "star_count": 31552, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 34, "word_count": 228}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import division\nfrom torchvision import models\nfrom torchvision import transforms\nfrom PIL import Image\nimport argparse\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport numpy as np\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef load_image(image_path, transform=None, max_size=None, shape=None):\n    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n    image = Image.open(image_path)\n    \n    if max_size:\n        scale = max_size / max(image.size)\n        size = np.array(image.size) * scale\n        image = image.resize(size.astype(int), Image.ANTIALIAS)\n    \n    if shape:\n        image = image.resize(shape, Image.LANCZOS)\n    \n    if transform:\n        image = transform(image).unsqueeze(0)\n    \n    return image.to(device)\n\n\nclass VGGNet(nn.Module):\n    def __init__(self):\n        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n        super(VGGNet, self).__init__()\n        self.select = ['0', '5', '10', '19', '28'] \n        self.vgg = models.vgg19(pretrained=True).features\n        \n    def forward(self, x):\n        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n        features = []\n        for name, layer in self.vgg._modules.items():\n            x = layer(x)\n            if name in self.select:\n                features.append(x)\n        return features\n\n\ndef main(config):\n    \n    # Image preprocessing\n    # VGGNet was trained on ImageNet where images are normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n    # We use the same normalization statistics here.\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.485, 0.456, 0.406), \n                             std=(0.229, 0.224, 0.225))])\n    \n    # Load content and style images\n    # Make the style image same size as the content image\n    content = load_image(config.content, transform, max_size=config.max_size)\n    style = load_image(config.style, transform, shape=[content.size(2), content.size(3)])\n    \n    # Initialize a target image with the content image\n    target = content.clone().requires_grad_(True)\n    \n    optimizer = torch.optim.Adam([target], lr=config.lr, betas=[0.5, 0.999])\n    vgg = VGGNet().to(device).eval()\n    \n    for step in range(config.total_step):\n        \n        # Extract multiple(5) conv feature vectors\n        target_features = vgg(target)\n        content_features = vgg(content)\n        style_features = vgg(style)\n\n        style_loss = 0\n        content_loss = 0\n        for f1, f2, f3 in zip(target_features, content_features, style_features):\n            # Compute content loss with target and content images\n            content_loss += torch.mean((f1 - f2)**2)\n\n            # Reshape convolutional feature maps\n            _, c, h, w = f1.size()\n            f1 = f1.view(c, h * w)\n            f3 = f3.view(c, h * w)\n\n            # Compute gram matrix\n            f1 = torch.mm(f1, f1.t())\n            f3 = torch.mm(f3, f3.t())\n\n            # Compute style loss with target and style images\n            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n        \n        # Compute total loss, backprop and optimize\n        loss = content_loss + config.style_weight * style_loss \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (step+1) % config.log_step == 0:\n            print ('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}' \n                   .format(step+1, config.total_step, content_loss.item(), style_loss.item()))\n\n        if (step+1) % config.sample_step == 0:\n            # Save the generated image\n            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n            img = target.clone().squeeze()\n            img = denorm(img).clamp_(0, 1)\n            torchvision.utils.save_image(img, 'output-{}.png'.format(step+1))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--content', type=str, default='png/content.png')\n    parser.add_argument('--style', type=str, default='png/style.png')\n    parser.add_argument('--max_size', type=int, default=400)\n    parser.add_argument('--total_step', type=int, default=2000)\n    parser.add_argument('--log_step', type=int, default=10)\n    parser.add_argument('--sample_step', type=int, default=500)\n    parser.add_argument('--style_weight', type=float, default=100)\n    parser.add_argument('--lr', type=float, default=0.003)\n    config = parser.parse_args()\n    print(config)\n    main(config)", "metadata": {"source": "github.com/yunjey/pytorch-tutorial", "file_path": "tutorials/03-advanced/neural_style_transfer/main.py", "language": "python", "complexity": "medium", "star_count": 31552, "quality_score": 0.98, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 126, "word_count": 427}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create step-by-step tutorial with clear explanations and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en v√≠deo: https://youtu.be/_y9qQZXE24A?t=20480)\n\n### MongoDB client ###\n\n# Descarga versi√≥n community: https://www.mongodb.com/try/download\n# Instalaci√≥n:https://www.mongodb.com/docs/manual/tutorial\n# M√≥dulo conexi√≥n MongoDB: pip install pymongo\n# Ejecuci√≥n: sudo mongod --dbpath \"/path/a/la/base/de/datos/\"\n# Conexi√≥n: mongodb://localhost\n\nfrom pymongo import MongoClient\n\n# Descomentar el db_client local o remoto correspondiente\n\n# Base de datos local MongoDB\ndb_client = MongoClient().local\n\n# Clase en v√≠deo: https://youtu.be/_y9qQZXE24A?t=25470\n\n# Base de datos remota MongoDB Atlas (https://mongodb.com)\n# db_client = MongoClient(\n#     \"mongodb+srv://<user>:<password>@<url>/?retryWrites=true&w=majority\").test\n\n# Despliegue API en la nube:\n# Deta - https://www.deta.sh/\n# Intrucciones - https://fastapi.tiangolo.com/deployment/deta/\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Backend/FastAPI/db/client.py", "language": "python", "complexity": "simple", "star_count": 31194, "quality_score": 0.8600000000000001, "features": {"has_code_examples": false, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 27, "word_count": 86}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Hello Python\n\n[![Python](https://img.shields.io/badge/Python-3.10+-yellow?style=for-the-badge&logo=python&logoColor=white&labelColor=101010)](https://python.org)\n[![FastAPI](https://img.shields.io/badge/FastAPI-0.88.0+-00a393?style=for-the-badge&logo=fastapi&logoColor=white&labelColor=101010)](https://fastapi.tiangolo.com)\n[![MongoDB](https://img.shields.io/badge/MongoDB-6.0+-00684A?style=for-the-badge&logo=mongodb&logoColor=white&labelColor=101010)](https://www.mongodb.com)\n[![ChatGPT](https://img.shields.io/badge/ChatGPT-GPT--4-7CF178?style=for-the-badge&logo=openai&logoColor=white&labelColor=101010)](https://platform.openai.com)\n[![Reflex](https://img.shields.io/badge/Reflex-0.4.6+-5646ED?style=for-the-badge&logo=reflex&logoColor=white&labelColor=101010)](https://reflex.dev)\n\n## Curso para aprender el lenguaje de programaci√≥n Python desde cero y para principiantes\n\n![](./Images/header.jpg)\n\n### Proyecto realizado durante emisiones en directo desde [Twitch](https://twitch.tv/mouredev)\n> ##### Si consideras √∫til el curso, ap√≥yalo haciendo \"‚òÖ Star\" en el repositorio. ¬°Gracias!\n\n## ¬°NUEVO! Curso de Python para web\n\n[![Curso Python Web](https://img.shields.io/github/stars/mouredev/python-web?label=Curso%20Python%20web&style=social)](https://github.com/mouredev/python-web)\n\n<a href=\"https://github.com/mouredev/python-web\"><img src=\"https://raw.githubusercontent.com/mouredev/python-web/main/Images/header.jpg\"/></a>\n\n## Clases en v√≠deo\n\n### Curso de fundamentos desde cero\n\nCurso que agrupa todas las clases en directo que hacen referencia a los fundamentos de Python.\n\n> C√≥digo: Directorio \"Basic\" en el proyecto\n\n<a href=\"https://youtu.be/Kp4Mvapo5kc\"><img src=\"http://i3.ytimg.com/vi/Kp4Mvapo5kc/maxresdefault.jpg\" style=\"height: 50%; width:50%;\"/></a>\n\n* [Introducci√≥n](https://youtu.be/Kp4Mvapo5kc)\n* [Contexto](https://youtu.be/Kp4Mvapo5kc?t=244)\n* [Lecci√≥n 1 - Configuraci√≥n](https://youtu.be/Kp4Mvapo5kc?t=850)\n* [Lecci√≥n 2 - Hola Mundo](https://youtu.be/Kp4Mvapo5kc?t=1518)\n* [Lecci√≥n 3 - Variables](https://youtu.be/Kp4Mvapo5kc?t=2938)\n* [Lecci√≥n 4 - Operadores](https://youtu.be/Kp4Mvapo5kc?t=5665)\n* [Lecci√≥n 5 - Strings](https://youtu.be/Kp4Mvapo5kc?t=8643)\n* [Lecci√≥n 6 - Listas](https://youtu.be/Kp4Mvapo5kc?t=10872)\n* [Lecci√≥n 7 - Tuplas](https://youtu.be/Kp4Mvapo5kc?t=14711)\n* [Lecci√≥n 8 - Sets](https://youtu.be/Kp4Mvapo5kc?t=16335)\n* [Lecci√≥n 9 - Diccionarios](https://youtu.be/Kp4Mvapo5kc?t=18506)\n* [Lecci√≥n 10 - Condicionales](https://youtu.be/Kp4Mvapo5kc?t=21442)\n* [Lecci√≥n 11 - Bucles/Loops/Ciclos](https://youtu.be/Kp4Mvapo5kc?t=23822)\n* [Lecci√≥n 12 - Funciones](https://youtu.be/Kp4Mvapo5kc?t=26619)\n* [Lecci√≥n 13 - Clases](https://youtu.be/Kp4Mvapo5kc?t=29327)\n* [Lecci√≥n 14 - Excepciones](https://youtu.be/Kp4Mvapo5kc?t=32030)\n* [Lecci√≥n 15 - M√≥dulos](https://youtu.be/Kp4Mvapo5kc?t=34583)\n* [Pr√≥ximos pasos](https://youtu.be/Kp4Mvapo5kc?t=36390)\n\n### Curso intermedio de fundamentos desde cero\n\nCurso en el que continuamos aprendiendo Python desde sus bases, siguiendo la ruta de aprendizaje desde la √∫ltima lecci√≥n del curso de inicial.\n\n> C√≥digo: Directorio \"Intermediate\" en el proyecto\n \n<a href=\"https://youtu.be/TbcEqkabAWU\"><img src=\"http://i3.ytimg.com/vi/TbcEqkabAWU/maxresdefault.jpg\" style=\"height: 50%; width:50%;\"/></a>\n\n* [Introducci√≥n](https://youtu.be/TbcEqkabAWU)\n* [Lecci√≥n 1 - Dates](https://youtu.be/TbcEqkabAWU?t=202)\n* [Lecci√≥n 2 - List Comprehension](https://youtu.be/TbcEqkabAWU?t=3239)\n* [Lecci√≥n 3 - Resoluci√≥n de retos de programaci√≥n](https://youtu.be/TbcEqkabAWU?t=4142)\n* [Lecci√≥n 4 - Lambdas](https://youtu.be/TbcEqkabAWU?t=9145)\n* [Lecci√≥n 5 - Funciones de orden superior](https://youtu.be/TbcEqkabAWU?t=10172)\n* [Lecci√≥n 6 - Tipos de error](https://youtu.be/TbcEqkabAWU?t=12721)\n* [Lecci√≥n 7 - Manejo de ficheros](https://youtu.be/TbcEqkabAWU?t=15524)\n* [Lecci√≥n 8 - Expresiones regulares](https://youtu.be/TbcEqkabAWU?t=19762)\n* [Lecci√≥n 9 - Manejo de paquetes](https://youtu.be/TbcEqkabAWU?t=24010)\n* [Pr√≥ximos pasos](https://youtu.be/TbcEqkabAWU?t=26228)\n\n### Backend desde cero\n\nCurso en el que aprenderemos a utilizar Python para backend e implementaremos un API REST con autenticaci√≥n, base de datos y desplegaremos el proyecto en un servidor real.\n\n> C√≥digo: Directorio \"Backend\" en el proyecto\n\n<a href=\"https://youtu.be/_y9qQZXE24A\"><img src=\"http://i3.ytimg.com/vi/_y9qQZXE24A/maxresdefault.jpg\" style=\"height: 50%; width:50%;\"/></a>\n\n* [Introducci√≥n](https://youtu.be/_y9qQZXE24A)\n* [Lecci√≥n 01 - ¬øQu√© es un backend?](https://youtu.be/_y9qQZXE24A?t=125)\n* [Lecci√≥n 02 - API y FastAPI](https://youtu.be/_y9qQZXE24A?t=834)\n* [Lecci√≥n 03 - Type Hints](https://youtu.be/_y9qQZXE24A?t=1810)\n* [Lecci√≥n 04 - Configuraci√≥n FastAPI](https://youtu.be/_y9qQZXE24A?t=2629)\n* [Lecci√≥n 05 - Hola mundo](https://youtu.be/_y9qQZXE24A?t=3504)\n* [Lecci√≥n 06 - Operaci√≥n GET](https://youtu.be/_y9qQZXE24A?t=5382)\n* [Lecci√≥n 07 - Peticiones HTTP](https://youtu.be/_y9qQZXE24A?t=5925)\n* [Lecci√≥n 08 - Creaci√≥n API](https://youtu.be/_y9qQZXE24A?t=6099)\n* [Lecci√≥n 09 - Path y Query](https://youtu.be/_y9qQZXE24A?t=7510)\n* [Lecci√≥n 10 - Operaciones POST, PUT y DELETE](https://youtu.be/_y9qQZXE24A?t=8529)\n* [Lecci√≥n 11 - HTTP status codes](https://youtu.be/_y9qQZXE24A?t=11072)\n* [Lecci√≥n 12 - Routers](https://youtu.be/_y9qQZXE24A?t=12475)\n* [Lecci√≥n 13 - Recursos est√°ticos](https://youtu.be/_y9qQZXE24A?t=13618)\n* [Lecci√≥n 14 - Autorizaci√≥n OAuth2](https://youtu.be/_y9qQZXE24A?t=14094)\n* [Lecci√≥n 15 - OAuth2 JWT](https://youtu.be/_y9qQZXE24A?t=17664)\n* [Lecci√≥n 16 - MongoDB](https://youtu.be/_y9qQZXE24A?t=20480)\n* [Lecci√≥n 17 - MongoDB Atlas](https://youtu.be/_y9qQZXE24A?t=25470)\n* [Lecci√≥n 18 - Despliegue en Deta *](https://youtu.be/_y9qQZXE24A?t=27335)\n* [Pr√≥ximos pasos](https://youtu.be/_y9qQZXE24A?t=28484)\n\n***ACTUALIZACI√ìN Sobre la lecci√≥n 18:** Deta ha actualizado ligeramente su servicio de despliegue de aplicaciones con FastAPI. Tienes toda la documentaci√≥n [aqu√≠](https://deta.space/docs/en/quickstart-guides/python#fastapi). Tambi√©n han creado una [gu√≠a de migraci√≥n](https://deta.space/migration/guides/migrate-a-micro/).\n\n### Frontend desde cero\n\nCursos en los que aprenderemos a utilizar Python para desarrollo web con dos proyectos reales desplegados en producci√≥n. Tutoriales en v√≠deo paso a paso con 9 horas de contenido.\n\n<a href=\"https://github.com/mouredev/python-web\"><img src=\"http://i3.ytimg.com/vi/n2YrGsXJC6Y/maxresdefault.jpg\" style=\"height: 50%; width:50%;\"/></a>\n\n<a href=\"https://github.com/mouredev/python-web\"><img src=\"http://i3.ytimg.com/vi/bNy8OZJfA6I/maxresdefault.jpg\" style=\"height: 50%; width:50%;\"/></a>\n\n[![Curso Python Web](https://img.shields.io/github/stars/mouredev/python-web?label=Curso%20Python%20web&style=social)](https://github.com/mouredev/python-web)\n\n<a href=\"https://github.com/mouredev/adeviento-web\"><img src=\"http://i3.ytimg.com/vi/h8Tn0ITRoQs/maxresdefault.jpg\" style=\"height: 50%; width:50%;\"/></a>\n\n[![Curso Python Web](https://img.shields.io/github/stars/mouredev/adeviento-web?label=Tutorial%20Python%20web%20extra&style=social)](https://github.com/mouredev/adeviento-web)\n\n### Aprende a integrar ChatGPT en tu proyecto desde cero\n\n<a href=\"https://youtu.be/b8COygWdvmw\"><img src=\"http://i3.ytimg.com/vi/b8COygWdvmw/maxresdefault.jpg\" style=\"height: 50%; width:50%;\"/></a>\n\nClase de una hora de duraci√≥n donde aprender√°s a interactuar con ChatGPT desde tu aplicaci√≥n, mantener conversaciones y establecer el contexto de la IA para potenciar tu proyecto.\n\nCon todo el c√≥digo publicado [aqu√≠](https://gist.github.com/mouredev/58abfbcef017efaf3852e8821564c011).\n\n### Traductor de Voz con IA\n\n<a href=\"https://youtu.be/oxLvf2nDCvQ\"><img src=\"http://i3.ytimg.com/vi/oxLvf2nDCvQ/maxresdefault.jpg\" style=\"height: 50%; width:50%;\"/></a>\n\nAprende a desarrollar un traductor de voz a varios idiomas utilizando con IA. Creando su Web y todo en 100 l√≠neas.\n\nCon todo el c√≥digo publicado [aqu√≠](https://gist.github.com/mouredev/0ea42112751f0187d90d5403d1f333e2).\n\n### Introducci√≥n al Testing\n\nTaller de introducci√≥n a testing con Python creado junto a [Carlos Bl√©](https://www.carlosble.com) y [Miguel A. G√≥mez](https://softwarecrafters.io), expertos en la materia.\n\n<a href=\"https://youtu.be/344uwF1z2Gg\"><img src=\"http://i3.ytimg.com/vi/344uwF1z2Gg/maxresdefault.jpg\" style=\"height: 50%; width:50%;\"/></a>\n\n### Extra: 15 curiosidades sobre Python\n\nY para finalizar... ¬øQuieres saber aun m√°s sobre √©l? Aqu√≠ tienes 15 curiosidades que quiz√°s no conozcas sobre el lenguaje.\n\n<a href=\"https://youtu.be/q2lCm2KAz3w\"><img src=\"http://i3.ytimg.com/vi/q2lCm2KAz3w/maxresdefault.jpg\" style=\"height: 50%; width:50%;\"/></a>\n\n## Informaci√≥n importante y preguntas frecuentes\n\nActualmente el curso est√° en pausa. Se han finalizados los bloques b√°sico, intermedio y backend, y ese era el objetivo inicial del proyecto.\nNo descarto a√±adir nuevas lecciones a futuro, pero creo que por el momento puede servir de base a cualquier persona que quiera empezar a aprender este lenguaje.\n\n* Recuerda que he creado en el [Discord](https://discord.gg/mouredev) un canal \"üêçpython\" para que puedas comentar lo que quieras.\n* En el momento que el curso contin√∫e, actualizar√© el repositorio y avisar√© en redes.\n\n¬°Much√≠simas gracias por todo el apoyo mostrado!\n\n## Enlaces de inter√©s\n\n* [Web oficial de Python](https://www.python.org/)\n* [Tutorial oficial de Python en Espa√±ol](https://docs.python.org/es/3/tutorial/index.html)\n* [Repo 30 d√≠as de Python](https://github.com/Asabeneh/30-Days-Of-Python)\n* [Juego Cod√©dex para aprender Python](https://www.codedex.io/)\n* [Visual Studio Code](https://code.visualstudio.com/): El editor que estoy usando\n* [FastAPI](https://fastapi.tiangolo.com/es/): El framework para crear nuestra API Backend\n* [MongoDB](https://www.mongodb.com/): La base de datos que utiliza nuestro backend\n* [Deta](https://www.deta.sh/): Para desplegar nuestra API en la nube\n\n## √önete al campus de programaci√≥n de la comunidad\n\n![https://mouredev.pro](./Images/pro.jpg)\n\n#### Te presento [mouredev pro](https://mouredev.pro), mi proyecto m√°s importante para ayudarte a estudiar programaci√≥n y desarrollo de software de manera diferente.\n\n> **¬øBuscas un extra?** Aqu√≠ encontrar√°s este y otros cursos editados por lecciones individuales, para avanzar a tu ritmo y guardar el progreso. Tambi√©n dispondr√°s de ejercicios y correcciones, test para validar tus conocimientos, examen y certificado p√∫blico de finalizaci√≥n, soporte, foro de estudiantes, reunionnes grupales, cursos exclusivos y mucho m√°s.\n> \n> Entra en **[mouredev.pro](https://mouredev.pro)** y utiliza el cup√≥n **\"PYTHON\"** con un 10% de descuento en tu primera suscripci√≥n.\n\n## ![https://mouredev.com](https://raw.githubusercontent.com/mouredev/mouredev/master/mouredev_emote.png) Hola, mi nombre es Brais Moure.\n### Freelance full-stack iOS & Android engineer\n\n[![YouTube Channel Subscribers](https://img.shields.io/youtube/channel/subscribers/UCxPD7bsocoAMq8Dj18kmGyQ?style=social)](https://youtube.com/mouredevapps?sub_confirmation=1)\n[![Twitch Status](https://img.shields.io/twitch/status/mouredev?style=social)](https://twitch.com/mouredev)\n[![Discord](https://img.shields.io/discord/729672926432985098?style=social&label=Discord&logo=discord)](https://mouredev.com/discord)\n[![Twitter Follow](https://img.shields.io/twitter/follow/mouredev?style=social)](https://twitter.com/mouredev)\n![GitHub Followers](https://img.shields.io/github/followers/mouredev?style=social)\n![GitHub Followers](https://img.shields.io/github/stars/mouredev?style=social)\n\nSoy ingeniero de software desde 2010. Desde 2018 combino mi trabajo desarrollando Apps con la creaci√≥n de contenido formativo sobre programaci√≥n y tecnolog√≠a en diferentes redes sociales como **[@mouredev](https://moure.dev)**.\n\nSi quieres unirte a nuestra comunidad de desarrollo, aprender programaci√≥n, mejorar tus habilidades y ayudar a la continuidad del proyecto, puedes encontrarnos en:\n\n[![Twitch](https://img.shields.io/badge/Twitch-Programaci√≥n_en_directo-9146FF?style=for-the-badge&logo=twitch&logoColor=white&labelColor=101010)](https://twitch.tv/mouredev)\n[![Discord](https://img.shields.io/badge/Discord-Servidor_de_la_comunidad-5865F2?style=for-the-badge&logo=discord&logoColor=white&labelColor=101010)](https://mouredev.com/discord) [![Pro](https://img.shields.io/badge/Cursos-mouredev.pro-FF5500?style=for-the-badge&logo=gnometerminal&logoColor=white&labelColor=101010)](https://moure.dev)\n[![Link](https://img.shields.io/badge/Links_de_inter√©s-moure.dev-14a1f0?style=for-the-badge&logo=Linktree&logoColor=white&labelColor=101010)](https://moure.dev) [![Web](https://img.shields.io/badge/GitHub-MoureDev-087ec4?style=for-the-badge&logo=github&logoColor=white&labelColor=101010)](https://github.com/mouredev)\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 31194, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 192, "word_count": 1039}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "### Description\n<!--- Describe your issue/bug/request in detail -->\n\n\n### In which platform does it happen?\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\n<!--- For example: -->\n<!--- * Azure Data Science Virtual Machine. -->\n<!--- * Azure Databricks.  -->\n<!--- * Other platforms.  -->\n\n\n### How do we replicate the issue?\n<!--- Please be specific as possible (use a list if needed). -->\n<!--- For example: -->\n<!--- * Create a conda environment for pyspark -->\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\n<!--- * ... -->\n\n\n### Expected behavior (i.e. solution)\n<!--- For example:  -->\n<!--- * The tests for SAR PySpark should pass successfully. -->\n\n\n### Willingness to contribute\n<!--- Go over all the following points, and put an `x` in the box that apply. -->\n- [ ] Yes, I can contribute for this issue independently.\n- [ ] Yes, I can contribute for this issue with guidance from Recommenders community.\n- [ ] No, I cannot contribute at this time.\n\n\n### Other Comments\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": ".github/ISSUE_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 20514, "quality_score": 0.8600000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 34, "word_count": 180}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Bug report\nabout: Create a report to help us improve\ntitle: \"[BUG] \"\nlabels: 'bug'\nassignees: ''\n\n---\n\n### Description\n<!--- Describe your issue/bug/request in detail -->\n\n### In which platform does it happen?\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\n<!--- For example: -->\n<!--- * Azure Data Science Virtual Machine. -->\n<!--- * Azure Databricks.  -->\n<!--- * Other platforms.  -->\n\n### How do we replicate the issue?\n<!--- Please be specific as possible (use a list if needed). -->\n<!--- For example: -->\n<!--- * Create a conda environment for pyspark -->\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\n<!--- * ... -->\n\n### Expected behavior (i.e. solution)\n<!--- For example:  -->\n<!--- * The tests for SAR PySpark should pass successfully. -->\n\n### Willingness to contribute\n<!--- Go over all the following points, and put an `x` in the box that apply. -->\n- [ ] Yes, I can contribute for this issue independently.\n- [ ] Yes, I can contribute for this issue with guidance from Recommenders community.\n- [ ] No, I cannot contribute at this time.\n\n### Other Comments\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": ".github/ISSUE_TEMPLATE/bug_report.md", "language": "markdown", "complexity": "simple", "star_count": 20514, "quality_score": 0.8600000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 38, "word_count": 200}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Feature request\nabout: Suggest an idea for this project\ntitle: \"[FEATURE] \"\nlabels: 'enhancement'\nassignees: ''\n\n---\n\n### Description\n<!--- Describe your expected feature in detail -->\n\n### Expected behavior with the suggested feature\n<!--- For example:  -->\n<!--- *Adding algorithm xxx will help people understand more about xxx use case scenarios. -->\n\n### Willingness to contribute\n<!--- Go over all the following points, and put an `x` in the box that apply. -->\n- [ ] Yes, I can contribute for this issue independently.\n- [ ] Yes, I can contribute for this issue with guidance from Recommenders community.\n- [ ] No, I cannot contribute at this time.\n\n### Other Comments\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": ".github/ISSUE_TEMPLATE/feature_request.md", "language": "markdown", "complexity": "simple", "star_count": 20514, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 24, "word_count": 115}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "### Description\n<!--- Describe your changes in detail -->\n<!--- Why is this change required? What problem does it solve? -->\n\n\n### Related Issues\n<!--- If it fixes an open issue, please link to the issue here. -->\n\n\n### References\n<!--- References would be helpful to understand the changes. -->\n<!--- References can be books, links, etc. -->\n\n\n### Checklist:\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\n- [ ] I have followed the [contribution guidelines](CONTRIBUTING.md) and code style for this project.\n- [ ] I have added tests covering my contributions.\n- [ ] I have updated the documentation accordingly.\n- [ ] I have [signed the commits](https://github.com/recommenders-team/recommenders/wiki/How-to-sign-commits), e.g. `git commit -s -m \"your commit message\"`. \n- [ ] This PR is being made to `staging branch` AND NOT TO `main branch`.\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": ".github/PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "medium", "star_count": 20514, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 23, "word_count": 161}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# ---------------------------------------------------------\n# Copyright (c) Recommenders contributors.\n# Licensed under the MIT License.\n# ---------------------------------------------------------\n\nname: azureml-tests\ndescription: \"Submit experiment to AzureML cluster\"\ninputs:\n  EXP_NAME:\n    required: true\n    description: AzureML experiment Name\n  ENV_NAME:\n    required: true\n    description: AzureML environment Name\n  TEST_KIND:\n    required: true\n    description: Type of test - unit or nightly\n  AZUREML_TEST_UMI_CLIENT_ID:\n    required: true\n    description: AzureML User-managed identity client ID\n  AZUREML_TEST_UMI_TENANT_ID:\n    required: true\n    description: AzureML User-managed identity tenant ID\n  AZUREML_TEST_UMI_SUB_ID:\n    required: true\n    description: AzureML User-managed identity subscription ID\n  AZUREML_TEST_SUBID:\n    required: true\n    description: AzureML subscription ID\n  PYTHON_VERSION:\n    required: true\n    description: Python version used for the tests\n  TEST_GROUP:\n    required: true\n    description: Test group defined in test_group.py\n  RG:\n    required: false\n    description: AzureML resource group name\n    default: \"recommenders_project_resources\"\n  WS:\n    required: false\n    description: AzureML workspace name\n    default: \"azureml-test-workspace\"\n  LOG_DIR:\n    required: false\n    description: Directory storing the test logs\n    default: \"test_logs\"\n\nruns:\n  using: \"composite\"\n  steps:\n    - name: Setup python\n      uses: actions/setup-python@v5\n      with:\n        python-version: \"3.10\"\n    - name: Install AzureML Python SDK\n      shell: bash\n      run: pip install \"azure-ai-ml>=1.26.4,<2\" \"azureml-mlflow>1,<2\"\n    - name: Log in to Azure\n      uses: azure/login@v2\n      with:\n        client-id: ${{ inputs.AZUREML_TEST_UMI_CLIENT_ID }}\n        tenant-id: ${{ inputs.AZUREML_TEST_UMI_TENANT_ID }}\n        subscription-id: ${{ inputs.AZUREML_TEST_UMI_SUB_ID }}\n    - name: Submit tests to AzureML\n      shell: bash\n      run: |\n        echo \"::group::Running tests ...\"\n        python tests/ci/azureml_tests/submit_groupwise_azureml_pytest.py \\\n          --subid ${{ inputs.AZUREML_TEST_SUBID }} \\\n          --rg ${{ inputs.RG }} \\\n          --ws ${{ inputs.WS }} \\\n          --cluster ${{ contains(inputs.TEST_GROUP, 'gpu') && 'gpu-cluster' || 'cpu-cluster' }} \\\n          --expname ${{ inputs.EXP_NAME }} \\\n          --envname ${{ inputs.ENV_NAME }} \\\n          --testkind ${{ inputs.TEST_KIND}} \\\n          --python-version ${{ inputs.PYTHON_VERSION }} \\\n          --testgroup ${{ inputs.TEST_GROUP }} \\\n          --sha ${GITHUB_SHA}\n        echo \"::endgroup::\"\n    - name: Post tests\n      if: ${{ ! cancelled() }}\n      shell: bash\n      run: |\n        echo \"::group::Pytest logs\"\n        python tests/ci/azureml_tests/post_pytest.py \\\n          --subid ${{ inputs.AZUREML_TEST_SUBID }} \\\n          --rg ${{ inputs.RG }} \\\n          --ws ${{ inputs.WS }} \\\n          --expname ${{ inputs.EXP_NAME }} \\\n          --log-dir ${{ inputs.LOG_DIR }}\n        echo \"::endgroup::\"\n    - name: Save logs\n      if: ${{ ! cancelled() }}\n      uses: actions/upload-artifact@v4\n      with:\n        name: logs-${{ inputs.TEST_GROUP }}-python${{ inputs.PYTHON_VERSION }}\n        path: ${{ inputs.LOG_DIR }}\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": ".github/actions/azureml-test/action.yml", "language": "yaml", "complexity": "simple", "star_count": 20514, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 99, "word_count": 316}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# ---------------------------------------------------------\n# Copyright (c) Recommenders contributors.\n# Licensed under the MIT License.\n# ---------------------------------------------------------\n\nname: get-test-groups\ndescription: \"Get test group names from tests_groups.py\"\ninputs:\n  TEST_KIND:\n    required: true\n    description: Type of test - pr gate or nightly\n  TEST_ENV:\n    required: false\n    description: Test environment - cpu, gpu or spark\n    default: 'cpu'\noutputs:\n  test_groups:\n    description: A list of test groups\n    value: ${{ steps.get_test_groups.outputs.test_groups }}\n\nruns:\n  using: \"composite\"\n  steps:\n    - name: Get test group names\n      id: get_test_groups\n      shell: bash\n      run: |\n        if [[ ${{ inputs.TEST_KIND }} == \"nightly\" ]]; then\n          test_groups_str=$(python -c 'from tests.ci.azureml_tests.test_groups import nightly_test_groups; print([t for t in nightly_test_groups.keys() if \"${{inputs.TEST_ENV}}\" in t])')\n        else\n          test_groups_str=$(python -c 'from tests.ci.azureml_tests.test_groups import pr_gate_test_groups; print(list(pr_gate_test_groups.keys()))')\n        fi\n        echo \"test_groups=$test_groups_str\" >> $GITHUB_OUTPUT\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": ".github/actions/get-test-groups/action.yml", "language": "yaml", "complexity": "simple", "star_count": 20514, "quality_score": 0.89, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 34, "word_count": 115}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# ---------------------------------------------------------\n# Copyright (c) Recommenders contributors.\n# Licensed under the MIT License.\n# ---------------------------------------------------------\n\nname: azureml-cpu-nightly\n\non:\n  #          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ minute (0 - 59)\n  #          ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ hour (0 - 23)\n  #          ‚îÇ ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the month (1 - 31)\n  #          ‚îÇ ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ month (1 - 12 or JAN-DEC)\n  #          ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the week (0 - 6 or SUN-SAT)                                  \n  #          ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ ‚îÇ\n  #          ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ ‚îÇ\n  schedule: \n    - cron: '0 0 */5 * *'  # running every 5 days at 12AM\n    # cron works with default branch (main) only: # https://github.community/t/on-schedule-per-branch/17525/2\n  \n  push:\n    # Because we can't schedule runs for non-main branches,\n    # to ensure we are running the build on the staging branch, we can add push policy for it\n    branches: [staging]\n    paths:\n      # Tests will be run only when there are changes in the code:\n      - examples/**\n      - '!examples/**/*.md'\n      - recommenders/**\n      - '!recommenders/**/*.md'\n      - tests/**\n      - '!tests/**/*.md'\n      - setup.py\n\n\n  # Enable manual trigger\n  workflow_dispatch:\n    inputs:\n      tags:\n        description: 'Tags to label this manual run (optional)'\n        default: 'Manual trigger'\n\n  # Make this workflow reusable\n  workflow_call:\n\njobs:\n  get-test-groups:\n    runs-on: ubuntu-24.04\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v4\n      - name: Get test group names\n        id: get_test_groups\n        uses: ./.github/actions/get-test-groups\n        with:\n          TEST_KIND: \"nightly\"\n          TEST_ENV: \"cpu\"\n      - name: Print test group names\n        run: echo ${{ steps.get_test_groups.outputs.test_groups }}\n        shell: bash\n    outputs:\n      test_groups: ${{ steps.get_test_groups.outputs.test_groups }}\n\n  execute-tests:\n    needs: get-test-groups\n    name: ${{ join(matrix.*, ', ') }}\n    runs-on: ubuntu-24.04\n    timeout-minutes: 120 \n    permissions:\n      id-token: write # This is required for requesting the JWT\n    strategy:\n      max-parallel: 50 # Usage limits: https://docs.github.com/en/actions/learn-github-actions/usage-limits-billing-and-administration\n      matrix:\n        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\"]\n        test-group:  ${{ fromJSON(needs.get-test-groups.outputs.test_groups) }}\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v4\n      - name: Execute tests\n        uses: ./.github/actions/azureml-test\n        id: execute_tests\n        with:\n          EXP_NAME: recommenders-nightly-${{ matrix.test-group }}-python${{ matrix.python-version }}-${{ github.ref }}\n          ENV_NAME: recommenders-${{ github.sha }}-python${{ matrix.python-version }}${{ contains(matrix.test-group, 'gpu') && '-gpu' || '' }}${{ contains(matrix.test-group, 'spark') && '-spark' || '' }}\n          TEST_KIND: 'nightly'\n          AZUREML_TEST_UMI_CLIENT_ID: ${{ secrets.AZUREML_TEST_UMI_CLIENT_ID }}\n          AZUREML_TEST_UMI_TENANT_ID: ${{ secrets.AZUREML_TEST_UMI_TENANT_ID }}\n          AZUREML_TEST_UMI_SUB_ID: ${{ secrets.AZUREML_TEST_UMI_SUB_ID }}\n          AZUREML_TEST_SUBID: ${{ secrets.AZUREML_TEST_SUBID }}\n          PYTHON_VERSION: ${{ matrix.python-version }}\n          TEST_GROUP: ${{ matrix.test-group }}\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": ".github/workflows/azureml-cpu-nightly.yml", "language": "yaml", "complexity": "medium", "star_count": 20514, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 91, "word_count": 347}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# ---------------------------------------------------------\n# Copyright (c) Recommenders contributors.\n# Licensed under the MIT License.\n# ---------------------------------------------------------\n\nname: azureml-gpu-nightly\n\non:\n  #          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ minute (0 - 59)\n  #          ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ hour (0 - 23)\n  #          ‚îÇ ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the month (1 - 31)\n  #          ‚îÇ ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ month (1 - 12 or JAN-DEC)\n  #          ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the week (0 - 6 or SUN-SAT)                                  \n  #          ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ ‚îÇ\n  #          ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ ‚îÇ\n  schedule: \n    - cron: '0 0 */5 * *'  # running every 5 days at 12AM\n    # cron works with default branch (main) only: # https://github.community/t/on-schedule-per-branch/17525/2\n  \n  push:\n    # Because we can't schedule runs for non-main branches,\n    # to ensure we are running the build on the staging branch, we can add push policy for it\n    branches: [staging]\n    paths:\n      # Tests will be run only when there are changes in the code:\n      - examples/**\n      - '!examples/**/*.md'\n      - recommenders/**\n      - '!recommenders/**/*.md'\n      - tests/**\n      - '!tests/**/*.md'\n      - setup.py\n\n\n  # Enable manual trigger\n  workflow_dispatch:\n    inputs:\n      tags:\n        description: 'Tags to label this manual run (optional)'\n        default: 'Manual trigger'\n\n  # Make this workflow reusable\n  workflow_call:\n\njobs:\n  get-test-groups:\n    runs-on: ubuntu-24.04\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v4\n      - name: Get test group names\n        id: get_test_groups\n        uses: ./.github/actions/get-test-groups\n        with:\n          TEST_KIND: \"nightly\"\n          TEST_ENV: \"gpu\"\n      - name: Print test group names\n        run: echo ${{ steps.get_test_groups.outputs.test_groups }}\n        shell: bash\n    outputs:\n      test_groups: ${{ steps.get_test_groups.outputs.test_groups }}\n\n  execute-tests:\n    needs: get-test-groups\n    name: ${{ join(matrix.*, ', ') }}\n    runs-on: ubuntu-24.04\n    timeout-minutes: 120 \n    permissions:\n      id-token: write # This is required for requesting the JWT\n    strategy:\n      max-parallel: 50 # Usage limits: https://docs.github.com/en/actions/learn-github-actions/usage-limits-billing-and-administration\n      matrix:\n        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\"]\n        test-group:  ${{ fromJSON(needs.get-test-groups.outputs.test_groups) }}\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v4\n      - name: Execute tests\n        uses: ./.github/actions/azureml-test\n        id: execute_tests\n        with:\n          EXP_NAME: recommenders-nightly-${{ matrix.test-group }}-python${{ matrix.python-version }}-${{ github.ref }}\n          ENV_NAME: recommenders-${{ github.sha }}-python${{ matrix.python-version }}${{ contains(matrix.test-group, 'gpu') && '-gpu' || '' }}${{ contains(matrix.test-group, 'spark') && '-spark' || '' }}\n          TEST_KIND: 'nightly'\n          AZUREML_TEST_UMI_CLIENT_ID: ${{ secrets.AZUREML_TEST_UMI_CLIENT_ID }}\n          AZUREML_TEST_UMI_TENANT_ID: ${{ secrets.AZUREML_TEST_UMI_TENANT_ID }}\n          AZUREML_TEST_UMI_SUB_ID: ${{ secrets.AZUREML_TEST_UMI_SUB_ID }}\n          AZUREML_TEST_SUBID: ${{ secrets.AZUREML_TEST_SUBID }}\n          PYTHON_VERSION: ${{ matrix.python-version }}\n          TEST_GROUP: ${{ matrix.test-group }}\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": ".github/workflows/azureml-gpu-nightly.yml", "language": "yaml", "complexity": "medium", "star_count": 20514, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 91, "word_count": 347}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright (c) Recommenders contributors.\n# Licensed under the MIT License.\n\non:\n  push:\n    # Sequence of patterns matched against refs/tags\n    tags:\n      - '0.*' # Push events to matching 0.*, 1.*\n      - '1.*'\n\nname: Create GitHub Draft Release\n\njobs:\n  \n  unit-test-workflow:\n    uses: ./.github/workflows/azureml-unit-tests.yml\n    secrets: inherit\n\n  cpu-nightly-workflow:\n    uses: ./.github/workflows/azureml-cpu-nightly.yml\n    secrets: inherit\n\n  gpu-nightly-workflow:\n    uses: ./.github/workflows/azureml-gpu-nightly.yml\n    secrets: inherit\n\n  spark-nightly-workflow:\n    uses: ./.github/workflows/azureml-spark-nightly.yml\n    secrets: inherit\n\n  create-release:\n    runs-on: ubuntu-24.04\n    needs: [unit-test-workflow, cpu-nightly-workflow, gpu-nightly-workflow, spark-nightly-workflow]\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v4\n      - name: Setup python\n        uses: actions/setup-python@v5\n        with: \n          python-version: \"3.10\"\n      - name: Install wheel package\n        run: pip install wheel\n      - name: Create wheel from setup.py\n        run: python setup.py bdist_wheel\n      - name: Create tar gz from setup.py\n        run: python setup.py sdist\n      - name: Create GitHub Draft Release\n        id: create_release\n        uses: actions/create-release@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          tag_name: ${{ github.ref }}\n          release_name: Recommenders ${{ github.ref }}\n          body: |\n            Changes in this Release\n            - First Change\n            - Second Change\n          draft: true\n          prerelease: false\n      - name: Upload .whl and .tar.gz files as Release Assets\n        id: upload-release-assets\n        uses: dwenegar/upload-release-assets@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          release_id: ${{ steps.create_release.outputs.id }}\n          assets_path: dist\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": ".github/workflows/azureml-release-pipeline.yml", "language": "yaml", "complexity": "simple", "star_count": 20514, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 69, "word_count": 181}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# ---------------------------------------------------------\n# Copyright (c) Recommenders contributors.\n# Licensed under the MIT License.\n# ---------------------------------------------------------\n\nname: azureml-spark-nightly\n\non:\n  #          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ minute (0 - 59)\n  #          ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ hour (0 - 23)\n  #          ‚îÇ ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the month (1 - 31)\n  #          ‚îÇ ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ month (1 - 12 or JAN-DEC)\n  #          ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the week (0 - 6 or SUN-SAT)                                  \n  #          ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ ‚îÇ\n  #          ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ ‚îÇ\n  schedule: \n    - cron: '0 0 */5 * *'  # running every 5 days at 12AM\n    # cron works with default branch (main) only: # https://github.community/t/on-schedule-per-branch/17525/2\n  \n  push:\n    # Because we can't schedule runs for non-main branches,\n    # to ensure we are running the build on the staging branch, we can add push policy for it\n    branches: [staging]\n    paths:\n      # Tests will be run only when there are changes in the code:\n      - examples/**\n      - '!examples/**/*.md'\n      - recommenders/**\n      - '!recommenders/**/*.md'\n      - tests/**\n      - '!tests/**/*.md'\n      - setup.py\n\n  # Enable manual trigger\n  workflow_dispatch:\n    inputs:\n      tags:\n        description: 'Tags to label this manual run (optional)'\n        default: 'Manual trigger'\n\n  # Make this workflow reusable\n  workflow_call:\n\njobs:\n  get-test-groups:\n    runs-on: ubuntu-24.04\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v4\n      - name: Get test group names\n        id: get_test_groups\n        uses: ./.github/actions/get-test-groups\n        with:\n          TEST_KIND: \"nightly\"\n          TEST_ENV: \"spark\"\n      - name: Print test group names\n        run: echo ${{ steps.get_test_groups.outputs.test_groups }}\n        shell: bash\n    outputs:\n      test_groups: ${{ steps.get_test_groups.outputs.test_groups }}\n\n  execute-tests:\n    needs: get-test-groups\n    name: ${{ join(matrix.*, ', ') }}\n    runs-on: ubuntu-24.04\n    timeout-minutes: 120 \n    permissions:\n      id-token: write # This is required for requesting the JWT\n    strategy:\n      max-parallel: 50 # Usage limits: https://docs.github.com/en/actions/learn-github-actions/usage-limits-billing-and-administration\n      matrix:\n        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\"]\n        test-group:  ${{ fromJSON(needs.get-test-groups.outputs.test_groups) }}\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v4\n      - name: Execute tests\n        uses: ./.github/actions/azureml-test\n        id: execute_tests\n        with:\n          EXP_NAME: recommenders-nightly-${{ matrix.test-group }}-python${{ matrix.python-version }}-${{ github.ref }}\n          ENV_NAME: recommenders-${{ github.sha }}-python${{ matrix.python-version }}${{ contains(matrix.test-group, 'gpu') && '-gpu' || '' }}${{ contains(matrix.test-group, 'spark') && '-spark' || '' }}\n          TEST_KIND: 'nightly'\n          AZUREML_TEST_UMI_CLIENT_ID: ${{ secrets.AZUREML_TEST_UMI_CLIENT_ID }}\n          AZUREML_TEST_UMI_TENANT_ID: ${{ secrets.AZUREML_TEST_UMI_TENANT_ID }}\n          AZUREML_TEST_UMI_SUB_ID: ${{ secrets.AZUREML_TEST_UMI_SUB_ID }}\n          AZUREML_TEST_SUBID: ${{ secrets.AZUREML_TEST_SUBID }}\n          PYTHON_VERSION: ${{ matrix.python-version }}\n          TEST_GROUP: ${{ matrix.test-group }}\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": ".github/workflows/azureml-spark-nightly.yml", "language": "yaml", "complexity": "medium", "star_count": 20514, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 90, "word_count": 347}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# ---------------------------------------------------------\n# Copyright (c) Recommenders contributors.\n# Licensed under the MIT License.\n# ---------------------------------------------------------\n\nname: azureml-unit-tests\n\non:\n  pull_request:\n    branches:\n      - 'staging'\n      - 'main'\n    paths:\n      # Tests will be run only when there are changes in the code:\n      - examples/**\n      - '!examples/**/*.md'\n      - recommenders/**\n      - '!recommenders/**/*.md'\n      - tests/**\n      - '!tests/**/*.md'\n      - setup.py\n      - .github/**\n    types: [opened, synchronize, reopened, ready_for_review]\n\n  # Enable manual trigger\n  workflow_dispatch:\n    inputs:\n      tags:\n        description: 'Tags to label this manual run (optional)'\n        default: 'Manual trigger'\n\n  # Make this workflow reusable\n  workflow_call:\n\njobs:\n  get-test-groups:\n    runs-on: ubuntu-24.04\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v4\n      - name: Get test group names\n        id: get_test_groups\n        uses: ./.github/actions/get-test-groups\n        with:\n          TEST_KIND: \"unit\"\n      - name: Print test group names\n        run: echo ${{ steps.get_test_groups.outputs.test_groups }}\n        shell: bash\n    outputs:\n      test_groups: ${{ steps.get_test_groups.outputs.test_groups }}\n    if: github.event.pull_request.draft == false\n\n  execute-tests:\n    needs: get-test-groups\n    name: ${{ join(matrix.*, ', ') }}\n    runs-on: ubuntu-24.04\n    timeout-minutes: 120 \n    permissions:\n      id-token: write # This is required for requesting the JWT\n    strategy:\n      max-parallel: 50 # Usage limits: https://docs.github.com/en/actions/learn-github-actions/usage-limits-billing-and-administration\n      matrix:\n        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\"]\n        test-group:  ${{ fromJSON(needs.get-test-groups.outputs.test_groups) }}\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v4\n      - name: Execute tests\n        uses: ./.github/actions/azureml-test\n        id: execute_tests\n        with:\n          EXP_NAME: recommenders-unit-${{ matrix.test-group }}-python${{ matrix.python-version }}-${{ github.sha }}\n          ENV_NAME: recommenders-${{ github.sha }}-python${{ matrix.python-version }}${{ contains(matrix.test-group, 'gpu') && '-gpu' || '' }}${{ contains(matrix.test-group, 'spark') && '-spark' || '' }}\n          TEST_KIND: 'unit'\n          AZUREML_TEST_UMI_CLIENT_ID: ${{ secrets.AZUREML_TEST_UMI_CLIENT_ID }}\n          AZUREML_TEST_UMI_TENANT_ID: ${{ secrets.AZUREML_TEST_UMI_TENANT_ID }}\n          AZUREML_TEST_UMI_SUB_ID: ${{ secrets.AZUREML_TEST_UMI_SUB_ID }}\n          AZUREML_TEST_SUBID: ${{ secrets.AZUREML_TEST_SUBID }}\n          PYTHON_VERSION: ${{ matrix.python-version }}\n          TEST_GROUP: ${{ matrix.test-group }}\n    if: github.event.pull_request.draft == false\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": ".github/workflows/azureml-unit-tests.yml", "language": "yaml", "complexity": "medium", "star_count": 20514, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 82, "word_count": 248}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# ---------------------------------------------------------\n# Copyright (c) Recommenders contributors.\n# Licensed under the MIT License.\n# ---------------------------------------------------------\n# This workflow will run tests and do packaging for contrib/sarplus.\n#\n# References:\n#   * GitHub Actions workflow templates\n#       + [python package](https://github.com/actions/starter-workflows/blob/main/ci/python-package.yml)\n#       + [scala](https://github.com/actions/starter-workflows/blob/main/ci/scala.yml)\n#   * [GitHub hosted runner - Ubuntu 24.04 LTS](https://github.com/actions/runner-images/blob/main/images/ubuntu/Ubuntu2404-Readme.md)\n#   * [Azure Databricks runtime releases](https://docs.microsoft.com/en-us/azure/databricks/release-notes/runtime/releases)\n#   * [Azure Synapse Analytics runtimes](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-version-support)\n\n\nname: sarplus test and package\n\non:\n  push:\n    paths:\n      - contrib/sarplus/python/**\n      - contrib/sarplus/scala/**\n      - contrib/sarplus/VERSION\n      - .github/workflows/sarplus.yml\n  \n  # Enable manual trigger\n  workflow_dispatch:\n\nenv:\n  SARPLUS_ROOT: ${{ github.workspace }}/contrib/sarplus\n  PYTHON_ROOT: ${{ github.workspace }}/contrib/sarplus/python\n  SCALA_ROOT: ${{ github.workspace }}/contrib/sarplus/scala\n\njobs:\n  python:\n    # Test pysarplus with different versions of Python.\n    # Package pysarplus and upload as GitHub workflow artifact when merged into\n    # the main branch.\n    runs-on: ubuntu-24.04\n    strategy:\n      matrix:\n        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\"]\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install dependencies\n        run: |\n          python -m pip install -U build cibuildwheel pip twine\n          python -m pip install -U flake8 pytest pytest-cov scikit-learn\n\n          # Install sbt\n          # See https://github.com/yokra9/akka-http-example/pull/119/files\n          echo \"deb https://repo.scala-sbt.org/scalasbt/debian all main\" | sudo tee /etc/apt/sources.list.d/sbt.list\n          echo \"deb https://repo.scala-sbt.org/scalasbt/debian /\" | sudo tee /etc/apt/sources.list.d/sbt_old.list\n          curl -sL \"https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823\" | sudo apt-key add\n          sudo apt-get update\n          sudo apt-get install sbt\n\n      - name: Lint with flake8\n        run: |\n          cd \"${PYTHON_ROOT}\"\n          # See https://flake8.pycqa.org/en/latest/user/index.html\n          flake8 .\n\n      - name: Package and check\n        run: |\n          # build\n          cd \"${PYTHON_ROOT}\"\n          cp \"${SARPLUS_ROOT}/VERSION\" ./pysarplus/VERSION\n          python -m build --sdist\n          PYTHON_VER='${{ matrix.python-version }}'\n          MINOR_VER=\"${PYTHON_VER#*.}\"\n          CIBW_BUILD=\"cp3${MINOR_VER}-manylinux_x86_64\" python -m cibuildwheel --platform linux --output-dir dist\n          python -m twine check dist/*\n\n          # set sarplus_version for the artifact upload step\n          SARPLUS_VERSION=$(cat \"${SARPLUS_ROOT}/VERSION\")\n          echo \"sarplus_version=${SARPLUS_VERSION}\" >> $GITHUB_ENV\n\n      - name: Test\n        run: |\n          cd \"${PYTHON_ROOT}\"\n          python -m pip install dist/*.whl\n\n          cd \"${SCALA_ROOT}\"\n          export SPARK_VERSION=$(python -m pip show pyspark | grep -i version | cut -d ' ' -f 2)\n          SPARK_JAR_DIR=$(python -m pip show pyspark | grep -i location | cut -d ' ' -f2)/pyspark/jars\n          SCALA_JAR=$(ls ${SPARK_JAR_DIR}/scala-library*)\n          HADOOP_JAR=$(ls ${SPARK_JAR_DIR}/hadoop-client-api*)\n          SCALA_VERSION=${SCALA_JAR##*-}\n          export SCALA_VERSION=${SCALA_VERSION%.*}\n          HADOOP_VERSION=${HADOOP_JAR##*-}\n          export HADOOP_VERSION=${HADOOP_VERSION%.*}\n          sbt ++\"${SCALA_VERSION}\"! package\n\n          cd \"${PYTHON_ROOT}\"\n          pytest ./tests\n\n      - name: Upload Python wheel as GitHub artifact when merged into main\n        # Upload the whl file of the specific python version\n        if: github.ref == 'refs/heads/main'\n        uses: actions/upload-artifact@v4\n        with:\n          name: pysarplus-${{ env.sarplus_version }}-cp${{ matrix.python-version }}-wheel\n          path: ${{ env.PYTHON_ROOT }}/dist/*.whl\n\n      - name: Upload Python source as GitHub artifact when merged into main\n        # Only one pysarplus source tar file is needed\n        if: github.ref == 'refs/heads/main' && matrix.python-version == '3.10'\n        uses: actions/upload-artifact@v4\n        with:\n          name: pysarplus-${{ env.sarplus_version }}-source\n          path: ${{ env.PYTHON_ROOT }}/dist/*.tar.gz\n\n  scala:\n    # Test sarplus with different versions of Databricks and Synapse runtime\n    runs-on: ubuntu-22.04\n    strategy:\n      matrix:\n        include:\n          # For Azure Databricks 7.3 LTS\n          - scala-version: \"2.12.10\"\n            spark-version: \"3.0.1\"\n            hadoop-version: \"2.7.4\"\n\n          # For Azure Databricks 9.1 LTS and Azure Synapse Apache Spark 3.1 runtime\n          - scala-version: \"2.12.10\"\n            spark-version: \"3.1.2\"\n            hadoop-version: \"2.7.4\"\n\n          # For Azure Databricks 10.4 LTS\n          - scala-version: \"2.12.14\"\n            spark-version: \"3.2.1\"\n            hadoop-version: \"3.3.1\"\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install sbt\n        run: |\n          # See https://github.com/yokra9/akka-http-example/pull/119/files\n          echo \"deb https://repo.scala-sbt.org/scalasbt/debian all main\" | sudo tee /etc/apt/sources.list.d/sbt.list\n          echo \"deb https://repo.scala-sbt.org/scalasbt/debian /\" | sudo tee /etc/apt/sources.list.d/sbt_old.list\n          curl -sL \"https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823\" | sudo apt-key add\n          sudo apt-get update\n          sudo apt-get install sbt\n\n      - name: Test\n        run: |\n          cd \"${SCALA_ROOT}\"\n          export SPARK_VERSION=\"${{ matrix.spark-version }}\"\n          export HADOOP_VERSION=\"${{ matrix.hadoop-version }}\"\n          sbt ++${{ matrix.scala-version }}! test\n\n      - name: Package\n        # Generate jar files for Spark below 3.2 and above 3.2\n        if: github.ref == 'refs/heads/main' && matrix.spark-version != '3.0.1'\n        env:\n          GPG_KEY: ${{ secrets.SARPLUS_GPG_PRI_KEY_ASC }}\n        run: |\n          SARPLUS_VERSION=$(cat \"${SARPLUS_ROOT}/VERSION\")\n\n          # generate artifacts\n          cd \"${SCALA_ROOT}\"\n          export SPARK_VERSION='${{ matrix.spark-version }}'\n          export HADOOP_VERSION='${{ matrix.hadoop-version }}'\n          export SCALA_VERSION='${{ matrix.scala-version }}'\n          sbt ++${SCALA_VERSION}! package\n          sbt ++${SCALA_VERSION}! packageDoc\n          sbt ++${SCALA_VERSION}! packageSrc\n          sbt ++${SCALA_VERSION}! makePom\n\n          # sign with GPG\n          cd \"${SCALA_ROOT}/target/scala-2.12\"\n          gpg --import <(cat <<< \"${GPG_KEY}\")\n          for file in {*.jar,*.pom}; do gpg -ab \"${file}\"; done\n\n          # bundle\n          [ '${{ matrix.spark-version }}' == '3.1.2' ] \\\n            && jar cvf sarplus-${SARPLUS_VERSION}-bundle_2.12-spark-${SPARK_VERSION}.jar \\\n                       sarplus_*.jar \\\n                       sarplus_*.pom \\\n                       sarplus_*.asc\n          [ '${{ matrix.spark-version }}' == '3.2.1' ] \\\n            && jar cvf sarplus-spark-3.2-plus-bundle_2.12-${SARPLUS_VERSION}.jar \\\n                       sarplus-spark*.jar \\\n                       sarplus-spark*.pom \\\n                       sarplus-spark*.asc\n\n          # set sarplus_version for the artifact upload step\n          echo \"sarplus_version=${SARPLUS_VERSION}\" >> $GITHUB_ENV\n\n      - name: Upload Scala bundle as GitHub artifact when merged into main\n        if: github.ref == 'refs/heads/main'\n        uses: actions/upload-artifact@v4\n        with:\n          name: sarplus-${{ env.sarplus_version }}-bundle_2.12-spark-${{ matrix.spark-version }}-jar\n          path: ${{ env.SCALA_ROOT }}/target/scala-2.12/*bundle*.jar\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": ".github/workflows/sarplus.yml", "language": "yaml", "complexity": "complex", "star_count": 20514, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 204, "word_count": 703}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# ---------------------------------------------------------\n# Copyright (c) Recommenders contributors.\n# Licensed under the MIT License.\n# ---------------------------------------------------------\n\nname: Update Documentation\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  build:\n    runs-on: ubuntu-24.04\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.10\"\n\n      - name: Install dependencies\n        run: |\n          pip install -q --upgrade pip setuptools wheel\n          pip install -q --no-use-pep517 lightfm\n          pip install -q .[all]\n          pip install -q -r docs/requirements-doc.txt\n      \n      - name: List dependencies\n        run: |\n          pip list\n\n      - name: Build documentation\n        run: |\n          jupyter-book config sphinx docs/\n          sphinx-build docs docs/_build/html -b html\n\n      - name: Configure Git\n        run: |\n          git config --global user.email \"actions@github.com\"\n          git config --global user.name \"GitHub Actions\"\n\n      - name: Create and switch to gh-pages branch\n        run: |\n          git checkout -b gh-pages\n          git pull origin gh-pages || true\n      \n      - name: Copy built documentation\n        run: cp -r docs/_build/html/* .\n  \n      - name: Add and commit changes\n        run: |\n          git add * -f\n          git commit -m \"Update documentation\"\n      \n      - name: Configure pull strategy (rebase)\n        run: git config pull.rebase true\n\n      - name: Pull latest changes from remote gh-pages branch\n        run: git pull -Xtheirs origin gh-pages\n\n      - name: Push changes to gh-pages\n        run: git push origin gh-pages\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": ".github/workflows/update_documentation.yml", "language": "yaml", "complexity": "simple", "star_count": 20514, "quality_score": 0.8600000000000001, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 68, "word_count": 195}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!--\nCopyright (c) Recommenders contributors.\nLicensed under the MIT License.\n-->\n\nContributors to Recommenders \n============================\nRecommenders is developed and maintained by a community of people interested in exploring recommendation algorithms and how best to deploy them in industry settings. The goal is to accelerate the workflow of any individual or organization working on recommender systems. Everyone is encouraged to contribute at any level to add and improve the implemented algorithms, notebooks and utilities.\n\n<p align=\"center\">\n  <img src=\"https://contributors-img.web.app/image?repo=recommenders-team/recommenders\" width = 500/>\n</p>\n\nMaintainers (sorted alphabetically)\n---------------------------------------\nMaintainers are actively supporting the project and have made substantial contributions to the repository.<br>\nThey have admin access to the repo and provide support reviewing issues and pull requests.\n\n* **[Andreas Argyriou](https://github.com/anargyri)**\n   * SAR single node improvements\n   * Reco utils metrics computations\n   * Tests for Surprise\n   * Model selection notebooks (AzureML for SVD, NNI) \n* **[Jianxun Lian](https://github.com/Leavingseason)**\n   * xDeepFM algorithm\n   * DKN algorithm\n   * Review, development and optimization of MSRA algorithms.\n* **[Jun Ki Min](https://github.com/loomlike)**\n   * ALS notebook\n   * Wide & Deep algorithm\n   * Hyperparameter tuning notebooks\n* **[Miguel Gonz√°lez-Fierro](https://github.com/miguelfierro)**\n   * Recommendation algorithms review, development and optimization.\n   * Reco utils review, development and optimization.\n   * Continuous integration build / test setup.\n* **[Scott Graham](https://github.com/gramhagen)**\n   * Improving documentation\n   * VW notebook\n* **[Simon Zhao](https://github.com/simonyansenzhao)**\n   * SARplus algorithm upgrade\n* **[Tao Wu](https://github.com/wav8k)**\n   * Improving documentation\n\n\nContributors  (sorted alphabetically)\n-------------------------------------\n[Full List of Contributors](https://github.com/Microsoft/Recommenders/graphs/contributors)\n\nTo contributors: please add your name to the list when you submit a patch to the project.\n\n* **[Aaron He](https://github.com/AaronHeee)**\n   * Reco utils of NCF\n   * Deep dive notebook demonstrating the use of NCF\n* **[Aaron Palpallatoc](https://github.com/ubergonmx)**\n   * Corrected variable in pickle dump in `mind_utils.ipynb` notebook\n* **[Abir Chakraborty](https://github.com/aeroabir)**\n   * Self-Attentive Sequential Recommendation (SASRec)\n   * Sequential Recommendation Via Personalized Transformer (SSEPT)\n* **[Alexandros Ioannou](https://github.com/aioannou96)**\n   * Standard VAE algorithm \n   * Multinomial VAE algorithm  \n* **[Bamdev Mishra](https://github.com/bamdevm)**\n   * RLRMC algorithm\n   * GeoIMC algorithm\n* **[Beth Zeranski](https://github.com/bethz)**\n   * DevOps Pipelines used as a control plane to run existing Pytests on AzureML\n   * Automation scripts to configure AzureML environment for pipeline use \n* **[Chuyang Ke](https://github.com/ChuyangKe)**\n   * Reco utils optimization\n   * Performance tests\n* **[Dan Bianchini](https://github.com/danb27)**\n   * SAR Single Node algorithm improvements\n* **[Dan Ciborowski](https://github.com/dciborow)**\n   * ALS operationalization notebook\n   * SAR PySpark improvement\n* **[Daniel Schneider](https://github.com/danielsc)**\n   * FastAI notebook\n* **[David Dav√≥](https://github.com/daviddavo)**\n   * Added R-Precision metric\n* **[Evgenia Chroni](https://github.com/EvgeniaChroni)**\n   * Multinomial VAE algorithm\n   * Standard VAE algorithm\n* **[Gianluca Campanella](https://github.com/gcampanella)**\n   * Spark optimization and support\n* **[Heather Spetalnick (Shapiro)](https://github.com/heatherbshapiro)**\n   * AzureML documentation and support\n* **[Jeremy Reynolds](https://github.com/jreynolds01)**\n   * Reference architecture\n* **[Jianjie Liu](https://github.com/laserprec/)**\n   * GitHub Action Migration\n   * Test Infrastructure Optimization\n* **[Kaisar Mussalim](https://github.com/kmussalim)**\n   * Multinomial VAE algorithm\n   * Standard VAE algorithm\n* **[Le Zhang](https://github.com/yueguoguo)**\n   * Reco utils\n   * Continuous integration build / test setup\n   * Quickstart, deep dive, algorithm comparison, notebooks\n* **[Markus Cozowicz](https://github.com/eisber)**\n   * SAR improvements on Spark\n* **[Max Kaznady](https://github.com/maxkazmsft)**\n   * Early SAR single node code and port from another internal codebase\n   * Early SAR on Spark-SQL implementation\n   * SAR notebooks\n   * SAR unit / integration / smoke tests\n   * Early infrastructure design based on collapsing another internal project\n* **[Mirco Milletar√¨](https://github.com/WessZumino)**\n   * Restricted Boltzmann Machine algorithm\n* **[Nicolas Hug](https://github.com/NicolasHug)**\n   * Jupyter notebook demonstrating the use of [Surprise](https://github.com/NicolasHug/Surprise) library for recommendations\n* **[Nikhil Joglekar](https://github.com/nikhilrj)**\n   * Improving documentation\n   * Quick start notebook\n   * Operationalization notebook\n* **[Nile Wilson](https://github.com/niwilso)**\n   * Term Frequency - Inverse Document Frequency (TF-IDF) quickstart, utils\n* **[Pradnyesh Vineet Joshi](https://github.com/pradnyeshjoshi)**\n   * GitHub workflows to trigger unit, smoke and integration tests in parallel on AzureML\n   * Scripts to configure AzureML environment\n* **[Pratik Jawanpuria](https://github.com/pratikjawanpuria)**\n   * RLRMC algorithm\n   * GeoIMC algorithm\n* **[Qi Wan](https://github.com/Qcactus)**\n   * LightGCN algorithm\n   * Deep dive notebook demonstrating the use of LightGCN\n* **[Quoc-Tuan Truong](https://github.com/tqtg)**\n   * BPR notebook using [Cornac](https://github.com/PreferredAI/cornac) framework\n   * BiVAE notebook using [Cornac](https://github.com/PreferredAI/cornac) framework\n* **[Robert Alexander](https://github.com/roalexan)**\n   * Windows test pipelines\n* **[Satyadev Ntv](https://github.com/satyadevntv)**\n   * GeoIMC algorithm\n* **[Yan Zhang](https://github.com/YanZhangADS)**\n   * Diversity metrics including coverage, novelty, diversity, and serendipity\n   * Diversity metrics evaluation sample notebook\n* **[Yassine Khelifi](https://github.com/datashinobi)**\n   * SAR notebook quickstart\n* **[Zhenhui Xu](https://github.com/motefly)**\n   * Reco utils of LightGBM\n   * LightGBM notebook quickstart\n    \n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": "AUTHORS.md", "language": "markdown", "complexity": "medium", "star_count": 20514, "quality_score": 0.89, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 142, "word_count": 664}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!--\nCopyright (c) Recommenders contributors.\nLicensed under the MIT License.\n-->\n\n# Recommenders Code of Conduct\n\nThis code of conduct outlines expectations for participation in the Recommenders open source community, as well as steps for reporting unacceptable behavior. We are committed to providing a welcoming and inspiring community for all. People violating this code of conduct may be banned from the community. Our open source community strives to:\n\n* **Be friendly and patient**: Remember you might not be communicating in someone else's primary spoken or programming language, and others may not have your level of understanding.\n\n* **Be welcoming**: Our community welcomes and supports people of all backgrounds and identities. This includes, but is not limited to members of any race, ethnicity, culture, national origin, color, immigration status, social and economic class, educational level, sex, sexual orientation, gender identity and expression, age, size, family status, political belief, religion, and mental and physical ability.\n\n* **Be respectful**: We are a world-wide community of professionals, and we conduct ourselves professionally. Disagreement is no excuse for poor behavior and poor manners. Disrespectful and unacceptable behavior includes, but is not limited to:\n    1. Violent threats or language.\n    1. Discriminatory or derogatory jokes and language.\n    1. Posting sexually explicit or violent material.\n    1. Posting, or threatening to post, people's personally identifying information (\"doxing\").\n    1. Insults, especially those using discriminatory terms or slurs.\n    1. Behavior that could be perceived as sexual attention.\n    1. Advocating for or encouraging any of the above behaviors.\n\n* **Understand disagreements**: Disagreements, both social and technical, are useful learning opportunities. Seek to understand the other viewpoints and resolve differences constructively.\n\n* **Remember that we‚Äôre different**. The strength of our community comes from its diversity, people from a wide range of backgrounds. Different people have different perspectives on issues. Being unable to understand why someone holds a viewpoint doesn‚Äôt mean that they‚Äôre wrong. Focus on helping to resolve issues and learning from mistakes.\n\n* This code is not exhaustive or complete. It serves to capture our common understanding of a productive, collaborative environment. We expect the code to be followed in spirit as much as in the letter.\n\n## Reporting Code of Conduct Issues\n\nWe encourage all communities to resolve issues on their own whenever possible. This builds a broader and deeper understanding and ultimately a healthier interaction. In the event that an issue cannot be resolved locally, please feel free to report your concerns by contacting conduct@lfai.foundation. In your report please include:\n\n1. Your contact information.\n1. Names (real, usernames or pseudonyms) of any individuals involved. If there are additional witnesses, please include them as well.\n1. Your account of what occurred, and if you believe the incident is ongoing. If there is a publicly available record (e.g. a mailing list archive or a public chat log), please include a link or attachment.\n1. Any additional information that may be helpful.\n\nAll reports will be reviewed by a multi-person team and will result in a response that is deemed necessary and appropriate to the circumstances. Where additional perspectives are needed, the team may seek insight from others with relevant expertise or experience. The confidentiality of the person reporting the incident will be kept at all times. Involved parties are never part of the review team.\n\nAnyone asked to stop unacceptable behavior is expected to comply immediately. If an individual engages in unacceptable behavior, the review team may take any action they deem appropriate, including a permanent ban from the community.\n\n*This code of conduct is based on the [template](http://todogroup.org/opencodeofconduct) established by the [TODO Group](http://todogroup.org/) and used by numerous other large communities and the Scope section from the [Contributor Covenant version 1.4](http://contributor-covenant.org/version/1/4/).*\n", "metadata": {"source": "github.com/recommenders-team/recommenders", "file_path": "CODE_OF_CONDUCT.md", "language": "markdown", "complexity": "simple", "star_count": 20514, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 43, "word_count": 609}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Anyone interested in transfer learning is welcomed to contribute to this repo (by pull request):\n\n- You can add the latest publications / tools / tutorials directly to `readme.md` and `awesome_paper.md`.\n- You can add **code** to the code directory. You are welcomed to place the code of your published paper in this repo!\n- You are welcomed to update anything helpful.\n\nÂ¶ÇÊûú‰Ω†ÂØπÊú¨È°πÁõÆÊÑüÂÖ¥Ë∂£ÔºåÈùûÂ∏∏Ê¨¢Ëøé‰Ω†Âä†ÂÖ•ÔºÅ\n\n- ÂèØ‰ª•Êé®ËçêÊúÄÊñ∞ÁöÑÁõ∏ÂÖ≥ËÆ∫Êñá/Â∑•ÂÖ∑/ËÆ≤Â∫ßÔºåÂ∞Ü‰ø°ÊÅØÈÄöËøápull requestÁöÑÊñπÂºèÊõ¥Êñ∞Âà∞`readme.md`Âíå`awesome_paper.md`‰∏≠„ÄÇ\n- Êé®Ëçê‰ª£Á†ÅÂà∞**code**Êñá‰ª∂Â§π‰∏≠„ÄÇÊ¨¢ËøéÂ∞Ü‰Ω†ËÆ∫Êñá‰∏≠ÁöÑ‰ª£Á†ÅÂºÄÊ∫êÂà∞Êú¨È°πÁõÆ‰∏≠ÔºÅ\n- ‰ªª‰ΩïÊúâÁî®ÁöÑÂª∫ËÆÆÈÉΩÂèØËøõË°åË¥°ÁåÆ„ÄÇ\n\nÊ¨¢Ëøé!", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "medium", "star_count": 14009, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 13, "word_count": 71}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "[![Contributors][contributors-shield]][contributors-url]\n[![Forks][forks-shield]][forks-url]\n[![Stargazers][stars-shield]][stars-url]\n[![Issues][issues-shield]][issues-url]\n\n<h1 align=\"center\">\n  <br>\n  <img src=\"png/logo.jpg\" alt=\"Transfer Leanring\" width=\"500\">\n</h1>\n\n<h4 align=\"center\">Everything about Transfer Learning. ËøÅÁßªÂ≠¶‰π†.</h4>\n\n<p align=\"center\">\n  <strong><a href=\"#0papers-ËÆ∫Êñá\">Papers</a></strong> ‚Ä¢\n  <strong><a href=\"#1introduction-and-tutorials-ÁÆÄ‰ªã‰∏éÊïôÁ®ã\">Tutorials</a></strong> ‚Ä¢\n  <a href=\"#2transfer-learning-areas-and-papers-Á†îÁ©∂È¢ÜÂüü‰∏éÁõ∏ÂÖ≥ËÆ∫Êñá\">Research areas</a> ‚Ä¢\n  <a href=\"#3theory-and-survey-ÁêÜËÆ∫‰∏éÁªºËø∞\">Theory</a> ‚Ä¢\n  <a href=\"#3theory-and-survey-ÁêÜËÆ∫‰∏éÁªºËø∞\">Survey</a> ‚Ä¢\n  <strong><a href=\"https://github.com/jindongwang/transferlearning/tree/master/code\">Code</a></strong> ‚Ä¢\n  <strong><a href=\"#7datasets-and-benchmarks-Êï∞ÊçÆÈõÜ‰∏éËØÑÊµãÁªìÊûú\">Dataset & benchmark</a></strong>\n</p>\n<p align=\"center\">\n  <a href=\"#6transfer-learning-thesis-Á°ïÂçöÂ£´ËÆ∫Êñá\">Thesis</a> ‚Ä¢\n  <a href=\"#5transfer-learning-scholars-ËëóÂêçÂ≠¶ËÄÖ\">Scholars</a> ‚Ä¢\n  <a href=\"#8transfer-learning-challenges-ËøÅÁßªÂ≠¶‰π†ÊØîËµõ\">Contests</a> ‚Ä¢\n  <a href=\"#journals-and-conferences\">Journal/conference</a> ‚Ä¢\n  <a href=\"#applications-ËøÅÁßªÂ≠¶‰π†Â∫îÁî®\">Applications</a> ‚Ä¢\n  <a href=\"#other-resources-ÂÖ∂‰ªñËµÑÊ∫ê\">Others</a> ‚Ä¢\n  <a href=\"#contributing-Ê¨¢ËøéÂèÇ‰∏éË¥°ÁåÆ\">Contributing</a>\n</p>\n\n**Widely used by top conferences and journals:** \n- Conferences: [[CVPR'22](https://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Zhang_Segmenting_Across_Places_The_Need_for_Fair_Transfer_Learning_With_CVPRW_2022_paper.html)] [[NeurIPS'21](https://proceedings.neurips.cc/paper/2021/file/731b03008e834f92a03085ef47061c4a-Paper.pdf)] [[IJCAI'21](https://arxiv.org/abs/2103.03097)] [[ESEC/FSE'20](https://dl.acm.org/doi/abs/10.1145/3368089.3409696)] [[IJCNN'20](https://ieeexplore.ieee.org/abstract/document/9207556)] [[ACMMM'18](https://dl.acm.org/doi/abs/10.1145/3240508.3240512)] [[ICME'19](https://ieeexplore.ieee.org/abstract/document/8784776/)]\n- Journals: [[IEEE TKDE](https://ieeexplore.ieee.org/abstract/document/9782500/)] [[ACM TIST](https://dl.acm.org/doi/abs/10.1145/3360309)] [[Information sciences](https://www.sciencedirect.com/science/article/pii/S0020025520308458)] [[Neurocomputing](https://www.sciencedirect.com/science/article/pii/S0925231221007025)] [[IEEE Transactions on Cognitive and Developmental Systems](https://ieeexplore.ieee.org/abstract/document/9659817)]\n\n```\n@Misc{transferlearning.xyz,\nhowpublished = {\\url{http://transferlearning.xyz}},   \ntitle = {Everything about Transfer Learning and Domain Adapation},  \nauthor = {Wang, Jindong and others}  \n}  \n```\n\n[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT) [![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg)](https://github.com/996icu/996.ICU/blob/master/LICENSE) [![996.icu](https://img.shields.io/badge/link-996.icu-red.svg)](https://996.icu) \n\nRelated Codes:\n  - Large language model evaluation: [[llm-eval](https://llm-eval.github.io/)]\n  - Large language model enhancement: [[llm-enhance](https://llm-enhance.github.io/)]\n  - Robust machine learning: [[robustlearn: robust machine learning](https://github.com/microsoft/robustlearn)]\n  - Semi-supervised learning: [[USB: unified semi-supervised learning benchmark](https://github.com/microsoft/Semi-supervised-learning)] | [[TorchSSL: a unified SSL library](https://github.com/TorchSSL/TorchSSL)] \n  - LLM benchmark: [[PromptBench: adversarial robustness of prompts of LLMs](https://github.com/microsoft/promptbench)]\n  - Federated learning: [[PersonalizedFL: library for personalized federated learning](https://github.com/microsoft/PersonalizedFL)]\n  - Activity recognition and machine learning [[Activity recognition](https://github.com/jindongwang/activityrecognition)]ÔΩú[[Machine learning](https://github.com/jindongwang/MachineLearning)]\n\n- - -\n\n**NOTE:** You can directly open the code in [Gihub Codespaces](https://docs.github.com/en/codespaces/getting-started/quickstart#introduction) on the web to run them without downloading! Also, try [github.dev](https://github.dev/jindongwang/transferlearning).\n\n## 0.Papers (ËÆ∫Êñá)\n\n[Awesome transfer learning papers (ËøÅÁßªÂ≠¶‰π†ÊñáÁ´†Ê±áÊÄª)](https://github.com/jindongwang/transferlearning/tree/master/doc/awesome_paper.md)\n\n- [Paperweekly](http://www.paperweekly.site/collections/231/papers): A website to recommend and read paper notes\n\n**Latest papers**: \n\n- By topic: [doc/awesome_papers.md](/doc/awesome_paper.md)\n- By date: [doc/awesome_paper_date.md](/doc/awesome_paper_date.md)\n\n*Updated at 2024-02-18:*\n\n- Simulations of Common Unsupervised Domain Adaptation Algorithms for Image Classification [[arxiv](https://arxiv.org/abs/2502.10694)]\n  - Unsupervised domain adaptaiton for image classification\n\n- Semantics-aware Test-time Adaptation for 3D Human Pose Estimation [[arxiv](https://arxiv.org/abs/2502.10724)]\n  - Test-time adaptation for3D human pose estimation\n\n- Transfer Learning of CATE with Kernel Ridge Regression [[arxiv](https://arxiv.org/abs/2502.11331)]\n  - Transfer learning with kernel ridge regression\n\n- Why Domain Generalization Fail? A View of Necessity and Sufficiency [[arxiv](https://arxiv.org/abs/2502.10716)] \n  - Analyze why domain generalization fail from the view of necessity and sufficiency\n\n\n*Updated at 2024-02-11:*\n\n- Beyond Batch Learning: Global Awareness Enhanced Domain Adaptation [[arxiv](https://arxiv.org/abs/2502.06272)]\n  - Global awareness for enhanced domain adaptation\n\n- - -\n\n## 1.Introduction and Tutorials (ÁÆÄ‰ªã‰∏éÊïôÁ®ã)\n\nWant to quickly learn transfer learningÔºüÊÉ≥Â∞ΩÂø´ÂÖ•Èó®ËøÅÁßªÂ≠¶‰π†ÔºüÁúã‰∏ãÈù¢ÁöÑÊïôÁ®ã„ÄÇ\n\n- Books ‰π¶Á±ç\n  - **Introduction to Transfer Learning: Algorithms and Practice** [[Buy or read](https://link.springer.com/book/9789811975837)]\n  - **„ÄäËøÅÁßªÂ≠¶‰π†„ÄãÔºàÊù®Âº∫Ôºâ** [[Buy](https://item.jd.com/12930984.html)] [[English version](https://www.cambridge.org/core/books/transfer-learning/CCFFAFE3CDBC245047F1DEC71D9EF3C7)]\n  - **„ÄäËøÅÁßªÂ≠¶‰π†ÂØºËÆ∫„Äã(ÁéãÊôã‰∏ú„ÄÅÈôàÁõäÂº∫Ëëó)** [[Homepage](http://jd92.wang/tlbook)] [[Buy](https://item.jd.com/13272157.html)]\n\n- Blogs ÂçöÂÆ¢\n  - [Zhihu blogs - Áü•‰πé‰∏ìÊ†è„ÄäÂ∞èÁéãÁà±ËøÅÁßª„ÄãÁ≥ªÂàóÊñáÁ´†](https://zhuanlan.zhihu.com/p/130244395)\n\t\n- Video tutorials ËßÜÈ¢ëÊïôÁ®ã\n  - Transfer learning ËøÅÁßªÂ≠¶‰π†:\n    - [Recent advance of transfer learning - 2022Âπ¥ÊúÄÊñ∞ËøÅÁßªÂ≠¶‰π†ÂèëÂ±ïÁé∞Áä∂Êé¢ËÆ®](https://www.bilibili.com/video/BV1nY411E7Uc/)\n    - [Definitions of transfer learning area - ËøÅÁßªÂ≠¶‰π†È¢ÜÂüüÂêçËØçËß£Èáä](https://www.bilibili.com/video/BV1fu411o7BW) [[Article](https://zhuanlan.zhihu.com/p/428097044)]\n    - [Transfer learning by Hung-yi Lee @ NTU - Âè∞ÊπæÂ§ßÂ≠¶ÊùéÂÆèÊØÖÁöÑËßÜÈ¢ëËÆ≤Ëß£(‰∏≠ÊñáËßÜÈ¢ë)](https://www.youtube.com/watch?v=qD6iD4TFsdQ)\n  - Domain generalization È¢ÜÂüüÊ≥õÂåñÔºö\n    - [IJCAI-ECAI'22 tutorial on domain generalization - È¢ÜÂüüÊ≥õÂåñtutorial](https://dgresearch.github.io/)\n    - [Domain generalization - ËøÅÁßªÂ≠¶‰π†Êñ∞ÂÖ¥Á†îÁ©∂ÊñπÂêëÈ¢ÜÂüüÊ≥õÂåñ](https://www.bilibili.com/video/BV1ro4y1S7dd/)\n  - Domain adaptation È¢ÜÂüüËá™ÈÄÇÂ∫îÔºö\n    - [Domain adaptation - ËøÅÁßªÂ≠¶‰π†‰∏≠ÁöÑÈ¢ÜÂüüËá™ÈÄÇÂ∫îÊñπÊ≥ï(‰∏≠Êñá)](https://www.bilibili.com/video/BV1T7411R75a/) \n  \n\n- Brief introduction and slides ÁÆÄ‰ªã‰∏épptËµÑÊñô\n  - [Recent advance of transfer learning](https://jd92.wang/assets/files/l16_aitime.pdf)\n  - [Domain generalization survey](http://jd92.wang/assets/files/DGSurvey-ppt.pdf)\n  - [Brief introduction in Chinese](https://github.com/jindongwang/transferlearning/blob/master/doc/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B.md)\n\t- [PPT (English)](http://jd92.wang/assets/files/l03_transferlearning.pdf) | [PPT (‰∏≠Êñá)](http://jd92.wang/assets/files/l08_tl_zh.pdf)\n  - ËøÅÁßªÂ≠¶‰π†‰∏≠ÁöÑÈ¢ÜÂüüËá™ÈÄÇÂ∫îÊñπÊ≥ï Domain adaptation: [PDF](http://jd92.wang/assets/files/l12_da.pdf) ÔΩú [Video on Bilibili](https://www.bilibili.com/video/BV1T7411R75a/) | [Video on Youtube](https://www.youtube.com/watch?v=RbIsHNtluwQ&t=22s)\n  - Tutorial on transfer learning by Qiang Yang: [IJCAI'13](http://ijcai13.org/files/tutorial_slides/td2.pdf) | [2016 version](http://kddchina.org/file/IntroTL2016.pdf)\n\n- Talk is cheap, show me the code Âä®ÊâãÊïôÁ®ã„ÄÅ‰ª£Á†Å„ÄÅÊï∞ÊçÆ \n  - [Pytorch tutorial on transfer learning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n\t- [Pytorch finetune](https://github.com/jindongwang/transferlearning/tree/master/code/AlexNet_ResNet)\n\t- [DeepDA: a unified deep domain adaptation toolbox](https://github.com/jindongwang/transferlearning/tree/master/code/DeepDA)\n\t- [DeepDG: a unified deep domain generalization toolbox](https://github.com/jindongwang/transferlearning/tree/master/code/DeepDG)\n\t- [Êõ¥Â§ö More...](https://github.com/jindongwang/transferlearning/tree/master/code)\n\n- [Transfer Learning Scholars and Labs - ËøÅÁßªÂ≠¶‰π†È¢ÜÂüüÁöÑËëóÂêçÂ≠¶ËÄÖ„ÄÅ‰ª£Ë°®Â∑•‰ΩúÂèäÂÆûÈ™åÂÆ§‰ªãÁªç](https://github.com/jindongwang/transferlearning/blob/master/doc/scholar_TL.md)\n- [Negative transfer - Ë¥üËøÅÁßª](https://www.zhihu.com/question/66492194/answer/242870418)\n\n- - -\n\n## 2.Transfer Learning Areas and Papers (Á†îÁ©∂È¢ÜÂüü‰∏éÁõ∏ÂÖ≥ËÆ∫Êñá)\n\n- [Survey](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#survey)\n- [Theory](#theory)\n- [Per-training/Finetuning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#per-trainingfinetuning)\n- [Knowledge distillation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#knowledge-distillation)\n- [Traditional domain adaptation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#traditional-domain-adaptation)\n- [Deep domain adaptation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#deep-domain-adaptation)\n- [Domain generalization](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#domain-generalization)\n- [Source-free domain adaptation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#source-free-domain-adaptation)\n- [Multi-source domain adaptation](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#multi-source-domain-adaptation)\n- [Heterogeneous transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#heterogeneous-transfer-learning)\n- [Online transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#online-transfer-learning)\n- [Zero-shot / few-shot learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#zero-shot--few-shot-learning)\n- [Multi-task learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#multi-task-learning)\n- [Transfer reinforcement learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#transfer-reinforcement-learning)\n- [Transfer metric learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#transfer-metric-learning)\n- [Federated transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#federated-transfer-learning)\n- [Lifelong transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#lifelong-transfer-learning)\n- [Safe transfer learning](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#safe-transfer-learning)\n- [Transfer learning applications](https://github.com/jindongwang/transferlearning/blob/master/doc/awesome_paper.md#transfer-learning-applications)\n\n- - -\n\n## 3.Theory and Survey (ÁêÜËÆ∫‰∏éÁªºËø∞)\n\nHere are some articles on transfer learning theory and survey.\n\n**Survey (ÁªºËø∞ÊñáÁ´†)Ôºö**\n\n- 2023 Source-Free Unsupervised Domain Adaptation: A Survey [[arxiv](http://arxiv.org/abs/2301.00265)]\n- 2022 [Transfer Learning for Future Wireless Networks: A Comprehensive Survey](https://arxiv.org/abs/2102.07572)\n- 2022 [A Review of Deep Transfer Learning and Recent Advancements](https://arxiv.org/abs/2201.09679)\n- 2022 [Transferability in Deep Learning: A Survey](https://paperswithcode.com/paper/transferability-in-deep-learning-a-survey), from Mingsheng Long in THU.\n- 2021 Domain generalization: IJCAI-21 [Generalizing to Unseen Domains: A Survey on Domain Generalization](https://arxiv.org/abs/2103.03097) | [Áü•‰πéÊñáÁ´†](https://zhuanlan.zhihu.com/p/354740610) | [ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑](https://mp.weixin.qq.com/s/DsoVDYqLB1N7gj9X5UnYqw)\n  - First survey on domain generalization\n  - Á¨¨‰∏ÄÁØáÂØπDomain generalization (È¢ÜÂüüÊ≥õÂåñ)ÁöÑÁªºËø∞\n- 2021 Vision-based activity recognition: [A Survey of Vision-Based Transfer Learning in Human Activity Recognition](https://www.mdpi.com/2079-9292/10/19/2412)\n- 2021 ICSAI [A State-of-the-Art Survey of Transfer Learning in Structural Health Monitoring](https://ieeexplore.ieee.org/abstract/document/9664171)\n- 2020 [Transfer learning: survey and classification](https://link.springer.com/chapter/10.1007/978-981-15-5345-5_13), Advances in Intelligent Systems and Computing. \n- 2020 ËøÅÁßªÂ≠¶‰π†ÊúÄÊñ∞surveyÔºåÊù•Ëá™‰∏≠ÁßëÈô¢ËÆ°ÁÆóÊâÄÂ∫ÑÁ¶èÊåØÂõ¢ÈòüÔºåÂèëË°®Âú®Proceedings of the IEEE: [A Comprehensive Survey on Transfer Learning](https://arxiv.org/abs/1911.02685)\n- 2020 Ë¥üËøÅÁßªÁöÑÁªºËø∞Ôºö[Overcoming Negative Transfer: A Survey](https://arxiv.org/abs/2009.00909)\n- 2020 Áü•ËØÜËí∏È¶èÁöÑÁªºËø∞: [Knowledge Distillation: A Survey](https://arxiv.org/abs/2006.05525)\n- Áî®transfer learningËøõË°åsentiment classificationÁöÑÁªºËø∞Ôºö[A Survey of Sentiment Analysis Based on Transfer Learning](https://ieeexplore.ieee.org/abstract/document/8746210) \n- 2019 ‰∏ÄÁØáÊñ∞surveyÔºö[Transfer Adaptation Learning: A Decade Survey](https://arxiv.org/abs/1903.04687)\n- 2018 ‰∏ÄÁØáËøÅÁßªÂ∫¶ÈáèÂ≠¶‰π†ÁöÑÁªºËø∞: [Transfer Metric Learning: Algorithms, Applications and Outlooks](https://arxiv.org/abs/1810.03944)\n- 2018 ‰∏ÄÁØáÊúÄËøëÁöÑÈùûÂØπÁß∞ÊÉÖÂÜµ‰∏ãÁöÑÂºÇÊûÑËøÅÁßªÂ≠¶‰π†ÁªºËø∞Ôºö[Asymmetric Heterogeneous Transfer Learning: A Survey](https://arxiv.org/abs/1804.10834)\n- 2018 Neural style transferÁöÑ‰∏Ä‰∏™surveyÔºö[Neural Style Transfer: A Review](https://arxiv.org/abs/1705.04058)\n- 2018 Ê∑±Â∫¶domain adaptationÁöÑ‰∏Ä‰∏™ÁªºËø∞Ôºö[Deep Visual Domain Adaptation: A Survey](https://www.sciencedirect.com/science/article/pii/S0925231218306684)\n- 2017 Â§ö‰ªªÂä°Â≠¶‰π†ÁöÑÁªºËø∞ÔºåÊù•Ëá™È¶ôÊ∏ØÁßëÊäÄÂ§ßÂ≠¶Êù®Âº∫Âõ¢ÈòüÔºö[A survey on multi-task learning](https://arxiv.org/abs/1707.08114)\n- 2017 ÂºÇÊûÑËøÅÁßªÂ≠¶‰π†ÁöÑÁªºËø∞Ôºö[A survey on heterogeneous transfer learning](https://link.springer.com/article/10.1186/s40537-017-0089-0)\n- 2017 Ë∑®È¢ÜÂüüÊï∞ÊçÆËØÜÂà´ÁöÑÁªºËø∞Ôºö[Cross-dataset recognition: a survey](https://arxiv.org/abs/1705.04396)\n- 2016 [A survey of transfer learning](https://pan.baidu.com/s/1gfgXLXT)„ÄÇÂÖ∂‰∏≠‰∫§‰ª£‰∫Ü‰∏Ä‰∫õÊØîËæÉÁªèÂÖ∏ÁöÑÂ¶ÇÂêåÊûÑ„ÄÅÂºÇÊûÑÁ≠âÂ≠¶‰π†ÊñπÊ≥ï‰ª£Ë°®ÊÄßÊñáÁ´†„ÄÇ\n- 2015 ‰∏≠ÊñáÁªºËø∞Ôºö[ËøÅÁßªÂ≠¶‰π†Á†îÁ©∂ËøõÂ±ï](https://pan.baidu.com/s/1bpautob)\n- 2010 [A survey on transfer learning](http://ieeexplore.ieee.org/abstract/document/5288526/)\n- Survey on applications - Â∫îÁî®ÂØºÂêëÁöÑÁªºËø∞Ôºö\n\t- ËßÜËßâdomain adaptationÁªºËø∞Ôºö[Visual Domain Adaptation: A Survey of Recent Advances](https://pan.baidu.com/s/1o8BR7Vc)\n\t- ËøÅÁßªÂ≠¶‰π†Â∫îÁî®‰∫éË°å‰∏∫ËØÜÂà´ÁªºËø∞Ôºö[Transfer Learning for Activity Recognition: A Survey](https://pan.baidu.com/s/1kVABOYr)\n\t- ËøÅÁßªÂ≠¶‰π†‰∏éÂ¢ûÂº∫Â≠¶‰π†Ôºö[Transfer Learning for Reinforcement Learning Domains: A Survey](https://pan.baidu.com/s/1slfr0w1)\n\t- Â§ö‰∏™Ê∫êÂüüËøõË°åËøÅÁßªÁöÑÁªºËø∞Ôºö[A Survey of Multi-source Domain Adaptation](https://pan.baidu.com/s/1eSGREF4)„ÄÇ\n\n**Theory ÔºàÁêÜËÆ∫ÊñáÁ´†Ôºâ:**\n\n- ICML-20 [Few-shot domain adaptation by causal mechanism transfer](https://arxiv.org/pdf/2002.03497.pdf)\n\t- The first work on causal transfer learning\n\t- Êó•Êú¨ÁêÜËÆ∫ÁªÑÂ§ß‰Ω¨SugiyamaÁöÑÂ∑•‰ΩúÔºåcausal transfer learning\n- CVPR-19 [Characterizing and Avoiding Negative Transfer](https://arxiv.org/abs/1811.09751)\n\t- Characterizing and avoid negative transfer\n\t- ÂΩ¢ÂºèÂåñÂπ∂ÊèêÂá∫Â¶Ç‰ΩïÈÅøÂÖçË¥üËøÅÁßª\n- ICML-20 [On Learning Language-Invariant Representations for Universal Machine Translation](https://arxiv.org/abs/2008.04510)\n  - Theory for universal machine translation\n  - ÂØπÁªü‰∏ÄÊú∫Âô®ÁøªËØëÊ®°ÂûãËøõË°å‰∫ÜÁêÜËÆ∫ËÆ∫ËØÅ\n- NIPS-06 [Analysis of Representations for Domain Adaptation](https://dl.acm.org/citation.cfm?id=2976474)\n- ML-10 [A Theory of Learning from Different Domains](https://link.springer.com/article/10.1007/s10994-009-5152-4)\n- NIPS-08 [Learning Bounds for Domain Adaptation](http://papers.nips.cc/paper/3212-learning-bounds-for-domain-adaptation)\n- COLT-09 [Domain adaptation: Learning bounds and algorithms](https://arxiv.org/abs/0902.3430)\n- MMD paperÔºö[A Hilbert Space Embedding for Distributions](https://link.springer.com/chapter/10.1007/978-3-540-75225-7_5) and [A Kernel Two-Sample Test](http://www.jmlr.org/papers/v13/gretton12a.html)\n- Multi-kernel MMD paper: [Optimal kernel choice for large-scale two-sample tests](http://papers.nips.cc/paper/4727-optimal-kernel-choice-for-large-scale-two-sample-tests)\n\n_ _ _\n\n## 4.Code (‰ª£Á†Å)\n\nUnified codebases for:\n- [Deep domain adaptation](https://github.com/jindongwang/transferlearning/tree/master/code/DeepDA)\n- [Deep domain generalization](https://github.com/jindongwang/transferlearning/tree/master/code/DeepDG)\n- See all codes here: https://github.com/jindongwang/transferlearning/tree/master/code.\n\nMore: see [HERE](https://github.com/jindongwang/transferlearning/tree/master/code) and [HERE](https://colab.research.google.com/drive/1MVuk95mMg4ecGyUAIG94vedF81HtWQAr?usp=sharing) for an instant run using Google's Colab.\n\n_ _ _\n\n## 5.Transfer Learning Scholars (ËëóÂêçÂ≠¶ËÄÖ)\n\nHere are some transfer learning scholars and labs.\n\n**ÂÖ®ÈÉ®ÂàóË°®‰ª•Âèä‰ª£Ë°®Â∑•‰ΩúÊÄßËßÅ[ËøôÈáå](https://github.com/jindongwang/transferlearning/blob/master/doc/scholar_TL.md)** \n\nPlease note that this list is far not complete. A full list can be seen in [here](https://github.com/jindongwang/transferlearning/blob/master/doc/scholar_TL.md). Transfer learning is an active field. *If you are aware of some scholars, please add them here.*\n\n_ _ _\n\n## 6.Transfer Learning Thesis (Á°ïÂçöÂ£´ËÆ∫Êñá)\n\nHere are some popular thesis on transfer learning.\n\n[ËøôÈáå](https://pan.baidu.com/share/init?surl=iuzZhHdumrD64-yx_VAybA), ÊèêÂèñÁ†ÅÔºötxyz„ÄÇ\n\n- - -\n\n## 7.Datasets and Benchmarks (Êï∞ÊçÆÈõÜ‰∏éËØÑÊµãÁªìÊûú)\n\nPlease see [HERE](https://github.com/jindongwang/transferlearning/blob/master/data) for the popular transfer learning **datasets and benchmark** results.\n\n[ËøôÈáå](https://github.com/jindongwang/transferlearning/blob/master/data)Êï¥ÁêÜ‰∫ÜÂ∏∏Áî®ÁöÑÂÖ¨ÂºÄÊï∞ÊçÆÈõÜÂíå‰∏Ä‰∫õÂ∑≤ÂèëË°®ÁöÑÊñáÁ´†Âú®Ëøô‰∫õÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûú„ÄÇ\n\n- - -\n\n## 8.Transfer Learning Challenges (ËøÅÁßªÂ≠¶‰π†ÊØîËµõ)\n\n- [Visual Domain Adaptation Challenge (VisDA)](http://ai.bu.edu/visda-2018/)\n\n- - -\n\n## Journals and Conferences\n\nSee [here](https://github.com/jindongwang/transferlearning/blob/master/doc/venues.md) for a full list of related journals and conferences.\n\n- - -\n\n## Applications (ËøÅÁßªÂ≠¶‰π†Â∫îÁî®)\n\n- [Computer vision](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#computer-vision)\n- [Medical and healthcare](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#medical-and-healthcare)\n- [Natural language processing](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#natural-language-processing)\n- [Time series](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#time-series)\n- [Speech](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#speech)\n- [Multimedia](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#multimedia)\n- [Recommendation](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#recommendation)\n- [Human activity recognition](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#human-activity-recognition)\n- [Autonomous driving](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#autonomous-driving)\n- [Others](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md#others)\n\nSee [HERE](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md) for transfer learning applications.\n\nËøÅÁßªÂ≠¶‰π†Â∫îÁî®ËØ∑ËßÅ[ËøôÈáå](https://github.com/jindongwang/transferlearning/blob/master/doc/transfer_learning_application.md)„ÄÇ\n\n- - -\n\n## Other Resources (ÂÖ∂‰ªñËµÑÊ∫ê)\n\n- Call for papers:\n  - [Advances in Transfer Learning: Theory, Algorithms, and Applications](https://www.frontiersin.org/research-topics/21133/advances-in-transfer-learning-theory-algorithms-and-applications), DDL: October 2021\n\n- Related projects:\n  - Salad: [A semi-supervised domain adaptation library](https://domainadaptation.org)\n\n- - -\n\n## Contributing (Ê¨¢ËøéÂèÇ‰∏éË¥°ÁåÆ)\n\nIf you are interested in contributing, please refer to [HERE](https://github.com/jindongwang/transferlearning/blob/master/CONTRIBUTING.md) for instructions in contribution.\n\n- - -\n\n### Copyright notice\n\n> ***[Notes]This Github repo can be used by following the corresponding licenses. I want to emphasis that it may contain some PDFs or thesis, which were downloaded by me and can only be used for academic purposes. The copyrights of these materials are owned by corresponding publishers or organizations. All this are for better academic research. If any of the authors or publishers have concerns, please contact me to delete or replace them.***\n\n[contributors-shield]: https://img.shields.io/github/contributors/jindongwang/transferlearning.svg?style=for-the-badge\n[contributors-url]: https://github.com/jindongwang/transferlearning/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/jindongwang/transferlearning.svg?style=for-the-badge\n[forks-url]: https://github.com/jindongwang/transferlearning/network/members\n[stars-shield]: https://img.shields.io/github/stars/jindongwang/transferlearning.svg?style=for-the-badge\n[stars-url]: https://github.com/jindongwang/transferlearning/stargazers\n[issues-shield]: https://img.shields.io/github/issues/jindongwang/transferlearning.svg?style=for-the-badge\n[issues-url]: https://github.com/jindongwang/transferlearning/issues\n[license-shield]: https://img.shields.io/github/license/jindongwang/transferlearning.svg?style=for-the-badge\n[license-url]: https://github.com/jindongwang/transferlearning/blob/main/LICENSE.txt\n", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 14009, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 315, "word_count": 1386}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with installation and setup instructions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Adapter-based Cross-lingual ASR with EasyEspnet\r\n\r\n**NOTICE:** The latest code of Adapter has been moved to https://github.com/microsoft/NeuralSpeech/tree/master/AdapterASR for unified organization. Please refer to that repo.\r\n\r\n---\r\n\r\nThis is a Adapter-based cross-lingual ASR implementation of our works [1, 2] built on top of [EasyEspnet](https://github.com/jindongwang/EasyEspnet). Please refer to it for the basic introduction, installation and usage.\r\n\r\n## Run\r\n\r\nAfter extracting features using Espnet and set the data folder path as introduced in [EasyEspnet](https://github.com/jindongwang/EasyEspnet).\r\n\r\nYou need to check or modify in `train.py arg_list`, config should be in ESPnet config style (remember to include decoding information if you want to compute cer/wer), then, you can run train.py. For example, \r\n\r\n```\r\npython train.py --root_path commonvoice/asr1 --dataset ar --config config/adapter_example.yaml\r\n```\r\n\r\ndataset here refers to the language code used in the Common Voice corpus. Results (log, model, snapshots) are saved in results_(dataset)/(config_name) by default.\r\n\r\n### Adapters\r\n\r\nAdapters is a parameter-efficient way for cross-lingual ASR as introduced in [1, 2]. Given a pre-trained multilingual ASR model, the adapters are injected into the model, during adaptation, only the adapters and language-specific heads are updated, while the main body of the model is frozen, resulting in much faster training speed and making the adaptation more stable. Please refer to our SimAdapter paper [2] for details.\r\n\r\nTo train the adapters, generally you need to specify the `load_pretrained_model` in the config files to load the multilingual ASR model. We found that by splitting the training of language-specific heads and adapters, the adaptation performance can be further improved [2]. You may also train the language-specific heads first and set `train_adapter_with_head` to `false` during adapters' training.\r\n\r\nFor the vanilla Adapter method, we provide an example config in `config/adapter_example.yaml`, please refer to it for modification to train your own adapter.\r\n\r\nFor the Meta-Adapter, there are two stages, the first step is pre-training, we provide an example config in `config/meta_adapter_example.yaml`, please refer to it for modification to train your own meta-adapter; the second stage is fine-tuning, please refer to the `config/finetune_meta_adapter_example.yaml`. Please refer to our meta-adapter paper [1] for details.\r\n\r\n### SimAdapter\r\n\r\nTo leverage the information from multiple source / target adapters, we introduce SimAdapter for cross-lingual ASR [2]. To train a SimAdapter, you need a model with multiple adapters/meta-adapters injected and pre-trained. During SimAdapter, we initialize attention layers after every adapter outputs to fuse the information from multiple adapters (languages). During training of SimAdapter, parameters of the main body, the adapters, and the language heads are all frozen. Only the SimAdapter's fusion layers are trained. Please refer to our SimAdapter paper [2] for details.\r\n\r\nWe also provide a template config for you to perform SimAdapter: `config/simadapter_example.yaml`\r\n\r\n### Demo\r\n\r\nTBD\r\n\r\n## Decoding and WER/CER evaluation\r\n\r\nSet `--decoding_mode` to `true` to perform decoding and CER/WER evaluation. For example:\r\n\r\n```\r\npython train.py --root_path commonvoice/asr1 --dataset ar --decoding_mode true --config config/adapter_example.yaml\r\n```\r\n\r\n## Distributed training\r\n\r\nFollowing EasyEspnet, you can also perform distributed training which is much faster. For example, using 4 GPUs, 1 node: \r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 train.py --dist_train true --root_path commonvoice/asr1 --dataset ar --config config/adapter_example.yaml\r\n```\r\n\r\n## Acknowledgement\r\n\r\n- ESPNet: https://github.com/espnet/espnet\r\n- EasyEspnet: https://github.com/jindongwang/EasyEspnet\r\n\r\n## Contact\r\n\r\n- [Wenxin Hou](https://houwenxin.github.io/): houwx001@gmail.com\r\n- [Jindong Wang](http://www.jd92.wang/): jindongwang@outlook.com\r\n\r\n\r\n\r\n## References\r\n\r\n[1] Wenxin Hou, Yidong Wang, Shengzhou Gao, Takahiro Shinozaki, ‚ÄúMeta-Adapter: Efficient Cross-Lingual Adaptation with Meta-Learning‚Äù, in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Toronto, Ontario, Canada, June 2021. [[paper]](https://ieeexplore.ieee.org/document/9414959)\r\n\r\n[2] Wenxin Hou, Han Zhu, Yidong Wang, Jindong Wang, Tao Qin, Renjun Xu, Takahiro Shinozaki, ‚ÄùExploiting Adapters for Cross-lingual Low-resource Speech Recognition‚Äù, Arxiv preprint. [[paper]](https://arxiv.org/abs/2105.11905)", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "code/ASR/Adapter/README.md", "language": "markdown", "complexity": "medium", "star_count": 14009, "quality_score": 0.8600000000000001, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 73, "word_count": 580}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# network architecture\n# encoder related\nelayers: 12\neunits: 2048\n# decoder related\ndlayers: 6\ndunits: 2048\n# attention related\nadim: 256\naheads: 4\n\n# hybrid CTC/attention\nmtlalpha: 0.3\n\n# label smoothing\nlsm-weight: 0.1\n\n# minibatch related\nbatch-size: 32\nmaxlen-in: 512  # if input length  > maxlen-in, batchsize is automatically reduced\nmaxlen-out: 150 # if output length > maxlen-out, batchsize is automatically reduced\n\n# optimization related\nsortagrad: 0 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs\n# opt: noam\naccum-grad: 1\ngrad-clip: 5\npatience: 10\nepochs: 100\ndropout-rate: 0.1\n\n# transformer specific setting\nbackend: pytorch\nmodel-module: \"espnet.nets.pytorch_backend.e2e_asr_transformer:E2E\"\ntransformer-input-layer: conv2d     # encoder architecture type\ntransformer-lr: 6.0\ntransformer-warmup-steps: 1800\ntransformer-attn-dropout-rate: 0.0\ntransformer-length-normalized-loss: false\ntransformer-init: pytorch\n\n\n# Decoding related\nbeam-size: 10\npenalty: 0.0\nmaxlenratio: 0.0\nminlenratio: 0.0\nctc-weight: 0.3\nlm-weight: 0.7\n\nuse_adapters: true\nload_pretrained_model: \"<path-to-your-pretrained-model>:<modules-to-load>:<excluded-modules>\" # Example: lid_42.pt::decoder.embed,decoder.output_layer,ctc.\ndataset: \"ru_cy_it_eu_pt_ar\" # <languages for which adapters are initialized>, we recommend to write all the dataset that you want to perform AdapterFusion later (if you want)\nadapter_train_languages: \"ar\" # The language(s) that is really used for this time\nadapter_fusion: false\ntrain_adapter_with_head: true\n\nopt: adam\nadam_lr: 0.001\n", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "code/ASR/Adapter/config/adapter_example.yaml", "language": "yaml", "complexity": "medium", "star_count": 14009, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 60, "word_count": 193}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# network architecture\n# encoder related\nelayers: 12\neunits: 2048\n# decoder related\ndlayers: 6\ndunits: 2048\n# attention related\nadim: 256\naheads: 4\n\n# hybrid CTC/attention\nmtlalpha: 0.3\n\n# label smoothing\nlsm-weight: 0.1\n\n# minibatch related\nbatch-size: 32\nmaxlen-in: 512  # if input length  > maxlen-in, batchsize is automatically reduced\nmaxlen-out: 150 # if output length > maxlen-out, batchsize is automatically reduced\n\n# optimization related\nsortagrad: 0 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs\naccum-grad: 1\ngrad-clip: 5\npatience: 10\nepochs: 200\ndropout-rate: 0.1\n\n# transformer specific setting\nbackend: pytorch\nmodel-module: \"espnet.nets.pytorch_backend.e2e_asr_transformer:E2E\"\ntransformer-input-layer: conv2d     # encoder architecture type\ntransformer-lr: 0.1\ntransformer-warmup-steps: 1800\ntransformer-attn-dropout-rate: 0.0\ntransformer-length-normalized-loss: false\ntransformer-init: pytorch\n\n\n# Decoding related\nbeam-size: 10\npenalty: 0.0\nmaxlenratio: 0.0\nminlenratio: 0.0\nctc-weight: 0.3\nlm-weight: 0.7\n\nuse_adapters: true\nload_pretrained_model: \"<path-to-model-with-adapters>:<modules-to-load>:<excluded-modules>\"  # Example: lid_42_adapters.pt::\ndataset: \"ru_cy_it_eu_pt_ar\"\nfusion_languages: \"ru_cy_it_eu_pt_ar\" # adapters to fuse\nadapter_train_languages: \"ar\"\nsim_adapter: true\ntrain_adapter_with_head: false\n\nguide_loss_weight: 1.0\nguide_loss_weight_decay_steps: 0\nopt: adam\nadam_lr: 0.001", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "code/ASR/Adapter/config/adapterfusion_example.yaml", "language": "yaml", "complexity": "complex", "star_count": 14009, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 61, "word_count": 166}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# network architecture\n# encoder related\nelayers: 12\neunits: 2048\n# decoder related\ndlayers: 6\ndunits: 2048\n# attention related\nadim: 256\naheads: 4\n\n# hybrid CTC/attention\nmtlalpha: 0.3\n\n# label smoothing\nlsm-weight: 0.1\n\n# minibatch related\nbatch-size: 32\nmaxlen-in: 512  # if input length  > maxlen-in, batchsize is automatically reduced\nmaxlen-out: 150 # if output length > maxlen-out, batchsize is automatically reduced\n\n# optimization related\nsortagrad: 0 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs\nopt: noam\naccum-grad: 1\ngrad-clip: 5\npatience: 10\nepochs: 100\ndropout-rate: 0.1\n\n# transformer specific setting\nbackend: pytorch\nmodel-module: \"espnet.nets.pytorch_backend.e2e_asr_transformer:E2E\"\ntransformer-input-layer: conv2d     # encoder architecture type\ntransformer-lr: 6.0\ntransformer-warmup-steps: 1800\ntransformer-attn-dropout-rate: 0.0\ntransformer-length-normalized-loss: false\ntransformer-init: pytorch\n\n\n# Decoding related\nbeam-size: 10\npenalty: 0.0\nmaxlenratio: 0.0\nminlenratio: 0.0\nctc-weight: 0.3\nlm-weight: 0.7\n\nuse_adapters: true\nload_pretrained_model: \"<path-to-model-with-adapters>:<modules-to-load>:<excluded-modules>\"  # Example: lid_42_meta_adapter.pt::\nadapter_train_languages: \"ar\"\nshared_adapter: \"ru_cy_it_eu_pt\" # Set to the source languages on which it is meta-trained\ndataset: ru_cy_it_eu_pt_ar\nadapter_fusion: false\ntrain_adapter_with_head: true\n# cs\nopt: adam\nadam_lr: 0.01", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "code/ASR/Adapter/config/finetune_meta_adapter_example.yaml", "language": "yaml", "complexity": "medium", "star_count": 14009, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 60, "word_count": 173}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# network architecture\n# encoder related\nelayers: 12\neunits: 2048\n# decoder related\ndlayers: 6\ndunits: 2048\n# attention related\nadim: 256\naheads: 4\n\n# hybrid CTC/attention\nmtlalpha: 0.3\n\n# label smoothing\nlsm-weight: 0.1\n\n# minibatch related\n\nmaxlen-in: 512  # if input length  > maxlen-in, batchsize is automatically reduced\nmaxlen-out: 150 # if output length > maxlen-out, batchsize is automatically reduced\n\n# optimization related\nsortagrad: 0 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs\naccum-grad: 1\ngrad-clip: 5\npatience: 0\nepochs: 30\ndropout-rate: 0.1\n\n# transformer specific setting\nbackend: pytorch\nmodel-module: \"espnet.nets.pytorch_backend.e2e_asr_transformer:E2E\"\ntransformer-input-layer: conv2d     # encoder architecture type\nopt: adam\ntransformer-lr: 6.0\ntransformer-warmup-steps: 250\ntransformer-attn-dropout-rate: 0.0\ntransformer-length-normalized-loss: false\ntransformer-init: pytorch\n\n\n# Decoding related\nbeam-size: 10\npenalty: 0.0\nmaxlenratio: 0.0\nminlenratio: 0.0\nctc-weight: 0.3\nlm-weight: 0.7\n\nload_pretrained_model: \"<path-to-your-pretrained-model>:<modules-to-load>:<excluded-modules>\" # Example: lid_42.pt::decoder.embed,decoder.output_layer,ctc.\nuse_adapters: true\nadapter_train_languages: \"ru_cy_it_eu_pt\"\ndataset: \"ru_cy_it_eu_pt\"\nmeta_train: true\nshared_adapter: \"ru_cy_it_eu_pt\" # The name of shared_adapter, other name is also okay\nadapter_fusion: false\nbatch-size: 32 # Real batch size is given by this batch-size * number of languages\ntrain_adapter_with_head: true\n\nmeta_lr: 0.0001 # 1.0\nadam_lr: 0.01 # inner learning rate\n# cs", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "code/ASR/Adapter/config/meta_adapter_example.yaml", "language": "yaml", "complexity": "medium", "star_count": 14009, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 63, "word_count": 193}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\"\"\"Transformer speech recognition model (pytorch).\"\"\"\n\nfrom argparse import Namespace\nfrom distutils.util import strtobool\nimport logging\nimport math\n\nimport numpy\nimport torch\n\nfrom espnet.nets.pytorch_backend.e2e_asr_transformer import *\n\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\nfrom espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\n\nfrom espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer\nfrom espnet.nets.pytorch_backend.transformer.decoder_layer import DecoderLayer\n\nfrom espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\nfrom espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (\n    PositionwiseFeedForward,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.transformer.repeat import repeat\nfrom espnet.nets.pytorch_backend.e2e_asr_transformer import E2E as E2ETransformer\nfrom espnet.nets.pytorch_backend.transformer.encoder import Encoder\nfrom espnet.nets.pytorch_backend.transformer.decoder import Decoder\n\nfrom espnet.nets.pytorch_backend.transformer.label_smoothing_loss import (\n    LabelSmoothingLoss,  # noqa: H301\n)\nfrom data_load import low_resource_languages\nlow_resource_adapter_dim = 64\nhigh_resource_adapter_dim = 128\n\nclass SimAdapter(MultiHeadedAttention):\n    def __init__(self, n_feat, dropout_rate, fusion_languages=None, num_shared_layers=-1):\n        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n        super(MultiHeadedAttention, self).__init__()\n        self.linear_q = torch.nn.Linear(n_feat, n_feat)\n        self.linear_k = torch.nn.Linear(n_feat, n_feat)\n        self.linear_v = torch.nn.Linear(n_feat, n_feat, bias=False)\n        self.linear_v.weight.data = (\n            torch.zeros(n_feat, n_feat) + 0.000001\n        ).fill_diagonal_(1.0)\n        self.attn = None\n        self.num_shared_layers = num_shared_layers\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n        self.temperature = 1.0\n        self.fusion_languages = fusion_languages\n\n    def forward_qkv(self, query, key, value):\n        q = self.linear_q(query) # (batch, time, d_k)\n        k = self.linear_k(key) # (batch, time, n_adapters, d_k)\n        v = self.linear_v(value) # (batch, time, n_adapters, d_k)\n        return q, k, v\n\n    def forward_attention(self, value, scores):\n        self.attn = torch.softmax(scores, dim=-1)  # (batch, time, n_adapters)\n        p_attn = self.dropout(self.attn)\n        x = torch.matmul(p_attn.unsqueeze(2), value)\n        # (batch, time, 1, n_adapters), (batch, time, n_adapters, d_k)\n        x = torch.squeeze(x, dim=2)\n        return x  # (batch, time, d_k)\n\n    def forward(self, query, key, value, residual=None):\n        q, k, v = self.forward_qkv(query, key, value)\n        # q: (batch, time, 1, d_k); k, v: (batch, time, n_adapters, d_k)\n        scores = torch.matmul(q.unsqueeze(2), k.transpose(-2, -1)) / math.sqrt(self.temperature)\n        scores = torch.squeeze(scores, dim=2)\n        # scores: (batch, time, n_adapters)\n        out = self.forward_attention(v, scores)\n        if residual is not None:\n            out = out + residual\n        return out\n\nclass Adapter(torch.nn.Module):\n    def __init__(self, adapter_dim, embed_dim):\n        super().__init__()\n        self.layer_norm = LayerNorm(embed_dim)\n        self.down_project = torch.nn.Linear(embed_dim, adapter_dim, bias=False)\n        self.up_project = torch.nn.Linear(adapter_dim, embed_dim, bias=False)\n\n    def forward(self, z):\n        normalized_z = self.layer_norm(z)\n        h = torch.nn.functional.relu(self.down_project(normalized_z))\n        return self.up_project(h) + z\nclass CustomEncoderLayer(EncoderLayer):\n    def forward(self, x, mask, cache=None):\n        residual = x\n        if self.normalize_before:\n            x = self.norm1(x)\n        if cache is None:\n            x_q = x\n        else:\n            assert cache.shape == (x.shape[0], x.shape[1] - 1, self.size)\n            x_q = x[:, -1:, :]\n            residual = residual[:, -1:, :]\n            mask = None if mask is None else mask[:, -1:, :]\n        if self.concat_after:\n            x_concat = torch.cat((x, self.self_attn(x_q, x, x, mask)), dim=-1)\n            x = residual + self.concat_linear(x_concat)\n        else:\n            x = residual + self.dropout(self.self_attn(x_q, x, x, mask))\n        if not self.normalize_before:\n            x = self.norm1(x)\n        residual = x\n        if self.normalize_before:\n            x = self.norm2(x)\n            x_norm = x\n        x = residual + self.dropout(self.feed_forward(x))\n        if not self.normalize_before:\n            x = self.norm2(x)\n        if cache is not None:\n            x = torch.cat([cache, x], dim=1)\n        return x, x_norm, mask\nclass AdaptiveEncoderLayer(CustomEncoderLayer):\n    def __init__(\n        self,\n        languages,\n        size,\n        self_attn,\n        feed_forward,\n        dropout_rate,\n        normalize_before=True,\n        concat_after=False,\n        sim_adapter_layer=None,\n        shared_adapter=None,\n        use_adapters=True,\n    ):\n        super().__init__(size,\n            self_attn,\n            feed_forward,\n            dropout_rate,\n            normalize_before,\n            concat_after,\n        )\n        self.use_adapters = use_adapters\n        if use_adapters:\n            self.adapters = torch.nn.ModuleDict()\n            self.shared_adapter = shared_adapter\n            if shared_adapter:\n                languages = [shared_adapter]\n            for lang in languages:\n                if lang in low_resource_languages or self.shared_adapter:\n                    adapter_dim = low_resource_adapter_dim\n                else:\n                    adapter_dim = high_resource_adapter_dim\n                self.adapters[lang] = Adapter(adapter_dim, size)\n            self.sim_adapter = sim_adapter_layer\n\n    def forward(self, x, mask, language, cache=None, use_sim_adapter=True):\n        x, x_norm, mask = super().forward(x, mask, cache=cache)\n\n        if not self.use_adapters:\n            return x, mask, language, cache, use_sim_adapter\n        if self.shared_adapter:\n            assert len(self.adapters.keys()) == 1\n            language = list(self.adapters.keys())[0]\n        if (not use_sim_adapter) or not (self.sim_adapter):\n            out = self.adapters[language](x)\n        else:\n            out = []\n            fusion_languages = list(self.sim_adapter.keys())[0]\n            for lang in fusion_languages.split(\"_\"):\n                if lang != \"self\":\n                    out.append(self.adapters[lang](x))\n                else:\n                    out.append(x)\n            out = torch.stack(out).permute(1, 2, 0, 3) # B, T, n_adapters, F\n            out = self.sim_adapter[fusion_languages](x, out, out, residual=x_norm)\n            #out = self.adapters['cs'](x)\n        return out, mask, language, cache, use_sim_adapter\n\nclass AdaptiveEncoder(Encoder):\n    def __init__(\n        self,\n        languages,\n        idim,\n        selfattention_layer_type=\"selfattn\",\n        attention_dim=256,\n        attention_heads=4,\n        conv_wshare=4,\n        conv_kernel_length=11,\n        conv_usebias=False,\n        linear_units=2048,\n        num_blocks=6,\n        dropout_rate=0.1,\n        positional_dropout_rate=0.1,\n        attention_dropout_rate=0.0,\n        input_layer=\"conv2d\",\n        pos_enc_class=PositionalEncoding,\n        normalize_before=True,\n        concat_after=False,\n        positionwise_layer_type=\"linear\",\n        positionwise_conv_kernel_size=1,\n        padding_idx=-1,\n        sim_adapter=False,\n        shared_adapter=None,\n        use_adapters=True,\n        fusion_languages=None,\n    ):\n        super().__init__(idim,\n            selfattention_layer_type,\n            attention_dim,\n            attention_heads,\n            conv_wshare,\n            conv_kernel_length,\n            conv_usebias,\n            linear_units,\n            num_blocks,\n            dropout_rate,\n            positional_dropout_rate,\n            attention_dropout_rate,\n            input_layer,\n            pos_enc_class,\n            normalize_before,\n            concat_after,\n            positionwise_layer_type,\n            positionwise_conv_kernel_size,\n            padding_idx)\n        positionwise_layer, positionwise_layer_args = self.get_positionwise_layer(\n            positionwise_layer_type,\n            attention_dim,\n            linear_units,\n            dropout_rate,\n            positionwise_conv_kernel_size,\n        )\n        if selfattention_layer_type == \"selfattn\":\n            logging.info(\"encoder self-attention layer type = self-attention\")\n            self.encoders = repeat(\n                num_blocks,\n                lambda lnum: AdaptiveEncoderLayer(\n                    languages,\n                    attention_dim,\n                    MultiHeadedAttention(\n                        attention_heads, attention_dim, attention_dropout_rate\n                    ),\n                    positionwise_layer(*positionwise_layer_args),\n                    dropout_rate,\n                    normalize_before,\n                    concat_after,\n                    torch.nn.ModuleDict({\"_\".join(sorted(fusion_languages)): SimAdapter(attention_dim, attention_dropout_rate, fusion_languages)}) if sim_adapter else None,\n                    shared_adapter,\n                    use_adapters,\n                ),\n            )\n        else:\n            raise NotImplementedError(\"Only support self-attention encoder layer\")\n    def forward(self, xs, masks, language, use_sim_adapter=True):\n        xs, masks = self.embed(xs, masks)\n        xs, masks, _, _, _ = self.encoders(xs, masks, language, None, use_sim_adapter)\n        if self.normalize_before:\n            xs = self.after_norm(xs)\n        return xs, masks\nclass CustomDecoderLayer(DecoderLayer):\n    def forward(self, tgt, tgt_mask, memory, memory_mask, cache=None):\n        residual = tgt\n        if self.normalize_before:\n            tgt = self.norm1(tgt)\n        if cache is None:\n            tgt_q = tgt\n            tgt_q_mask = tgt_mask\n        else:\n            # compute only the last frame query keeping dim: max_time_out -> 1\n            assert cache.shape == (\n                tgt.shape[0],\n                tgt.shape[1] - 1,\n                self.size,\n            ), f\"{cache.shape} == {(tgt.shape[0], tgt.shape[1] - 1, self.size)}\"\n            tgt_q = tgt[:, -1:, :]\n            residual = residual[:, -1:, :]\n            tgt_q_mask = None\n            if tgt_mask is not None:\n                tgt_q_mask = tgt_mask[:, -1:, :]\n        if self.concat_after:\n            tgt_concat = torch.cat(\n                (tgt_q, self.self_attn(tgt_q, tgt, tgt, tgt_q_mask)), dim=-1\n            )\n            x = residual + self.concat_linear1(tgt_concat)\n        else:\n            x = residual + self.dropout(self.self_attn(tgt_q, tgt, tgt, tgt_q_mask))\n        if not self.normalize_before:\n            x = self.norm1(x)\n        residual = x\n        if self.normalize_before:\n            x = self.norm2(x)\n        if self.concat_after:\n            x_concat = torch.cat(\n                (x, self.src_attn(x, memory, memory, memory_mask)), dim=-1\n            )\n            x = residual + self.concat_linear2(x_concat)\n        else:\n            x = residual + self.dropout(self.src_attn(x, memory, memory, memory_mask))\n        if not self.normalize_before:\n            x = self.norm2(x)\n        residual = x\n        if self.normalize_before:\n            x = self.norm3(x)\n            x_norm = x\n        x = residual + self.dropout(self.feed_forward(x))\n        if not self.normalize_before:\n            x = self.norm3(x)\n        if cache is not None:\n            x = torch.cat([cache, x], dim=1)\n        return x, x_norm, tgt_mask, memory, memory_mask\nclass AdaptiveDecoderLayer(CustomDecoderLayer):\n    def __init__(\n        self,\n        languages,\n        size,\n        self_attn,\n        src_attn,\n        feed_forward,\n        dropout_rate,\n        normalize_before=True,\n        concat_after=False,\n        sim_adapter_layer=None,\n        shared_adapter=None,\n        use_adapters=True,\n    ):\n        super().__init__(size,\n        self_attn,\n        src_attn,\n        feed_forward,\n        dropout_rate,\n        normalize_before,\n        concat_after\n        )\n        self.use_adapters = use_adapters\n        if use_adapters:\n            self.adapters = torch.nn.ModuleDict()\n            self.shared_adapter = shared_adapter\n            if shared_adapter:\n                languages = [shared_adapter]\n            for lang in languages:\n                if lang in low_resource_languages or self.shared_adapter:\n                    adapter_dim = low_resource_adapter_dim\n                else:\n                    adapter_dim = high_resource_adapter_dim\n                self.adapters[lang] = Adapter(adapter_dim, size)\n            self.sim_adapter = sim_adapter_layer\n            \n    def forward(self, tgt, tgt_mask, memory, memory_mask, language, cache=None, use_sim_adapter=True):\n        x, x_norm, tgt_mask, memory, memory_mask = super().forward(tgt, tgt_mask, memory, memory_mask, cache=cache)\n        if not self.use_adapters:\n            return x, tgt_mask, memory, memory_mask, language, cache, use_sim_adapter\n        if self.shared_adapter:\n            assert len(self.adapters.keys()) == 1\n            language = list(self.adapters.keys())[0]\n        if (not use_sim_adapter) or (not self.sim_adapter):\n            out = self.adapters[language](x)\n        else:\n            out = []\n            fusion_languages = list(self.sim_adapter.keys())[0]\n            for lang in fusion_languages.split(\"_\"):\n                if lang != \"self\":\n                    out.append(self.adapters[lang](x))\n                else:\n                    out.append(x)\n            out = torch.stack(out).permute(1, 2, 0, 3) # B, T, n_adapters, F\n            out = self.sim_adapter[fusion_languages](x, out, out, residual=x_norm)\n            \n        return out, tgt_mask, memory, memory_mask, language, cache, use_sim_adapter\n\n\nclass AdaptiveDecoder(Decoder):\n    def __init__(\n        self,\n        languages,\n        odim_dict,\n        selfattention_layer_type=\"selfattn\",\n        attention_dim=256,\n        attention_heads=4,\n        conv_wshare=4,\n        conv_kernel_length=11,\n        conv_usebias=False,\n        linear_units=2048,\n        num_blocks=6,\n        dropout_rate=0.1,\n        positional_dropout_rate=0.1,\n        self_attention_dropout_rate=0.0,\n        src_attention_dropout_rate=0.0,\n        input_layer=\"embed\",\n        use_output_layer=True,\n        pos_enc_class=PositionalEncoding,\n        normalize_before=True,\n        concat_after=False,\n        sim_adapter=False,\n        shared_adapter=False,\n        use_adapters=True,\n        fusion_languages=None,\n    ):\n        super().__init__(1,\n        selfattention_layer_type,\n        attention_dim,\n        attention_heads,\n        conv_wshare,\n        conv_kernel_length,\n        conv_usebias,\n        linear_units,\n        num_blocks,\n        dropout_rate,\n        positional_dropout_rate,\n        self_attention_dropout_rate,\n        src_attention_dropout_rate,\n        input_layer,\n        use_output_layer,\n        pos_enc_class,\n        normalize_before,\n        concat_after)\n        if input_layer == \"embed\":\n            self.embed = torch.nn.ModuleDict()\n            for lang in odim_dict.keys():\n                self.embed[lang] = torch.nn.Sequential(\n                    torch.nn.Embedding(odim_dict[lang], attention_dim),\n                    pos_enc_class(attention_dim, positional_dropout_rate),\n                )\n        else:\n            raise NotImplementedError(\"only support embed embedding layer\")\n        assert self_attention_dropout_rate == src_attention_dropout_rate\n        if selfattention_layer_type == \"selfattn\":\n            logging.info(\"decoder self-attention layer type = self-attention\")\n            self.decoders = repeat(\n                num_blocks,\n                lambda lnum: AdaptiveDecoderLayer(\n                    languages,\n                    attention_dim,\n                    MultiHeadedAttention(\n                        attention_heads, attention_dim, self_attention_dropout_rate\n                    ),\n                    MultiHeadedAttention(\n                        attention_heads, attention_dim, src_attention_dropout_rate\n                    ),\n                    PositionwiseFeedForward(attention_dim, linear_units, dropout_rate),\n                    dropout_rate,\n                    normalize_before,\n                    concat_after,\n                    torch.nn.ModuleDict({\"_\".join(sorted(fusion_languages)): SimAdapter(attention_dim, self_attention_dropout_rate, fusion_languages)}) if sim_adapter else None,\n                    shared_adapter,\n                    use_adapters,\n                ),\n            )\n        else:\n            raise NotImplementedError(\"Only support self-attention decoder layer\")\n        if use_output_layer:\n            self.output_layer = torch.nn.ModuleDict()\n            for lang in odim_dict.keys():\n                self.output_layer[lang] = torch.nn.Linear(attention_dim, odim_dict[lang])\n        else:\n            self.output_layer = None\n    def forward(self, tgt, tgt_mask, memory, memory_mask, language, use_sim_adapter=True):\n        x = self.embed[language](tgt)\n        x, tgt_mask, memory, memory_mask, _, _, _ = self.decoders(\n            x, tgt_mask, memory, memory_mask, language, None, use_sim_adapter\n        )\n        if self.normalize_before:\n            x = self.after_norm(x)\n        if self.output_layer is not None:\n            x = self.output_layer[language](x)\n        return x, tgt_mask\n    def forward_one_step(self, tgt, tgt_mask, memory, memory_mask, language, cache=None):\n        x = self.embed[language](tgt)\n        if cache is None:\n            cache = [None] * len(self.decoders)\n        new_cache = []\n        for c, decoder in zip(cache, self.decoders):\n            x, tgt_mask, memory, memory_mask, _, _, _ = decoder(\n                x, tgt_mask, memory, None, language, cache=c,\n            )\n            new_cache.append(x)\n        if self.normalize_before:\n            y = self.after_norm(x[:, -1])\n        else:\n            y = x[:, -1]\n        if self.output_layer is not None:\n            y = torch.log_softmax(self.output_layer[language](y), dim=-1)\n        return y, new_cache\n\nclass E2E(E2ETransformer):\n    def __init__(self, idim, odim_dict, args, languages, ignore_id=-1):\n        super().__init__(idim, 1, args, ignore_id)\n        if args.transformer_attn_dropout_rate is None:\n            args.transformer_attn_dropout_rate = args.dropout_rate\n        if args.fusion_languages:\n            args.fusion_languages = args.fusion_languages.split(\"_\")\n        self.fusion_languages = sorted(args.fusion_languages) if args.fusion_languages else sorted(languages)\n        self.encoder = AdaptiveEncoder(\n            languages=languages,\n            idim=idim,\n            selfattention_layer_type=args.transformer_encoder_selfattn_layer_type,\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            conv_wshare=args.wshare,\n            conv_kernel_length=args.ldconv_encoder_kernel_length,\n            conv_usebias=args.ldconv_usebias,\n            linear_units=args.eunits,\n            num_blocks=args.elayers,\n            input_layer=args.transformer_input_layer,\n            dropout_rate=args.dropout_rate,\n            positional_dropout_rate=args.dropout_rate,\n            attention_dropout_rate=args.transformer_attn_dropout_rate,\n            sim_adapter=args.sim_adapter,\n            shared_adapter=args.shared_adapter,\n            use_adapters=args.use_adapters,\n            fusion_languages=self.fusion_languages\n        )\n        if args.mtlalpha < 1:\n            self.decoder = AdaptiveDecoder(\n                languages=languages,\n                odim_dict=odim_dict,\n                selfattention_layer_type=args.transformer_decoder_selfattn_layer_type,\n                attention_dim=args.adim,\n                attention_heads=args.aheads,\n                conv_wshare=args.wshare,\n                conv_kernel_length=args.ldconv_decoder_kernel_length,\n                conv_usebias=args.ldconv_usebias,\n                linear_units=args.dunits,\n                num_blocks=args.dlayers,\n                dropout_rate=args.dropout_rate,\n                positional_dropout_rate=args.dropout_rate,\n                self_attention_dropout_rate=args.transformer_attn_dropout_rate,\n                src_attention_dropout_rate=args.transformer_attn_dropout_rate,\n                sim_adapter=args.sim_adapter,\n                shared_adapter=args.shared_adapter,\n                use_adapters=args.use_adapters,\n                fusion_languages=self.fusion_languages,\n            )\n        else:\n            self.decoder = None\n        self.soss = {lang: odim_dict[lang] - 1 for lang in languages}\n        self.eoss = {lang: odim_dict[lang] - 1 for lang in languages}\n        self.odim_dict = odim_dict\n\n        self.criterion = torch.nn.ModuleDict()\n        for lang in languages:\n            self.criterion[lang] = LabelSmoothingLoss(\n                                        self.odim_dict[lang],\n                                        self.ignore_id,\n                                        args.lsm_weight,\n                                        args.transformer_length_normalized_loss,\n                                    )\n        if args.mtlalpha > 0.0:\n            self.ctc = torch.nn.ModuleDict()\n            for lang in languages:\n                self.ctc[lang] = CTC(\n                    odim_dict[lang], args.adim, args.dropout_rate, ctc_type=args.ctc_type, reduce=True\n                )\n        else:\n            self.ctc = None\n\n        if args.report_cer or args.report_wer:\n            self.error_calculator = ErrorCalculator(\n                args.char_list,\n                args.sym_space,\n                args.sym_blank,\n                args.report_cer,\n                args.report_wer,\n            )\n        else:\n            self.error_calculator = None\n\n        self.reset_parameters(args)\n        # Adapter\n        self.meta_train = args.meta_train\n        self.shared_adapter = args.shared_adapter\n        self.sim_adapter = False\n        self.use_adapters = args.use_adapters\n        if self.use_adapters:\n            for p in self.parameters():\n                p.requires_grad = False\n            self.sim_adapter = args.sim_adapter\n            if not args.sim_adapter:\n                adapter_train_languages = args.adapter_train_languages\n                self.enable_adapter_training(adapter_train_languages, \n                            shared_adapter=args.shared_adapter, enable_head=args.train_adapter_with_head)\n            else:\n                self.reset_sim_adapter_parameters()\n                self.enable_sim_adapter_training()\n        self.recognize_language_branch = None # Set default recognize language for decoding\n\n    def reset_sim_adapter_parameters(self):\n        key = \"_\".join(self.fusion_languages)\n        for layer in self.encoder.encoders:\n            layer.sim_adapter[key].linear_v.weight.data = (\n                torch.zeros(self.adim, self.adim) + 0.000001\n            ).fill_diagonal_(1.0)\n        for layer in self.decoder.decoders:\n            layer.sim_adapter[key].linear_v.weight.data = (\n                torch.zeros(self.adim, self.adim) + 0.000001\n            ).fill_diagonal_(1.0)\n    def enable_sim_adapter_training(self):\n        key = \"_\".join(self.fusion_languages)\n        logging.warning(f\"Unfreezing the SimAdapter parameters: {key}\")\n        for layer in self.encoder.encoders:\n            for p in layer.sim_adapter[key].parameters():\n                p.requires_grad = True\n            # for p in layer.adapter_norm.parameters():\n            #     p.requires_grad = True\n        for layer in self.decoder.decoders:\n            for p in layer.sim_adapter[key].parameters():\n                p.requires_grad = True\n    def get_fusion_guide_loss(self, language):\n        device = next(self.parameters()).device\n        if language not in self.fusion_languages:\n            return torch.tensor(0.0).to(device)\n        guide_loss = torch.tensor(0.0).to(device)\n        loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n        \n        lang_id = sorted(self.fusion_languages).index(language)\n        key = \"_\".join(self.fusion_languages)\n        target = torch.tensor(lang_id).unsqueeze(0).to(device)\n        for layer in self.encoder.encoders:\n            logits = layer.sim_adapter[key].attn.mean(axis=(0, 1)).unsqueeze(0) # (batch, time, n_adapters)\n            guide_loss = guide_loss + loss_fn(logits.exp(), target)\n            layer.sim_adapter.attn = None\n        for layer in self.decoder.decoders:\n            logits = layer.sim_adapter[key].attn.mean(axis=(0, 1)).unsqueeze(0) # (batch, time, n_adapters)\n            guide_loss = guide_loss + loss_fn(logits.exp(), target)\n            layer.sim_adapter[key].attn = None\n        return guide_loss\n\n    def get_fusion_regularization_loss(self):\n        reg_loss = 0.0\n        fusion_reg_loss_weight = 0.01\n        device = next(self.parameters()).device\n        key = \"_\".join(self.fusion_languages)\n        target = torch.zeros((self.adim, self.adim)).fill_diagonal_(1.0).to(device)\n        for layer in self.encoder.encoders:\n            reg_loss = reg_loss + fusion_reg_loss_weight * (target - layer.sim_adapter[key].linear_v.weight).pow(2).sum()\n        for layer in self.decoder.decoders:\n            reg_loss = reg_loss + fusion_reg_loss_weight * (target - layer.sim_adapter[key].linear_v.weight).pow(2).sum()\n        return reg_loss\n\n    def enable_adapter_training(self, specified_languages=None, shared_adapter=False, enable_head=False):\n        # Unfreeze the adapter parameters\n        if specified_languages:\n            enable_languages = specified_languages\n        else:\n            enable_languages = self.criterion.keys()\n        logging.warning(f\"Unfreezing the adapter parameters of {' '.join(enable_languages)}\")\n        for lang in enable_languages:\n            if enable_head:\n                for p in self.decoder.embed[lang].parameters():\n                    p.requires_grad = True\n                for p in self.decoder.output_layer[lang].parameters():\n                    p.requires_grad = True\n                for p in self.ctc[lang].parameters():\n                    p.requires_grad = True\n            if shared_adapter:\n                lang = shared_adapter\n            for layer in self.encoder.encoders:\n                for p in layer.adapters[lang].parameters():\n                    p.requires_grad = True\n            for layer in self.decoder.decoders:\n                for p in layer.adapters[lang].parameters():\n                    p.requires_grad = True\n    \n    def forward(self, xs_pad, ilens, ys_pad, language, use_sim_adapter=True):\n        # 1. forward encoder\n        xs_pad = xs_pad[:, : max(ilens)]  # for data parallel\n        src_mask = make_non_pad_mask(ilens.tolist()).to(xs_pad.device).unsqueeze(-2)\n        hs_pad, hs_mask = self.encoder(xs_pad, src_mask, language, use_sim_adapter=True)\n        self.hs_pad = hs_pad\n\n        # 2. forward decoder\n        if self.decoder is not None:\n            ys_in_pad, ys_out_pad = add_sos_eos(\n                ys_pad, self.soss[language], self.eoss[language], self.ignore_id\n            )\n            ys_mask = target_mask(ys_in_pad, self.ignore_id)\n            pred_pad, pred_mask = self.decoder(ys_in_pad, ys_mask, hs_pad, hs_mask, language, use_sim_adapter=True)\n            self.pred_pad = pred_pad\n\n            # 3. compute attention loss\n            loss_att = self.criterion[language](pred_pad, ys_out_pad)\n            self.acc = th_accuracy(\n                pred_pad.view(-1, self.odim_dict[language]), ys_out_pad, ignore_label=self.ignore_id\n            )\n        else:\n            loss_att = None\n            self.acc = None\n\n        # TODO(karita) show predicted text\n        # TODO(karita) calculate these stats\n        cer_ctc = None\n        if self.mtlalpha == 0.0:\n            loss_ctc = None\n        else:\n            batch_size = xs_pad.size(0)\n            hs_len = hs_mask.view(batch_size, -1).sum(1)\n            ctc_pred_pad = self.ctc[language].ctc_lo(hs_pad)\n            loss_ctc = self.ctc[language](hs_pad.view(batch_size, -1, self.adim), hs_len, ys_pad)\n            if not self.training and self.error_calculator is not None:\n                ys_hat = self.ctc[language].argmax(hs_pad.view(batch_size, -1, self.adim)).data\n                cer_ctc = self.error_calculator[language](ys_hat.cpu(), ys_pad.cpu(), is_ctc=True)\n            # for visualization\n            if not self.training:\n                self.ctc[language].softmax(hs_pad)\n        \n        # 5. compute cer/wer\n        if self.training or self.error_calculator is None or self.decoder is None:\n            cer, wer = None, None\n        else:\n            ys_hat = pred_pad.argmax(dim=-1)\n            cer, wer = self.error_calculator(ys_hat.cpu(), ys_pad.cpu())\n\n        # copied from e2e_asr\n        alpha = self.mtlalpha\n        if alpha == 0:\n            self.loss = loss_att\n            loss_att_data = float(loss_att)\n            loss_ctc_data = None\n        elif alpha == 1:\n            self.loss = loss_ctc\n            loss_att_data = None\n            loss_ctc_data = float(loss_ctc)\n        else:\n            self.loss = alpha * loss_ctc + (1 - alpha) * loss_att\n            loss_att_data = float(loss_att)\n            loss_ctc_data = float(loss_ctc)\n        \n        if self.sim_adapter and self.training:\n            guide_loss = self.get_fusion_guide_loss(language)\n        loss_data = float(self.loss)\n        if loss_data < CTC_LOSS_THRESHOLD and not math.isnan(loss_data):\n            self.reporter.report(\n                loss_ctc_data, loss_att_data, self.acc, cer_ctc, cer, wer, loss_data\n            )\n        else:\n            logging.warning(\"loss (=%f) is not correct\", loss_data)\n        if self.training and not self.meta_train:\n            if not self.sim_adapter:\n                guide_loss = torch.tensor(0.0).cuda()\n            return (self.loss, guide_loss)\n        return self.loss\n    \n    def calculate_all_ctc_probs(self, xs_pad, ilens, ys_pad, language):\n        \"\"\"E2E CTC probability calculation.\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax)\n        :return: CTC probability (B, Tmax, vocab)\n        :rtype: float ndarray\n        \"\"\"\n        ret = None\n        if self.mtlalpha == 0:\n            return ret\n\n        self.eval()\n        with torch.no_grad():\n            self.forward(xs_pad, ilens, ys_pad, language)\n        for name, m in self.named_modules():\n            if isinstance(m, CTC) and m.probs is not None:\n                ret = m.probs.cpu().numpy()\n        self.train()\n        return ret\n    \n    def calculate_all_attentions(self, xs_pad, ilens, ys_pad, language):\n        \"\"\"E2E attention calculation.\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax)\n        :return: attention weights (B, H, Lmax, Tmax)\n        :rtype: float ndarray\n        \"\"\"\n        self.eval()\n        with torch.no_grad():\n            self.forward(xs_pad, ilens, ys_pad, language)\n        ret = dict()\n        for name, m in self.named_modules():\n            if (\n                isinstance(m, MultiHeadedAttention)\n                or isinstance(m, DynamicConvolution)\n                or isinstance(m, RelPositionMultiHeadedAttention)\n            ):\n                ret[name] = m.attn.cpu().numpy()\n            if isinstance(m, DynamicConvolution2D):\n                ret[name + \"_time\"] = m.attn_t.cpu().numpy()\n                ret[name + \"_freq\"] = m.attn_f.cpu().numpy()\n        self.train()\n        return ret\n    \n    def calculate_sim_adapter_attentions(self, xs_pad, ilens, ys_pad, language):\n        self.eval()\n        with torch.no_grad():\n            self.forward(xs_pad, ilens, ys_pad, language)\n        ret = dict()\n        for name, m in self.named_modules():\n            if (\n                isinstance(m, MultiHeadedAttention)\n                or isinstance(m, DynamicConvolution)\n                or isinstance(m, RelPositionMultiHeadedAttention)\n            ) and \"sim_adapter\" in name:\n                ret[name] = m.attn.cpu().numpy()\n        return ret\n\n    def encode(self, x, language):\n        self.eval()\n        x = torch.as_tensor(x).unsqueeze(0)\n        enc_output, _ = self.encoder(x, None, language)\n        return enc_output.squeeze(0)\n    \n    def set_recognize_language_branch(self, language):\n        self.recognize_language_branch = language\n    def recognize_batch(self, xs, recog_args, char_list=None, rnnlm=None, language=None):\n        assert language is not None or self.recognize_language_branch is not None, \\\n                                \"Recognize language is not specified\"\n        if language is None:\n            language = self.recognize_language_branch\n        prev = self.training\n        self.eval()\n        ilens = numpy.fromiter((xx.shape[0] for xx in xs), dtype=numpy.int64)\n        # subsample frame\n        xs = [xx[:: self.subsample[0], :] for xx in xs]\n        xs = [to_device(self, to_torch_tensor(xx).float()) for xx in xs]\n\n        xs_pad = pad_list(xs, 0.0)\n\n        src_mask = make_non_pad_mask(ilens.tolist()).to(xs_pad.device).unsqueeze(-2)\n\n        # 1. Encoder\n        hs_pad, hs_mask = self.encoder(xs_pad, src_mask, language)\n        hlens = torch.tensor([int(sum(mask[0])) for mask in hs_mask])\n\n        # calculate log P(z_t|X) for CTC scores\n        if recog_args.ctc_weight > 0.0:\n            lpz = self.ctc[language].log_softmax(hs_pad)\n            normalize_score = False\n        else:\n            lpz = None\n            normalize_score = True\n\n        logging.info(\"max input length: \" + str(hs_pad.size(1)))\n\n        # search params\n        batch = len(hlens)\n        beam = recog_args.beam_size\n        penalty = recog_args.penalty\n        ctc_weight = getattr(recog_args, \"ctc_weight\", 0)  # for NMT\n        att_weight = 1.0 - ctc_weight\n\n        n_bb = batch * beam\n        pad_b = to_device(hs_pad, torch.arange(batch) * beam).view(-1, 1)\n        max_hlens = hlens\n\n        if recog_args.maxlenratio == 0:\n            maxlens = max_hlens\n        else:\n            maxlens = [\n                max(1, int(recog_args.maxlenratio * max_hlen)) for max_hlen in max_hlens\n            ]\n        minlen = min([int(recog_args.minlenratio * max_hlen) for max_hlen in max_hlens])\n        logging.info(\"max output lengths: \" + str(maxlens))\n        logging.info(\"min output length: \" + str(minlen))\n\n        vscores = to_device(hs_pad, torch.zeros(batch, beam))\n        rnnlm_state = None\n\n        import six\n\n        # initialize hypothesis\n        yseq = [[self.soss[language]] for _ in six.moves.range(n_bb)]\n        accum_odim_ids = [self.soss[language] for _ in six.moves.range(n_bb)]\n        stop_search = [False for _ in six.moves.range(batch)]\n        nbest_hyps = [[] for _ in six.moves.range(batch)]\n        ended_hyps = [[] for _ in six.moves.range(batch)]\n\n        exp_hs_mask = (\n            hs_mask.unsqueeze(1).repeat(1, beam, 1, 1).contiguous()\n        )  # (batch, beam, 1, T)\n        exp_hs_mask = exp_hs_mask.view(n_bb, hs_mask.size()[1], hs_mask.size()[2])\n        exp_h = (\n            hs_pad.unsqueeze(1).repeat(1, beam, 1, 1).contiguous()\n        )  # (batch, beam, T, F)\n        exp_h = exp_h.view(n_bb, hs_pad.size()[1], hs_pad.size()[2])\n\n        ctc_scorer, ctc_state = None, None\n        if lpz is not None:\n            scoring_num = min(\n                int(beam * CTC_SCORING_RATIO) if att_weight > 0.0 else 0,\n                lpz.size(-1),\n            )\n            ctc_scorer = CTCPrefixScoreTH(lpz, hlens, 0, self.eoss[language])\n\n        for i in six.moves.range(max(maxlens)):\n            logging.debug(\"position \" + str(i))\n\n            # get nbest local scores and their ids\n            ys_mask = subsequent_mask(i + 1).to(hs_pad.device).unsqueeze(0)\n\n            ys = torch.tensor(yseq).to(hs_pad.device)\n            vy = to_device(hs_pad, torch.LongTensor(self._get_last_yseq(yseq)))\n\n            # local_att_scores (n_bb = beam * batch, vocab)\n            if self.decoder is not None:\n                local_att_scores = self.decoder.forward_one_step(\n                    ys, ys_mask, exp_h, memory_mask=exp_hs_mask, language=language,\n                )[0]\n            else:\n                local_att_scores = to_device(\n                    hs_pad, torch.zeros((n_bb, lpz.size(-1)), dtype=lpz.dtype)\n                )\n\n            if rnnlm:\n                rnnlm_state, local_lm_scores = rnnlm.buff_predict(rnnlm_state, vy, n_bb)\n                local_scores = local_att_scores + recog_args.lm_weight * local_lm_scores\n            else:\n                local_scores = local_att_scores\n\n            # ctc\n            if ctc_scorer:\n                local_scores = att_weight * local_att_scores\n                local_scores[:, 0] = self.logzero  # avoid choosing blank\n                part_ids = (\n                    torch.topk(local_scores, scoring_num, dim=-1)[1]\n                    if scoring_num > 0\n                    else None\n                )\n                local_ctc_scores, ctc_state = ctc_scorer(\n                    yseq, ctc_state, part_ids\n                )  # local_ctc_scores (n_bb, odim)\n\n                local_scores = local_scores + ctc_weight * local_ctc_scores\n                if rnnlm:\n                    local_scores = local_scores + recog_args.lm_weight * local_lm_scores\n            \n            local_scores = local_scores.view(batch, beam, self.odim_dict[language])\n            if i == 0:\n                local_scores[:, 1:, :] = self.logzero\n\n            # accumulate scores\n            eos_vscores = local_scores[:, :, self.eoss[language]] + vscores\n            vscores = vscores.view(batch, beam, 1).repeat(1, 1, self.odim_dict[language])\n            vscores[:, :, self.eoss[language]] = self.logzero\n            vscores = (vscores + local_scores).view(batch, -1)  # (batch, odim * beam)\n\n            # global pruning\n            accum_best_scores, accum_best_ids = torch.topk(vscores, beam, 1)\n\n            accum_odim_ids = (\n                torch.fmod(accum_best_ids, self.odim_dict[language]).view(-1).data.cpu().tolist()\n            )\n            accum_padded_beam_ids = (\n                (accum_best_ids // self.odim_dict[language] + pad_b).view(-1).data.cpu().tolist()\n            )\n            y_prev = yseq[:][:]\n            yseq = self._index_select_list(yseq, accum_padded_beam_ids)\n            yseq = self._append_ids(yseq, accum_odim_ids)\n            vscores = accum_best_scores\n\n            vidx = to_device(hs_pad, torch.LongTensor(accum_padded_beam_ids))\n\n            # pick ended hyps\n            if i >= minlen:\n                k = 0\n                penalty_i = (i + 1) * penalty\n                thr = accum_best_scores[:, -1]\n                for samp_i in six.moves.range(batch):\n                    if stop_search[samp_i]:\n                        k = k + beam\n                        continue\n                    for beam_j in six.moves.range(beam):\n                        _vscore = None\n                        if eos_vscores[samp_i, beam_j] > thr[samp_i]:\n                            yk = y_prev[k][:]\n                            if len(yk) <= maxlens[samp_i]:\n                                _vscore = eos_vscores[samp_i][beam_j] + penalty_i\n                            rnnlm_idx = k\n                        elif i == maxlens[samp_i] - 1:\n                            yk = yseq[k][:]\n                            _vscore = vscores[samp_i][beam_j] + penalty_i\n                            rnnlm_idx = accum_padded_beam_ids[k]\n                        if _vscore:\n                            yk.append(self.eoss[language])\n                            if rnnlm:\n                                _vscore += recog_args.lm_weight * rnnlm.final(\n                                    rnnlm_state, index=rnnlm_idx\n                                )\n                            ended_hyps[samp_i].append(\n                                {\"yseq\": yk, \"score\": _vscore.data.cpu().numpy()}\n                            )\n                        k = k + 1\n            # end detection\n            stop_search = [\n                stop_search[samp_i]\n                or end_detect(ended_hyps[samp_i], i)\n                or i >= maxlens[samp_i]\n                for samp_i in six.moves.range(batch)\n            ]\n\n            stop_search_summary = list(set(stop_search))\n\n            if len(stop_search_summary) == 1 and stop_search_summary[0]:\n                break\n\n            if rnnlm:\n                rnnlm_state = self._index_select_lm_state(rnnlm_state, 0, vidx)\n            if ctc_scorer:\n                ctc_state = ctc_scorer.index_select_state(ctc_state, accum_best_ids)\n\n        torch.cuda.empty_cache()\n\n        dummy_hyps = [\n            {\"yseq\": [self.soss[language], self.eoss[language]], \"score\": numpy.array([-float(\"inf\")])}\n        ]\n        ended_hyps = [\n            ended_hyps[samp_i] if len(ended_hyps[samp_i]) != 0 else dummy_hyps\n            for samp_i in six.moves.range(batch)\n        ]\n        if normalize_score:\n            for samp_i in six.moves.range(batch):\n                for x in ended_hyps[samp_i]:\n                    x[\"score\"] /= len(x[\"yseq\"])\n\n        nbest_hyps = [\n            sorted(ended_hyps[samp_i], key=lambda x: x[\"score\"], reverse=True)[\n                : min(len(ended_hyps[samp_i]), recog_args.nbest)\n            ]\n            for samp_i in six.moves.range(batch)\n        ]\n        if prev:\n            self.train()\n        return nbest_hyps\n", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "code/ASR/Adapter/e2e_asr_adaptertransformer.py", "language": "python", "complexity": "complex", "star_count": 14009, "quality_score": 0.91, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 1027, "word_count": 3069}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nimport os\nimport collections\nfrom espnet.bin.asr_train import get_parser\nfrom espnet.utils.dynamic_import import dynamic_import\nfrom espnet.utils.deterministic_utils import set_deterministic_pytorch\nfrom espnet.asr.pytorch_backend.asr_init import freeze_modules\n\nfrom torch.nn.parallel import data_parallel\nfrom torch.nn.utils.clip_grad import clip_grad_norm_\nimport torch\nimport numpy as np\n\nimport data_load\nimport random\nimport json\nimport sys\nfrom utils import setup_logging, str2bool, dict_average\nfrom utils import load_pretrained_model, load_head_from_pretrained_model, torch_save, torch_load\nfrom utils import recognize_and_evaluate\nimport math\nfrom e2e_asr_adaptertransformer import E2E as E2EAdapterTransformer\nimport matplotlib.pyplot as plt\nimport copy\n\ndef add_custom_arguments(parser):\n    # EasyEspnet arguments\n    parser.add_argument('--data_file', type=str, default=None)\n    parser.add_argument(\"--root_path\", type=str, required=True, help=\"Path to the ESPnet features, e.g.: <espnet_path>/egs/commonvoice/asr1/\")\n    #parser.add_argument(\"--root_path\", type=str, default=\"/opt/espnet/egs/an4/asr1/\")\n    parser.add_argument('--dataset', type=str, required=True, help=\"Dataset name to be referred in data_load, e.g.: an4\")\n    #parser.add_argument(\"--dataset\", type=str, default=\"an4\")\n    parser.add_argument(\"--exp\", type=str, default=\"exp\")\n    parser.add_argument(\"--decoding_mode\", type=str2bool, default=False, help=\"if true, then only perform decoding test\")\n    parser.add_argument(\"--load_pretrained_model\", type=str, default=\"\", nargs=\"?\",\n                    help=\"<model_path>:<load_modules>:<exclude_modules>\")\n    parser.add_argument(\"--compute_cer\", type=str2bool, default=True)\n    parser.add_argument(\"--decoding_config\", type=str, default=None)\n    parser.add_argument(\n        \"--bpemodel\", type=bool, default=True\n    )  # Set to true when testing CER/WERs\n    parser.add_argument(\"--dist_train\", type=str2bool, default=False)\n    parser.add_argument(\"--local_rank\", type=int, default=0)\n    parser.add_argument(\"--result_label\", type=str, default=None)\n    parser.add_argument(\"--recog_json\", type=str, default=None)\n    parser.add_argument(\"--adam_lr\", type=float, default=1e-3)\n    \n    # Adapter-related\n    parser.add_argument(\"--use_adapters\", type=str2bool, default=True, help=\"whether to inject adapters into the model\")\n    parser.add_argument(\"--shared_adapter\", type=str, default=None, help=\"Share one adapter across all languages\")\n    parser.add_argument('--adapter_train_languages', type=str, default=None,\n                                help=\"Splitted by _, e.g., en_zh_jp\")\n    parser.add_argument(\"--train_adapter_with_head\", type=str2bool, \n                        default=False, help=\"whether to train adapter with language-specific head jointly\")\n    # SimAdapter-related\n    parser.add_argument(\"--sim_adapter\", type=str2bool, default=False)\n    parser.add_argument(\"--fusion_languages\", type=str, default=None)\n    parser.add_argument(\"--guide_loss_weight\", type=float, default=0.1)\n    parser.add_argument(\"--guide_loss_weight_decay_steps\", type=int, default=0)\n\n    # MAML-related\n    parser.add_argument(\"--meta_train\", type=str2bool, default=False)\n    parser.add_argument(\"--meta_lr\", type=float, default=1.0, help=\"used for meta-learning outer step\")\n    \n    # Others\n    parser.add_argument(\"--load_head_from_pretrained_model\", type=str, default=\"\", nargs=\"?\",\n                    help=\"<model_path>\")\n\ndef train_epoch(dataloader, model, optimizer, epoch=None):\n    model.train()\n    stats = collections.defaultdict(list)\n    for batch_idx, data in enumerate(dataloader):\n        fbank, seq_lens, tokens, language = data\n        fbank, seq_lens, tokens = fbank.cuda(), seq_lens.cuda(), tokens.cuda()\n        if isinstance(optimizer, dict):\n            optimizer[language].zero_grad()\n        else:\n            optimizer.zero_grad()\n        model.zero_grad()\n        if args.ngpu <= 1 or args.dist_train:\n            ctc_att_loss, sim_adapter_guide_loss = model(fbank, seq_lens, tokens, language)# .mean() # / self.accum_grad\n        else:\n            # apex does not support torch.nn.DataParallel\n            ctc_att_loss, sim_adapter_guide_loss = (\n                data_parallel(model, (fbank, seq_lens, tokens, language), range(args.ngpu))# .mean() # / self.accum_grad\n            )\n        loss = ctc_att_loss.mean()\n        if args.sim_adapter:\n            if hasattr(model, \"module\"):\n                sim_adapter_reg_loss = model.module.get_fusion_regularization_loss()\n            else:\n                sim_adapter_reg_loss = model.get_fusion_regularization_loss()\n            loss = loss + sim_adapter_reg_loss\n            stats[\"sim_adapter_reg_loss_lst\"].append(sim_adapter_reg_loss.item())\n            if args.guide_loss_weight > 0:\n                if args.guide_loss_weight_decay_steps > 0:\n                    n_batch = len(dataloader)\n                    current_iter = float(batch_idx + (epoch - 1) * n_batch)\n                    frac_done = 1.0 * float(current_iter) / args.guide_loss_weight_decay_steps\n                    current_weight = args.guide_loss_weight * max(0., 1. - frac_done)\n                    stats[\"sim_adapter_guide_loss_weight\"] = current_weight\n                else:\n                    current_weight = args.guide_loss_weight\n                sim_adapter_guide_loss = sim_adapter_guide_loss.mean()\n                loss = loss + current_weight * sim_adapter_guide_loss\n                stats[\"sim_adapter_guide_loss_lst\"].append(sim_adapter_guide_loss.item())\n\n        if not hasattr(model, \"module\"):\n            if hasattr(model, \"acc\") and model.acc is not None:\n                stats[\"acc_lst\"].append(model.acc)\n                model.acc = None\n        else:\n            if hasattr(model, \"acc\") and model.module.acc is not None:\n                stats[\"acc_lst\"].append(model.module.acc)\n                model.module.acc = None\n        loss.backward()\n        grad_norm = clip_grad_norm_(model.parameters(), args.grad_clip)\n        if math.isnan(grad_norm):\n            logging.warning(\"grad norm is nan. Do not update model.\")\n        else:\n            if isinstance(optimizer, dict):\n                optimizer[language].step()\n            else:\n                optimizer.step()\n            stats[\"loss_lst\"].append(loss.item())\n        logging.warning(f\"Training batch: {batch_idx+1}/{len(dataloader)}\")\n    return dict_average(stats)\n\ndef train_maml_epoch(dataloader, model, optimizer, epoch=None):\n    model.train()\n    stats = collections.defaultdict(list)\n    \n    for batch_idx, total_batches in enumerate(dataloader):\n        i = batch_idx # current iteration in epoch\n        len_dataloader = len(dataloader) # total iteration in epoch\n        meta_iters = args.epochs * len_dataloader\n        current_iter = float(i + (epoch - 1) * len_dataloader)\n        frac_done = 1.0 * float(current_iter) / meta_iters\n        current_outerstepsize = args.meta_lr * (1. - frac_done)\n\n        weights_original = copy.deepcopy(model.state_dict())\n        new_weights = []\n        for total_batch in total_batches: # Iter by languages\n            in_batch_size = int(total_batch[0].shape[0] / 2) # In-language batch size\n            for meta_step in range(2): # Meta-train & meta-valid\n                if meta_step == 1:\n                    last_backup = copy.deepcopy(model.state_dict())\n                else:\n                    last_backup = None\n                batch = list(copy.deepcopy(total_batch))\n                for i_batch in range(len(batch)-1):\n                    batch[i_batch] = batch[i_batch][meta_step*in_batch_size:(1+meta_step)*in_batch_size]\n                batch = tuple(batch)\n                \n                fbank, seq_lens, tokens, language = batch\n                fbank, seq_lens, tokens = fbank.cuda(), seq_lens.cuda(), tokens.cuda()\n                optimizer.zero_grad()\n                model.zero_grad()\n                if args.ngpu <= 1 or args.dist_train:\n                    loss = model(fbank, seq_lens, tokens, language).mean() # / self.accum_grad\n                else:\n                    # apex does not support torch.nn.DataParallel\n                    loss = (\n                        data_parallel(model, (fbank, seq_lens, tokens, language), range(args.ngpu)).mean() # / self.accum_grad\n                    )\n                # print(loss.item())\n                loss.backward()\n                grad_norm = clip_grad_norm_(model.parameters(), args.grad_clip)\n                if math.isnan(grad_norm):\n                    logging.warning(\"grad norm is nan. Do not update model.\")\n                else:\n                    optimizer.step()\n                \n                if meta_step == 1: # Record meta valid\n                    if not hasattr(model, \"module\"):\n                        if hasattr(model, \"acc\") and model.acc is not None:\n                            stats[\"acc_lst\"].append(model.acc)\n                            model.acc = None\n                    else:\n                        if hasattr(model, \"acc\") and model.module.acc is not None:\n                            stats[\"acc_lst\"].append(model.module.acc)\n                            model.module.acc = None\n                    stats[\"loss_lst\"].append(loss.item())\n                    stats[\"meta_lr\"] = current_outerstepsize\n                    optimizer.zero_grad()\n\n            for name in last_backup:\n                # Compute meta-gradient\n                last_backup[name] = model.state_dict()[name] - last_backup[name]\n            # Change back to the original parameters for the new language\n            new_weights.append(last_backup) # updates.append(subtract_vars(self._model_state.export_variables(), last_backup))\n            model.load_state_dict({ name: weights_original[name] for name in weights_original})\n        \n        ws = len(new_weights)\n        # Compute average meta-gradient\n        fweights = { name : new_weights[0][name]/float(ws) for name in new_weights[0] }\n        for i in range(1, ws):\n            for name in new_weights[i]:\n                fweights[name] = fweights[name] + new_weights[i][name] / float(ws)\n        model.load_state_dict({name : weights_original[name] + (fweights[name] * current_outerstepsize) for name in weights_original})\n\n        logging.warning(f\"Training batch: {batch_idx+1}/{len(dataloader)}\")\n    return dict_average(stats)\n\ndef test(epoch, dataloader, model, model_path=None, language=None, visualize_sim_adapter=False):\n    if model_path:\n        torch_load(model_path, model)\n    orig_model = None\n    if hasattr(model, \"module\"):\n        orig_model = model\n        model = model.module\n    model.eval()\n    stats = collections.defaultdict(list)\n    for batch_idx, data in enumerate(dataloader):\n        logging.warning(f\"Testing batch: {batch_idx+1}/{len(dataloader)}\")\n        if len(data) == 4:\n            fbank, seq_lens, tokens, language = data\n        else:\n            assert language is not None\n            fbank, seq_lens, tokens = data\n        fbank, seq_lens, tokens = fbank.cuda(), seq_lens.cuda(), tokens.cuda()\n        with torch.no_grad():\n            loss = model(fbank, seq_lens, tokens, language)\n        \n        if visualize_sim_adapter:\n            atts = model.calculate_sim_adapter_attentions(fbank, seq_lens, tokens, language)\n            init_mat = lambda: np.zeros((len(model.fusion_languages),))\n            avg_atts = collections.defaultdict(init_mat)\n            count = collections.defaultdict(int)\n            for key in atts.keys():\n                avg_atts[key] = avg_atts[key] + atts[key].sum(axis=(0, 1))\n                count[key] = count[key] + atts[key].shape[0] * atts[key].shape[1]\n        stats[\"loss_lst\"].append(loss.item())\n        if not hasattr(model, \"module\"):\n            if model.acc is not None:\n                stats[\"acc_lst\"].append(model.acc)\n                model.acc = None\n        else:\n            if model.module.acc is not None:\n                stats[\"acc_lst\"].append(model.module.acc)\n                model.module.acc = None\n    if visualize_sim_adapter:\n        for key in avg_atts.keys():\n            avg_atts[key] = avg_atts[key] / count[key]\n            logging.warning(f\"Attention scores of {key}: {avg_atts[key]}\")\n        fig = plt.figure(figsize=(16, 8))\n        ax = fig.subplots()\n        atts, labels = [], []\n        for key in avg_atts.keys():\n            atts.append(avg_atts[key])\n            labels.append(key)\n        atts = np.stack(atts)\n        tick_marks = np.arange(len(labels))\n        ax.set_yticks(tick_marks)\n        ax.set_yticklabels(labels)\n        x_labels = list(sorted(model.fusion_languages))\n        ax.set_xticks(np.arange(len(x_labels)))\n        ax.set_xticklabels(x_labels)\n        ax.imshow(atts)\n        import itertools\n        for i, j in itertools.product(range(atts.shape[0]), range(atts.shape[1])):\n            plt.text(j, i, \"{:0.2f}\".format(atts[i, j]),\n                    horizontalalignment=\"center\",\n                    color=\"white\")\n        fig.tight_layout()\n        fig.savefig(f\"{args.outdir}/att_{epoch}.png\")\n        plt.close()\n    if orig_model is not None:\n        model = orig_model\n    return dict_average(stats)\n\ndef train(dataloaders, model, optimizer, save_path):\n    train_loader, val_loader, test_loader = dataloaders\n    best_loss = float(\"inf\")\n    early_stop = 0\n    log_json = []\n    for epoch in range(args.start_epoch, args.epochs + 1):\n        early_stop += 1\n        epoch_stats = collections.OrderedDict(epoch=epoch)\n        train_stats = train_epoch(train_loader, model, optimizer, epoch)\n        valid_stats = test(f\"val_{epoch}\", val_loader, model, visualize_sim_adapter=args.sim_adapter)\n        if best_loss > valid_stats[\"loss\"]:  # Save loss best model\n            best_loss = valid_stats[\"loss\"]\n            torch_save(model, save_path)\n            early_stop = 0\n\n        test_stats = test(f\"test_{epoch}\", test_loader, model)\n        logging.warning(\n            f\"Epoch: {epoch}, Iteration: {epoch * len(train_loader)}, \"\n            + f\"train loss: {train_stats['loss']:.4f}, dev loss: {valid_stats['loss']:.3f}, test loss: {test_stats['loss']:.3f}\"\n        )\n\n        torch_save(model, f\"{args.outdir}/snapshot.ep.{epoch}\", optimizer=optimizer)\n        for key in sorted(list(set(list(train_stats.keys()) + list(test_stats.keys())))):\n            if not key.endswith(\"_lst\"):\n                if key in train_stats:\n                    epoch_stats[f\"main/{key}\"] = train_stats[key]\n                if key in valid_stats:\n                    epoch_stats[f\"validation/main/{key}\"] = valid_stats[key]\n                if key in test_stats:\n                    epoch_stats[f\"test/main/{key}\"] = test_stats[key]\n                \n        log_json.append(epoch_stats)\n        with open(f\"{args.outdir}/log\", \"w\") as f:\n            json.dump(log_json, f,\n                indent=4,\n                ensure_ascii=False,\n                separators=(\",\", \": \"),\n            )\n            logging.warning(f\"Log saved at {args.outdir}/log\")\n            \n        if args.patience > 0 and early_stop >= args.patience:\n            test_stats = test(\"test_best\", test_loader, model, save_path)\n            logging.warning(f\"=====Early stop! Final best test loss: {test_stats['loss']}\")\n            break\n\nif __name__ == \"__main__\":\n    # ÊâßË°åËØ•ÂëΩ‰ª§ËøêË°å4 GPUËÆ≠ÁªÉÔºöCUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=2 train.py\n    setup_logging(verbose=0)  # Should come first before other package import logging\n    parser = get_parser()\n    add_custom_arguments(parser)\n\n    arg_list = sys.argv[1:] + [\n        \"--dict\", '',\n        #\"--dataset\", \"_\".join(\"cv mt cnh ky dv sl el lv fyNL sah\".split()),\n    ]\n    if \"--config\" not in arg_list:\n        arg_list += [\"--config\", \"config/train.yaml\"]\n    if \"--outdir\" not in arg_list:\n        arg_list += [\"--outdir\", '']\n\n    args, _ = parser.parse_known_args(arg_list)\n    # Use all GPUs\n    ngpu = torch.cuda.device_count() if args.ngpu is None else args.ngpu\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(\n                [str(item) for item in range(ngpu)])\n    logging.warning(f\"ngpu: {ngpu}\")\n    \n    # set random seed\n    logging.info(\"random seed = %d\" % args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    set_deterministic_pytorch(args)\n    torch.cuda.manual_seed(args.seed)\n    if ngpu > 1:\n        torch.cuda.manual_seed_all(args.seed) # multi-gpu setting\n\n    model_module =  \"e2e_asr_adaptertransformer:E2E\"\n    model_class = E2EAdapterTransformer\n    model_class.add_arguments(parser)\n    args = parser.parse_args(arg_list)\n\n    setattr(args, \"conf_name\", \".\".join(os.path.basename(args.config).split(\".\")[:-1]))\n    if not args.outdir:\n        args.outdir = f\"./outputs/results_{args.dataset}/{args.conf_name}\"\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n    \n    setattr(args, \"ngpu\", ngpu)\n    if args.data_file is not None:\n        args.root_path = args.data_file\n    \n    if args.ngpu > 1:\n\n        if args.opt == \"noam\" and hasattr(args, \"transformer_lr\"):\n            logging.warning(f\"Multi-GPU training: increase transformer lr {args.transformer_lr} --> {args.transformer_lr * np.sqrt(args.ngpu)}\")\n            args.transformer_lr = args.transformer_lr * np.sqrt(args.ngpu)\n        elif (args.opt == \"adam\" or args.meta_train) and hasattr(args, \"adam_lr\"):\n            logging.warning(f\"Multi-GPU training: increase adam lr {args.adam_lr} --> {args.adam_lr * np.sqrt(args.ngpu)}\")\n            args.adam_lr = args.adam_lr * np.sqrt(args.ngpu)\n\n        if args.dist_train:\n            torch.distributed.init_process_group(backend=\"nccl\")\n            local_rank = torch.distributed.get_rank()\n            args.local_rank = local_rank\n            torch.cuda.set_device(local_rank)\n            device = torch.device(\"cuda\", local_rank)\n        else:\n            logging.warning(\n                \"Training batch size is automatically increased (%d -> %d)\"\n                % (args.batch_size, args.batch_size * args.ngpu)\n            )\n            args.batch_size *= args.ngpu\n    \n    if args.accum_grad > 1:\n        logging.warning(\n                \"gradient accumulation is not implemented. batch size is increased (%d -> %d)\"\n                % (args.batch_size, args.batch_size * args.accum_grad)\n            )\n        args.batch_size *= args.accum_grad\n        args.accum_grad = 1\n\n    dataloaders = {}\n    token_dict = {}\n    idim, odim_dict = None, {}\n    args.dataset = args.dataset.split(\"_\")\n    languages = args.dataset\n    data_load_languages = languages\n\n    if args.adapter_train_languages is not None:\n        args.adapter_train_languages = args.adapter_train_languages.split(\"_\")\n        data_load_languages = args.adapter_train_languages\n    else:\n        logging.warning(\"adapter_train_languages is None, will use all datasets for training\")\n        args.adapter_train_languages = args.dataset\n    \n    dataloaders, (idim, odim_dict) = data_load.load_multilingual_data(args.root_path, args.dataset, args, data_load_languages)\n    for idx, data_set in enumerate(args.dataset):\n        if languages[idx] not in data_load_languages:\n            continue\n        token_dict[languages[idx]] = data_load.load_token_list(\n                os.path.join(args.root_path, data_load.data_config[data_set][\"token\"])\n        )\n    setattr(args, \"char_list\", token_dict)\n    model = model_class(idim, odim_dict, args, languages)\n\n    model_conf = args.outdir + \"/model.json\"\n    with open(model_conf, \"wb\") as f:\n        logging.info(\"writing a model config file to \" + model_conf)\n        f.write(\n            json.dumps(\n                (idim, odim_dict, vars(args)),\n                indent=4,\n                ensure_ascii=False,\n                sort_keys=True,\n            ).encode(\"utf_8\")\n        )\n\n    model.cuda()\n    if args.freeze_mods:\n        model, model_params = freeze_modules(model, args.freeze_mods)\n    else:\n        model_params = model.parameters()\n\n    logging.warning(\"Trainable parameters:\")\n    for name, parameter in model.named_parameters():\n        if parameter.requires_grad:\n            logging.warning(name)\n            \n    # Setup an optimizer\n    if args.meta_train:\n        logging.warning(f\"Use Adam optimizer with lr={args.adam_lr}, beta0=0 for meta-training inner step\")\n        optimizer = torch.optim.Adam(model_params, lr=args.adam_lr, betas=(0, 0.999), weight_decay=args.weight_decay)\n    else:\n        if args.opt == \"adadelta\":\n            optimizer = torch.optim.Adadelta(\n                model_params, rho=0.95, eps=args.eps, weight_decay=args.weight_decay\n            )\n        elif args.opt == \"adam\":\n            logging.warning(f\"Using Adam optimizer with lr={args.adam_lr}\")\n            optimizer = torch.optim.Adam(model_params, lr=args.adam_lr, weight_decay=args.weight_decay)\n        elif args.opt == \"noam\":\n            from espnet.nets.pytorch_backend.transformer.optimizer import get_std_opt\n            optimizer = get_std_opt(\n                model_params, args.adim, args.transformer_warmup_steps, args.transformer_lr\n            )\n    \n    if len(args.adapter_train_languages) > 1 and not args.sim_adapter and not args.shared_adapter:\n        model_params = collections.defaultdict(list)\n        optimizer = {}\n        for lang in args.adapter_train_languages:\n            for name, parameter in model.named_parameters():\n                if parameter.requires_grad and lang in name.split(\".\"):\n                    model_params[lang].append(parameter)\n            logging.warning(f\"Number of trainable parameters for language {lang} \" + str(sum(p.numel() for p in model_params[lang])))\n            optimizer[lang] = torch.optim.Adam(model_params[lang], lr=args.adam_lr, weight_decay=args.weight_decay)\n    \n    # Resume from a snapshot\n    if args.resume:\n        logging.warning(\"resumed from %s\" % args.resume)\n        torch_load(args.resume, model, optimizer)\n        setattr(args, \"start_epoch\", int(args.resume.split('.')[-1]) + 1)\n    else:\n        setattr(args, \"start_epoch\", 1)\n\n    if args.load_pretrained_model:\n        model_path, modules_to_load, exclude_modules = args.load_pretrained_model.split(\":\")\n        logging.warning(\"load pretrained model from %s\" % args.load_pretrained_model)\n        load_pretrained_model(model=model, model_path=model_path, \n                                modules_to_load=modules_to_load, exclude_modules=exclude_modules)\n    if args.load_head_from_pretrained_model:\n        logging.warning(\"load pretrained model head from %s\" % args.load_head_from_pretrained_model)\n        load_head_from_pretrained_model(model=model, model_path=args.load_head_from_pretrained_model)\n        \n    logging.warning(\n        \"Total parameter of the model = \"\n        + str(sum(p.numel() for p in model.parameters()))\n    )\n    logging.warning(\n        \"Trainable parameter of the model = \"\n        + str(sum(p.numel() for p in filter(lambda x: x.requires_grad, model.parameters())))\n    )\n\n    if args.ngpu > 1 and args.dist_train:\n        model = torch.nn.parallel.DistributedDataParallel(model, \n                                                        device_ids=[local_rank],\n                                                        output_device=local_rank,\n                                                        find_unused_parameters=True,\n                                                        )\n    \n    save_path = f\"{args.outdir}/model.loss.best\"\n    if args.meta_train:\n        train_epoch = train_maml_epoch\n    if not args.decoding_mode:\n        train(dataloaders, model, optimizer, save_path)\n    if args.compute_cer and args.local_rank == 0:\n        # For CER/WER computing\n        for idx, dataset in enumerate(args.dataset):\n            language = languages[idx]\n            if args.adapter_train_languages and not (language in args.adapter_train_languages):\n                continue\n            if args.bpemodel and \"bpemodel\" in data_load.data_config[dataset]:\n                logging.warning(f\"load bpe model for dataset {dataset}\")\n                args.bpemodel = data_load.load_bpemodel(args.root_path, dataset)\n            dataloaders, _ = data_load.load_data(args.root_path, dataset, args)\n            splits = [\"test\", \"val\"]\n            for split in splits:\n                logging.warning(f\"---------Recognizing {dataset} {split}----------\")\n                args.result_label = f\"{args.outdir}/{dataset}_{split}_recog.json\"\n                if not data_load.data_config[dataset][split]:\n                    split_path = os.path.join(args.root_path, f\"{args.root_path}/tmp_dev_set_{dataset}.json\")\n                else:\n                    split_path = data_load.data_config[dataset][split]\n                args.recog_json = os.path.join(args.root_path, split_path)\n                idx = [\"train\", \"val\", \"test\"].index(split)\n                test_stats = test(f\"{split}_best\", dataloaders[idx],\n                                model, \n                                save_path, \n                                language=language, \n                                visualize_sim_adapter=args.sim_adapter)\n                logging.warning(f\"Loss: {test_stats['loss']}\")\n                err_dict = recognize_and_evaluate(dataloaders[idx], model, args, language, \n                                    model_path=save_path, wer=True, write_to_json=True)\n                logging.warning(f\"CER: {err_dict['cer']['err']}\")\n                logging.warning(f\"WER: {err_dict['wer']['err']}\")", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "code/ASR/Adapter/train.py", "language": "python", "complexity": "complex", "star_count": 14009, "quality_score": 1.0, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 535, "word_count": 1836}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import torch\nimport logging\nfrom espnet.asr.asr_utils import add_results_to_json\nimport argparse\nimport numpy as np\nimport collections\nimport json\n\n\ndef load_head_from_pretrained_model(model, model_path):\n    model_dict = torch.load(model_path, map_location=lambda storage, loc: storage)\n    if \"model\" in model_dict.keys():\n        model_dict = model_dict[\"model\"]\n    src_dict = {k: v for k, v in model_dict.items() if \"decoder.embed.\" in k or \"ctc.\" in k or \"decoder.output_layer.\" in k}\n    dst_state = model.state_dict()\n    dst_state.update(src_dict)\n    for key in dst_state.keys():\n        if key in src_dict.keys():\n            logging.info(\"loading \" + key)\n    model.load_state_dict(dst_state)\n\ndef load_adapter_from_pretrained_model(model, model_path, src_adapter, tgt_adapter):\n    '''\n    src_adapter, tgt_adapter: str, names of the source and target adapters to load parameters\n    '''\n    model_dict = torch.load(model_path, map_location=lambda storage, loc: storage)\n    if \"model\" in model_dict.keys():\n        model_dict = model_dict[\"model\"]\n    src_dict = {k.replace(src_adapter, tgt_adapter): v for k, v in model_dict.items() if f\"adapters.{src_adapter}\" in k}\n    dst_state = model.state_dict()\n    dst_state.update(src_dict)\n    for key in dst_state.keys():\n        if key in src_dict.keys():\n            logging.info(\"loading \" + key)\n    model.load_state_dict(dst_state)\n\n# ====================  EasyEspnet functions =================================\n\ndef str2bool(str):\n\treturn True if str.lower() == 'true' else False\ndef setup_logging(verbose=1):\n    if verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n        )\n        logging.warning(\"Skip DEBUG/INFO messages\")\n\n# Training stats\ndef dict_average(dic):\n    avg_key, avg_val = [], []\n    for key, lst in dic.items():\n        if key.endswith(\"_lst\"):\n            avg_key.append(key[:-4])\n            avg_val.append(np.mean(lst))\n    for key, val in zip(avg_key, avg_val):\n        dic[key] = val\n    return dic\n\n# Load and save\ndef load_pretrained_model(model, model_path, modules_to_load=None, exclude_modules=None):\n    '''\n    load_pretrained_model(model=model, model_path=\"\", \n                                modules_to_load=None, exclude_modules=\"\")\n    '''\n    model_dict = torch.load(model_path, map_location=lambda storage, loc: storage)\n    if exclude_modules:\n        for e in exclude_modules.split(\",\"):\n            model_dict = {k: v for k, v in model_dict.items() if not k.startswith(e)}\n\n    if not modules_to_load:\n        src_dict = model_dict\n    else:\n        src_dict = {}\n        for module in modules_to_load.split(\",\"):\n            src_dict.update({k: v for k, v in model_dict.items() if k.startswith(module)})\n    \n    dst_state = model.state_dict()\n    dst_state.update(src_dict)\n    model.load_state_dict(dst_state)\ndef torch_save(model, save_path, optimizer=None, local_rank=0):\n    if local_rank != 0:\n        return\n    if hasattr(model, \"module\"):\n        state_dict = model.module.state_dict() if not optimizer else collections.OrderedDict(model=model.module.state_dict(), optimizer=optimizer.state_dict())\n    else:\n        state_dict = model.state_dict() if not optimizer else collections.OrderedDict(model=model.state_dict(), optimizer=optimizer.state_dict())\n    torch.save(state_dict, save_path)\ndef torch_load(snapshot_path, model, optimizer=None):\n    # load snapshot\n    snapshot_dict = torch.load(snapshot_path, map_location=lambda storage, loc: storage)\n    if not \"model\" in snapshot_dict.keys():\n        model_dict = snapshot_dict\n        snapshot_dict = collections.OrderedDict(model=model_dict)\n    if hasattr(model, \"module\"):\n        model.module.load_state_dict(snapshot_dict[\"model\"])\n    else:\n        model.load_state_dict(snapshot_dict[\"model\"])\n    if optimizer:\n        optimizer.load_state_dict(snapshot_dict[\"optimizer\"])\n    del snapshot_dict\n\n# Decoding\ndef compute_wer(ref, hyp, normalize=False):\n    \"\"\"Compute Word Error Rate.\n        [Reference]\n            https://martin-thoma.com/word-error-rate-calculation/\n    Args:\n        ref (list): words in the reference transcript\n        hyp (list): words in the predicted transcript\n        normalize (bool, optional): if True, divide by the length of ref\n    Returns:\n        wer (float): Word Error Rate between ref and hyp\n        n_sub (int): the number of substitution\n        n_ins (int): the number of insertion\n        n_del (int): the number of deletion\n    \"\"\"\n    # Initialisation\n    d = np.zeros((len(ref) + 1) * (len(hyp) + 1), dtype=np.uint16)\n    d = d.reshape((len(ref) + 1, len(hyp) + 1))\n    for i in range(len(ref) + 1):\n        for j in range(len(hyp) + 1):\n            if i == 0:\n                d[0][j] = j\n            elif j == 0:\n                d[i][0] = i\n\n    # Computation\n    for i in range(1, len(ref) + 1):\n        for j in range(1, len(hyp) + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                d[i][j] = d[i - 1][j - 1]\n            else:\n                sub_tmp = d[i - 1][j - 1] + 1\n                ins_tmp = d[i][j - 1] + 1\n                del_tmp = d[i - 1][j] + 1\n                d[i][j] = min(sub_tmp, ins_tmp, del_tmp)\n\n    wer = d[len(ref)][len(hyp)]\n\n    # Find out the manipulation steps\n    x = len(ref)\n    y = len(hyp)\n    error_list = []\n    while True:\n        if x == 0 and y == 0:\n            break\n        else:\n            if x > 0 and y > 0:\n                if d[x][y] == d[x - 1][y - 1] and ref[x - 1] == hyp[y - 1]:\n                    error_list.append(\"C\")\n                    x = x - 1\n                    y = y - 1\n                elif d[x][y] == d[x][y - 1] + 1:\n                    error_list.append(\"I\")\n                    y = y - 1\n                elif d[x][y] == d[x - 1][y - 1] + 1:\n                    error_list.append(\"S\")\n                    x = x - 1\n                    y = y - 1\n                else:\n                    error_list.append(\"D\")\n                    x = x - 1\n            elif x == 0 and y > 0:\n                if d[x][y] == d[x][y - 1] + 1:\n                    error_list.append(\"I\")\n                    y = y - 1\n                else:\n                    error_list.append(\"D\")\n                    x = x - 1\n            elif y == 0 and x > 0:\n                error_list.append(\"D\")\n                x = x - 1\n            else:\n                raise ValueError\n\n    n_sub = error_list.count(\"S\")\n    n_ins = error_list.count(\"I\")\n    n_del = error_list.count(\"D\")\n    n_cor = error_list.count(\"C\")\n\n    assert wer == (n_sub + n_ins + n_del)\n    assert n_cor == (len(ref) - n_sub - n_del)\n\n    if normalize:\n        wer /= len(ref)\n    return wer, n_sub, n_ins, n_del, n_cor\n\ndef token2text(tokens, bpemodel=None):\n    if bpemodel:\n        text = bpemodel.decode_pieces(tokens)\n    else:\n        text = (\n            \" \".join(tokens)\n            .replace(\" \", \"\")\n            .replace(\"<space>\", \" \")\n        )  # sclite does not consider the number of spaces when splitting\n    return text\ndef recognize_and_evaluate(dataloader, model, args, model_path=None, wer=False, write_to_json=False):\n    if model_path:\n        torch_load(model_path, model)\n    orig_model = model\n    if hasattr(model, \"module\"):\n        model = model.module\n    if write_to_json:\n        # read json data\n        assert args.result_label and args.recog_json\n        with open(args.recog_json, \"rb\") as f:\n            js = json.load(f)[\"utts\"]\n            new_js = {}\n    model.eval()\n    recog_args = {\n        \"beam_size\": args.beam_size,\n        \"penalty\": args.penalty,\n        \"ctc_weight\": args.ctc_weight,\n        \"maxlenratio\": args.maxlenratio,\n        \"minlenratio\": args.minlenratio,\n        \"lm_weight\": args.lm_weight,\n        \"rnnlm\": args.rnnlm,\n        \"nbest\": args.nbest,\n        \"space\": args.sym_space,\n        \"blank\": args.sym_blank,\n    }\n    recog_args = argparse.Namespace(**recog_args)\n\n    #progress_bar = tqdm(dataloader)\n    #progress_bar.set_description(\"Testing CER/WERs\")\n    err_dict = (\n        dict(cer=None)\n        if not wer\n        else dict(cer=collections.defaultdict(int), wer=collections.defaultdict(int))\n    )\n    with torch.no_grad():\n        for batch_idx, data in enumerate(dataloader):\n            logging.warning(f\"Testing CER/WERs: {batch_idx+1}/{len(dataloader)}\")\n            fbank, ilens, tokens = data\n            fbanks = []\n            for i, fb in enumerate(fbank):\n                fbanks.append(fb[: ilens[i], :])\n            fbank = fbanks\n            nbest_hyps = model.recognize_batch(\n               fbank, recog_args, char_list=None, rnnlm=None\n            )\n            y_hats = [nbest_hyp[0][\"yseq\"][1:-1] for nbest_hyp in nbest_hyps]\n            if write_to_json:\n                for utt_idx in range(len(fbank)):\n                    name = dataloader.dataset[batch_idx][utt_idx][0]\n                    new_js[name] = add_results_to_json(\n                        js[name], nbest_hyps[utt_idx], args.char_list\n                    )\n            for i, y_hat in enumerate(y_hats):\n                y_true = tokens[i]\n\n                hyp_token = [\n                    args.char_list[int(idx)] for idx in y_hat if int(idx) != -1\n                ]\n                ref_token = [\n                    args.char_list[int(idx)] for idx in y_true if int(idx) != -1\n                ]\n                for key in sorted(err_dict.keys()):  # cer then wer\n                    if key == \"wer\":\n                        ref_token = token2text(ref_token, args.bpemodel).split()\n                        hyp_token = token2text(hyp_token, args.bpemodel).split()\n                        logging.debug(\"HYP: \" + str(hyp_token))\n                        logging.debug(\"REF: \" + str(ref_token))\n                    utt_err, utt_nsub, utt_nins, utt_ndel, utt_ncor = compute_wer(\n                        ref_token, hyp_token\n                    )\n\n                    err_dict[key][\"n_word\"] += len(ref_token)\n                    if utt_err != 0:\n                        err_dict[key][\"n_err\"] += utt_err  # Char / word error\n                        err_dict[key][\"n_ser\"] += 1  # Sentence error\n                    err_dict[key][\"n_cor\"] += utt_ncor\n                    err_dict[key][\"n_sub\"] += utt_nsub\n                    err_dict[key][\"n_ins\"] += utt_nins\n                    err_dict[key][\"n_del\"] += utt_ndel\n                    err_dict[key][\"n_sent\"] += 1\n\n    for key in err_dict.keys():\n        err_dict[key][\"err\"] = err_dict[key][\"n_err\"] / err_dict[key][\"n_word\"] * 100.0\n        err_dict[key][\"ser\"] = err_dict[key][\"n_ser\"] / err_dict[key][\"n_word\"] * 100.0\n    torch.cuda.empty_cache()\n    if write_to_json:\n        with open(args.result_label, \"wb\") as f:\n            f.write(\n                json.dumps(\n                    {\"utts\": new_js}, indent=4, ensure_ascii=False, sort_keys=True\n                ).encode(\"utf_8\")\n            )\n    model = orig_model\n    return err_dict", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "code/ASR/Adapter/utils.py", "language": "python", "complexity": "complex", "star_count": 14009, "quality_score": 1.0, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 297, "word_count": 1037}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with installation and setup instructions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# CMatch: Cross-domain Speech Recognition with Unsupervised Character-level Distribution Matching\r\n\r\n**NOTICE:** The latest code of CMatch has been moved to https://github.com/microsoft/NeuralSpeech/tree/master/CMatchASR for unified organization. Please refer to that repo.\r\n\r\n---\r\n\r\nThis project implements our paper [Cross-domain Speech Recognition with Unsupervised Character-level Distribution Matching](https://arxiv.org/abs/2104.07491) based on [EasyEspnet](https://github.com/jindongwang/EasyEspnet). Please refer to [EasyEspnet](https://github.com/jindongwang/EasyEspnet) for the program introduction, installation and usage. And our paper [1] for the method and technical details.\r\n\r\n## Run\r\n\r\nAfter extracting features using Espnet and set the data folder path as introduced in [EasyEspnet](https://github.com/jindongwang/EasyEspnet).\r\n\r\nYou need to check or modify in `train.py arg_list`, config should be in ESPnet config style (remember to include decoding information if you want to compute cer/wer), then, you can run train.py. For example, \r\n\r\n```\r\npython train.py --root_path libriadapt/asr1 --dataset libriadapt_en_us_clean_matrix --config config/train.yaml\r\n```\r\n\r\nThis commands pre-trains an ASR model on the specified dataset. Results (log, model, snapshots) are saved in results_(dataset)/(config_name) by default.\r\n\r\n### Cross-domain Adaptation\r\n\r\nWe provide three methods for cross-domain adaptation in this implementation: adversarial training (adv), maximum mean discrepancy (mmd) and our CMatch.\r\n\r\nFor the adversarial and mmd, you can refer to and modify the target dataset, pre-trained model setting in `config/{mmd, adv}_example.yaml`, and then execute this command for example:\r\n\r\n```\r\npython train.py --root_path libriadapt/asr1 --dataset libriadapt_en_us_clean_matrix --config config/mmd_example.yaml\r\n```\r\n\r\nFor the CMatch method, there are two steps.\r\n\r\n#### Step 1:\r\nFirst we need to obtain the pseudo labels, for example, suppose we trained the source model using the command above, we need to re-execute it with the `--pseudo_labeling` being set `true` and the `--tgt_dataset` specifying the target dataset:\r\n\r\n```\r\npython train.py --root_path libriadapt/asr1 --dataset libriadapt_en_us_clean_matrix --config config/train.yaml --pseudo_labeling true --tgt_dataset libriadapt_en_us_clean_respeaker\r\n```\r\n\r\nSo that we will get the pseudo labels for `libriadapt_en_us_clean_respeaker` dataset.\r\n\r\n#### Step 2:\r\nThen we can refer to and modify the target dataset, pre-trained model, non-character symbol, and pseudo label setting in `config/{ctc_align, pseudo_ctc_pred, frame_average}_example.yaml`, and then execute this command for example:\r\n```\r\npython train.py --root_path libriadapt/asr1 --dataset libriadapt_en_us_clean_matrix --config config/ctc_align_example.yaml\r\n```\r\n\r\n### Demo\r\n\r\nTBD\r\n\r\n## Decoding and WER/CER evaluation\r\n\r\nSet `--decoding_mode` to `true` to perform decoding and CER/WER evaluation. For example:\r\n\r\n```\r\npython train.py --root_path libriadapt/asr1 --dataset libriadapt_en_us_clean_matrix --config config/train.yaml --decoding_mode true\r\n```\r\n\r\n## Distributed training\r\n\r\nFollowing EasyEspnet, you can also perform distributed training which is much faster. For example, using 4 GPUs, 1 node: \r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 train.py --dist_train true --root_path libriadapt/asr1 --dataset libriadapt_en_us_clean_matrix --config config/mmd_example.yaml\r\n```\r\n\r\n## Acknowledgement\r\n\r\n- ESPNet: https://github.com/espnet/espnet\r\n- EasyEspnet: https://github.com/jindongwang/EasyEspnet\r\n- NeuralSP: https://github.com/hirofumi0810/neural_sp\r\n\r\n## Contact\r\n\r\n- [Wenxin Hou](https://houwenxin.github.io/): houwx001@gmail.com\r\n- [Jindong Wang](http://www.jd92.wang/): jindongwang@outlook.com\r\n\r\n\r\n\r\n## References\r\n\r\n[1] Wenxin Hou, Jindong Wang, Xu Tan, Tao Qin, Takahiro Shinozaki, \"Cross-domain Speech Recognition with Unsupervised Character-level Distribution Matching\", submitted to INTERSPEECH 2021.\r\n", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "code/ASR/CMatch/README.md", "language": "markdown", "complexity": "medium", "star_count": 14009, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 84, "word_count": 440}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# network architecture\n# encoder related\nelayers: 12\neunits: 2048\n# decoder related\ndlayers: 6\ndunits: 2048\n# attention related\nadim: 256\naheads: 4\n\n# hybrid CTC/attention\nmtlalpha: 0.3\n\n# label smoothing\nlsm-weight: 0.1\n\n# minibatch related\nbatch-size: 32 # There are two datasets, reduce batch size by half\ntransformer-lr: 6.0\n#batch-size: 64\n#transformer-lr: 10.0\nmaxlen-in: 512  # if input length  > maxlen-in, batchsize is automatically reduced\nmaxlen-out: 150 # if output length > maxlen-out, batchsize is automatically reduced\n\n# optimization related\nsortagrad: 0 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs\nopt: noam\naccum-grad: 1\ngrad-clip: 5\npatience: 5\nepochs: 100\ndropout-rate: 0.1\n\n# transformer specific setting\nbackend: pytorch\nmodel-module: \"espnet.nets.pytorch_backend.e2e_asr_transformer:E2E\"\ntransformer-input-layer: conv2d     # encoder architecture type\n#transformer-lr: 6.0\ntransformer-warmup-steps: 25000\ntransformer-attn-dropout-rate: 0.0\ntransformer-length-normalized-loss: false\ntransformer-init: pytorch\n\n\n# Decoding related\nbeam-size: 10\npenalty: 0.0\nmaxlenratio: 0.0\nminlenratio: 0.0\nctc-weight: 0.3\nlm-weight: 0.7\n\n# Transfer related\nload_pretrained_model: <pretrained_model:modules_to_load:excluded_modules> # Example: <path_to_source_model>/model.loss.best::\ntgt_dataset: <target_dataset_to_adapt_to>\n\ntransfer_type: adv\ntransfer_loss_weight: 0.3", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "code/ASR/CMatch/config/adv_example.yaml", "language": "yaml", "complexity": "medium", "star_count": 14009, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 59, "word_count": 169}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# network architecture\n# encoder related\nelayers: 12\neunits: 2048\n# decoder related\ndlayers: 6\ndunits: 2048\n# attention related\nadim: 256\naheads: 4\n\n# hybrid CTC/attention\nmtlalpha: 0.3\n\n# label smoothing\nlsm-weight: 0.1\n\n# minibatch related\nbatch-size: 32 # There are two datasets, reduce batch size by half\ntransformer-lr: 6.0\n#batch-size: 64\n#transformer-lr: 10.0\nmaxlen-in: 512  # if input length  > maxlen-in, batchsize is automatically reduced\nmaxlen-out: 150 # if output length > maxlen-out, batchsize is automatically reduced\n\n# optimization related\nsortagrad: 0 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs\nopt: noam\naccum-grad: 1\ngrad-clip: 5\npatience: 5\nepochs: 100\ndropout-rate: 0.1\n\n# transformer specific setting\nbackend: pytorch\nmodel-module: \"espnet.nets.pytorch_backend.e2e_asr_transformer:E2E\"\ntransformer-input-layer: conv2d     # encoder architecture type\n#transformer-lr: 6.0\ntransformer-warmup-steps: 25000\ntransformer-attn-dropout-rate: 0.0\ntransformer-length-normalized-loss: false\ntransformer-init: pytorch\n\n\n# Decoding related\nbeam-size: 10\npenalty: 0.0\nmaxlenratio: 0.0\nminlenratio: 0.0\nctc-weight: 0.3\nlm-weight: 0.7\n\n# Transfer related\nload_pretrained_model: <pretrained_model:modules_to_load:excluded_modules> # Example: <path_to_source_model>/model.loss.best::\ntgt_dataset: <target_dataset_to_adapt_to>\nnon_char_symbols: <non_character_symbols_to_filter_in_cmatch> # Example: 0_1_2_29_30\ntransfer_type: cmatch\ntransfer_loss_weight: 10.0\ncmatch_method: ctc_align\ntranfer_loss_weight_warmup_steps: 0\nself_training: True\npseudo_label_json: <pseudo_label_json_of_target_dataset>", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "code/ASR/CMatch/config/ctc_align_example.yaml", "language": "yaml", "complexity": "medium", "star_count": 14009, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 63, "word_count": 182}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# network architecture\n# encoder related\nelayers: 12\neunits: 2048\n# decoder related\ndlayers: 6\ndunits: 2048\n# attention related\nadim: 256\naheads: 4\n\n# hybrid CTC/attention\nmtlalpha: 0.3\n\n# label smoothing\nlsm-weight: 0.1\n\n# minibatch related\nbatch-size: 32 # There are two datasets, reduce batch size by half\ntransformer-lr: 6.0\n#batch-size: 64\n#transformer-lr: 10.0\nmaxlen-in: 512  # if input length  > maxlen-in, batchsize is automatically reduced\nmaxlen-out: 150 # if output length > maxlen-out, batchsize is automatically reduced\n\n# optimization related\nsortagrad: 0 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs\nopt: noam\naccum-grad: 1\ngrad-clip: 5\npatience: 5\nepochs: 100\ndropout-rate: 0.1\n\n# transformer specific setting\nbackend: pytorch\nmodel-module: \"espnet.nets.pytorch_backend.e2e_asr_transformer:E2E\"\ntransformer-input-layer: conv2d     # encoder architecture type\n#transformer-lr: 6.0\ntransformer-warmup-steps: 25000\ntransformer-attn-dropout-rate: 0.0\ntransformer-length-normalized-loss: false\ntransformer-init: pytorch\n\n\n# Decoding related\nbeam-size: 10\npenalty: 0.0\nmaxlenratio: 0.0\nminlenratio: 0.0\nctc-weight: 0.3\nlm-weight: 0.7\n\n# Transfer related\nload_pretrained_model: <pretrained_model:modules_to_load:excluded_modules> # Example: <path_to_source_model>/model.loss.best::\ntgt_dataset: <target_dataset_to_adapt_to>\nnon_char_symbols: <non_character_symbols_to_filter_in_cmatch> # Example: 0_1_2_29_30\ntransfer_type: cmatch\ntransfer_loss_weight: 10.0\ncmatch_method: frame_average\nself_training: True\npseudo_label_json: <pseudo_label_json_of_target_dataset>", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "code/ASR/CMatch/config/frame_average_example.yaml", "language": "yaml", "complexity": "medium", "star_count": 14009, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 62, "word_count": 180}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# network architecture\n# encoder related\nelayers: 12\neunits: 2048\n# decoder related\ndlayers: 6\ndunits: 2048\n# attention related\nadim: 256\naheads: 4\n\n# hybrid CTC/attention\nmtlalpha: 0.3\n\n# label smoothing\nlsm-weight: 0.1\n\n# minibatch related\nbatch-size: 32 # There are two datasets, reduce batch size by half\ntransformer-lr: 6.0\n#batch-size: 64\n#transformer-lr: 10.0\nmaxlen-in: 512  # if input length  > maxlen-in, batchsize is automatically reduced\nmaxlen-out: 150 # if output length > maxlen-out, batchsize is automatically reduced\n\n# optimization related\nsortagrad: 0 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs\nopt: noam\naccum-grad: 1\ngrad-clip: 5\npatience: 5\nepochs: 100\ndropout-rate: 0.1\n\n# transformer specific setting\nbackend: pytorch\nmodel-module: \"espnet.nets.pytorch_backend.e2e_asr_transformer:E2E\"\ntransformer-input-layer: conv2d     # encoder architecture type\n#transformer-lr: 6.0\ntransformer-warmup-steps: 25000\ntransformer-attn-dropout-rate: 0.0\ntransformer-length-normalized-loss: false\ntransformer-init: pytorch\n\n\n# Decoding related\nbeam-size: 10\npenalty: 0.0\nmaxlenratio: 0.0\nminlenratio: 0.0\nctc-weight: 0.3\nlm-weight: 0.7\n\n# Transfer related\nload_pretrained_model: <pretrained_model:modules_to_load:excluded_modules> # Example: <path_to_source_model>/model.loss.best::\ntgt_dataset: <target_dataset_to_adapt_to>\n\ntransfer_type: mmd\ntransfer_loss_weight: 10.0", "metadata": {"source": "github.com/jindongwang/transferlearning", "file_path": "code/ASR/CMatch/config/mmd_example.yaml", "language": "yaml", "complexity": "medium", "star_count": 14009, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 59, "word_count": 169}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Introduction\n# Day 1 - 30DaysOfPython Challenge\n\nprint(3 + 2)   # addition(+)\nprint(3 - 2)   # subtraction(-)\nprint(3 * 2)   # multiplication(*)\nprint(3 / 2)   # division(/)\nprint(3 ** 2)  # exponential(**)\nprint(3 % 2)   # modulus(%)\nprint(3 // 2)  # Floor division operator(//)\n\n# Checking data types\n\nprint(type(10))                  # Int\nprint(type(3.14))                # Float\nprint(type(1 + 3j))              # Complex\nprint(type('Asabeneh'))          # String\nprint(type([1, 2, 3]))           # List\nprint(type({'name':'Asabeneh'})) # Dictionary\nprint(type({9.8, 3.14, 2.7}))    # Set\nprint(type((9.8, 3.14, 2.7)))    # Tuple\nprint(type(3 == 3))              # Bool\nprint(type(3 >= 3))              # Bool\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "01_Day_Introduction/helloworld.py", "language": "python", "complexity": "simple", "star_count": 47914, "quality_score": 0.8, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 24, "word_count": 91}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 2 - Variables, Builtin Functions</h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n<sub>Author:\n<a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n<small> Second Edition: July, 2021</small>\n</sub>\n\n</div>\n\n[<< Day 1](../readme.md) | [Day 3 >>](../03_Day_Operators/03_operators.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n\n- [üìò Day 2](#-day-2)\n  - [Built in functions](#built-in-functions)\n  - [Variables](#variables)\n    - [Declaring Multiple Variable in a Line](#declaring-multiple-variable-in-a-line)\n  - [Data Types](#data-types)\n  - [Checking Data types and Casting](#checking-data-types-and-casting)\n  - [Numbers](#numbers)\n  - [üíª Exercises - Day 2](#-exercises---day-2)\n    - [Exercises: Level 1](#exercises-level-1)\n    - [Exercises: Level 2](#exercises-level-2)\n\n# üìò Day 2\n\n## Built in functions\n\nIn Python we have lots of built-in functions. Built-in functions are globally available for your use that mean you can make use of the built-in functions without importing or configuring. Some of the most commonly used Python built-in functions are the following: _print()_, _len()_, _type()_, _int()_, _float()_, _str()_, _input()_, _list()_, _dict()_, _min()_, _max()_, _sum()_, _sorted()_, _open()_, _file()_, _help()_, and _dir()_. In the following table you will see an exhaustive list of Python built-in functions taken from [python documentation](https://docs.python.org/3.9/library/functions.html).\n\n![Built-in Functions](../images/builtin-functions.png)\n\nLet us open the Python shell and start using some of the most common built-in functions.\n\n![Built-in functions](../images/builtin-functions_practice.png)\n\nLet us practice more by using different built-in functions\n\n![Help and Dir Built in Functions](../images/help_and_dir_builtin.png)\n\nAs you can see from the terminal above, Python has got reserved words. We do not use reserved words to declare variables or functions. We will cover variables in the next section.\n\nI believe, by now you are familiar with built-in functions. Let us do one more practice of built-in functions and we will move on to the next section.\n\n![Min Max Sum](../images/builtin-functional-final.png)\n\n## Variables\n\nVariables store data in a computer memory. Mnemonic variables are recommended to use in many programming languages. A mnemonic variable is a variable name that can be easily remembered and associated. A variable refers to a memory address in which data is stored.\nNumber at the beginning, special character, hyphen are not allowed when naming a variable. A variable can have a short name (like x, y, z), but a more descriptive name (firstname, lastname, age, country) is highly recommended.\n\nPython Variable Name Rules\n\n- A variable name must start with a letter or the underscore character\n- A variable name cannot start with a number\n- A variable name can only contain alpha-numeric characters and underscores (A-z, 0-9, and \\_ )\n- Variable names are case-sensitive (firstname, Firstname, FirstName and FIRSTNAME) are different variables)\n\nHere are some example of valid variable names:\n\n```shell\nfirstname\nlastname\nage\ncountry\ncity\nfirst_name\nlast_name\ncapital_city\n_if # if we want to use reserved word as a variable\nyear_2021\nyear2021\ncurrent_year_2021\nbirth_year\nnum1\nnum2\n```\n\nInvalid variables names\n\n```shell\nfirst-name\nfirst@name\nfirst$name\nnum-1\n1num\n```\n\nWe will use standard Python variable naming style which has been adopted by many Python developers. Python developers use snake case(snake_case) variable naming convention. We use underscore character after each word for a variable containing more than one word(eg. first_name, last_name, engine_rotation_speed).  The example below is an example of standard naming of variables, underscore is required when the variable name is more than one word.\n\nWhen we assign a certain data type to a variable, it is called variable declaration. For instance in the example below my first name is assigned to a variable first_name. The equal sign is an assignment operator. Assigning means storing data in the variable. The equal sign in Python is not equality as in Mathematics.\n\n_Example:_\n\n```py\n# Variables in Python\nfirst_name = 'Asabeneh'\nlast_name = 'Yetayeh'\ncountry = 'Finland'\ncity = 'Helsinki'\nage = 250\nis_married = True\nskills = ['HTML', 'CSS', 'JS', 'React', 'Python']\nperson_info = {\n   'firstname':'Asabeneh',\n   'lastname':'Yetayeh',\n   'country':'Finland',\n   'city':'Helsinki'\n   }\n```\n\nLet us use the _print()_ and _len()_ built-in functions. Print function takes unlimited number of arguments. An argument is a value which we can be passed or put inside the function parenthesis, see the example below.\n\n**Example:**\n\n```py\nprint('Hello, World!') # The text Hello, World! is an argument\nprint('Hello',',', 'World','!') # it can take multiple arguments, four arguments have been passed\nprint(len('Hello, World!')) # it takes only one argument\n```\n\nLet us print and also find the length of the variables declared at the top:\n\n**Example:**\n\n```py\n# Printing the values stored in the variables\n\nprint('First name:', first_name)\nprint('First name length:', len(first_name))\nprint('Last name: ', last_name)\nprint('Last name length: ', len(last_name))\nprint('Country: ', country)\nprint('City: ', city)\nprint('Age: ', age)\nprint('Married: ', is_married)\nprint('Skills: ', skills)\nprint('Person information: ', person_info)\n```\n\n### Declaring Multiple Variable in a Line\n\nMultiple variables can also be declared in one line:\n\n**Example:**\n\n```py\nfirst_name, last_name, country, age, is_married = 'Asabeneh', 'Yetayeh', 'Helsink', 250, True\n\nprint(first_name, last_name, country, age, is_married)\nprint('First name:', first_name)\nprint('Last name: ', last_name)\nprint('Country: ', country)\nprint('Age: ', age)\nprint('Married: ', is_married)\n```\n\nGetting user input using the _input()_ built-in function. Let us assign the data we get from a user into first_name and age variables.\n**Example:**\n\n```py\nfirst_name = input('What is your name: ')\nage = input('How old are you? ')\n\nprint(first_name)\nprint(age)\n```\n\n## Data Types\n\nThere are several data types in Python. To identify the data type we use the _type_ built-in function. I would like to ask you to focus on understanding different data types very well. When it comes to programming, it is all about data types. I introduced data types at the very beginning and it comes again, because every topic is related to data types. We will cover data types in more detail in their respective sections.\n\n## Checking Data types and Casting\n\n- Check Data types: To check the data type of certain data/variable we use the _type_\n  **Examples:**\n\n```py\n# Different python data types\n# Let's declare variables with various data types\n\nfirst_name = 'Asabeneh'     # str\nlast_name = 'Yetayeh'       # str\ncountry = 'Finland'         # str\ncity= 'Helsinki'            # str\nage = 250                   # int, it is not my real age, don't worry about it\n\n# Printing out types\nprint(type('Asabeneh'))          # str\nprint(type(first_name))          # str\nprint(type(10))                  # int\nprint(type(3.14))                # float\nprint(type(1 + 1j))              # complex\nprint(type(True))                # bool\nprint(type([1, 2, 3, 4]))        # list\nprint(type({'name':'Asabeneh'})) # dict\nprint(type((1,2)))               # tuple\nprint(type(zip([1,2],[3,4])))    # zip\n```\n\n- Casting: Converting one data type to another data type. We use _int()_, _float()_, _str()_, _list_, _set_\n  When we do arithmetic operations string numbers should be first converted to int or float otherwise it will return an error. If we concatenate a number with a string, the number should be first converted to a string. We will talk about concatenation in String section.\n\n  **Examples:**\n\n```py\n# int to float\nnum_int = 10\nprint('num_int',num_int)         # 10\nnum_float = float(num_int)\nprint('num_float:', num_float)   # 10.0\n\n# float to int\ngravity = 9.81\nprint(int(gravity))             # 9\n\n# int to str\nnum_int = 10\nprint(num_int)                  # 10\nnum_str = str(num_int)\nprint(num_str)                  # '10'\n\n# str to int or float\nnum_str = '10.6'\nnum_float = float(num_str)\nprint('num_float', float(num_str))  # 10.6\nnum_int = int(num_float)\nprint('num_int', int(num_int))      # 10\n\n# str to list\nfirst_name = 'Asabeneh'\nprint(first_name)               # 'Asabeneh'\nfirst_name_to_list = list(first_name)\nprint(first_name_to_list)            # ['A', 's', 'a', 'b', 'e', 'n', 'e', 'h']\n```\n\n## Numbers\n\nNumber data types in Python:\n\n1. Integers: Integer(negative, zero and positive) numbers\n   Example:\n   ... -3, -2, -1, 0, 1, 2, 3 ...\n\n2. Floating Point Numbers(Decimal numbers)\n   Example:\n   ... -3.5, -2.25, -1.0, 0.0, 1.1, 2.2, 3.5 ...\n\n3. Complex Numbers\n   Example:\n   1 + j, 2 + 4j, 1 - 1j\n\nüåï You are awesome. You have just completed day 2 challenges and you are two steps ahead on your way to greatness. Now do some exercises for your brain and muscles.\n\n## üíª Exercises - Day 2\n\n### Exercises: Level 1\n\n1. Inside 30DaysOfPython create a folder called day_2. Inside this folder create a file named variables.py\n2. Write a python comment saying 'Day 2: 30 Days of python programming'\n3. Declare a first name variable and assign a value to it\n4. Declare a last name variable and assign a value to it\n5. Declare a full name variable and assign a value to it\n6. Declare a country variable and assign a value to it\n7. Declare a city variable and assign a value to it\n8. Declare an age variable and assign a value to it\n9. Declare a year variable and assign a value to it\n10. Declare a variable is_married and assign a value to it\n11. Declare a variable is_true and assign a value to it\n12. Declare a variable is_light_on and assign a value to it\n13. Declare multiple variable on one line\n\n### Exercises: Level 2\n\n1. Check the data type of all your variables using type() built-in function\n1. Using the _len()_ built-in function, find the length of your first name\n1. Compare the length of your first name and your last name\n1. Declare 5 as num_one and 4 as num_two\n1. Add num_one and num_two and assign the value to a variable total\n1. Subtract num_two from num_one and assign the value to a variable diff\n1. Multiply num_two and num_one and assign the value to a variable product\n1. Divide num_one by num_two and assign the value to a variable division\n1. Use modulus division to find num_two divided by num_one and assign the value to a variable remainder\n1. Calculate num_one to the power of num_two and assign the value to a variable exp\n1. Find floor division of num_one by num_two and assign the value to a variable floor_division\n1. The radius of a circle is 30 meters.\n    1. Calculate the area of a circle and assign the value to a variable name of _area_of_circle_\n    2. Calculate the circumference of a circle and assign the value to a variable name of _circum_of_circle_\n    3. Take radius as user input and calculate the area.\n1. Use the built-in input function to get first name, last name, country and age from a user and store the value to their corresponding variable names\n1. Run help('keywords') in Python shell or in your file to check for the Python reserved words or keywords\n\nüéâ CONGRATULATIONS ! üéâ\n\n[<< Day 1](../readme.md) | [Day 3 >>](../03_Day_Operators/03_operators.md)\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "02_Day_Variables_builtin_functions/02_variables_builtin_functions.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.89, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 303, "word_count": 1692}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 3 - Operators</h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n<sub>Author:\n<a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n<small> Second Edition: July, 2021</small>\n</sub>\n</div>\n\n[<< Day 2](../02_Day_Variables_builtin_functions/02_variables_builtin_functions.md) | [Day 4 >>](../04_Day_Strings/04_strings.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n\n- [üìò Day 3](#-day-3)\n  - [Boolean](#boolean)\n  - [Operators](#operators)\n    - [Assignment Operators](#assignment-operators)\n    - [Arithmetic Operators:](#arithmetic-operators)\n    - [Comparison Operators](#comparison-operators)\n    - [Logical Operators](#logical-operators)\n  - [üíª Exercises - Day 3](#-exercises---day-3)\n\n# üìò Day 3\n\n## Boolean\n\nA boolean data type represents one of the two values: _True_ or _False_. The use of these data types will be clear once we start using the comparison operator. The first letter **T** for True and **F** for False should be capital unlike JavaScript.\n**Example: Boolean Values**\n\n```py\nprint(True)\nprint(False)\n```\n\n## Operators\n\nPython language supports several types of operators. In this section, we will focus on few of them.\n\n### Assignment Operators\n\nAssignment operators are used to assign values to variables. Let us take = as an example. Equal sign in mathematics shows that two values are equal, however in Python it means we are storing a value in a certain variable and we call it assignment or a assigning value to a variable. The table below shows the different types of python assignment operators, taken from [w3school](https://www.w3schools.com/python/python_operators.asp).\n\n![Assignment Operators](../images/assignment_operators.png)\n\n### Arithmetic Operators:\n\n- Addition(+): a + b\n- Subtraction(-): a - b\n- Multiplication(*): a * b\n- Division(/): a / b\n- Modulus(%): a % b\n- Floor division(//): a // b\n- Exponentiation(**): a ** b\n\n![Arithmetic Operators](../images/arithmetic_operators.png)\n\n**Example:Integers**\n\n```py\n# Arithmetic Operations in Python\n# Integers\n\nprint('Addition: ', 1 + 2)        # 3\nprint('Subtraction: ', 2 - 1)     # 1\nprint('Multiplication: ', 2 * 3)  # 6\nprint ('Division: ', 4 / 2)       # 2.0  Division in Python gives floating number\nprint('Division: ', 6 / 2)        # 3.0         \nprint('Division: ', 7 / 2)        # 3.5\nprint('Division without the remainder: ', 7 // 2)   # 3,  gives without the floating number or without the remaining\nprint ('Division without the remainder: ',7 // 3)   # 2\nprint('Modulus: ', 3 % 2)         # 1, Gives the remainder\nprint('Exponentiation: ', 2 ** 3) # 9 it means 2 * 2 * 2\n```\n\n**Example:Floats**\n\n```py\n# Floating numbers\nprint('Floating Point Number, PI', 3.14)\nprint('Floating Point Number, gravity', 9.81)\n```\n\n**Example:Complex numbers**\n\n```py\n# Complex numbers\nprint('Complex number: ', 1 + 1j)\nprint('Multiplying complex numbers: ',(1 + 1j) * (1 - 1j))\n```\n\nLet's declare a variable and assign a number data type. I am going to use single character variable but remember do not develop a habit of declaring such types of variables. Variable names should be all the time mnemonic.\n\n**Example:**\n\n```python\n# Declaring the variable at the top first\n\na = 3 # a is a variable name and 3 is an integer data type\nb = 2 # b is a variable name and 3 is an integer data type\n\n# Arithmetic operations and assigning the result to a variable\ntotal = a + b\ndiff = a - b\nproduct = a * b\ndivision = a / b\nremainder = a % b\nfloor_division = a // b\nexponential = a ** b\n\n# I should have used sum instead of total but sum is a built-in function - try to avoid overriding built-in functions\nprint(total) # if you do not label your print with some string, you never know where the result is coming from\nprint('a + b = ', total)\nprint('a - b = ', diff)\nprint('a * b = ', product)\nprint('a / b = ', division)\nprint('a % b = ', remainder)\nprint('a // b = ', floor_division)\nprint('a ** b = ', exponentiation)\n```\n\n**Example:**\n\n```py\nprint('== Addition, Subtraction, Multiplication, Division, Modulus ==')\n\n# Declaring values and organizing them together\nnum_one = 3\nnum_two = 4\n\n# Arithmetic operations\ntotal = num_one + num_two\ndiff = num_two - num_one\nproduct = num_one * num_two\ndiv = num_two / num_one\nremainder = num_two % num_one\n\n# Printing values with label\nprint('total: ', total)\nprint('difference: ', diff)\nprint('product: ', product)\nprint('division: ', div)\nprint('remainder: ', remainder)\n```\n\nLet us start start connecting the dots and start making use of what we already know to calculate (area, volume,density,  weight, perimeter, distance, force).\n\n**Example:**\n\n```py\n# Calculating area of a circle\nradius = 10                                 # radius of a circle\narea_of_circle = 3.14 * radius ** 2         # two * sign means exponent or power\nprint('Area of a circle:', area_of_circle)\n\n# Calculating area of a rectangle\nlength = 10\nwidth = 20\narea_of_rectangle = length * width\nprint('Area of rectangle:', area_of_rectangle)\n\n# Calculating a weight of an object\nmass = 75\ngravity = 9.81\nweight = mass * gravity\nprint(weight, 'N')                         # Adding unit to the weight\n\n# Calculate the density of a liquid\nmass = 75 # in Kg\nvolume = 0.075 # in cubic meter\ndensity = mass / volume # 1000 Kg/m^3\n\n```\n\n### Comparison Operators\n\nIn programming we compare values, we use comparison operators to compare two values. We check if a value is greater or less or equal to other value. The following table shows Python comparison operators which was taken from [w3shool](https://www.w3schools.com/python/python_operators.asp).\n\n![Comparison Operators](../images/comparison_operators.png)\n**Example: Comparison Operators**\n\n```py\nprint(3 > 2)     # True, because 3 is greater than 2\nprint(3 >= 2)    # True, because 3 is greater than 2\nprint(3 < 2)     # False,  because 3 is greater than 2\nprint(2 < 3)     # True, because 2 is less than 3\nprint(2 <= 3)    # True, because 2 is less than 3\nprint(3 == 2)    # False, because 3 is not equal to 2\nprint(3 != 2)    # True, because 3 is not equal to 2\nprint(len('mango') == len('avocado'))  # False\nprint(len('mango') != len('avocado'))  # True\nprint(len('mango') < len('avocado'))   # True\nprint(len('milk') != len('meat'))      # False\nprint(len('milk') == len('meat'))      # True\nprint(len('tomato') == len('potato'))  # True\nprint(len('python') > len('dragon'))   # False\n\n\n# Comparing something gives either a True or False\n\nprint('True == True: ', True == True)\nprint('True == False: ', True == False)\nprint('False == False:', False == False)\n```\n\nIn addition to the above comparison operator Python uses:\n\n- _is_: Returns true if both variables are the same object(x is y)\n- _is not_: Returns true if both variables are not the same object(x is not y)\n- _in_: Returns True if the queried list contains a certain item(x in y)\n- _not in_: Returns True if the queried list doesn't have a certain item(x in y)\n\n```py\nprint('1 is 1', 1 is 1)                   # True - because the data values are the same\nprint('1 is not 2', 1 is not 2)           # True - because 1 is not 2\nprint('A in Asabeneh', 'A' in 'Asabeneh') # True - A found in the string\nprint('B in Asabeneh', 'B' in 'Asabeneh') # False - there is no uppercase B\nprint('coding' in 'coding for all') # True - because coding for all has the word coding\nprint('a in an:', 'a' in 'an')      # True\nprint('4 is 2 ** 2:', 4 is 2 ** 2)   # True\n```\n\n### Logical Operators\n\nUnlike other programming languages python uses keywords _and_, _or_ and _not_ for logical operators. Logical operators are used to combine conditional statements:\n\n![Logical Operators](../images/logical_operators.png)\n\n```py\nprint(3 > 2 and 4 > 3) # True - because both statements are true\nprint(3 > 2 and 4 < 3) # False - because the second statement is false\nprint(3 < 2 and 4 < 3) # False - because both statements are false\nprint('True and True: ', True and True)\nprint(3 > 2 or 4 > 3)  # True - because both statements are true\nprint(3 > 2 or 4 < 3)  # True - because one of the statements is true\nprint(3 < 2 or 4 < 3)  # False - because both statements are false\nprint('True or False:', True or False)\nprint(not 3 > 2)     # False - because 3 > 2 is true, then not True gives False\nprint(not True)      # False - Negation, the not operator turns true to false\nprint(not False)     # True\nprint(not not True)  # True\nprint(not not False) # False\n\n```\n\nüåï You have boundless energy. You have just completed day 3 challenges and you are three steps ahead on your way to greatness. Now do some exercises for your brain and your muscles.\n\n## üíª Exercises - Day 3\n\n1. Declare your age as integer variable\n2. Declare your height as a float variable\n3. Declare a variable that store a complex number\n4. Write a script that prompts the user to enter base and height of the triangle and calculate an area of this triangle (area = 0.5 x b x h).\n\n```py\n    Enter base: 20\n    Enter height: 10\n    The area of the triangle is 100\n```\n\n5. Write a script that prompts the user to enter side a, side b, and side c of the triangle. Calculate the perimeter of the triangle (perimeter = a + b + c).\n\n```py\nEnter side a: 5\nEnter side b: 4\nEnter side c: 3\nThe perimeter of the triangle is 12\n```\n\n6. Get length and width of a rectangle using prompt. Calculate its area (area = length x width) and perimeter (perimeter = 2 x (length + width))\n7. Get radius of a circle using prompt. Calculate the area (area = pi x r x r) and circumference (c = 2 x pi x r) where pi = 3.14.\n8. Calculate the slope, x-intercept and y-intercept of y = 2x -2\n9. Slope is (m = y2-y1/x2-x1). Find the slope and [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance#:~:text=In%20mathematics%2C%20the%20Euclidean%20distance,being%20called%20the%20Pythagorean%20distance.) between point (2, 2) and point (6,10) \n10. Compare the slopes in tasks 8 and 9.\n11. Calculate the value of y (y = x^2 + 6x + 9). Try to use different x values and figure out at what x value y is going to be 0.\n12. Find the length of 'python' and 'dragon' and make a falsy comparison statement.\n13. Use _and_ operator to check if 'on' is found in both 'python' and 'dragon'\n14. _I hope this course is not full of jargon_. Use _in_ operator to check if _jargon_ is in the sentence.\n15. There is no 'on' in both dragon and python\n16. Find the length of the text _python_ and convert the value to float and convert it to string\n17. Even numbers are divisible by 2 and the remainder is zero. How do you check if a number is even or not using python?\n18. Check if the floor division of 7 by 3 is equal to the int converted value of 2.7.\n19. Check if type of '10' is equal to type of 10\n20. Check if int('9.8') is equal to 10\n21. Writ a script that prompts the user to enter hours and rate per hour. Calculate pay of the person?\n\n```py\nEnter hours: 40\nEnter rate per hour: 28\nYour weekly earning is 1120\n```\n\n22. Write a script that prompts the user to enter number of years. Calculate the number of seconds a person can live. Assume a person can live hundred years\n\n```py\nEnter number of years you have lived: 100\nYou have lived for 3153600000 seconds.\n```\n\n23. Write a Python script that displays the following table\n\n```py\n1 1 1 1 1\n2 1 2 4 8\n3 1 3 9 27\n4 1 4 16 64\n5 1 5 25 125\n```\n\nüéâ CONGRATULATIONS ! üéâ\n\n[<< Day 2](../02_Day_Variables_builtin_functions/02_variables_builtin_functions.md) | [Day 4 >>](../04_Day_Strings/04_strings.md)\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "03_Day_Operators/03_operators.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 318, "word_count": 1932}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 4 - Strings</h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n<sub>Author:\n<a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n<small> Second Edition: July, 2021</small>\n</sub>\n\n</div>\n\n[<< Day 3](../03_Day_Operators/03_operators.md) | [Day 5 >>](../05_Day_Lists/05_lists.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n\n- [Day 4](#day-4)\n  - [Strings](#strings)\n    - [Creating a String](#creating-a-string)\n    - [String Concatenation](#string-concatenation)\n    - [Escape Sequences in Strings](#escape-sequences-in-strings)\n    - [String formatting](#string-formatting)\n      - [Old Style String Formatting (% Operator)](#old-style-string-formatting--operator)\n      - [New Style String Formatting (str.format)](#new-style-string-formatting-strformat)\n      - [String Interpolation / f-Strings (Python 3.6+)](#string-interpolation--f-strings-python-36)\n    - [Python Strings as Sequences of Characters](#python-strings-as-sequences-of-characters)\n      - [Unpacking Characters](#unpacking-characters)\n      - [Accessing Characters in Strings by Index](#accessing-characters-in-strings-by-index)\n      - [Slicing Python Strings](#slicing-python-strings)\n      - [Reversing a String](#reversing-a-string)\n      - [Skipping Characters While Slicing](#skipping-characters-while-slicing)\n    - [String Methods](#string-methods)\n  - [üíª Exercises - Day 4](#-exercises---day-4)\n\n# Day 4\n\n## Strings\n\nText is a string data type. Any data type written as text is a string. Any data under single, double or triple quote are strings. There are different string methods and built-in functions to deal with string data types. To check the length of a string use the len() method.\n\n### Creating a String\n\n```py\nletter = 'P'                # A string could be a single character or a bunch of texts\nprint(letter)               # P\nprint(len(letter))          # 1\ngreeting = 'Hello, World!'  # String could be made using a single or double quote,\"Hello, World!\"\nprint(greeting)             # Hello, World!\nprint(len(greeting))        # 13\nsentence = \"I hope you are enjoying 30 days of Python Challenge\"\nprint(sentence)\n```\n\nMultiline string is created by using triple single (''') or triple double quotes (\"\"\"). See the example below.\n\n```py\nmultiline_string = '''I am a teacher and enjoy teaching.\nI didn't find anything as rewarding as empowering people.\nThat is why I created 30 days of python.'''\nprint(multiline_string)\n\n# Another way of doing the same thing\nmultiline_string = \"\"\"I am a teacher and enjoy teaching.\nI didn't find anything as rewarding as empowering people.\nThat is why I created 30 days of python.\"\"\"\nprint(multiline_string)\n```\n\n### String Concatenation\n\nWe can connect strings together. Merging or connecting strings is called concatenation. See the example below:\n\n```py\nfirst_name = 'Asabeneh'\nlast_name = 'Yetayeh'\nspace = ' '\nfull_name = first_name  +  space + last_name\nprint(full_name) # Asabeneh Yetayeh\n# Checking the length of a string using len() built-in function\nprint(len(first_name))  # 8\nprint(len(last_name))   # 7\nprint(len(first_name) > len(last_name)) # True\nprint(len(full_name)) # 16\n```\n\n### Escape Sequences in Strings\n\nIn Python and other programming languages \\ followed by a character is an escape sequence. Let us see the most common escape characters:\n\n- \\n: new line\n- \\t: Tab means(8 spaces)\n- \\\\\\\\: Back slash\n- \\\\': Single quote (')\n- \\\\\": Double quote (\")\n\nNow, let us see the use of the above escape sequences with examples.\n\n```py\nprint('I hope everyone is enjoying the Python Challenge.\\nAre you ?') # line break\nprint('Days\\tTopics\\tExercises') # adding tab space or 4 spaces \nprint('Day 1\\t5\\t5')\nprint('Day 2\\t6\\t20')\nprint('Day 3\\t5\\t23')\nprint('Day 4\\t1\\t35')\nprint('This is a backslash  symbol (\\\\)') # To write a backslash\nprint('In every programming language it starts with \\\"Hello, World!\\\"') # to write a double quote inside a single quote\n\n# output\nI hope every one is enjoying the Python Challenge.\nAre you ?\nDays\tTopics\tExercises\nDay 1\t5\t    5\nDay 2\t6\t    20\nDay 3\t5\t    23\nDay 4\t1\t    35\nThis is a backslash  symbol (\\)\nIn every programming language it starts with \"Hello, World!\"\n```\n\n### String formatting\n\n#### Old Style String Formatting (% Operator)\n\nIn Python there are many ways of formatting strings. In this section, we will cover some of them.\nThe \"%\" operator is used to format a set of variables enclosed in a \"tuple\" (a fixed size list), together with a format string, which contains normal text together with \"argument specifiers\", special symbols like \"%s\", \"%d\", \"%f\", \"%.<small>number of digits</small>f\".\n\n- %s - String (or any object with a string representation, like numbers)\n- %d - Integers\n- %f - Floating point numbers\n- \"%.<small>number of digits</small>f\" - Floating point numbers with fixed precision\n\n```py\n# Strings only\nfirst_name = 'Asabeneh'\nlast_name = 'Yetayeh'\nlanguage = 'Python'\nformated_string = 'I am %s %s. I teach %s' %(first_name, last_name, language)\nprint(formated_string)\n\n# Strings  and numbers\nradius = 10\npi = 3.14\narea = pi * radius ** 2\nformated_string = 'The area of circle with a radius %d is %.2f.' %(radius, area) # 2 refers the 2 significant digits after the point\n\npython_libraries = ['Django', 'Flask', 'NumPy', 'Matplotlib','Pandas']\nformated_string = 'The following are python libraries:%s' % (python_libraries)\nprint(formated_string) # \"The following are python libraries:['Django', 'Flask', 'NumPy', 'Matplotlib','Pandas']\"\n```\n\n#### New Style String Formatting (str.format)\n\nThis formatting is introduced in Python version 3.\n\n```py\n\nfirst_name = 'Asabeneh'\nlast_name = 'Yetayeh'\nlanguage = 'Python'\nformated_string = 'I am {} {}. I teach {}'.format(first_name, last_name, language)\nprint(formated_string)\na = 4\nb = 3\n\nprint('{} + {} = {}'.format(a, b, a + b))\nprint('{} - {} = {}'.format(a, b, a - b))\nprint('{} * {} = {}'.format(a, b, a * b))\nprint('{} / {} = {:.2f}'.format(a, b, a / b)) # limits it to two digits after decimal\nprint('{} % {} = {}'.format(a, b, a % b))\nprint('{} // {} = {}'.format(a, b, a // b))\nprint('{} ** {} = {}'.format(a, b, a ** b))\n\n# output\n4 + 3 = 7\n4 - 3 = 1\n4 * 3 = 12\n4 / 3 = 1.33\n4 % 3 = 1\n4 // 3 = 1\n4 ** 3 = 64\n\n# Strings  and numbers\nradius = 10\npi = 3.14\narea = pi * radius ** 2\nformated_string = 'The area of a circle with a radius {} is {:.2f}.'.format(radius, area) # 2 digits after decimal\nprint(formated_string)\n\n```\n\n#### String Interpolation / f-Strings (Python 3.6+)\n\nAnother new string formatting is string interpolation, f-strings. Strings start with f and we can inject the data in their corresponding positions.\n\n```py\na = 4\nb = 3\nprint(f'{a} + {b} = {a +b}')\nprint(f'{a} - {b} = {a - b}')\nprint(f'{a} * {b} = {a * b}')\nprint(f'{a} / {b} = {a / b:.2f}')\nprint(f'{a} % {b} = {a % b}')\nprint(f'{a} // {b} = {a // b}')\nprint(f'{a} ** {b} = {a ** b}')\n```\n\n### Python Strings as Sequences of Characters\n\nPython strings are sequences of characters, and share their basic methods of access with other Python ordered sequences of objects ‚Äì lists and tuples. The simplest way of extracting single characters from strings (and individual members from any sequence) is to unpack them into corresponding variables.\n\n#### Unpacking Characters\n\n```\nlanguage = 'Python'\na,b,c,d,e,f = language # unpacking sequence characters into variables\nprint(a) # P\nprint(b) # y\nprint(c) # t\nprint(d) # h\nprint(e) # o\nprint(f) # n\n```\n\n#### Accessing Characters in Strings by Index\n\nIn programming counting starts from zero. Therefore the first letter of a string is at zero index and the last letter of a string is the length of a string minus one.\n\n![String index](../images/string_index.png)\n\n```py\nlanguage = 'Python'\nfirst_letter = language[0]\nprint(first_letter) # P\nsecond_letter = language[1]\nprint(second_letter) # y\nlast_index = len(language) - 1\nlast_letter = language[last_index]\nprint(last_letter) # n\n```\n\nIf we want to start from right end we can use negative indexing. -1 is the last index.\n\n```py\nlanguage = 'Python'\nlast_letter = language[-1]\nprint(last_letter) # n\nsecond_last = language[-2]\nprint(second_last) # o\n```\n\n#### Slicing Python Strings\n\nIn python we can slice strings into substrings.\n\n```py\nlanguage = 'Python'\nfirst_three = language[0:3] # starts at zero index and up to 3 but not include 3\nprint(first_three) #Pyt\nlast_three = language[3:6]\nprint(last_three) # hon\n# Another way\nlast_three = language[-3:]\nprint(last_three)   # hon\nlast_three = language[3:]\nprint(last_three)   # hon\n```\n\n#### Reversing a String\n\nWe can easily reverse strings in python.\n\n```py\ngreeting = 'Hello, World!'\nprint(greeting[::-1]) # !dlroW ,olleH\n```\n\n#### Skipping Characters While Slicing\n\nIt is possible to skip characters while slicing by passing step argument to slice method.\n\n```py\nlanguage = 'Python'\npto = language[0:6:2] #\nprint(pto) # Pto\n```\n\n### String Methods\n\nThere are many string methods which allow us to format strings. See some of the string methods in the following example:\n\n- capitalize(): Converts the first character of the string to capital letter\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.capitalize()) # 'Thirty days of python'\n```\n\n- count(): returns occurrences of substring in string, count(substring, start=.., end=..). The start is a starting indexing for counting and end is the last index to count.\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.count('y')) # 3\nprint(challenge.count('y', 7, 14)) # 1, \nprint(challenge.count('th')) # 2`\n```\n\n- endswith(): Checks if a string ends with a specified ending\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.endswith('on'))   # True\nprint(challenge.endswith('tion')) # False\n```\n\n- expandtabs(): Replaces tab character with spaces, default tab size is 8. It takes tab size argument\n\n```py\nchallenge = 'thirty\\tdays\\tof\\tpython'\nprint(challenge.expandtabs())   # 'thirty  days    of      python'\nprint(challenge.expandtabs(10)) # 'thirty    days      of        python'\n```\n\n- find(): Returns the index of the first occurrence of a substring, if not found returns -1\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.find('y'))  # 5\nprint(challenge.find('th')) # 0\n```\n\n- rfind(): Returns the index of the last occurrence of a substring, if not found returns -1\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.rfind('y'))  # 16\nprint(challenge.rfind('th')) # 17\n```\n\n- format(): formats string into a nicer output  \n   More about string formatting check this [link](https://www.programiz.com/python-programming/methods/string/format)\n\n```py\nfirst_name = 'Asabeneh'\nlast_name = 'Yetayeh'\nage = 250\njob = 'teacher'\ncountry = 'Finland'\nsentence = 'I am {} {}. I am a {}. I am {} years old. I live in {}.'.format(first_name, last_name, age, job, country)\nprint(sentence) # I am Asabeneh Yetayeh. I am 250 years old. I am a teacher. I live in Finland.\n\nradius = 10\npi = 3.14\narea = pi * radius ** 2\nresult = 'The area of a circle with radius {} is {}'.format(str(radius), str(area))\nprint(result) # The area of a circle with radius 10 is 314\n```\n\n- index(): Returns the lowest index of a substring, additional arguments indicate starting and ending index (default 0 and string length - 1). If the substring is not found it raises a valueError. \n\n```py\nchallenge = 'thirty days of python'\nsub_string = 'da'\nprint(challenge.index(sub_string))  # 7\nprint(challenge.index(sub_string, 9)) # error\n```\n\n- rindex(): Returns the highest index of a substring, additional arguments indicate starting and ending index (default 0 and string length - 1)\n\n```py\nchallenge = 'thirty days of python'\nsub_string = 'da'\nprint(challenge.rindex(sub_string))  # 7\nprint(challenge.rindex(sub_string, 9)) # error\nprint(challenge.rindex('on', 8)) # 19\n```\n\n- isalnum(): Checks alphanumeric character\n\n```py\nchallenge = 'ThirtyDaysPython'\nprint(challenge.isalnum()) # True\n\nchallenge = '30DaysPython'\nprint(challenge.isalnum()) # True\n\nchallenge = 'thirty days of python'\nprint(challenge.isalnum()) # False, space is not an alphanumeric character\n\nchallenge = 'thirty days of python 2019'\nprint(challenge.isalnum()) # False\n```\n\n- isalpha(): Checks if all string elements are alphabet characters (a-z and A-Z)\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.isalpha()) # False, space is once again excluded\nchallenge = 'ThirtyDaysPython'\nprint(challenge.isalpha()) # True\nnum = '123'\nprint(num.isalpha())      # False\n```\n\n- isdecimal(): Checks if all characters in a string are decimal (0-9)\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.isdecimal())  # False\nchallenge = '123'\nprint(challenge.isdecimal())  # True\nchallenge = '\\u00B2'\nprint(challenge.isdigit())   # False\nchallenge = '12 3'\nprint(challenge.isdecimal())  # False, space not allowed\n```\n\n- isdigit(): Checks if all characters in a string are numbers (0-9 and some other unicode characters for numbers)\n\n```py\nchallenge = 'Thirty'\nprint(challenge.isdigit()) # False\nchallenge = '30'\nprint(challenge.isdigit())   # True\nchallenge = '\\u00B2'\nprint(challenge.isdigit())   # True\n```\n\n- isnumeric(): Checks if all characters in a string are numbers or number related (just like isdigit(), just accepts more symbols, like ¬Ω)\n\n```py\nnum = '10'\nprint(num.isnumeric()) # True\nnum = '\\u00BD' # ¬Ω\nprint(num.isnumeric()) # True\nnum = '10.5'\nprint(num.isnumeric()) # False\n```\n\n- isidentifier(): Checks for a valid identifier - it checks if a string is a valid variable name\n\n```py\nchallenge = '30DaysOfPython'\nprint(challenge.isidentifier()) # False, because it starts with a number\nchallenge = 'thirty_days_of_python'\nprint(challenge.isidentifier()) # True\n```\n\n- islower(): Checks if all alphabet characters in the string are lowercase\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.islower()) # True\nchallenge = 'Thirty days of python'\nprint(challenge.islower()) # False\n```\n\n- isupper(): Checks if all alphabet characters in the string are uppercase\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.isupper()) #  False\nchallenge = 'THIRTY DAYS OF PYTHON'\nprint(challenge.isupper()) # True\n```\n\n- join(): Returns a concatenated string\n\n```py\nweb_tech = ['HTML', 'CSS', 'JavaScript', 'React']\nresult = ' '.join(web_tech)\nprint(result) # 'HTML CSS JavaScript React'\n```\n\n```py\nweb_tech = ['HTML', 'CSS', 'JavaScript', 'React']\nresult = '# '.join(web_tech)\nprint(result) # 'HTML# CSS# JavaScript# React'\n```\n\n- strip(): Removes all given characters starting from the beginning and end of the string\n\n```py\nchallenge = 'thirty days of pythoonnn'\nprint(challenge.strip('noth')) # 'irty days of py'\n```\n\n- replace(): Replaces substring with a given string\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.replace('python', 'coding')) # 'thirty days of coding'\n```\n\n- split(): Splits the string, using given string or space as a separator\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.split()) # ['thirty', 'days', 'of', 'python']\nchallenge = 'thirty, days, of, python'\nprint(challenge.split(', ')) # ['thirty', 'days', 'of', 'python']\n```\n\n- title(): Returns a title cased string\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.title()) # Thirty Days Of Python\n```\n\n- swapcase(): Converts all uppercase characters to lowercase and all lowercase characters to uppercase characters\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.swapcase())   # THIRTY DAYS OF PYTHON\nchallenge = 'Thirty Days Of Python'\nprint(challenge.swapcase())  # tHIRTY dAYS oF pYTHON\n```\n\n- startswith(): Checks if String Starts with the Specified String\n\n```py\nchallenge = 'thirty days of python'\nprint(challenge.startswith('thirty')) # True\n\nchallenge = '30 days of python'\nprint(challenge.startswith('thirty')) # False\n```\n\nüåï You are an extraordinary person and you have a remarkable potential. You have just completed day 4 challenges and you are four steps a head in to your way to greatness. Now do some exercises for your brain and muscles.\n\n## üíª Exercises - Day 4\n\n1. Concatenate the string 'Thirty', 'Days', 'Of', 'Python' to a single string, 'Thirty Days Of Python'.\n2. Concatenate the string 'Coding', 'For' , 'All' to a single string, 'Coding For All'.\n3. Declare a variable named company and assign it to an initial value \"Coding For All\".\n4. Print the variable company using _print()_.\n5. Print the length of the company string using _len()_ method and _print()_.\n6. Change all the characters to uppercase letters using _upper()_ method.\n7. Change all the characters to lowercase letters using _lower()_ method.\n8. Use capitalize(), title(), swapcase() methods to format the value of the string _Coding For All_.\n9. Cut(slice) out the first word of _Coding For All_ string.\n10. Check if _Coding For All_ string contains a word Coding using the method index, find or other methods.\n11. Replace the word coding in the string 'Coding For All' to Python.\n12. Change Python for Everyone to Python for All using the replace method or other methods.\n13. Split the string 'Coding For All' using space as the separator (split()) .\n14. \"Facebook, Google, Microsoft, Apple, IBM, Oracle, Amazon\" split the string at the comma.\n15. What is the character at index 0 in the string _Coding For All_.\n16. What is the last index of the string _Coding For All_.\n17. What character is at index 10 in \"Coding For All\" string.\n18. Create an acronym or an abbreviation for the name 'Python For Everyone'.\n19. Create an acronym or an abbreviation for the name 'Coding For All'.\n20. Use index to determine the position of the first occurrence of C in Coding For All.\n21. Use index to determine the position of the first occurrence of F in Coding For All.\n22. Use rfind to determine the position of the last occurrence of l in Coding For All People.\n23. Use index or find to find the position of the first occurrence of the word 'because' in the following sentence: 'You cannot end a sentence with because because because is a conjunction'\n24. Use rindex to find the position of the last occurrence of the word because in the following sentence: 'You cannot end a sentence with because because because is a conjunction'\n25. Slice out the phrase 'because because because' in the following sentence: 'You cannot end a sentence with because because because is a conjunction'\n26. Find the position of the first occurrence of the word 'because' in the following sentence: 'You cannot end a sentence with because because because is a conjunction'\n27. Slice out the phrase 'because because because' in the following sentence: 'You cannot end a sentence with because because because is a conjunction'\n28. Does '\\'Coding For All' start with a substring _Coding_?\n29. Does 'Coding For All' end with a substring _coding_?\n30. '&nbsp;&nbsp; Coding For All &nbsp;&nbsp;&nbsp; &nbsp;' &nbsp;, remove the left and right trailing spaces in the given string.\n31. Which one of the following variables return True when we use the method isidentifier():\n    - 30DaysOfPython\n    - thirty_days_of_python\n32. The following list contains the names of some of python libraries: ['Django', 'Flask', 'Bottle', 'Pyramid', 'Falcon']. Join the list with a hash with space string.\n33. Use the new line escape sequence to separate the following sentences.\n    ```py\n    I am enjoying this challenge.\n    I just wonder what is next.\n    ```\n34. Use a tab escape sequence to write the following lines.\n    ```py\n    Name      Age     Country   City\n    Asabeneh  250     Finland   Helsinki\n    ```\n35. Use the string formatting method to display the following:\n\n```sh\nradius = 10\narea = 3.14 * radius ** 2\nThe area of a circle with radius 10 is 314 meters square.\n```\n\n36. Make the following using string formatting methods:\n\n```sh\n8 + 6 = 14\n8 - 6 = 2\n8 * 6 = 48\n8 / 6 = 1.33\n8 % 6 = 2\n8 // 6 = 1\n8 ** 6 = 262144\n```\n\nüéâ CONGRATULATIONS ! üéâ\n\n[<< Day 3](../03_Day_Operators/03_operators.md) | [Day 5 >>](../05_Day_Lists/05_lists.md)\n\n\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "04_Day_Strings/04_strings.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.89, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 606, "word_count": 3021}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 5 - Lists</h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n<sub>Author:\n<a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n<small> Second Edition: July - 2021</small>\n</sub>\n\n</div>\n\n[<< Day 4](../04_Day_Strings/04_strings.md) | [Day 6 >>](../06_Day_Tuples/06_tuples.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n\n- [Day 5](#day-5)\n  - [Lists](#lists)\n    - [How to Create a List](#how-to-create-a-list)\n    - [Accessing List Items Using Positive Indexing](#accessing-list-items-using-positive-indexing)\n    - [Accessing List Items Using Negative Indexing](#accessing-list-items-using-negative-indexing)\n    - [Unpacking List Items](#unpacking-list-items)\n    - [Slicing Items from a List](#slicing-items-from-a-list)\n    - [Modifying Lists](#modifying-lists)\n    - [Checking Items in a List](#checking-items-in-a-list)\n    - [Adding Items to a List](#adding-items-to-a-list)\n    - [Inserting Items into a List](#inserting-items-into-a-list)\n    - [Removing Items from a List](#removing-items-from-a-list)\n    - [Removing Items Using Pop](#removing-items-using-pop)\n    - [Removing Items Using Del](#removing-items-using-del)\n    - [Clearing List Items](#clearing-list-items)\n    - [Copying a List](#copying-a-list)\n    - [Joining Lists](#joining-lists)\n    - [Counting Items in a List](#counting-items-in-a-list)\n    - [Finding Index of an Item](#finding-index-of-an-item)\n    - [Reversing a List](#reversing-a-list)\n    - [Sorting List Items](#sorting-list-items)\n  - [üíª Exercises: Day 5](#-exercises-day-5)\n    - [Exercises: Level 1](#exercises-level-1)\n    - [Exercises: Level 2](#exercises-level-2)\n\n# Day 5\n\n## Lists\n\nThere are four collection data types in Python :\n\n- List: is a collection which is ordered and changeable(modifiable). Allows duplicate members.\n- Tuple: is a collection which is ordered and unchangeable or unmodifiable(immutable). Allows duplicate members.\n- Set: is a collection which is unordered, un-indexed and unmodifiable, but we can add new items to the set. Duplicate members are not allowed.\n- Dictionary: is a collection which is unordered, changeable(modifiable) and indexed. No duplicate members.\n\nA list is collection of different data types which is ordered and modifiable(mutable). A list can be empty or it may have different data type items.\n\n### How to Create a List\n\nIn Python we can create lists in two ways:\n\n- Using list built-in function\n\n```py\n# syntax\nlst = list()\n```\n\n```py\nempty_list = list() # this is an empty list, no item in the list\nprint(len(empty_list)) # 0\n```\n\n- Using square brackets, []\n\n```py\n# syntax\nlst = []\n```\n\n```py\nempty_list = [] # this is an empty list, no item in the list\nprint(len(empty_list)) # 0\n```\n\nLists with initial values. We use _len()_ to find the length of a list.\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']                     # list of fruits\nvegetables = ['Tomato', 'Potato', 'Cabbage','Onion', 'Carrot']      # list of vegetables\nanimal_products = ['milk', 'meat', 'butter', 'yoghurt']             # list of animal products\nweb_techs = ['HTML', 'CSS', 'JS', 'React','Redux', 'Node', 'MongDB'] # list of web technologies\ncountries = ['Finland', 'Estonia', 'Denmark', 'Sweden', 'Norway']\n\n# Print the lists and its length\nprint('Fruits:', fruits)\nprint('Number of fruits:', len(fruits))\nprint('Vegetables:', vegetables)\nprint('Number of vegetables:', len(vegetables))\nprint('Animal products:',animal_products)\nprint('Number of animal products:', len(animal_products))\nprint('Web technologies:', web_techs)\nprint('Number of web technologies:', len(web_techs))\nprint('Countries:', countries)\nprint('Number of countries:', len(countries))\n```\n\n```sh\noutput\nFruits: ['banana', 'orange', 'mango', 'lemon']\nNumber of fruits: 4\nVegetables: ['Tomato', 'Potato', 'Cabbage', 'Onion', 'Carrot']\nNumber of vegetables: 5\nAnimal products: ['milk', 'meat', 'butter', 'yoghurt']\nNumber of animal products: 4\nWeb technologies: ['HTML', 'CSS', 'JS', 'React', 'Redux', 'Node', 'MongDB']\nNumber of web technologies: 7\nCountries: ['Finland', 'Estonia', 'Denmark', 'Sweden', 'Norway']\nNumber of countries: 5\n```\n\n- Lists can have items of different data types\n\n```py\n lst = ['Asabeneh', 250, True, {'country':'Finland', 'city':'Helsinki'}] # list containing different data types\n```\n\n### Accessing List Items Using Positive Indexing\n\nWe access each item in a list using their index. A list index starts from 0. The picture below shows clearly where the index starts\n![List index](../images/list_index.png)\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\nfirst_fruit = fruits[0] # we are accessing the first item using its index\nprint(first_fruit)      # banana\nsecond_fruit = fruits[1]\nprint(second_fruit)     # orange\nlast_fruit = fruits[3]\nprint(last_fruit) # lemon\n# Last index\nlast_index = len(fruits) - 1\nlast_fruit = fruits[last_index]\n```\n\n### Accessing List Items Using Negative Indexing\n\nNegative indexing means beginning from the end, -1 refers to the last item, -2 refers to the second last item.\n\n![List negative indexing](../images/list_negative_indexing.png)\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\nfirst_fruit = fruits[-4]\nlast_fruit = fruits[-1]\nsecond_last = fruits[-2]\nprint(first_fruit)      # banana\nprint(last_fruit)       # lemon\nprint(second_last)      # mango\n```\n\n### Unpacking List Items\n\n```py\nlst = ['item1','item2','item3', 'item4', 'item5']\nfirst_item, second_item, third_item, *rest = lst\nprint(first_item)     # item1\nprint(second_item)    # item2\nprint(third_item)     # item3\nprint(rest)           # ['item4', 'item5']\n\n```\n\n```py\n# First Example\nfruits = ['banana', 'orange', 'mango', 'lemon','lime','apple']\nfirst_fruit, second_fruit, third_fruit, *rest = fruits \nprint(first_fruit)     # banana\nprint(second_fruit)    # orange\nprint(third_fruit)     # mango\nprint(rest)           # ['lemon','lime','apple']\n# Second Example about unpacking list\nfirst, second, third,*rest, tenth = [1,2,3,4,5,6,7,8,9,10]\nprint(first)          # 1\nprint(second)         # 2\nprint(third)          # 3\nprint(rest)           # [4,5,6,7,8,9]\nprint(tenth)          # 10\n# Third Example about unpacking list\ncountries = ['Germany', 'France','Belgium','Sweden','Denmark','Finland','Norway','Iceland','Estonia']\ngr, fr, bg, sw, *scandic, es = countries\nprint(gr)\nprint(fr)\nprint(bg)\nprint(sw)\nprint(scandic)\nprint(es)\n```\n\n### Slicing Items from a List\n\n- Positive Indexing: We can specify a range of positive indexes by specifying the start, end and step, the return value will be a new list. (default values for start = 0, end = len(lst) - 1 (last item), step = 1)\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\nall_fruits = fruits[0:4] # it returns all the fruits\n# this will also give the same result as the one above\nall_fruits = fruits[0:] # if we don't set where to stop it takes all the rest\norange_and_mango = fruits[1:3] # it does not include the first index\norange_mango_lemon = fruits[1:]\norange_and_lemon = fruits[::2] # here we used a 3rd argument, step. It will take every 2cnd item - ['banana', 'mango']\n```\n\n- Negative Indexing: We can specify a range of negative indexes by specifying the start, end and step, the return value will be a new list.\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\nall_fruits = fruits[-4:] # it returns all the fruits\norange_and_mango = fruits[-3:-1] # it does not include the last index,['orange', 'mango']\norange_mango_lemon = fruits[-3:] # this will give starting from -3 to the end,['orange', 'mango', 'lemon']\nreverse_fruits = fruits[::-1] # a negative step will take the list in reverse order,['lemon', 'mango', 'orange', 'banana']\n```\n\n### Modifying Lists\n\nList is a mutable or modifiable ordered collection of items. Lets modify the fruit list.\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\nfruits[0] = 'avocado'\nprint(fruits)       #  ['avocado', 'orange', 'mango', 'lemon']\nfruits[1] = 'apple'\nprint(fruits)       #  ['avocado', 'apple', 'mango', 'lemon']\nlast_index = len(fruits) - 1\nfruits[last_index] = 'lime'\nprint(fruits)        #  ['avocado', 'apple', 'mango', 'lime']\n```\n\n### Checking Items in a List\n\nChecking an item if it is a member of a list using *in* operator. See the example below.\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\ndoes_exist = 'banana' in fruits\nprint(does_exist)  # True\ndoes_exist = 'lime' in fruits\nprint(does_exist)  # False\n```\n\n### Adding Items to a List\n\nTo add item to the end of an existing list we use the method *append()*.\n\n```py\n# syntax\nlst = list()\nlst.append(item)\n```\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\nfruits.append('apple')\nprint(fruits)           # ['banana', 'orange', 'mango', 'lemon', 'apple']\nfruits.append('lime')   # ['banana', 'orange', 'mango', 'lemon', 'apple', 'lime']\nprint(fruits)\n```\n\n### Inserting Items into a List\n\nWe can use *insert()* method to insert a single item at a specified index in a list. Note that other items are shifted to the right. The *insert()* methods takes two arguments:index and an item to insert.\n\n```py\n# syntax\nlst = ['item1', 'item2']\nlst.insert(index, item)\n```\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\nfruits.insert(2, 'apple') # insert apple between orange and mango\nprint(fruits)           # ['banana', 'orange', 'apple', 'mango', 'lemon']\nfruits.insert(3, 'lime')   # ['banana', 'orange', 'apple', 'lime', 'mango', 'lemon']\nprint(fruits)\n```\n\n### Removing Items from a List\n\nThe remove method removes a specified item from a list\n\n```py\n# syntax\nlst = ['item1', 'item2']\nlst.remove(item)\n```\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon', 'banana']\nfruits.remove('banana')\nprint(fruits)  # ['orange', 'mango', 'lemon', 'banana'] - this method removes the first occurrence of the item in the list\nfruits.remove('lemon')\nprint(fruits)  # ['orange', 'mango', 'banana']\n```\n\n### Removing Items Using Pop\n\nThe *pop()* method removes the specified index, (or the last item if index is not specified):\n\n```py\n# syntax\nlst = ['item1', 'item2']\nlst.pop()       # last item\nlst.pop(index)\n```\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\nfruits.pop()\nprint(fruits)       # ['banana', 'orange', 'mango']\n\nfruits.pop(0)\nprint(fruits)       # ['orange', 'mango']\n```\n\n### Removing Items Using Del\n\nThe *del* keyword removes the specified index and it can also be used to delete items within index range. It can also delete the list completely\n\n```py\n# syntax\nlst = ['item1', 'item2']\ndel lst[index] # only a single item\ndel lst        # to delete the list completely\n```\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon', 'kiwi', 'lime']\ndel fruits[0]\nprint(fruits)       # ['orange', 'mango', 'lemon', 'kiwi', 'lime']\ndel fruits[1]\nprint(fruits)       # ['orange', 'lemon', 'kiwi', 'lime']\ndel fruits[1:3]     # this deletes items between given indexes, so it does not delete the item with index 3!\nprint(fruits)       # ['orange', 'lime']\ndel fruits\nprint(fruits)       # This should give: NameError: name 'fruits' is not defined\n```\n\n### Clearing List Items\n\nThe *clear()* method empties the list:\n\n```py\n# syntax\nlst = ['item1', 'item2']\nlst.clear()\n```\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\nfruits.clear()\nprint(fruits)       # []\n```\n\n### Copying a List\n\nIt is possible to copy a list by reassigning it to a new variable in the following way: list2 = list1. Now, list2 is a reference of list1, any changes we make in list2 will also modify the original, list1. But there are lots of case in which we do not like to modify the original instead we like to have a different copy. One of way of avoiding the problem above is using _copy()_.\n\n```py\n# syntax\nlst = ['item1', 'item2']\nlst_copy = lst.copy()\n```\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\nfruits_copy = fruits.copy()\nprint(fruits_copy)       # ['banana', 'orange', 'mango', 'lemon']\n```\n\n### Joining Lists\n\nThere are several ways to join, or concatenate, two or more lists in Python.\n\n- Plus Operator (+)\n\n```py\n# syntax\nlist3 = list1 + list2\n```\n\n```py\npositive_numbers = [1, 2, 3, 4, 5]\nzero = [0]\nnegative_numbers = [-5,-4,-3,-2,-1]\nintegers = negative_numbers + zero + positive_numbers\nprint(integers) # [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\nfruits = ['banana', 'orange', 'mango', 'lemon']\nvegetables = ['Tomato', 'Potato', 'Cabbage', 'Onion', 'Carrot']\nfruits_and_vegetables = fruits + vegetables\nprint(fruits_and_vegetables ) # ['banana', 'orange', 'mango', 'lemon', 'Tomato', 'Potato', 'Cabbage', 'Onion', 'Carrot']\n```\n\n- Joining using extend() method\n  The *extend()* method allows to append list in a list. See the example below.\n\n```py\n# syntax\nlist1 = ['item1', 'item2']\nlist2 = ['item3', 'item4', 'item5']\nlist1.extend(list2)\n```\n\n```py\nnum1 = [0, 1, 2, 3]\nnum2= [4, 5, 6]\nnum1.extend(num2)\nprint('Numbers:', num1) # Numbers: [0, 1, 2, 3, 4, 5, 6]\nnegative_numbers = [-5,-4,-3,-2,-1]\npositive_numbers = [1, 2, 3,4,5]\nzero = [0]\n\nnegative_numbers.extend(zero)\nnegative_numbers.extend(positive_numbers)\nprint('Integers:', negative_numbers) # Integers: [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\nfruits = ['banana', 'orange', 'mango', 'lemon']\nvegetables = ['Tomato', 'Potato', 'Cabbage', 'Onion', 'Carrot']\nfruits.extend(vegetables)\nprint('Fruits and vegetables:', fruits ) # Fruits and vegetables: ['banana', 'orange', 'mango', 'lemon', 'Tomato', 'Potato', 'Cabbage', 'Onion', 'Carrot']\n```\n\n### Counting Items in a List\n\nThe *count()* method returns the number of times an item appears in a list:\n\n```py\n# syntax\nlst = ['item1', 'item2']\nlst.count(item)\n```\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\nprint(fruits.count('orange'))   # 1\nages = [22, 19, 24, 25, 26, 24, 25, 24]\nprint(ages.count(24))           # 3\n```\n\n### Finding Index of an Item\n\nThe *index()* method returns the index of an item in the list:\n\n```py\n# syntax\nlst = ['item1', 'item2']\nlst.index(item)\n```\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\nprint(fruits.index('orange'))   # 1\nages = [22, 19, 24, 25, 26, 24, 25, 24]\nprint(ages.index(24))           # 2, the first occurrence\n```\n\n### Reversing a List\n\nThe *reverse()* method reverses the order of a list.\n\n```py\n# syntax\nlst = ['item1', 'item2']\nlst.reverse()\n\n```\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon']\nfruits.reverse()\nprint(fruits) # ['lemon', 'mango', 'orange', 'banana']\nages = [22, 19, 24, 25, 26, 24, 25, 24]\nages.reverse()\nprint(ages) # [24, 25, 24, 26, 25, 24, 19, 22]\n```\n\n### Sorting List Items\n\nTo sort lists we can use _sort()_ method or _sorted()_ built-in functions. The _sort()_ method reorders the list items in ascending order and modifies the original list. If an argument of _sort()_ method reverse is equal to true, it will arrange the list in descending order.\n\n- sort(): this method modifies the original list\n\n  ```py\n  # syntax\n  lst = ['item1', 'item2']\n  lst.sort()                # ascending\n  lst.sort(reverse=True)    # descending\n  ```\n\n  **Example:**\n\n  ```py\n  fruits = ['banana', 'orange', 'mango', 'lemon']\n  fruits.sort()\n  print(fruits)             # sorted in alphabetical order, ['banana', 'lemon', 'mango', 'orange']\n  fruits.sort(reverse=True)\n  print(fruits) # ['orange', 'mango', 'lemon', 'banana']\n  ages = [22, 19, 24, 25, 26, 24, 25, 24]\n  ages.sort()\n  print(ages) #  [19, 22, 24, 24, 24, 25, 25, 26]\n \n  ages.sort(reverse=True)\n  print(ages) #  [26, 25, 25, 24, 24, 24, 22, 19]\n  ```\n\n  sorted(): returns the ordered list without modifying the original list\n  **Example:**\n\n  ```py\n  fruits = ['banana', 'orange', 'mango', 'lemon']\n  print(sorted(fruits))   # ['banana', 'lemon', 'mango', 'orange']\n  # Reverse order\n  fruits = ['banana', 'orange', 'mango', 'lemon']\n  fruits = sorted(fruits,reverse=True)\n  print(fruits)     # ['orange', 'mango', 'lemon', 'banana']\n  ```\n\nüåï You are diligent and you have already achieved quite a lot. You have just completed day 5 challenges and you are 5 steps a head in to your way to greatness. Now do some exercises for your brain and muscles.\n\n## üíª Exercises: Day 5\n\n### Exercises: Level 1\n\n1. Declare an empty list\n2. Declare a list with more than 5 items\n3. Find the length of your list\n4. Get the first item, the middle item and the last item of the list\n5. Declare a list called mixed_data_types, put your(name, age, height, marital status, address)\n6. Declare a list variable named it_companies and assign initial values Facebook, Google, Microsoft, Apple, IBM, Oracle and Amazon.\n7. Print the list using _print()_\n8. Print the number of companies in the list\n9. Print the first, middle and last company\n10. Print the list after modifying one of the companies\n11. Add an IT company to it_companies\n12. Insert an IT company in the middle of the companies list\n13. Change one of the it_companies names to uppercase (IBM excluded!)\n14. Join the it_companies with a string '#;&nbsp; '\n15. Check if a certain company exists in the it_companies list.\n16. Sort the list using sort() method\n17. Reverse the list in descending order using reverse() method\n18. Slice out the first 3 companies from the list\n19. Slice out the last 3 companies from the list\n20. Slice out the middle IT company or companies from the list\n21. Remove the first IT company from the list\n22. Remove the middle IT company or companies from the list\n23. Remove the last IT company from the list\n24. Remove all IT companies from the list\n25. Destroy the IT companies list\n26. Join the following lists:\n\n    ```py\n    front_end = ['HTML', 'CSS', 'JS', 'React', 'Redux']\n    back_end = ['Node','Express', 'MongoDB']\n    ```\n\n27. After joining the lists in question 26. Copy the joined list and assign it to a variable full_stack, then insert Python and SQL after Redux.\n\n### Exercises: Level 2\n\n1. The following is a list of 10 students ages:\n\n```sh\nages = [19, 22, 19, 24, 20, 25, 26, 24, 25, 24]\n```\n\n- Sort the list and find the min and max age\n- Add the min age and the max age again to the list\n- Find the median age (one middle item or two middle items divided by two)\n- Find the average age (sum of all items divided by their number )\n- Find the range of the ages (max minus min)\n- Compare the value of (min - average) and (max - average), use _abs()_ method\n\n1. Find the middle country(ies) in the [countries list](https://github.com/Asabeneh/30-Days-Of-Python/tree/master/data/countries.py)\n1. Divide the countries list into two equal lists if it is even if not one more country for the first half.\n1. ['China', 'Russia', 'USA', 'Finland', 'Sweden', 'Norway', 'Denmark']. Unpack the first three countries and the rest as scandic countries.\n\nüéâ CONGRATULATIONS ! üéâ\n\n[<< Day 4](../04_Day_Strings/04_strings.md) | [Day 6 >>](../06_Day_Tuples/06_tuples.md)\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "05_Day_Lists/05_lists.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 589, "word_count": 2667}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 6 - Tuples</h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n<sub>Author:\n<a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n<small> Second Edition: July, 2021</small>\n</sub>\n\n</div>\n\n[<< Day 5](../05_Day_Lists/05_lists.md) | [Day 7 >>](../07_Day_Sets/07_sets.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n\n- [Day 6:](#day-6)\n  - [Tuples](#tuples)\n    - [Creating a Tuple](#creating-a-tuple)\n    - [Tuple length](#tuple-length)\n    - [Accessing Tuple Items](#accessing-tuple-items)\n    - [Slicing tuples](#slicing-tuples)\n    - [Changing Tuples to Lists](#changing-tuples-to-lists)\n    - [Checking an Item in a Tuple](#checking-an-item-in-a-tuple)\n    - [Joining Tuples](#joining-tuples)\n    - [Deleting Tuples](#deleting-tuples)\n  - [üíª Exercises: Day 6](#-exercises-day-6)\n    - [Exercises: Level 1](#exercises-level-1)\n    - [Exercises: Level 2](#exercises-level-2)\n\n# Day 6:\n\n## Tuples\n\nA tuple is a collection of different data types which is ordered and unchangeable (immutable). Tuples are written with round brackets, (). Once a tuple is created, we cannot change its values. We cannot use add, insert, remove methods in a tuple because it is not modifiable (mutable). Unlike list, tuple has few methods. Methods related to tuples:\n\n- tuple(): to create an empty tuple\n- count(): to count the number of a specified item in a tuple\n- index(): to find the index of a specified item in a tuple\n- + operator: to join two or more tuples and to create a new tuple\n\n### Creating a Tuple\n\n- Empty tuple: Creating an empty tuple\n  \n  ```py\n  # syntax\n  empty_tuple = ()\n  # or using the tuple constructor\n  empty_tuple = tuple()\n  ```\n\n- Tuple with initial values\n  \n  ```py\n  # syntax\n  tpl = ('item1', 'item2','item3')\n  ```\n\n  ```py\n  fruits = ('banana', 'orange', 'mango', 'lemon')\n  ```\n\n### Tuple length\n\nWe use the _len()_ method to get the length of a tuple.\n\n```py\n# syntax\ntpl = ('item1', 'item2', 'item3')\nlen(tpl)\n```\n\n### Accessing Tuple Items\n\n- Positive Indexing\n  Similar to the list data type we use positive or negative indexing to access tuple items.\n  ![Accessing tuple items](../images/tuples_index.png)\n\n  ```py\n  # Syntax\n  tpl = ('item1', 'item2', 'item3')\n  first_item = tpl[0]\n  second_item = tpl[1]\n  ```\n\n  ```py\n  fruits = ('banana', 'orange', 'mango', 'lemon')\n  first_fruit = fruits[0]\n  second_fruit = fruits[1]\n  last_index =len(fruits) - 1\n  last_fruit = fruits[las_index]\n  ```\n\n- Negative indexing\n  Negative indexing means beginning from the end, -1 refers to the last item, -2 refers to the second last and the negative of the list/tuple length refers to the first item.\n  ![Tuple Negative indexing](../images/tuple_negative_indexing.png)\n\n  ```py\n  # Syntax\n  tpl = ('item1', 'item2', 'item3','item4')\n  first_item = tpl[-4]\n  second_item = tpl[-3]\n  ```\n\n  ```py\n  fruits = ('banana', 'orange', 'mango', 'lemon')\n  first_fruit = fruits[-4]\n  second_fruit = fruits[-3]\n  last_fruit = fruits[-1]\n  ```\n\n### Slicing tuples\n\nWe can slice out a sub-tuple by specifying a range of indexes where to start and where to end in the tuple, the return value will be a new tuple with the specified items.\n\n- Range of Positive Indexes\n\n  ```py\n  # Syntax\n  tpl = ('item1', 'item2', 'item3','item4')\n  all_items = tpl[0:4]         # all items\n  all_items = tpl[0:]         # all items\n  middle_two_items = tpl[1:3]  # does not include item at index 3\n  ```\n\n  ```py\n  fruits = ('banana', 'orange', 'mango', 'lemon')\n  all_fruits = fruits[0:4]    # all items\n  all_fruits= fruits[0:]      # all items\n  orange_mango = fruits[1:3]  # doesn't include item at index 3\n  orange_to_the_rest = fruits[1:]\n  ```\n\n- Range of Negative Indexes\n\n  ```py\n  # Syntax\n  tpl = ('item1', 'item2', 'item3','item4')\n  all_items = tpl[-4:]         # all items\n  middle_two_items = tpl[-3:-1]  # does not include item at index 3 (-1)\n  ```\n\n  ```py\n  fruits = ('banana', 'orange', 'mango', 'lemon')\n  all_fruits = fruits[-4:]    # all items\n  orange_mango = fruits[-3:-1]  # doesn't include item at index 3\n  orange_to_the_rest = fruits[-3:]\n  ```\n\n### Changing Tuples to Lists\n\nWe can change tuples to lists and lists to tuples. Tuple is immutable if we want to modify a tuple we should change it to a list.\n\n```py\n# Syntax\ntpl = ('item1', 'item2', 'item3','item4')\nlst = list(tpl)\n```\n\n```py\nfruits = ('banana', 'orange', 'mango', 'lemon')\nfruits = list(fruits)\nfruits[0] = 'apple'\nprint(fruits)     # ['apple', 'orange', 'mango', 'lemon']\nfruits = tuple(fruits)\nprint(fruits)     # ('apple', 'orange', 'mango', 'lemon')\n```\n\n### Checking an Item in a Tuple\n\nWe can check if an item exists or not in a tuple using _in_, it returns a boolean.\n\n```py\n# Syntax\ntpl = ('item1', 'item2', 'item3','item4')\n'item2' in tpl # True\n```\n\n```py\nfruits = ('banana', 'orange', 'mango', 'lemon')\nprint('orange' in fruits) # True\nprint('apple' in fruits) # False\nfruits[0] = 'apple' # TypeError: 'tuple' object does not support item assignment\n```\n\n### Joining Tuples\n\nWe can join two or more tuples using + operator\n\n```py\n# syntax\ntpl1 = ('item1', 'item2', 'item3')\ntpl2 = ('item4', 'item5','item6')\ntpl3 = tpl1 + tpl2\n```\n\n```py\nfruits = ('banana', 'orange', 'mango', 'lemon')\nvegetables = ('Tomato', 'Potato', 'Cabbage','Onion', 'Carrot')\nfruits_and_vegetables = fruits + vegetables\n```\n\n### Deleting Tuples\n\nIt is not possible to remove a single item in a tuple but it is possible to delete the tuple itself using _del_.\n\n```py\n# syntax\ntpl1 = ('item1', 'item2', 'item3')\ndel tpl1\n\n```\n\n```py\nfruits = ('banana', 'orange', 'mango', 'lemon')\ndel fruits\n```\n\nüåï You are so brave, you made it to this far. You have just completed day 6 challenges and you are 6 steps a head in to your way to greatness. Now do some exercises for your brain and for your muscle.\n\n## üíª Exercises: Day 6\n\n### Exercises: Level 1\n\n1. Create an empty tuple\n2. Create a tuple containing names of your sisters and your brothers (imaginary siblings are fine)\n3. Join brothers and sisters tuples and assign it to siblings\n4. How many siblings do you have?\n5. Modify the siblings tuple and add the name of your father and mother and assign it to family_members\n\n### Exercises: Level 2\n\n1. Unpack siblings and parents from family_members\n1. Create fruits, vegetables and animal products tuples. Join the three tuples and assign it to a variable called food_stuff_tp.\n1. Change the about food_stuff_tp  tuple to a food_stuff_lt list\n1. Slice out the middle item or items from the food_stuff_tp tuple or food_stuff_lt list.\n1. Slice out the first three items and the last three items from food_staff_lt list\n1. Delete the food_staff_tp tuple completely\n1. Check if an item exists in  tuple:\n\n- Check if 'Estonia' is a nordic country\n- Check if 'Iceland' is a nordic country\n\n  ```py\n  nordic_countries = ('Denmark', 'Finland','Iceland', 'Norway', 'Sweden')\n  ```\n\n\n[<< Day 5](../05_Day_Lists/05_lists.md) | [Day 7 >>](../07_Day_Sets/07_sets.md)\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "06_Day_Tuples/06_tuples.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 255, "word_count": 1041}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 7 - Sets</h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n<sub>Author:\n<a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n<small> Second Edition: July, 2021</small>\n</sub>\n\n</div>\n\n[<< Day 6](../06_Day_Tuples/06_tuples.md) | [Day 8 >>](../08_Day_Dictionaries/08_dictionaries.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n\n- [üìò Day 7](#-day-7)\n  - [Sets](#sets)\n    - [Creating a Set](#creating-a-set)\n    - [Getting Set's Length](#getting-sets-length)\n    - [Accessing Items in a Set](#accessing-items-in-a-set)\n    - [Checking an Item](#checking-an-item)\n    - [Adding Items to a Set](#adding-items-to-a-set)\n    - [Removing Items from a Set](#removing-items-from-a-set)\n    - [Clearing Items in a Set](#clearing-items-in-a-set)\n    - [Deleting a Set](#deleting-a-set)\n    - [Converting List to Set](#converting-list-to-set)\n    - [Joining Sets](#joining-sets)\n    - [Finding Intersection Items](#finding-intersection-items)\n    - [Checking Subset and Super Set](#checking-subset-and-super-set)\n    - [Checking the Difference Between Two Sets](#checking-the-difference-between-two-sets)\n    - [Finding Symmetric Difference Between Two Sets](#finding-symmetric-difference-between-two-sets)\n    - [Joining Sets](#joining-sets-1)\n  - [üíª Exercises: Day 7](#-exercises-day-7)\n    - [Exercises: Level 1](#exercises-level-1)\n    - [Exercises: Level 2](#exercises-level-2)\n    - [Exercises: Level 3](#exercises-level-3)\n\n# üìò Day 7\n\n## Sets\n\nSet is a collection of items. Let me take you back to your elementary or high school Mathematics lesson. The Mathematics definition of a set can be applied also in Python. Set is a collection of unordered and un-indexed distinct elements. In Python set is used to store unique items, and it is possible to find the _union_, _intersection_, _difference_, _symmetric difference_, _subset_, _super set_ and _disjoint set_ among sets.\n\n### Creating a Set\n\nWe use the _set()_ built-in function.\n\n- Creating an empty set\n\n```py\n# syntax\nst = set()\n```\n\n- Creating a set with initial items\n\n```py\n# syntax\nst = {'item1', 'item2', 'item3', 'item4'}\n```\n\n**Example:**\n\n```py\n# syntax\nfruits = {'banana', 'orange', 'mango', 'lemon'}\n```\n\n### Getting Set's Length\n\nWe use **len()** method to find the length of a set.\n\n```py\n# syntax\nst = {'item1', 'item2', 'item3', 'item4'}\nlen(st)\n```\n\n**Example:**\n\n```py\nfruits = {'banana', 'orange', 'mango', 'lemon'}\nlen(fruits)\n```\n\n### Accessing Items in a Set\n\nWe use loops to access items. We will see this in loop section\n\n### Checking an Item\n\nTo check if an item exist in a list we use _in_ membership operator.\n\n```py\n# syntax\nst = {'item1', 'item2', 'item3', 'item4'}\nprint(\"Does set st contain item3? \", 'item3' in st) # Does set st contain item3? True\n```\n\n**Example:**\n\n```py\nfruits = {'banana', 'orange', 'mango', 'lemon'}\nprint('mango' in fruits ) # True\n```\n\n### Adding Items to a Set\n\nOnce a set is created we cannot change any items and we can also add additional items.\n\n- Add one item using _add()_\n\n```py\n# syntax\nst = {'item1', 'item2', 'item3', 'item4'}\nst.add('item5')\n```\n\n**Example:**\n\n```py\nfruits = {'banana', 'orange', 'mango', 'lemon'}\nfruits.add('lime')\n```\n\n- Add multiple items using _update()_\n  The _update()_ allows to add multiple items to a set. The _update()_ takes a list argument.\n\n```py\n# syntax\nst = {'item1', 'item2', 'item3', 'item4'}\nst.update(['item5','item6','item7'])\n```\n\n**Example:**\n\n```py\nfruits = {'banana', 'orange', 'mango', 'lemon'}\nvegetables = ('tomato', 'potato', 'cabbage','onion', 'carrot')\nfruits.update(vegetables)\n```\n\n### Removing Items from a Set\n\nWe can remove an item from a set using _remove()_ method. If the item is not found _remove()_ method will raise errors, so it is good to check if the item exist in the given set. However, _discard()_ method doesn't raise any errors.\n\n```py\n# syntax\nst = {'item1', 'item2', 'item3', 'item4'}\nst.remove('item2')\n```\n\nThe pop() methods remove a random item from a list and it returns the removed item.\n\n**Example:**\n\n```py\nfruits = {'banana', 'orange', 'mango', 'lemon'}\nfruits.pop()  # removes a random item from the set\n\n```\n\nIf we are interested in the removed item.\n\n```py\nfruits = {'banana', 'orange', 'mango', 'lemon'}\nremoved_item = fruits.pop() \n```\n\n### Clearing Items in a Set\n\nIf we want to clear or empty the set we use _clear_ method.\n\n```py\n# syntax\nst = {'item1', 'item2', 'item3', 'item4'}\nst.clear()\n```\n\n**Example:**\n\n```py\nfruits = {'banana', 'orange', 'mango', 'lemon'}\nfruits.clear()\nprint(fruits) # set()\n```\n\n### Deleting a Set\n\nIf we want to delete the set itself we use _del_ operator.\n\n```py\n# syntax\nst = {'item1', 'item2', 'item3', 'item4'}\ndel st\n```\n\n**Example:**\n\n```py\nfruits = {'banana', 'orange', 'mango', 'lemon'}\ndel fruits\n```\n\n### Converting List to Set\n\nWe can convert list to set and set to list. Converting list to set removes duplicates and only unique items will be reserved.\n\n```py\n# syntax\nlst = ['item1', 'item2', 'item3', 'item4', 'item1']\nst = set(lst)  # {'item2', 'item4', 'item1', 'item3'} - the order is random, because sets in general are unordered\n```\n\n**Example:**\n\n```py\nfruits = ['banana', 'orange', 'mango', 'lemon','orange', 'banana']\nfruits = set(fruits) # {'mango', 'lemon', 'banana', 'orange'}\n```\n\n### Joining Sets\n\nWe can join two sets using the _union()_ or _update()_ method.\n\n- Union\n  This method returns a new set\n\n```py\n# syntax\nst1 = {'item1', 'item2', 'item3', 'item4'}\nst2 = {'item5', 'item6', 'item7', 'item8'}\nst3 = st1.union(st2)\n```\n\n**Example:**\n\n```py\nfruits = {'banana', 'orange', 'mango', 'lemon'}\nvegetables = {'tomato', 'potato', 'cabbage','onion', 'carrot'}\nprint(fruits.union(vegetables)) # {'lemon', 'carrot', 'tomato', 'banana', 'mango', 'orange', 'cabbage', 'potato', 'onion'}\n```\n\n- Update\n  This method inserts a set into a given set\n\n```py\n# syntax\nst1 = {'item1', 'item2', 'item3', 'item4'}\nst2 = {'item5', 'item6', 'item7', 'item8'}\nst1.update(st2) # st2 contents are added to st1\n```\n\n**Example:**\n\n```py\nfruits = {'banana', 'orange', 'mango', 'lemon'}\nvegetables = {'tomato', 'potato', 'cabbage','onion', 'carrot'}\nfruits.update(vegetables)\nprint(fruits) # {'lemon', 'carrot', 'tomato', 'banana', 'mango', 'orange', 'cabbage', 'potato', 'onion'}\n```\n\n### Finding Intersection Items\n\nIntersection returns a set of items which are in both the sets. See the example\n\n```py\n# syntax\nst1 = {'item1', 'item2', 'item3', 'item4'}\nst2 = {'item3', 'item2'}\nst1.intersection(st2) # {'item3', 'item2'}\n```\n\n**Example:**\n\n```py\nwhole_numbers = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\neven_numbers = {0, 2, 4, 6, 8, 10}\nwhole_numbers.intersection(even_numbers) # {0, 2, 4, 6, 8, 10}\n\npython = {'p', 'y', 't', 'h', 'o','n'}\ndragon = {'d', 'r', 'a', 'g', 'o','n'}\npython.intersection(dragon)     # {'o', 'n'}\n```\n\n### Checking Subset and Super Set\n\nA set can be a subset or super set of other sets:\n\n- Subset: _issubset()_\n- Super set: _issuperset_\n\n```py\n# syntax\nst1 = {'item1', 'item2', 'item3', 'item4'}\nst2 = {'item2', 'item3'}\nst2.issubset(st1) # True\nst1.issuperset(st2) # True\n```\n\n**Example:**\n\n```py\nwhole_numbers = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\neven_numbers = {0, 2, 4, 6, 8, 10}\nwhole_numbers.issubset(even_numbers) # False, because it is a super set\nwhole_numbers.issuperset(even_numbers) # True\n\npython = {'p', 'y', 't', 'h', 'o','n'}\ndragon = {'d', 'r', 'a', 'g', 'o','n'}\npython.issubset(dragon)     # False\n```\n\n### Checking the Difference Between Two Sets\n\nIt returns the difference between two sets.\n\n```py\n# syntax\nst1 = {'item1', 'item2', 'item3', 'item4'}\nst2 = {'item2', 'item3'}\nst2.difference(st1) # set()\nst1.difference(st2) # {'item1', 'item4'} => st1\\st2\n```\n\n**Example:**\n\n```py\nwhole_numbers = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\neven_numbers = {0, 2, 4, 6, 8, 10}\nwhole_numbers.difference(even_numbers) # {1, 3, 5, 7, 9}\n\npython = {'p', 'y', 't', 'o','n'}\ndragon = {'d', 'r', 'a', 'g', 'o','n'}\npython.difference(dragon)     # {'p', 'y', 't'}  - the result is unordered (characteristic of sets)\ndragon.difference(python)     # {'d', 'r', 'a', 'g'}\n```\n\n### Finding Symmetric Difference Between Two Sets\n\nIt returns the symmetric difference between two sets. It means that it returns a set that contains all items from both sets, except items that are present in both sets, mathematically: (A\\B) ‚à™ (B\\A)\n\n```py\n# syntax\nst1 = {'item1', 'item2', 'item3', 'item4'}\nst2 = {'item2', 'item3'}\n# it means (A\\B)‚à™(B\\A)\nst2.symmetric_difference(st1) # {'item1', 'item4'}\n```\n\n**Example:**\n\n```py\nwhole_numbers = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\nsome_numbers = {1, 2, 3, 4, 5}\nwhole_numbers.symmetric_difference(some_numbers) # {0, 6, 7, 8, 9, 10}\n\npython = {'p', 'y', 't', 'h', 'o','n'}\ndragon = {'d', 'r', 'a', 'g', 'o','n'}\npython.symmetric_difference(dragon)  # {'r', 't', 'p', 'y', 'g', 'a', 'd', 'h'}\n```\n\n### Joining Sets\n\nIf two sets do not have a common item or items we call them disjoint sets. We can check if two sets are joint or disjoint using _isdisjoint()_ method.\n\n```py\n# syntax\nst1 = {'item1', 'item2', 'item3', 'item4'}\nst2 = {'item2', 'item3'}\nst2.isdisjoint(st1) # False\n```\n\n**Example:**\n\n```py\neven_numbers = {0, 2, 4 ,6, 8}\nodd_numbers = {1, 3, 5, 7, 9}\neven_numbers.isdisjoint(odd_numbers) # True, because no common item\n\npython = {'p', 'y', 't', 'h', 'o','n'}\ndragon = {'d', 'r', 'a', 'g', 'o','n'}\npython.isdisjoint(dragon)  # False, there are common items {'o', 'n'}\n```\n\nüåï You are a rising star . You have just completed day 7 challenges and you are 7 steps ahead in to your way to greatness. Now do some exercises for your brain and muscles.\n\n## üíª Exercises: Day 7\n\n```py\n# sets\nit_companies = {'Facebook', 'Google', 'Microsoft', 'Apple', 'IBM', 'Oracle', 'Amazon'}\nA = {19, 22, 24, 20, 25, 26}\nB = {19, 22, 20, 25, 26, 24, 28, 27}\nage = [22, 19, 24, 25, 26, 24, 25, 24]\n```\n\n### Exercises: Level 1\n\n1. Find the length of the set it_companies\n2. Add 'Twitter' to it_companies\n3. Insert multiple IT companies at once to the set it_companies\n4. Remove one of the companies from the set it_companies\n5. What is the difference between remove and discard\n\n### Exercises: Level 2\n\n1. Join A and B\n1. Find A intersection B\n1. Is A subset of B\n1. Are A and B disjoint sets\n1. Join A with B and B with A\n1. What is the symmetric difference between A and B\n1. Delete the sets completely\n\n### Exercises: Level 3\n\n1. Convert the ages to a set and compare the length of the list and the set, which one is bigger?\n1. Explain the difference between the following data types: string, list, tuple and set\n2. _I am a teacher and I love to inspire and teach people._ How many unique words have been used in the sentence? Use the split methods and set to get the unique words.\n\nüéâ CONGRATULATIONS ! üéâ\n\n[<< Day 6](../06_Day_Tuples/06_tuples.md) | [Day 8 >>](../08_Day_Dictionaries/08_dictionaries.md)\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "07_Day_Sets/07_sets.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 430, "word_count": 1646}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 8 - Dictionaries</h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n<sub>Author:\n<a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n<small> Second Edition: July, 2021</small>\n</sub>\n\n</div>\n\n[<< Day 7 ](../07_Day_Sets/07_sets.md) | [Day 9 >>](../09_Day_Conditionals/09_conditionals.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n\n- [üìò Day 8](#-day-8)\n  - [Dictionaries](#dictionaries)\n    - [Creating a Dictionary](#creating-a-dictionary)\n    - [Dictionary Length](#dictionary-length)\n    - [Accessing Dictionary Items](#accessing-dictionary-items)\n    - [Adding Items to a Dictionary](#adding-items-to-a-dictionary)\n    - [Modifying Items in a Dictionary](#modifying-items-in-a-dictionary)\n    - [Checking Keys in a Dictionary](#checking-keys-in-a-dictionary)\n    - [Removing Key and Value Pairs from a Dictionary](#removing-key-and-value-pairs-from-a-dictionary)\n    - [Changing Dictionary to a List of Items](#changing-dictionary-to-a-list-of-items)\n    - [Clearing a Dictionary](#clearing-a-dictionary)\n    - [Deleting a Dictionary](#deleting-a-dictionary)\n    - [Copy a Dictionary](#copy-a-dictionary)\n    - [Getting Dictionary Keys as a List](#getting-dictionary-keys-as-a-list)\n    - [Getting Dictionary Values as a List](#getting-dictionary-values-as-a-list)\n  - [üíª Exercises: Day 8](#-exercises-day-8)\n\n# üìò Day 8\n\n## Dictionaries\n\nA dictionary is a collection of unordered, modifiable(mutable) paired (key: value) data type.\n\n### Creating a Dictionary\n\nTo create a dictionary we use curly brackets, {} or the *dict()* built-in function.\n\n```py\n# syntax\nempty_dict = {}\n# Dictionary with data values\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\n```\n\n**Example:**\n\n```py\nperson = {\n    'first_name':'Asabeneh',\n    'last_name':'Yetayeh',\n    'age':250,\n    'country':'Finland',\n    'is_marred':True,\n    'skills':['JavaScript', 'React', 'Node', 'MongoDB', 'Python'],\n    'address':{\n        'street':'Space street',\n        'zipcode':'02210'\n    }\n    }\n```\n\nThe dictionary above shows that a value could be any data types:string, boolean, list, tuple, set or a dictionary.\n\n### Dictionary Length\n\nIt checks the number of 'key: value' pairs in the dictionary.\n\n```py\n# syntax\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\nprint(len(dct)) # 4\n```\n\n**Example:**\n\n```py\nperson = {\n    'first_name':'Asabeneh',\n    'last_name':'Yetayeh',\n    'age':250,\n    'country':'Finland',\n    'is_married':True,\n    'skills':['JavaScript', 'React', 'Node', 'MongoDB', 'Python'],\n    'address':{\n        'street':'Space street',\n        'zipcode':'02210'\n    }\n    }\nprint(len(person)) # 7\n\n```\n\n### Accessing Dictionary Items\n\nWe can access Dictionary items by referring to its key name.\n\n```py\n# syntax\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\nprint(dct['key1']) # value1\nprint(dct['key4']) # value4\n```\n\n**Example:**\n\n```py\nperson = {\n    'first_name':'Asabeneh',\n    'last_name':'Yetayeh',\n    'age':250,\n    'country':'Finland',\n    'is_marred':True,\n    'skills':['JavaScript', 'React', 'Node', 'MongoDB', 'Python'],\n    'address':{\n        'street':'Space street',\n        'zipcode':'02210'\n    }\n    }\nprint(person['first_name']) # Asabeneh\nprint(person['country'])    # Finland\nprint(person['skills'])     # ['JavaScript', 'React', 'Node', 'MongoDB', 'Python']\nprint(person['skills'][0])  # JavaScript\nprint(person['address']['street']) # Space street\nprint(person['city'])       # Error\n```\n\nAccessing an item by key name raises an error if the key does not exist. To avoid this error first we have to check if a key exist or we can use the _get_ method. The get method returns None, which is a NoneType object data type, if the key does not exist.\n```py\nperson = {\n    'first_name':'Asabeneh',\n    'last_name':'Yetayeh',\n    'age':250,\n    'country':'Finland',\n    'is_marred':True,\n    'skills':['JavaScript', 'React', 'Node', 'MongoDB', 'Python'],\n    'address':{\n        'street':'Space street',\n        'zipcode':'02210'\n    }\n    }\nprint(person.get('first_name')) # Asabeneh\nprint(person.get('country'))    # Finland\nprint(person.get('skills')) #['HTML','CSS','JavaScript', 'React', 'Node', 'MongoDB', 'Python']\nprint(person.get('city'))   # None\n```\n\n### Adding Items to a Dictionary\n\nWe can add new key and value pairs to a dictionary\n\n```py\n# syntax\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\ndct['key5'] = 'value5'\n```\n\n**Example:**\n\n```py\nperson = {\n    'first_name':'Asabeneh',\n    'last_name':'Yetayeh',\n    'age':250,\n    'country':'Finland',\n    'is_marred':True,\n    'skills':['JavaScript', 'React', 'Node', 'MongoDB', 'Python'],\n    'address':{\n        'street':'Space street',\n        'zipcode':'02210'\n        }\n}\nperson['job_title'] = 'Instructor'\nperson['skills'].append('HTML')\nprint(person)\n```\n\n### Modifying Items in a Dictionary\n\nWe can modify items in a dictionary\n\n```py\n# syntax\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\ndct['key1'] = 'value-one'\n```\n\n**Example:**\n\n```py\nperson = {\n    'first_name':'Asabeneh',\n    'last_name':'Yetayeh',\n    'age':250,\n    'country':'Finland',\n    'is_marred':True,\n    'skills':['JavaScript', 'React', 'Node', 'MongoDB', 'Python'],\n    'address':{\n        'street':'Space street',\n        'zipcode':'02210'\n    }\n    }\nperson['first_name'] = 'Eyob'\nperson['age'] = 252\n```\n\n### Checking Keys in a Dictionary\n\nWe use the _in_ operator to check if a key exist in a dictionary\n\n```py\n# syntax\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\nprint('key2' in dct) # True\nprint('key5' in dct) # False\n```\n\n### Removing Key and Value Pairs from a Dictionary\n\n- _pop(key)_: removes the item with the specified key name:\n- _popitem()_: removes the last item\n- _del_: removes an item with specified key name\n\n```py\n# syntax\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\ndct.pop('key1') # removes key1 item\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\ndct.popitem() # removes the last item\ndel dct['key2'] # removes key2 item\n```\n\n**Example:**\n\n```py\nperson = {\n    'first_name':'Asabeneh',\n    'last_name':'Yetayeh',\n    'age':250,\n    'country':'Finland',\n    'is_marred':True,\n    'skills':['JavaScript', 'React', 'Node', 'MongoDB', 'Python'],\n    'address':{\n        'street':'Space street',\n        'zipcode':'02210'\n    }\n    }\nperson.pop('first_name')        # Removes the firstname item\nperson.popitem()                # Removes the address item\ndel person['is_married']        # Removes the is_married item\n```\n\n### Changing Dictionary to a List of Items\n\nThe _items()_ method changes dictionary to a list of tuples.\n\n```py\n# syntax\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\nprint(dct.items()) # dict_items([('key1', 'value1'), ('key2', 'value2'), ('key3', 'value3'), ('key4', 'value4')])\n```\n\n### Clearing a Dictionary\n\nIf we don't want the items in a dictionary we can clear them using _clear()_ method\n\n```py\n# syntax\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\nprint(dct.clear()) # None\n```\n\n### Deleting a Dictionary\n\nIf we do not use the dictionary we can delete it completely\n\n```py\n# syntax\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\ndel dct\n```\n\n### Copy a Dictionary\n\nWe can copy a dictionary using a _copy()_ method. Using copy we can avoid mutation of the original dictionary.\n\n```py\n# syntax\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\ndct_copy = dct.copy() # {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\n```\n\n### Getting Dictionary Keys as a List\n\nThe _keys()_ method gives us all the keys of a a dictionary as a list.\n\n```py\n# syntax\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\nkeys = dct.keys()\nprint(keys)     # dict_keys(['key1', 'key2', 'key3', 'key4'])\n```\n\n### Getting Dictionary Values as a List\n\nThe _values_ method gives us all the values of a a dictionary as a list.\n\n```py\n# syntax\ndct = {'key1':'value1', 'key2':'value2', 'key3':'value3', 'key4':'value4'}\nvalues = dct.values()\nprint(values)     # dict_values(['value1', 'value2', 'value3', 'value4'])\n```\n\nüåï You are astonishing. Now, you are super charged with the power of dictionaries. You have just completed day 8 challenges and you are 8 steps a head in to your way to greatness. Now do some exercises for your brain and  muscles.\n\n## üíª Exercises: Day 8\n\n1. Create  an empty dictionary called dog\n2. Add name, color, breed, legs, age to the dog dictionary\n3. Create a student dictionary and add first_name, last_name, gender, age, marital status, skills, country, city and address as keys for the dictionary\n4. Get the length of the student dictionary\n5. Get the value of skills and check the data type, it should be a list\n6. Modify the skills values by adding one or two skills\n7. Get the dictionary keys as a list\n8. Get the dictionary values as a list\n9. Change the dictionary to a list of tuples using _items()_ method\n10. Delete one of the items in the dictionary\n11. Delete one of the dictionaries\n\nüéâ CONGRATULATIONS ! üéâ\n\n[<< Day 7 ](../07_Day_Sets/07_sets.md) | [Day 9 >>](../09_Day_Conditionals/09_conditionals.md)\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "08_Day_Dictionaries/08_dictionaries.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 343, "word_count": 1104}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 9 - Conditionals</h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n<sub>Author:\n<a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n<small> Second Edition: July, 2021</small>\n</sub>\n\n</div>\n\n[<< Day 8](../08_Day_Dictionaries/08_dictionaries.md) | [Day 10 >>](../10_Day_Loops/10_loops.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n\n- [üìò Day 9](#-day-9)\n  - [Conditionals](#conditionals)\n    - [If Condition](#if-condition)\n    - [If Else](#if-else)\n    - [If Elif Else](#if-elif-else)\n    - [Short Hand](#short-hand)\n    - [Nested Conditions](#nested-conditions)\n    - [If Condition and Logical Operators](#if-condition-and-logical-operators)\n    - [If and Or Logical Operators](#if-and-or-logical-operators)\n  - [üíª Exercises: Day 9](#-exercises-day-9)\n    - [Exercises: Level 1](#exercises-level-1)\n    - [Exercises: Level 2](#exercises-level-2)\n    - [Exercises: Level 3](#exercises-level-3)\n\n# üìò Day 9\n\n## Conditionals\n\nBy default, statements in Python script are executed sequentially from top to bottom. If the processing logic require so, the sequential flow of execution can be altered in two way:\n\n- Conditional execution: a block of one or more statements will be executed if a certain expression is true\n- Repetitive execution: a block of one or more statements will be repetitively executed as long as a certain expression is true. In this section, we will cover _if_, _else_, _elif_ statements. The comparison and logical operators we learned in previous sections will be useful here.\n\n### If Condition\n\nIn python and other programming languages the key word _if_ is used to check if a condition is true and to execute the block code. Remember the indentation after the colon.\n\n```py\n# syntax\nif condition:\n    this part of code runs for truthy conditions\n```\n\n**Example: 1**\n\n```py\na = 3\nif a > 0:\n    print('A is a positive number')\n# A is a positive number\n```\n\nAs you can see in the example above, 3 is greater than 0. The condition was true and the block code was executed. However, if the condition is false, we do not see the result. In order to see the result of the falsy condition, we should have another block, which is going to be _else_.\n\n### If Else\n\nIf condition is true the first block will be executed, if not the else condition will run.\n\n```py\n# syntax\nif condition:\n    this part of code runs for truthy conditions\nelse:\n     this part of code runs for false conditions\n```\n\n**Example:**\n\n```py\na = 3\nif a < 0:\n    print('A is a negative number')\nelse:\n    print('A is a positive number')\n```\n\nThe condition above proves false, therefore the else block was executed. How about if our condition is more than two? We could use _elif_.\n\n### If Elif Else\n\nIn our daily life, we make decisions on daily basis. We make decisions not by checking one or two conditions but multiple conditions. As similar to life, programming is also full of conditions. We use _elif_ when we have multiple conditions.\n\n```py\n# syntax\nif condition:\n    code\nelif condition:\n    code\nelse:\n    code\n\n```\n\n**Example:**\n\n```py\na = 0\nif a > 0:\n    print('A is a positive number')\nelif a < 0:\n    print('A is a negative number')\nelse:\n    print('A is zero')\n```\n\n### Short Hand\n\n```py\n# syntax\ncode if condition else code\n```\n\n**Example:**\n\n```py\na = 3\nprint('A is positive') if a > 0 else print('A is negative') # first condition met, 'A is positive' will be printed\n```\n\n### Nested Conditions\n\nConditions can be nested\n\n```py\n# syntax\nif condition:\n    code\n    if condition:\n    code\n```\n\n**Example:**\n\n```py\na = 0\nif a > 0:\n    if a % 2 == 0:\n        print('A is a positive and even integer')\n    else:\n        print('A is a positive number')\nelif a == 0:\n    print('A is zero')\nelse:\n    print('A is a negative number')\n\n```\n\nWe can avoid writing nested condition by using logical operator _and_.\n\n### If Condition and Logical Operators\n\n```py\n# syntax\nif condition and condition:\n    code\n```\n\n**Example:**\n\n```py\na = 0\nif a > 0 and a % 2 == 0:\n        print('A is an even and positive integer')\nelif a > 0 and a % 2 !=  0:\n     print('A is a positive integer')\nelif a == 0:\n    print('A is zero')\nelse:\n    print('A is negative')\n```\n\n### If and Or Logical Operators\n\n```py\n# syntax\nif condition or condition:\n    code\n```\n\n**Example:**\n\n```py\nuser = 'James'\naccess_level = 3\nif user == 'admin' or access_level >= 4:\n        print('Access granted!')\nelse:\n    print('Access denied!')\n```\n\nüåï You are doing great.Never give up because great things take time. You have just completed day 9 challenges and you are 9 steps a head in to your way to greatness. Now do some exercises for your brain and muscles.\n\n## üíª Exercises: Day 9\n\n### Exercises: Level 1\n\n1. Get user input using input(‚ÄúEnter your age: ‚Äù). If user is 18 or older, give feedback: You are old enough to drive. If below 18 give feedback to wait for the missing amount of years. Output:\n\n    ```sh\n    Enter your age: 30\n    You are old enough to learn to drive.\n    Output:\n    Enter your age: 15\n    You need 3 more years to learn to drive.\n    ```\n\n2. Compare the values of my_age and your_age using if ‚Ä¶ else. Who is older (me or you)? Use input(‚ÄúEnter your age: ‚Äù) to get the age as input. You can use a nested condition to print 'year' for 1 year difference in age, 'years' for bigger differences, and a custom text if my_age = your_age. Output:\n\n    ```sh\n    Enter your age: 30\n    You are 5 years older than me.\n    ```\n\n3. Get two numbers from the user using input prompt. If a is greater than b return a is greater than b, if a is less b return a is smaller than b, else a is equal to b. Output:\n\n```sh\nEnter number one: 4\nEnter number two: 3\n4 is greater than 3\n```\n\n### Exercises: Level 2\n\n   1. Write a code which gives grade to students according to theirs scores:\n\n        ```sh\n        80-100, A\n        70-89, B\n        60-69, C\n        50-59, D\n        0-49, F\n        ```\n\n   1. Check if the season is Autumn, Winter, Spring or Summer. If the user input is:\n    September, October or November, the season is Autumn.\n    December, January or February, the season is Winter.\n    March, April or May, the season is Spring\n    June, July or August, the season is Summer\n   2. The following list contains some fruits:\n\n    ```sh\n    fruits = ['banana', 'orange', 'mango', 'lemon']\n    ```\n\n    If a fruit doesn't exist in the list add the fruit to the list and print the modified list. If the fruit exists print('That fruit already exist in the list')\n\n### Exercises: Level 3\n\n   1. Here we have a person dictionary. Feel free to modify it!\n\n```py\n        person={\n    'first_name': 'Asabeneh',\n    'last_name': 'Yetayeh',\n    'age': 250,\n    'country': 'Finland',\n    'is_marred': True,\n    'skills': ['JavaScript', 'React', 'Node', 'MongoDB', 'Python'],\n    'address': {\n        'street': 'Space street',\n        'zipcode': '02210'\n    }\n    }\n```\n\n     * Check if the person dictionary has skills key, if so print out the middle skill in the skills list.\n     * Check if the person dictionary has skills key, if so check if the person has 'Python' skill and print out the result.\n     * If a person skills has only JavaScript and React, print('He is a front end developer'), if the person skills has Node, Python, MongoDB, print('He is a backend developer'), if the person skills has React, Node and MongoDB, Print('He is a fullstack developer'), else print('unknown title') - for more accurate results more conditions can be nested!\n     * If the person is married and if he lives in Finland, print the information in the following format:\n\n```py\n    Asabeneh Yetayeh lives in Finland. He is married.\n```\n\nüéâ CONGRATULATIONS ! üéâ\n\n[<< Day 8](../08_Day_Dictionaries/08_dictionaries.md) | [Day 10 >>](../10_Day_Loops/10_loops.md)\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "09_Day_Conditionals/09_conditionals.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 288, "word_count": 1253}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 10 - Loops</h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n<sub>Author:\n<a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n<small> Second Edition: July, 2021</small>\n</sub>\n</div>\n\n[<< Day 9](../09_Day_Conditionals/09_conditionals.md) | [Day 11 >>](../11_Day_Functions/11_functions.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n\n- [üìò Day 10](#-day-10)\n  - [Loops](#loops)\n    - [While Loop](#while-loop)\n    - [Break and Continue - Part 1](#break-and-continue---part-1)\n    - [For Loop](#for-loop)\n    - [Break and Continue - Part 2](#break-and-continue---part-2)\n    - [The Range Function](#the-range-function)\n    - [Nested For Loop](#nested-for-loop)\n    - [For Else](#for-else)\n    - [Pass](#pass)\n  - [üíª Exercises: Day 10](#-exercises-day-10)\n    - [Exercises: Level 1](#exercises-level-1)\n    - [Exercises: Level 2](#exercises-level-2)\n    - [Exercises: Level 3](#exercises-level-3)\n\n# üìò Day 10\n\n## Loops\n\nLife is full of routines. In programming we also do lots of repetitive tasks. In order to handle repetitive task programming languages use loops. Python programming language also provides the following types of two loops:\n\n1. while loop\n2. for loop\n\n### While Loop\n\nWe use the reserved word _while_ to make a while loop. It is used to execute a block of statements repeatedly until a given condition is satisfied. When the condition becomes false, the lines of code after the loop will be continued to be executed.\n\n```py\n  # syntax\nwhile condition:\n    code goes here\n```\n\n**Example:**\n\n```py\ncount = 0\nwhile count < 5:\n    print(count)\n    count = count + 1\n#prints from 0 to 4\n```\n\nIn the above while loop, the condition becomes false when count is 5. That is when the loop stops.\nIf we are interested to run block of code once the condition is no longer true, we can use _else_.\n\n```py\n  # syntax\nwhile condition:\n    code goes here\nelse:\n    code goes here\n```\n\n**Example:**\n\n```py\ncount = 0\nwhile count < 5:\n    print(count)\n    count = count + 1\nelse:\n    print(count)\n```\n\nThe above loop condition will be false when count is 5 and the loop stops, and execution starts the else statement. As a result 5 will be printed.\n\n\n### Break and Continue - Part 1\n\n- Break: We use break when we like to get out of or stop the loop.\n\n```py\n# syntax\nwhile condition:\n    code goes here\n    if another_condition:\n        break\n```\n\n**Example:**\n\n```py\ncount = 0\nwhile count < 5:\n    print(count)\n    count = count + 1\n    if count == 3:\n        break\n```\n\nThe above while loop only prints 0, 1, 2, but when it reaches 3 it stops.\n\n- Continue: With the continue statement we can skip the current iteration, and continue with the next:\n\n```py\n  # syntax\nwhile condition:\n    code goes here\n    if another_condition:\n        continue\n```\n\n**Example:**\n\n```py\ncount = 0\nwhile count < 5:\n    if count == 3:\n        count = count + 1\n        continue\n    print(count)\n    count = count + 1\n```\n\nThe above while loop only prints 0, 1, 2 and 4 (skips 3).\n\n### For Loop\n\nA _for_ keyword is used to make a for loop, similar with other programming languages, but with some syntax differences. Loop is used for iterating over a sequence (that is either a list, a tuple, a dictionary, a set, or a string).\n\n- For loop with list\n\n```py\n# syntax\nfor iterator in lst:\n    code goes here\n```\n\n**Example:**\n\n```py\nnumbers = [0, 1, 2, 3, 4, 5]\nfor number in numbers: # number is temporary name to refer to the list's items, valid only inside this loop\n    print(number)       # the numbers will be printed line by line, from 0 to 5\n```\n\n- For loop with string\n\n```py\n# syntax\nfor iterator in string:\n    code goes here\n```\n\n**Example:**\n\n```py\nlanguage = 'Python'\nfor letter in language:\n    print(letter)\n\n\nfor i in range(len(language)):\n    print(language[i])\n```\n\n- For loop with tuple\n\n```py\n# syntax\nfor iterator in tpl:\n    code goes here\n```\n\n**Example:**\n\n```py\nnumbers = (0, 1, 2, 3, 4, 5)\nfor number in numbers:\n    print(number)\n```\n\n- For loop with dictionary\n  Looping through a dictionary gives you the key of the dictionary.\n\n```py\n  # syntax\nfor iterator in dct:\n    code goes here\n```\n\n**Example:**\n\n```py\nperson = {\n    'first_name':'Asabeneh',\n    'last_name':'Yetayeh',\n    'age':250,\n    'country':'Finland',\n    'is_marred':True,\n    'skills':['JavaScript', 'React', 'Node', 'MongoDB', 'Python'],\n    'address':{\n        'street':'Space street',\n        'zipcode':'02210'\n    }\n}\nfor key in person:\n    print(key)\n\nfor key, value in person.items():\n    print(key, value) # this way we get both keys and values printed out\n```\n\n- Loops in set\n\n```py\n# syntax\nfor iterator in st:\n    code goes here\n```\n\n**Example:**\n\n```py\nit_companies = {'Facebook', 'Google', 'Microsoft', 'Apple', 'IBM', 'Oracle', 'Amazon'}\nfor company in it_companies:\n    print(company)\n```\n\n### Break and Continue - Part 2\n\nShort reminder:\n_Break_: We use break when we like to stop our loop before it is completed.\n\n```py\n# syntax\nfor iterator in sequence:\n    code goes here\n    if condition:\n        break\n```\n\n**Example:**\n\n```py\nnumbers = (0,1,2,3,4,5)\nfor number in numbers:\n    print(number)\n    if number == 3:\n        break\n```\n\nIn the above example, the loop stops when it reaches 3.\n\nContinue: We use continue when we like to skip some of the steps in the iteration of the loop.\n\n```py\n  # syntax\nfor iterator in sequence:\n    code goes here\n    if condition:\n        continue\n```\n\n**Example:**\n\n```py\nnumbers = (0,1,2,3,4,5)\nfor number in numbers:\n    print(number)\n    if number == 3:\n        continue\n    print('Next number should be ', number + 1) if number != 5 else print(\"loop's end\") # for short hand conditions need both if and else statements\nprint('outside the loop')\n```\n\nIn the example above, if the number equals 3, the step *after* the condition (but inside the loop) is skipped and the execution of the loop continues if there are any iterations left.\n\n### The Range Function\n\nThe _range()_ function is used list of numbers. The _range(start, end, step)_ takes three parameters: starting, ending and increment. By default it starts from 0 and the increment is 1. The range sequence needs at least 1 argument (end).\nCreating sequences using range\n\n```py\nlst = list(range(11)) \nprint(lst) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nst = set(range(1, 11))    # 2 arguments indicate start and end of the sequence, step set to default 1\nprint(st) # {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\nlst = list(range(0,11,2))\nprint(lst) # [0, 2, 4, 6, 8, 10]\nst = set(range(0,11,2))\nprint(st) #  {0, 2, 4, 6, 8, 10}\n```\n\n```py\n# syntax\nfor iterator in range(start, end, step):\n```\n\n**Example:**\n\n```py\nfor number in range(11):\n    print(number)   # prints 0 to 10, not including 11\n```\n\n### Nested For Loop\n\nWe can write loops inside a loop.\n\n```py\n# syntax\nfor x in y:\n    for t in x:\n        print(t)\n```\n\n**Example:**\n\n```py\nperson = {\n    'first_name': 'Asabeneh',\n    'last_name': 'Yetayeh',\n    'age': 250,\n    'country': 'Finland',\n    'is_marred': True,\n    'skills': ['JavaScript', 'React', 'Node', 'MongoDB', 'Python'],\n    'address': {\n        'street': 'Space street',\n        'zipcode': '02210'\n    }\n}\nfor key in person:\n    if key == 'skills':\n        for skill in person['skills']:\n            print(skill)\n```\n\n### For Else\n\nIf we want to execute some message when the loop ends, we use else.\n\n```py\n# syntax\nfor iterator in range(start, end, step):\n    do something\nelse:\n    print('The loop ended')\n```\n\n**Example:**\n\n```py\nfor number in range(11):\n    print(number)   # prints 0 to 10, not including 11\nelse:\n    print('The loop stops at', number)\n```\n\n### Pass\n\nIn python when statement is required (after semicolon), but we don't like to execute any code there, we can write the word _pass_ to avoid errors. Also we can use it as a placeholder, for future statements.\n\n**Example:**\n\n```py\nfor number in range(6):\n    pass\n```\n\nüåï You established a big milestone, you are unstoppable. Keep going! You have just completed day 10 challenges and you are 10 steps a head in to your way to greatness. Now do some exercises for your brain and muscles.\n\n## üíª Exercises: Day 10\n\n### Exercises: Level 1\n\n1. Iterate 0 to 10 using for loop, do the same using while loop.\n2. Iterate 10 to 0 using for loop, do the same using while loop.\n3. Write a loop that makes seven calls to print(), so we get on the output the following triangle:\n\n   ```py\n     #\n     ##\n     ###\n     ####\n     #####\n     ######\n     #######\n   ```\n\n4. Use nested loops to create the following:\n\n   ```sh\n   # # # # # # # #\n   # # # # # # # #\n   # # # # # # # #\n   # # # # # # # #\n   # # # # # # # #\n   # # # # # # # #\n   # # # # # # # #\n   # # # # # # # #\n   ```\n\n5. Print the following pattern:\n\n   ```sh\n   0 x 0 = 0\n   1 x 1 = 1\n   2 x 2 = 4\n   3 x 3 = 9\n   4 x 4 = 16\n   5 x 5 = 25\n   6 x 6 = 36\n   7 x 7 = 49\n   8 x 8 = 64\n   9 x 9 = 81\n   10 x 10 = 100\n   ```\n\n6. Iterate through the list, ['Python', 'Numpy','Pandas','Django', 'Flask'] using a for loop and print out the items.\n7. Use for loop to iterate from 0 to 100 and print only even numbers\n8. Use for loop to iterate from 0 to 100 and print only odd numbers\n   \n### Exercises: Level 2\n    \n1.  Use for loop to iterate from 0 to 100 and print the sum of all numbers.\n\n   ```sh\n   The sum of all numbers is 5050.\n   ```\n\n1. Use for loop to iterate from 0 to 100 and print the sum of all evens and the sum of all odds.\n\n    ```sh\n    The sum of all evens is 2550. And the sum of all odds is 2500.\n    ```\n\n### Exercises: Level 3\n\n1. Go to the data folder and use the [countries.py](https://github.com/Asabeneh/30-Days-Of-Python/blob/master/data/countries.py) file. Loop through the countries and extract all the countries containing the word _land_.\n1. This is a fruit list, ['banana', 'orange', 'mango', 'lemon'] reverse the order using loop.\n2. Go to the data folder and use the [countries_data.py](https://github.com/Asabeneh/30-Days-Of-Python/blob/master/data/countries-data.py) file. \n   1. What are the total number of languages in the data\n   2. Find the ten most spoken languages from the data\n   3. Find the 10 most populated countries in the world\n\nüéâ CONGRATULATIONS ! üéâ\n\n[<< Day 9](../09_Day_Conditionals/09_conditionals.md) | [Day 11 >>](../11_Day_Functions/11_functions.md)\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "10_Day_Loops/10_loops.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 462, "word_count": 1684}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 11 - Functions</h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n<sub>Author:\n<a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n<small> Second Edition: July, 2021</small>\n</sub>\n\n</div>\n\n[<< Day 10](../10_Day_Loops/10_loops.md) | [Day 12 >>](../12_Day_Modules/12_modules.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n\n- [üìò Day 11](#-day-11)\n  - [Functions](#functions)\n    - [Defining a Function](#defining-a-function)\n    - [Declaring and Calling a Function](#declaring-and-calling-a-function)\n    - [Function without Parameters](#function-without-parameters)\n    - [Function Returning a Value - Part 1](#function-returning-a-value---part-1)\n    - [Function with Parameters](#function-with-parameters)\n    - [Passing Arguments with Key and Value](#passing-arguments-with-key-and-value)\n    - [Function Returning a Value - Part 2](#function-returning-a-value---part-2)\n    - [Function with Default Parameters](#function-with-default-parameters)\n    - [Arbitrary Number of Arguments](#arbitrary-number-of-arguments)\n    - [Default and Arbitrary Number of Parameters in Functions](#default-and-arbitrary-number-of-parameters-in-functions)\n    - [Function as a Parameter of Another Function](#function-as-a-parameter-of-another-function)\n  - [Testimony](#testimony)\n  - [üíª Exercises: Day 11](#-exercises-day-11)\n    - [Exercises: Level 1](#exercises-level-1)\n    - [Exercises: Level 2](#exercises-level-2)\n    - [Exercises: Level 3](#exercises-level-3)\n\n# üìò Day 11\n\n## Functions\n\nSo far we have seen many built-in Python functions. In this section, we will focus on custom functions. What is a function? Before we start making functions, let us learn what a function is and why we need them?\n\n### Defining a Function\n\nA function is a reusable block of code or programming statements designed to perform a certain task. To define or declare a function, Python provides the _def_ keyword. The following is the syntax for defining a function. The function block of code is executed only if the function is called or invoked.\n\n### Declaring and Calling a Function\n\nWhen we make a function, we call it declaring a function. When we start using the it,  we call it _calling_ or _invoking_ a function. Function can be declared with or without parameters.\n\n```py\n# syntax\n# Declaring a function\ndef function_name():\n    codes\n    codes\n# Calling a function\nfunction_name()\n```\n\n### Function without Parameters\n\nFunction can be declared without parameters.\n\n**Example:**\n\n```py\ndef generate_full_name ():\n    first_name = 'Asabeneh'\n    last_name = 'Yetayeh'\n    space = ' '\n    full_name = first_name + space + last_name\n    print(full_name)\ngenerate_full_name () # calling a function\n\ndef add_two_numbers ():\n    num_one = 2\n    num_two = 3\n    total = num_one + num_two\n    print(total)\nadd_two_numbers()\n```\n\n### Function Returning a Value - Part 1\n\nFunction can also return values, if a function does not have a return statement, the value of the function is None. Let us rewrite the above functions using return. From now on, we get a value from a function when we call the function and print it.\n\n```py\ndef generate_full_name ():\n    first_name = 'Asabeneh'\n    last_name = 'Yetayeh'\n    space = ' '\n    full_name = first_name + space + last_name\n    return full_name\nprint(generate_full_name())\n\ndef add_two_numbers ():\n    num_one = 2\n    num_two = 3\n    total = num_one + num_two\n    return total\nprint(add_two_numbers())\n```\n\n### Function with Parameters\n\nIn a function we can pass different data types(number, string, boolean, list, tuple, dictionary or set) as a parameter\n\n- Single Parameter: If our function takes a parameter we should call our function with an argument\n\n```py\n  # syntax\n  # Declaring a function\n  def function_name(parameter):\n    codes\n    codes\n  # Calling function\n  print(function_name(argument))\n```\n\n**Example:**\n\n```py\ndef greetings (name):\n    message = name + ', welcome to Python for Everyone!'\n    return message\n\nprint(greetings('Asabeneh'))\n\ndef add_ten(num):\n    ten = 10\n    return num + ten\nprint(add_ten(90))\n\ndef square_number(x):\n    return x * x\nprint(square_number(2))\n\ndef area_of_circle (r):\n    PI = 3.14\n    area = PI * r ** 2\n    return area\nprint(area_of_circle(10))\n\ndef sum_of_numbers(n):\n    total = 0\n    for i in range(n+1):\n        total+=i\n    print(total)\nprint(sum_of_numbers(10)) # 55\nprint(sum_of_numbers(100)) # 5050\n```\n\n- Two Parameter: A function may or may not have a parameter or parameters. A function may also have two or more parameters. If our function takes parameters we should call it with arguments. Let us check a function with two parameters:\n\n```py\n  # syntax\n  # Declaring a function\n  def function_name(para1, para2):\n    codes\n    codes\n  # Calling function\n  print(function_name(arg1, arg2))\n```\n\n**Example:**\n\n```py\ndef generate_full_name (first_name, last_name):\n    space = ' '\n      full_name = first_name + space + last_name\n      return full_name\nprint('Full Name: ', generate_full_name('Asabeneh','Yetayeh'))\n\ndef sum_two_numbers (num_one, num_two):\n    sum = num_one + num_two\n    return sum\nprint('Sum of two numbers: ', sum_two_numbers(1, 9))\n\ndef calculate_age (current_year, birth_year):\n    age = current_year - birth_year\n    return age;\n\nprint('Age: ', calculate_age(2021, 1819))\n\ndef weight_of_object (mass, gravity):\n    weight = str(mass * gravity)+ ' N' # the value has to be changed to a string first\n    return weight\nprint('Weight of an object in Newtons: ', weight_of_object(100, 9.81))\n```\n\n### Passing Arguments with Key and Value\n\nIf we pass the arguments with key and value, the order of the arguments does not matter.\n\n```py\n# syntax\n# Declaring a function\ndef function_name(para1, para2):\n    codes\n    codes\n# Calling function\nprint(function_name(para1 = 'John', para2 = 'Doe')) # the order of arguments does not matter here\n```\n\n**Example:**\n\n```py\ndef print_fullname(firstname, lastname):\n    space = ' '\n    full_name = firstname  + space + lastname\n    print(full_name)\nprint(print_fullname(firstname = 'Asabeneh', lastname = 'Yetayeh'))\n\ndef add_two_numbers (num1, num2):\n    total = num1 + num2\n    print(total)\nprint(add_two_numbers(num2 = 3, num1 = 2)) # Order does not matter\n```\n\n### Function Returning a Value - Part 2\n\nIf we do not return a value with a function, then our function is returning _None_ by default. To return a value with a function we use the keyword _return_ followed by the variable we are returning. We can return any kind of data types from a function.\n\n- Returning a string:\n**Example:**\n\n```py\ndef print_name(firstname):\n    return firstname\nprint_name('Asabeneh') # Asabeneh\n\ndef print_full_name(firstname, lastname):\n    space = ' '\n    full_name = firstname  + space + lastname\n    return full_name\nprint_full_name(firstname='Asabeneh', lastname='Yetayeh')\n```\n\n- Returning a number:\n\n**Example:**\n\n```py\ndef add_two_numbers (num1, num2):\n    total = num1 + num2\n    return total\nprint(add_two_numbers(2, 3))\n\ndef calculate_age (current_year, birth_year):\n    age = current_year - birth_year\n    return age;\nprint('Age: ', calculate_age(2019, 1819))\n```\n\n- Returning a boolean:\n  **Example:**\n\n```py\ndef is_even (n):\n    if n % 2 == 0:\n        print('even')\n        return True    # return stops further execution of the function, similar to break \n    return False\nprint(is_even(10)) # True\nprint(is_even(7)) # False\n```\n\n- Returning a list:\n  **Example:**\n\n```py\ndef find_even_numbers(n):\n    evens = []\n    for i in range(n + 1):\n        if i % 2 == 0:\n            evens.append(i)\n    return evens\nprint(find_even_numbers(10))\n```\n\n### Function with Default Parameters\n\nSometimes we pass default values to parameters, when we invoke the function. If we do not pass arguments when calling the function, their default values will be used.\n\n```py\n# syntax\n# Declaring a function\ndef function_name(param = value):\n    codes\n    codes\n# Calling function\nfunction_name()\nfunction_name(arg)\n```\n\n**Example:**\n\n```py\ndef greetings (name = 'Peter'):\n    message = name + ', welcome to Python for Everyone!'\n    return message\nprint(greetings())\nprint(greetings('Asabeneh'))\n\ndef generate_full_name (first_name = 'Asabeneh', last_name = 'Yetayeh'):\n    space = ' '\n    full_name = first_name + space + last_name\n    return full_name\n\nprint(generate_full_name())\nprint(generate_full_name('David','Smith'))\n\ndef calculate_age (birth_year,current_year = 2021):\n    age = current_year - birth_year\n    return age;\nprint('Age: ', calculate_age(1821))\n\ndef weight_of_object (mass, gravity = 9.81):\n    weight = str(mass * gravity)+ ' N' # the value has to be changed to string first\n    return weight\nprint('Weight of an object in Newtons: ', weight_of_object(100)) # 9.81 - average gravity on Earth's surface\nprint('Weight of an object in Newtons: ', weight_of_object(100, 1.62)) # gravity on the surface of the Moon\n```\n\n### Arbitrary Number of Arguments\n\nIf we do not know the number of arguments we pass to our function, we can create a function which can take arbitrary number of arguments by adding \\* before the parameter name.\n\n```py\n# syntax\n# Declaring a function\ndef function_name(*args):\n    codes\n    codes\n# Calling function\nfunction_name(param1, param2, param3,..)\n```\n\n**Example:**\n\n```py\ndef sum_all_nums(*nums):\n    total = 0\n    for num in nums:\n        total += num     # same as total = total + num \n    return total\nprint(sum_all_nums(2, 3, 5)) # 10\n```\n\n### Default and Arbitrary Number of Parameters in Functions\n\n```py\ndef generate_groups (team,*args):\n    print(team)\n    for i in args:\n        print(i)\nprint(generate_groups('Team-1','Asabeneh','Brook','David','Eyob'))\n```\n\n### Function as a Parameter of Another Function\n\n```py\n#You can pass functions around as parameters\ndef square_number (n):\n    return n * n\ndef do_something(f, x):\n    return f(x)\nprint(do_something(square_number, 3)) # 27\n```\n\nüåï You achieved quite a lot so far.  Keep going! You have just completed day 11 challenges and you are 11 steps a head in to your way to greatness. Now do some exercises for your brain and muscles.\n\n## Testimony\n\nNow it is time to express your thoughts about the Author and 30DaysOfPython. You can leave your testimonial on this [link](https://testimonial-s3sw.onrender.com/)\n\n## üíª Exercises: Day 11\n\n### Exercises: Level 1\n\n1. Declare a function _add_two_numbers_. It takes two parameters and it returns a sum.\n2. Area of a circle is calculated as follows: area = œÄ x r x r. Write a function that calculates _area_of_circle_.\n3. Write a function called add_all_nums which takes arbitrary number of arguments and sums all the arguments. Check if all the list items are number types. If not do give a reasonable feedback.\n4. Temperature in ¬∞C can be converted to ¬∞F using this formula: ¬∞F = (¬∞C x 9/5) + 32. Write a function which converts ¬∞C to ¬∞F, _convert_celsius_to-fahrenheit_.\n5. Write a function called check-season, it takes a month parameter and returns the season: Autumn, Winter, Spring or Summer.\n6. Write a function called calculate_slope which return the slope of a linear equation\n7. Quadratic equation is calculated as follows: ax¬≤ + bx + c = 0. Write a function which calculates solution set of a quadratic equation, _solve_quadratic_eqn_.\n8. Declare a function named print_list. It takes a list as a parameter and it prints out each element of the list.\n9. Declare a function named reverse_list. It takes an array as a parameter and it returns the reverse of the array (use loops).\n\n```py\nprint(reverse_list([1, 2, 3, 4, 5]))\n# [5, 4, 3, 2, 1]\nprint(reverse_list1([\"A\", \"B\", \"C\"]))\n# [\"C\", \"B\", \"A\"]\n```\n\n10. Declare a function named capitalize_list_items. It takes a list as a parameter and it returns a capitalized list of items\n11. Declare a function named add_item. It takes a list and an item parameters. It returns a list with the item added at the end.\n\n```py\nfood_staff = ['Potato', 'Tomato', 'Mango', 'Milk']\nprint(add_item(food_staff, 'Meat'))     # ['Potato', 'Tomato', 'Mango', 'Milk','Meat']\nnumbers = [2, 3, 7, 9]\nprint(add_item(numbers, 5))      [2, 3, 7, 9, 5]\n```\n\n12. Declare a function named remove_item. It takes a list and an item parameters. It returns a list with the item removed from it.\n\n```py\nfood_staff = ['Potato', 'Tomato', 'Mango', 'Milk']\nprint(remove_item(food_staff, 'Mango'))  # ['Potato', 'Tomato', 'Milk'];\nnumbers = [2, 3, 7, 9]\nprint(remove_item(numbers, 3))  # [2, 7, 9]\n```\n\n13. Declare a function named sum_of_numbers. It takes a number parameter and it adds all the numbers in that range.\n\n```py\nprint(sum_of_numbers(5))  # 15\nprint(sum_of_numbers(10)) # 55\nprint(sum_of_numbers(100)) # 5050\n```\n\n14. Declare a function named sum_of_odds. It takes a number parameter and it adds all the odd numbers in that range.\n15. Declare a function named sum_of_even. It takes a number parameter and it adds all the even numbers in that - range.\n\n### Exercises: Level 2\n\n1. Declare a function named evens_and_odds . It takes a positive integer as parameter and it counts number of evens and odds in the number.\n\n```py\n    print(evens_and_odds(100))\n    # The number of odds are 50.\n    # The number of evens are 51.\n```\n\n1. Call your function factorial, it takes a whole number as a parameter and it return a factorial of the number\n1. Call your function _is_empty_, it takes a parameter and it checks if it is empty or not\n1. Write different functions which take lists. They should calculate_mean, calculate_median, calculate_mode, calculate_range, calculate_variance, calculate_std (standard deviation).\n\n### Exercises: Level 3\n\n1. Write a function called is_prime, which checks if a number is prime.\n1. Write a functions which checks if all items are unique in the list.\n1. Write a function which checks if all the items of the list are of the same data type.\n1. Write a function which check if provided variable is a valid python variable\n1. Go to the data folder and access the countries-data.py file.\n\n- Create a function called the most_spoken_languages in the world. It should return 10 or 20 most spoken languages in the world in descending order\n- Create a function called the most_populated_countries. It should return 10 or 20 most populated countries in descending order.\n\nüéâ CONGRATULATIONS ! üéâ\n\n[<< Day 10](../10_Day_Loops/10_loops.md) | [Day 12 >>](../12_Day_Modules/12_modules.md)\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "11_Day_Functions/11_functions.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.8600000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 460, "word_count": 2047}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 12 - Modules </h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n<sub>Author:\n<a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n<small> Second Edition: July, 2021</small>\n</sub>\n\n</div>\n</div>\n\n[<< Day 11](../11_Day_Functions/11_functions.md) | [Day 13>>](../13_Day_List_comprehension/13_list_comprehension.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n\n- [üìò Day 12](#-day-12)\n  - [Modules](#modules)\n    - [What is a Module](#what-is-a-module)\n    - [Creating a Module](#creating-a-module)\n    - [Importing a Module](#importing-a-module)\n    - [Import Functions from a Module](#import-functions-from-a-module)\n    - [Import Functions from a Module and Renaming](#import-functions-from-a-module-and-renaming)\n  - [Import Built-in Modules](#import-built-in-modules)\n    - [OS Module](#os-module)\n    - [Sys Module](#sys-module)\n    - [Statistics Module](#statistics-module)\n    - [Math Module](#math-module)\n    - [String Module](#string-module)\n    - [Random Module](#random-module)\n  - [üíª Exercises: Day 12](#-exercises-day-12)\n    - [Exercises: Level 1](#exercises-level-1)\n    - [Exercises: Level 2](#exercises-level-2)\n    - [Exercises: Level 3](#exercises-level-3)\n\n# üìò Day 12\n\n## Modules\n\n### What is a Module\n\nA module is a file containing a set of codes or a set of functions which can be included to an application. A module could be a file containing a single variable, a function or a big code base.\n\n### Creating a Module\n\nTo create a module we write our codes in a python script and we save it as a .py file. Create a file named mymodule.py inside your project folder. Let us write some code in this file.\n\n```py\n# mymodule.py file\ndef generate_full_name(firstname, lastname):\n    return firstname + ' ' + lastname\n```\n\nCreate main.py file in your project directory and import the mymodule.py file.\n\n### Importing a Module\n\nTo import the file we use the _import_ keyword and the name of the file only.\n\n```py\n# main.py file\nimport mymodule\nprint(mymodule.generate_full_name('Asabeneh', 'Yetayeh')) # Asabeneh Yetayeh\n```\n\n### Import Functions from a Module\n\nWe can have many functions in a file and we can import all the functions differently.\n\n```py\n# main.py file\nfrom mymodule import generate_full_name, sum_two_nums, person, gravity\nprint(generate_full_name('Asabneh','Yetayeh'))\nprint(sum_two_nums(1,9))\nmass = 100;\nweight = mass * gravity\nprint(weight)\nprint(person['firstname'])\n```\n\n### Import Functions from a Module and Renaming\n\nDuring importing we can rename the name of the module.\n\n```py\n# main.py file\nfrom mymodule import generate_full_name as fullname, sum_two_nums as total, person as p, gravity as g\nprint(fullname('Asabneh','Yetayeh'))\nprint(total(1, 9))\nmass = 100;\nweight = mass * g\nprint(weight)\nprint(p)\nprint(p['firstname'])\n```\n\n## Import Built-in Modules\n\nLike other programming languages we can also import modules by importing the file/function using the key word _import_. Let's import the common module we will use most of the time. Some of the common built-in modules: _math_, _datetime_, _os_,_sys_, _random_, _statistics_, _collections_, _json_,_re_\n\n### OS Module\n\nUsing python _os_ module it is possible to automatically perform many operating system tasks. The OS module in Python provides functions for creating, changing current working directory, and removing a directory (folder), fetching its contents, changing and identifying the current directory.\n\n```py\n# import the module\nimport os\n# Creating a directory\nos.mkdir('directory_name')\n# Changing the current directory\nos.chdir('path')\n# Getting current working directory\nos.getcwd()\n# Removing directory\nos.rmdir()\n```\n\n### Sys Module\n\nThe sys module provides functions and variables used to manipulate different parts of the Python runtime environment. Function sys.argv returns a list of command line arguments passed to a Python script. The item at index 0 in this list is always the name of the script, at index 1 is the argument passed from the command line.\n\nExample of a script.py file:\n\n```py\nimport sys\n#print(sys.argv[0], argv[1],sys.argv[2])  # this line would print out: filename argument1 argument2\nprint('Welcome {}. Enjoy  {} challenge!'.format(sys.argv[1], sys.argv[2]))\n```\n\nNow to check how this script works I wrote in command line:\n\n```sh\npython script.py Asabeneh 30DaysOfPython\n```\n\nThe result:\n\n```sh\nWelcome Asabeneh. Enjoy  30DayOfPython challenge! \n```\n\nSome useful sys commands:\n\n```py\n# to exit sys\nsys.exit()\n# To know the largest integer variable it takes\nsys.maxsize\n# To know environment path\nsys.path\n# To know the version of python you are using\nsys.version\n```\n\n### Statistics Module\n\nThe statistics module provides functions for mathematical statistics of numeric data. The popular statistical functions which are defined in this module: _mean_, _median_, _mode_, _stdev_ etc.\n\n```py\nfrom statistics import * # importing all the statistics modules\nages = [20, 20, 4, 24, 25, 22, 26, 20, 23, 22, 26]\nprint(mean(ages))       # ~22.9\nprint(median(ages))     # 23\nprint(mode(ages))       # 20\nprint(stdev(ages))      # ~2.3\n```\n\n### Math Module\n\nModule containing many mathematical operations and constants.\n\n```py\nimport math\nprint(math.pi)           # 3.141592653589793, pi constant\nprint(math.sqrt(2))      # 1.4142135623730951, square root\nprint(math.pow(2, 3))    # 8.0, exponential function\nprint(math.floor(9.81))  # 9, rounding to the lowest\nprint(math.ceil(9.81))   # 10, rounding to the highest\nprint(math.log10(100))   # 2, logarithm with 10 as base\n```\n\nNow, we have imported the *math* module which contains lots of function which can help us to perform mathematical calculations. To check what functions the module has got, we can use _help(math)_, or _dir(math)_. This will display the available functions in the module. If we want to import only a specific function from the module we import it as follows:\n\n```py\nfrom math import pi\nprint(pi)\n```\n\nIt is also possible to import multiple functions at once\n\n```py\n\nfrom math import pi, sqrt, pow, floor, ceil, log10\nprint(pi)                 # 3.141592653589793\nprint(sqrt(2))            # 1.4142135623730951\nprint(pow(2, 3))          # 8.0\nprint(floor(9.81))        # 9\nprint(ceil(9.81))         # 10\nprint(math.log10(100))    # 2\n\n```\n\nBut if we want to import all the function in math module we can use \\* .\n\n```py\nfrom math import *\nprint(pi)                  # 3.141592653589793, pi constant\nprint(sqrt(2))             # 1.4142135623730951, square root\nprint(pow(2, 3))           # 8.0, exponential\nprint(floor(9.81))         # 9, rounding to the lowest\nprint(ceil(9.81))          # 10, rounding to the highest\nprint(math.log10(100))     # 2\n```\n\nWhen we import we can also rename the name of the function.\n\n```py\nfrom math import pi as  PI\nprint(PI) # 3.141592653589793\n```\n\n### String Module\n\nA string module is a useful module for many purposes. The example below shows some use of the string module.\n\n```py\nimport string\nprint(string.ascii_letters) # abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\nprint(string.digits)        # 0123456789\nprint(string.punctuation)   # !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n```\n\n### Random Module\n\nBy now you are familiar with importing modules. Let us do one more import to get very familiar with it. Let us import _random_ module which gives us a random number between 0 and 0.9999.... The _random_ module has lots of functions but in this section we will only use _random_ and _randint_.\n\n```py\nfrom random import random, randint\nprint(random())   # it doesn't take any arguments; it returns a value between 0 and 0.9999\nprint(randint(5, 20)) # it returns a random integer number between [5, 20] inclusive\n```\n\nüåï You are going far. Keep going! You have just completed day 12 challenges and you are 12 steps a head in to your way to greatness. Now do some exercises for your brain and muscles.\n\n## üíª Exercises: Day 12\n\n### Exercises: Level 1\n\n1. Writ a function which generates a six digit/character random_user_id.\n   ```py\n     print(random_user_id());\n     '1ee33d'\n   ```\n2. Modify the previous task. Declare a function named user_id_gen_by_user. It doesn‚Äôt take any parameters but it takes two inputs using input(). One of the inputs is the number of characters and the second input is the number of IDs which are supposed to be generated.\n   \n```py\nprint(user_id_gen_by_user()) # user input: 5 5\n#output:\n#kcsy2\n#SMFYb\n#bWmeq\n#ZXOYh\n#2Rgxf\n   \nprint(user_id_gen_by_user()) # 16 5\n#1GCSgPLMaBAVQZ26\n#YD7eFwNQKNs7qXaT\n#ycArC5yrRupyG00S\n#UbGxOFI7UXSWAyKN\n#dIV0SSUTgAdKwStr\n```\n\n3. Write a function named rgb_color_gen. It will generate rgb colors (3 values ranging from 0 to 255 each).\n   \n```py\nprint(rgb_color_gen())\n# rgb(125,244,255) - the output should be in this form\n```\n\n### Exercises: Level 2\n\n1. Write a function list_of_hexa_colors which returns any number of hexadecimal colors in an array (six hexadecimal numbers written after #. Hexadecimal numeral system is made out of 16 symbols, 0-9 and first 6 letters of the alphabet, a-f. Check the task 6 for output examples).\n1. Write a function list_of_rgb_colors which returns any number of RGB colors in an array.\n1. Write a function generate_colors which can generate any number of hexa or rgb colors.\n\n```py\n   generate_colors('hexa', 3) # ['#a3e12f','#03ed55','#eb3d2b'] \n   generate_colors('hexa', 1) # ['#b334ef']\n   generate_colors('rgb', 3)  # ['rgb(5, 55, 175','rgb(50, 105, 100','rgb(15, 26, 80'] \n   generate_colors('rgb', 1)  # ['rgb(33,79, 176)']\n   ```\n\n### Exercises: Level 3\n\n1. Call your function shuffle_list, it takes a list as a parameter and it returns a shuffled list\n1. Write a function which returns an array of seven random numbers in a range of 0-9. All the numbers must be unique.\n\nüéâ CONGRATULATIONS ! üéâ\n\n[<< Day 11](../11_Day_Functions/11_functions.md) | [Day 13>>](../13_Day_List_comprehension/13_list_comprehension.md)\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "12_Day_Modules/12_modules.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.89, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 306, "word_count": 1389}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 13 - List Comprehension</h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n<sub>Author:\n<a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n<small> Second Edition: July, 2021</small>\n</sub>\n\n</div>\n</div>\n\n[<< Day 12](../12_Day_Modules/12_modules.md) | [Day 14>>](../14_Day_Higher_order_functions/14_higher_order_functions.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n\n- [üìò Day 13](#-day-13)\n  - [List Comprehension](#list-comprehension)\n  - [Lambda Function](#lambda-function)\n    - [Creating a Lambda Function](#creating-a-lambda-function)\n    - [Lambda Function Inside Another Function](#lambda-function-inside-another-function)\n  - [üíª Exercises: Day 13](#-exercises-day-13)\n\n# üìò Day 13\n\n## List Comprehension\n\nList comprehension in Python is a compact way of creating a list from a sequence. It is a short way to create a new list. List comprehension is considerably faster than processing a list using the _for_ loop.\n\n```py\n# syntax\n[i for i in iterable if expression]\n```\n\n**Example:1**\n\nFor instance if you want to change a string to a list of characters. You can use a couple of methods. Let's see some of them:\n\n```py\n# One way\nlanguage = 'Python'\nlst = list(language) # changing the string to list\nprint(type(lst))     # list\nprint(lst)           # ['P', 'y', 't', 'h', 'o', 'n']\n\n# Second way: list comprehension\nlst = [i for i in language]\nprint(type(lst)) # list\nprint(lst)       # ['P', 'y', 't', 'h', 'o', 'n']\n\n```\n\n**Example:2**\n\nFor instance if you want to generate a list of numbers\n\n```py\n# Generating numbers\nnumbers = [i for i in range(11)]  # to generate numbers from 0 to 10\nprint(numbers)                    # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n# It is possible to do mathematical operations during iteration\nsquares = [i * i for i in range(11)]\nprint(squares)                    # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\n# It is also possible to make a list of tuples\nnumbers = [(i, i * i) for i in range(11)]\nprint(numbers)                             # [(0, 0), (1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n\n```\n\n**Example:2**\n\nList comprehension can be combined with if expression\n\n\n```py\n# Generating even numbers\neven_numbers = [i for i in range(21) if i % 2 == 0]  # to generate even numbers list in range 0 to 21\nprint(even_numbers)                    # [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n\n# Generating odd numbers\nodd_numbers = [i for i in range(21) if i % 2 != 0]  # to generate odd numbers in range 0 to 21\nprint(odd_numbers)                      # [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n# Filter numbers: let's filter out positive even numbers from the list below\nnumbers = [-8, -7, -3, -1, 0, 1, 3, 4, 5, 7, 6, 8, 10]\npositive_even_numbers = [i for i in numbers if i % 2 == 0 and i > 0]\nprint(positive_even_numbers)                    # [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n\n# Flattening a three dimensional array\nlist_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nflattened_list = [ number for row in list_of_lists for number in row]\nprint(flattened_list)    # [1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\n\n## Lambda Function\n\nLambda function is a small anonymous function without a name. It can take any number of arguments, but can only have one expression. Lambda function is similar to anonymous functions in JavaScript. We need it when we want to write an anonymous function inside another function.\n\n### Creating a Lambda Function\n\nTo create a lambda function we use _lambda_ keyword followed by a parameter(s), followed by an expression. See the syntax and the example below. Lambda function does not use return but it explicitly returns the expression.\n\n```py\n# syntax\nx = lambda param1, param2, param3: param1 + param2 + param2\nprint(x(arg1, arg2, arg3))\n```\n\n**Example:**\n\n```py\n# Named function\ndef add_two_nums(a, b):\n    return a + b\n\nprint(add_two_nums(2, 3))     # 5\n# Lets change the above function to a lambda function\nadd_two_nums = lambda a, b: a + b\nprint(add_two_nums(2,3))    # 5\n\n# Self invoking lambda function\n(lambda a, b: a + b)(2,3) # 5 - need to encapsulate it in print() to see the result in the console\n\nsquare = lambda x : x ** 2\nprint(square(3))    # 9\ncube = lambda x : x ** 3\nprint(cube(3))    # 27\n\n# Multiple variables\nmultiple_variable = lambda a, b, c: a ** 2 - 3 * b + 4 * c\nprint(multiple_variable(5, 5, 3)) # 22\n```\n\n### Lambda Function Inside Another Function\n\nUsing a lambda function inside another function.\n\n```py\ndef power(x):\n    return lambda n : x ** n\n\ncube = power(2)(3)   # function power now need 2 arguments to run, in separate rounded brackets\nprint(cube)          # 8\ntwo_power_of_five = power(2)(5) \nprint(two_power_of_five)  # 32\n```\n\nüåï Keep up the good work. Keep the momentum going, the sky is the limit! You have just completed day 13 challenges and you are 13 steps a head in to your way to greatness. Now do some exercises for your brain and muscles.\n\n## üíª Exercises: Day 13\n\n1. Filter only negative and zero in the list using list comprehension\n   ```py\n   numbers = [-4, -3, -2, -1, 0, 2, 4, 6]\n   ```\n2. Flatten the following list of lists of lists to a one dimensional list :\n\n   ```py\n   list_of_lists =[[[1, 2, 3]], [[4, 5, 6]], [[7, 8, 9]]]\n\n   output\n   [1, 2, 3, 4, 5, 6, 7, 8, 9]\n   ```\n\n3. Using list comprehension create the following list of tuples:\n   ```py\n   [(0, 1, 0, 0, 0, 0, 0),\n   (1, 1, 1, 1, 1, 1, 1),\n   (2, 1, 2, 4, 8, 16, 32),\n   (3, 1, 3, 9, 27, 81, 243),\n   (4, 1, 4, 16, 64, 256, 1024),\n   (5, 1, 5, 25, 125, 625, 3125),\n   (6, 1, 6, 36, 216, 1296, 7776),\n   (7, 1, 7, 49, 343, 2401, 16807),\n   (8, 1, 8, 64, 512, 4096, 32768),\n   (9, 1, 9, 81, 729, 6561, 59049),\n   (10, 1, 10, 100, 1000, 10000, 100000)]\n   ```\n4. Flatten the following list to a new list:\n   ```py\n   countries = [[('Finland', 'Helsinki')], [('Sweden', 'Stockholm')], [('Norway', 'Oslo')]]\n   output:\n   [['FINLAND','FIN', 'HELSINKI'], ['SWEDEN', 'SWE', 'STOCKHOLM'], ['NORWAY', 'NOR', 'OSLO']]\n   ```\n5. Change the following list to a list of dictionaries:\n   ```py\n   countries = [[('Finland', 'Helsinki')], [('Sweden', 'Stockholm')], [('Norway', 'Oslo')]]\n   output:\n   [{'country': 'FINLAND', 'city': 'HELSINKI'},\n   {'country': 'SWEDEN', 'city': 'STOCKHOLM'},\n   {'country': 'NORWAY', 'city': 'OSLO'}]\n   ```\n6. Change the following list of lists to a list of concatenated strings:\n   ```py\n   names = [[('Asabeneh', 'Yetayeh')], [('David', 'Smith')], [('Donald', 'Trump')], [('Bill', 'Gates')]]\n   output\n   ['Asabeneh Yetaeyeh', 'David Smith', 'Donald Trump', 'Bill Gates']\n   ```\n7. Write a lambda function which can solve a slope or y-intercept of linear functions.\n\nüéâ CONGRATULATIONS ! üéâ\n\n[<< Day 12](../12_Day_Modules/12_modules.md) | [Day 14>>](../14_Day_Higher_order_functions/14_higher_order_functions.md)\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "13_Day_List_comprehension/13_list_comprehension.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.8600000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 210, "word_count": 1096}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 14 - Higher Order Functions</h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n  <sub>Author:\n  <a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n  <small>Second Edition: July, 2021</small>\n  </sub>\n</div>\n</div>\n\n[<< Day 13](../13_Day_List_comprehension/13_list_comprehension.md) | [Day 15>>](../15_Day_Python_type_errors/15_python_type_errors.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n- [üìò Day 14](#-day-14)\n  - [Higher Order Functions](#higher-order-functions)\n    - [Function as a Parameter](#function-as-a-parameter)\n    - [Function as a Return Value](#function-as-a-return-value)\n  - [Python Closures](#python-closures)\n  - [Python Decorators](#python-decorators)\n    - [Creating Decorators](#creating-decorators)\n    - [Applying Multiple Decorators to a Single Function](#applying-multiple-decorators-to-a-single-function)\n    - [Accepting Parameters in Decorator Functions](#accepting-parameters-in-decorator-functions)\n  - [Built-in Higher Order Functions](#built-in-higher-order-functions)\n    - [Python - Map Function](#python---map-function)\n    - [Python - Filter Function](#python---filter-function)\n    - [Python - Reduce Function](#python---reduce-function)\n  - [üíª Exercises: Day 14](#-exercises-day-14)\n    - [Exercises: Level 1](#exercises-level-1)\n    - [Exercises: Level 2](#exercises-level-2)\n    - [Exercises: Level 3](#exercises-level-3)\n\n# üìò Day 14\n\n## Higher Order Functions\n\nIn Python functions are treated as first class citizens, allowing you to perform the following operations on functions:\n\n- A function can take one or more functions as parameters\n- A function can be returned as a result of another function\n- A function can be modified\n- A function can be assigned to a variable\n\nIn this section, we will cover:\n\n1. Handling functions as parameters\n2. Returning functions as return value from another functions\n3. Using Python closures and decorators\n\n### Function as a Parameter\n\n```py\ndef sum_numbers(nums):  # normal function\n    return sum(nums)    # a sad function abusing the built-in sum function :<\n\ndef higher_order_function(f, lst):  # function as a parameter\n    summation = f(lst)\n    return summation\nresult = higher_order_function(sum_numbers, [1, 2, 3, 4, 5])\nprint(result)       # 15\n```\n\n### Function as a Return Value\n\n```py\ndef square(x):          # a square function\n    return x ** 2\n\ndef cube(x):            # a cube function\n    return x ** 3\n\ndef absolute(x):        # an absolute value function\n    if x >= 0:\n        return x\n    else:\n        return -(x)\n\ndef higher_order_function(type): # a higher order function returning a function\n    if type == 'square':\n        return square\n    elif type == 'cube':\n        return cube\n    elif type == 'absolute':\n        return absolute\n\nresult = higher_order_function('square')\nprint(result(3))       # 9\nresult = higher_order_function('cube')\nprint(result(3))       # 27\nresult = higher_order_function('absolute')\nprint(result(-3))      # 3\n```\n\nYou can see from the above example that the higher order function is returning different functions depending on the passed parameter\n\n## Python Closures\n\nPython allows a nested function to access the outer scope of the enclosing function. This is is known as a Closure. Let us have a look at how closures work in Python. In Python, closure is created by nesting a function inside another encapsulating function and then returning the inner function. See the example below.\n\n**Example:**\n\n```py\ndef add_ten():\n    ten = 10\n    def add(num):\n        return num + ten\n    return add\n\nclosure_result = add_ten()\nprint(closure_result(5))  # 15\nprint(closure_result(10))  # 20\n```\n\n## Python Decorators\n\nA decorator is a design pattern in Python that allows a user to add new functionality to an existing object without modifying its structure. Decorators are usually called before the definition of a function you want to decorate.\n\n### Creating Decorators\n\nTo create a decorator function, we need an outer function with an inner wrapper function.\n\n**Example:**\n\n```py\n# Normal function\ndef greeting():\n    return 'Welcome to Python'\ndef uppercase_decorator(function):\n    def wrapper():\n        func = function()\n        make_uppercase = func.upper()\n        return make_uppercase\n    return wrapper\ng = uppercase_decorator(greeting)\nprint(g())          # WELCOME TO PYTHON\n\n## Let us implement the example above with a decorator\n\n'''This decorator function is a higher order function\nthat takes a function as a parameter'''\ndef uppercase_decorator(function):\n    def wrapper():\n        func = function()\n        make_uppercase = func.upper()\n        return make_uppercase\n    return wrapper\n@uppercase_decorator\ndef greeting():\n    return 'Welcome to Python'\nprint(greeting())   # WELCOME TO PYTHON\n\n```\n\n### Applying Multiple Decorators to a Single Function\n\n```py\n\n'''These decorator functions are higher order functions\nthat take functions as parameters'''\n\n# First Decorator\ndef uppercase_decorator(function):\n    def wrapper():\n        func = function()\n        make_uppercase = func.upper()\n        return make_uppercase\n    return wrapper\n\n# Second decorator\ndef split_string_decorator(function):\n    def wrapper():\n        func = function()\n        splitted_string = func.split()\n        return splitted_string\n\n    return wrapper\n\n@split_string_decorator\n@uppercase_decorator     # order with decorators is important in this case - .upper() function does not work with lists\ndef greeting():\n    return 'Welcome to Python'\nprint(greeting())   # WELCOME TO PYTHON\n```\n\n### Accepting Parameters in Decorator Functions\n\nMost of the time we need our functions to take parameters, so we might need to define a decorator that accepts parameters.\n\n```py\ndef decorator_with_parameters(function):\n    def wrapper_accepting_parameters(para1, para2, para3):\n        function(para1, para2, para3)\n        print(\"I live in {}\".format(para3))\n    return wrapper_accepting_parameters\n\n@decorator_with_parameters\ndef print_full_name(first_name, last_name, country):\n    print(\"I am {} {}. I love to teach.\".format(\n        first_name, last_name, country))\n\nprint_full_name(\"Asabeneh\", \"Yetayeh\",'Finland')\n```\n\n## Built-in Higher Order Functions\n\nSome of the built-in higher order functions that we cover in this part are _map()_, _filter_, and _reduce_.\nLambda function can be passed as a parameter and the best use case of lambda functions is in functions like map, filter and reduce.\n\n### Python - Map Function\n\nThe map() function is a built-in function that takes a function and iterable as parameters.\n\n```py\n    # syntax\n    map(function, iterable)\n```\n\n**Example:1**\n\n```py\nnumbers = [1, 2, 3, 4, 5] # iterable\ndef square(x):\n    return x ** 2\nnumbers_squared = map(square, numbers)\nprint(list(numbers_squared))    # [1, 4, 9, 16, 25]\n# Lets apply it with a lambda function\nnumbers_squared = map(lambda x : x ** 2, numbers)\nprint(list(numbers_squared))    # [1, 4, 9, 16, 25]\n```\n\n**Example:2**\n\n```py\nnumbers_str = ['1', '2', '3', '4', '5']  # iterable\nnumbers_int = map(int, numbers_str)\nprint(list(numbers_int))    # [1, 2, 3, 4, 5]\n```\n\n**Example:3**\n\n```py\nnames = ['Asabeneh', 'Lidiya', 'Ermias', 'Abraham']  # iterable\n\ndef change_to_upper(name):\n    return name.upper()\n\nnames_upper_cased = map(change_to_upper, names)\nprint(list(names_upper_cased))    # ['ASABENEH', 'LIDIYA', 'ERMIAS', 'ABRAHAM']\n\n# Let us apply it with a lambda function\nnames_upper_cased = map(lambda name: name.upper(), names)\nprint(list(names_upper_cased))    # ['ASABENEH', 'LIDIYA', 'ERMIAS', 'ABRAHAM']\n```\n\nWhat actually map does is iterating over a list. For instance, it changes the names to upper case and returns a new list.\n\n### Python - Filter Function\n\nThe filter() function calls the specified function which returns boolean for each item of the specified iterable (list). It filters the items that satisfy the filtering criteria.\n\n```py\n    # syntax\n    filter(function, iterable)\n```\n\n**Example:1**\n\n```py\n# Lets filter only even nubers\nnumbers = [1, 2, 3, 4, 5]  # iterable\n\ndef is_even(num):\n    if num % 2 == 0:\n        return True\n    return False\n\neven_numbers = filter(is_even, numbers)\nprint(list(even_numbers))       # [2, 4]\n```\n\n**Example:2**\n\n```py\nnumbers = [1, 2, 3, 4, 5]  # iterable\n\ndef is_odd(num):\n    if num % 2 != 0:\n        return True\n    return False\n\nodd_numbers = filter(is_odd, numbers)\nprint(list(odd_numbers))       # [1, 3, 5]\n```\n\n```py\n# Filter long name\nnames = ['Asabeneh', 'Lidiya', 'Ermias', 'Abraham']  # iterable\ndef is_name_long(name):\n    if len(name) > 7:\n        return True\n    return False\n\nlong_names = filter(is_name_long, names)\nprint(list(long_names))         # ['Asabeneh']\n```\n\n### Python - Reduce Function\n\nThe _reduce()_ function is defined in the functools module and we should import it from this module. Like map and filter it takes two parameters, a function and an iterable. However, it does not return another iterable, instead it returns a single value.\n**Example:1**\n\n```py\nnumbers_str = ['1', '2', '3', '4', '5']  # iterable\ndef add_two_nums(x, y):\n    return int(x) + int(y)\n\ntotal = reduce(add_two_nums, numbers_str)\nprint(total)    # 15\n```\n\n## üíª Exercises: Day 14\n\n```py\ncountries = ['Estonia', 'Finland', 'Sweden', 'Denmark', 'Norway', 'Iceland']\nnames = ['Asabeneh', 'Lidiya', 'Ermias', 'Abraham']\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n```\n\n### Exercises: Level 1\n\n1. Explain the difference between map, filter, and reduce.\n2. Explain the difference between higher order function, closure and decorator\n3. Define a call function before map, filter or reduce, see examples.\n4. Use for loop to print each country in the countries list.\n5. Use for to print each name in the names list.\n6. Use for to print each number in the numbers list.\n\n### Exercises: Level 2\n\n1. Use map to create a new list by changing each country to uppercase in the countries list\n1. Use map to create a new list by changing each number to its square in the numbers list\n1. Use map to change each name to uppercase in the names list\n1. Use filter to filter out countries containing 'land'.\n1. Use filter to filter out countries having exactly six characters.\n1. Use filter to filter out countries containing six letters and more in the country list.\n1. Use filter to filter out countries starting with an 'E'\n1. Chain two or more list iterators (eg. arr.map(callback).filter(callback).reduce(callback))\n1. Declare a function called get_string_lists which takes a list as a parameter and then returns a list containing only string items.\n1. Use reduce to sum all the numbers in the numbers list.\n1. Use reduce to concatenate all the countries and to produce this sentence: Estonia, Finland, Sweden, Denmark, Norway, and Iceland are north European countries\n1. Declare a function called categorize_countries that returns a list of countries with some common pattern (you can find the [countries list](https://github.com/Asabeneh/30-Days-Of-Python/blob/master/data/countries.py) in this repository as countries.js(eg 'land', 'ia', 'island', 'stan')).\n1. Create a function returning a dictionary, where keys stand for starting letters of countries and values are the number of country names starting with that letter.\n2. Declare a get_first_ten_countries function - it returns a list of first ten countries from the countries.js list in the data folder.\n1. Declare a get_last_ten_countries function that returns the last ten countries in the countries list.\n\n### Exercises: Level 3\n\n1. Use the countries_data.py (https://github.com/Asabeneh/30-Days-Of-Python/blob/master/data/countries-data.py) file and follow the tasks below:\n   - Sort countries by name, by capital, by population\n   - Sort out the ten most spoken languages by location.\n   - Sort out the ten most populated countries.\n\nüéâ CONGRATULATIONS ! üéâ\n\n[<< Day 13](../13_Day_List_comprehension/13_list_comprehension.md) | [Day 15>>](../15_Day_Python_type_errors/15_python_type_errors.md)", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "14_Day_Higher_order_functions/14_higher_order_functions.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.9500000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 370, "word_count": 1602}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create step-by-step tutorial with clear explanations and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<div align=\"center\">\n  <h1> 30 Days Of Python: Day 15 - Python Type Errors </h1>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/in/asabeneh/\">\n  <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\">\n  </a>\n  <a class=\"header-badge\" target=\"_blank\" href=\"https://twitter.com/Asabeneh\">\n  <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/asabeneh?style=social\">\n  </a>\n\n  <sub>Author:\n  <a href=\"https://www.linkedin.com/in/asabeneh/\" target=\"_blank\">Asabeneh Yetayeh</a><br>\n  <small> Second Edition: July, 2021</small>\n  </sub>\n</div>\n</div>\n\n[<< Day 14](../14_Day_Higher_order_functions/14_higher_order_functions.md) | [Day 16 >>](../16_Day_Python_date_time/16_python_datetime.md)\n\n![30DaysOfPython](../images/30DaysOfPython_banner3@2x.png)\n- [üìò Day 15](#-day-15)\n  - [Python Error Types](#python-error-types)\n    - [SyntaxError](#syntaxerror)\n    - [NameError](#nameerror)\n    - [IndexError](#indexerror)\n    - [ModuleNotFoundError](#modulenotfounderror)\n    - [AttributeError](#attributeerror)\n    - [KeyError](#keyerror)\n    - [TypeError](#typeerror)\n    - [ImportError](#importerror)\n    - [ValueError](#valueerror)\n    - [ZeroDivisionError](#zerodivisionerror)\n  - [üíª Exercises: Day 15](#-exercises-day-15)\n\n# üìò Day 15\n\n## Python Error Types\n\nWhen we write code it is common that we make a typo or some other common error. If our code fails to run, the Python interpreter will display a message, containing feedback with information on where the problem occurs and the type of an error. It will also sometimes gives us suggestions on a possible fix. Understanding different types of errors in programming languages will help us to debug our code quickly and also it makes us better at what we do.\n\nLet us see the most common error types one by one. First let us open our Python interactive shell. Go to your you computer terminal and write 'python'. The python interactive shell will be opened.\n\n### SyntaxError\n\n**Example 1: SyntaxError**\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> print 'hello world'\n  File \"<stdin>\", line 1\n    print 'hello world'\n                      ^\nSyntaxError: Missing parentheses in call to 'print'. Did you mean print('hello world')?\n>>>\n```\n\nAs you can see we made a syntax error because we forgot to enclose the string with parenthesis and Python already suggests the solution. Let us fix it.\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> print 'hello world'\n  File \"<stdin>\", line 1\n    print 'hello world'\n                      ^\nSyntaxError: Missing parentheses in call to 'print'. Did you mean print('hello world')?\n>>> print('hello world')\nhello world\n>>>\n```\n\nThe error was a _SyntaxError_. After the fix our code was executed without a hitch. Let see more error types.\n\n### NameError\n\n**Example 1: NameError**\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> print(age)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'age' is not defined\n>>>\n```\n\nAs you can see from the message above, name age is not defined. Yes, it is true that we did not define an age variable but we were trying to print it out as if we had had declared it. Now, lets fix this by declaring it and assigning with a value.\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> print(age)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'age' is not defined\n>>> age = 25\n>>> print(age)\n25\n>>>\n```\n\nThe type of error was a _NameError_. We debugged the error by defining the variable name.\n\n### IndexError\n\n**Example 1: IndexError**\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> numbers = [1, 2, 3, 4, 5]\n>>> numbers[5]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: list index out of range\n>>>\n```\n\nIn the example above, Python raised an _IndexError_, because the list has only indexes from 0 to 4 , so it was out of range.\n\n### ModuleNotFoundError\n\n**Example 1: ModuleNotFoundError**\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import maths\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'maths'\n>>>\n```\n\nIn the example above, I added an extra s to math deliberately and _ModuleNotFoundError_ was raised. Lets fix it by removing the extra s from math.\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import maths\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'maths'\n>>> import math\n>>>\n```\n\nWe fixed it, so let's use some of the functions from the math module.\n\n### AttributeError\n\n**Example 1: AttributeError**\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import maths\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'maths'\n>>> import math\n>>> math.PI\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: module 'math' has no attribute 'PI'\n>>>\n```\n\nAs you can see, I made a mistake again! Instead of pi, I tried to call a PI function from maths module. It raised an attribute error, it means, that the function does not exist in the module. Lets fix it by changing from PI to pi.\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import maths\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'maths'\n>>> import math\n>>> math.PI\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: module 'math' has no attribute 'PI'\n>>> math.pi\n3.141592653589793\n>>>\n```\n\nNow, when we call pi from the math module we got the result.\n\n### KeyError\n\n**Example 1: KeyError**\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> users = {'name':'Asab', 'age':250, 'country':'Finland'}\n>>> users['name']\n'Asab'\n>>> users['county']\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nKeyError: 'county'\n>>>\n```\n\nAs you can see, there was a typo in the key used to get the dictionary value. so, this is a key error and the fix is quite straight forward. Let's do this!\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> user = {'name':'Asab', 'age':250, 'country':'Finland'}\n>>> user['name']\n'Asab'\n>>> user['county']\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nKeyError: 'county'\n>>> user['country']\n'Finland'\n>>>\n```\n\nWe debugged the error, our code ran and we got the value.\n\n### TypeError\n\n**Example 1: TypeError**\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> 4 + '3'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n>>>\n```\n\nIn the example above, a TypeError is raised because we cannot add a number to a string. First solution would be to convert the string to int or float. Another solution would be converting the number to a string (the result then would be '43'). Let us follow the first fix.\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> 4 + '3'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n>>> 4 + int('3')\n7\n>>> 4 + float('3')\n7.0\n>>>\n```\n\nError removed and we got the result we expected.\n\n### ImportError\n\n**Example 1: TypeError**\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from math import power\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: cannot import name 'power' from 'math'\n>>>\n```\n\nThere is no function called power in the math module, it goes with a different name: _pow_. Let's correct it:\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from math import power\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: cannot import name 'power' from 'math'\n>>> from math import pow\n>>> pow(2,3)\n8.0\n>>>\n```\n\n### ValueError\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> int('12a')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: invalid literal for int() with base 10: '12a'\n>>>\n```\n\nIn this case we cannot change the given string to a number, because of the 'a' letter in it.\n\n### ZeroDivisionError\n\n```py\nasabeneh@Asabeneh:~$ python\nPython 3.9.6 (default, Jun 28 2021, 15:26:21)\n[Clang 11.0.0 (clang-1100.0.33.8)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> 1/0\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nZeroDivisionError: division by zero\n>>>\n```\n\nWe cannot divide a number by zero.\n\nWe have covered some of the python error types, if you want to check more about it check the python documentation about python error types.\nIf you are good at reading the error types then you will be able to fix your bugs fast and you will also become a better programmer.\n\nüåï You are excelling. You made it to half way to your way to greatness. Now do some exercises for your brain and for your muscle.\n\n## üíª Exercises: Day 15\n\n1. Open you python interactive shell and try all the examples covered in this section.\n\nüéâ CONGRATULATIONS ! üéâ\n\n[<< Day 14](../14_Day_Higher_order_functions/14_higher_order_functions.md) | [Day 16 >>](../16_Day_Python_date_time/16_python_datetime.md)", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "15_Day_Python_type_errors/15_python_type_errors.md", "language": "markdown", "complexity": "complex", "star_count": 47914, "quality_score": 0.8600000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 361, "word_count": 1651}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "## Review the Contributing Guidelines\n\nBefore submitting a pull request, verify it meets all requirements in the [Contributing Guidelines](https://github.com/donnemartin/awesome-aws/blob/master/CONTRIBUTING.md).\n\n## Describe Why This Is Awesome\n\nWhy is this awesome?\n\n--\n\nLike this pull request?  Vote for it by adding a :+1:\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": ".github/PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 13620, "quality_score": 0.8, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 12, "word_count": 41}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/aws.png\">\n</p>\n<br/>\n\n# Awesome AWS [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\nA curated list of awesome AWS libraries, open source repos, guides, blogs, and other resources.\n\nInspired by the [awesome](https://github.com/sindresorhus/awesome) list.\n\n## The Fiery Meter of AWSome\n\n* Repo with 0100+ Stars: :fire:\n* Repo with 0200+ Stars: :fire::fire:\n* Repo with 0500+ Stars: :fire::fire::fire:\n* Repo with 1000+ Stars: :fire::fire::fire::fire:\n* Repo with 2000+ Stars: :fire::fire::fire::fire::fire:\n\nRepos not on `The Fiery Meter of AWSome` can still be awesome, see [A Note on Repo AWSomeness](https://github.com/donnemartin/awesome-aws/blob/master/CONTRIBUTING.md#a-note-on-repo-awsomeness).\n\n### `awesome-aws` Python Module\n\n[![Build Status](https://travis-ci.org/donnemartin/awesome-aws.svg?branch=master)](https://travis-ci.org/donnemartin/awesome-aws) [![Codecov](https://img.shields.io/codecov/c/github/donnemartin/awesome-aws.svg)](https://codecov.io/github/donnemartin/saws/awesome-aws) [![PyPI version](https://badge.fury.io/py/awesome-aws.svg)](http://badge.fury.io/py/awesome-aws)\n\nThe Python module [`awesome-aws`](https://github.com/donnemartin/awesome-aws/tree/master/awesome) regularly scans repos on [Awesome AWS](https://github.com/donnemartin/awesome-aws) to maintain the accuracy of the `Fiery Meter of AWSome`.\n\n## Contributing\n\nContributions are welcome!\n\nCheck out the [Contributing Guidelines](https://github.com/donnemartin/awesome-aws/blob/master/CONTRIBUTING.md).\n\n## Index\n\n* [SDKs and Samples](#sdks-and-samples)\n    * [Android](#android-sdk)\n    * [C++](#c-sdk)\n    * [Clojure](#clojure-sdk)\n    * [Go](#go-sdk)\n    * [iOS](#ios-sdk)\n    * [IoT](#iot-sdk)\n    * [Java](#java-sdk)\n    * [JavaScript](#javascript-sdk)\n    * [Haskell](#haskell-sdk)\n    * [OCaml](#ocaml-sdk)\n    * [Perl](#perl-sdk)\n    * [PHP](#php-sdk)\n    * [Python](#python-sdk)\n    * [Ruby](#ruby-sdk)\n    * [Scala](#scala-sdk)\n    * [Xamarin](#xamarin-sdk)\n    * [Unity](#unity-sdk)\n    * [.NET](#net-sdk)\n* [Command Line Tools](#command-line-tools)\n    * [Universal Command Line Interface](#universal-command-line-interface)\n    * [Windows PowerShell](#windows-powershell)\n* [IDE Toolkits](#ide-toolkits)\n    * [Eclipse Toolkit](#eclipse-toolkit)\n    * [Visual Studio Toolkit](#visual-studio-toolkit)\n* [Open Source Repos](#open-source-repos)\n    * [API Gateway](#api-gateway)\n    * [AppStream](#appstream)\n    * [CLI](#cli)\n    * [CloudFormation](#cloudformation)\n    * [CloudSearch](#cloudsearch)\n    * [CloudTrail](#cloudtrail)\n    * [CloudWatch](#cloudwatch)\n    * [Code Deploy](#code-deploy)\n    * [Code Pipeline](#code-pipeline)\n    * [Cognito](#cognito)\n    * [Data Pipeline](#data-pipeline)\n    * [Device Farm](#device-farm)\n    * [DynamoDB](#dynamodb)\n    * [Elastic Beanstalk](#elastic-beanstalk)\n    * [Elastic Container Service](#elastic-container-service)\n    * [Elastic File System](#elastic-file-system)\n    * [Elastic MapReduce](#elastic-mapreduce)\n    * [Elastic Search](#elastic-search)\n    * [Elasticache](#elasticache)\n    * [Glacier](#glacier)\n    * [Kinesis](#kinesis)\n    * [Lambda](#lambda)\n    * [Machine Learning](#machine-learning)\n    * [Mobile Analytics](#mobile-analytics)\n    * [OpsWorks](#opsworks)\n    * [Redshift](#redshift)\n    * [Route 53](#route-53)\n    * [S3](#s3)\n    * [SNS](#sns)\n    * [SQS](#sqs)\n    * [Data](#data)\n    * [DevOps](#devops)\n    * [Security](#security)\n    * [Accompanying](#accompanying-repos)\n    * [Miscellaneous](#miscellaneous-repos)\n* [Guides, Books, Documentation, and Training](#guides-books-documentation-and-training)\n    * [Getting Started Guides](#getting-started-guides)\n    * [General Guides](#general-guides)\n    * [Books](#books)\n    * [Whitepapers](#whitepapers)\n    * [Documentation](#documentation)\n    * [Training](#training)\n    * [Case Studies: Powered by AWS](#case-studies-powered-by-aws)\n* [Social](#social)\n    * [Blogs](#blogs)\n    * [Twitter Influencers](#twitter-influencers)\n    * [Facebook Pages](#facebook-pages)\n    * [YouTube Channels](#youtube-channels)\n    * [LinkedIn Groups](#linkedin-groups)\n    * [Subreddits](#subreddits)\n    * [Conferences](#conferences)\n* [Latest KPIs and Stats](#latest-kpis-and-stats)\n* [Appendix of Core Services](#appendix-of-core-services)\n    * [Services in Plain English](#services-in-plain-english)\n    * [Compute](#compute-services)\n    * [Networking](#networking-services)\n    * [Enterprise Applications](#enterprise-applications)\n    * [Analytics](#analytics-services)\n    * [Management Tools](#management-tools)\n    * [Security and Identity](#security-and-identity-services)\n    * [Internet of Things](#internet-of-things-service)\n    * [Mobile Services](#mobile-services)\n    * [Storage and Content Delivery](#storage-and-content-delivery-services)\n    * [Databases](#databases)\n    * [Application Services](#application-services)\n    * [Developer Tools](#developer-tools)\n    * [Miscellaneous Services](#miscellaneous-services)\n* [Contributing](#contributing)\n* [Credits](#credits)\n* [Other Awesome Lists](#other-awesome-lists)\n* [Contact Info](#contact-info)\n* [License](#license)\n\nChangelog\n=========\n\n0.1.0 (2015-12-21)\n------------------\n\n- Initial release.\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "CHANGELOG.md", "language": "markdown", "complexity": "medium", "star_count": 13620, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 143, "word_count": 399}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "tutorials_guides", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create step-by-step tutorial with clear explanations and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": ".. raw:: html\n\n   <p align=\"center\">\n\n.. raw:: html\n\n   </p>\n\nAwesome AWS |Awesome|\n=====================\n\nA curated list of awesome AWS libraries, open source repos, guides,\nblogs, and other resources.\n\nInspired by the `awesome <https://github.com/sindresorhus/awesome>`__\nlist.\n\nThe Fiery Meter of AWSome\n-------------------------\n\n-  Repo with 0100+ Stars: :fire:\n-  Repo with 0200+ Stars: :fire::fire:\n-  Repo with 0500+ Stars: :fire::fire::fire:\n-  Repo with 1000+ Stars: :fire::fire::fire::fire:\n-  Repo with 2000+ Stars: :fire::fire::fire::fire::fire:\n\nRepos not on ``The Fiery Meter of AWSome`` can still be awesome, see `A\nNote on Repo\nAWSomeness <https://github.com/donnemartin/awesome-aws/blob/master/CONTRIBUTING.md#a-note-on-repo-awsomeness>`__.\n\n``awesome-aws`` Python Module\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n|Build Status| |Codecov| |PyPI version|\n\nThe Python module\n```awesome-aws`` <https://github.com/donnemartin/awesome-aws/tree/master/awesome>`__\nregularly scans repos on `Awesome\nAWS <https://github.com/donnemartin/awesome-aws>`__ to maintain the\naccuracy of the ``Fiery Meter of AWSome``.\n\nContributing\n------------\n\nContributions are welcome!\n\nCheck out the `Contributing\nGuidelines <https://github.com/donnemartin/awesome-aws/blob/master/CONTRIBUTING.md>`__.\n\nIndex\n-----\n\n-  `SDKs and Samples <#sdks-and-samples>`__\n\n   -  `Android <#android-sdk>`__\n   -  `C++ <#c-sdk>`__\n   -  `Clojure <#clojure-sdk>`__\n   -  `Go <#go-sdk>`__\n   -  `iOS <#ios-sdk>`__\n   -  `IoT <#iot-sdk>`__\n   -  `Java <#java-sdk>`__\n   -  `JavaScript <#javascript-sdk>`__\n   -  `Haskell <#haskell-sdk>`__\n   -  `OCaml <#ocaml-sdk>`__\n   -  `Perl <#perl-sdk>`__\n   -  `PHP <#php-sdk>`__\n   -  `Python <#python-sdk>`__\n   -  `Ruby <#ruby-sdk>`__\n   -  `Scala <#scala-sdk>`__\n   -  `Xamarin <#xamarin-sdk>`__\n   -  `Unity <#unity-sdk>`__\n   -  `.NET <#net-sdk>`__\n\n-  `Command Line Tools <#command-line-tools>`__\n\n   -  `Universal Command Line\n      Interface <#universal-command-line-interface>`__\n   -  `Windows PowerShell <#windows-powershell>`__\n\n-  `IDE Toolkits <#ide-toolkits>`__\n\n   -  `Eclipse Toolkit <#eclipse-toolkit>`__\n   -  `Visual Studio Toolkit <#visual-studio-toolkit>`__\n\n-  `Open Source Repos <#open-source-repos>`__\n\n   -  `API Gateway <#api-gateway>`__\n   -  `AppStream <#appstream>`__\n   -  `CLI <#cli>`__\n   -  `CloudFormation <#cloudformation>`__\n   -  `CloudSearch <#cloudsearch>`__\n   -  `CloudTrail <#cloudtrail>`__\n   -  `CloudWatch <#cloudwatch>`__\n   -  `Code Deploy <#code-deploy>`__\n   -  `Code Pipeline <#code-pipeline>`__\n   -  `Cognito <#cognito>`__\n   -  `Data Pipeline <#data-pipeline>`__\n   -  `Device Farm <#device-farm>`__\n   -  `DynamoDB <#dynamodb>`__\n   -  `Elastic Beanstalk <#elastic-beanstalk>`__\n   -  `Elastic Container Service <#elastic-container-service>`__\n   -  `Elastic File System <#elastic-file-system>`__\n   -  `Elastic MapReduce <#elastic-mapreduce>`__\n   -  `Elastic Search <#elastic-search>`__\n   -  `Elasticache <#elasticache>`__\n   -  `Glacier <#glacier>`__\n   -  `Kinesis <#kinesis>`__\n   -  `Lambda <#lambda>`__\n   -  `Machine Learning <#machine-learning>`__\n   -  `Mobile Analytics <#mobile-analytics>`__\n   -  `OpsWorks <#opsworks>`__\n   -  `Redshift <#redshift>`__\n   -  `Route 53 <#route-53>`__\n   -  `S3 <#s3>`__\n   -  `SNS <#sns>`__\n   -  `SQS <#sqs>`__\n   -  `Data <#data>`__\n   -  `DevOps <#devops>`__\n   -  `Security <#security>`__\n   -  `Accompanying <#accompanying-repos>`__\n   -  `Miscellaneous <#miscellaneous-repos>`__\n\n-  `Guides, Books, Documentation, and\n   Training <#guides-books-documentation-and-training>`__\n\n   -  `Getting Started Guides <#getting-started-guides>`__\n   -  `General Guides <#general-guides>`__\n   -  `Books <#books>`__\n   -  `Whitepapers <#whitepapers>`__\n   -  `Documentation <#documentation>`__\n   -  `Training <#training>`__\n   -  `Case Studies: Powered by AWS <#case-studies-powered-by-aws>`__\n\n-  `Social <#social>`__\n\n   -  `Blogs <#blogs>`__\n   -  `Twitter Influencers <#twitter-influencers>`__\n   -  `Facebook Pages <#facebook-pages>`__\n   -  `YouTube Channels <#youtube-channels>`__\n   -  `LinkedIn Groups <#linkedin-groups>`__\n   -  `Subreddits <#subreddits>`__\n   -  `Conferences <#conferences>`__\n\n-  `Latest KPIs and Stats <#latest-kpis-and-stats>`__\n-  `Appendix of Core Services <#appendix-of-core-services>`__\n\n   -  `Services in Plain English <#services-in-plain-english>`__\n   -  `Compute <#compute-services>`__\n   -  `Networking <#networking-services>`__\n   -  `Enterprise Applications <#enterprise-applications>`__\n   -  `Analytics <#analytics-services>`__\n   -  `Management Tools <#management-tools>`__\n   -  `Security and Identity <#security-and-identity-services>`__\n   -  `Internet of Things <#internet-of-things-service>`__\n   -  `Mobile Services <#mobile-services>`__\n   -  `Storage and Content\n      Delivery <#storage-and-content-delivery-services>`__\n   -  `Databases <#databases>`__\n   -  `Application Services <#application-services>`__\n   -  `Developer Tools <#developer-tools>`__\n   -  `Miscellaneous Services <#miscellaneous-services>`__\n\n-  `Contributing <#contributing>`__\n-  `Credits <#credits>`__\n-  `Other Awesome Lists <#other-awesome-lists>`__\n-  `Contact Info <#contact-info>`__\n-  `License <#license>`__\n\nChangelog\n=========\n\n0.1.0 (2015-12-21)\n------------------\n\n-  Initial release.\n\n.. |Awesome| image:: https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg\n   :target: https://github.com/sindresorhus/awesome\n.. |Build Status| image:: https://travis-ci.org/donnemartin/awesome-aws.svg?branch=master\n   :target: https://travis-ci.org/donnemartin/awesome-aws\n.. |Codecov| image:: https://img.shields.io/codecov/c/github/donnemartin/awesome-aws.svg\n   :target: https://codecov.io/github/donnemartin/saws/awesome-aws\n.. |PyPI version| image:: https://badge.fury.io/py/awesome-aws.svg\n   :target: http://badge.fury.io/py/awesome-aws\n", "metadata": {"source": "github.com/donnemartin/awesome-aws", "file_path": "CHANGELOG.rst", "language": "restructuredtext", "complexity": "medium", "star_count": 13620, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 185, "word_count": 530}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CI-Backend\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python 3.11\n        uses: actions/setup-python@v2\n        with:\n          python-version: 3.11\n      - name: Install dependencies\n        run: |\n          cd backend\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n      - name: Test with unittest\n        run: |\n          cd backend\n          python -m unittest\n        env:\n          AUTH_TOKEN: ${{ secrets.AUTH_TOKEN }}\n          MONGODB_PASSWORD: ${{ secrets.MONGODB_PASSWORD }}\n      - name: Upload coverage to Coveralls\n        run: |\n          cd backend\n          coverage run --source=src -m unittest\n          coveralls\n        env:\n          AUTH_TOKEN: ${{ secrets.AUTH_TOKEN }}\n          COVERALLS_REPO_TOKEN: ${{ secrets.COVERALLS_REPO_TOKEN }}\n          MONGODB_PASSWORD: ${{ secrets.MONGODB_PASSWORD }}\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": ".github/workflows/backend.yaml", "language": "yaml", "complexity": "simple", "star_count": 16218, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": false, "line_count": 40, "word_count": 96}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CI-Frontend\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Install modules\n      run: |\n        cd frontend\n        yarn\n    - name: Run ESLint\n      run: |\n        cd frontend\n        yarn eslint .", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": ".github/workflows/frontend.yaml", "language": "yaml", "complexity": "simple", "star_count": 16218, "quality_score": 0.8, "features": {"has_code_examples": false, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": false, "line_count": 21, "word_count": 41}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# GitHub Trends\n\n## SPECIAL: GitHub Wrapped\n\nCheck out your GitHub Wrapped at `githubwrapped.io`!\n\n![github-wrapped](https://github.com/avgupta456/github-trends/assets/16708871/bf9406a4-6a49-4dbf-8f60-af221bb84bd6)\n\n---\n\n## What is GitHub Trends\n\nGitHub Trends dives deep into the GitHub API to bring you exciting and impactful metrics about your code contributions. Generate insights on lines written by language, repository, and time. Easily embed dynamic images into your GitHub profile to share your statistics with the world. Check out some of the examples below:\n\n<a href=\"https://githubtrends.io\">\n  <img align=\"center\" src=\"https://api.githubtrends.io/user/svg/avgupta456/langs?time_range=one_year&include_private=True&loc_metric=changed\" />\n</a>\n<a href=\"https://githubtrends.io\">\n  <img align=\"center\" src=\"https://api.githubtrends.io/user/svg/avgupta456/repos?time_range=one_year&include_private=True&group=private&loc_metric=changed\" />\n</a>\n\n## Quickstart\n\nFirst, visit `https://api.githubtrends.io/auth/signup/public` and create an account with GitHub Trends. Then, paste this string into your Markdown content, substituting your username.\n\n```md\n[![GitHub Trends SVG](https://api.githubtrends.io/user/svg/avgupta456/langs)](https://githubtrends.io)\n```\n\nAnd voila, you get a card like above. Keep reading to learn more!\n\n## Why GitHub Trends?\n\nUnlike other projects which look at just your public repositories, GitHub Trends computes metrics based on your individual commits. If you commit to open-source projects, or have collaborators contribute to your own repositories, GitHub Trends will better measure your own code contributions. Through this method, GitHub Trends is the first project that allows users to surface lines of code written (LOC) by language and repository. Our web interface also allows for easier customization.\n\n# Usage\n\n## Website Workflow (Alpha)\n\nVisit [githubtrends.io](https://www.githubtrends.io) to create an account and get started!\n\nHave questions? Check out [the demo](https://www.githubtrends.io/demo)!\n\n![image](https://user-images.githubusercontent.com/16708871/138611082-105e4dbc-8a27-4f68-8045-f9d86c912429.png)\n\n---\n\n## API Workflow (Alpha)\n\nAlternatively, you can communicate directly with the API to create and customize your cards. Read [docs/API.md](https://github.com/avgupta456/github-trends/blob/main/docs/API.md) to learn more about the API and customizations.\n\n## FAQ\n\nSee [docs/FAQ.md](https://github.com/avgupta456/github-trends/blob/main/docs/FAQ.md).\n\n## Contributing\n\nSee [docs/CONTRIBUTING.md](https://github.com/avgupta456/github-trends/blob/main/docs/CONTRIBUTING.md).\n\n## Acknowledgements\n\nMuch inspiration was taken from [GitHub Readme Stats](https://github.com/anuraghazra/github-readme-stats). If you haven't already, check it out and give it a star!\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 16218, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 63, "word_count": 285}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with installation and setup instructions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Backend\n\n## Installation\n\n```\npoetry install\npoetry run pre-commit install\n```\n\n## Run Locally\n\nNavigate to localhost:8000\n\n```\nyarn start\n```\n\n## Test with Code Coverage\n\n```\nyarn test\n```\n\nView coverage with GitHub badge or on coveralls.io\n\n## Build\n\nIf a new requirement has been added, make sure to update the requirements.txt\n\n```\nyarn set-reqs\n```\n\nThen, just commit on the main branch (Google Cloud Run takes care of the rest)\n\n## Adding a Secret\n\nUpdate cloudbuild.yaml, .env, .env-template, and GCP Cloud Run Trigger Substitution Variables.\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": "backend/README.md", "language": "markdown", "complexity": "simple", "star_count": 16218, "quality_score": 0.8, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 39, "word_count": 88}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "steps:\n  - name: node:10.15.1\n    entrypoint: npm\n    args: [\"install\"]\n    dir: \"backend\"\n  - name: node:10.15.1\n    entrypoint: npm\n    args: [\"run\", \"create-env\"]\n    dir: \"backend\"\n    env:\n      - \"DEV_OAUTH_CLIENT_ID=${_DEV_OAUTH_CLIENT_ID}\"\n      - \"DEV_OAUTH_CLIENT_SECRET=${_DEV_OAUTH_CLIENT_SECRET}\"\n      - \"DEV_OAUTH_REDIRECT_URI=${_DEV_OAUTH_REDIRECT_URI}\"\n      - \"PROD_OAUTH_CLIENT_ID=${_PROD_OAUTH_CLIENT_ID}\"\n      - \"PROD_OAUTH_CLIENT_SECRET=${_PROD_OAUTH_CLIENT_SECRET}\"\n      - \"PROD_OAUTH_REDIRECT_URI=${_PROD_OAUTH_REDIRECT_URI}\"\n      - \"MONGODB_PASSWORD=${_MONGODB_PASSWORD}\"\n      - \"SENTRY_DSN=${_SENTRY_DSN}\"\n  - name: \"gcr.io/cloud-builders/gcloud\"\n    args: [\"app\", \"deploy\", \"--appyaml\", \"./deploy/app.yaml\"]\n    dir: \"backend\"\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": "backend/deploy/cloudbuild.yaml", "language": "yaml", "complexity": "simple", "star_count": 16218, "quality_score": 0.8, "features": {"has_code_examples": false, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 22, "word_count": 47}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from src.aggregation.layer2.auth import get_is_valid_user\nfrom src.aggregation.layer2.user import get_user, get_user_demo\n\n__all__ = [\"get_is_valid_user\", \"get_user\", \"get_user_demo\"]\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": "backend/src/aggregation/layer2/__init__.py", "language": "python", "complexity": "simple", "star_count": 16218, "quality_score": 0.6600000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 5, "word_count": 14}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from datetime import date, timedelta\nfrom typing import Optional, Tuple\n\nfrom src.aggregation.layer0 import get_user_data\nfrom src.constants import USER_BLACKLIST\nfrom src.data.mongo.secret.functions import update_keys\nfrom src.data.mongo.user import PublicUserModel, get_public_user as db_get_public_user\nfrom src.data.mongo.user_months import get_user_months\nfrom src.models import UserPackage\nfrom src.models.background import UpdateUserBackgroundTask\nfrom src.utils import alru_cache\n\n# Formerly the publisher, loads existing data here\n\n\nasync def _get_user(\n    user_id: str, private_access: bool, start_date: date, end_date: date\n) -> Tuple[Optional[UserPackage], bool]:\n    user_months = await get_user_months(user_id, private_access, start_date, end_date)\n    if len(user_months) == 0:\n        return None, False\n\n    expected_num_months = (\n        (end_date.year - start_date.year) * 12 + (end_date.month - start_date.month) + 1\n    )\n    complete = len(user_months) == expected_num_months\n\n    user_data = user_months[0].data\n    for user_month in user_months[1:]:\n        user_data += user_month.data\n\n    # TODO: handle timezone_str here\n    return user_data.trim(start_date, end_date), complete\n\n\n@alru_cache()\nasync def get_user(\n    user_id: str,\n    start_date: date,\n    end_date: date,\n    no_cache: bool = False,\n) -> Tuple[\n    bool, Tuple[Optional[UserPackage], bool, Optional[UpdateUserBackgroundTask]]\n]:\n    if user_id in USER_BLACKLIST:\n        return (False, (None, False, None))\n\n    user: Optional[PublicUserModel] = await db_get_public_user(user_id)\n    if user is None:\n        return (False, (None, False, None))\n\n    private_access = user.private_access or False\n    user_data, complete = await _get_user(user_id, private_access, start_date, end_date)\n    background_task = UpdateUserBackgroundTask(\n        user_id=user_id,\n        access_token=user.access_token,\n        private_access=private_access,\n        start_date=start_date,\n        end_date=end_date,\n    )\n    return (complete, (user_data, complete, background_task))\n\n\n@alru_cache(ttl=timedelta(minutes=15))\nasync def get_user_demo(\n    user_id: str, start_date: date, end_date: date, no_cache: bool = False\n) -> Tuple[bool, UserPackage]:\n    await update_keys()\n    timezone_str = \"US/Eastern\"\n    # recompute/cache but don't save to db\n    data = await get_user_data(\n        user_id=user_id,\n        start_date=start_date,\n        end_date=end_date,\n        timezone_str=timezone_str,\n        access_token=None,\n        catch_errors=True,\n    )\n    return (True, data)\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": "backend/src/aggregation/layer2/user.py", "language": "python", "complexity": "simple", "star_count": 16218, "quality_score": 0.89, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 80, "word_count": 237}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\n\n# GLOBAL\nLOCAL = os.getenv(\"LOCAL\", \"False\") == \"True\"\nPROD = os.getenv(\"PROD\", \"False\") == \"True\"\nPROJECT_ID = \"github-334619\"\nBACKEND_URL = \"https://api.githubtrends.io\" if PROD else \"http://localhost:8000\"\n\nOWNER = \"avgupta456\"\nREPO = \"github-trends\"\n\n# API\n# https://docs.github.com/en/rest/reference/rate-limit\n# https://docs.github.com/en/rest/guides/best-practices-for-integrators#dealing-with-secondary-rate-limits\n# https://docs.github.com/en/graphql/overview/resource-limitations\nTIMEOUT = 15  # max seconds to wait for api response\nGRAPHQL_NODE_CHUNK_SIZE = 50  # number of nodes (commits) to query (max 100)\nGRAPHQL_NODE_THREADS = 5  # number of node queries simultaneously (avoid blacklisting)\nREST_NODE_THREADS = 50  # number of node queries simultaneously (avoid blacklisting)\n\nPR_FILES = 5  # max number of files to query for PRs\nNODE_QUERIES = 20  # max number of node queries to make\nCUTOFF = 1000  # if additions or deletions > CUTOFF, or sum > 2 * CUTOFF, ignore LOC\nFILE_CUTOFF = 1000  # if less than cutoff in file, count LOC\n\nAPI_VERSION = 0.02  # determines when to overwrite MongoDB data\n\n# CUSTOMIZATION\nBLACKLIST = [\"Jupyter Notebook\", \"HTML\"]  # languages to ignore\n\n# OAUTH\nprefix = \"PROD\" if PROD else \"DEV\"\n# client ID for GitHub OAuth App\nOAUTH_CLIENT_ID = os.getenv(f\"{prefix}_OAUTH_CLIENT_ID\", \"\")\n# client secret for App\nOAUTH_CLIENT_SECRET = os.getenv(f\"{prefix}_OAUTH_CLIENT_SECRET\", \"\")\n# redirect uri for App\nOAUTH_REDIRECT_URI = os.getenv(f\"{prefix}_OAUTH_REDIRECT_URI\", \"\")\n\n# MONGODB\nMONGODB_PASSWORD = os.getenv(\"MONGODB_PASSWORD\", \"\")\n\n# SVG\nDEFAULT_COLOR = \"#858585\"\n\n# SENTRY\nSENTRY_DSN = os.getenv(\"SENTRY_DSN\", \"\")\n\n# TESTING\nTEST_USER_ID = \"avgupta456\"\nTEST_REPO = \"github-trends\"\nTEST_TOKEN = os.getenv(\"AUTH_TOKEN\", \"\")  # for authentication\nTEST_NODE_IDS = [\n    \"C_kwDOENp939oAKGM1MzdlM2QzMTZjMmEyZGIyYWU4ZWI0MmNmNjQ4YWEwNWQ5OTBiMjM\",\n    \"C_kwDOD_-BVNoAKDFhNTIxNWE1MGM4ZDllOGEwYTFhNjhmYWZkYzE5MzA5YTRkMDMwZmM\",\n    \"C_kwDOD_-BVNoAKDRiZTQ4MTQ0MzgwYjBlNGEwNjQ4YjY4YWI4ZjFjYmQ3MWU4M2VhMzU\",\n]\nTEST_SHA = \"ad83e6340377904fa0295745b5314202b23d2f3f\"\n\n# WRAPPED\n\n# example users, don't need to star the repo\nUSER_WHITELIST = [\n    \"torvalds\",\n    \"yyx990803\",\n    \"shadcn\",\n    \"sindresorhus\",\n]\n\nUSER_BLACKLIST = [\"kangmingtay\", \"ae7er\", \"stalukdar7\", \"piyush7833\"]\n\nprint(\"PROD\", PROD)\nprint(\"API_VERSION\", API_VERSION)\nprint()\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": "backend/src/constants.py", "language": "python", "complexity": "medium", "star_count": 16218, "quality_score": 0.8300000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 75, "word_count": 269}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create project README with installation, usage, and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Dict\n\nimport sentry_sdk\nfrom dotenv import find_dotenv, load_dotenv\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom sentry_sdk.integrations.asgi import SentryAsgiMiddleware\n\nload_dotenv(find_dotenv())\n\n# flake8: noqa E402\n\n# add endpoints here (after load dotenv)\nfrom src.constants import PROD, SENTRY_DSN\nfrom src.routers import (\n    asset_router,\n    auth_router,\n    dev_router,\n    user_router,\n    wrapped_router,\n)\n\n\"\"\"\nSETUP\n\"\"\"\n\napp = FastAPI()\n\norigins = [\n    \"http://localhost:3000\",\n    \"http://localhost:3001\",\n    \"https://githubtrends.io\",\n    \"https://www.githubtrends.io\",\n    \"https://githubwrapped.io\",\n    \"https://www.githubwrapped.io\",\n]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\nsentry_sdk.init(\n    SENTRY_DSN,\n    traces_sample_rate=(1.0 if PROD else 1.0),\n)\n\napp.add_middleware(\n    SentryAsgiMiddleware,\n)\n\n\n@app.get(\"/\")\nasync def read_root() -> Dict[str, str]:\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/info\")\ndef get_info() -> Dict[str, bool]:\n    return {\"PROD\": PROD}\n\n\napp.include_router(asset_router, prefix=\"/assets\", tags=[\"Assets\"])\napp.include_router(auth_router, prefix=\"/auth\", tags=[\"Auth\"])\napp.include_router(dev_router, prefix=\"/dev\", tags=[\"Dev\"])\napp.include_router(user_router, prefix=\"/user\", tags=[\"Users\"])\napp.include_router(wrapped_router, prefix=\"/wrapped\", tags=[\"Wrapped\"])\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": "backend/src/main.py", "language": "python", "complexity": "simple", "star_count": 16218, "quality_score": 0.92, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 71, "word_count": 118}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from datetime import date\nfrom typing import Optional, Tuple\n\nfrom src.aggregation.layer2.user import get_user, get_user_demo\nfrom src.models import UserPackage\nfrom src.models.background import UpdateUserBackgroundTask\nfrom src.utils import use_time_range\n\n\nasync def svg_base(\n    user_id: str,\n    start_date: date,\n    end_date: date,\n    time_range: str,\n    demo: bool,\n    no_cache: bool = False,\n) -> Tuple[Optional[UserPackage], bool, Optional[UpdateUserBackgroundTask], str]:\n    # process time_range, start_date, end_date\n    time_range = \"one_month\" if demo else time_range\n    start_date, end_date, time_str = use_time_range(time_range, start_date, end_date)\n    complete = True  # overwritten later if not complete\n    background_task = None\n\n    # fetch data, either using demo or user method\n    if demo:\n        output = await get_user_demo(user_id, start_date, end_date, no_cache=no_cache)\n    else:\n        output, complete, background_task = await get_user(\n            user_id, start_date, end_date, no_cache=no_cache\n        )\n\n    return output, complete, background_task, time_str\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": "backend/src/processing/user/svg.py", "language": "python", "complexity": "simple", "star_count": 16218, "quality_score": 0.89, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 33, "word_count": 115}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from src.render.error import (\n    get_empty_demo_svg,\n    get_error_svg,\n    get_loading_svg,\n    get_no_data_svg,\n)\nfrom src.render.top_langs import get_top_langs_svg\nfrom src.render.top_repos import get_top_repos_svg\n\n__all__ = [\n    \"get_empty_demo_svg\",\n    \"get_error_svg\",\n    \"get_loading_svg\",\n    \"get_no_data_svg\",\n    \"get_top_langs_svg\",\n    \"get_top_repos_svg\",\n]\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": "backend/src/render/__init__.py", "language": "python", "complexity": "simple", "star_count": 16218, "quality_score": 0.8600000000000001, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 18, "word_count": 27}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# type: ignore\n\nfrom svgwrite import Drawing\n\nfrom src.constants import BACKEND_URL\nfrom src.render.style import styles_no_animation, themes\nfrom src.render.template import get_template\n\nTHEME = \"classic\"\n\n\ndef get_error_svg() -> Drawing:\n    d = Drawing(size=(300, 285))\n    d.defs.add(d.style(styles_no_animation[THEME]))\n\n    d.add(\n        d.rect(\n            size=(299, 284),\n            insert=(0.5, 0.5),\n            rx=4.5,\n            stroke=themes[THEME][\"border_color\"],\n            fill=themes[THEME][\"bg_color\"],\n        )\n    )\n\n    d.add(d.text(\"Unknown Error\", insert=(25, 35), class_=f\"{THEME}-header\"))\n\n    d.add(\n        d.text(\n            \"Please try again later or raise a ticket on GitHub\",\n            insert=(25, 60),\n            class_=f\"{THEME}-lang-name\",\n        )\n    )\n\n    d.add(\n        d.image(f\"{BACKEND_URL}/assets/error\", insert=(85, 100), style=\"opacity: 50%\")\n    )\n\n    return d\n\n\ndef get_empty_demo_svg(header: str) -> Drawing:\n    d = Drawing(size=(300, 285))\n    d.defs.add(d.style(styles_no_animation[THEME]))\n\n    d.add(\n        d.rect(\n            size=(299, 284),\n            insert=(0.5, 0.5),\n            rx=4.5,\n            stroke=themes[THEME][\"border_color\"],\n            fill=themes[THEME][\"bg_color\"],\n        )\n    )\n\n    d.add(d.text(header, insert=(25, 35), class_=f\"{THEME}-header\"))\n\n    d.add(\n        d.text(\n            \"Enter your username to start!\",\n            insert=(25, 60),\n            class_=f\"{THEME}-lang-name\",\n        )\n    )\n\n    d.add(\n        d.image(\n            f\"{BACKEND_URL}/assets/stopwatch\", insert=(85, 100), style=\"opacity: 50%\"\n        )\n    )\n\n    return d\n\n\ndef get_loading_svg() -> Drawing:\n    d = Drawing(size=(300, 285))\n    d.defs.add(d.style(styles_no_animation[THEME]))\n\n    d.add(\n        d.rect(\n            size=(299, 284),\n            insert=(0.5, 0.5),\n            rx=4.5,\n            stroke=themes[THEME][\"border_color\"],\n            fill=themes[THEME][\"bg_color\"],\n        )\n    )\n\n    d.add(\n        d.text(\"Loading data, hang tight!\", insert=(25, 35), class_=f\"{THEME}-header\")\n    )\n\n    d.add(\n        d.text(\n            \"Please wait a couple seconds and refresh the page.\",\n            insert=(25, 60),\n            class_=f\"{THEME}-lang-name\",\n        )\n    )\n\n    d.add(\n        d.image(\n            f\"{BACKEND_URL}/assets/stopwatch\", insert=(85, 100), style=\"opacity: 50%\"\n        )\n    )\n\n    return d\n\n\ndef get_no_data_svg(header: str, subheader: str) -> Drawing:\n    d, dp = get_template(\n        width=300,\n        height=285,\n        padding=20,\n        header_text=header,\n        subheader_text=subheader,\n        debug=False,\n        theme=THEME,\n    )\n\n    d.add(d.image(f\"{BACKEND_URL}/assets/error\", insert=(85, 80), style=\"opacity: 50%\"))\n\n    dp.add(d.text(\"No data to show\", insert=(55, 220), class_=f\"{THEME}-image-text\"))\n\n    d.add(dp)\n    return d\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": "backend/src/render/error.py", "language": "python", "complexity": "medium", "star_count": 16218, "quality_score": 0.89, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 128, "word_count": 212}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create project README with installation, usage, and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from datetime import date, timedelta\nfrom typing import Any\n\nfrom fastapi import BackgroundTasks, Response, status\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.routing import APIRouter\n\nfrom src.processing.user import get_top_languages, get_top_repos, svg_base\nfrom src.render import (\n    get_empty_demo_svg,\n    get_loading_svg,\n    get_top_langs_svg,\n    get_top_repos_svg,\n)\nfrom src.routers.background import run_in_background\nfrom src.routers.decorators import svg_fail_gracefully\n\nrouter = APIRouter()\n\n\n@router.get(\n    \"/{user_id}/langs\", status_code=status.HTTP_200_OK, response_class=HTMLResponse\n)\n@svg_fail_gracefully\nasync def get_user_lang_svg(\n    response: Response,\n    background_tasks: BackgroundTasks,\n    user_id: str,\n    start_date: date = date.today() - timedelta(30),\n    end_date: date = date.today(),\n    time_range: str = \"one_year\",\n    timezone_str: str = \"US/Eastern\",\n    use_percent: bool = False,\n    include_private: bool = False,\n    loc_metric: str = \"added\",\n    compact: bool = False,\n    demo: bool = False,\n    no_cache: bool = False,\n    use_animation: bool = True,\n    theme: str = \"classic\",\n) -> Any:\n    output, complete, background_task, time_str = await svg_base(\n        user_id, start_date, end_date, time_range, demo, no_cache\n    )\n    if background_task is not None:\n        # set a background task to update the user\n        background_tasks.add_task(run_in_background, task=background_task)\n\n    # if no data, return loading svg\n    if output is None:\n        return get_loading_svg()\n\n    # get top languages\n    processed, commits_excluded = get_top_languages(output, loc_metric, include_private)\n    return get_top_langs_svg(\n        data=processed,\n        time_str=time_str,\n        use_percent=use_percent,\n        loc_metric=loc_metric,\n        complete=complete,\n        commits_excluded=commits_excluded,\n        compact=compact,\n        use_animation=use_animation,\n        theme=theme,\n    )\n\n\n@router.get(\n    \"/{user_id}/repos\", status_code=status.HTTP_200_OK, response_class=HTMLResponse\n)\n@svg_fail_gracefully\nasync def get_user_repo_svg(\n    response: Response,\n    background_tasks: BackgroundTasks,\n    user_id: str,\n    start_date: date = date.today() - timedelta(30),\n    end_date: date = date.today(),\n    time_range: str = \"one_year\",\n    timezone_str: str = \"US/Eastern\",\n    include_private: bool = False,\n    group: str = \"none\",\n    loc_metric: str = \"added\",\n    demo: bool = False,\n    no_cache: bool = False,\n    use_animation: bool = True,\n    theme: str = \"classic\",\n) -> Any:\n    output, complete, background_task, time_str = await svg_base(\n        user_id, start_date, end_date, time_range, demo, no_cache\n    )\n    if background_task is not None:\n        # set a background task to update the user\n        background_tasks.add_task(run_in_background, task=background_task)\n\n    # if no data, return loading svg\n    if output is None:\n        return get_loading_svg()\n\n    # get top repos\n    processed, commits_excluded = get_top_repos(\n        output, loc_metric, include_private, group\n    )\n    return get_top_repos_svg(\n        data=processed,\n        time_str=time_str,\n        loc_metric=loc_metric,\n        complete=complete,\n        commits_excluded=commits_excluded,\n        use_animation=use_animation,\n        theme=theme,\n    )\n\n\n@router.get(\n    \"/demo\",\n    status_code=status.HTTP_200_OK,\n    response_class=HTMLResponse,\n    include_in_schema=False,\n)\n@svg_fail_gracefully\nasync def get_demo_svg(response: Response, card: str) -> Any:\n    if card == \"langs\":\n        return get_empty_demo_svg(\"Most Used Languages\")\n    elif card == \"repos\":\n        return get_empty_demo_svg(\"Most Contributed Repositories\")\n    else:\n        return get_empty_demo_svg(card)\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": "backend/src/routers/users/svg.py", "language": "python", "complexity": "medium", "star_count": 16218, "quality_score": 0.89, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 128, "word_count": 346}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# GitHub Trends API\n\nGitHub Trends provides two methods to access GitHub Trends data: the Website Workflow at githubtrends.io and the API Workflow described below.\n\n## Available Cards\n\nAfter authenticating with either the public or private workflow (see below), users can create the following cards to display their GitHub Trends data:\n\n- **[Languages Card](https://github.com/avgupta456/github-trends/blob/main/docs/API.md#languages-card)**: See your top languages over a given time interval, based on all commits to personal and open-source repositories.\n\n- **[Repositories Card](https://github.com/avgupta456/github-trends/blob/main/docs/API.md#repositories-card)**: See your top repositories based on lines of code contributed over a given time period. This includes both personal and open-source repositories.\n\n# Authentication\n\nYou will need to create an account with GitHub Trends to create cards. The account is used to assosciate queries to the GitHub API made on your behalf with your GitHub account's API quota. We use less than 5% of your quota in almost all scenarios. There are two levels of authentication possible:\n\n- Public Workflow: The Public Workflow asks for read-only permission to public information. This will allow us to analyze your public contributions and repositories only.\n- Private Workflow: The Private Workflow asks for read and write permission to public and private information. This will allow us to analyze your entire contribution history. See [the FAQ](https://github.com/avgupta456/github-trends/blob/main/docs/FAQ.md) for further information.\n\nYou will only need to authenticate once with GitHub Trends. Subsequent requests will use your stored access token.\n\nFor the public workflow, visit\n\n```md\nhttps://api.githubtrends.io/auth/signup/public\n```\n\nFor the private workflow, visit\n\n```md\nhttps://api.githubtrends.io/auth/signup/private\n```\n\nYou will be prompted to allow access, and (hopefully) redirected to a success screen.\n\nIf you have previously authenticated with the public workflow, you can upgrade to the private workflow by using the private link. If you would like to delete your account, go to your GitHub settings and revoke the access token.\n\n# Languages Card\n\nSee your top languages over a given time interval, based on all commits to personal and open-source repositories. Your top five languages will be displayed. Due to the approximations used internally, LOC metrics will be rounded to the nearest 100 lines.\n\nAfter authentication, visit\n\n```md\nhttps://api.githubtrends.io/user/svg/{user_id}/langs\n```\n\n## Customization\n\nThe following customization options are available:\n\n| Option            | Description                                                                                                                                | Default     |\n| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------ | ----------- |\n| `time_range`      | Specifies the time range to query statistics for. Valid options are `one_month`, `three_months`, `six_months`, `one_year`, and `all_time`. | `one_month` |\n| `include_private` | Determines if private contributions are included (requires private workflow).                                                              | `false`     |\n| `compact`         | Determines if compact layout is used (forces percentages over LOC)                                                                         | `false`     |\n| `use_percent`     | Valid if `compact=false`, determines if line of code (default) or percentages are displayed.                                               | `false`     |\n| `loc_metric`      | Options are LOC added (`added`) and LOC changed (`changed`).                                                                               | `added`     |\n| `theme`           | Theme to use for the card. See [docs/THEME.md](https://github.com/avgupta456/github-trends/blob/main/docs/THEME.md) for options.           | `classic`   |\n\nCustomizations can be appended to the endpoint, separated first with `?` and subsequently with `&`.\n\n## Example\n\nEndpoint: `https://api.githubtrends.io/user/svg/avgupta456/langs?time_range=three_months&include_private=true&compact=true`\n\n[![GitHub Trends SVG](https://api.githubtrends.io/user/svg/avgupta456/langs?time_range=three_months&include_private=true&compact=true)](https://githubtrends.io)\n\n# Repositories Card\n\nAfter authentication, visit\n\n```md\nhttps://api.githubtrends.io/user/svg/{user_id}/repos\n```\n\n## Customization\n\nThe following customization options are available:\n\n| Option            | Description                                                                                                                                | Default     |\n| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------ | ----------- |\n| `time_range`      | Specifies the time range to query statistics for. Valid options are `one_month`, `three_months`, `six_months`, `one_year`, and `all_time`. | `one_month` |\n| `include_private` | Determines if private contributions are included (requires private workflow).                                                              | `false`     |\n| `group`           | Options are `none` (default), `other` (group all other repos together), and `private` (force private repos to be grouped)                  | `none`      |\n| `use_percent`     | Valid if `compact=false`, determines if line of code (default) or percentages are displayed.                                               | `false`     |\n| `loc_metric`      | Options are LOC added (`added`) and LOC changed (`changed`).                                                                               | `added`     |\n| `theme`           | Theme to use for the card. See [docs/THEME.md](https://github.com/avgupta456/github-trends/blob/main/docs/THEME.md) for options.           | `classic`   |\n\nCustomizations can be appended to the endpoint, separated first with `?` and subsequently with `&`.\n\n## Example\n\nEndpoint: `https://api.githubtrends.io/user/svg/avgupta456/repos?time_range=one_year&include_private=true&group=private&loc_metric=changed&theme=dark`\n\n[![GitHub Trends SVG](https://api.githubtrends.io/user/svg/avgupta456/repos?time_range=one_year&include_private=true&group=private&loc_metric=changed&theme=dark)](https://githubtrends.io)\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": "docs/API.md", "language": "markdown", "complexity": "medium", "star_count": 16218, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 97, "word_count": 660}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# GitHub Trends\n\nIf you are interested in contributing to GitHub Trends, take a look through the codebase and at the open issues. Follow the guide below to set up your local environment, and contact Abhijit Gupta at `avgupta456@gmail.com` if you have any questions or need additional permissions. Thank you in advance for contributing!\n\n## Local Development\n\nFirst, copy `backend/.env-template` into `backend/.env` and fill in the missing variables. Similarly, copy `frontend/.env-template` into `frontend/.env` and fill in the missing variables. Create a Google Cloud Platform service account and include the key in `backend/gcloud_key.json`. Then run:\n\nWith Python3.11, install the dependencies from `backend/requirements.txt` and run `yarn start`.\n\nWith Node16 and Yarn, install the dependencies from `frontend/package.json` and run on a separate terminal window `yarn start-trends`.\n\n## Testing\n\nCreate a pull request and let GitHub Actions run. Alternatively, explore `.github/backend.yaml` and `.github/frontend.yaml` to run tests locally. Backend coverage must increase for PRs to be merged.\n", "metadata": {"source": "github.com/avgupta456/github-trends", "file_path": "docs/CONTRIBUTING.md", "language": "markdown", "complexity": "simple", "star_count": 16218, "quality_score": 0.8300000000000001, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 16, "word_count": 152}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\ntask:\n  name: debian check/lint root\n  only_if: $CIRRUS_BRANCH != \"master\"\n  container:\n    image: debian:bullseye\n    cpu: 4\n    memory: 2\n  script: |\n    set -xe\n    sed -ie '/buster/d' /etc/apt/sources.list\n    dev/prep-for-debianish-build\n    export LANG=C.UTF-8\n    dev/system-info\n    BUP_PYTHON_CONFIG=python3-config ./configure --with-pylint=yes\n    make -j6 check\n  on_failure:\n    debug_script:\n      test -e config/config.vars || cat config.log 1>&2\n\ntask:\n  name: debian long-check\n  only_if: $CIRRUS_BRANCH != \"master\"\n  container:\n    image: debian:bullseye\n    cpu: 4\n    memory: 2\n  script: |\n    set -xe\n    sed -ie '/buster/d' /etc/apt/sources.list\n    dev/prep-for-debianish-build\n    export LANG=C.UTF-8\n    DEBIAN_FRONTEND=noninteractive apt-get -y install bup\n    export BUP_TEST_OTHER_BUP=\"$(command -v bup)\"\n    \"$BUP_TEST_OTHER_BUP\" version\n    dev/system-info\n    adduser --disabled-password --gecos '' bup\n    chown -R bup:bup .\n    printf \"make -j6 -C %q BUP_PYTHON_CONFIG=python3-config long-check\" \\\n      \"$(pwd)\" | su -l -w BUP_TEST_OTHER_BUP bup\n  on_failure:\n    debug_script:\n      test -e config/config.vars || cat config.log 1>&2\n\ntask:\n  name: debian check\n  only_if: $CIRRUS_BRANCH != \"master\"\n  container:\n    image: debian:bookworm\n    cpu: 4\n    memory: 2\n  script: |\n    set -xe\n    dev/prep-for-debianish-build\n    export LANG=C.UTF-8\n    dev/system-info\n    adduser --disabled-password --gecos '' bup\n    chown -R bup:bup .\n    printf \"make -j6 -C %q BUP_PYTHON_CONFIG=python3-config check\" \\\n      \"$(pwd)\" | su -l bup\n  on_failure:\n    debug_script:\n      test -e config/config.vars || cat config.log 1>&2\n\ntask:\n  name: freebsd check\n  only_if: $CIRRUS_BRANCH != \"master\"\n  freebsd_instance:\n    image: freebsd-13-5-release-amd64\n    cpu: 4\n    memory: 4\n  script: |\n    set -xe\n    dev/prep-for-freebsd-build\n    dev/system-info\n    gmake -j6 check\n  on_failure:\n    debug_script:\n      test -e config/config.vars || cat config.log 1>&2\n\ntask:\n  name: macos check\n  only_if: $CIRRUS_BRANCH != \"master\"\n  macos_instance:\n    # https://cirrus-ci.org/guide/macOS/\n    image: ghcr.io/cirruslabs/macos-runner:sonoma\n  script: |\n    set -xe\n    dev/prep-for-macos-build\n    brew install bup\n    export BUP_TEST_OTHER_BUP=\"$(command -v bup)\"\n    \"$BUP_TEST_OTHER_BUP\" version\n    export PKG_CONFIG_PATH=/usr/local/opt/readline/lib/pkgconfig\n    dev/system-info\n    gmake -j6 BUP_PYTHON_CONFIG=python3-config LDFLAGS=-L/usr/local/lib check\n  on_failure:\n    debug_script:\n      test -e config/config.vars || cat config.log 1>&2\n", "metadata": {"source": "github.com/bup/bup", "file_path": ".cirrus.yml", "language": "yaml", "complexity": "simple", "star_count": 7258, "quality_score": 0.73, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 99, "word_count": 245}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\nThe Crazy Hacker's Crazy Guide to Bup Craziness\n===============================================\n\nDespite what you might have heard, bup is not that crazy, and neither are\nyou if you're trying to figure out how it works.  But it's also (as of this\nwriting) rather new and the source code doesn't have a lot of comments, so\nit can be a little confusing at first glance.  This document is designed to\nmake it easier for you to get started if you want to add a new feature, fix\na bug, or just understand how it all works.\n\n\nBup Source Code Layout\n----------------------\n\nAs you're reading this, you might want to look at different parts of the bup\nsource code to follow along and see what we're talking about.  bup's code is\nwritten primarily in python with a bit of C code in speed-sensitive places. \nHere are the most important things to know:\n\n - The main program is a fairly small C program that mostly just\n   initializes the correct Python interpreter and then runs\n   bup.main.main().  This arrangement was chosen in order to give us\n   more flexibility.  For example:\n\n     - It allows us to avoid\n       [crashing on some Unicode-unfriendly command line arguments](https://bugs.python.org/issue35883)\n       which is critical, given that paths can be arbitrary byte\n       sequences.\n\n     - It allows more flexibility in dealing with upstream changes\n       like the breakage of our ability to manipulate the\n       processes arguement list on platforms that support it during\n       the Python 3.9 series.\n\n     - It means that we'll no longer be affected by any changes to the\n       `#!/...` path, i.e. if `/usr/bin/python`, or\n       `/usr/bin/python3`, or whatever we'd previously selected during\n       `./configure` were to change from 2 to 3, or 3.5 to 3.20.\n\n   The version of python bup uses is determined by the `python-config`\n   program selected by `./configure`.  It tries to find a suitable\n   default unless `BUP_PYTHON_CONFIG` is set in the environment.\n\n - bup supports both internal and external subcommands.  The former\n   are the most common, and are all located in lib/bup/cmd/.  They\n   must be python modules named lib/bup/cmd/COMMAND.py, and must\n   contain a `main(argv)` function that will be passed the *binary*\n   command line arguments (bytes, not strings).  The filename must\n   have underscores for any dashes in the subcommand name.  The\n   external subcommands are in lib/cmd/.\n\n - The python code is all in lib/bup.\n\n - lib/bup/\\*.py contains the python code (modules) that bup depends on.\n   That directory name seems a little silly (and worse, redundant) but there\n   seemed to be no better way to let programs write \"from bup import\n   index\" and have it work.  Putting bup in the top level conflicted with\n   the 'bup' command; calling it anything other than 'bup' was fundamentally\n   wrong, and doesn't work when you install bup on your system in /usr/lib\n   somewhere.  So we get the annoyingly long paths.\n\n\nRepository Structure\n====================\n\nBefore you can talk about how bup works, we need to first address what it\ndoes.  The purpose of bup is essentially to let you \"replicate\" data between\ntwo main data structures:\n\n1. Your computer's filesystem;\n\n2. A bup repository. (Yes, we know, that part also resides in your\n   filesystem.  Stop trying to confuse yourself.  Don't worry, we'll be\n   plenty confusing enough as it is.)\n\nEssentially, copying data from the filesystem to your repository is called\n\"backing stuff up,\" which is what bup specializes in.  Normally you initiate\na backup using the 'bup save' command, but that's getting ahead of\nourselves.\n\nFor the inverse operation, ie. copying from the repository to your\nfilesystem, you have several choices; the main ones are 'bup restore', 'bup\nftp', 'bup fuse', and 'bup web'.\n\nNow, those are the basics of backups.  In other words, we just spent about\nhalf a page telling you that bup backs up and restores data.  Are we having\nfun yet?\n\nThe next thing you'll want to know is the format of the bup repository,\nbecause hacking on bup is rather impossible unless you understand that part. \nIn short, a bup repository is a git repository.  If you don't know about\ngit, you'll want to read about it now.  A really good article to read is\n\"Git for Computer Scientists\" - you can find it in Google.  Go read it now. \nWe'll wait.\n\nGot it?  Okay, so now you're an expert in blobs, trees, commits, and refs,\nthe four building blocks of a git repository.  bup uses these four things,\nand they're formatted in exactly the same way as git does it, so you can use\ngit to manipulate the bup repository if you want, and you probably won't\nbreak anything.  It's also a comfort to know you can squeeze data out using\ngit, just in case bup fails you, and as a developer, git offers some nice\ntools (like 'git rev-list' and 'git log' and 'git diff' and 'git show' and\nso on) that allow you to explore your repository and help debug when things\ngo wrong.\n\nNow, bup does use these tools a little bit differently than plain git.  We\nneed to do this in order to address two deficiencies in git when used for\nlarge backups, namely a) git bogs down and crashes if you give it really\nlarge files; b) git is too slow when you give it too many files; and c) git\ndoesn't store detailed filesystem metadata.\n\nLet's talk about each of those problems in turn.\n\n\nHandling large files (cmd/split, hashsplit.split_to_blob_or_tree)\n--------------------\n\nThe primary reason git can't handle huge files is that it runs them through\nxdelta, which generally means it tries to load the entire contents of a file\ninto memory at once.  If it didn't do this, it would have to store the\nentire contents of every single revision of every single file, even if you\nonly changed a few bytes of that file.  That would be a terribly inefficient\nuse of disk space, and git is well known for its amazingly efficient\nrepository format.\n\nUnfortunately, xdelta works great for small files and gets amazingly slow\nand memory-hungry for large files.  For git's main purpose, ie. managing\nyour source code, this isn't a problem.  But when backing up your\nfilesystem, you're going to have at least a few large files, and so it's a\nnon-starter.  bup has to do something totally different.\n\nWhat bup does instead of xdelta is what we call \"hashsplitting.\"  We wanted\na general-purpose way to efficiently back up *any* large file that might\nchange in small ways, without storing the entire file every time.  In fact,\nthe original versions of bup could only store a single file at a time;\nsurprisingly enough, this was enough to give us a large part of bup's\nfunctionality.  If you just take your entire filesystem and put it in a\ngiant tarball each day, then send that tarball to bup, bup will be able to\nefficiently store only the changes to that tarball from one day to the next. \nFor small files, bup's compression won't be as good as xdelta's, but for\nanything over a few megabytes in size, bup's compression will actually\n*work*, which is a big advantage over xdelta.\n\nHow does hashsplitting work?  It's deceptively simple.  We read through the\nfile one byte at a time, calculating a rolling checksum of the last 64\nbytes.  (Why 64?  No reason.  Literally.  We picked it out of the air.\nProbably some other number is better.  Feel free to join the mailing list\nand tell us which one and why.)  (The rolling checksum idea is actually\nstolen from rsync and xdelta, although we use it differently.  And they use\nsome kind of variable window size based on a formula we don't totally\nunderstand.)\n\nThe original rolling checksum algorithm we used was called \"stupidsum,\"\nbecause it was based on the only checksum Avery remembered how to calculate at\nthe time.  He also remembered that it was the introductory checksum\nalgorithm in a whole article about how to make good checksums that he read\nabout 15 years ago, and it was thoroughly discredited in that article for\nbeing very stupid.  But, as so often happens, Avery couldn't remember any\nbetter algorithms from the article.  So what we got is stupidsum.\n\nSince then, we have replaced the stupidsum algorithm with what we call\n\"rollsum,\" based on code in librsync.  It's essentially the same as what\nrsync does, except we use a fixed window size.\n\n(If you're a computer scientist and can demonstrate that some other rolling\nchecksum would be faster and/or better and/or have fewer screwy edge cases,\nwe need your help!  Avery's out of control!  Join our mailing list!  Please! \nSave us! ...  oh boy, I sure hope he doesn't read this)\n\nIn any case, rollsum seems to do pretty well at its job.  You can find\nit in bupsplit.c.  Basically, it converts the last 64 bytes read into\na 32-bit integer.  What we then do is take the lowest 13 bits of the\nrollsum, and if they're all 1's, we consider that to be the end of a\nchunk.  This happens on average once every 2^13 = 8192 bytes, so the\naverage chunk size is 8192 bytes.  We also cap the maximum chunk size\nat four times the average chunk size, i.e. 4 * 2^13, to avoid having\nany arbitrarily large chunks.\n\n(Why 13 bits?  Well, we picked the number at random and... eugh.  You're\ngetting the idea, right?  Join the mailing list and tell us why we're\nwrong.)\n\n(Incidentally, even though the average chunk size is 8192 bytes, the actual\nprobability distribution of block sizes ends up being non-uniform; if we\nremember our stats classes correctly, which we probably don't, it's probably\nan \"exponential distribution.\"  The idea is that for each byte in the block,\nthe probability that it's the last block is one in 8192.  Thus, the\nblock sizes end up being skewed toward the smaller end.  That's not\nnecessarily for the best, but maybe it is.  Computer science to the rescue? \nYou know the drill.)\n\nAnyway, so we're dividing up those files into chunks based on the rolling\nchecksum.  Then we store each chunk separately (indexed by its sha1sum) as a\ngit blob.  Why do we split this way?  Well, because the results are actually\nreally nice.  Let's imagine you have a big mysql database dump (produced by\nmysqldump) and it's basically 100 megs of SQL text.  Tomorrow's database\ndump adds 100 rows to the middle of the file somewhere, soo it's 100.01 megs\nof text.\n\nA naive block splitting algorithm - for example, just dividing the file into\n8192-byte blocks - would be a disaster.  After the first bit of text has\nchanged, every block after that would have a different boundary, so most of\nthe blocks in the new backup would be different from the previous ones, and\nyou'd have to store the same data all over again.  But with hashsplitting,\nno matter how much data you add, modify, or remove in the middle of the\nfile, all the chunks *before* and *after* the affected chunk are absolutely\nthe same.  All that matters to the hashsplitting algorithm is the\n\"separator\" sequence, and a single change can only affect, at most, one\nseparator sequence or the bytes between two separator sequences.  And\nbecause of rollsum, about one in 8192 possible 64-byte sequences is a\nseparator sequence.  Like magic, the hashsplit chunking algorithm will chunk\nyour file the same way every time, even without knowing how it had chunked\nit previously.\n\nThe next problem is less obvious: after you store your series of chunks as\ngit blobs, how do you store their sequence?  Each blob has a 20-byte sha1\nidentifier, which means the simple list of blobs is going to be 20/8192 =\n0.25% of the file length.  For a 200GB file, that's 488 megs of just\nsequence data.\n\nAs an overhead percentage, 0.25% basically doesn't matter.  488 megs sounds\nlike a lot, but compared to the 200GB you have to store anyway, it's\nirrelevant.  What *is* relevant is that 488 megs is a lot of memory you have\nto use in order to keep track of the list.  Worse, if you back up an\nalmost-identical file tomorrow, you'll have *another* 488 meg blob to keep\ntrack of, and it'll be almost but not quite the same as last time.\n\nHmm, big files, each one almost the same as the last... you know where this\nis going, right?\n\nActually no!  Ha!  We didn't split this list in the same way.  We could\nhave, in fact, but it wouldn't have been very \"git-like\", since we'd like to\nstore the list as a git 'tree' object in order to make sure git's\nrefcounting and reachability analysis doesn't get confused.  Never mind the\nfact that we want you to be able to 'git checkout' your data without any\nspecial tools.\n\nWhat we do instead is we extend the hashsplit algorithm a little further\nusing what we call \"fanout.\" Instead of checking just the last 13 bits of\nthe checksum, we use additional checksum bits to produce additional splits.\nNote that (most likely due to an implementation bug), the next higher bit\nafter the 13 bits (marked 'x'):\n\n  ...... '..x1'1111'1111'1111\n\nis actually ignored (so the fanout starts just to the left of the x).\nNow, let's say we use a 4-bit fanout. That means we'll break a series\nof chunks into its own tree object whenever the next 4 bits of the\nrolling checksum are 1, in addition to the 13 lowest ones.  Since the\n13 lowest bits already have to be 1, the boundary of a group of chunks\nis necessarily also always the boundary of a particular chunk.\n\nAnd so on.  Eventually you'll have too many chunk groups, but you can group\nthem into supergroups by using another 4 bits, and continue from\nthere.\n\nWhat you end up with is an actual tree of blobs - which git 'tree'\nobjects are ideal to represent.  For example, with a fanout of 4,\n\n  ...... '1011'1111'1111' 'x1'1111'1111'1111\n\nwould produce a tree two levels deep because there are 8 (two blocks\nof 4) contiguous 1 bits to the left of the ignored 14th bit.\n\nAnd if you think about it, just like the original list of chunks, the\ntree itself is pretty stable across file modifications.  Any one\nmodification will only affect the chunks actually containing the\nmodifications, thus only the groups containing those chunks, and so on\nup the tree.  Essentially, the number of changed git objects is O(log\nn) where n is the number of chunks.  Since log 200 GB, using a base of\n16 or so, is not a very big number, this is pretty awesome.  Remember,\nany git object we *don't* change in a new backup is one we can reuse\nfrom last time, so the deduplication effect is pretty awesome.\n\nBetter still, the hashsplit-tree format is good for a) random instead of\nsequential access to data (which you can see in action with 'bup fuse'); and\nb) quickly showing the differences between huge files (which we haven't\nreally implemented because we don't need it, but you can try 'git diff -M -C\n-C backup1 backup2 -- filename' for a good start).\n\nSo now we've split out 200 GB file into about 24 million pieces.  That\nbrings us to git limitation number 2.\n\n\nHandling huge numbers of files (git.PackWriter)\n------------------------------\n\ngit is designed for handling reasonably-sized repositories that change\nrelatively infrequently.  (You might think you change your source code\n\"frequently\" and that git handles much more frequent changes than, say, svn\ncan handle.  But that's not the same kind of \"frequently\" we're talking\nabout.  Imagine you're backing up all the files on your disk, and one of\nthose files is a 100 GB database file with hundreds of daily users.  Your\ndisk changes so frequently you can't even back up all the revisions even if\nyou were backing stuff up 24 hours a day.  That's \"frequently.\")\n\ngit's way of doing things works really nicely for the way software\ndevelopers write software, but it doesn't really work so well for everything\nelse.  The #1 killer is the way it adds new objects to the repository: it\ncreates one file per blob.  Then you later run 'git gc' and combine those\nfiles into a single file (using highly efficient xdelta compression, and\nignoring any files that are no longer relevant).\n\n'git gc' is slow, but for source code repositories, the resulting\nsuper-efficient storage (and associated really fast access to the stored\nfiles) is worth it.  For backups, it's not; you almost never access your\nbacked-up data, so storage time is paramount, and retrieval time is mostly\nunimportant.\n\nTo back up that 200 GB file with git and hashsplitting, you'd have to create\n24 million little 8k files, then copy them into a 200 GB packfile, then\ndelete the 24 million files again.  That would take about 400 GB of disk\nspace to run, require lots of random disk seeks, and require you to go\nthrough your data twice.\n\nSo bup doesn't do that.  It just writes packfiles directly.  Luckily, these\npackfiles are still git-formatted, so git can happily access them once\nthey're written.\n\nBut that leads us to our next problem.\n\n\nHuge numbers of huge packfiles (midx.py, bloom.py, cmd/midx, cmd/bloom)\n------------------------------\n\nGit isn't actually designed to handle super-huge repositories.  Most git\nrepositories are small enough that it's reasonable to merge them all into a\nsingle packfile, which 'git gc' usually does eventually.\n\nThe problematic part of large packfiles isn't the packfiles themselves - git\nis designed to expect the total size of all packs to be larger than\navailable memory, and once it can handle that, it can handle virtually any\namount of data about equally efficiently.  The problem is the packfile\nindexes (.idx) files.  In bup we call these idx (pronounced \"idix\") files\ninstead of using the word \"index,\" because the word index is already used\nfor something totally different in git (and thus bup) and we'll become\nhopelessly confused otherwise.\n\nAnyway, each packfile (*.pack) in git has an associated idx (*.idx) that's a\nsorted list of git object hashes and file offsets.  If you're looking for a\nparticular object based on its sha1, you open the idx, binary search it to\nfind the right hash, then take the associated file offset, seek to that\noffset in the packfile, and read the object contents.\n\nThe performance of the binary search is about O(log n) with the number of\nhashes in the pack, with an optimized first step (you can read about it\nelsewhere) that somewhat improves it to O(log(n)-7).\n\nUnfortunately, this breaks down a bit when you have *lots* of packs.  Say\nyou have 24 million objects (containing around 200 GB of data) spread across\n200 packfiles of 1GB each.  To look for an object requires you search\nthrough about 122000 objects per pack; ceil(log2(122000)-7) = 10, so you'll\nhave to search 10 times.  About 7 of those searches will be confined to a\nsingle 4k memory page, so you'll probably have to page in about 3-4 pages\nper file, times 200 files, which makes 600-800 4k pages (2.4-3.6 megs)...\nevery single time you want to look for an object.\n\nThis brings us to another difference between git's and bup's normal use\ncase.  With git, there's a simple optimization possible here: when looking\nfor an object, always search the packfiles in MRU (most recently used)\norder.  Related objects are usually clusted together in a single pack, so\nyou'll usually end up searching around 3 pages instead of 600, which is a\ntremendous improvement.  (And since you'll quickly end up swapping in all\nthe pages in a particular idx file this way, it isn't long before searching\nfor a nearby object doesn't involve any swapping at all.)\n\nbup isn't so lucky.  git users spend most of their time examining existing\nobjects (looking at logs, generating diffs, checking out branches), which\nlends itself to the above optimization.  bup, on the other hand, spends most\nof its time looking for *nonexistent* objects in the repository so that it\ncan back them up.  When you're looking for objects that aren't in the\nrepository, there's no good way to optimize; you have to exhaustively check\nall the packs, one by one, to ensure that none of them contain the data you\nwant.\n\nTo improve performance of this sort of operation, bup introduces midx\n(pronounced \"midix\" and short for \"multi-idx\") files.  As the name implies,\nthey index multiple packs at a time.\n\nImagine you had a midx file for your 200 packs.  midx files are a lot like\nidx files; they have a lookup table at the beginning that narrows down the\ninitial search, followed by a binary search.  Then unlike idx files (which\nhave a fixed-size 256-entry lookup table) midx tables have a variably-sized\ntable that makes sure the entire binary search can be contained to a single\npage of the midx file.  Basically, the lookup table tells you which page to\nload, and then you binary search inside that page.  A typical search thus\nonly requires the kernel to swap in two pages, which is better than results\nwith even a single large idx file.  And if you have lots of RAM, eventually\nthe midx lookup table (at least) will end up cached in memory, so only a\nsingle page should be needed for each lookup.\n\nYou generate midx files with 'bup midx'.  The downside of midx files is that\ngenerating one takes a while, and you have to regenerate it every time you\nadd a few packs.\n\nUPDATE: Brandon Low contributed an implementation of \"bloom filters\", which\nhave even better characteristics than midx for certain uses.  Look it up in\nWikipedia.  He also massively sped up both midx and bloom by rewriting the\nkey parts in C.  The nicest thing about bloom filters is we can update them\nincrementally every time we get a new idx, without regenerating from\nscratch.  That makes the update phase much faster, and means we can also get\naway with generating midxes less often.\n\nmidx files are a bup-specific optimization and git doesn't know what to do\nwith them.  However, since they're stored as separate files, they don't\ninterfere with git's ability to read the repository.\n\n\nHandling large directories (tree splitting)\n-------------------------------------------\n\nSimilarly to large files, git doesn't handle frequently changing large\ndirectories well either, since they're stored in a single tree object\nusing up 28 bytes plus the length of the filename for each entry\n(record). If there are a lot of files, these tree objects can become\nvery large, and every minor change to a directory such as creating a\nsingle new file in it requires storing an entirely new tree object.\nImagine an active Maildir containing tens or hundreds of thousands of\nfiles.\n\nIf tree splitting is permitted (via the `bup.split.trees` config\noption), then instead of writing a tree as one potentially large git\ntree object, bup may split it into a subtree whose \"leaves\" are git\ntree objects, each containing part of the original tree.  Note that\nfor `bup on HOST ...`  `bup.split.trees` must be set on the `HOST`.\n\nThe trees are split in a manner similar to the file hashsplitting\ndescribed above, but with the restriction that splits may only occur\nbetween (not within) tree entries.\n\nWhen a tree is split, a large directory like\n\n\tdir/{.bupm,aa,bb,...,zz}\n\nmight become\n\n\tdir/.b/{.bupm,aa,...,ii}\n\tdir/.bupd.1.bupd\n\tdir/j/{jj,...,rr}\n\tdir/s/{ss,...,zz}\n\nwhere the \".bupd.1.bupd\" inside dir/ indicates that the tree for dir/\nwas split, and the number (here \"1\") describes the number of levels\nthat were created (just one in this case).  The names in an\nintermediate level (inside dir/, but not the leaves -- in this\nexample, \".b\", \"j\", \"s\", etc.) are derived from the first filename\ncontained within each subtree, abbreviated to the shortest valid\nunique prefix.  At any level, the names contained in a subtree will\nalways be greater than or equal to the name of the subtree itself and\nless than the name of the next subtree at that level.  This makes it\npossible to know which split subtree to read at every level when\nlooking for a given filename.\n\nWhen parsing the split tree depth info file name, i.e. `.bupd.1.bupd`\nin the example above, any extra bytes in the name after the\n`.bupd.DEPTH` prefix, and before the final `.bupd` suffix must be\nignored.  The `DEPTH` will always be a sequence of `[0-9]+`, and any\nextra bytes will begin with `.`, e.g.  `.bupd.3.SOMETHING.NEW.bupd`.\nThis allows future extensions.\n\nDetailed Metadata\n-----------------\n\nSo that's the basic structure of a bup repository, which is also a git\nrepository.  There's just one more thing we have to deal with:\nfilesystem metadata.  Git repositories are really only intended to\nstore file contents with a small bit of extra information, like\nsymlink targets and executable bits, so we have to store the rest\nsome other way.\n\nBup stores more complete metadata in the VFS in a file named .bupm in\neach tree.  This file contains one entry for each file in the tree\nobject, sorted in the same order as the tree.  The first .bupm entry\nis for the directory itself, i.e. \".\", and its name is the empty\nstring, \"\".\n\nEach .bupm entry contains a variable length sequence of records\ncontaining the metadata for the corresponding path.  Each record\nrecords one type of metadata.  Current types include a common record\ntype (containing the normal stat information), a symlink target type,\na hardlink target type, a POSIX1e ACL type, etc.  See metadata.py for\nthe complete list.\n\nThe .bupm file is optional, and when it's missing, bup will behave as\nit did before the addition of metadata, and restore files using the\ntree information.\n\nThe nice thing about this design is that you can walk through each\nfile in a tree just by opening the tree and the .bupm contents, and\niterating through both at the same time.\n\nSince the contents of any .bupm file should match the state of the\nfilesystem when it was *indexed*, bup must record the detailed\nmetadata in the index.  To do this, bup records four values in the\nindex, the atime, mtime, and ctime (as timespecs), and an integer\noffset into a secondary \"metadata store\" which has the same name as\nthe index, but with \".meta\" appended.  This secondary store contains\nthe encoded Metadata object corresponding to each path in the index.\n\nCurrently, in order to decrease the storage required for the metadata\nstore, bup only writes unique values there, reusing offsets when\nappropriate across the index.  The effectiveness of this approach\nrelies on the expectation that there will be many duplicate metadata\nrecords.  Storing the full timestamps in the index is intended to make\nthat more likely, because it makes it unnecessary to record those\nvalues in the secondary store.  So bup clears them before encoding the\nMetadata objects destined for the index, and timestamp differences\ndon't contribute to the uniqueness of the metadata.\n\nBup supports recording and restoring hardlinks, and it does so by\ntracking sets of paths that correspond to the same dev/inode pair when\nindexing.  This information is stored in an optional file with the\nsame name as the index, but ending with \".hlink\".\n\nIf there are multiple index runs, and the hardlinks change, bup will\nnotice this (within whatever subtree it is asked to reindex) and\nupdate the .hlink information accordingly.\n\nThe current hardlink implementation will refuse to link to any file\nthat resides outside the restore tree, and if the restore tree spans a\ndifferent set of filesystems than the save tree, complete sets of\nhardlinks may not be restored.\n\n\nFilesystem Interaction\n======================\n\nStoring data is just half of the problem of making a backup; figuring out\nwhat to store is the other half.\n\nAt the most basic level, piping the output of 'tar' into 'bup split' is an\neasy way to offload that decision; just let tar do all the hard stuff.  And\nif you like tar files, that's a perfectly acceptable way to do it.  But we\ncan do better.\n\nBacking up with tarballs would totally be the way to go, except for two\nserious problems:\n\n1. The result isn't easily \"seekable.\"  Tar files have no index, so if (as\n   commonly happens) you only want to restore one file in a 200 GB backup,\n   you'll have to read up to 200 GB before you can get to the beginning of\n   that file.  tar is short for \"tape archive\"; on a tape, there was no\n   better way to do it anyway, so they didn't try.  But on a disk, random\n   file access is much, much better when you can figure out how.\n   \n2. tar doesn't remember which files it backed up last time, so it has to\n   read through the entire file contents again in order to generate the\n   tarball, large parts of which will then be skipped by bup since they've\n   already been stored.  This is much slower than necessary.\n\n(The second point isn't entirely true for all versions of tar. For example,\nGNU tar has an \"incremental\" mode that can somewhat mitigate this problem,\nif you're smart enough to know how to use it without hurting yourself.  But\nyou still have to decide which backups are \"incremental\" and which ones will\nbe \"full\" and so on, so even when it works, it's more error-prone than bup.)\n\nbup divides the backup process into two major steps: a) indexing the\nfilesystem, and b) saving file contents into the repository.  Let's look at\nthose steps in detail.\n\n\nIndexing the filesystem (cmd/drecurse, cmd/index, index.py)\n-----------------------\n\nSplitting the filesystem indexing phase into its own program is\nnontraditional, but it gives us several advantages.\n\nThe first advantage is trivial, but might be the most important: you can\nindex files a lot faster than you can back them up.  That means we can\ngenerate the index (.bup/bupindex) first, then have a nice, reliable,\nnon-lying completion bar that tells you how much of your filesystem remains\nto be backed up.  The alternative would be annoying failures like counting\nthe number of *files* remaining (as rsync does), even though one of the\nfiles is a virtual machine image of 80 GB, and the 1000 other files are each\nunder 10k.  With bup, the percentage complete is the *real* percentage\ncomplete, which is very pleasant.\n\nSecondly, it makes it easier to debug and test; you can play with the index\nwithout actually backing up any files.\n\nThirdly, you can replace the 'bup index' command with something else and not\nhave to change anything about the 'bup save' command.  The current 'bup\nindex' implementation just blindly walks the whole filesystem looking for\nfiles that have changed since the last time it was indexed; this works fine,\nbut something using inotify instead would be orders of magnitude faster. \nWindows and MacOS both have inotify-like services too, but they're totally\ndifferent; if we want to support them, we can simply write new bup commands\nthat do the job, and they'll never interfere with each other.\n\nAnd fourthly, git does it that way, and git is awesome, so who are we to\nargue?\n\nSo let's look at how the index file works.\n\nFirst of all, note that the \".bup/bupindex\" file is not the same as git's\n\".git/index\" file.  The latter isn't used in bup; as far as git is\nconcerned, your bup repository is a \"bare\" git repository and doesn't have a\nworking tree, and thus it doesn't have an index either.\n\nHowever, the bupindex file actually serves exactly the same purpose as git's\nindex file, which is why we still call it \"the index.\" We just had to\nredesign it for the usual bup-vs-git reasons, mostly that git just isn't\ndesigned to handle millions of files in a single repository.  (The only way\nto find a file in git's index is to search it linearly; that's very fast in\ngit-sized repositories, but very slow in bup-sized ones.)\n\nLet's not worry about the exact format of the bupindex file; it's still not\noptimal, and will probably change again.  The most important things to know\nabout bupindex are:\n\n - You can iterate through it much faster than you can iterate through the\n   \"real\" filesystem (using something like the 'find' command).\n   \n - If you delete it, you can get it back just by reindexing your filesystem\n   (although that can be annoying to wait for); it's not critical to the\n   repository itself.\n   \n - You can iterate through only particular subtrees if you want.\n \n - There is no need to have more than one index for a particular filesystem,\n   since it doesn't store anything about backups; it just stores file\n   metadata.  It's really just a cache (or 'index') of your filesystem's\n   existing metadata.  You could share the bupindex between repositories, or\n   between multiple users on the same computer.  If you back up your\n   filesystem to multiple remote repositories to be extra safe, you can\n   still use the same bupindex file across all of them, because it's the\n   same filesystem every time.\n   \n - Filenames in the bupindex are absolute paths, because that's the best way\n   to ensure that you only need one bupindex file and that they're\n   interchangeable.\n   \n\nA note on file \"dirtiness\"\n--------------------------\n\nThe concept on which 'bup save' operates is simple enough; it reads through\nthe index and backs up any file that is \"dirty,\" that is, doesn't already\nexist in the repository.\n\nDetermination of dirtiness is a little more complicated than it sounds.  The\nmost dirtiness-relevant flag in the bupindex is IX_HASHVALID; if\nthis flag is reset, the file *definitely* is dirty and needs to be backed\nup.  But a file may be dirty even if IX_HASHVALID is set, and that's the\nconfusing part.\n\nThe index stores a listing of files, their attributes, and\ntheir git object ids (sha1 hashes), if known.  The \"if known\" is what\nIX_HASHVALID is about.  When 'bup save' backs up a file, it sets\nthe sha1 and sets IX_HASHVALID; when 'bup index' sees that a file has\nchanged, it leaves the sha1 alone and resets IX_HASHVALID.\n\nRemember that the index can be shared between users, repositories, and\nbackups.  So IX_HASHVALID doesn't mean your repository *has* that sha1 in\nit; it only means that if you *do* have it, that you don't need to back up\nthe file.  Thus, 'bup save' needs to check every file in the index to make\nsure its hash exists, not just that it's valid.\n\nThere's an optimization possible, however: if you know a particular tree's\nhash is valid and exists (say /usr), then you don't need to check the\nvalidity of all its children; because of the way git trees and blobs work,\nif your repository is valid and you have a tree object, then you have all\nthe blobs it points to.  You won't back up a tree object without backing up\nits blobs first, so you don't need to double check it next time.  (If you\nreally want to double check this, it belongs in a tool like 'bup fsck' or\n'git fsck'.) So in short, 'bup save' on a \"clean\" index (all files are\nmarked IX_HASHVALID) can be very fast; we just check our repository and see\nif the top level IX_HASHVALID sha1 exists.  If it does, then we're done.\n\nSimilarly, if not the entire index is valid, you can still avoid recursing\ninto subtrees if those particular subtrees are IX_HASHVALID and their sha1s\nare in the repository.  The net result is that, as long as you never lose\nyour index, 'bup save' can always run very fast.\n\nAnother interesting trick is that you can skip backing up files even if\nIX_HASHVALID *isn't* set, as long as you have that file's sha1 in the\nrepository.  What that means is you've chosen not to backup the latest\nversion of that file; instead, your new backup set just contains the\nmost-recently-known valid version of that file.  This is a good trick if you\nwant to do frequent backups of smallish files and infrequent backups of\nlarge ones.  Each of your backups will be \"complete,\" in that they contain\nall the small files and the large ones, but intermediate ones will just\ncontain out-of-date copies of the large files. Note that this isn't done\nright now, and 'bup save --smaller' doesn't store bigger files _at all_.\n\nA final game we can play with the bupindex involves restoring: when you\nrestore a directory from a previous backup, you can update the bupindex\nright away.  Then, if you want to restore a different backup on top, you can\ncompare the files in the index against the ones in the backup set, and\nupdate only the ones that have changed.  (Even more interesting things\nhappen if people are using the files on the restored system and you haven't\nupdated the index yet; the net result would be an automated merge of all\nnon-conflicting files.) This would be a poor man's distributed filesystem. \nThe only catch is that nobody has written this feature for 'bup restore'\nyet.  Someday!\n\n\nHow 'bup save' works (cmd/save)\n--------------------\n\nThis section is too boring and has been omitted.  Once you understand the\nindex, there's nothing special about bup save.\n\n\nRetrieving backups: the bup vfs layer (vfs.py, cmd/ls, cmd/ftp, cmd/fuse)\n=====================================\n\nOne of the neat things about bup's storage format, at least compared to most\nbackup tools, is it's easy to read a particular file, or even part of a\nfile.  That means a read-only virtual filesystem is easy to generate and\nit'll have good performance characteristics.  Because of git's commit\nstructure, you could even use branching and merging to make a transactional\nread-write filesystem... but that's probably getting a little out of bup's\nscope.  Who knows what the future might bring, though?\n\nRead-only filesystems are well within our reach today, however.  The 'bup\nls', 'bup ftp', and 'bup fuse' commands all use a \"VFS\" (virtual filesystem)\nlayer to let you access your repositories.  Feel free to explore the source\ncode for these tools and vfs.py - they're pretty straightforward.  Some\nthings to note:\n\n - None of these use the bupindex for anything.\n \n - For user-friendliness, they present your refs/commits/trees as a single\n   hierarchy (ie.  a filesystem), which isn't really how git repositories\n   are formatted.  So don't get confused!\n\n\nHandling Python 3's insistence on strings\n=========================================\n\nIn Python 2 strings were bytes, and bup used them for all kinds of\ndata.  Python 3 made a pervasive backward-incompatible change to treat\nall strings as Unicode, i.e. in Python 2 'foo' and b'foo' were the\nsame thing, while u'foo' was a Unicode string.  In Python 3 'foo'\nbecame synonymous with u'foo', completely changing the type and\npotential content, depending on the locale.\n\nIn addition, and particularly bad for bup, Python 3 also (initially)\ninsisted that all kinds of things were strings that just aren't (at\nleast not on many platforms), i.e. user names, groups, filesystem\npaths, etc.  There's no guarantee that any of those are always\nrepresentable in Unicode.\n\nOver the years, Python 3 has gradually backed away from that initial\nposition, adding alternate interfaces like os.environb or allowing\nbytes arguments to many functions like open(b'foo'...), so that in\nthose cases it's at least possible to accurately retrieve the system\ndata.\n\nAfter a while, they devised the concept of\n[bytesmuggling](https://www.python.org/dev/peps/pep-0383/) as a more\ncomprehensive solution.  In theory, this might be sufficient, but our\ninitial randomized testing discovered that some binary arguments would\ncrash Python during startup[1].  Eventually Johannes Berg tracked down\nthe [cause](https://sourceware.org/bugzilla/show_bug.cgi?id=26034),\nand we hope that the problem will be fixed eventually in glibc or\nworked around by Python, but in either case, it will be a long time\nbefore any fix is widely available.\n\nBefore we tracked down that bug we were pursuing an approach that\nwould let us side step the issue entirely by manipulating the\nLC_CTYPE, but that approach was somewhat complicated, and once we\nunderstood what was causing the crashes, we decided to just let Python\n3 operate \"normally\", and work around the issues.\n\nConsequently, we've had to wrap a number of things ourselves that\nincorrectly return Unicode strings (libacl, libreadline, hostname,\netc.)  and we've had to come up with a way to avoid the fatal crashes\ncaused by some command line arguments (sys.argv) described above.  To\nfix the latter, for the time being, we just use a trivial sh wrapper\nto redirect all of the command line arguments through the environment\nin BUP_ARGV_{0,1,2,...} variables, since the variables are unaffected,\nand we can access them directly in Python 3 via environb.\n\n[1] Our randomized argv testing found that the byte smuggling approach\n    was not working correctly for some values (initially discovered in\n    Python 3.7, and observed in other versions).  The interpreter\n    would just crash while starting up like this:\n\n    Fatal Python error: _PyMainInterpreterConfig_Read: memory allocation failed\n    ValueError: character U+134bd2 is not in range [U+0000; U+10ffff]\n\n    Current thread 0x00007f2f0e1d8740 (most recent call first):\n    Traceback (most recent call last):\n      File \"t/test-argv\", line 28, in <module>\n        out = check_output(cmd)\n      File \"/usr/lib/python3.7/subprocess.py\", line 395, in check_output\n        **kwargs).stdout\n      File \"/usr/lib/python3.7/subprocess.py\", line 487, in run\n        output=stdout, stderr=stderr)\n\nWe hope you'll enjoy bup.  Looking forward to your patches!\n\n-- apenwarr and the rest of the bup team\n\n<!--\nLocal Variables:\nmode: markdown\nEnd:\n-->\n", "metadata": {"source": "github.com/bup/bup", "file_path": "DESIGN.md", "language": "markdown", "complexity": "complex", "star_count": 7258, "quality_score": 0.76, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 803, "word_count": 6832}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "% bup-cat-file(1) Bup %BUP_VERSION%\n% Rob Browning <rlb@defaultvalue.org>\n% %BUP_DATE%\n\n# NAME\n\nbup-cat-file - extract archive content (low-level)\n\n# SYNOPSIS\n\nbup cat-file [\\--meta|\\--bupm] <*path*>\n\n# DESCRIPTION\n\n`bup cat-file` extracts content associated with *path* from the\narchive and dumps it to standard output.  If nothing special is\nrequested, the actual data contained by *path* (which must be a\nregular file) will be dumped.\n\n# OPTIONS\n\n\\--meta\n:   retrieve the metadata entry associated with *path*.  Note that\n    currently this does not return the raw bytes for the entry\n    recorded in the relevant .bupm in the archive, but rather a\n    decoded and then re-encoded version.  When that matters, it should\n    be possible (though awkward) to use `--bupm` on the parent\n    directory and then find the relevant entry in the output.\n\n\\--bupm\n:   retrieve the .bupm file associated with *path*, which must be a\n    directory.\n\n# EXAMPLES\n\n    # Retrieve the content of somefile.\n    $ bup cat-file /foo/latest/somefile > somefile-content\n\n    # Examine the metadata associated with something.\n    $ bup cat-file --meta /foo/latest/something | bup meta -tvvf -\n\n    # Examine the metadata for somedir, including the items it contains.\n    $ bup cat-file --bupm /foo/latest/somedir | bup meta -tvvf -\n\n# SEE ALSO\n\n`bup-join`(1), `bup-meta`(1)\n\n# BUP\n\nPart of the `bup`(1) suite.\n", "metadata": {"source": "github.com/bup/bup", "file_path": "Documentation/bup-cat-file.1.md", "language": "markdown", "complexity": "medium", "star_count": 7258, "quality_score": 0.73, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 52, "word_count": 205}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "% bup-config(5) Bup %BUP_VERSION%\n% Rob Browning <rlb@defaultvalue.org>\n% %BUP_DATE%\n\n# NAME\n\nbup-config - bup configuration options\n\n# DESCRIPTION\n\nThe following options may be set in the relevant `git` config\n(`git-config(1)`).\n\n# OPTIONS\n\nbup.repo.id\n:   When set, an identifier for the repository which should be unique\n    across all repositories encountered. Because it is currently used\n    in filesystem paths, it must consist of only the characters within\n    double-quotes here: \"0123456789_-\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\",\n    and \"abcdefghijklmnopqrstuvwxyz\". Two repo-ids must also not\n    differ only in case (\"something\" vs \"SOMETHING\") unless all all\n    relevant filesystems are case sensitive.\n\n    `bup init` now adds a random identifier when it creates new\n    repositories or refreshes a existing repository that doesn't have\n    a `bup.repo.id`, so you can add one to repositories created before\n    this became the norm by re-running `bup init`. If you do set your\n    own identifier, consider including randomized content to help\n    ensure uniqueness.\n\n    Note: you should always change the identifier of a copied\n    repository (e.g. locally via `cp`(1) or across hosts via\n    `rsync`(1)) since duplicate identifiers can cause significant\n    performance problems, if nothing else.\n\nbup.server.deduplicate-writes (default `true`)\n:   When `true` `bup-server`(1) checks each incoming object against its\n    local index, and if the object already exists, the server suggests\n    to the client that it download the `*.idx` file that the object\n    was found in so that it can avoid sending duplicate data.\n\n    When false the server does not check its local index before\n    writing objects.  To avoid writing duplicate objects, the server\n    tells the client to download all of its `*.idx` files at the start\n    of the session.  This mode is useful on more limited server\n    hardware (i.e. routers, slow NAS devices, etc.).\n\n    If no value is set, and a `$BUP_DIR/dumb-server-mode` file exists,\n    then `bup` will act as if this setting were `false`.\n\nbup.split.files (default `legacy:13`)\n:   Method used to split data for deduplication, for example by `bup\n    save` or `bup split`. This determines the \"granularity\" of the\n    deduplication, with larger values producing, on average, larger\n    chunks. The value must be a string like `legacy:N` where the\n    integer `N` must be greater than 12 and less than 22. The default\n    of 13 provides backward compatibility, but it is recommended to\n    increase this, say to 16, for all but very small repos.\n\n    `N` specifies the number of fixed bits in the hash-split algorithm\n    that when all set to one produce a chunk boundary, and thus it\n    determines the average size of the deduplicated objects. This\n    represents a trade-off between the efficiency of the deduplication\n    (fewer bits means better deduplication) as compared to the amount\n    of metadata to keep on disk and the RAM usage during repo\n    operations (more bits means fewer objects, means less metadata\n    space and RAM use).  The expected average block size is 2^bits (1\n    << bits). A sufficiently small change in a file would cause that\n    much new data to be saved (plus tree metadata). The maximum blob\n    size is four times that.\n\n    `legacy` refers to the current split method, which has an\n    unintentional quirk where it \"skips a bit\".\n\n    *NOTE:* Changing this value in an existing repository will\n    duplicate data because it causes the split boundaries to change,\n    so subsequent saves will not deduplicate against the existing\n    data; they will just store the data again.\n\n    *NOTE:* As with `bup.split.trees` below (see NOTE), using the same\n    index for repositories with different `bup.split.files` settings\n    will result in the index optimizations not working correctly, and\n    so `bup save` will have to completely re-read files that haven't\n    been modified, which is expensive.\n\nbup.split.trees\n:   When this boolean option is set to true, `bup` will attempt to\n    split trees (directories) when writing to the repository during,\n    for example `bup save ...`, `bup gc ..`, etc.  This can notably\n    decrease the size of the new data added to the repository when\n    large directories have changed (e.g. large active Maildirs).  See\n    \"Handling large directories\" in the DESIGN in the `bup` source for\n    additional information.\n\n    *NOTE:* Using the same index to save to repositories that have\n    differing values for this option can decrease performance because\n    the index includes hashes for directories that have been saved and\n    changing this option changes the hashes for directories that are\n    affected by splitting.\n\n    A directory tree's hash allows bup to avoid traversing the\n    directory if the index indicates that it didn't otherwise change\n    and the tree object with that hash already exists in the\n    destination repository.  Since the the value of this setting\n    changes the hashes of splittable trees, the hash in the index\n    won't be found in a repository that has a different\n    `bup.split.trees` value from the one to which that tree was last\n    saved.  As a result, any (usually big) directory subject to tree\n    splitting will have to be re-read and its related hashes\n    recalculated.\n\ncore.compression\n:   The default pack file compression level if `core.compression`\n    isn't set.  If this isn't set either, the default is 1 (unlike\n    git, which defaults to -1).  A compression level given on the\n    command-line overrides this. See also `git-config`(1).\n\npack.compression\n:   The default pack file compression level.  If not given, falls back\n    to `core.compression`. See also `git-config`(1).\n\npack.packSizeLimit\n:   Limits the maximum pack size (see `git-config`(1)) when writing\n    pack files (e.g. via `bup save`). This setting is relevant when\n    set in the destination repository (which may be remote). The\n    default value is 1e9 bytes, i.e. about 0.93 GiB, and `bup` may\n    exceed this limit by a chunk. However, setting it to e.g. \"2g\" (2\n    GiB) will still mean that all objects in the pack can be addressed\n    by a 31-bit offset, and thus need no large offset in the idx file.\n\n# ENVIRONMENT\n\nBUP_DIR\n:   When set, the default repository location, unless overridden by\n    `bup -d` on the command line.\n\nXDG_CACHE_HOME/bup\n:   The preferred cache location.\n\n# FILES\n\n\\$XDG_CACHE_HOME/bup/remote \\\n\\~/.cache/bup/remote \\\n\n\\$BUP_DIR/index-cache\n:   The client index cache locations, in order of precedence. Remote\n    repository data is stored here during operations like `bup save\n    -r`. Currently `bup` looks for existing data in precedence order,\n    and if none is found for the repository of interest then `bup`\n    will use `$XDG_CACHE_HOME/bup/remote` if `$XDG_CACHE_HOME` is set\n    and `~/.cache/bup/remote` otherwise.\n\n# SEE ALSO\n\n`git-config`(1)\n\n# BUP\n\nPart of the `bup`(1) suite.\n", "metadata": {"source": "github.com/bup/bup", "file_path": "Documentation/bup-config.5.md", "language": "markdown", "complexity": "medium", "star_count": 7258, "quality_score": 0.7899999999999999, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 161, "word_count": 1027}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "% bup-damage(1) Bup %BUP_VERSION%\n% Avery Pennarun <apenwarr@gmail.com>\n% %BUP_DATE%\n\n# NAME\n\nbup-damage - randomly destroy blocks of a file\n\n# SYNOPSIS\n\nbup damage [-n count] [-s maxsize] [\\--percent pct] [-S seed]\n[\\--equal] \\<filenames...\\>\n\n# DESCRIPTION\n\nUse `bup damage` to deliberately destroy blocks in a\n`.pack` or `.idx` file (from `.bup/objects/pack`) to test\nthe recovery features of `bup-fsck`(1) or other programs.\n\n*THIS PROGRAM IS EXTREMELY DANGEROUS AND WILL DESTROY YOUR\nDATA*\n\n`bup damage` is primarily useful for automated or manual tests\nof data recovery tools, to reassure yourself that the tools\nactually work.\n\nNote that the details of the current behavior may change (particularly\nthe details not documented here).  For example the moment, the damage\nis strictly probabilistic, and so may or may not actually alter any\ngiven block.  With a block size of 1, there should be a 1/256 chance\nthat the block won't actually change.  This behavior may change.\n\n# OPTIONS\n\n-n, \\--num=*numblocks*\n:   the number of separate blocks to damage in each file\n    (default 10).\n    Note that it's possible for more than one damaged\n    segment to fall in the same `bup-fsck`(1) recovery block,\n    so you might not damage as many recovery blocks as you\n    expect.  If this is a problem, use `--equal`.\n\n-s, \\--size=*maxblocksize*\n:   the maximum size, in bytes, of each damaged block\n    (default 1 unless `--percent` is specified).  Note that\n    because of the way `bup-fsck`(1) works, a multi-byte\n    block could fall on the boundary between two recovery\n    blocks, and thus damaging two separate recovery blocks. \n    In small files, it's also possible for a damaged block\n    to be larger than a recovery block.  If these issues\n    might be a problem, you should use the default damage\n    size of one byte.\n    \n\\--percent=*maxblockpercent*\n:   the maximum size, in percent of the original file, of\n    each damaged block.  If both `--size` and `--percent`\n    are given, the maximum block size is the minimum of the\n    two restrictions.  You can use this to ensure that a\n    given block will never damage more than one or two\n    `git-fsck`(1) recovery blocks.\n    \n-S, \\--seed=*randomseed*\n:   seed the random number generator with the given value. \n    If you use this option, your tests will be repeatable,\n    since the damaged block offsets, sizes, and contents\n    will be the same every time.  By default, the random\n    numbers are different every time (so you can run tests\n    in a loop and repeatedly test with different\n    damage each time).\n    \n\\--equal\n:   instead of choosing random offsets for each damaged\n    block, space the blocks equally throughout the file,\n    starting at offset 0.  If you also choose a correct\n    maximum block size, this can guarantee that any given\n    damage block never damages more than one `git-fsck`(1)\n    recovery block.  (This is also guaranteed if you use\n    `-s 1`.)\n    \n# EXAMPLES\n    # make a backup in case things go horribly wrong\n    cp -pPR ~/.bup/objects/pack ~/bup-packs.bak\n    \n    # generate recovery blocks for all packs\n    bup fsck -g\n    \n    # deliberately damage the packs\n    bup damage -n 10 -s 1 -S 0 ~/.bup/objects/pack/*.{pack,idx}\n    \n    # recover from the damage\n    bup fsck -r\n\n# SEE ALSO\n\n`bup-fsck`(1), `par2`(1)\n\n# BUP\n\nPart of the `bup`(1) suite.\n", "metadata": {"source": "github.com/bup/bup", "file_path": "Documentation/bup-damage.1.md", "language": "markdown", "complexity": "simple", "star_count": 7258, "quality_score": 0.73, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 100, "word_count": 515}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "% bup-drecurse(1) Bup %BUP_VERSION%\n% Avery Pennarun <apenwarr@gmail.com>\n% %BUP_DATE%\n\n# NAME\n\nbup-drecurse - recursively list files in your filesystem\n\n# SYNOPSIS\n\nbup drecurse [-x] [-q] [\\--exclude *path*]\n\\ [\\--exclude-from *filename*] [\\--exclude-rx *pattern*]\n\\ [\\--exclude-rx-from *filename*] [\\--profile] \\<path\\>\n\n# DESCRIPTION\n\n`bup drecurse` traverses files in the filesystem in a way\nsimilar to `find`(1).  In most cases, you should use\n`find`(1) instead.\n\nThis program is useful mainly for testing the file\ntraversal algorithm used in `bup-index`(1).\n\nNote that filenames are returned in reverse alphabetical\norder, as in `bup-index`(1).  This is important because you\ncan't generate the hash of a parent directory until you\nhave generated the hashes of all its children.  When\nlisting files in reverse order, the parent directory will\ncome after its children, making this easy.\n\n# OPTIONS\n\n-x, \\--xdev, \\--one-file-system\n:   don't cross filesystem boundaries -- though as with tar and rsync,\n    the mount points themselves will still be reported.\n\n-q, \\--quiet\n:   don't print filenames as they are encountered.  Useful\n    when testing performance of the traversal algorithms.\n\n\\--exclude=*path*\n:   exclude *path* from the backup (may be repeated).\n\n\\--exclude-from=*filename*\n:   read --exclude paths from *filename*, one path per-line (may be\n    repeated).  Ignore completely empty lines.\n    \n\\--exclude-rx=*pattern*\n:   exclude any path matching *pattern*.  See `bup-index`(1) for\n    details, but note that unlike index, drecurse will produce\n    relative paths if the drecurse target is a relative path. (may be\n    repeated).\n\n\\--exclude-rx-from=*filename*\n:   read --exclude-rx patterns from *filename*, one pattern per-line\n    (may be repeated).  Ignore completely empty lines.\n\n\\--profile\n:   print profiling information upon completion.  Useful\n    when testing performance of the traversal algorithms.\n    \n# EXAMPLES\n    bup drecurse -x /\n\n# SEE ALSO\n\n`bup-index`(1)\n\n# BUP\n\nPart of the `bup`(1) suite.\n", "metadata": {"source": "github.com/bup/bup", "file_path": "Documentation/bup-drecurse.1.md", "language": "markdown", "complexity": "simple", "star_count": 7258, "quality_score": 0.73, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 72, "word_count": 278}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with installation and setup instructions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "% bup-features(1) Bup %BUP_VERSION%\n% Rob Browning <rlb@defaultvalue.org>\n% %BUP_DATE%\n\n# NAME\n\nbup-features - report the current status and capabilities of bup itself\n\n# SYNOPSIS\n\nbup features\n\n# DESCRIPTION\n\n`bup features` reports information about the current bup installation,\nfor example, which version of the Python interpreter is used, whether command\nline editing is supported by `bup ftp`, or POSIX ACLs can be saved and\nrestored.\n\n# EXAMPLES\n\n    $ bup features\n    bup 0.31~a7ff2d5b8c12b24b97858aad1251d28c18f8c1e1\n    source a7ff2d5b8c12b24b97858aad1251d28c18f8c1e1 2020-07-05 14:54:06 -0500\n        Python: 3.7.3\n        Command line editing (e.g. bup ftp): yes\n        Saving and restoring POSIX ACLs: yes\n    ....\n\n# SEE ALSO\n\n`bup-version`(1)\n\n# BUP\n\nPart of the `bup`(1) suite.\n", "metadata": {"source": "github.com/bup/bup", "file_path": "Documentation/bup-features.1.md", "language": "markdown", "complexity": "medium", "star_count": 7258, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 37, "word_count": 104}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "% bup-fsck(1) Bup %BUP_VERSION%\n% Avery Pennarun <apenwarr@gmail.com>\n% %BUP_DATE%\n\n# NAME\n\nbup-fsck - verify or repair a bup repository\n\n# SYNOPSIS\n\nbup fsck [-r] [-g] [-v] [\\--quick] [-j *jobs*] [\\--par2-ok]\n[\\--disable-par2] [packfile...]\n\n# DESCRIPTION\n\n`bup fsck` validates bup repositories much the way `git fsck`\nvalidates git repositories.  When *packfile*s (which must end in\n.pack) are specified, pack-related operations are limited to those\nfiles, otherwise all packfiles in the current repository are\nconsidered.\n\nIt can also generate and/or use \"recovery blocks\" using the\n`par2`(1) tool (if you have it installed).  This allows you\nto recover from damaged blocks covering up to 5% of your\n`.pack` files.\n\nIn a normal backup system, damaged blocks are less\nimportant, because there tends to be enough data duplicated\nbetween backup sets that a single damaged backup set is\nnon-critical.  In a deduplicating backup system like bup,\nhowever, no block is ever stored more than once, even if it\nis used in every single backup.  If that block were to be\nunrecoverable, *all* your backup sets would be\ndamaged at once.  Thus, it's important to be able to verify\nthe integrity of your backups and recover from disk errors\nif they occur.\n\nWhen attempting to `--repair`, bup will exit with status 1 if and only\nif repairs were needed and were successful, and there were no other\nerrors.\n\n*WARNING*: bup fsck obviously cannot recover from a\ncomplete disk failure.  If your backups are important, you\nneed to carefully consider redundancy (such as using RAID\nfor multi-disk redundancy, or making off-site backups for\nsite redundancy).\n\n# OPTIONS\n\n-r, \\--repair\n:   attempt to repair any damaged packs using\n    existing recovery blocks.  (Requires `par2`(1).)\n    \n-g, \\--generate\n:   generate recovery blocks for any packs that don't\n    already have them.  (Requires `par2`(1).)\n\n-v, \\--verbose\n:   increase verbosity (can be used more than once).\n\n\\--quick\n:   don't run a full `git verify-pack` on each pack file;\n    instead just check the final checksum.  This can cause\n    a significant speedup with no obvious decrease in\n    reliability.  However, you may want to avoid this\n    option if you're paranoid.  Has no effect on packs that\n    already have recovery information.\n    \n-j, \\--jobs=*numjobs*\n:   maximum number of pack verifications to run at a time. \n    The optimal value for this option depends how fast your\n    CPU can verify packs vs. your disk throughput.  If you\n    run too many jobs at once, your disk will get saturated\n    by seeking back and forth between files and performance\n    will actually decrease, even if *numjobs* is less than\n    the number of CPU cores on your system.  You can\n    experiment with this option to find the optimal value.\n    \n\\--par2-ok\n:   immediately return 0 if `par2`(1) is installed and\n    working, or 1 otherwise.  Do not actually check\n    anything.\n    \n\\--disable-par2\n:   pretend that `par2`(1) is not installed, and ignore all\n    recovery blocks.\n\n\n# EXAMPLES\n    # generate recovery blocks for all packs that don't\n    # have them\n    bup fsck -g\n    \n    # generate recovery blocks for a particular pack\n    bup fsck -g ~/.bup/objects/pack/153a1420cb1c8*.pack\n    \n    # check all packs for correctness (can be very slow!)\n    bup fsck\n    \n    # check all packs for correctness and recover any\n    # damaged ones\n    bup fsck -r\n    \n    # check a particular pack for correctness and recover\n    # it if damaged\n    bup fsck -r ~/.bup/objects/pack/153a1420cb1c8*.pack\n    \n    # check if recovery blocks are available on this system\n    if bup fsck --par2-ok; then\n    \techo \"par2 is ok\"\n    fi\n\n# EXIT STATUS\n\nExits with 1 if `--repair` was requested, needed, successful, and\nthere were no other errors.  Otherwise exits with 0 if there were no\nerrors and a value other than zero or one for errors.\n\n# SEE ALSO\n\n`bup-damage`(1), `fsck`(1), `git-fsck`(1)\n\n# BUP\n\nPart of the `bup`(1) suite.\n", "metadata": {"source": "github.com/bup/bup", "file_path": "Documentation/bup-fsck.1.md", "language": "markdown", "complexity": "medium", "star_count": 7258, "quality_score": 0.73, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 126, "word_count": 610}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "% bup-ftp(1) Bup %BUP_VERSION%\n% Avery Pennarun <apenwarr@gmail.com>\n% %BUP_DATE%\n\n# NAME\n\nbup-ftp - ftp-like client for navigating bup repositories\n\n# SYNOPSIS\n\nbup ftp\n\n# DESCRIPTION\n\n`bup ftp` is a command-line tool for navigating bup\nrepositories.  It has commands similar to the Unix `ftp`(1)\ncommand.  The file hierarchy is the same as that shown by\n`bup-fuse`(1) and `bup-ls`(1).\n\nNote: if your system has the python-readline library\ninstalled, you can use the \\<tab\\> key to complete filenames\nwhile navigating your backup data.  This will save you a\nlot of typing.\n\n\n# COMMANDS\n\nThe following commands are available inside `bup ftp`:\n\nls [-s] [-a] [*path*]\n:   print the contents of a directory. If no path argument\n    is given, the current directory's contents are listed.\n    If -a is given, also include hidden files (files which\n    start with a `.` character). If -s is given, each file\n    is displayed with its hash from the bup archive to its\n    left.\n\ncd *dirname*\n:   change to a different working directory\n\npwd\n:   print the path of the current working directory\n\ncat *filenames...*\n:   print the contents of one or more files to stdout\n\nget *filename* *localname*\n:   download the contents of *filename* and save it to disk\n    as *localname*.  If *localname* is omitted, uses\n    *filename* as the local name.\n    \nmget *filenames...*\n:   download the contents of the given *filenames* and\n    stores them to disk under the same names.  The\n    filenames may contain Unix filename globs (`*`, `?`,\n    etc.)\n    \nhelp\n:   print a list of available commands\n\nquit\n:   exit the `bup ftp` client\n\n\n# EXAMPLES\n    $ bup ftp\n    bup> ls\n    mybackup/    yourbackup/\n\n    bup> cd mybackup/\n    bup> ls\n    2010-02-05-185507@   2010-02-05-185508@    latest@\n\n    bup> cd latest/\n    bup> ls\n      (...etc...)\n\n    bup> get myfile\n    Saving 'myfile'\n    bup> quit\n\n\n# SEE ALSO\n\n`bup-fuse`(1), `bup-ls`(1), `bup-save`(1), `bup-restore`(1)\n\n\n# BUP\n\nPart of the `bup`(1) suite.\n", "metadata": {"source": "github.com/bup/bup", "file_path": "Documentation/bup-ftp.1.md", "language": "markdown", "complexity": "simple", "star_count": 7258, "quality_score": 0.73, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 91, "word_count": 301}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "% bup-fuse(1) Bup %BUP_VERSION%\n% Avery Pennarun <apenwarr@gmail.com>\n% %BUP_DATE%\n\n# NAME\n\nbup-fuse - mount a bup repository as a filesystem\n\n# SYNOPSIS\n\nbup fuse [-d] [-f] [-o] \\<mountpoint\\>\n\n# DESCRIPTION\n\n`bup fuse` opens a bup repository and exports it as a\n`fuse`(7) userspace filesystem.\n\nThis feature is only available on systems (such as Linux)\nwhich support FUSE.\n\n**WARNING**: bup fuse is still experimental and does not\nenforce any file permissions!  All files will be readable\nby all users.\n\nWhen you're done accessing the mounted fuse filesystem, you\nshould unmount it with `umount`(8).\n\n# OPTIONS\n\n-d, \\--debug\n:   run in the foreground and print FUSE debug information\n    for each request.\n\n-f, \\--foreground\n:   run in the foreground and exit only when the filesystem\n    is unmounted.\n\n-o, \\--allow-other\n:   permit other users to access the filesystem. Necessary for\n    exporting the filesystem via Samba, for example.\n\n\\--meta\n:   report some of the original metadata (when available) for the\n    mounted paths (currently the uid, gid, mode, and timestamps).\n    Without this, only generic values will be presented.  This option\n    is not yet enabled by default because it may negatively affect\n    performance, and note that any timestamps before 1970-01-01 UTC\n    (i.e. before the Unix epoch) will be presented as 1970-01-01 UTC.\n\n-v, \\--verbose\n:   increase verbosity (can be used more than once).\n\n# EXAMPLES\n    rm -rf /tmp/buptest\n    mkdir /tmp/buptest\n    sudo bup fuse -d /tmp/buptest\n    ls /tmp/buptest/*/latest\n    ...\n    umount /tmp/buptest\n\n# SEE ALSO\n\n`fuse`(7), `fusermount`(1), `bup-ls`(1), `bup-ftp`(1),\n`bup-restore`(1), `bup-web`(1)\n\n# BUP\n\nPart of the `bup`(1) suite.\n", "metadata": {"source": "github.com/bup/bup", "file_path": "Documentation/bup-fuse.1.md", "language": "markdown", "complexity": "simple", "star_count": 7258, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 69, "word_count": 250}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "% bup-gc(1) Bup %BUP_VERSION%\n% Rob Browning <rlb@defaultvalue.org>\n% %BUP_DATE%\n\n# NAME\n\nbup-gc - remove unreferenced, unneeded data\n\n# SYNOPSIS\n\nbup gc [-#|\\--verbose] <*branch*|*save*...>\n\n# DESCRIPTION\n\n`bup gc` removes (permanently deletes) unreachable data from the\nrepository, data that isn't referred to directly or indirectly by the\ncurrent set of branches (backup sets) and tags.  But bear in mind that\ngiven deduplication, deleting a save and running the garbage collector\nmight or might not actually delete anything (or reclaim any space).\n\nWith the current, proababilistic implementation, some fraction of the\nunreachable data may be retained.  In exchange, the garbage collection\nshould require less RAM than might be required by some more precise\napproaches.\n\nTypically, the garbage collector would be invoked after some set of\ninvocations of `bup rm`.\n\nWARNING: This is one of the few bup commands that modifies your\narchive in intentionally destructive ways.  Though if an attempt to\n`join` or `restore` the data you still care about after a `gc`\nsucceeds, that's a fairly encouraging sign that the commands worked\ncorrectly.  (The `dev/compare-trees` command in the source tree can be\nused to help test before/after results.)\n\n# OPTIONS\n\n\\--threshold=N\n:   only rewrite a packfile if it's over N percent garbage and\n    contains no unreachable trees or commits.  The default threshold\n    is 10%.\n\n-v, \\--verbose\n: increase verbosity (can be given more than once).\n\n-*#*, \\--compress=*#*\n:   set the compression level to # (a value from 0-9, where 9 is the\n    highest and 0 is no compression). Defaults to a configured\n    pack.compression or core.compression, or 1 (fast, loose\n    compression).\n\n\\--ignore-missing\n:   report missing objects, but don't stop the collection.\n\n# EXIT STATUS\n\nThe exit status will be nonzero if there were any errors.\nEncountering any missing object is considered an error.\n\n# EXAMPLES\n\n    # Remove all saves of \"home\" and most of the otherwise unreferenced data.\n    $ bup rm home\n    $ bup gc\n\n# SEE ALSO\n\n`bup-rm`(1) and `bup-fsck`(1)\n\n# BUP\n\nPart of the `bup`(1) suite.\n", "metadata": {"source": "github.com/bup/bup", "file_path": "Documentation/bup-gc.1.md", "language": "markdown", "complexity": "simple", "star_count": 7258, "quality_score": 0.76, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 73, "word_count": 326}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "% bup-get(1) Bup %BUP_VERSION%\n% Rob Browning <rlb@defaultvalue.org>\n% %BUP_DATE%\n\n# NAME\n\nbup-get - copy repository items (note CAUTION below)\n\n# SYNOPSIS\n\nbup get \\[-s *source-path*\\] \\[-r *host*:*path*\\]  OPTIONS \\<(METHOD *ref* [*dest*])\\>...\n\n# DESCRIPTION\n\n`bup get` copies the indicated *ref*s from the source repository to\nthe destination repository (respecting `--bup-dir` and `BUP_DIR`),\naccording to the specified METHOD, which may be one of `--ff`,\n`--ff:`, `--append`, `--append:`, `--pick`, `--pick:`, `--force-pick`,\n`--force-pick:`, `--new-tag`, `--new-tag:`, `--replace`, `--replace:`,\nor `--unnamed`.  See the EXAMPLES below for a quick introduction.\n\nThe *ref* is the source repository reference of the object to be\nfetched, and the *dest* is the optional destination reference.  A\n*dest* may only be specified for a METHOD whose name ends in a colon.\nFor example:\n\n    bup get -s /source/repo --ff foo\n    bup get -s /source/repo --ff: foo/latest bar\n    bup get -s /source/repo --pick: foo/2010-10-10-101010 .tag/bar\n\nAs a special case, if *ref* names the \"latest\" save symlink, then bup\nwill act exactly as if the save that \"latest\" points to had been\nspecified, rather than the \"latest\" symlink itself, so `bup get\nfoo/latest` will actually be interpreted as something like `bup get\nfoo/2013-01-01-030405`.\n\nIn some situations `bup get` will evaluate a branch operation\naccording to whether or not it will be a \"fast-forward\" (which\nrequires that any existing destination branch be an ancestor of the\nsource).\n\nAn existing destination tag can only be overwritten by a `--replace`\nor `--force-pick`.\n\nWhen a new commit is created (i.e. via `--append`, `--pick`, etc.), it\nwill have the same author, author date, and message as the original,\nbut a committer and committer date corresponding to the current user\nand time.\n\nIf requested by the appropriate options, bup will print the commit,\ntree, or tag hash for each destination reference updated.  When\nrelevant, the tree hash will be printed before the commit hash.\n\nLocal *ref*s can be pushed to a remote repository with the `--remote`\noption, and remote *ref*s can be pulled into a local repository via\n\"bup on HOST get ...\".  See `bup-on`(1) and the EXAMPLES below for\nfurther information.\n\nCAUTION: This is one of the few bup commands that can modify your\narchives in intentionally destructive ways.  Though if an attempt to\njoin or restore the data you still care about succeeds after you've\nrun this command, then that's a fairly encouraging sign that it worked\ncorrectly.  (The dev/compare-trees command in the source tree can be\nused to help test before/after results.)\n\n# METHODS\n\n\\--ff *ref*, \\--ff: *ref* *dest*\n:   fast-forward *dest* to match *ref*.  If *dest* is not specified\n    and *ref* names a save, set *dest* to the save's branch.  If\n    *dest* is not specified and *ref* names a branch or a tag, use the\n    same name for *dest*.\n\n\\--append *ref*, \\--append: *ref* *dest*\n:   append all of the commits represented by *ref* to *dest* as new\n    commits.  If *ref* names a directory/tree, append a new commit for\n    that tree.  If *dest* is not specified and *ref* names a save or\n    branch, set *dest* to the *ref* branch name.  If *dest* is not\n    specified and *ref* names a tag, use the same name for *dest*.\n\n\\--pick *ref*, \\--pick: *ref* *dest*\n:   append the single commit named by *ref* to *dest* as a new commit.\n    If *dest* is not specified and *ref* names a save, set *dest* to\n    the *ref* branch name.  If *dest* is not specified and *ref* names\n    a tag, use the same name for *dest*.\n\n\\--force-pick *ref*, \\--force-pick: *ref* *dest*\n:   do the same thing as `--pick`, but don't refuse to overwrite an\n    existing tag, and if the tag refers to a commit, make it the\n    parent of *ref*.\n\n\\--new-tag *ref*, \\--new-tag: *ref* *dest*\n:   create a *dest* tag for *ref*, but refuse to overwrite an existing\n    tag.  If *dest* is not specified and *ref* names a tag, use the\n    same name for *dest*.\n\n\\--replace *ref*, \\--replace: *ref* *dest*\n:   clobber *dest* with *ref*, overwriting any existing tag, or\n    replacing any existing branch.  If *dest* is not specified and\n    *ref* names a branch or tag, use the same name for *dest*.\n\n\\--unnamed *ref*\n:   copy *ref* into the destination repository, without any name,\n    leaving a potentially dangling reference until/unless the object\n    named by *ref* is referred to some other way (cf. `bup tag`).\n\n# OPTIONS\n\n-s, \\--source=*path*\n:   use *path* as the source repository, instead of the default.\n\n-r, \\--remote=*host*:*path*\n:   store the indicated items on the given remote server.  If *path*\n    is omitted, uses the default path on the remote server (you still\n    need to include the ':').  The connection to the remote server is\n    made with SSH.  If you'd like to specify which port, user or\n    private key to use for the SSH connection, we recommend you use\n    the `~/.ssh/config` file.\n\n-c, \\--print-commits\n:   for each updated branch, print the new git commit id.\n\n-t, \\--print-trees\n:   for each updated branch, print the new git tree id of the\n    filesystem root.\n\n\\--print-tags\n:   for each updated tag, print the new git id.\n\n-v, \\--verbose\n:   increase verbosity (can be used more than once).  With\n    `-v`, print the name of every item fetched, with `-vv` add\n    directory names, and with `-vvv` add every filename.\n\n\\--bwlimit=*bytes/sec*\n:   don't transmit more than *bytes/sec* bytes per second to the\n    server.  This can help avoid sucking up all your network\n    bandwidth.  Use a suffix like k, M, or G to specify multiples of\n    1024, 1024\\*1024, 1024\\*1024\\*1024 respectively.\n\n-*#*, \\--compress=*#*\n:   set the compression level to # (a value from 0-9, where 9 is the\n    highest and 0 is no compression). Defaults to a configured\n    pack.compression or core.compression, or 1 (fast, loose\n    compression).\n\n\\--ignore-missing\n:   ignore missing objects encountered during a transfer.  Currently\n    only supported by `--unnamed`, and potentially *dangerous*.\n\n# EXAMPLES\n\n    # Update or copy the archives branch in src-repo to the local repository.\n    $ bup get -s src-repo --ff archives\n\n    # Append a particular archives save to the pruned-archives branch.\n    $ bup get -s src-repo --pick: archives/2013-01-01-030405 pruned-archives\n\n    # Update or copy the archives branch on remotehost to the local\n    # repository.\n    $ bup on remotehost get --ff archives\n\n    # Update or copy the local branch archives to remotehost.\n    $ bup get -r remotehost: --ff archives\n\n    # Update or copy the archives branch in src-repo to remotehost.\n    $ bup get -s src-repo -r remotehost: --ff archives\n\n    # Update the archives-2 branch on remotehost to match archives.\n    # If archives-2 exists and is not an ancestor of archives, bup\n    # will refuse.\n    $ bup get -r remotehost: --ff: archives archives-2\n\n    # Replace the contents of branch y with those of x.\n    $ bup get --replace: x y\n\n    # Copy the latest local save from the archives branch to the\n    # remote tag foo.\n    $ bup get -r remotehost: --pick: archives/latest .tag/foo\n\n    # Or if foo already exists:\n    $ bup get -r remotehost: --force-pick: archives/latest .tag/foo\n\n    # Append foo (from above) to the local other-archives branch.\n    $ bup on remotehost get --append: .tag/foo other-archives\n\n    # Append only the /home directory from archives/latest to only-home.\n    $ bup get -s \"$BUP_DIR\" --append: archives/latest/home only-home\n\n# SEE ALSO\n\n`bup-on`(1), `bup-tag`(1), `ssh_config`(5)\n\n# BUP\n\nPart of the `bup`(1) suite.\n", "metadata": {"source": "github.com/bup/bup", "file_path": "Documentation/bup-get.1.md", "language": "markdown", "complexity": "medium", "star_count": 7258, "quality_score": 0.76, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 197, "word_count": 1187}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "% bup-help(1) Bup %BUP_VERSION%\n% Avery Pennarun <apenwarr@gmail.com>\n% %BUP_DATE%\n\n# NAME\n\nbup-help - open the documentation for a given bup command\n\n# SYNOPSIS\n\nbup help \\<command\\>\n\n# DESCRIPTION\n\n`bup help <command>` opens the documentation for the given command.\nThis is currently equivalent to typing `man bup-<command>`.\n\n\n# EXAMPLES\n\n    $ bup help help\n    (Imagine that this man page was pasted below,\n     recursively.  Since that would cause an endless loop\n     we include this silly remark instead.  Chicken.)\n    \n# BUP\n\nPart of the `bup`(1) suite.\n", "metadata": {"source": "github.com/bup/bup", "file_path": "Documentation/bup-help.1.md", "language": "markdown", "complexity": "simple", "star_count": 7258, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 29, "word_count": 83}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "% bup-import-duplicity(1) Bup %BUP_VERSION%\n% Zoran Zaric <zz@zoranzaric.de>, Rob Browning <rlb@defaultvalue.org>\n% %BUP_DATE%\n\n# NAME\n\nbup-import-duplicity - import duplicity backups\n\n# WARNING\n\nbup-import-duplicity is **EXPERIMENTAL** (proceed with caution)\n\n# SYNOPSIS\n\nbup import-duplicity [-n] \\<source-url\\> \\<save-name\\>\n\n# DESCRIPTION\n\n`bup import-duplicity` imports all of the duplicity backups at\n`source-url` into `bup` via `bup save -n save-name`.  The bup saves\nwill have the same timestamps (via `bup save --date`) as the original\nbackups.\n\nBecause this command operates by restoring each duplicity backup to a\ntemporary directory, the extent to which the metadata is preserved\nwill depend on the characteristics of the underlying filesystem,\nwhether or not you run `import-duplicity` as root (or under\n`fakeroot`(1)), etc.\n\nNote that this command will use [`mkdtemp`][mkdtemp] to create\ntemporary directories, which means that it should respect any\n`TEMPDIR`, `TEMP`, or `TMP` environment variable settings.  Make sure\nthat the relevant filesystem has enough space for the largest\nduplicity backup being imported.\n\nSince all invocations of duplicity use a temporary `--archive-dir`,\n`import-duplicity` should not affect ~/.cache/duplicity.\n\n# OPTIONS\n\n-n, \\--dry-run\n:   don't do anything; just print out what would be done\n\n# EXAMPLES\n\n    $ bup import-duplicity file:///duplicity/src/ legacy-duplicity\n\n# BUP\n\nPart of the `bup`(1) suite.\n\n[mkdtemp]: https://docs.python.org/3/library/tempfile.html#tempfile.mkdtemp\n", "metadata": {"source": "github.com/bup/bup", "file_path": "Documentation/bup-import-duplicity.1.md", "language": "markdown", "complexity": "simple", "star_count": 7258, "quality_score": 0.76, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 53, "word_count": 198}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "% bup-import-rdiff-backup(1) Bup %BUP_VERSION%\n% Zoran Zaric <zz@zoranzaric.de>\n% %BUP_DATE%\n\n# NAME\n\nbup-import-rdiff-backup - import a rdiff-backup archive\n\n# SYNOPSIS\n\nbup import-rdiff-backup [-n] <path to rdiff-backup root> <backup name>\n\n# DESCRIPTION\n\n`bup import-rdiff-backup` imports a rdiff-backup archive. The\ntimestamps for the backups are preserved and the path to\nthe rdiff-backup archive is stripped from the paths.\n\n# OPTIONS\n\n-n, \\--dry-run\n:   don't do anything just print out what would be done\n\n# EXAMPLES\n\n    $ bup import-rdiff-backup /.snapshots legacy-rdiff-backup\n\n# BUP\n\nPart of the `bup`(1) suite.\n", "metadata": {"source": "github.com/bup/bup", "file_path": "Documentation/bup-import-rdiff-backup.1.md", "language": "markdown", "complexity": "simple", "star_count": 7258, "quality_score": 0.76, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 31, "word_count": 85}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "dist: xenial\nlanguage: python\npython:\n  - 3.7\n  - 3.8\n  - 3.9\n  - 3.10\n  - pypy3\ninstall:\n  - pip install -e .[tests]\nscript:\n  - flake8\n  - pytest -m \"not assumption\"\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": ".travis.yml", "language": "yaml", "complexity": "simple", "star_count": 6671, "quality_score": 0.7, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 14, "word_count": 29}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Authors\n=======\n\n[Grip][home] is written and maintained by Joe Esposito,\nalong with the following contributors:\n\n- Vlad Wing ([@vladwing](https://github.com/vladwing))\n- Ismail Badawi ([@isbadawi](https://github.com/isbadawi))\n- Joe Littlejohn ([@joelittlejohn](https://github.com/joelittlejohn))\n- Brian Cappello ([@briancappello](https://github.com/briancappello))\n- John Gallagher ([@jgallagher](https://github.com/jgallagher))\n- Ilya Rumyantsev ([@iliggio](https://github.com/iliggio))\n- Jon Chen ([@fly](https://github.com/fly))\n- Silas Snider ([@swsnider](https://github.com/swsnider))\n- Dave James Miller ([@davejamesmiller](https://github.com/davejamesmiller))\n- Alexandre Magno ([@alexandre-mbm](https://github.com/alexandre-mbm))\n- [@madflow](https://github.com/madflow)\n- Zhiming Wang ([@zmwangx](https://github.com/zmwangx))\n- Dan Davison ([@dandavison](https://github.com/dandavison))\n- Sriram Sundarraj ([@ssundarraj](https://github.com/ssundarraj))\n- Jose Honorato ([@jlhonora](https://github.com/jlhonora))\n- Aka.Why ([@akawhy](https://github.com/akawhy))\n- Mark Thomas ([@markbt](https://github.com/markbt))\n- Gast√≥n N. Charkiewicz ([@mekoda](https://github.com/mekoda))\n- Erik Hummel ([@ErikMHummel](https://github.com/ErikMHummel))\n- Matthew R. Tanudjaja ([@mrexmelle](https://github.com/mrexmelle))\n- Tom Dunlap ([@motevets](https://github.com/motevets))\n- Konstantin Baierer ([@kba](https://github.com/kba))\n- Jakub Wilk ([@jwilk](https://github.com/jwilk))\n- Devin Chen ([@xxd3vin](https://github.com/xxd3vin))\n- Jamie Davis ([@davisjam](https://github.com/davisjam))\n- JasonThomasData ([@JasonThomasData](https://github.com/JasonThomasData))\n- Andrej ([@4ndrej](https://github.com/4ndrej))\n- Karl Goffin ([@kagof](https://github.com/kagof))\n- Gideon Richter ([@Godron629](https://github.com/Godron629))\n- Tom Dupr√© la Tour ([@TomDLT](https://github.com/TomDLT))\n- Joshua Adelman ([@synapticarbors](https://github.com/synapticarbors))\n- Simeon Visser ([@svisser](https://github.com/svisser))\n- Jace Browning ([@jacebrowning](https://github.com/jacebrowning))\n- Daniel Shannon ([@phyllisstein](https://github.com/phyllisstein))\n- Aaron Sikes ([@courajs](https://github.com/courajs))\n- Winsley ([@wvspee](https://github.com/wvspee))\n- Methacrylon ([@Methacrylon](https://github.com/Methacrylon))\n- Bryce Carson ([@Methacrylon](https://github.com/bryce-carson))\n\n\n[home]: README.md\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "AUTHORS.md", "language": "markdown", "complexity": "simple", "star_count": 6671, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 48, "word_count": 167}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create project README with installation, usage, and examples with installation and setup instructions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Grip Changelog\n--------------\n\n\n#### Version 4.6.2 (2023-10-12)\n\n##### Bugs fixed\n\n- Support Werkzeug 3 by using a default encoding of UTF-8 ([#377](https://github.com/joeyespo/grip/ issues/377) - thanks, [@daniel-eichinger-bl][]!)\n\n\n#### Version 4.6.1 (2022-03-30)\n\n##### Bugs fixed\n\n- Fix \"ImportError: cannot import name 'safe_join' from 'flask'\" when using Flask 2.1.0+ ([#360](https://github.com/joeyespo/grip/pull/360) - thanks, [@bryce-carson][]!)\n\n\n#### Version 4.6.0 (2022-02-01)\n\n##### Notable changes\n\n- Update output and **fix style issues**\n- Add support for **Python 3.7, 3.8, 3.9, 3.10**\n- Improve handling of PDFs and other binary files\n- Fix deprecation warnings from newer dependency versions\n\n##### Deprecations\n\n- End-of-life [Python versions](https://devguide.python.org/#status-of-python-branches) no longer intentionally supported\n  - Python 2.6, 2.7 (Python 2 compatibility will be removed entirely in the next major release)\n  - Python 3.0, 3.1, 3.2, 3.3 ([#275](https://github.com/joeyespo/grip/pull/275) - thanks, [@svisser][]!), 3.4, 3.5, 3.6\n- Current [dependency versions](requirements.txt) (more recent versions will be required in the next major release)\n\n##### Bugs fixed\n\n- Fix typo in error message ([#266](https://github.com/joeyespo/grip/pull/266) - thanks, [@Godron629][]!)\n- Update HTML/CSS scaffold to match GitHub ([#297](https://github.com/joeyespo/grip/pull/297) - thanks, [@phyllisstein][]!)\n- Fix tests by adding a missing `USER_CONTEXT` argument to one of the `GitHubRenderer` calls\n- Serve non-`text/` MIME types as raw files for better handling of, e.g. PDF files ([#234](https://github.com/joeyespo/grip/pull/234) - thanks, [@wvspee][]!)\n- Fix style parsing, which caused some styles to be missing ([#347](https://github.com/joeyespo/grip/issues/347))\n\n##### Other changes\n\n- Readme: Update to correct `--no-inline` option ([#267](https://github.com/joeyespo/grip/issues/267) - thanks, [@TomDLT][]!)\n- Readme: Fix links to Grip class ([#276](https://github.com/joeyespo/grip/issues/276) - thanks, [@jacebrowning][]!)\n- Readme: Clarify that this sends your markdown to GitHub ([#251](https://github.com/joeyespo/grip/issues/251) - thanks, [@courajs][]!)\n- Update `pytest` usages (see [pytest 3.0.0 changelog](https://github.com/pytest-dev/pytest/blob/master/CHANGELOG.rst#300-2016-08-18))\n- Add `.pytest_cache` to `.gitignore` ([#287](https://github.com/joeyespo/grip/pull/287) - thanks, [@svisser][]!)\n- Add `.venv` and `.idea` to `.gitignore` and clean up\n- Add `flake8` test dependency and fix linting errors ([#299](https://github.com/joeyespo/grip/pull/299))\n- Add `pytest` test dependency ([#294](https://github.com/joeyespo/grip/pull/294) - thanks, [@Methacrylon][]!)\n- License: Distribute LICENSE file ([#274](https://github.com/joeyespo/grip/pull/274) - thanks, [@synapticarbors][]!)\n- License: Update year\n\n\n###### Version 4.5.2 (2018-03-18)\n\n- Enhancement: Show more helpful error message on TLSV1_ALERT_PROTOCOL_VERSION to help people through ([#262](https://github.com/joeyespo/grip/pull/262))\n- Docs: Use smaller header for patch version titles in CHANGES.md\n\n\n###### Version 4.5.1 (2018-03-18)\n\n- Bugfix: Don't print \"Downloading\" from `--export` when using `--quiet`\n- Cleanup: Move `quiet` kwarg of `export()` to the end like the other API methods\n- Readme: Document `quiet` kwarg of `export()`\n- Readme: Document `quiet` kwarg of `render_page()`\n\n\n#### Version 4.5.0 (2018-03-18)\n\n- Enhancement: Show warning if no styles are collected ([#265](https://github.com/joeyespo/grip/pull/265))\n- Bugfix: Don't print \"Exporting\" from `--export` when using `--quiet` ([#247](https://github.com/joeyespo/grip/pull/247) - thanks, [@kagof][]!)\n- Bugfix: Fix style parsing ([#265](https://github.com/joeyespo/grip/pull/265))\n- Bugfix: Loosen up the style URLs matching ([#265](https://github.com/joeyespo/grip/pull/265))\n- Internals: Fix `_strip_url_params` method name typo ([#254](https://github.com/joeyespo/grip/pull/254) - thanks, [@jwilk][]!)\n- Readme: Fix syntax highlighting in README ([#264](https://github.com/joeyespo/grip/pull/264) - thanks, [@jwilk][]!)\n- Readme: Remove Gratipay :frowning_face: ([Article](https://gratipay.news/the-end-cbfba8f50981))\n- Readme: Tweak Contributing copy\n- Tests: Wrap map() with list() for multiple iteration for Python 3 support\n- Tests: Call `Popen.communicate()` only once\n- Tests: Regenerate render output\n- Cleanup: PEP8\n- Thanks: [@jwilk][] for getting the GitHub repo to finally show the right Language stats :smiley: ([#263](https://github.com/joeyespo/grip/pull/263))\n\n\n#### Version 4.4.0 (2017-09-24)\n\n- Patch: Work around a GitHub API header bug ([#245](https://github.com/joeyespo/grip/pull/245))\n- Bugfix: Fail cleanly when server address is already in use ([#216](https://github.com/joeyespo/grip/pull/216) - thanks, [@davisjam][]!)\n- Bugfix: Fix `--browser` when running on a random port (port 0) ([#219](https://github.com/joeyespo/grip/pull/219))\n- Bugfix: Add defaults to USERNAME and PASSWORD in case just one is provided ([#222](https://github.com/joeyespo/grip/pull/222))\n- Internals: Fix deprecation warning in Python 3.6 by using triple-quoted raw strings in regexp constants ([#223](https://github.com/joeyespo/grip/pull/223) - thanks, [@jwilk][]!)\n- Internals: Decouple patchers from renderers\n- Docs: Fix typo in API docs ([#224](https://github.com/joeyespo/grip/pull/224) - thanks, [@jwilk][]!)\n- Readme: Add Homebrew installation instructions ([#201](https://github.com/joeyespo/grip/issues/201) - thanks, [@zmwangx][]!)\n- Readme: Use `pwd` instead of `PWD` in bash shell example ([#207](https://github.com/joeyespo/grip/pull/207) - thanks, [@xxd3vin][]!)\n- Readme: Use 'index.html' in --export example for searchability\n- Readme: Fix link to `offline-renderer` branch ([#230](https://github.com/joeyespo/grip/pull/230) - thanks, [@4ndrej][]!)\n- Readme: Add Configuration troubleshooting tip and example\n- Readme: Add [Say Thanks](http://saythanks.io/) badge :smiley:\n- Thanks: [@JasonThomasData][] for investigating and working with GitHub Support to fix a heading bug ([#235](https://github.com/joeyespo/grip/pull/235))\n\n\n#### Version 4.3.2 (2016-07-21)\n\n- Bugfix: Fix `setup.py install` ([#199](https://github.com/joeyespo/grip/issues/199))\n\n\n###### Version 4.3.1 (2016-07-21)\n\n- Bugfix: Allow port 0 ([#197](https://github.com/joeyespo/grip/pull/197))\n- Bugfix: Fix setup.py build and include octicon files in the build asset ([#198](https://github.com/joeyespo/grip/issues/198))\n\n\n#### Version 4.3.0 (2016-07-19)\n\n- Bugfix: Fix `--export` deadlock ([#184](https://github.com/joeyespo/grip/issues/184))\n- Bugfix: Make `--quiet` truly quiet ([#188](https://github.com/joeyespo/grip/pull/188) - thanks, [@kba][]!)\n- Bugfix: Allow `--clear` to work without a README file present ([#189](https://github.com/joeyespo/grip/issues/189))\n- Bugfix: Fix tests by patching sys.exc_clear to have Flask 0.11 work on pypy3 ([#190](https://github.com/joeyespo/grip/issues/190))\n- Bugfix: Fix minor version check during patch_svg ([#191](https://github.com/joeyespo/grip/issues/191))\n- Bugfix: Add Octicon font until the Readme API inlines SVGs ([#192](https://github.com/joeyespo/grip/issues/192))\n- Bugfix: Show '- Grip' in `<title>` only, not in page header ([#193](https://github.com/joeyespo/grip/issues/193))\n- Readme: Remove duplicated words ([#177](https://github.com/joeyespo/grip/pull/177) - thanks, [@jwilk][]!)\n- Readme: Remove download count badge since it's broken ([#194](https://github.com/joeyespo/grip/pull/194))\n\n\n#### Version 4.2.0 (2016-04-12)\n\n- Enhancement: Expose `render_inline` through `--no-inline` CLI option ([#165](https://github.com/joeyespo/grip/pull/165) - thanks, [@mrexmelle][]!)\n- Bugfix: Update Readme width to match latest GitHub changes ([#167](https://github.com/joeyespo/grip/pull/167) - thanks, [@motevets][]!)\n- Bugfix: Update Octicons to match latest GitHub changes ([#167](https://github.com/joeyespo/grip/pull/167) - thanks, [@motevets][]!)\n- Bugfix: Enable SVG support for Python 2.6 and below ([#157](https://github.com/joeyespo/grip/issues/157))\n- Readme: Fix Mac OS name ([#162](https://github.com/joeyespo/grip/pull/162) - thanks, [@ErikMHummel][]!)\n- Readme: Add tip for generating docs from Readme files ([#168](https://github.com/joeyespo/grip/pull/168) - thanks, [@mrexmelle][]!)\n- License: Update year\n\n\n#### Version 4.1.0 (2015-12-11)\n\n- Enhancement: Prompt for password when `--user` is provided without a `--pass` ([#154](https://github.com/joeyespo/grip/pull/154) - thanks, [@mekoda][]!)\n- Bugfix: Default to UTF-8 when reading from stdin and there's no encoding ([#152](https://github.com/joeyespo/grip/issues/152))\n- Readme: Document `-b`\n- Readme: Add [link to Grip Release Letter](http://joeyespo.us2.list-manage.com/subscribe?u=255dd49194fa8078258d9a735&id=d76534e979&group[14285][1]=true) for getting notified about future releases\n- Readme: Remove Known Issues section since they are now working\n\n\n### Version 4.0.0 (2015-11-18)\n\n##### Notable changes\n\n- Content is now refreshed when the file changes ([#135](https://github.com/joeyespo/grip/pull/135) - thanks, [@markbt][]!)\n- Rename `--gfm` to `--user-content` to reduce confusion ([#139](https://github.com/joeyespo/grip/pull/139))\n- Fix general Readme [Task Lists](https://github.com/blog/1825-task-lists-in-all-markdown-documents) ([#149](https://github.com/joeyespo/grip/issues/149))\n- Rearchitect the internals\n- Add tests\n\n##### Breaking changes (API)\n\n- Reorder API function arguments for consistency\n- Remove `STATIC_URL_PATH` from settings (use `Grip` constructor or the ENV variable instead)\n- Remove `STYLE_URLS_SOURCE` and `STYLE_ASSET_` settings (they're now constants)\n- Remove `CACHE_URL` setting (the cache URL is now `{}/cache.format(GRIPURL)`)\n- Remove `github_renderer.render_content` (use `GitHubRenderer` instead)\n- Remove `offline_renderer.render_content` (use `OfflineRenderer` instead)\n- Remove `read_binary` and `read_text` functions (use `io.open` directly)\n- Remove `resolve_readme` (use `DirectoryReader(path, force).filename_for(None)` instead)\n- Remove `text.read` call in `TextReader.read(text)`\n- Route Grip assets through `/__/grip` by default instead of `/`\n- Raise `ReadmeNotFoundError` when a Readme is not found instead of `ValueError`\n- Require a Unicode string when rendering Markdown\n- Set `DEBUG` to `False` by default in settings\n- Use UPPERCASE for constants\n\n##### Assumptions fixed\n\n- Update URL of GitHub assets\n\n##### Bugs fixed\n\n- Stop `--browser` from consuming all the sockets ([#136](https://github.com/joeyespo/grip/pull/136) - thanks, [@markbt][]!)\n- Fix `--browser` to stop waiting when the server is terminated before it listens\n- Fix `--browser` when listening on `0.0.0.0`\n- Fix Python 2.6\n- Fix Python 3.x with `python -m grip`\n- Allow caching of assets that include query parameters\n- Take the `route` argument into account in `Grip.render` (this was broken in the old `render_app`)\n- Fix rendering Readme containing Unicode by manually decoding UTF-8 from the GitHub response\n\n##### Other changes\n\n- Add `Home.md` as a supported default file title for GitHub Wikis\n- Add `AlreadyRunningError` for calling `Grip.run` while the server is already running\n- Add `ReadmeNotFoundError` for cross-Python-version file-not-found errors\n- Add `Grip`, a subclass of `Flask`\n- Add `ReadmeAssetManager` and `GitHubAssetManager`\n- Add `ReadmeReader`, `DirectoryReader`, `StdinReader`, and `TextReader`\n- Add `ReadmeRenderer`, `GitHubRenderer`, and `OfflineRenderer`\n- Add `grip.command.version` for printing version information (similar to `grip.command.usage`)\n- Print version with `-V`\n- Make `port` and `cancel_event` optional arguments in `wait_and_start_browser`\n- Add `start_browser_when_ready` to wait and start the browser in a background thread\n- Add the `--quiet` CLI option\n- The `GRIPHOME` ENV variable now expands the user directory (`~`), e.g. `~/.config/grip`\n- Add `DEFAULT_API_URL` constant as a fallback for when `api_url` is not specified in `render_content` or `GitHubRenderer`\n- Add `grip_url` and its fallback constant `DEFAULT_GRIPURL` for specifying a route to serve Grip assets from\n- Remove implicit dependencies in `requirements.txt`\n- Readme: Add [Tips section](https://github.com/joeyespo/grip#tips)\n\n\n#### Version 3.3.0 (2015-06-28)\n\n- Enhancement: Add `GRIPHOME` environment variable for alternative `settings.py` locations ([#117](https://github.com/joeyespo/grip/pull/117) - thanks, [@zmwangx][]!)\n- Enhancement: Allow alternative github API URL ([#119](https://github.com/joeyespo/grip/pull/119) - thanks, [@dandavison][]!)\n- Enhancement: Change the default port to `6419` to avoid conflicts ([#124](https://github.com/joeyespo/grip/pull/124))\n- Enhancement: Automatically open grip in a new tab in browser ([122](https://github.com/joeyespo/grip/pull/122) - thanks, [@ssundarraj][]!)\n- Enhancement: Only cache styles and assets if all downloads are successful\n- Enhancement: Add `--title` option to manually set the title on the rendered page ([#125](https://github.com/joeyespo/grip/pull/125) - thanks, [@jlhonora][]!)\n- Enhancement: Render tables and lists in `render_offline` mode ([#133](https://github.com/joeyespo/grip/pull/133) - thanks, [@akawhy][]!)\n- Bugfix: Don't print info messages to `STDOUT` for when exporting to `STDOUT` ([#101](https://github.com/joeyespo/grip/issues/101))\n- Bugfix: Don't swallow system exceptions\n- Bugfix: Use list builder notation instead of `map` to get `default_filenames` to evaluate it to non-empty on Python 3\n- Bugfix: Fix asset retrieval on both Python 2 and 3\n- Bugfix: Fix `InsecureRequestWarning` problems ([#111](https://github.com/joeyespo/grip/issues/111), [#128](https://github.com/joeyespo/grip/issues/128))\n- Bugfix: Fix missing Octicons by properly downloading assets as binary files ([#127](https://github.com/joeyespo/grip/issues/127))\n- Bugfix: Add trailing slashes to directories and remove them for files so relative links are correct ([#131](https://github.com/joeyespo/grip/issues/131))\n- CLI: Add \"Omit this to render as a normal GitHub README file.\" to help avoid confusion of `--gfm`\n- Readme: Add note about `^D` and `^Z` on Windows ([#105](https://github.com/joeyespo/grip/issues/105))\n- Cleanup: PEP8\n\n\n#### Version 3.2.0 (2015-02-24)\n\n- Bugfix: Encode to UTF-8 before sending text off to GitHub to support non-`latin-1` characters ([#99](https://github.com/joeyespo/grip/issues/99))\n\n\n#### Version 3.1.0 (2015-02-08)\n\n- Bugfix: Support non-ascii languages ([#86](https://github.com/joeyespo/grip/issues/86))\n- Bugfix: Fix links to header anchors ([#94](https://github.com/joeyespo/grip/issues/94))\n- Bugfix: Unpin dependencies so there's no conflict with other globally-installed packages\n- Bugfix: Fix missing octicons ([#95](https://github.com/joeyespo/grip/pull/95) - thanks, [@madflow][]!)\n- Bugfix: Fix \"Could not retrieve styles\" error on Windows ([#90](https://github.com/joeyespo/grip/pull/90) - thanks, [@alexandre-mbm][]!)\n- Cleanup\n\n\n### Version 3.0.0 (2014-08-08)\n\n- Enhancement: Add `{version}` format argument to `CACHE_DIRECTORY` so upgrades can start fresh and also drive cache clearing\n- Enhancement: Allow exporting to stdout ([#73](https://github.com/joeyespo/grip/issues/73))\n- Enhancement: Allow reading from stdin ([#72](https://github.com/joeyespo/grip/issues/72))\n- Enhancement: Allow `:<port>` pattern in CLI\n- Enhancement: Add a favicon ![favicon](artwork/favicon.ico) ([#60](https://github.com/joeyespo/grip/issues/60))\n- Enhancement: Add \"GitHub rate limit\" page to replace the generic 403 error ([#48](https://github.com/joeyespo/grip/issues/48))\n- Enhancement: Add option to clear the cache ([#68](https://github.com/joeyespo/grip/issues/68))\n- Enhancement: Allow storing credentials in config file ([#61](https://github.com/joeyespo/grip/issues/61))\n- Enhancement: Read user settings from `~/.grip`\n- Enhancement: Add `__main__.py` for allowing grip to be run as a module with `python -m grip`\n- Enhancement: Add `--wide` option to render as the old GitHub size (to opt out of [#47](https://github.com/joeyespo/grip/issues/47))\n- Enhancement: Add title to rendered page to look more like GitHub\n- Bugfix: Allow using [personal auth tokens](https://github.com/settings/tokens/new?scopes=) without a username\n- Bugfix: Show images from their canonical source instead of using GitHub's cache ([#50](https://github.com/joeyespo/grip/issues/50))\n- Bugfix: Inline assets into the exported file ([#69](https://github.com/joeyespo/grip/issues/69))\n- Bugfix: Cache the assets of the styles, ([#56](https://github.com/joeyespo/grip/issues/56))\n- Bugfix: Allow cross-platform newlines in config ([#67](https://github.com/joeyespo/grip/pull/67))\n- Bugfix: Fix running from another directory ([#36](https://github.com/joeyespo/grip/issues/36))\n- Bugfix: Move `instance_path` to `~/.grip` to cache to a non-privileged directory ([#39](https://github.com/joeyespo/grip/pull/39) - thanks, [@swsnider][]!)\n- Bugfix: Change the default width to match GitHub's new README style ([#47](https://github.com/joeyespo/grip/issues/47))\n- Readme: Mention personal access tokens and link to the appropriate GitHub page ([#74](https://github.com/joeyespo/grip/pull/74) - thanks, [@davejamesmiller][]!)\n- Readme: Add badges, more usage example, and support and contact information\n- Readme: Specify that HTTPS is always used to access the GitHub API\n- Readme: Document credentials and rate limit ([#46](https://github.com/joeyespo/grip/issues/46))\n- Readme: Document configuration options\n- Readme: Clarify command line arguments and `--gfm`\n- Readme: Add Known Issues section\n- Infrastructure and code cleanup\n\n\n###### Version 2.0.1 (2014-06-14)\n\n- Enhancement: Add ability to export to a specific file using the CLI ([#33](https://github.com/joeyespo/grip/issues/33))\n- Enhancement: Python 3 compatibility ([#54](https://github.com/joeyespo/grip/pull/54) - thanks, [@fly][]!)\n- Bugfix: Fix issue styles weren't being downloaded properly (thanks, [@fly][]!)\n- Bugfix: Support anchoring to section headers like GitHub ([#58](https://github.com/joeyespo/grip/issues/58))\n- Readme: Document rate limits and --user / --pass\n- Readme: Refer to the \"offline rendering\" work\n- Readme: Fix 'GitHub' spelling\n- Extract and expose constants\n- Update requirements\n\n\n### Version 2.0.0 (2013-09-26)\n\n- Feature: Styles are now cached (from the not-yet-released offline rendering) (thanks, [@isbadawi][]!)\n- Feature: Add user/pass options for GitHub auth (thanks, [@joelittlejohn][]!)\n- Feature: Add export to single HTML file (thanks, [@iliggio][]!)\n- Enhancement: Better HTML titles by normalizing the path, always providing a title\n- Enhancement: Allow styles to be overridden, with examples in static directory\n- Enhancement: Relay GitHub API HTTP errors to browser for debuggability\n- Enhancement: Extract render_app and add create_app in API\n- Bugfix: Fix manual installs using setup.py (thanks, [@briancappello][]!)\n- Bugfix: Fix rendering local images (thanks, [@jgallagher][]!)\n- Bugfix: Handle File Not Found errors as 404 when given a directory\n- Rename and re-arrange the configuration files\n- Update README.md with new features\n- Update AUTHORS.md format\n- Upgrade requirements\n- Simplify code\n\n\n#### Version 1.2.0 (2013-03-17)\n\n- Add AUTHORS.md for attributing credit\n- Feature: Can now click hyperlinks to render other files without re-running (thanks, [@vladwing][]!)\n- Update GitHub CSS regular expression\n\n\n###### Version 1.1.1 (2013-01-05)\n\n- Upgrade path-and-address\n- Use exact versions in requirements\n\n\n#### Version 1.1 (2013-01-04)\n\n- Readme: Clarify and add examples\n- Bugfix: Typo in requirements\n- Clean up setup.py\n\n\n### Version 1.0 (2012-12-08)\n\n- CLI: now accepts an address, not just a port\n- CLI: now accepts --gfm and --context=<repo> arguments for rendering GitHub Flavored Markdown\n- API: `serve` function now accepts `None` for its arguments to indicate 'use default'\n- API: `serve` function now accepts a 'host'\n- API: `serve` now resolves the default file when given a path\n- Now using docopt for more advanced argument processing\n- Now using path-and-address for humanistic path / address handling\n\n\n#### Version 0.2.1 (2012-12-02)\n\n- Issue #5: Fixed the installer to work in the case where the requirements are not already installed\n\n\n#### Version 0.2 (2012-12-01)\n\n- GitHub styles are now retrieved dynamically when run, instead of using the outdated styles from the config\n\n\n#### Version 0.1.1 (2012-11-20)\n\n- Added the port command-line argument\n\n\n#### Version 0.1 (2012-11-19)\n\n- First public preview release\n\n\n[@vladwing]: https://github.com/vladwing\n[@isbadawi]: https://github.com/isbadawi\n[@joelittlejohn]: https://github.com/joelittlejohn\n[@briancappello]: https://github.com/briancappello\n[@jgallagher]: https://github.com/jgallagher\n[@iliggio]: https://github.com/iliggio\n[@fly]: https://github.com/fly\n[@swsnider]: https://github.com/swsnider\n[@davejamesmiller]: https://github.com/davejamesmiller\n[@alexandre-mbm]: https://github.com/alexandre-mbm\n[@madflow]: https://github.com/madflow\n[@zmwangx]: https://github.com/zmwangx\n[@dandavison]: https://github.com/dandavison\n[@ssundarraj]: https://github.com/ssundarraj\n[@jlhonora]: https://github.com/jlhonora\n[@akawhy]: https://github.com/akawhy\n[@markbt]: https://github.com/markbt\n[@mekoda]: https://github.com/mekoda\n[@ErikMHummel]: https://github.com/ErikMHummel\n[@mrexmelle]: https://github.com/mrexmelle\n[@motevets]: https://github.com/motevets\n[@kba]: https://github.com/kba\n[@jwilk]: https://github.com/jwilk\n[@xxd3vin]: https://github.com/xxd3vin\n[@davisjam]: https://github.com/davisjam\n[@JasonThomasData]: https://github.com/JasonThomasData\n[@4ndrej]: https://github.com/4ndrej\n[@erikrtn]: https://github.com/erikrtn\n[@kagof]: https://github.com/kagof\n[@Godron629]: https://github.com/Godron629\n[@TomDLT]: https://github.com/TomDLT\n[@synapticarbors]: https://github.com/synapticarbors\n[@svisser]: https://github.com/svisser\n[@jacebrowning]: https://github.com/jacebrowning\n[@phyllisstein]: https://github.com/phyllisstein\n[@courajs]: https://github.com/courajs\n[@wvspee]: https://github.com/wvspee\n[@Methacrylon]: https://github.com/Methacrylon\n[@bryce-carson]: https://github.com/bryce-carson\n[@daniel-eichinger-bl]: https://github.com/daniel-eichinger-bl\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "CHANGES.md", "language": "markdown", "complexity": "complex", "star_count": 6671, "quality_score": 0.8799999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 404, "word_count": 2464}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create project README with installation, usage, and examples with installation and setup instructions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Grip -- GitHub Readme Instant Preview\n=====================================\n\n[![Current version on PyPI](http://img.shields.io/pypi/v/grip.svg)][pypi]\n[![Say Thanks!](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/joeyespo)\n\nRender local readme files before sending off to GitHub.\n\nGrip is a command-line server application written in Python that uses the\n[GitHub markdown API][markdown] to render a local readme file. The styles\nand rendering come directly from GitHub, so you'll know exactly how it will appear.\nChanges you make to the Readme will be instantly reflected in the browser without\nrequiring a page refresh.\n\n\nMotivation\n----------\n\nSometimes you just want to see the exact readme\nresult before committing and pushing to GitHub.\n\nEspecially when doing [Readme-driven development][rdd].\n\n\nInstallation\n------------\n\nTo install grip, simply:\n\n```console\n$ pip install grip\n```\n\nOn OS X, you can also install with Homebrew:\n\n```console\n$ brew install grip\n```\n\n\nUsage\n-----\n\nTo render the readme of a repository:\n\n```console\n$ cd myrepo\n$ grip\n * Running on http://localhost:6419/\n```\n\nNow open a browser and visit [http://localhost:6419](http://localhost:6419/).\nOr run with `-b` and Grip will open a new browser tab for you.\n\nYou can also specify a port:\n\n```console\n$ grip 80\n * Running on http://localhost:80/\n```\n\nOr an explicit file:\n\n```console\n$ grip AUTHORS.md\n * Running on http://localhost:6419/\n```\n\nAlternatively, you could just run `grip` and visit [localhost:6419/AUTHORS.md][AUTHORS.md]\nsince grip supports relative URLs.\n\nYou can combine the previous examples. Or specify a hostname instead of a port. Or provide both.\n\n```console\n$ grip AUTHORS.md 80\n * Running on http://localhost:80/\n```\n\n```console\n$ grip CHANGES.md 0.0.0.0\n * Running on http://0.0.0.0:6419/\n```\n\n```console\n$ grip . 0.0.0.0:80\n * Running on http://0.0.0.0:80/\n```\n\nYou can even bypass the server and **export** to a single HTML file, with all the styles and assets inlined:\n\n```console\n$ grip --export\nExporting to README.html\n```\n\nControl the output name with the second argument:\n\n```console\n$ grip README.md --export index.html\nExporting to index.html\n```\n\nIf you're exporting a bunch of files, you can prevent styles from being inlining to save space with `--no-inline`:\n\n```console\n$ grip README.md --export --no-inline introduction.html\nExporting to introduction.html\n```\n\nReading and writing from **stdin** and **stdout** is also supported, allowing you to use Grip with other programs:\n\n```console\n$ cat README.md | grip -\n * Running on http://localhost:6419/\n```\n\n```console\n$ grip AUTHORS.md --export - | bcat\n```\n\n```console\n$ cat README.md | grip --export - | less\n```\n\nThis allows you to quickly test how things look by entering Markdown directly in your terminal:\n\n```console\n$ grip -\nHello **world**!\n^D\n * Running on http://localhost:6419/\n```\n\n*Note: `^D` means `Ctrl+D`, which works on Linux and OS X. On Windows you'll have to use `Ctrl+Z`.*\n\nRendering as user-content like **comments** and **issues** is also supported, with an optional repository context for linking to issues:\n\n```console\n$ grip --user-content --context=joeyespo/grip\n * Running on http://localhost:6419/\n```\n\nFor more details and additional options, see the help:\n\n```console\n$ grip -h\n```\n\n\nAccess\n------\n\nGrip strives to be as close to GitHub as possible. To accomplish this, grip\nuses [GitHub's Markdown API][markdown] so that changes to their rendering\nengine are reflected immediately without requiring you to upgrade grip.\nHowever, because of this you may hit the API's hourly rate limit. If this\nhappens, grip offers a way to access the API using your credentials\nto unlock a much higher rate limit.\n\n```console\n$ grip --user <your-username> --pass <your-password>\n```\n\nOr use a [personal access token][] with an empty scope (note that a token is\n*required* if your GitHub account is set up with two-factor authentication):\n\n```console\n$ grip --pass <token>\n```\n\nYou can persist these options [in your local configuration](#configuration).\nFor security purposes, it's highly recommended that you **use an access token\nover a password**. (You could also keep your password safe by configuring\nGrip to [grab your password from a password manager][keychain-access].)\n\nThere's also a [work-in-progress branch][offline-renderer] to provide\n**offline rendering**. Once this resembles GitHub more precisely, it'll\nbe exposed in the CLI, and will ultimately be used as a seamless fallback\nengine for when the API can't be accessed.\n\nGrip always accesses GitHub over HTTPS,\nso your README and credentials are protected.\n\n\nTips\n----\n\nHere's how others from the community are using Grip.\n\nWant to share your own? [Say hello @joeyespo][twitter] or [submit a pull request](#contributing).\n\n\n#### Create a local mirror of a Github Wiki\n\n```console\n$ git clone https://github.com/YOUR_USERNAME/YOUR_REPOSITORY.wiki.git\n$ cd YOUR_REPOSITORY.wiki\n$ grip\n```\n\n*By [Joshua Gourneau](https://twitter.com/gourneau/status/636329126643658753).*\n\n\n#### Generate HTML documentation from a collection of linked README files\n\n1. Enter the directory:\n\n   ```console\n   $ cd YOUR_DIR\n   $ export GRIPURL=$(pwd)\n   ```\n\n2. Include all assets by setting the `CACHE_DIRECTORY` [config variable](#configuration):\n\n   ```console\n   $ echo \"CACHE_DIRECTORY = '$(pwd)/assets'\" >> ~/.grip/settings.py\n   ```\n\n3. Export all your Markdown files with Grip and replace absolute asset paths with relative paths:\n\n   ```console\n   $ for f in *.md; do grip --export $f --no-inline; done\n   $ for f in *.html; do sed -i '' \"s?$GRIPURL/??g\" $f; done\n   ```\n\nYou can optionally compress the set of HTML files to `docs.tgz` with:\n\n   ```console\n   $ tar -czvf docs.tgz `ls | grep [\\.]html$` assets\n   ```\n\nLooking for a cross platform solution? Here's an equivalent [Python script](https://gist.github.com/mrexmelle/659abc02ae1295d60647).\n\n*By [Matthew R. Tanudjaja](https://github.com/mrexmelle).*\n\n\nConfiguration\n-------------\n\nTo customize Grip, create `~/.grip/settings.py`, then add one or more of the following variables:\n\n- `HOST`: The host to use when not provided as a CLI argument, `localhost` by default\n- `PORT`: The port to use when not provided as a CLI argument, `6419` by default\n- `DEBUG`: Whether to use Flask's debugger when an error happens, `False` by default\n- `DEBUG_GRIP`: Prints extended information when an error happens, `False` by default\n- `API_URL`: Base URL for the github API, for example that of a Github Enterprise instance. `https://api.github.com` by default\n- `CACHE_DIRECTORY`: The directory, relative to `~/.grip`, to place cached assets (this gets run through the following filter: `CACHE_DIRECTORY.format(version=__version__)`), `'cache-{version}'` by default\n- `AUTOREFRESH`: Whether to automatically refresh the Readme content when the file changes, `True` by default\n- `QUIET`: Do not print extended information, `False` by default\n- `STYLE_URLS`: Additional URLs that will be added to the rendered page, `[]` by default\n- `USERNAME`: The username to use when not provided as a CLI argument, `None` by default\n- `PASSWORD`: The password or [personal access token][] to use when not provided as a CLI argument (*Please don't save your passwords here.* Instead, use an access token or drop in this code [grab your password from a password manager][keychain-access]), `None` by default\n\nNote that this is a Python file. If you see `'X' is not defined` errors, you\nmay have overlooked some quotes. For example:\n\n```py\nUSERNAME = 'your-username'\nPASSWORD = 'your-personal-access-token'\n```\n\n\n#### Environment variables\n\n- `GRIPHOME`: Specify an alternative `settings.py` location, `~/.grip` by default\n- `GRIPURL`: The URL of the Grip server, `/__/grip` by default\n\n#### Advanced\n\nThis file is a normal Python script, so you can add more advanced configuration.\n\nFor example, to read a setting from the environment and provide a default value\nwhen it's not set:\n\n```python\nPORT = os.environ.get('GRIP_PORT', 8080)\n```\n\n\nAPI\n---\n\nYou can access the API directly with Python, using it in your own projects:\n\n```python\nfrom grip import serve\n\nserve(port=8080)\n * Running on http://localhost:8080/\n```\n\nRun main directly:\n\n```python\nfrom grip import main\n\nmain(argv=['-b', '8080'])\n * Running on http://localhost:8080/\n```\n\nOr access the underlying Flask application for even more flexibility:\n\n```python\nfrom grip import create_app\n\ngrip_app = create_app(user_content=True)\n# Use in your own app\n```\n\n\n### Documentation\n\n#### serve\n\nRuns a local server and renders the Readme file located\nat `path` when visited in the browser.\n\n```python\nserve(path=None, host=None, port=None, user_content=False, context=None, username=None, password=None, render_offline=False, render_wide=False, render_inline=False, api_url=None, title=None, autorefresh=True, browser=False, grip_class=None)\n```\n\n- `path`: The filename to render, or the directory containing your Readme file, defaulting to the current working directory\n- `host`: The host to listen on, defaulting to the HOST configuration variable\n- `port`: The port to listen on, defaulting to the PORT configuration variable\n- `user_content`: Whether to render a document as [user-content][] like user comments or issues\n- `context`: The project context to use when `user_content` is true, which\n             takes the form of `username/project`\n- `username`: The user to authenticate with GitHub to extend the API limit\n- `password`: The password to authenticate with GitHub to extend the API limit\n- `render_offline`: Whether to render locally using [Python-Markdown][] (Note: this is a work in progress)\n- `render_wide`: Whether to render a wide page, `False` by default (this has no effect when used with `user_content`)\n- `render_inline`: Whether to inline the styles within the HTML file\n- `api_url`: A different base URL for the github API, for example that of a Github Enterprise instance. The default is the public API https://api.github.com.\n- `title`: The page title, derived from `path` by default\n- `autorefresh`: Automatically update the rendered content when the Readme file changes, `True` by default\n- `browser`: Open a tab in the browser after the server starts., `False` by default\n- `grip_class`: Use a custom [Grip class](#class-gripflask)\n\n\n#### export\n\nWrites the specified Readme file to an HTML file with styles and assets inlined.\n\n```python\nexport(path=None, user_content=False, context=None, username=None, password=None, render_offline=False, render_wide=False, render_inline=True, out_filename=None, api_url=None, title=None, quiet=None, theme='light', grip_class=None)\n```\n\n- `path`: The filename to render, or the directory containing your Readme file, defaulting to the current working directory\n- `user_content`: Whether to render a document as [user-content][] like user comments or issues\n- `context`: The project context to use when `user_content` is true, which\n             takes the form of `username/project`\n- `username`: The user to authenticate with GitHub to extend the API limit\n- `password`: The password to authenticate with GitHub to extend the API limit\n- `render_offline`: Whether to render locally using [Python-Markdown][] (Note: this is a work in progress)\n- `render_wide`: Whether to render a wide page, `False` by default (this has no effect when used with `user_content`)\n- `render_inline`: Whether to inline the styles within the HTML file (Note: unlike the other API functions, this defaults to `True`)\n- `out_filename`: The filename to write to, `<in_filename>.html` by default\n- `api_url`: A different base URL for the github API, for example that of a Github Enterprise instance. The default is the public API https://api.github.com.\n- `title`: The page title, derived from `path` by default\n- `quiet`: Do not print to the terminal\n- `theme`: Theme to view markdown file (light mode or dark mode). Valid options (\"light\", \"dark\"). Default: \"light\".\n- `grip_class`: Use a custom [Grip class](#class-gripflask)\n\n\n#### create_app\n\nCreates a Flask application you can use to render and serve the Readme files.\nThis is the same app used by `serve` and `export` and initializes the cache,\nusing the cached styles when available.\n\n```python\ncreate_app(path=None, user_content=False, context=None, username=None, password=None, render_offline=False, render_wide=False, render_inline=False, api_url=None, title=None, text=None, grip_class=None)\n```\n\n- `path`: The filename to render, or the directory containing your Readme file, defaulting to the current working directory\n- `user_content`: Whether to render a document as [user-content][] like user comments or issues\n- `context`: The project context to use when `user_content` is true, which\n             takes the form of `username/project`\n- `username`: The user to authenticate with GitHub to extend the API limit\n- `password`: The password to authenticate with GitHub to extend the API limit\n- `render_offline`: Whether to render locally using [Python-Markdown][] (Note: this is a work in progress)\n- `render_wide`: Whether to render a wide page, `False` by default (this has no effect when used with `user_content`)\n- `render_inline`: Whether to inline the styles within the HTML file\n- `api_url`: A different base URL for the github API, for example that of a Github Enterprise instance. The default is the public API https://api.github.com.\n- `title`: The page title, derived from `path` by default\n- `text`: A string or stream of Markdown text to render instead of being loaded from `path` (Note: `path` can be used to set the page title)\n- `grip_class`: Use a custom [Grip class](#class-gripflask)\n\n\n#### render_app\n\nRenders the application created by `create_app` and returns the HTML that would\nnormally appear when visiting that route.\n\n```python\nrender_app(app, route='/')\n```\n\n- `app`: The Flask application to render\n- `route`: The route to render, '/' by default\n\n\n#### render_content\n\nRenders the specified markdown text without caching.\n\n```python\nrender_content(text, user_content=False, context=None, username=None, password=None, render_offline=False, api_url=None, title=None)\n```\n\n- `text`: The Markdown text to render\n- `user_content`: Whether to render a document as [user-content][] like user comments or issues\n- `context`: The project context to use when `user_content` is true, which\n             takes the form of `username/project`\n- `username`: The user to authenticate with GitHub to extend the API limit\n- `password`: The password to authenticate with GitHub to extend the API limit\n- `render_offline`: Whether to render locally using [Python-Markdown][] (Note: this is a work in progress)\n- `api_url`: A different base URL for the github API, for example that of a Github Enterprise instance. This is required when not using the offline renderer.\n- `title`: The page title, derived from `path` by default\n\n\n#### render_page\n\nRenders the markdown from the specified path or text, without caching,\nand returns an HTML page that resembles the GitHub Readme view.\n\n```python\nrender_page(path=None, user_content=False, context=None, username=None, password=None, render_offline=False, render_wide=False, render_inline=False, api_url=None, title=None, text=None, quiet=None, theme='light', grip_class=None)\n```\n\n- `path`: The path to use for the page title, rendering `'README.md'` if None\n- `user_content`: Whether to render a document as [user-content][] like user comments or issues\n- `context`: The project context to use when `user_content` is true, which\n             takes the form of `username/project`\n- `username`: The user to authenticate with GitHub to extend the API limit\n- `password`: The password to authenticate with GitHub to extend the API limit\n- `render_offline`: Whether to render offline using [Python-Markdown][] (Note: this is a work in progress)\n- `render_wide`: Whether to render a wide page, `False` by default (this has no effect when used with `user_content`)\n- `render_inline`: Whether to inline the styles within the HTML file\n- `api_url`: A different base URL for the github API, for example that of a Github Enterprise instance. The default is the public API https://api.github.com.\n- `title`: The page title, derived from `path` by default\n- `text`: A string or stream of Markdown text to render instead of being loaded from `path` (Note: `path` can be used to set the page title)\n- `quiet`: Do not print to the terminal\n- `theme`: Theme to view markdown file (light mode or dark mode). Valid options (\"light\", \"dark\"). Default: \"light\".\n- `grip_class`: Use a custom [Grip class](#class-gripflask)\n\n\n#### clear_cache\n\nClears the cached styles and assets.\n\n```python\nclear_cache(grip_class=None)\n```\n\n#### main\n\nRuns Grip with the specified arguments.\n\n```python\nmain(argv=None, force_utf8=True)\n```\n\n- `argv`: The arguments to run with, `sys.argv[1:]` by default\n- `force_utf8`: Sets the default encoding to `utf-8` in the current Python instance. This has no effect on Python 3 since Unicode is handled by default\n\n\n### Classes\n\n#### class Grip(Flask)\n\nA Flask application that can serve a file or directory containing a README.\n\n```python\nGrip(source=None, auth=None, renderer=None, assets=None, render_wide=None, render_inline=None, title=None, autorefresh=None, quiet=None, theme='light', grip_url=None, static_url_path=None, instance_path=None, **kwargs)\n```\n\n##### default_renderer\n\nReturns the default renderer using the current config. This is only used if\nrenderer is set to None in the constructor.\n\n```python\nGrip.default_renderer()\n```\n\n##### default_asset_manager\n\nReturns the default asset manager using the current config. This is only used\nif asset_manager is set to None in the constructor.\n\n```python\nGrip.default_asset_manager()\n```\n\n##### add_content_types\n\nAdds the application/x-font-woff and application/octet-stream content types if\nthey are missing. Override to add additional content types on initialization.\n\n```python\nGrip.add_content_types()\n```\n\n##### clear_cache\n\nClears the downloaded assets.\n\n```python\nGrip.clear_cache()\n```\n\n##### render\n\nRenders the application and returns the HTML unicode that would normally appear\nwhen visiting in the browser.\n\n```python\nGrip.render(route=None)\n```\n\n- `route`: The route to render, `/` by default\n\n##### run\n\nStarts a server to render the README. This calls [Flask.run][] internally.\n\n```python\nGrip.run(host=None, port=None, debug=None, use_reloader=None, open_browser=False)\n```\n\n- `host`: The hostname to listen on. Set this to `'0.0.0.0'` to have the server\n          available externally as well, `'localhost'` by default\n- `port`: The port of the webserver. Defaults to `6419`\n- `debug`: If given, enable or disable debug mode. See [Flask.debug][].\n- `use_reloader`: Should the server automatically restart the python process\n                  if modules were changed? `False` by default unless the\n                  `DEBUG_GRIP` setting is specified.\n- `open_browser`: Opens the browser to the address when the server starts\n\n\n#### class AlreadyRunningError(RuntimeError)\n\nRaised when `Grip.run` is called while the server is already running.\n\n```python\nAlreadyRunningError()\n```\n\n\n#### class ReadmeNotFoundError(NotFoundError or IOError)\n\nRaised when the specified Readme could not be found.\n\n```python\nReadmeNotFoundError(path=None, message=None)\n```\n\n\n#### class ReadmeAssetManager(object)\n\nManages the style and font assets rendered with Readme pages. This is an\nabstract base class.\n\n```python\nReadmeAssetManager(cache_path, style_urls=None)\n```\n\n\n#### class GitHubAssetManager(ReadmeAssetManager)\n\nManages the style and font assets rendered with Readme pages. Set cache_path to\nNone to disable caching.\n\n\n#### class ReadmeReader(object)\n\nReads Readme content from a URL subpath. This is an abstract base class.\n\n```python\nReadmeReader()\n```\n\n\n#### class DirectoryReader(ReadmeReader)\n\nReads Readme files from URL subpaths.\n\n```python\nDirectoryReader(path=None, silent=False)\n```\n\n\n#### class TextReader(ReadmeReader)\n\nReads Readme content from the provided unicode string.\n\n```python\nTextReader(text, display_filename=None)\n```\n\n\n#### class StdinReader(TextReader)\n\nReads Readme text from STDIN.\n\n```python\nStdinReader(display_filename=None)\n```\n\n\n#### class ReadmeRenderer(object)\n\nRenders the Readme. This is an abstract base class.\n\n```python\nReadmeRenderer(user_content=None, context=None)\n```\n\n\n#### class GitHubRenderer(ReadmeRenderer)\n\nRenders the specified Readme using the GitHub Markdown API.\n\n```python\nGitHubRenderer(user_content=None, context=None, api_url=None, raw=None)\n```\n\n\n#### class OfflineRenderer(ReadmeRenderer)\n\nRenders the specified Readme locally using pure Python. Note: This is currently\nan incomplete feature.\n\n```python\nOfflineRenderer(user_content=None, context=None)\n```\n\n\n### Constants\n\n\n#### SUPPORTED_TITLES\n\nThe common Markdown file titles on GitHub.\n\n```python\nSUPPORTED_TITLES = ['README', 'Home']\n```\n\n- `filename`: The UTF-8 file to read.\n\n\n#### SUPPORTED_EXTENSIONS\n\nThe supported extensions, as defined by [GitHub][markdown].\n\n```python\nSUPPORTED_EXTENSIONS = ['.md', '.markdown']\n```\n\n\n#### DEFAULT_FILENAMES\n\nThis constant contains the names Grip looks for when no file is provided.\n\n```python\nDEFAULT_FILENAMES = [title + ext\n                     for title in SUPPORTED_TITLES\n                     for ext in SUPPORTED_EXTENSIONS]\n```\n\n\n#### DEFAULT_FILENAME\n\nThis constant contains the default Readme filename, namely:\n\n```python\nDEFAULT_FILENAME = DEFAULT_FILENAMES[0]  # README.md\n```\n\n\n#### DEFAULT_GRIPHOME\n\nThis constant points to the default value if the `GRIPHOME`\n[environment variable](#environment-variables) is not specified.\n\n```python\nDEFAULT_GRIPHOME = '~/.grip'\n```\n\n\n#### DEFAULT_GRIPURL\n\nThe default URL of the Grip server and all its assets:\n\n```python\nDEFAULT_GRIPURL = '/__/grip'\n```\n\n\n#### DEFAULT_API_URL\n\nThe default app_url value:\n\n```python\nDEFAULT_API_URL = 'https://api.github.com'\n```\n\n\nTesting\n-------\n\nInstall the package and test requirements:\n\n```console\n$ pip install -e .[tests]\n```\n\nRun tests with [pytest][]:\n\n```console\n$ pytest\n```\n\nOr to re-run tests as you make changes, use [pytest-watch][]:\n\n```console\n$ ptw\n```\n\n\n#### External assumption tests\n\nIf you're experiencing a problem with Grip, it's likely that an assumption made\nabout the GitHub API has been broken. To verify this, run:\n\n```console\n$ pytest -m assumption\n```\n\nSince the external assumptions rely on an internet connection, you may want to skip\nthem when developing locally. Tighten the cycle even further by stopping on the\nfirst failure with `-x`:\n\n```console\n$ pytest -xm \"not assumption\"\n```\n\nOr with [pytest-watch][]:\n\n```console\n$ ptw -- -xm \"not assumption\"\n```\n\n\nContributing\n------------\n\n1. Check the open issues or open a new issue to start a discussion around\n   your feature idea or the bug you found\n2. Fork the repository and make your changes\n3. Open a new pull request\n\nIf your PR has been waiting a while, feel free to [ping me on Twitter][twitter].\n\nUse this software often? <a href=\"https://saythanks.io/to/joeyespo\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg\" align=\"center\" alt=\"Say Thanks!\"></a>\n:smiley:\n\n\n[pypi]: http://pypi.python.org/pypi/grip/\n[markdown]: http://developer.github.com/v3/markdown\n[rdd]: http://tom.preston-werner.com/2010/08/23/readme-driven-development.html\n[authors.md]: AUTHORS.md\n[offline-renderer]: http://github.com/joeyespo/grip/tree/offline-renderer\n[personal access token]: https://github.com/settings/tokens/new?scopes=\n[keychain-access]: https://gist.github.com/klmr/3840aa3c12f947e4064c\n[task-lists]: https://github.com/blog/1825-task-lists-in-all-markdown-documents\n[user-content]: http://github.github.com/github-flavored-markdown\n[python-markdown]: http://github.com/waylan/Python-Markdown\n[flask.run]: http://flask.pocoo.org/docs/0.10/api/#flask.Flask.run\n[flask.debug]: http://flask.pocoo.org/docs/0.10/api/#flask.Flask.debug\n[pytest]: http://pytest.org/\n[pytest-watch]: https://github.com/joeyespo/pytest-watch\n[twitter]: http://twitter.com/joeyespo\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "README.md", "language": "markdown", "complexity": "complex", "star_count": 6671, "quality_score": 0.85, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 786, "word_count": 3238}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create project README with installation, usage, and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\\\nGrip\n----\n\nRender local readme files before sending off to GitHub.\n\n:copyright: (c) 2014-2022 by Joe Esposito.\n:license: MIT, see LICENSE for more details.\n\"\"\"\n\n__version__ = '4.6.2'  # noqa\n\nimport sys\n\n# Patch for Flask 11.0+ on Python 3 (pypy3)\nif not hasattr(sys, 'exc_clear'):  # noqa\n    sys.exc_clear = lambda: None\n\nfrom .api import (\n    clear_cache, create_app, export, render_content, render_page, serve)\nfrom .app import Grip\nfrom .assets import GitHubAssetManager, ReadmeAssetManager\nfrom .command import main\nfrom .constants import (\n    DEFAULT_API_URL, DEFAULT_FILENAMES, DEFAULT_FILENAME, DEFAULT_GRIPHOME,\n    DEFAULT_GRIPURL, STYLE_ASSET_URLS_INLINE_FORMAT, STYLE_ASSET_URLS_RE,\n    STYLE_ASSET_URLS_SUB_FORMAT, STYLE_URLS_RES, STYLE_URLS_SOURCE,\n    SUPPORTED_EXTENSIONS, SUPPORTED_TITLES)\nfrom .exceptions import AlreadyRunningError, ReadmeNotFoundError\nfrom .readers import ReadmeReader, DirectoryReader, StdinReader, TextReader\nfrom .renderers import ReadmeRenderer, GitHubRenderer, OfflineRenderer\n\n\n__all__ = [\n    '__version__',\n\n    'DEFAULT_API_URL', 'DEFAULT_FILENAMES', 'DEFAULT_FILENAME',\n    'DEFAULT_GRIPHOME', 'DEFAULT_GRIPURL', 'STYLE_ASSET_URLS_INLINE_FORMAT',\n    'STYLE_ASSET_URLS_RE', 'STYLE_ASSET_URLS_SUB_FORMAT', 'STYLE_URLS_RES',\n    'STYLE_URLS_SOURCE', 'SUPPORTED_EXTENSIONS', 'SUPPORTED_TITLES',\n\n    'AlreadyRunningError', 'DirectoryReader', 'GitHubAssetManager',\n    'GitHubRenderer', 'Grip', 'OfflineRenderer', 'ReadmeNotFoundError',\n    'ReadmeAssetManager', 'ReadmeReader', 'ReadmeRenderer', 'StdinReader',\n    'TextReader',\n\n    'clear_cache', 'create_app', 'export', 'main', 'render_content',\n    'render_page', 'serve',\n]\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "grip/__init__.py", "language": "python", "complexity": "simple", "star_count": 6671, "quality_score": 0.7899999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 50, "word_count": 145}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\\\nGrip\n----\n\nRender local readme files before sending off to GitHub.\n\n:copyright: (c) 2014-2022 by Joe Esposito.\n:license: MIT, see LICENSE for more details.\n\"\"\"\n\nimport os\nimport sys\n\n\nif __name__ == '__main__':\n    sys.path.insert(1, os.path.dirname(os.path.dirname(\n        os.path.abspath(__file__))))\n\n    from grip.command import main\n    main()\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "grip/__main__.py", "language": "python", "complexity": "simple", "star_count": 6671, "quality_score": 0.7899999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 21, "word_count": 42}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create project README with installation, usage, and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import print_function, unicode_literals\n\nimport io\nimport os\nimport sys\nimport errno\n\nfrom .app import Grip\nfrom .readers import DirectoryReader, StdinReader, TextReader\nfrom .renderers import GitHubRenderer, OfflineRenderer\n\n\ndef create_app(path=None, user_content=False, context=None, username=None,\n               password=None, render_offline=False, render_wide=False,\n               render_inline=False, api_url=None, title=None, text=None,\n               autorefresh=None, quiet=None, theme='light', grip_class=None):\n    \"\"\"\n    Creates a Grip application with the specified overrides.\n    \"\"\"\n    # Customize the app\n    if grip_class is None:\n        grip_class = Grip\n\n    # Customize the reader\n    if text is not None:\n        display_filename = DirectoryReader(path, True).filename_for(None)\n        source = TextReader(text, display_filename)\n    elif path == '-':\n        source = StdinReader()\n    else:\n        source = DirectoryReader(path)\n\n    # Customize the renderer\n    if render_offline:\n        renderer = OfflineRenderer(user_content, context)\n    elif user_content or context or api_url:\n        renderer = GitHubRenderer(user_content, context, api_url)\n    else:\n        renderer = None\n\n    # Optional basic auth\n    auth = (username, password) if username or password else None\n\n    # Create the customized app with default asset manager\n    return grip_class(source, auth, renderer, None, render_wide,\n                      render_inline, title, autorefresh, quiet, theme)\n\n\ndef serve(path=None, host=None, port=None, user_content=False, context=None,\n          username=None, password=None, render_offline=False,\n          render_wide=False, render_inline=False, api_url=None, title=None,\n          autorefresh=True, browser=False, quiet=None, theme='light', grip_class=None):\n    \"\"\"\n    Starts a server to render the specified file or directory containing\n    a README.\n    \"\"\"\n    app = create_app(path, user_content, context, username, password,\n                     render_offline, render_wide, render_inline, api_url,\n                     title, None, autorefresh, quiet, theme, grip_class)\n    app.run(host, port, open_browser=browser)\n\n\ndef clear_cache(grip_class=None):\n    \"\"\"\n    Clears the cached styles and assets.\n    \"\"\"\n    if grip_class is None:\n        grip_class = Grip\n    grip_class(StdinReader()).clear_cache()\n\n\ndef render_page(path=None, user_content=False, context=None,\n                username=None, password=None,\n                render_offline=False, render_wide=False, render_inline=False,\n                api_url=None, title=None, text=None, quiet=None, theme='light',\n                grip_class=None):\n    \"\"\"\n    Renders the specified markup text to an HTML page and returns it.\n    \"\"\"\n    return create_app(path, user_content, context, username, password,\n                      render_offline, render_wide, render_inline, api_url,\n                      title, text, False, quiet, theme, grip_class).render()\n\n\ndef render_content(text, user_content=False, context=None, username=None,\n                   password=None, render_offline=False, api_url=None):\n    \"\"\"\n    Renders the specified markup and returns the result.\n    \"\"\"\n    renderer = (GitHubRenderer(user_content, context, api_url)\n                if not render_offline else\n                OfflineRenderer(user_content, context))\n    auth = (username, password) if username or password else None\n    return renderer.render(text, auth)\n\n\ndef export(path=None, user_content=False, context=None,\n           username=None, password=None, render_offline=False,\n           render_wide=False, render_inline=True, out_filename=None,\n           api_url=None, title=None, quiet=False, theme='light', grip_class=None):\n    \"\"\"\n    Exports the rendered HTML to a file.\n    \"\"\"\n    export_to_stdout = out_filename == '-'\n    if out_filename is None:\n        if path == '-':\n            export_to_stdout = True\n        else:\n            filetitle, _ = os.path.splitext(\n                os.path.relpath(DirectoryReader(path).root_filename))\n            out_filename = '{0}.html'.format(filetitle)\n\n    if not export_to_stdout and not quiet:\n        print('Exporting to', out_filename, file=sys.stderr)\n\n    page = render_page(path, user_content, context, username, password,\n                       render_offline, render_wide, render_inline, api_url,\n                       title, None, quiet, theme, grip_class)\n\n    if export_to_stdout:\n        try:\n            print(page)\n        except IOError as ex:\n            if ex.errno != 0 and ex.errno != errno.EPIPE:\n                raise\n    else:\n        with io.open(out_filename, 'w', encoding='utf-8') as f:\n            f.write(page)\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "grip/api.py", "language": "python", "complexity": "medium", "star_count": 6671, "quality_score": 0.8799999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 129, "word_count": 410}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create project README with installation, usage, and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import print_function, unicode_literals\n\nimport base64\nimport json\nimport mimetypes\nimport os\nimport posixpath\nimport re\nimport socket\nimport sys\nimport threading\nimport time\nimport errno\nfrom traceback import format_exc\ntry:\n    from urlparse import urlparse\nexcept ImportError:\n    from urllib.parse import urlparse\ntry:\n    str_type = basestring\nexcept NameError:\n    str_type = str\n\nimport requests\nfrom flask import (\n    Flask, Response, abort, redirect, render_template, request,\n    send_from_directory, url_for)\n\nfrom . import __version__\nfrom .assets import GitHubAssetManager, ReadmeAssetManager\nfrom .browser import start_browser_when_ready\nfrom .constants import (\n    DEFAULT_GRIPHOME, DEFAULT_GRIPURL, STYLE_ASSET_URLS_INLINE_FORMAT)\nfrom .exceptions import AlreadyRunningError, ReadmeNotFoundError\nfrom .readers import DirectoryReader\nfrom .renderers import GitHubRenderer, ReadmeRenderer\n\n\nclass Grip(Flask):\n    \"\"\"\n    A Flask application that can serve the specified file or directory\n    containing a README.\n    \"\"\"\n    def __init__(self, source=None, auth=None, renderer=None,\n                 assets=None, render_wide=None, render_inline=None, title=None,\n                 autorefresh=None, quiet=None, theme='light', grip_url=None,\n                 static_url_path=None, instance_path=None, **kwargs):\n        # Defaults\n        if source is None or isinstance(source, str_type):\n            source = DirectoryReader(source)\n        if render_wide is None:\n            render_wide = False\n        if render_inline is None:\n            render_inline = False\n\n        # Defaults from ENV\n        if grip_url is None:\n            grip_url = os.environ.get('GRIPURL')\n            if grip_url is None:\n                grip_url = DEFAULT_GRIPURL\n        grip_url = grip_url.rstrip('/')\n        if static_url_path is None:\n            static_url_path = posixpath.join(grip_url, 'static')\n        if instance_path is None:\n            instance_path = os.environ.get('GRIPHOME')\n            if instance_path is None:\n                instance_path = DEFAULT_GRIPHOME\n        instance_path = os.path.abspath(os.path.expanduser(instance_path))\n\n        # Flask application\n        super(Grip, self).__init__(\n            __name__, static_url_path=static_url_path,\n            instance_path=instance_path, **kwargs)\n        self.config.from_object('grip.settings')\n\n        try:\n            self.config.from_pyfile('settings_local.py', silent=True)\n            self.config.from_pyfile(\n                os.path.join(instance_path, 'settings.py'), silent=True)\n        except IOError as ex:\n            # Flask workaround for when ~/.grip exists but is not a directory\n            if ex.errno != errno.ENOTDIR:\n                raise\n\n        # Defaults from settings\n        if autorefresh is None:\n            autorefresh = self.config['AUTOREFRESH']\n        if quiet is None:\n            quiet = self.config['QUIET']\n        if auth is None:\n            username = self.config['USERNAME']\n            password = self.config['PASSWORD']\n            if username or password:\n                auth = (username or '', password or '')\n\n        # Thread-safe event to signal to the polling threads to exit\n        self._run_mutex = threading.Lock()\n        self._shutdown_event = None\n\n        # Parameterized attributes\n        self.auth = auth\n        self.autorefresh = autorefresh\n        self.reader = source\n        self.renderer = renderer\n        self.assets = assets\n        self.render_wide = render_wide\n        self.render_inline = render_inline\n        self.title = title\n        self.quiet = quiet\n        if self.quiet:\n            import logging\n            log = logging.getLogger('werkzeug')\n            log.setLevel(logging.ERROR)\n        self.theme = theme\n\n        # Overridable attributes\n        if self.renderer is None:\n            renderer = self.default_renderer()\n            if not isinstance(renderer, ReadmeRenderer):\n                raise TypeError(\n                    'Expected Grip.default_renderer to return a '\n                    'ReadmeRenderer instance, got {0}.'.format(type(renderer)))\n            self.renderer = renderer\n        if self.assets is None:\n            assets = self.default_asset_manager()\n            if not isinstance(assets, ReadmeAssetManager):\n                raise TypeError(\n                    'Expected Grip.default_asset_manager to return an '\n                    'ReadmeAssetManager instance, got {0}.'.format(\n                        type(assets)))\n            self.assets = assets\n\n        # Add missing content types\n        self.add_content_types()\n\n        # Construct routes\n        asset_route = posixpath.join(grip_url, 'asset', '')\n        asset_subpath = posixpath.join(asset_route, '<path:subpath>')\n        refresh_route = posixpath.join(grip_url, 'refresh', '')\n        refresh_subpath = posixpath.join(refresh_route, '<path:subpath>')\n        rate_limit_route = posixpath.join(grip_url, 'rate-limit-preview')\n\n        # Initialize views\n        self._styles_retrieved = False\n        self.before_request(self._retrieve_styles)\n        self.add_url_rule(asset_route, 'asset', self._render_asset)\n        self.add_url_rule(asset_subpath, 'asset', self._render_asset)\n        self.add_url_rule('/', 'render', self._render_page)\n        self.add_url_rule('/<path:subpath>', 'render', self._render_page)\n        self.add_url_rule(refresh_route, 'refresh', self._render_refresh)\n        self.add_url_rule(refresh_subpath, 'refresh', self._render_refresh)\n        self.add_url_rule(rate_limit_route, 'rate_limit',\n                          self._render_rate_limit_page)\n        self.errorhandler(403)(self._render_rate_limit_page)\n\n    def _redirect_to_subpath(self, subpath=None):\n        \"\"\"\n        Redirects to the specified subpath, which is the relative path\n        part of the root location (i.e. the current working directory\n        or the path part of a URL excluding the initial '/').\n        \"\"\"\n        route = posixpath.normpath('/' + (subpath or '').lstrip('/'))\n        return redirect(route)\n\n    def _render_asset(self, subpath):\n        \"\"\"\n        Renders the specified cache file.\n        \"\"\"\n        return send_from_directory(\n            self.assets.cache_path, self.assets.cache_filename(subpath))\n\n    def _render_page(self, subpath=None):\n        # Normalize the subpath\n        normalized = self.reader.normalize_subpath(subpath)\n        if normalized != subpath:\n            return self._redirect_to_subpath(normalized)\n\n        # Read the Readme text or asset\n        try:\n            text = self.reader.read(subpath)\n        except ReadmeNotFoundError:\n            abort(404)\n\n        # Return binary asset\n        if self.reader.is_binary(subpath):\n            mimetype = self.reader.mimetype_for(subpath)\n            return Response(text, mimetype=mimetype)\n\n        # Render the Readme content\n        try:\n            content = self.renderer.render(text, self.auth)\n        except requests.HTTPError as ex:\n            if ex.response.status_code == 403:\n                abort(403)\n            raise\n        except requests.exceptions.SSLError as ex:\n            if 'TLSV1_ALERT_PROTOCOL_VERSION' in str(ex):\n                print('Error: GitHub has turned off TLS1.0 support. '\n                      'Please upgrade your version of Python or Homebrew '\n                      'to use a later version of openssl. '\n                      'For more information, see '\n                      'https://github.com/joeyespo/grip/issues/262')\n                abort(500)\n            raise\n\n        # Inline favicon asset\n        favicon = None\n        if self.render_inline:\n            favicon_url = url_for('static', filename='favicon.ico')\n            favicon = self._to_data_url(favicon_url, 'image/x-icon')\n\n        autorefresh_url = (url_for('refresh', subpath=subpath)\n                           if self.autorefresh\n                           else None)\n\n        if self.theme == 'dark':\n            data_color_mode = 'dark'\n            data_light_theme = 'light'\n            data_dark_theme = 'dark'\n        else:\n            data_color_mode = 'light'\n            data_light_theme = 'light'\n            data_dark_theme = 'dark'\n\n        return render_template(\n            'index.html', filename=self.reader.filename_for(subpath),\n            title=self.title, content=content, favicon=favicon,\n            user_content=self.renderer.user_content,\n            wide_style=self.render_wide, style_urls=self.assets.style_urls,\n            styles=self.assets.styles, autorefresh_url=autorefresh_url,\n            data_color_mode=data_color_mode, data_light_theme=data_light_theme,\n            data_dark_theme=data_dark_theme)\n\n    def _render_refresh(self, subpath=None):\n        if not self.autorefresh:\n            abort(404)\n\n        # Normalize the subpath\n        normalized = self.reader.normalize_subpath(subpath)\n        if normalized != subpath:\n            return self._redirect_to_subpath(normalized)\n\n        # Get the full filename for display\n        filename = self.reader.filename_for(subpath)\n\n        # Check whether app is running\n        shutdown_event = self._shutdown_event\n        if not shutdown_event or shutdown_event.is_set():\n            return ''\n\n        def gen():\n            last_updated = self.reader.last_updated(subpath)\n            try:\n                while not shutdown_event.is_set():\n                    time.sleep(0.3)\n\n                    # Check for update\n                    updated = self.reader.last_updated(subpath)\n                    if updated == last_updated:\n                        continue\n                    last_updated = updated\n                    # Notify user that a refresh is in progress\n                    if not self.quiet:\n                        print(' * Change detected in {0}, refreshing'\n                              .format(filename))\n                    yield 'data: {0}\\r\\n\\r\\n'.format(\n                        json.dumps({'updating': True}))\n                    # Binary assets not supported\n                    if self.reader.is_binary(subpath):\n                        return\n                    # Read the Readme text\n                    try:\n                        text = self.reader.read(subpath)\n                    except ReadmeNotFoundError:\n                        return\n                    # Render the Readme content\n                    try:\n                        content = self.renderer.render(text, self.auth)\n                    except requests.HTTPError as ex:\n                        if ex.response.status_code == 403:\n                            abort(403)\n                        raise\n                    # Return the Readme content\n                    yield 'data: {0}\\r\\n\\r\\n'.format(\n                        json.dumps({'content': content}))\n            except GeneratorExit:\n                pass\n\n        return Response(gen(), mimetype='text/event-stream')\n\n    def _render_rate_limit_page(self, exception=None):\n        \"\"\"\n        Renders the rate limit page.\n        \"\"\"\n        auth = request.args.get('auth')\n        is_auth = auth == '1' if auth else bool(self.auth)\n        return render_template('limit.html', is_authenticated=is_auth), 403\n\n    def _download(self, url, binary=False):\n        if urlparse(url).netloc:\n            r = requests.get(url)\n            return r.content if binary else r.text\n\n        with self.test_client() as c:\n            r = c.get(url)\n            charset = r.mimetype_params.get('charset', 'utf-8')\n            data = c.get(url).data\n            return data if binary else data.decode(charset)\n\n    def _to_data_url(self, url, content_type):\n        asset = self._download(url, binary=True)\n        asset64_bytes = base64.b64encode(asset)\n        asset64_string = asset64_bytes.decode('ascii')\n        return 'data:{0};base64,{1}'.format(content_type, asset64_string)\n\n    def _match_asset(self, match):\n        url = match.group(1)\n        ext = os.path.splitext(url)[1][1:]\n        return 'url({0})'.format(\n            self._to_data_url(url, 'font/' + ext))\n\n    def _get_styles(self, style_urls, asset_url_path):\n        \"\"\"\n        Gets the content of the given list of style URLs and\n        inlines assets.\n        \"\"\"\n        styles = []\n        for style_url in style_urls:\n            urls_inline = STYLE_ASSET_URLS_INLINE_FORMAT.format(\n                asset_url_path.rstrip('/'))\n            asset_content = self._download(style_url)\n            content = re.sub(urls_inline, self._match_asset, asset_content)\n            styles.append(content)\n\n        return styles\n\n    def _inline_styles(self):\n        \"\"\"\n        Downloads the assets from the style URL list, clears it, and adds\n        each style with its embedded asset to the literal style list.\n        \"\"\"\n        styles = self._get_styles(self.assets.style_urls, url_for('asset'))\n        self.assets.styles.extend(styles)\n        self.assets.style_urls[:] = []\n\n    def _retrieve_styles(self):\n        \"\"\"\n        Retrieves the style URLs from the source and caches them. This\n        is called before the first request is dispatched.\n        \"\"\"\n        if self._styles_retrieved:\n            return\n        self._styles_retrieved = True\n\n        try:\n            self.assets.retrieve_styles(url_for('asset'))\n        except Exception as ex:\n            if self.debug:\n                print(format_exc(), file=sys.stderr)\n            else:\n                print(' * Error: could not retrieve styles:', ex,\n                      file=sys.stderr)\n        if self.render_inline:\n            self._inline_styles()\n\n    def default_renderer(self):\n        \"\"\"\n        Returns the default renderer using the current config.\n\n        This is only used if renderer is set to None in the constructor.\n        \"\"\"\n        return GitHubRenderer(api_url=self.config['API_URL'])\n\n    def default_asset_manager(self):\n        \"\"\"\n        Returns the default asset manager using the current config.\n\n        This is only used if asset_manager is set to None in the constructor.\n        \"\"\"\n        cache_path = None\n        cache_directory = self.config['CACHE_DIRECTORY']\n        if cache_directory:\n            cache_directory = cache_directory.format(version=__version__)\n            cache_path = os.path.join(self.instance_path, cache_directory)\n        return GitHubAssetManager(\n            cache_path, self.config['STYLE_URLS'], self.quiet)\n\n    def add_content_types(self):\n        \"\"\"\n        Adds the application/x-font-woff and application/octet-stream\n        content types if they are missing.\n\n        Override to add additional content types on initialization.\n        \"\"\"\n        mimetypes.add_type('application/x-font-woff', '.woff')\n        mimetypes.add_type('application/octet-stream', '.ttf')\n\n    def clear_cache(self):\n        self.assets.clear()\n        if not self.quiet:\n            print('Cache cleared.')\n\n    def render(self, route=None):\n        \"\"\"\n        Renders the application and returns the HTML unicode that would\n        normally appear when visiting in the browser.\n        \"\"\"\n        if route is None:\n            route = '/'\n        with self.test_client() as c:\n            response = c.get(route, follow_redirects=True)\n            encoding = getattr(response, 'charset', 'utf-8')\n            return response.data.decode(encoding)\n\n    def run(self, host=None, port=None, debug=None, use_reloader=None,\n            open_browser=False):\n        \"\"\"\n        Starts a server to render the README.\n        \"\"\"\n        if host is None:\n            host = self.config['HOST']\n        if port is None:\n            port = self.config['PORT']\n        if debug is None:\n            debug = self.debug\n        if use_reloader is None:\n            use_reloader = self.config['DEBUG_GRIP']\n\n        # Verify the server is not already running and start\n        with self._run_mutex:\n            if self._shutdown_event:\n                raise AlreadyRunningError()\n            self._shutdown_event = threading.Event()\n\n        # Authentication message\n        if self.auth and not self.quiet:\n            if isinstance(self.auth, tuple):\n                username, password = self.auth\n                auth_method = ('credentials: {0}'.format(username)\n                               if username\n                               else 'personal access token')\n            else:\n                auth_method = type(self.auth).__name__\n            print(' * Using', auth_method, file=sys.stderr)\n\n        # Get random port manually when needed ahead of time\n        if port == 0 and open_browser:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            sock.bind(('localhost', 0))\n            port = sock.getsockname()[1]\n            sock.close()\n\n        # Open browser\n        browser_thread = (\n            start_browser_when_ready(host, port, self._shutdown_event)\n            if open_browser else None)\n\n        # Run local server\n        super(Grip, self).run(host, port, debug=debug,\n                              use_reloader=use_reloader,\n                              threaded=True)\n\n        # Signal to the polling and browser threads that they should exit\n        if not self.quiet:\n            print(' * Shutting down...')\n        self._shutdown_event.set()\n\n        # Wait for browser thread to finish\n        if browser_thread:\n            browser_thread.join()\n\n        # Cleanup\n        self._shutdown_event = None\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "grip/app.py", "language": "python", "complexity": "complex", "star_count": 6671, "quality_score": 1.0, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 474, "word_count": 1388}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import print_function, unicode_literals\n\nimport errno\nimport os\nimport posixpath\nimport re\nimport sys\nimport shutil\nfrom abc import ABCMeta, abstractmethod\ntry:\n    from urlparse import urljoin\nexcept ImportError:\n    from urllib.parse import urljoin\n\nimport requests\n\nfrom ._compat import safe_join\n\nfrom .constants import (\n    STYLE_URLS_SOURCE, STYLE_URLS_RES, STYLE_ASSET_URLS_RE,\n    STYLE_ASSET_URLS_SUB_FORMAT)\nfrom .vendor.six import add_metaclass\n\n\n@add_metaclass(ABCMeta)\nclass ReadmeAssetManager(object):\n    \"\"\"\n    Manages the style and font assets rendered with Readme pages.\n\n    Set cache_path to None to disable caching.\n    \"\"\"\n    def __init__(self, cache_path, style_urls=None, quiet=None):\n        super(ReadmeAssetManager, self).__init__()\n        self.cache_path = cache_path\n        self.style_urls = list(style_urls) if style_urls else []\n        self.styles = []\n        self.quiet = quiet\n\n    def _strip_url_params(self, url):\n        return url.rsplit('?', 1)[0].rsplit('#', 1)[0]\n\n    def clear(self):\n        \"\"\"\n        Clears the asset cache.\n        \"\"\"\n        if self.cache_path and os.path.exists(self.cache_path):\n            shutil.rmtree(self.cache_path)\n\n    def cache_filename(self, url):\n        \"\"\"\n        Gets a suitable relative filename for the specified URL.\n        \"\"\"\n        # FUTURE: Use url exactly instead of flattening it here\n        url = posixpath.basename(url)\n        return self._strip_url_params(url)\n\n    @abstractmethod\n    def retrieve_styles(self, asset_url_path):\n        \"\"\"\n        Get style URLs from the source HTML page and specified cached asset\n        URL path.\n        \"\"\"\n        pass\n\n\nclass GitHubAssetManager(ReadmeAssetManager):\n    \"\"\"\n    Reads the styles used for rendering Readme pages.\n\n    Set cache_path to None to disable caching.\n    \"\"\"\n    def __init__(self, cache_path, style_urls=None, quiet=None):\n        super(GitHubAssetManager, self).__init__(cache_path, style_urls, quiet)\n\n    def _get_style_urls(self, asset_url_path):\n        \"\"\"\n        Gets the specified resource and parses all style URLs and their\n        assets in the form of the specified patterns.\n        \"\"\"\n        # Check cache\n        if self.cache_path:\n            cached = self._get_cached_style_urls(asset_url_path)\n            # Skip fetching styles if there's any already cached\n            if cached:\n                return cached\n\n        # Find style URLs\n        r = requests.get(STYLE_URLS_SOURCE)\n        if not 200 <= r.status_code < 300:\n            print('Warning: retrieving styles gave status code',\n                  r.status_code, file=sys.stderr)\n        urls = []\n        content = r.text\n        for style_urls_re in STYLE_URLS_RES:\n            print(re.findall(style_urls_re, content))\n            urls.extend(re.findall(style_urls_re, content))\n        if not urls:\n            print('Warning: no styles found - see https://github.com/joeyespo/'\n                  'grip/issues/265', file=sys.stderr)\n\n        # Cache the styles and their assets\n        if self.cache_path:\n            is_cached = self._cache_contents(urls, asset_url_path)\n            if is_cached:\n                urls = self._get_cached_style_urls(asset_url_path)\n\n        return urls\n\n    def _get_cached_style_urls(self, asset_url_path):\n        \"\"\"\n        Gets the URLs of the cached styles.\n        \"\"\"\n        try:\n            cached_styles = os.listdir(self.cache_path)\n        except IOError as ex:\n            if ex.errno != errno.ENOENT and ex.errno != errno.ESRCH:\n                raise\n            return []\n        except OSError:\n            return []\n        return [posixpath.join(asset_url_path, style)\n                for style in cached_styles\n                if style.endswith('.css')]\n\n    def _cache_contents(self, style_urls, asset_url_path):\n        \"\"\"\n        Fetches the given URLs and caches their contents\n        and their assets in the given directory.\n        \"\"\"\n        files = {}\n\n        asset_urls = []\n        for style_url in style_urls:\n            if not self.quiet:\n                print(' * Downloading style', style_url, file=sys.stderr)\n            r = requests.get(style_url)\n            if not 200 <= r.status_code < 300:\n                print(' -> Warning: Style request responded with',\n                      r.status_code, file=sys.stderr)\n                files = None\n                continue\n            asset_content = r.text\n            # Find assets and replace their base URLs with the cache directory\n            for url in re.findall(STYLE_ASSET_URLS_RE, asset_content):\n                asset_urls.append(urljoin(style_url, url))\n            contents = re.sub(\n                STYLE_ASSET_URLS_RE,\n                STYLE_ASSET_URLS_SUB_FORMAT.format(asset_url_path.rstrip('/')),\n                asset_content)\n            # Prepare cache\n            if files is not None:\n                filename = self.cache_filename(style_url)\n                files[filename] = contents.encode('utf-8')\n\n        for asset_url in asset_urls:\n            if not self.quiet:\n                print(' * Downloading asset', asset_url, file=sys.stderr)\n            # Retrieve binary file and show message\n            r = requests.get(asset_url, stream=True)\n            if not 200 <= r.status_code < 300:\n                print(' -> Warning: Asset request responded with',\n                      r.status_code, file=sys.stderr)\n                files = None\n                continue\n            # Prepare cache\n            if files is not None:\n                filename = self.cache_filename(asset_url)\n                files[filename] = r.raw.read(decode_content=True)\n\n        # Skip caching if something went wrong to try again next time\n        if not files:\n            return False\n\n        # Cache files if all downloads were successful\n        cache = {}\n        for relname in files:\n            cache[safe_join(self.cache_path, relname)] = files[relname]\n        if not os.path.exists(self.cache_path):\n            os.makedirs(self.cache_path)\n        for filename in cache:\n            with open(filename, 'wb') as f:\n                f.write(cache[filename])\n        if not self.quiet:\n            print(\n                ' * Cached all downloads in', self.cache_path, file=sys.stderr)\n        return True\n\n    def retrieve_styles(self, asset_url_path):\n        \"\"\"\n        Get style URLs from the source HTML page and specified cached\n        asset base URL.\n        \"\"\"\n        if not asset_url_path.endswith('/'):\n            asset_url_path += '/'\n        self.style_urls.extend(self._get_style_urls(asset_url_path))\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "grip/assets.py", "language": "python", "complexity": "medium", "star_count": 6671, "quality_score": 0.8799999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 196, "word_count": 604}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\\\ngrip.command\n~~~~~~~~~~~~\n\nImplements the command-line interface for Grip.\n\n\nUsage:\n  grip [options] [<path>] [<address>]\n  grip -V | --version\n  grip -h | --help\n\nWhere:\n  <path> is a file to render or a directory containing README.md (- for stdin)\n  <address> is what to listen on, of the form <host>[:<port>], or just <port>\n\nOptions:\n  --user-content    Render as user-content like comments or issues.\n  --context=<repo>  The repository context, only taken into account\n                    when using --user-content.\n  --user=<username> A GitHub username for API authentication. If used\n                    without the --pass option, an upcoming password\n                    input will be necessary.\n  --pass=<password> A GitHub password or auth token for API auth.\n  --wide            Renders wide, i.e. when the side nav is collapsed.\n                    This only takes effect when --user-content is used.\n  --clear           Clears the cached styles and assets and exits.\n  --export          Exports to <path>.html or README.md instead of\n                    serving, optionally using [<address>] as the out\n                    file (- for stdout).\n  --no-inline       Link to styles instead inlining when using --export.\n  -b --browser      Open a tab in the browser after the server starts.\n  --api-url=<url>   Specify a different base URL for the github API,\n                    for example that of a Github Enterprise instance.\n                    Default is the public API: https://api.github.com\n  --title=<title>   Manually sets the page's title.\n                    The default is the filename.\n  --norefresh       Do not automatically refresh the Readme content when\n                    the file changes.\n  --quiet           Do not print to the terminal.\n  --theme=<theme>   Theme to view markdown file (light mode or dark mode).\n                    Valid options (\"light\", \"dark\"). Default: \"light\"\n\"\"\"\n\nfrom __future__ import print_function\n\nimport sys\nimport mimetypes\nimport socket\nimport errno\n\nfrom docopt import docopt\nfrom getpass import getpass\nfrom path_and_address import resolve, split_address\n\nfrom . import __version__\nfrom .api import clear_cache, export, serve\nfrom .exceptions import ReadmeNotFoundError\n\n\nusage = '\\n\\n\\n'.join(__doc__.split('\\n\\n\\n')[1:])\nversion = 'Grip ' + __version__\n\n# Note: GitHub supports more than light mode and dark mode (exp: light-high-constrast, dark-high-constrast).\nVALID_THEME_OPTIONS = ['light', 'dark']\n\ndef main(argv=None, force_utf8=True, patch_svg=True):\n    \"\"\"\n    The entry point of the application.\n    \"\"\"\n    if force_utf8 and sys.version_info[0] == 2:\n        reload(sys)  # noqa\n        sys.setdefaultencoding('utf-8')\n    if patch_svg and sys.version_info[0] == 2 and sys.version_info[1] <= 6:\n        mimetypes.add_type('image/svg+xml', '.svg')\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    # Show specific errors\n    if '-a' in argv or '--address' in argv:\n        print('Use grip [options] <path> <address> instead of -a')\n        print('See grip -h for details')\n        return 2\n    if '-p' in argv or '--port' in argv:\n        print('Use grip [options] [<path>] [<hostname>:]<port> instead of -p')\n        print('See grip -h for details')\n        return 2\n\n    # Parse options\n    args = docopt(usage, argv=argv, version=version)\n\n    # Handle printing version with -V (docopt handles --version)\n    if args['-V']:\n        print(version)\n        return 0\n\n    # Clear the cache\n    if args['--clear']:\n        clear_cache()\n        return 0\n\n    # Get password from prompt if necessary\n    password = args['--pass']\n    if args['--user'] and not password:\n        password = getpass()\n\n    # Parse theme argument\n    if args['--theme']:\n        if args['--theme'] in VALID_THEME_OPTIONS:\n            theme: str = args['--theme']\n        else:\n            print('Error: valid options for theme argument are \"light\", \"dark\"')\n            return 1\n    else:\n        theme = 'light'\n\n    # Export to a file instead of running a server\n    if args['--export']:\n        try:\n            export(args['<path>'], args['--user-content'], args['--context'],\n                   args['--user'], password, False, args['--wide'],\n                   not args['--no-inline'], args['<address>'],\n                   args['--api-url'], args['--title'], args['--quiet'], theme)\n            return 0\n        except ReadmeNotFoundError as ex:\n            print('Error:', ex)\n            return 1\n\n    # Parse arguments\n    path, address = resolve(args['<path>'], args['<address>'])\n    host, port = split_address(address)\n\n    # Validate address\n    if address and not host and port is None:\n        print('Error: Invalid address', repr(address))\n\n    # Run server\n    try:\n        serve(path, host, port, args['--user-content'], args['--context'],\n              args['--user'], password, False, args['--wide'], False,\n              args['--api-url'], args['--title'], not args['--norefresh'],\n              args['--browser'], args['--quiet'], theme, None)\n        return 0\n    except ReadmeNotFoundError as ex:\n        print('Error:', ex)\n        return 1\n    except socket.error as ex:\n        print('Error:', ex)\n        if ex.errno == errno.EADDRINUSE:\n            print('This port is in use. Is a grip server already running? '\n                  'Stop that instance or specify another port here.')\n        return 1\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "grip/command.py", "language": "python", "complexity": "medium", "star_count": 6671, "quality_score": 0.85, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 154, "word_count": 604}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# The common titles and supported extensions,\n# as defined by https://github.com/github/markup\nSUPPORTED_TITLES = ['README', 'Readme', 'readme', 'Home']\nSUPPORTED_EXTENSIONS = ['.md', '.markdown']\n\n\n# The default filenames when no file is provided\nDEFAULT_FILENAMES = [title + ext\n                     for title in SUPPORTED_TITLES\n                     for ext in SUPPORTED_EXTENSIONS]\nDEFAULT_FILENAME = DEFAULT_FILENAMES[0]\n\n\n# The default directory to load Grip settings from\nDEFAULT_GRIPHOME = '~/.grip'\n\n\n# The default URL of the Grip server\nDEFAULT_GRIPURL = '/__/grip'\n\n\n# The public GitHub API\nDEFAULT_API_URL = 'https://api.github.com'\n\n\n# Style parsing\nSTYLE_URLS_SOURCE = 'https://github.com/joeyespo/grip'\n# Note: Using a list in case the implementation limitation is a problem\n# https://docs.python.org/3/library/re.html#re.findall\nSTYLE_URLS_RES = [\n    r'''<link\\b[^>]+\\bhref=['\"]?([^'\" >]+)['\"]?\\brel=['\"]?stylesheet['\"]?[^>]+[^>]*(?=>)''',\n    r'''<link\\b[^>]+\\brel=['\"]?stylesheet['\"]?[^>]+\\bhref=['\"]?([^'\" >]+)['\"]?[^>]*(?=>)''',\n]\nSTYLE_ASSET_URLS_RE = (\n    r'''url\\(['\"]?(/static/fonts/octicons/[^'\" \\)]+)['\"]?\\)''')\nSTYLE_ASSET_URLS_SUB_FORMAT = r'url(\"{0}\\1\")'\nSTYLE_ASSET_URLS_INLINE_FORMAT = (\n    r'''url\\(['\"]?((?:/static|{0})/[^'\" \\)]+)['\"]?\\)''')\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "grip/constants.py", "language": "python", "complexity": "simple", "star_count": 6671, "quality_score": 0.76, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 39, "word_count": 120}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import errno\ntry:\n    NotFoundError = FileNotFoundError\nexcept NameError:\n    NotFoundError = IOError\n\n\nclass AlreadyRunningError(RuntimeError):\n    pass\n\n\nclass ReadmeNotFoundError(NotFoundError):\n    \"\"\"\n    This class inherits from FileNotFoundError on Python 3 and above.\n    \"\"\"\n    def __init__(self, path=None, message=None):\n        self.path = path\n        self.message = message\n        super(ReadmeNotFoundError, self).__init__(\n            errno.ENOENT, 'README not found', path)\n\n    def __repr__(self):\n        return '{0}({!r}, {!r})'.format(\n            type(self).__name__, self.path, self.message)\n\n    def __str__(self):\n        if self.message:\n            return self.message\n\n        if self.path is not None:\n            return 'No README found at {0}'.format(self.path)\n\n        return self.strerror\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "grip/exceptions.py", "language": "python", "complexity": "simple", "star_count": 6671, "quality_score": 0.8799999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 34, "word_count": 72}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import print_function, unicode_literals\n\nimport errno\nimport io\nimport mimetypes\nimport os\nimport posixpath\nimport sys\nfrom abc import ABCMeta, abstractmethod\n\nfrom ._compat import safe_join\n\nfrom .constants import DEFAULT_FILENAMES, DEFAULT_FILENAME\nfrom .exceptions import ReadmeNotFoundError\nfrom .vendor.six import add_metaclass\n\n\n@add_metaclass(ABCMeta)\nclass ReadmeReader(object):\n    \"\"\"\n    Reads Readme content from a URL subpath.\n    \"\"\"\n    def __init__(self):\n        super(ReadmeReader, self).__init__()\n\n    def normalize_subpath(self, subpath):\n        \"\"\"\n        Returns the normalized subpath.\n\n        This allows Readme files to be inferred from directories while\n        still allowing relative paths to work properly.\n\n        Override to change the default behavior of returning the\n        specified subpath as-is.\n        \"\"\"\n        if subpath is None:\n            return None\n\n        return posixpath.normpath(subpath)\n\n    def filename_for(self, subpath):\n        \"\"\"\n        Returns the relative filename for the specified subpath, or None\n        if the file does not exist.\n        \"\"\"\n        return None\n\n    def mimetype_for(self, subpath=None):\n        \"\"\"\n        Gets the mimetype for the specified subpath.\n        \"\"\"\n        if subpath is None:\n            subpath = DEFAULT_FILENAME\n        mimetype, _ = mimetypes.guess_type(subpath)\n        return mimetype\n\n    def is_binary(self, subpath=None):\n        \"\"\"\n        Gets whether the specified subpath is a supported binary file.\n        \"\"\"\n        return False\n\n    def last_updated(self, subpath=None):\n        \"\"\"\n        Returns the time of the last modification of the Readme or\n        specified subpath. None is returned if the reader doesn't\n        support modification tracking.\n\n        The format of return value is dependent on the implementing\n        reader. It can be any object as long as equality indicates\n        that the content was not updated.\n        \"\"\"\n        return None\n\n    @abstractmethod\n    def read(self, subpath=None):\n        \"\"\"\n        Returns the UTF-8 content of the specified subpath, or None if\n        subpath does not exist.\n        \"\"\"\n        pass\n\n\nclass DirectoryReader(ReadmeReader):\n    \"\"\"\n    Reads Readme files from URL subpaths.\n    \"\"\"\n    def __init__(self, path=None, silent=False):\n        super(DirectoryReader, self).__init__()\n        root_filename = os.path.abspath(self._resolve_readme(path, silent))\n        self.root_filename = root_filename\n        self.root_directory = os.path.dirname(root_filename)\n\n    def _find_file(self, path, silent=False):\n        \"\"\"\n        Gets the full path and extension, or None if a README file could not\n        be found at the specified path.\n        \"\"\"\n        for filename in DEFAULT_FILENAMES:\n            full_path = os.path.join(path, filename) if path else filename\n            if os.path.exists(full_path):\n                return full_path\n\n        # Return default filename if silent\n        if silent:\n            return os.path.join(path, DEFAULT_FILENAME)\n\n        raise ReadmeNotFoundError(path)\n\n    def _resolve_readme(self, path=None, silent=False):\n        \"\"\"\n        Returns the path if it's a file; otherwise, looks for a compatible\n        README file in the directory specified by path.\n\n        If path is None, the current working directory is used.\n\n        If silent is set, the default relative filename will be returned\n        if path is a directory or None if it does not exist.\n\n        Raises ReadmeNotFoundError if no compatible README file can be\n        found and silent is False.\n        \"\"\"\n        # Default to current working directory\n        if path is None:\n            path = '.'\n\n        # Normalize the path\n        path = os.path.normpath(path)\n\n        # Resolve README file if path is a directory\n        if os.path.isdir(path):\n            return self._find_file(path, silent)\n\n        # Return path if file exists or if silent\n        if silent or os.path.exists(path):\n            return path\n\n        raise ReadmeNotFoundError(path, 'File not found: ' + path)\n\n    def _read_text(self, filename):\n        \"\"\"\n        Helper that reads the UTF-8 content of the specified file, or\n        None if the file doesn't exist. This returns a unicode string.\n        \"\"\"\n        with io.open(filename, 'rt', encoding='utf-8') as f:\n            return f.read()\n\n    def _read_binary(self, filename):\n        \"\"\"\n        Helper that reads the binary content of the specified file, or\n        None if the file doesn't exist. This returns a byte string.\n        \"\"\"\n        with io.open(filename, 'rb') as f:\n            return f.read()\n\n    def normalize_subpath(self, subpath):\n        \"\"\"\n        Normalizes the specified subpath, or None if subpath is None.\n\n        This allows Readme files to be inferred from directories while\n        still allowing relative paths to work properly.\n\n        Raises werkzeug.exceptions.NotFound if the resulting path\n        would fall out of the root directory.\n        \"\"\"\n        if subpath is None:\n            return None\n\n        # Normalize the subpath\n        subpath = posixpath.normpath(subpath)\n\n        # Add or remove trailing slash to properly support relative links\n        filename = os.path.normpath(safe_join(self.root_directory, subpath))\n        if os.path.isdir(filename):\n            subpath += '/'\n\n        return subpath\n\n    def readme_for(self, subpath):\n        \"\"\"\n        Returns the full path for the README file for the specified\n        subpath, or the root filename if subpath is None.\n\n        Raises ReadmeNotFoundError if a README for the specified subpath\n        does not exist.\n\n        Raises werkzeug.exceptions.NotFound if the resulting path\n        would fall out of the root directory.\n        \"\"\"\n        if subpath is None:\n            return self.root_filename\n\n        # Join for safety and to convert subpath to normalized OS-specific path\n        filename = os.path.normpath(safe_join(self.root_directory, subpath))\n\n        # Check for existence\n        if not os.path.exists(filename):\n            raise ReadmeNotFoundError(filename)\n\n        # Resolve README file if path is a directory\n        if os.path.isdir(filename):\n            return self._find_file(filename)\n\n        return filename\n\n    def filename_for(self, subpath):\n        \"\"\"\n        Returns the relative filename for the specified subpath, or the\n        root filename if subpath is None.\n\n        Raises werkzeug.exceptions.NotFound if the resulting path\n        would fall out of the root directory.\n        \"\"\"\n        try:\n            filename = self.readme_for(subpath)\n            return os.path.relpath(filename, self.root_directory)\n        except ReadmeNotFoundError:\n            return None\n\n    def is_binary(self, subpath=None):\n        \"\"\"\n        Gets whether the specified subpath is a supported binary file.\n        \"\"\"\n        mimetype = self.mimetype_for(subpath)\n        return mimetype and not mimetype.startswith('text/')\n\n    def last_updated(self, subpath=None):\n        \"\"\"\n        Returns the time of the last modification of the Readme or\n        specified subpath, or None if the file does not exist.\n\n        The return value is a number giving the number of seconds since\n        the epoch (see the time module).\n\n        Raises werkzeug.exceptions.NotFound if the resulting path\n        would fall out of the root directory.\n        \"\"\"\n        try:\n            return os.path.getmtime(self.readme_for(subpath))\n        except ReadmeNotFoundError:\n            return None\n        # OSError for Python 3 base class, EnvironmentError for Python 2\n        except (OSError, EnvironmentError) as ex:\n            if ex.errno == errno.ENOENT:\n                return None\n            raise\n\n    def read(self, subpath=None):\n        \"\"\"\n        Returns the UTF-8 content of the specified subpath.\n\n        subpath is expected to already have been normalized.\n\n        Raises ReadmeNotFoundError if a README for the specified subpath\n        does not exist.\n\n        Raises werkzeug.exceptions.NotFound if the resulting path\n        would fall out of the root directory.\n        \"\"\"\n        is_binary = self.is_binary(subpath)\n        filename = self.readme_for(subpath)\n        try:\n            if is_binary:\n                return self._read_binary(filename)\n            return self._read_text(filename)\n        # OSError for Python 3 base class, EnvironmentError for Python 2\n        except (OSError, EnvironmentError) as ex:\n            if ex.errno == errno.ENOENT:\n                raise ReadmeNotFoundError(filename)\n            raise\n\n\nclass TextReader(ReadmeReader):\n    \"\"\"\n    Reads Readme content from the provided unicode string.\n    \"\"\"\n    def __init__(self, text, display_filename=None):\n        super(TextReader, self).__init__()\n        self.text = text\n        self.display_filename = display_filename\n\n    def filename_for(self, subpath):\n        \"\"\"\n        Returns the display filename, or None if subpath is specified\n        since subpaths are not supported for text readers.\n        \"\"\"\n        if subpath is not None:\n            return None\n\n        return self.display_filename\n\n    def read(self, subpath=None):\n        \"\"\"\n        Returns the UTF-8 Readme content.\n\n        Raises ReadmeNotFoundError if subpath is specified since\n        subpaths are not supported for text readers.\n        \"\"\"\n        if subpath is not None:\n            raise ReadmeNotFoundError(subpath)\n\n        return self.text\n\n\nclass StdinReader(TextReader):\n    \"\"\"\n    Reads Readme text from STDIN.\n    \"\"\"\n    def __init__(self, display_filename=None):\n        super(StdinReader, self).__init__(None, display_filename)\n\n    def read(self, subpath=None):\n        \"\"\"\n        Returns the UTF-8 Readme content.\n\n        Raises ReadmeNotFoundError if subpath is specified since\n        subpaths are not supported for text readers.\n        \"\"\"\n        # Lazily read STDIN\n        if self.text is None and subpath is None:\n            self.text = self.read_stdin()\n\n        return super(StdinReader, self).read(subpath)\n\n    def read_stdin(self):\n        \"\"\"\n        Reads STDIN until the end of input and returns a unicode string.\n        \"\"\"\n        text = sys.stdin.read()\n\n        # Decode the bytes returned from earlier Python STDIN implementations\n        if sys.version_info[0] < 3 and text is not None:\n            text = text.decode(sys.stdin.encoding or 'utf-8')\n\n        return text\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "grip/readers.py", "language": "python", "complexity": "complex", "star_count": 6671, "quality_score": 0.8799999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 336, "word_count": 1129}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create project README with installation, usage, and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import print_function, unicode_literals\n\nimport json\nimport sys\nfrom abc import ABCMeta, abstractmethod\n\nimport requests\n\ntry:\n    import markdown\n    from .vendor.mdx_urlize import UrlizeExtension\nexcept ImportError:\n    markdown = None\n    UrlizeExtension = None\n\nfrom .constants import DEFAULT_API_URL\nfrom .patcher import patch\nfrom .vendor.six import add_metaclass\n\n\n@add_metaclass(ABCMeta)\nclass ReadmeRenderer(object):\n    \"\"\"\n    Renders the Readme.\n    \"\"\"\n    def __init__(self, user_content=None, context=None):\n        if user_content is None:\n            user_content = False\n        super(ReadmeRenderer, self).__init__()\n        self.user_content = user_content\n        self.context = context\n\n    @abstractmethod\n    def render(self, text, auth=None):\n        \"\"\"\n        Renders the specified markdown content and embedded styles.\n        \"\"\"\n        pass\n\n\nclass GitHubRenderer(ReadmeRenderer):\n    \"\"\"\n    Renders the specified Readme using the GitHub Markdown API.\n    \"\"\"\n    def __init__(self, user_content=None, context=None, api_url=None,\n                 raw=None):\n        if api_url is None:\n            api_url = DEFAULT_API_URL\n        super(GitHubRenderer, self).__init__(user_content, context)\n        self.api_url = api_url\n        self.raw = raw\n\n    def render(self, text, auth=None):\n        \"\"\"\n        Renders the specified markdown content and embedded styles.\n\n        Raises TypeError if text is not a Unicode string.\n        Raises requests.HTTPError if the request fails.\n        \"\"\"\n        # Ensure text is Unicode\n        expected = str if sys.version_info[0] >= 3 else unicode  # noqa\n        if not isinstance(text, expected):\n            raise TypeError(\n                'Expected a Unicode string, got {!r}.'.format(text))\n\n        if self.user_content:\n            url = '{0}/markdown'.format(self.api_url)\n            data = {'text': text, 'mode': 'gfm'}\n            if self.context:\n                data['context'] = self.context\n            data = json.dumps(data, ensure_ascii=False).encode('utf-8')\n            headers = {'content-type': 'application/json; charset=UTF-8'}\n        else:\n            url = '{0}/markdown/raw'.format(self.api_url)\n            data = text.encode('utf-8')\n            headers = {'content-type': 'text/x-markdown; charset=UTF-8'}\n\n        r = requests.post(url, headers=headers, data=data, auth=auth)\n        r.raise_for_status()\n\n        # FUTURE: Remove this once GitHub API properly handles Unicode markdown\n        r.encoding = 'utf-8'\n\n        return r.text if self.raw else patch(r.text)\n\n\nclass OfflineRenderer(ReadmeRenderer):\n    \"\"\"\n    Renders the specified Readme locally using pure Python.\n\n    Note: This is currently an incomplete feature.\n    \"\"\"\n    def __init__(self, user_content=None, context=None):\n        super(OfflineRenderer, self).__init__(user_content, context)\n\n    def render(self, text, auth=None):\n        \"\"\"\n        Renders the specified markdown content and embedded styles.\n        \"\"\"\n        if markdown is None:\n            import markdown\n        if UrlizeExtension is None:\n            from .mdx_urlize import UrlizeExtension\n        return markdown.markdown(text, extensions=[\n            'fenced_code',\n            'codehilite(css_class=highlight)',\n            'toc',\n            'tables',\n            'sane_lists',\n            UrlizeExtension(),\n        ])\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "grip/renderers.py", "language": "python", "complexity": "medium", "star_count": 6671, "quality_score": 0.8799999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 112, "word_count": 306}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\\\nDefault Configuration\n\nDo NOT change the values here for risk of accidentally committing them.\nOverride them using command-line arguments or with a settings_local.py in\nthis directory or in ~/.grip/settings.py instead.\n\"\"\"\n\n\nHOST = 'localhost'\nPORT = 6419\nDEBUG = False\nDEBUG_GRIP = False\nCACHE_DIRECTORY = 'cache-{version}'\nAUTOREFRESH = True\nQUIET = False\n\n\n# Note: For security concerns, please don't save your GitHub password in your\n# local settings.py. Use a personal access token instead:\n# https://github.com/settings/tokens/new?scopes=\nUSERNAME = None\nPASSWORD = None\n\n\n# Custom GitHub API\nAPI_URL = None\n\n\n# Custom styles\nSTYLE_URLS = []\n", "metadata": {"source": "github.com/joeyespo/grip", "file_path": "grip/settings.py", "language": "python", "complexity": "simple", "star_count": 6671, "quality_score": 0.76, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 32, "word_count": 96}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# CV-Attention\nÂÖ≥‰∫éCVÁöÑ‰∏Ä‰∫õÁªèÂÖ∏Ê≥®ÊÑèÂäõÊú∫Âà∂‰ª£Á†Å„ÄÇ  \nÁõÆÂâç‰ª£Á†ÅÊ†ºÂºè‰∏ªË¶ÅÁî®‰∫éyolov3,yolov5,yolov7,yolov8.\n\n# Supports\n| name | need_chaneel | paper |\n| :----:| :----: | :----: |\n| BAM | True | https://arxiv.org/pdf/1807.06514.pdf |\n| CBAM | True | https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf |\n| SE | True | https://arxiv.org/abs/1709.01507 |\n| CoTAttention | True | https://arxiv.org/abs/2107.12292 |\n| MobileViTAttention | True | https://arxiv.org/abs/2110.02178 |\n| SimAM | False | http://proceedings.mlr.press/v139/yang21o/yang21o.pdf |\n| SK | True | https://arxiv.org/pdf/1903.06586.pdf |\n| ShuffleAttention | True | https://arxiv.org/pdf/2102.00240.pdf |\n| S2Attention | True | https://arxiv.org/abs/2108.01072 |\n| TripletAttention | False | https://arxiv.org/abs/2010.03045 |\n| ECA | True | https://arxiv.org/pdf/1910.03151.pdf |\n| ParNetAttention | True | https://arxiv.org/abs/2110.07641 |\n| CoordAttention | True | https://arxiv.org/abs/2103.02907 |\n| MHSA<br>Multi-Head-Self-Attention | True | https://wuch15.github.io/paper/EMNLP2019-NRMS.pdf |\n| SGE | False | https://arxiv.org/pdf/1905.09646.pdf |\n| A2Attention | True | https://arxiv.org/pdf/1810.11579.pdf |\n| GC<br>Global Context Attention | True | https://arxiv.org/abs/1904.11492 |\n| EffectiveSE<br>Effective Squeeze-Excitation | True | https://arxiv.org/abs/1911.06667 |\n| GE<br>Gather-Excite Attention | True | https://arxiv.org/abs/1810.12348 |\n| CrissCrossAttention | True | https://arxiv.org/abs/1811.11721 |\n| Polarized Self-Attention | True | https://arxiv.org/abs/2107.00782 |\n| Sequential Self-Attention | True | https://arxiv.org/abs/2107.00782 |\n| GAM | True | https://arxiv.org/pdf/2112.05561v1.pdf |\n| Biformer | True | https://arxiv.org/abs/2303.08810 |\n| EMA | True | https://arxiv.org/abs/2305.13563v2 |\n| CloAttention | True | https://arxiv.org/abs/2303.17803 |\n| LSKBlock | True | https://arxiv.org/pdf/2303.09030.pdf |\n| MLCA | True | https://www.sciencedirect.com/science/article/pii/S0952197623006267 |\n| LSKA | True | https://arxiv.org/abs/2309.01439 |\n| DAttention | True | https://openaccess.thecvf.com/content/CVPR2022/html/Xia_Vision_Transformer_With_Deformable_Attention_CVPR_2022_paper.html |\n| ELA | True | https://arxiv.org/abs/2403.01123 |\n| CAA | True | https://arxiv.org/pdf/2403.06258 |\n| CPCA | True | https://arxiv.org/abs/2306.05196 |\n\n# Install\nÂÆâË£ÖÂëΩ‰ª§:pip install timm einops efficientnet_pytorch -i https://pypi.tuna.tsinghua.edu.cn/simple\n\n# Course\n1. [yolov5Ê∑ªÂä†Ê≥®ÊÑèÂäõÂìîÂì©ÂìîÂì©ËßÜÈ¢ëÊïôÂ≠¶ÈìæÊé•](https://www.bilibili.com/video/BV1s84y1775U) [yolov5Ê∑ªÂä†Ê≥®ÊÑèÂäõ-Ë°•ÂÖÖ‰∫ãÈ°π-ÂìîÂì©ÂìîÂì©ËßÜÈ¢ëÊïôÂ≠¶ÈìæÊé•](https://www.bilibili.com/video/BV1hG4y1M71X)\n2. [yolov7Ê∑ªÂä†Ê≥®ÊÑèÂäõÂìîÂì©ÂìîÂì©ËßÜÈ¢ëÊïôÂ≠¶ÈìæÊé•](https://www.bilibili.com/video/BV1pd4y1H7BK)\n3. [yolov8Ê∑ªÂä†Ê≥®ÊÑèÂäõÂìîÂì©ÂìîÂì©ËßÜÈ¢ëÊïôÂ≠¶ÈìæÊé•](https://www.bilibili.com/video/BV1ZQ4y1J7oC/) [yolov8Ê∑ªÂä†Ê≥®ÊÑèÂäõËøõÈò∂ÁâàÂìîÂì©ÂìîÂì©ËßÜÈ¢ëÊïôÂ≠¶ÈìæÊé•](https://www.bilibili.com/video/BV1ZQ4y1J7oC/)\n\n# Reference\nhttps://github.com/xmu-xiaoma666/External-Attention-pytorch  \nhttps://github.com/rwightman/pytorch-image-models  \nhttps://github.com/rayleizhu/BiFormer  \nhttps://github.com/XiaLiPKU/EMANet  \nhttps://github.com/qhfan/CloFormer/tree/main  \nhttps://github.com/zcablii/LSKNet  \nhttps://github.com/wandahangFY/MLCA  \nhttps://github.com/StevenLauHKHK/Large-Separable-Kernel-Attention  \nhttps://github.com/LeapLabTHU/DAT  \nhttps://github.com/NUST-Machine-Intelligence-Laboratory/PKINet  \nhttps://github.com/Cuthbert-Huang/CPCANet  ", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "cv-attention/readme.md", "language": "markdown", "complexity": "simple", "star_count": 6481, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 61, "word_count": 288}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# 2025-SOTAÁõÆÊ†áÊ£ÄÊµãÊ®°ÂûãÈ°πÁõÆ(2025ÂèëËÆ∫ÊñáÂøÖÂ§áÈ°πÁõÆ)\n\nÈâ¥‰∫éÁõÆÂâçYOLOÁ≥ªÂàóÊ®°ÂûãÂèçÊò†ÁöÑÊãíÁ®øÁéáË∂äÊù•Ë∂äÈ´ò‰∏îYOLOÊ®°ÂûãÁ°ÆÂÆûÈùûÂ∏∏Ê≥õÊª•ÔºåÊó†ËÆ∫ÊòØ‰∏çÊòØËÆ°ÁÆóÊú∫‰∏ì‰∏ö„ÄÅÊòØ‰∏çÊòØÂ∞èÁôΩÈÉΩÂü∫Êú¨ÂèØ‰ª•Âø´ÈÄü‰∏äÊâãYOLOÊ®°ÂûãÔºåÂØºËá¥ËÆ°ÁÆóÊú∫‰∏ì‰∏öÂíåÊúâÊúüÂàäÁ∫ßÂà´Ë¶ÅÊ±ÇÁöÑÂ∞è‰ºô‰º¥Êó•ÁõäÈöæÂèóÔºåÁÆÄÂçïÊù•ËØ¥Â∞±ÊòØYOLOÂú®Â≠¶ÊúØÁïåÁöÑÁ∫¢Âà©Â∑≤ÁªèÂü∫Êú¨ÂêÉÈÄèÔºåÁõÆÂâçÂºÄÂßãË∂äÊù•Ë∂äÂ§ö‰∫∫ËΩ¨CVPR2024-RTDETRÔºåËÄå‰∏îÁõÆÂâçÁ†îÁ©∂ÁîüÊØï‰∏ö‰∏ÄÂπ¥ÊØî‰∏ÄÂπ¥ÈöæÔºå‰∏çÂÉè‰ª•ÂâçÈöè‰æøÁªìÂêàÁÇπÊ∑±Â∫¶Â≠¶‰π†Â∞±ÂèØ‰ª•ÊØï‰∏öÔºåÂ∞±ÂÉèË∂äÊù•Ë∂äÂ§ö‰∫∫ÂèçÈ¶àÔºåÂØºÂ∏àÂ∑≤ÁªèÊòéÁ°ÆÁ¶ÅÊ≠¢‰∏çËÉΩÁî®YOLOÔºåÂÜçÂä†‰∏äËøô‰πàÂ§öÂπ¥Êù•YOLOÂØπÂ≠¶ÊúØÁöÑÁÅåÊ∞¥Â∑≤ÁªèËÆ©ÂÆ°Á®ø‰∫∫Âá∫Áé∞ËßÜËßâÁñ≤Âä≥ÔºåÂ∏¶‰∏ä‰∫Ü‚ÄùÊúâËâ≤‚ÄùÁúºÈïúÁúãÂæÖYOLOÔºåÊâÄ‰ª•ÁªìÂêà‰ª•‰∏ä‰ºóÂ§öÂéüÂõ†ÔºåÂõ†Ê≠§Êàë‰ª¨ÈúÄË¶Å‰∏Ä‰∏™Êúâ‰∏ÄÂÆö‰∏äÊâãÈöæÂ∫¶‰∏îÊòØÈ°∂‰ºöÁöÑÊ®°ÂûãÊù•ÊîØÊíëÊàë‰ª¨ÂêéÁª≠ÁöÑÂ§ßÂ∞èËÆ∫ÊñáÁöÑÂ∑•‰Ωú„ÄÇ\nPS:20250614ÁâàÊú¨Êõ¥Êñ∞ÂêéÔºåÊú¨È°πÁõÆÁöÑdfineÂíådeimÂ∑≤ÁªèÊîØÊåÅUltralyticsÂêåÊ¨æÁöÑÈÖçÁΩÆÊñá‰ª∂ÂΩ¢ÂºèÔºåÂ§ßÂ§ßÈôç‰Ωé‰∏äÊâãÈöæÂ∫¶ÔºÅ[BÁ´ô‰ªãÁªçÈìæÊé•](https://www.bilibili.com/video/BV1Q4MHzXEdd/)\n\n### 1. Ëøô‰∏™È°πÁõÆÂåÖÂê´‰ªÄ‰πàÊ®°ÂûãÔºü\n\nËøô‰∏™È°πÁõÆÁöÑÊ∫ê‰ª£Á†ÅÊù•Ëá™Ôºö[DEIM](https://github.com/ShihuaHuang95/DEIM)  \nÂÖ∂ÂÜÖÈÉ®ÂèØ‰ª•Ë∑ë‰ª•‰∏ãÊ®°Âûã(‰ª•‰∏ã‰ªÖÊîØÊåÅÁõÆÊ†áÊ£ÄÊµã„ÄÅ‰∏çÊîØÊåÅÂÆû‰æãÂàÜÂâ≤„ÄÅÂßøÊÄÅÊ£ÄÊµã„ÄÅÊóãËΩ¨ÁõÆÊ†áÊ£ÄÊµã)Ôºö\n1. CVPR2025-DEIM\n2. ICLR2025-DFine\n3. RTDETRV2\n\nÈÄâÊã©Ëøô‰∏™ËØæÁ®ãÔºåËøô‰∫õÊ®°ÂûãÈÉΩÂèØ‰ª•ÊîπËøõÔºå‰∏çÈôê‰∫éDEIMÔºåËøô‰∫õÈÉΩÊòØÈ°∂‰ºöÁöÑÊ®°ÂûãÔºå‰∏çË¶ÅËØ¥2025ÔºåÂ∞±ÁÆóÊòØ2026ÈÉΩ‰∏çËêΩÂêéÔºÅËøòÊúâ‰∏Ä‰∏™ÈáçÁÇπÂ∞±ÊòØÂÉèCVPR2024-RTDETRÔºåÊúÄÂ∞èÁöÑÊ®°Âûã‰πüÊúâ50GFLOPsÔºå‰ΩÜÊòØÁé∞Âú®ÁöÑDEIMÂíåDFineÈÉΩÊúâÂÉèYOLO‰∏ÄÊ†∑ÁöÑNanoÂ§ßÂ∞èÁâàÊú¨ÁöÑÊ®°ÂûãÔºåÂèòÁõ∏Èôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨ÂíåËÆæÂ§áË¶ÅÊ±ÇÔºÅ(Âª∫ËÆÆÊúÄ‰Ωé12GÊòæÂ≠òÁöÑÊòæÂç°Ëµ∑Ê≠•)\n\n### 2. Ëøô‰∏™È°πÁõÆ‰ºö‰ª•‰ªÄ‰πàÂΩ¢ÂºèÂºÄÂ±ïÔºü\n\n1. Ëøô‰∏™È°πÁõÆË∑ü‰ª•ÂæÄÂå∫Âà´ÊØîËæÉÂ§ßÔºåÊàë‰ª¨ÂÖ∂‰ªñÊîπËøõÈ°πÁõÆÈÉΩÊòØÁõ¥Êé•Êèê‰æõÂ•Ω‰øÆÊîπÂ•ΩÁöÑ‰ª£Á†ÅÔºåÁî®Êà∑‰∏çÈúÄË¶ÅÊáÇ‰ª£Á†ÅÁöÑÊÉÖÂÜµ‰∏ã‰πüÂèØ‰ª•ÂºÄÂßãÂÅöÂÆûÈ™åÔºåÁîöËá≥ÂèØ‰ª•ÂÅöÂÆåÂÆûÈ™åÔºå‰ΩÜÊòØËøôÊ†∑‰πüÊúâ‰∏Ä‰∏™‰∏çÂ•ΩÁöÑÁÇπÔºåÂ∞±ÊòØ‰ºöÂ§ßÂπÖÂ∫¶Èôç‰Ωé‰∏äÊâãÈó®ÊßõÔºåËøôÁâπÂà´ÂØπËÆ°ÁÆóÊú∫‰∏ì‰∏öÁöÑÂêåÂ≠¶Êù•ËØ¥ÊòØÈùûÂ∏∏‰∏çÂà©ÁöÑÔºåÂõ†Ê≠§Ëøô‰∏™È°πÁõÆÂú®‰ª£Á†ÅÂ∑•Á®ãÊñπÈù¢ÔºåËøô‰∏™È°πÁõÆÊàë‰ª¨‰ºöÊúâÊïôÁ®ãÊïôÂ§ßÂÆ∂ÊÄé‰πàÂéªË∞ÉËØïÁ®ãÂ∫è„ÄÅ‰øÆÊîπ‰ª£Á†Å„ÄÅÊ∑ªÂä†Ê®°Âùó„ÄÇ\n2. Ëøô‰∏™È°πÁõÆ‰ºö**‰∏çÂÆöÊó∂(Áõ¥Êí≠Êó∂Èó¥Âà∞Êó∂ÂÄô‰ºöÁæ§ÈáåËøõË°åÈÄöÁü•ÔºåÊ≤°ÊúâÁ°¨ÊÄßËßÑÂÆöÂ§ö‰πÖ‰∏ÄÊ¨°Ôºå‰∏çÊñπ‰æøÁúãÁöÑ‰ºöÊúâÂΩïÊí≠)**Êúâ**Áõ¥Êí≠**ÔºåËØ¶ÁªÜÁõ¥Êí≠ÂÜÖÂÆπËØ∑ÁúãÁ¨¨‰∏âÂ§ßÁÇπ„ÄÇ\n3. Ëøô‰∏™È°πÁõÆ‰ºöÊåÅÁª≠Êõ¥Êñ∞ÂàõÊñ∞ÁÇπÔºåÂ¶ÇÊûúÂàõÊñ∞ÁÇπÊòØÊù•Ê∫ê‰∫éÁé∞ÊúâÁöÑÊ®°ÂûãÔºåËøò‰ºöÊèê‰æõÂØπÂ∫îÁöÑËÆ∫ÊñáÂèäÂÖ∂‰∏≠ÊñáÁøªËØëÁâàÊú¨ÔºàÂÅáËÆæÂÉèFasterNet‰∏≠ÁöÑFasterBlockÔºå‰ºöÊèê‰æõÂ•ΩÂØπÂ∫îÁöÑpyÊñá‰ª∂„ÄÅÂéüËÆ∫ÊñáÂèäÂÖ∂‰∏≠ÊñáÁøªËØëÁâàÊú¨ÔºâÔºåÁî®Êà∑ÂèØ‰ª•Ê†πÊçÆ‰ªéÊú¨ËØæÁ®ãÂ≠¶‰π†Âà∞ÁöÑÁºùÂêàÊ®°ÂùóÔºà‰ª£ÊåáÁ¨¨‰∏ÄÁÇπÔºâÂéªÂÆöÂà∂ÊàñËÄÖÂàõÊñ∞Ëá™Â∑±ÁöÑÁΩëÁªú„ÄÇ\n4. ÈôÑÂ∏¶Á≠îÁñëÁæ§ÔºåÁ≠îÁñëÁæ§‰∏ªË¶ÅÁ≠îÁñëÁöÑÂÜÖÂÆπÊòØÂÆûÈ™å„ÄÅ‰ª£Á†ÅÊìç‰Ωú„ÄÅ‰ª£Á†ÅÊä•ÈîôÁ≠âÁõ∏ÂÖ≥ÈóÆÈ¢ò(ÁªèËøáYOLO„ÄÅRTDETRÂ§ßÈáèÁöÑÁªèÈ™åÔºåÊàëÊ≤°Ê≥ï‰øùËØÅÊØè‰∏Ä‰∏™ÈóÆÈ¢òÈÉΩËÉΩÂõûÂ§çÂà∞Â§ßÂÆ∂ÔºåÂè™ËÉΩ‰øùËØÅÈÅáÂà∞ËøáÁöÑÈóÆÈ¢ò‰ºöÁªôÂ§ßÂÆ∂Êèê‰æõÂª∫ËÆÆÂíåÊñπÂêëÔºåÂΩìÁÑ∂Áæ§ÂÜÖÁöÑ‰∏Ä‰∫õÈ´òÈ¢ëÈóÆÈ¢òÔºåÊàë‰πü‰ºöÊî∂ÈõÜËµ∑Êù•ÊåëÂá∫ÈÉ®ÂàÜÂá∫ËßÜÈ¢ëÊàñËÄÖÁõ¥Êí≠ÁªôÂ§ßÂÆ∂ËøõË°åËß£Á≠î)„ÄÇ\n5. Â¶ÇÊûúÂêéÁª≠ÊúâÂâ™Êûù„ÄÅËí∏È¶èÔºå‰∏çÈúÄË¶ÅÈ¢ùÂ§ñ‰ªòË¥πÔºåÊú¨È°πÁõÆ‰ºöÂåÖÂê´Âú®ÂÜÖÔºåÊâÄ‰ª•ÊÄß‰ª∑ÊØîÁúüÁöÑÈùûÂ∏∏È´òÔºåYOLOÊîπËøõÂâ™ÊûùËí∏È¶è‰∏â‰ª∂Â•ó‰πüË¶Å200Â§ö‰∫Ü„ÄÇ\n\n### 3. Áõ¥Êí≠ÂÜÖÂÆπ\n\n1. Ëß£Á≠îÁæ§ÂÜÖ‰∏Ä‰∫õÈ´òÈ¢ëÁñëÈóÆÔºåÊØîÂ¶ÇÂæàÂ§ö‰∫∫ÈÉΩ‰ºöÈÅáÂà∞ÁöÑÊä•Èîô„ÄÅÊàñËÄÖÊ≥®ÊÑèÁÇπ„ÄÇ\n2. ÊïôÂ§ßÂÆ∂Â¶Ç‰ΩïÂéªÂÅö‰∫åÊ¨°ÂàõÊñ∞(PS:Ëøô‰∏™‰∏çÊòØÂè£Â§¥ÁªôÂ§ßÂÆ∂ËØ¥ÊÄé‰πà‰∫åÊ¨°ÂàõÊñ∞ÔºåËÄåÊòØ‰ªé‰ª£Á†ÅÁöÑÂ±ÇÈù¢Â∏¶Â§ßÂÆ∂ÂéªÂÆûË∑µ‰∫åÊ¨°ÂàõÊñ∞„ÄÇÂèØËÉΩËøôÈáå‰ºöÊúâÂêåÂ≠¶ÈóÆÔºåÈÇ£Ëá™Á†îÂàõÊñ∞Âë¢Ôºü‰Ω†‰ºöËá™Á†îÊ®°ÂùóÁöÑÂâçÊèêÊòØÂøÖÈ°ªË¶ÅÊáÇÂ¶Ç‰Ωï‰∫åÊ¨°ÂàõÊñ∞ÔºåÈ¶ñÂÖàËøôÊòØ‰∏Ä‰∏™ËøáÁ®ãÔºåÁÑ∂ÂêéÊàëÊúâÂæàÂ§öËá™Á†îÊ®°ÂùóÊòØÁ™ÅÁÑ∂ÊúâÁöÑÊÉ≥Ê≥ïÊàñËÄÖÁúãËÆ∫ÊñáÁúãÂà∞Êüê‰∫õÁªìÊûÑ‰∏é‰πãÂâçÁúãÂà∞ÁöÑËÆ∫ÊñáËÅîÂêàÂêéÊúâÊñ∞ÁöÑÊÉ≥Ê≥ïÔºåÊâÄ‰ª•‰πüÂæàÈöæÊèèËø∞Êàë‰∏∫‰ªÄ‰πàÂ∞±ÊÉ≥Âà∞Ëøô‰∏™ÁªìÊûÑÔºåÂ§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÔºåÂè™ÈúÄË¶Å‰ºöÊúâ‰∏ÄÂÆöÂ§çÊùÇÂ∫¶ÁöÑ‰∫åÊ¨°ÂàõÊñ∞Â∞±Ë∂≥Â§üÔºåÂΩìÁÑ∂Ëá™Á†îÊ®°ÂùóÊúâÊú∫‰ºöÊàë‰πü‰ºöÂéªËÆ≤)\n3. ÁªôÂ§ßÂÆ∂‰ªéÊµÖÂà∞Ê∑±Ëß£ËØ¥‰∏Ä‰∫õÊàëËÆ§‰∏∫ÊØîËæÉÁªèÂÖ∏ÁöÑÊ®°ÂùóÔºåÊèêÈ´òËá™Â∑±ËÉΩÂàõÊñ∞Êñ∞Ê®°ÂùóÁöÑËÉΩÂäõÂíåÂü∫Á°ÄÔºåÂõ†‰∏∫ÂæàÂ§öÊ®°ÂùóÈÉΩÊòØÁõ∏ÈÄöÁöÑÔºåÊú¨Ë¥®Ê≤°ÊúâÂèòÔºåÂè™ÊòØÊ®°Âùó‰∏äÁöÑÁªÑÂêà‰ΩìÊõøÊç¢„ÄÇ(Êúâ‰∏çÂ∞ë‰∫∫ÁßÅËÅäÊàëËØ¥ÔºåËÉΩ‰∏çËÉΩÂá∫‰∫õ‰Ω†ÊòØÂ¶Ç‰ΩïÁªìÂêà‰∏Ä‰∫õÁé∞ÊúâÁöÑÊ®°ÂùóÂéªÂàõÊñ∞ÁöÑÔºåËôΩÁÑ∂Áé∞Âú®BÁ´ô‰∏ä‰πüÊúâ‰∏çÂ∞ëËÆ≤ÂàõÊñ∞ÁÇπÁöÑÔºå‰ΩÜÊòØ‰ªñ‰ª¨ÁöÑÊÑüËßâÂ∞±ÊòØ‰ªéÂ§¥Âà∞Â∞æËØª‰∏ÄÁØá‰ª£Á†ÅÔºåÊàëÁúã‰∫ÜÂá†Ê¨°‰πãÂêéËßâÂæóÊàëÊää‰ª£Á†ÅÊâîÁªôGPTÁªôÊàëÊâì‰∏äÊ≥®ÈáäÁöÑÊÑüËßâÊòØ‰∏ÄÊ†∑ÁöÑÔºåÁúãÁöÑÊó∂ÂÄôÊÑüËßâÂì¶Âì¶Âì¶ËøôÊ†∑ÔºåÁúãÂÆåÂêéÂ∞±‰∏çÁü•ÊâÄÁÑ∂)\n\n### 3. ÂÖ•ÊâãÊú¨È°πÁõÆÈúÄË¶ÅÊ≥®ÊÑè‰∫õ‰ªÄ‰πàÔºü\n\n1. Âõ†‰∏∫Êú¨È°πÁõÆÂÆåÂÖ®‰∏çÊòØÂÉè‰πãÂâçYOLOÈ°πÁõÆËøôÊ†∑ÂÇªÁìúÂºèÊìç‰ΩúÔºåÊâÄ‰ª•Êú¨È°πÁõÆÊúâ‰∏ÄÂÆöÈöæÂ∫¶ÔºåÂÖ∑Êúâ‰ª•‰∏ãÁâπÂæÅÁöÑÂ∞è‰ºô‰º¥‰∏çÂª∫ËÆÆÂÖ•Êâã„ÄÇÔºàÁúãÂà∞ËøôÈáåÂèØËÉΩÊúâ‰∫∫‰ºöÈóÆÔºå‰∏∫‰ªÄ‰πà‰∏çËÄÉËôëÊääDEIM„ÄÅDFine„ÄÅRTDETRV2ÈÉΩÁßªÊ§çÂà∞UltralyticsÔºüÂõ†‰∏∫Ëøô‰∏™‰∏çÁ°ÆÂÆöÊÄßÂ§™Â§ßÔºåDETRÁ±ªÂûãÁöÑÊ®°ÂûãÂØπÂèÇÊï∞ÈùûÂ∏∏ÊïèÊÑüÔºåÂèØËÉΩÊúâ‰∏ÄÁÇπÂèÇÊï∞‰∏çÂêàÈÄÇÔºåÊïàÊûúÂ∞±‰ºöÂ§ßÊâìÊäòÊâ£Ôºå‰ΩÜÊòØÂØπ‰∫éËøôÁßçËæÉ‰∏∫Â§çÊùÇÁöÑÊ®°ÂûãÁßªÊ§çËøáÁ®ã‰∏≠ÂèàÂæàÈöæ‰øùËØÅ‰∏ÄÊØî‰∏ÄÂÖ®ËøáÁ®ãÁßªÊ§çÔºâ \n- Êú™ÂÖ•Èó®„ÄÅ100%Á∫ØÂ∞èÁôΩ(Â¶ÇÊûú‰Ω†ÊúâÂøÉÂ≠¶ÔºåËøô‰∏™‰∏çÊòØÈóÆÈ¢ò)\n- ‰∏çÂ§™ÊÉ≥Ëä±Â§™Â§öÊó∂Èó¥ÂéªÂ≠¶ÔºåÊêûËøô‰∏™Âè™ÊòØÊÉ≥‰∏∫‰∫ÜÊ∞¥‰∏™Êó†Ë¶ÅÊ±ÇÁöÑËÆ∫ÊñáÂ∞±Ë°å\n- Ê≤°Êúâ‰ªª‰ΩïËß£ÂÜ≥ÈóÆÈ¢òÁöÑËÉΩÂäõ(Â¶ÇÊûú‰Ω†ÊúâÂøÉÂ≠¶ÔºåËøô‰∏™‰∏çÊòØÈóÆÈ¢ò)\n- ‰ªéÊù•‰∏çÁúã‰ΩøÁî®ÊñáÊ°£„ÄÅËØ¥Êòé‰πãÁ±ªÁöÑ(Âº∫ÁÉà‰∏çÂª∫ËÆÆÂÖ•Êâã)  \n- Ê≠§È°πÁõÆ‰∏äÊâãÈúÄË¶ÅÊó∂Èó¥ÔºåÂ¶ÇÊûúÊÉ≥Êó†ËÑëÁõ¥Êé•Ë∑ëÂ∞±‰∏çÂêàÈÄÇË¥≠ÂÖ•  \nÊúÄÂêéË°•ÂÖÖÔºÅÂ¶ÇÊûú‰Ω†ÂÖ∑Êúâ‰ª•‰∏äÁâπÂæÅÔºå‰ΩÜÂèàË¶ÅÊ±ÇÊúüÂàä‰∏çËÉΩÂ§™Ê∞¥ÊàñËÄÖ‰∏çËÉΩÂÅöyoloÁöÑÈóÆÈ¢òÔºåÂ∞ΩÊó©ÂÖ•ÊâãCVPR2024-RTDETRÂêßÔºåÂéªÂπ¥Ê≤°Êäì‰∏äÔºå‰ªäÂπ¥‰∏çËÉΩÂÜçÁ≠â‰∫ÜÔºåÊ®°ÂûãÁ∫¢Âà©ÂèØ‰∏çÁ≠â‰∫∫„ÄÇ\n2. ÂÖ•ÊâãÂâçÂèØ‰ª•ÂÖàÂéªBÁ´ôÁúã‰∏Ä‰∏ã[CVPR025-DEIMÂêàÈõÜÈáåÈù¢ÁöÑÊïôÁ®ã](https://space.bilibili.com/286900343/lists/4909499)ÔºåÊúÄËµ∑Á†ÅÂÖàË∑ëÈÄöËøáDEIMÂéüÂßãÊ®°ÂûãÔºåËÉΩË∑üÁùÄËßÜÈ¢ëËÆ≠ÁªÉÂíåÊµãËØïÔºåÁÑ∂Âêé‰πüÊääÂêàÈõÜÈáåÈù¢ÁöÑÂü∫Á°ÄËØæÁ®ãÈÉΩÂÖàÁúã‰∏Ä‰∏ãÔºå‰∏∫ÂêéÈù¢ÊâìÂ•ΩÂü∫Á°Ä„ÄÇ\n3. ÊàëËÆ§‰∏∫Ëøô‰∏™‰∏çÊòØ‰ªÄ‰πà‰∏çÂèØËææÂà∞ÁöÑ‰∫ãÔºåÂ∞±Áúã‰Ω†ÊÉ≥‰∏çÊÉ≥ÊØï‰∏ö‰∫ÜÔºåÊúâÂøóËÄÖ‰∫ãÁ´üÊàê„ÄÇ\nPS:20250614ÁâàÊú¨Êõ¥Êñ∞ÂêéÔºåÊú¨È°πÁõÆÁöÑdfineÂíådeimÂ∑≤ÁªèÊîØÊåÅUltralyticsÂêåÊ¨æÁöÑÈÖçÁΩÆÊñá‰ª∂ÂΩ¢ÂºèÔºåÂ§ßÂ§ßÈôç‰Ωé‰∏äÊâãÈöæÂ∫¶ÔºÅ[BÁ´ô‰ªãÁªçÈìæÊé•](https://www.bilibili.com/video/BV1Q4MHzXEdd/)\n\n### 4. ‰ª∑Ê†º\n\n1. Êú¨È°πÁõÆ‰ª∑Ê†º‰∏∫268ÔºåÊ≤°ÊúâÊó∂ÊïàÈôêÂà∂„ÄÇÔºà‰∏éÂÖ∂150„ÄÅ200‰π∞‰∏™YOLOÁ∫ØÊ®°ÂûãÊîπËøõ‰∏ìÊ†èÔºå‰∏çÂ¶Ç268‰π∞‰∏™2025-SOTA‰∏ìÊ†èÔºåÊúÄËµ∑Á†Å‰∏çÁî®ÊÄïËä±‰∫ÜÈí±ÔºåÊúÄÂêéÂÅöÁöÑYOLOËøòÊäï‰∏çÂá∫ÂéªÔºåËøòÊØï‰∏ç‰∫Ü‰∏öÔºâ\n2. ËôöÊãüÈ°πÁõÆ‰∏ÄÁªèÂîÆÂá∫‰∏çÈÄÄ‰∏çÊç¢ÔºåÈúÄË¶ÅÂÖ•ÊâãÂâçËÄÉËôëÊ∏ÖÊ•öÔºåÂ¶ÇÊûú‰Ω†ÊòØÂàùÊ¨°ÂÖ•ÊâãÊàëÁöÑÈ°πÁõÆÔºåÊÄïÊàë‰∏çÈù†Ë∞±ÔºåÂèØ‰ª•ÂÖàËÄÉËôëÂÖ•Êâã‰∏™YOLOÂíåRTDETRÁúã‰∏ã„ÄÇ\n\n### 5. È°πÁõÆ‰ΩøÁî®ÈóÆÈ¢ò\n\n1. Ë¥≠‰π∞Êú¨È°πÁõÆÁöÑ‰ΩøÁî®ËÄÖÈÉΩ‰ºöÂæóÂà∞‰∏Ä‰∏™Áã¨‰∏ÄÊó†‰∫åÁöÑÁî®‰∫éËß£Âéã7zÁöÑÂØÜÁ†ÅÔºåÂà∞Êó∂ÂÄôÁî®‰∫éËß£ÂéãÂØπÂ∫îÁöÑÂéãÁº©ÂåÖÔºåÊ≠§ÂØÜÁ†ÅËá™Â∑±Â¶•ÂñÑ‰øùÁÆ°ÔºåËØ∑ÂãøÂëäËØâ‰ªñ‰∫∫„ÄÇ\n2. Êú¨È°πÁõÆÁöÑËßÜÈ¢ëÂíåÁõ¥Êí≠ÂõûÊîæÁªü‰∏ÄÈÉΩÊòØÂä†ÂØÜËßÜÈ¢ëÔºåÊØè‰∏™Ë¥≠‰π∞ËÄÖÈÉΩÂèØ‰ª•ÂæóÂà∞‰∏Ä‰∏™ÊøÄÊ¥ªÁ†ÅÔºåÊøÄÊ¥ªÁ†ÅÂú®ÊØè‰∏™‰∫∫‰∏ìÂ±ûÁöÑ7zÂéãÁº©Êñá‰ª∂ÂÜÖ„ÄÇ\n\n### 6. È°πÁõÆÊõ¥Êñ∞ÂÖ¨Âëä\n\n- 20250330\n\n    1. ÂàùÁâàÈ°πÁõÆÂèëÂ∏É.\n\n- 20250413\n\n    1. Êñ∞Â¢ûÂ§ö‰∏™ÊîπËøõÊ®°ÂùóÂπ∂Êñ∞Â¢ûÊ®°ÂùóÁÆÄ‰ªãÔºå‰ΩçÁΩÆÂú®engine/extre_module/module_imagesÂÜÖ„ÄÇ\n    2. Êñ∞Â¢ûËÆ≠ÁªÉÂíåÊµãËØïÈò∂ÊÆµÁöÑËøõÂ∫¶Êù°ÊòæÁ§∫„ÄÇ\n    3. ‰ºòÂåñtensorboard‰∏≠ÁöÑÁ≤æÂ∫¶ÂêçÁß∞ÊòæÁ§∫„ÄÇ\n    4. ‰ºòÂåñËæìÂá∫ÔºåÊääÈáçË¶Å‰ø°ÊÅØÊç¢È¢úËâ≤ÊòæÁ§∫„ÄÇ\n    5. Êñ∞Â¢ûplot_train_batch_freqÂèÇÊï∞ÔºåÁî®‰∫éÊéßÂà∂Èó¥ÈöîÂ§öÂ∞ëepoch‰øùÂ≠òÁ¨¨‰∏Ä‰∏™batch‰∏≠ÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÂêéÁöÑÂõæÂÉèÔºåÈªòËÆ§‰∏∫12„ÄÇ\n    6. Êñ∞Â¢û‰øùÂ≠òÂΩìÂâçÂèÇÊï∞‰ø°ÊÅØÔºå‰ºöËá™Âä®‰øùÂ≠òÂà∞output_dir‰∏≠ÁöÑargs.jsonÊñá‰ª∂ÂÜÖ„ÄÇ\n    7. ‰ºòÂåñoutput_dir‰øùÂ≠òÈÄªËæëÔºåÂΩìÂà§Êñ≠output_dirË∑ØÂæÑÂ≠òÂú®ÁöÑÊó∂ÂÄôÔºå‰ºöËá™Âä®Âú®ÂêéÁºÄÂä†1ÔºåÈÅøÂÖçË¶ÜÁõñÂéüÂÖà‰ª£Á†Å„ÄÇ\n\n- 20250419\n\n    1. Êñ∞Â¢ûverbose_typeÂèÇÊï∞ÔºåÁî®‰∫éÊéßÂà∂‰ΩøÁî®ÈªòËÆ§ËøòÊòØËøõÂ∫¶Êù°ËæìÂá∫ÔºåÈªòËÆ§‰∏∫ÂÆòÊñπÈªòËÆ§ËæìÂá∫ÂΩ¢Âºè„ÄÇ\n    2. Êñ∞Â¢ûthopËÆ°ÁÆóÊ®°ÂûãËÆ°ÁÆóÈáèÊñπÂºèÔºåÈÅøÂÖçcalflopsÂØπ‰∫éÈÉ®ÂàÜÁÆóÂ≠êÂá∫Áé∞‰∏çÊîØÊåÅÊä•ÈîôÁöÑÊìç‰Ωú„ÄÇ\n    3. ÂÆåÂñÑÊØè‰∏™Ê®°ÂùóÁöÑpyÊñá‰ª∂ÔºåÂ¢ûÂä†ËæìÂá∫ËÆ°ÁÆóÈáèÂíåÂèÇÊï∞ÈáèÁ≠âÊï∞ÂÄºÔºåÊñπ‰æøÁî®Êà∑ÂêéÁª≠Ë∞ÉËØï„ÄÇ\n    4. ÁªôDataLoader‰∏≠Ê∑ªÂä†pin_memoryÂèÇÊï∞‰∏∫TrueÔºåÂèØ‰ª•Âú®ËÆ≠ÁªÉÊó∂ÂÄôÂ¶ÇÊûúÊòØÊï∞ÊçÆÂä†ËΩΩÊàê‰∏∫Áì∂È¢àÔºåÂèØ‰ª•ÊèêÈ´òÈÄüÂ∫¶„ÄÇ\n    5. ‰øÆÂ§çÁî®Êà∑ÂèçÈ¶àÁöÑÂ∑≤Áü•ÈóÆÈ¢ò„ÄÇ\n    6. Êñ∞Â¢ûÂ§ö‰∏™ÊîπËøõÊ®°Âùó„ÄÇ\n\n- 20250429\n\n    1. ‰øÆÂ§çengine/extre_module/custom_nn/attention/SEAM.pyÊ®°ÂùóÔºåÂ∫îËØ•ÊòØMutilSEAM„ÄÇ\n    2. Êñ∞Â¢û‰∏Ä‰∫õËøõÈò∂ËØæÁ®ãÁöÑËßÜÈ¢ë„ÄÇ\n    3. Êñ∞Â¢ûÂ§ö‰∏™ÊîπËøõÊ®°Âùó„ÄÇ\n    4. ‰øÆÂ§çÁî®Êà∑ÂèçÈ¶àÁöÑÂ∑≤Áü•ÈóÆÈ¢ò„ÄÇ\n    5. ‰øÆÂ§çÁª≠ËÆ≠Êó∂ÂÄô‰ºöÊñ∞Â¢û‰∏Ä‰∏™‰øùÂ≠òË∑ØÂæÑÁöÑÈóÆÈ¢ò„ÄÇ\n    6. ‰øÆÂ§çÂ§öÂç°ËÆ≠ÁªÉStage2ÁöÑÊó∂ÂÄô‰ºöÂá∫Áé∞ÈÉ®ÂàÜËøõÁ®ãÊâæ‰∏çÂà∞ÊùÉÈáçÊñá‰ª∂ÁöÑÈóÆÈ¢ò„ÄÇ\n\n- 20250514\n\n    1. Êñ∞Â¢û‰∏Ä‰∫õËøõÈò∂ËØæÁ®ãÁöÑËßÜÈ¢ë„ÄÇ\n    2. Êñ∞Â¢ûÂ§ö‰∏™ÊîπËøõÊ®°Âùó„ÄÇ\n    3. ‰øÆÂ§çÁî®Êà∑ÂèçÈ¶àÁöÑÂ∑≤Áü•ÈóÆÈ¢ò„ÄÇ\n\n- 20250526\n\n    1. Êñ∞Â¢û‰∏Ä‰∫õËøõÈò∂ËØæÁ®ãÁöÑËßÜÈ¢ë„ÄÇ\n    2. Êñ∞Â¢ûÂ§ö‰∏™ÊîπËøõÊ®°Âùó„ÄÇ\n    3. Êñ∞Â¢ûcache_ramÂèÇÊï∞ÔºåËØ¶ÁªÜÂèØ‰ª•Áúãuserguide„ÄÇ\n    4. ‰øÆÂ§çÂú®torch2.7.0‰∏ãÂá∫Áé∞ÁöÑNotImplementedErrorÈóÆÈ¢ò„ÄÇ\n\n- 20250609\n\n    1. ‰øÆÂ§çÊñ∞Â¢û‰∫Ücache_ramÂäüËÉΩÂêéËÆ≠ÁªÉCOCOÊï∞ÊçÆÈõÜÁ≤æÂ∫¶‰∏çÊ≠£Â∏∏ÁöÑÈóÆÈ¢ò„ÄÇ\n    2. ‰øÆÂ§çÂú®ËÆ≠ÁªÉCOCOÊï∞ÊçÆÈõÜ‰∏≠Êï∞ÊçÆÂ¢ûÂº∫ÁöÑÁªòÂà∂BUG„ÄÇ\n    3. Êñ∞Â¢ûÂ§ö‰∏™ÊîπËøõÊ®°Âùó„ÄÇ\n    4. Êñ∞Â¢û‰∏Ä‰∫õËøõÈò∂ËØæÁ®ãÁöÑËßÜÈ¢ë„ÄÇ\n    5. ‰øÆÂ§çÁî®Êà∑ÂèçÈ¶àÁöÑÂ∑≤Áü•ÈóÆÈ¢ò„ÄÇ\n\n- 20250614\n\n    1. Êñ∞Â¢ûUltralyticsÁöÑÈÖçÁΩÆÊñá‰ª∂ÊñπÂºèÔºåÂ§ßÂ§ßÈôç‰ΩéÊîπËøõÈöæÂ∫¶„ÄÇ\n    2. Êñ∞Â¢û‰∏Ä‰∫õ<UltralyticsÁöÑÈÖçÁΩÆÊñá‰ª∂ÊñπÂºè>ËøõÈò∂ËØæÁ®ãÁöÑËßÜÈ¢ë„ÄÇ\n    3. Êñ∞Â¢ûÂ§ö‰∏™ÊîπËøõÊ®°Âùó„ÄÇ\n\n- 20250617\n\n    1. ‰øÆÂ§çÈÖçÁΩÆÊñá‰ª∂‰∏≠Â±ÇÂ∫èÂè∑ÊúâËØØÁöÑÈóÆÈ¢ò„ÄÇ\n\n- 20250619\n\n    1. ‰øÆÂ§çÈÖçÁΩÆÊñá‰ª∂‰∏≠Â±ÇÂ∫èÂè∑ÊúâËØØÁöÑÈóÆÈ¢ò„ÄÇ\n    2. Êñ∞Â¢ûÂ§ö‰∏™ÊîπËøõÊ®°Âùó„ÄÇ\n    3. Êñ∞Â¢û‰∏Ä‰∫õ<UltralyticsÁöÑÈÖçÁΩÆÊñá‰ª∂ÊñπÂºè>ËøõÈò∂ËØæÁ®ãÁöÑËßÜÈ¢ë„ÄÇ\n\n- 20250625\n\n    1. ‰øÆÂ§çbest_stg2‰øùÂ≠òÂºÇÂ∏∏ÁöÑÈóÆÈ¢ò„ÄÇ\n    2. Êñ∞Â¢ûYOLOV13‰∏≠ÁöÑHyperACEÊ®°Âùó„ÄÇ\n    3. Êñ∞Â¢ûÂ§ö‰∏™ÂÖ≥‰∫é<UltralyticsÁöÑÈÖçÁΩÆÊñá‰ª∂ÊñπÂºè>ËøõÈò∂ËØæÁ®ãÁöÑËßÜÈ¢ë„ÄÇ\n\n- 20250705\n\n    1. Êñ∞Â¢ûÂ§ö‰∏™ÊîπËøõÊ®°Âùó„ÄÇ\n    2. Êñ∞Â¢ûÂ§ö‰∏™ÂÖ≥‰∫é<UltralyticsÁöÑÈÖçÁΩÆÊñá‰ª∂ÊñπÂºè>ËøõÈò∂ËØæÁ®ãÁöÑËßÜÈ¢ë„ÄÇ\n    3. Êñ∞Â¢û20250704Âü∫Á°ÄÁñëÈóÆËß£Á≠îÁõ¥Êí≠ÂõûÊîæÈìæÊé•„ÄÇ\n\n- 20250714\n\n    1. Êñ∞Â¢ûÂ§ö‰∏™ÊîπËøõÊ®°Âùó„ÄÇ\n    2. Êñ∞Â¢ûÂ§ö‰∏™ÂÖ≥‰∫é<UltralyticsÁöÑÈÖçÁΩÆÊñá‰ª∂ÊñπÂºè>ËøõÈò∂ËØæÁ®ãÁöÑËßÜÈ¢ë„ÄÇ\n    3. Êñ∞Â¢ûÂ∞èÁõÆÊ†áÊ£ÄÊµãÁΩëÁªúÊû∂ÊûÑ‰∏ìÈ¢ò‰∏ÄÁæ§ËØæÈ¢òÁõ¥Êí≠ÂõûÊîæ„ÄÇ\n\n- 20250726\n\n    1. Êñ∞Â¢ûÂú®test-onlyÁöÑÁä∂ÊÄÅ‰∏ãËæìÂá∫ÊØè‰∏™Á±ªÂà´ÁöÑ'mAP', 'mAP_50', 'mAP_75', 'mAP_s', 'mAP_m', 'mAP_l'„ÄÇ\n    2. Êñ∞Â¢ûÂ§ö‰∏™ÊîπËøõÊ®°Âùó„ÄÇ\n    3. ‰øÆÂ§çÁî®Êà∑ÂèçÈ¶àÁöÑÂ∑≤Áü•ÈóÆÈ¢ò„ÄÇ\n    4. Êñ∞Â¢û‰∏Ä‰∏™JSONÊ†ºÂºèÊï∞ÊçÆÈõÜËÑöÊú¨„ÄÇ(ËæìÂá∫Á±ªÂà´Êï∞ÂíåÁ±ªÂà´id„ÄÅËæìÂá∫ÊØè‰∏™Á±ªÂà´ÁöÑÂÆû‰æãÊï∞Èáè)\n\n### 7. ËøõÈò∂ËßÜÈ¢ëÊïôÁ®ã\n\n------------------------------------- ËøõÈò∂ÊïôÁ®ã -------------------------------------  \n\n-------------------------------------------- Âü∫Á°ÄÁñëÈóÆÁõ¥Êí≠ÂõûÊîæÂíå‰∏Ä‰∫õÂäüËÉΩÊÄßÊñá‰ª∂ÁöÑ‰ΩøÁî®ÊïôÁ®ã <ËøôÈÉ®ÂàÜÊ≤°ÊúâËßÇÁúãÈ°∫Â∫è> --------------------------------------------  \nÂÖ∂‰∏≠Âü∫Á°ÄÁñëÈóÆËß£Á≠îÁ≥ªÂàóÁöÑËßÜÈ¢ëÂèØ‰ª•Âú®Áæ§Êñá‰ª∂‰∏≠ÁöÑÁæ§Âú®Á∫øÊñáÊ°£Êü•ÁúãËøôÊúüËÆ≤‰∫Ü‰ªÄ‰πàÈóÆÈ¢ò„ÄÇ\n1. 20250402Âü∫Á°ÄÁñëÈóÆËß£Á≠îÁõ¥Êí≠ÂõûÊîæ\n2. 20250416Âü∫Á°ÄÁñëÈóÆËß£Á≠îÁõ¥Êí≠ÂõûÊîæ\n3. featuremapËÑöÊú¨‰ΩøÁî®ÊïôÁ®ã\n4. heatmapËÑöÊú¨‰ΩøÁî®ÊïôÁ®ã\n5. tools/inference/torch_inf.py\n6. 20250513Âü∫Á°ÄÁñëÈóÆËß£Á≠îÁõ¥Êí≠ÂõûÊîæÈìæÊé•\n7. 20250618Âü∫Á°ÄÁñëÈóÆËß£Á≠îÁõ¥Êí≠ÂõûÊîæÈìæÊé•\n8. 20250704Âü∫Á°ÄÁñëÈóÆËß£Á≠îÁõ¥Êí≠ÂõûÊîæÈìæÊé•\n9. Â∞èÁõÆÊ†áÊ£ÄÊµãÁΩëÁªúÊû∂ÊûÑ‰∏ìÈ¢ò‰∏ÄÁæ§ËØæÈ¢òÁõ¥Êí≠ÂõûÊîæÈìæÊé•\n\n-------------------------------------------- Âü∫Á°ÄÊïôÁ®ãË°•ÂÖÖÁâà (ËøôÈÉ®ÂàÜÂª∫ËÆÆÁêÜËß£ÂÆåÂü∫Á°ÄËØæÁ®ãÂÖ®ÈÉ®ÂÜçÁúã) --------------------------------------------  \n1. Â¶Ç‰ΩïÂø´ÈÄüÂæóÁü•ÊØè‰∏™ÊîπËøõÊ®°ÂùóÁöÑËæìÂÖ•ËæìÂá∫Ê†ºÂºèÁõ∏ÂÖ≥‰ø°ÊÅØ\n2. Ëøô‰∏™È°πÁõÆÂÜÖÁöÑÈÄöÈÅìÊï∞ÊòØÊÄé‰πà‰º†ÈÄíÁöÑÔºüÁâπÂà´ÊòØbackbone‰∏éencoderÈÉ®ÂàÜÔºåÂØπÂêéÁª≠ÁöÑÊîπËøõÂæàÈáçË¶ÅÔºÅ\n3. ‰∏ªÂπ≤ËøõÈò∂ÊîπËøõÊñπÊ°à‰∏Ä-ÁªôÊØè‰∏™stageËÆæÂÆö‰∏çÂêåÁöÑÂèÇÊï∞\n4. ‰∏ªÂπ≤ËøõÈò∂ÊîπËøõÊñπÊ°à‰∫å-ÁªôÊØè‰∏™stage‰ΩøÁî®‰∏çÂêåÁöÑÊîπËøõÁªìÊûÑ\n5. ÊîπËøõÊ®°ÂûãÂêéÂèÇÊï∞ÈáèÂíåËÆ°ÁÆóÈáèÂèòÂæóÈùûÂ∏∏Â§ßÊÄé‰πàÂäûÔºü‰∏∫‰ªÄ‰πà‰ºöËøôÊ†∑ÔºüÊÄé‰πàËß£ÂÜ≥Ôºü\n\n-------------------------------------------- ÁâπÊÆäÈÖçÁΩÆÊñá‰ª∂-ËøõÈò∂ÊïôÁ®ã(ËøôÈÉ®ÂàÜÂøÖÈ°ªË¶ÅÁúãÂÆåÁêÜËß£ÂÆåÂü∫Á°ÄËØæÁ®ãÂÖ®ÈÉ®ÊâçËÉΩÁúãÔºå‰∏çÁÑ∂ÁôæÂàÜÁôæ‰∏çÁúãÊáÇ) <ËøôÈÉ®ÂàÜÊ≤°ÊúâËßÇÁúãÈ°∫Â∫è>  --------------------------------------------  \n1. engine/extre_module/custom_nn/featurefusion/mpca.py\n2. Â¶Ç‰Ωï‰ΩøÁî®engine/extre_module/custom_nn/transformerÊîπËøõHybridEncoder‰∏≠ÁöÑTransformer\n3. engine/extre_module/custom_nn/module.pyÁöÑÊê≠ÁßØÊú®Á•ûÂô®(‰∏áÁâ©ÁöÜÂèØËûç)ÊïôÁ®ã-‰∏äÈõÜ\n3. engine/extre_module/custom_nn/module.pyÁöÑÊê≠ÁßØÊú®Á•ûÂô®(‰∏áÁâ©ÁöÜÂèØËûç)ÊïôÁ®ã-‰∏≠ÈõÜ\n4. engine/extre_module/custom_nn/module.pyÁöÑÊê≠ÁßØÊú®Á•ûÂô®(‰∏áÁâ©ÁöÜÂèØËûç)ÊïôÁ®ã-‰∏ãÈõÜ\n5. engine/extre_module/custom_nn/mlp/SEFN.py\n6. engine/extre_module/custom_nn/neck/FDPN.py\n\n-------------------------------------------- UltralyticsÈÖçÁΩÆÊñá‰ª∂ÁâàÊú¨ÊïôÁ®ã <Âç≥‰Ωø‰Ω†‰ΩøÁî®ÈÖçÁΩÆÊñá‰ª∂ÊñπÂºèÂéªË∑ë‰πüÂª∫ËÆÆBÁ´ôÁöÑÂü∫Á°ÄËßÜÈ¢ëÂíåÈ°πÁõÆÂÜÖÁöÑÂÖ∂‰ªñËßÜÈ¢ëÈÉΩÁúã‰∏Ä‰∏ãÔºåÂØπÂêéÈù¢ÂÅö‰∫åÊ¨°ÂàõÊñ∞Êó∂ÂÄôÁöÑ‰ª£Á†ÅÊúâÂ∏ÆÂä©> --------------------------------------------\n1. ÂéüÂßãÈÖçÁΩÆÊñá‰ª∂ËÆ≤Ëß£\n2. ÊÄé‰πà‰ΩøÁî®È¢ÑËÆ≠ÁªÉÊùÉÈáç\n3. UltralyticsÈÖçÁΩÆÊñá‰ª∂ÁâàÊú¨ÁöÑÁÉ≠ÂäõÂõæÂíåÁâπÂæÅÂõæËÑöÊú¨‰ΩøÁî®ÊïôÁ®ã\n4. Ê≥®ÂÜåÊ®°ÂùóÁ§∫‰æãÊïôÁ®ã‰∏Ä\n5. Ê≥®ÂÜåÊ®°ÂùóÁ§∫‰æãÊïôÁ®ã‰∫å(Conv„ÄÅattentionÈÉ®ÂàÜ)\n6. Ê≥®ÂÜåÊ®°ÂùóÁ§∫‰æãÊïôÁ®ã‰∏â(module„ÄÅblockÈÉ®ÂàÜ)\n7. Ê≥®ÂÜåÊ®°ÂùóÁ§∫‰æãÊïôÁ®ãÂõõ(Êê≠ÁßØÊú®Á•ûÂô®(‰∏áÁâ©ÁöÜÂèØËûç)ÈÉ®ÂàÜ)\n8. Ê≥®ÂÜåÊ®°ÂùóÁ§∫‰æãÊïôÁ®ã‰∫î(HyperACE)\n9. Ê≥®ÂÜåÊ®°ÂùóÁ§∫‰æãÊïôÁ®ãÂÖ≠(featurefusionÈÉ®ÂàÜ)\n10. Ëá™Á†îÊ®°ÂùóFDPNÂú®ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑÂÆûÁé∞ËÆ≤Ëß£\n11. YOLO13‰∏≠ÁöÑHyperACEÂú®DEIM‰∏≠ÁöÑÂ∫îÁî®\n12. Ê≥®ÂÜåÊ®°ÂùóÁ§∫‰æãÊïôÁ®ã‰∏É(transformerÈÉ®ÂàÜ)\n13. Ê≥®ÂÜåÊ®°ÂùóÁ§∫‰æãÊïôÁ®ãÂÖ´(GOLD-YOLO)\n14. GOLO-YOLOÂú®DEIM‰∏≠ÁöÑÂ∫îÁî®(ÂåÖÂê´ÊÄé‰πàÂØπÂÖ∂‰∫åÊ¨°ÂàõÊñ∞)\n15. HS-FPNÊÄé‰πàËûçÂêàÂà∞DEIMÔºü\n16. ÊÄé‰πàÁî®module„ÄÅblock‰∏≠ÁöÑÊ®°ÂùóÂéªÊîπËøõ‰∏ªÂπ≤ÁΩëÁªúÔºü\n17. ‰ªé0Êê≠Âª∫‰∏Ä‰∏™yamlÔºÅ‰ª•CVPR2025-nnWNet‰∏∫‰æã.\n\n### 8. ÁõÆÂâçÂ∑≤ÊúâÁöÑÊ®°Âùó\n\n- engine/extre_module/custom_nn/attention \n\n    1. engine/extre_module/custom_nn/attention/SEAM.py\n    2. CVPR2021|engine/extre_module/custom_nn/attention/ca.py\n    3. ICASSP2023|engine/extre_module/custom_nn/attention/ema.py\n    4. ICML2021|engine/extre_module/custom_nn/attention/simam.py\n    5. ICCV2023|engine/extre_module/custom_nn/attention/lsk.py\n    6. WACV2024|engine/extre_module/custom_nn/attention/DeformableLKA.py\n    7. engine/extre_module/custom_nn/attention/mlca.py\n    8. BIBM2024|engine/extre_module/custom_nn/attention/FSA.py\n    9. AAAI2025|engine/extre_module/custom_nn/attention/CDFA.py\n    10. engine/extre_module/custom_nn/attention/GLSA.py\n    11. TGRS2025|engine/extre_module/custom_nn/attention/MCA.py\n\n- engine/extre_module/custom_nn/block\n\n    1. engine/extre_module/custom_nn/block/RepHMS.py\n    2. Ëá™Á†îÊ®°Âùó|engine/extre_module/custom_nn/block/rgcspelan.py\n    3. TPAMI2025|engine/extre_module/custom_nn/block/MANet.py\n\n- engine/extre_module/custom_nn/conv_module\n\n    1. CVPR2021|engine/extre_module/custom_nn/conv_module/dbb.py\n    2. IEEETIP2024|engine/extre_module/custom_nn/conv_module/deconv.py\n    3. ICCV2023|engine/extre_module/custom_nn/conv_module/dynamic_snake_conv.py\n    4. CVPR2023|engine/extre_module/custom_nn/conv_module/pconv.py\n    5. AAAI2025|engine/extre_module/custom_nn/conv_module/psconv.py\n    6. CVPR2025|engine/extre_module/custom_nn/conv_module/ShiftwiseConv.py\n    7. engine/extre_module/custom_nn/conv_module/wdbb.py\n    8. engine/extre_module/custom_nn/conv_module/deepdbb.py\n    9. ECCV2024|engine/extre_module/custom_nn/conv_module/wtconv2d.py\n    10. CVPR2023|engine/extre_module/custom_nn/conv_module/ScConv.py\n    11. engine/extre_module/custom_nn/conv_module/dcnv2.py\n    12. CVPR2024|engine/extre_module/custom_nn/conv_module/DilatedReparamConv.py\n    13. engine/extre_module/custom_nn/conv_module/gConv.py\n    14. CVPR2024|engine/extre_module/custom_nn/conv_module/IDWC.py\n    15. engine/extre_module/custom_nn/conv_module/DSA.py\n    16. CVPR2025|engine/extre_module/custom_nn/conv_module/FDConv.py\n    17. CVPR2023|engine/extre_module/custom_nn/conv_module/dcnv3.py\n    18. CVPR2024|engine/extre_module/custom_nn/conv_module/dcnv4.py\n    19. CVPR2024|engine/extre_module/custom_nn/conv_module/DynamicConv.py\n    20. CVPR2024|engine/extre_module/custom_nn/conv_module/FADC.py\n    21. CVPR2023|engine/extre_module/custom_nn/conv_module/SMPConv.py\n    22. MIA2025|engine/extre_module/custom_nn/conv_module/FourierConv.py\n    23. CVPR2024|engine/extre_module/custom_nn/conv_module/SFSConv.py\n    24. ICCV2025|engine/extre_module/custom_nn/conv_module/MBRConv.py\n\n- engine/extre_module/custom_nn/upsample\n\n    1. CVPR2024|engine/extre_module/custom_nn/upsample/eucb.py\n    2. CVPR2024|engine/extre_module/custom_nn/upsample/eucb_sc.py\n    3. engine/extre_module/custom_nn/upsample/WaveletUnPool.py\n    4. ICCV2019|engine/extre_module/custom_nn/upsample/CARAFE.py\n    5. ICCV2023|engine/extre_module/custom_nn/upsample/DySample.py\n\n- engine/extre_module/custom_nn/downsample\n\n    1. IEEETIP2020|engine/extre_module/custom_nn/downsample/gcnet.py\n    2. Ëá™Á†îÊ®°Âùó|engine/extre_module/custom_nn/downsample/lawds.py \n    3. engine/extre_module/custom_nn/downsample/WaveletPool.py\n    4. engine/extre_module/custom_nn/downsample/ADown.py\n    5. engine/extre_module/custom_nn/downsample/YOLOV7Down.py\n    6. engine/extre_module/custom_nn/downsample/SPDConv.py\n    7. engine/extre_module/custom_nn/downsample/HWD.py\n    8. engine/extre_module/custom_nn/downsample/DRFD.py\n\n- engine/extre_module/custom_nn/stem\n\n    1. engine/extre_module/custom_nn/stem/SRFD.py\n    2. engine/extre_module/custom_nn/stem/LoG.py\n\n- engine/extre_module/custom_nn/featurefusion\n\n    1. Ëá™Á†îÊ®°Âùó|engine/extre_module/custom_nn/featurefusion/cgfm.py\n    2. BMVC2024|engine/extre_module/custom_nn/featurefusion/msga.py\n    3. CVPR2024|engine/extre_module/custom_nn/featurefusion/mfm.py\n    4. IEEETIP2023|engine/extre_module/custom_nn/featurefusion/CSFCN.py\n    5. BIBM2024|engine/extre_module/custom_nn/featurefusion/mpca.py\n    6. ACMMM2024|engine/extre_module/custom_nn/featurefusion/wfu.py\n    7. CVPR2025|engine/extre_module/custom_nn/featurefusion/GDSAFusion.py\n    8. engine/extre_module/custom_nn/featurefusion/PST.py\n    9. TGRS2025|engine/extre_module/custom_nn/featurefusion/MSAM.py\n    10. INFFUS2025|engine/extre_module/custom_nn/featurefusion/DPCF.py\n\n- engine/extre_module/custom_nn/module\n\n    1. AAAI2025|engine/extre_module/custom_nn/module/APBottleneck.py\n    2. CVPR2025|engine/extre_module/custom_nn/module/efficientVIM.py\n    3. CVPR2023|engine/extre_module/custom_nn/module/fasterblock.py\n    4. CVPR2024|engine/extre_module/custom_nn/module/starblock.py\n    5. engine/extre_module/custom_nn/module/DWR.py\n    6. CVPR2024|engine/extre_module/custom_nn/module/UniRepLKBlock.py\n    7. CVPR2025|engine/extre_module/custom_nn/module/mambaout.py\n    8. AAAI2024|engine/extre_module/custom_nn/module/DynamicFilter.py\n    9. engine/extre_module/custom_nn/module/StripBlock.py\n    10. IEEETGRS2024|engine/extre_module/custom_nn/module/elgca.py\n    11. CVPR2024|engine/extre_module/custom_nn/module/LEGM.py\n    12. ICCV2023|engine/extre_module/custom_nn/module/iRMB.py\n    13. TPAMI2025|engine/extre_module/custom_nn/module/MSBlock.py\n    14. ICLR2024|engine/extre_module/custom_nn/module/FATBlock.py\n    15. CVPR2024|engine/extre_module/custom_nn/module/MSCB.py\n    16. engine/extre_module/custom_nn/module/LEGBlock.py\n    17. CVPR2025|engine/extre_module/custom_nn/module/RCB.py\n    18. ECCV2024|engine/extre_module/custom_nn/module/JDPM.py\n    19. CVPR2025|engine/extre_module/custom_nn/module/vHeat.py\n    20. CVPR2025|engine/extre_module/custom_nn/module/EBlock.py\n    21. CVPR2025|engine/extre_module/custom_nn/module/DBlock.py\n    22. ECCV2024|engine/extre_module/custom_nn/module/FMB.py\n    23. CVPR2024|engine/extre_module/custom_nn/module/IDWB.py\n    24. ECCV2022|engine/extre_module/custom_nn/module/LFE.py\n    25. AAAI2025|engine/extre_module/custom_nn/module/FCM.py\n    26. CVPR2024|engine/extre_module/custom_nn/module/RepViTBlock.py\n    27. CVPR2024|engine/extre_module/custom_nn/module/PKIModule.py\n    28. CVPR2024|engine/extre_module/custom_nn/module/camixer.py\n    29. ICCV2025|engine/extre_module/custom_nn/module/ESC.py\n    30. CVPR2025|engine/extre_module/custom_nn/module/nnWNet.py\n    31. TGRS2025|engine/extre_module/custom_nn/module/ARF.py\n\n- engine/extre_module/custom_nn/neck\n\n    1. Ëá™Á†îÊ®°Âùó|engine/extre_module/custom_nn/neck/FDPN.py\n\n- engine/extre_module/custom_nn/neck_module\n\n    1. TPAMI2025|engine/extre_module/custom_nn/neck_module/HyperCompute.py\n    2. engine/extre_module/custom_nn/neck_module/HyperACE.py\n    3. engine/extre_module/custom_nn/neck_module/GoldYOLO.py\n    4. AAAI2025|engine/extre_module/custom_nn/neck_module/HS_FPN.py\n\n- engine/extre_module/custom_nn/norm\n\n    1. ICML2024|engine/extre_module/custom_nn/transformer/repbn.py\n    2. CVPR2025|engine/extre_module/custom_nn/transformer/dyt.py\n\n- engine/extre_module/custom_nn/transformer\n\n    1. ICLR2025|engine/extre_module/custom_nn/transformer/PolaLinearAttention.py\n    2. CVPR2023|engine/extre_module/custom_nn/transformer/biformer.py\n    3. CVPR2023|engine/extre_module/custom_nn/transformer/CascadedGroupAttention.py\n    4. CVPR2022|engine/extre_module/custom_nn/transformer/DAttention.py\n    5. ICLR2022|engine/extre_module/custom_nn/transformer/DPBAttention.py\n    6. CVPR2024|engine/extre_module/custom_nn/transformer/AdaptiveSparseSA.py\n    7. engine/extre_module/custom_nn/transformer/GSA.py\n    8. engine/extre_module/custom_nn/transformer/RSA.py\n    9. ECCV2024|engine/extre_module/custom_nn/transformer/FSSA.py\n    10. AAAI2025|engine/extre_module/custom_nn/transformer/DilatedGCSA.py\n    11. AAAI2025|engine/extre_module/custom_nn/transformer/DilatedMWSA.py\n    12. CVPR2024|engine/extre_module/custom_nn/transformer/SHSA.py\n    13. IJCAI2024|engine/extre_module/custom_nn/transformer/CTA.py\n    13. IJCAI2024|engine/extre_module/custom_nn/transformer/SFA.py\n    14. engine/extre_module/custom_nn/transformer/MSLA.py\n\n- engine/extre_module/custom_nn/mlp\n\n    1. CVPR2024|engine/extre_module/custom_nn/mlp/ConvolutionalGLU.py\n    2. IJCAI2024|engine/extre_module/custom_nn/mlp/DFFN.py\n    3. ICLR2024|engine/extre_module/custom_nn/mlp/FMFFN.py\n    4. CVPR2024|engine/extre_module/custom_nn/mlp/FRFN.py\n    5. ECCV2024|engine/extre_module/custom_nn/mlp/EFFN.py \n    6. WACV2025|engine/extre_module/custom_nn/mlp/SEFN.py\n    7. ICLR2025|engine/extre_module/custom_nn/mlp/KAN.py\n    8. CVPR2025|engine/extre_module/custom_nn/mlp/EDFFN.py\n\n- engine/extre_module/custom_nn/mamba\n\n    1. AAAI2025|engine/extre_module/custom_nn/mamba/SS2D.py\n    2. CVPR2025|engine/extre_module/custom_nn/mamba/ASSM.py\n    3. CVPR2025|engine/extre_module/custom_nn/mamba/SAVSS.py\n    4. CVPR2025|engine/extre_module/custom_nn/mamba/MobileMamba/mobilemamba.py\n    5. CVPR2025|engine/extre_module/custom_nn/mamba/MaIR.py\n    6. TGRS2025|engine/extre_module/custom_nn/mamba/GLVSS.py\n    7. ICCV2025|engine/extre_module/custom_nn/mamba/VSSD.py\n    8. ICCV2025|engine/extre_module/custom_nn/mamba/TinyViM.py\n    9. INFFUS2025|engine/extre_module/custom_nn/mamba/CSI.py\n\n- ÁßØÊú®Ê®°Âùó,Á§∫‰æãÊïôÁ®ãengine/extre_module/custom_nn/module/example.py\n\n    1. YOLOV5|C3\n    2. YOLOV8|C2f\n    3. YOLO11|C3k2\n    4. TPAMI2025|MANet\n    5. TPAMI2024|MetaFormer_Block\n    6. TPAMI2024+CVPR2025|MetaFormer_Mona\n    7. TPAMI2024+CVPR2025+WACV2025|MetaFormer_SEFN\n    8. TPAMI2024+CVPR2025+WACV2025|MetaFormer_Mona_SEFN", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "cvpr2025-deim-project.md", "language": "markdown", "complexity": "complex", "star_count": 6481, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 386, "word_count": 627}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# data-offline-aug\n\n### ÁéØÂ¢É\n\n    pip install -i https://pypi.tuna.tsinghua.edu.cn/simple albumentations\n\n### 1. object_detection_data_aug.py\n\n    ÁõÆÊ†áÊ£ÄÊµãÊï∞ÊçÆÈõÜyoloÊ†ºÂºèÁ¶ªÁ∫øÊï∞ÊçÆÂ¢ûÂº∫ËÑöÊú¨.\n    ËßÜÈ¢ëÊïôÁ®ãÈìæÊé•:https://www.bilibili.com/video/BV1bT421k7iq/\n\n### 2. segment_data_aug.py\n\n    ËØ≠‰πâÂàÜÂâ≤Á¶ªÁ∫øÊï∞ÊçÆÂ¢ûÂº∫ËÑöÊú¨.\n    ËßÜÈ¢ëÊïôÁ®ãÈìæÊé•:https://www.bilibili.com/video/BV1xi421a7Gb/\n\n# Reference\nhttps://github.com/albumentations-team/albumentations  ", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "data-offline-aug/readme.md", "language": "markdown", "complexity": "simple", "star_count": 6481, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 18, "word_count": 22}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "_base_ = './yolox_tiny_8xb8-300e_coco.py'\n\n# Êàë‰ª¨ËøòÈúÄË¶ÅÊõ¥Êîπ head ‰∏≠ÁöÑ num_classes ‰ª•ÂåπÈÖçÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÁ±ªÂà´Êï∞\nmodel = dict(\n    bbox_head=dict(\n        num_classes=10\n    )\n)\n\n# ‰øÆÊîπÊï∞ÊçÆÈõÜÁõ∏ÂÖ≥ÈÖçÁΩÆ\n# dataset settings\ndata_root = '/home/hjj/Desktop/dataset/dataset_visdrone/'\ndataset_type = 'CocoDataset'\nmetainfo = {\n    'classes': ('pedestrian', 'people', 'bicycle', 'car', 'van', 'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor'),\n    # 'palette': [\n    #     (220, 20, 60),\n    # ]\n}\n\n# Example to use different file client\n# Method 1: simply set the data root and let the file I/O module\n# automatically infer from prefix (not support LMDB and Memcache yet)\n\n# data_root = 's3://openmmlab/datasets/detection/coco/'\n\n# Method 2: Use `backend_args`, `file_client_args` in versions before 3.0.0rc6\n# backend_args = dict(\n#     backend='petrel',\n#     path_mapping=dict({\n#         './data/': 's3://openmmlab/datasets/detection/',\n#         'data/': 's3://openmmlab/datasets/detection/'\n#     }))\nbackend_args = None\n\nimg_scale = (640, 640)  # width, height\n\ntrain_pipeline = [\n    dict(type='Mosaic', img_scale=img_scale, pad_val=114.0),\n    dict(\n        type='RandomAffine',\n        scaling_ratio_range=(0.1, 2),\n        # img_scale is (width, height)\n        border=(-img_scale[0] // 2, -img_scale[1] // 2)),\n    dict(\n        type='MixUp',\n        img_scale=img_scale,\n        ratio_range=(0.8, 1.6),\n        pad_val=114.0),\n    dict(type='YOLOXHSVRandomAug'),\n    dict(type='RandomFlip', prob=0.5),\n    # According to the official implementation, multi-scale\n    # training is not considered here but in the\n    # 'mmdet/models/detectors/yolox.py'.\n    # Resize and Pad are for the last 15 epochs when Mosaic,\n    # RandomAffine, and MixUp are closed by YOLOXModeSwitchHook.\n    dict(type='Resize', scale=img_scale, keep_ratio=True),\n    dict(\n        type='Pad',\n        pad_to_square=True,\n        # If the image is three-channel, the pad value needs\n        # to be set separately for each channel.\n        pad_val=dict(img=(114.0, 114.0, 114.0))),\n    dict(type='FilterAnnotations', min_gt_bbox_wh=(1, 1), keep_empty=False),\n    dict(type='PackDetInputs')\n]\n\ntrain_dataset = dict(\n    # use MultiImageMixDataset wrapper to support mosaic and mixup\n    type='MultiImageMixDataset',\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        metainfo=metainfo,\n        ann_file='VisDrone2019-DET-train/annotations/train.json',\n        data_prefix=dict(img='VisDrone2019-DET-train/images/'),\n        pipeline=[\n            dict(type='LoadImageFromFile', backend_args=backend_args),\n            dict(type='LoadAnnotations', with_bbox=True)\n        ],\n        filter_cfg=dict(filter_empty_gt=False, min_size=32),\n        backend_args=backend_args),\n    pipeline=train_pipeline)\n\ntest_pipeline = [\n    dict(type='LoadImageFromFile', backend_args=backend_args),\n    dict(type='Resize', scale=img_scale, keep_ratio=True),\n    dict(\n        type='Pad',\n        pad_to_square=True,\n        pad_val=dict(img=(114.0, 114.0, 114.0))),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PackDetInputs',\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor'))\n]\n\ntrain_dataloader = dict(\n    batch_size=16,\n    num_workers=8,\n    persistent_workers=True,\n    sampler=dict(type='DefaultSampler', shuffle=True),\n    dataset=train_dataset)\nval_dataloader = dict(\n    batch_size=16,\n    num_workers=8,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type='DefaultSampler', shuffle=False),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        metainfo=metainfo,\n        ann_file='VisDrone2019-DET-val/annotations/val.json',\n        data_prefix=dict(img='VisDrone2019-DET-val/images/'),\n        test_mode=True,\n        pipeline=test_pipeline,\n        backend_args=backend_args))\ntest_dataloader = dict(\n    batch_size=16,\n    num_workers=8,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type='DefaultSampler', shuffle=False),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        metainfo=metainfo,\n        ann_file='VisDrone2019-DET-test-dev/annotations/test.json',\n        data_prefix=dict(img='VisDrone2019-DET-test-dev/images/'),\n        test_mode=True,\n        pipeline=test_pipeline,\n        backend_args=backend_args))\n\nval_evaluator = dict(\n    type='CocoMetric',\n    ann_file=data_root + 'VisDrone2019-DET-val/annotations/val.json',\n    metric='bbox',\n    backend_args=backend_args)\ntest_evaluator = dict(\n    type='CocoMetric',\n    ann_file=data_root + 'VisDrone2019-DET-test-dev/annotations/test.json',\n    metric='bbox',\n    backend_args=backend_args)\n\ndefault_hooks = dict(logger=dict(type='LoggerHook', interval=200))\n\nload_form='yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth'\n\n# nohup python tools/train.py configs/yolox/yolox_tiny_8xb8-300e_visdrone.py > yolox-tiny-visdrone.log 2>&1 & tail -f yolox-tiny-visdrone.log\n# python tools/test.py configs/yolox/yolox_tiny_8xb8-300e_visdrone.py work_dirs/yolox_tiny_8xb8-300e_visdrone/epoch_300.pth --show --show-dir test_save\n# python tools/test.py configs/yolox/yolox_tiny_8xb8-300e_visdrone.py work_dirs/yolox_tiny_8xb8-300e_visdrone/epoch_300.pth --tta \n# python tools/analysis_tools/get_flops.py configs/yolox/yolox_tiny_8xb8-300e_visdrone.py", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "mmdet-course/config/yolox_tiny_8xb8-300e_visdrone.py", "language": "python", "complexity": "medium", "star_count": 6481, "quality_score": 0.85, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 154, "word_count": 375}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# mmdet‰ΩøÁî®ÊïôÁ®ã\n\n### mmdetÊïôÁ®ãÂëΩ‰ª§\n\n1. conda create -n mmdet_py39 python=3.9 anaconda\n2. https://mmdetection.readthedocs.io/en/latest/get_started.html\n3. https://pytorch.org/get-started/previous-versions/  \npip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n4. https://mmdetection.readthedocs.io/zh-cn/latest/user_guides/train.html#id7\n\n### mmdetËøêË°åÂëΩ‰ª§\n\n1. ËÆ≠ÁªÉ\n\n        python tools/train.py <your-config-file>\n2. ÊµãËØï  \n\n        python tools/test.py <your-config-file> <your-model-weights-file> --out <save-pickle-path>\n3. ËÆ°ÁÆóÈáè„ÄÅÂèÇÊï∞ÈáèËÆ°ÁÆóËÑöÊú¨  \n\n        python tools/analysis_tools/get_flops.py <your-config-file>\n4. Êé®ÁêÜÊó∂Èó¥„ÄÅfps„ÄÅgpu memoryËÆ°ÁÆóËÑöÊú¨  \n\n        python tools/analysis_tools/benchmark.py <your-config-file> --checkpoint <your-model-weights-file> --task inference --fuse-conv-bn\n5. ÁªòÂà∂Êõ≤Á∫øÂõæËÑöÊú¨  \n\n        python tools/analysis_tools/analyze_logs.py plot_curve <train-json-file> --keys <keys> --legend <legend> --out <save-path>\n6. ÁªìÊûúÂàÜÊûêËÑöÊú¨  \n\n        python tools/analysis_tools/analyze_results.py <your-config-file> <test-pickle-path> <save-path>\n\n### mmdetËßÜÈ¢ëÊïôÁ®ãÈìæÊé•(ÂèØÊåâÈ°∫Â∫èËßÇÁúã)\n\n1. [‰∏ÄÂ∫ìÊâìÂ∞ΩÁõÆÊ†áÊ£ÄÊµãÂØπÊØîÂÆûÈ™åÔºÅmmdetectionÁéØÂ¢É„ÄÅËÆ≠ÁªÉ„ÄÅÊµãËØïÊâãÊääÊâãÊïôÁ®ãÔºÅ](https://www.bilibili.com/video/BV1xA4m1c7H8/)\n2. [‰∏ÄÂ∫ìÊâìÂ∞ΩÁõÆÊ†áÊ£ÄÊµãÂØπÊØîÂÆûÈ™åÔºÅmmdetectionÂèÇÊï∞Èáè„ÄÅËÆ°ÁÆóÈáè„ÄÅFPS„ÄÅÁªòÂà∂logsÊâãÊääÊâãÊïôÁ®ã](https://www.bilibili.com/video/BV17C41137dW/)\n3. [‰∏ÄÂ∫ìÊâìÂ∞ΩÁõÆÊ†áÊ£ÄÊµãÂØπÊØîÂÆûÈ™åÔºÅmmdetectionÊåáÊ†áËΩ¨Êç¢YOLOÊåáÊ†áÔºÅ](https://www.bilibili.com/video/BV1AWtCesEc6/)\n\n### mmdetÂÆûÈ™åÊï∞ÊçÆ(ÊåáÊ†áÂùá‰∏∫COCOÊåáÊ†á)\n\n‰ª•‰∏ãÂÆûÈ™åÊï∞ÊçÆÁéØÂ¢É:  \npython:3.9.19  \ntorch:2.1.0+cu121  \ntorchvision:0.16.0  \nmmdet:3.3.0  \nmmcv:2.1.0  \nmmengine:0.10.3  \nÁ°¨‰ª∂ÁéØÂ¢É:  \nPlatform:Ubuntu  \nCPU:i7-12700K  \nRAM:32G  \nGPU:RTX3090  \n\n#### VisDrone2019-testset\n\n| model | Input Shape | GFlops | Params | coco/bbox_mAP | coco/bbox_mAP_50 | coco/bbox_mAP_s | coco/bbox_mAP_m | coco/bbox_mAP_l |\n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| Faster-RCNN-R50-FPN-CIOU | (768, 1344) | 208G | 41.39M | 0.194 | 0.329 | 0.095 | 0.309 | 0.429 |\n| Cascade-RCNN-R50-FPN | (768, 1344) | 236G | 69.29M | 0.197 | 0.326 | 0.099 | 0.309 | 0.406 |\n| ATSS-R50-FPN-DyHead | (768, 1344) | 110G | 38.91M | 0.204 | 0.338 | 0.100 | 0.317 | 0.485 |\n| TOOD-R50 | (768, 1344) | 199G | 32.04M | 0.204 | 0.339 | 0.102 | 0.317 | 0.403 |\n| DINO | (750, 1333) | 274G | 47.56M | 0.253 | 0.445 | 0.150 | 0.371 | 0.503 |\n| DDQ | (768, 1333) | - | - | 0.268 | 0.463 | 0.159 | 0.390 | 0.526 |\n| YOLOX-Tiny | (640, 640) | 7.578G | 5.035M | 0.148 | 0.278 | 0.076 | 0.221 | 0.278 |\n| GFL | (768, 1344) | 206G | 32.279M | 0.193 | 0.321 | 0.094 | 0.300 | 0.409 |\n| RTMDet-Tiny | (640, 640) | 8.033G | 4.876M | 0.184 | 0.312 | 0.077 | 0.288 | 0.445 |\n| RetinaNet-R50-FPN | (768, 1344) | 210G | 36.517M | 0.164 | 0.276 | 0.060 | 0.274 | 0.427 |", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "mmdet-course/readme.md", "language": "markdown", "complexity": "simple", "star_count": 6481, "quality_score": 0.73, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 66, "word_count": 337}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport numpy as np\nfrom prettytable import PrettyTable\nfrom ultralytics import YOLO\nfrom ultralytics.utils.torch_utils import model_info\n\n# BILIBILI UP È≠îÂÇÄÈù¢ÂÖ∑\n# È™åËØÅÂèÇÊï∞ÂÆòÊñπËØ¶Ëß£ÈìæÊé•Ôºöhttps://docs.ultralytics.com/modes/val/#usage-examples:~:text=of%20each%20category-,Arguments%20for%20YOLO%20Model%20Validation,-When%20validating%20YOLO\n\ndef get_weight_size(path):\n    stats = os.stat(path)\n    return f'{stats.st_size / 1024 / 1024:.1f}'\n\nif __name__ == '__main__':\n    model_path = 'runs/train/exp/weights/best.pt'\n    model = YOLO(model_path) # ÈÄâÊã©ËÆ≠ÁªÉÂ•ΩÁöÑÊùÉÈáçË∑ØÂæÑ\n    result = model.val(data='/root/dataset/dataset_visdrone/data.yaml',\n                        split='val', # splitÂèØ‰ª•ÈÄâÊã©train„ÄÅval„ÄÅtest Ê†πÊçÆËá™Â∑±ÁöÑÊï∞ÊçÆÈõÜÊÉÖÂÜµÊù•ÈÄâÊã©.\n                        imgsz=640,\n                        batch=16,\n                        project='runs/val',\n                        name='exp',\n                        )\n    \n    if model.task == 'detect': # ‰ªÖÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°ÈÄÇÁî®\n        length = result.box.p.size\n        model_names = list(result.names.values())\n        preprocess_time_per_image = result.speed['preprocess']\n        inference_time_per_image = result.speed['inference']\n        postprocess_time_per_image = result.speed['postprocess']\n        all_time_per_image = preprocess_time_per_image + inference_time_per_image + postprocess_time_per_image\n        \n        n_l, n_p, n_g, flops = model_info(model.model)\n        \n        print('-'*20 + 'ËÆ∫Êñá‰∏äÁöÑÊï∞ÊçÆ‰ª•‰ª•‰∏ãÁªìÊûú‰∏∫ÂáÜ' + '-'*20)\n        print('-'*20 + 'ËÆ∫Êñá‰∏äÁöÑÊï∞ÊçÆ‰ª•‰ª•‰∏ãÁªìÊûú‰∏∫ÂáÜ' + '-'*20)\n        print('-'*20 + 'ËÆ∫Êñá‰∏äÁöÑÊï∞ÊçÆ‰ª•‰ª•‰∏ãÁªìÊûú‰∏∫ÂáÜ' + '-'*20)\n        print('-'*20 + 'ËÆ∫Êñá‰∏äÁöÑÊï∞ÊçÆ‰ª•‰ª•‰∏ãÁªìÊûú‰∏∫ÂáÜ' + '-'*20)\n        print('-'*20 + 'ËÆ∫Êñá‰∏äÁöÑÊï∞ÊçÆ‰ª•‰ª•‰∏ãÁªìÊûú‰∏∫ÂáÜ' + '-'*20)\n\n        model_info_table = PrettyTable()\n        model_info_table.title = \"Model Info\"\n        model_info_table.field_names = [\"GFLOPs\", \"Parameters\", \"ÂâçÂ§ÑÁêÜÊó∂Èó¥/‰∏ÄÂº†Âõæ\", \"Êé®ÁêÜÊó∂Èó¥/‰∏ÄÂº†Âõæ\", \"ÂêéÂ§ÑÁêÜÊó∂Èó¥/‰∏ÄÂº†Âõæ\", \"FPS(ÂâçÂ§ÑÁêÜ+Ê®°ÂûãÊé®ÁêÜ+ÂêéÂ§ÑÁêÜ)\", \"FPS(Êé®ÁêÜ)\", \"Model File Size\"]\n        model_info_table.add_row([f'{flops:.1f}', f'{n_p:,}', \n                                  f'{preprocess_time_per_image / 1000:.6f}s', f'{inference_time_per_image / 1000:.6f}s', \n                                  f'{postprocess_time_per_image / 1000:.6f}s', f'{1000 / all_time_per_image:.2f}', \n                                  f'{1000 / inference_time_per_image:.2f}', f'{get_weight_size(model_path)}MB'])\n        print(model_info_table)\n\n        model_metrice_table = PrettyTable()\n        model_metrice_table.title = \"Model Metrice\"\n        model_metrice_table.field_names = [\"Class Name\", \"Precision\", \"Recall\", \"F1-Score\", \"mAP50\", \"mAP75\", \"mAP50-95\"]\n        for idx in range(length):\n            model_metrice_table.add_row([\n                                        model_names[idx], \n                                        f\"{result.box.p[idx]:.4f}\", \n                                        f\"{result.box.r[idx]:.4f}\", \n                                        f\"{result.box.f1[idx]:.4f}\", \n                                        f\"{result.box.ap50[idx]:.4f}\", \n                                        f\"{result.box.all_ap[idx, 5]:.4f}\", # 50 55 60 65 70 75 80 85 90 95 \n                                        f\"{result.box.ap[idx]:.4f}\"\n                                    ])\n        model_metrice_table.add_row([\n                                    \"all(Âπ≥ÂùáÊï∞ÊçÆ)\", \n                                    f\"{result.results_dict['metrics/precision(B)']:.4f}\", \n                                    f\"{result.results_dict['metrics/recall(B)']:.4f}\", \n                                    f\"{np.mean(result.box.f1[:length]):.4f}\", \n                                    f\"{result.results_dict['metrics/mAP50(B)']:.4f}\", \n                                    f\"{np.mean(result.box.all_ap[:length, 5]):.4f}\", # 50 55 60 65 70 75 80 85 90 95 \n                                    f\"{result.results_dict['metrics/mAP50-95(B)']:.4f}\"\n                                ])\n        print(model_metrice_table)\n\n        with open(result.save_dir / 'paper_data.txt', 'w+') as f:\n            f.write(str(model_info_table))\n            f.write('\\n')\n            f.write(str(model_metrice_table))\n        \n        print('-'*20, f'ÁªìÊûúÂ∑≤‰øùÂ≠òËá≥{result.save_dir}/paper_data.txt...', '-'*20)\n        print('-'*20, f'ÁªìÊûúÂ∑≤‰øùÂ≠òËá≥{result.save_dir}/paper_data.txt...', '-'*20)\n        print('-'*20, f'ÁªìÊûúÂ∑≤‰øùÂ≠òËá≥{result.save_dir}/paper_data.txt...', '-'*20)\n        print('-'*20, f'ÁªìÊûúÂ∑≤‰øùÂ≠òËá≥{result.save_dir}/paper_data.txt...', '-'*20)\n        print('-'*20, f'ÁªìÊûúÂ∑≤‰øùÂ≠òËá≥{result.save_dir}/paper_data.txt...', '-'*20)", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "objectdetection-tricks/tricks_14.py", "language": "python", "complexity": "medium", "star_count": 6481, "quality_score": 0.7899999999999999, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 85, "word_count": 248}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import warnings\nwarnings.filterwarnings('ignore')\nimport argparse\nimport logging\nimport math\nimport os\nimport random\nimport time\nimport sys\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom threading import Thread\n\nimport numpy as np\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport torch.utils.data\nimport yaml\nfrom torch.cuda import amp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom tqdm import tqdm\n\nfrom utils.torch_utils import select_device\nfrom models.common import DetectMultiBackend\n\ndef get_weight_size(path):\n    stats = os.stat(path)\n    return f'{stats.st_size / 1024 / 1024:.1f}'\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', type=str, default='', help='trained weights path')\n    parser.add_argument('--batch', type=int, default=1, help='total batch size for all GPUs')\n    parser.add_argument('--imgs', nargs='+', type=int, default=[640, 640], help='[height, width] image sizes')\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--warmup', default=200, type=int, help='warmup time')\n    parser.add_argument('--testtime', default=1000, type=int, help='test time')\n    parser.add_argument('--half', action='store_true', default=False, help='fp16 mode.')\n    opt = parser.parse_args()\n    \n    device = select_device(opt.device, batch_size=opt.batch)\n    \n    # Model\n    weights = opt.weights\n    pretrained = weights.endswith('.pt')\n    if pretrained:\n        model = DetectMultiBackend(weights, device=device)\n        print(f'Loaded {weights}')  # report\n    else:\n        assert weights.endswith('.pt'), \"compress need weights.\"\n    \n    example_inputs = torch.randn((opt.batch, 3, *opt.imgs)).to(device)\n    \n    if opt.half:\n        model = model.half()\n        example_inputs = example_inputs.half()\n    \n    print('begin warmup...')\n    for i in tqdm(range(opt.warmup), desc='warmup....'):\n        model(example_inputs)\n    \n    print('begin test latency...')\n    time_arr = []\n    \n    for i in tqdm(range(opt.testtime), desc='test latency....'):\n        if device.type == 'cuda':\n            torch.cuda.synchronize()\n        start_time = time.time()\n        \n        model(example_inputs)\n        \n        if device.type == 'cuda':\n            torch.cuda.synchronize()\n        end_time = time.time()\n        time_arr.append(end_time - start_time)\n    \n    std_time = np.std(time_arr)\n    infer_time_per_image = np.sum(time_arr) / (opt.testtime * opt.batch)\n    \n    print(f'model weights:{opt.weights} size:{get_weight_size(opt.weights)}M (bs:{opt.batch})Latency:{infer_time_per_image:.5f}s +- {std_time:.5f}s fps:{1 / infer_time_per_image:.1f}')", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "objectdetection-tricks/tricks_7.py", "language": "python", "complexity": "simple", "star_count": 6481, "quality_score": 0.85, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 83, "word_count": 249}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import warnings\nwarnings.filterwarnings('ignore')\nimport argparse\nimport logging\nimport math\nimport os\nimport random\nimport time\nimport sys\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom threading import Thread\n\nimport numpy as np\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport torch.utils.data\nimport yaml\nfrom torch.cuda import amp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\n\nfrom models.experimental import attempt_load\nfrom models.yolo import Model\nfrom utils.torch_utils import select_device\n\ndef get_weight_size(path):\n    stats = os.stat(path)\n    return f'{stats.st_size / 1024 / 1024:.1f}'\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', type=str, default='', help='trained weights path')\n    parser.add_argument('--batch', type=int, default=1, help='total batch size for all GPUs')\n    parser.add_argument('--imgs', nargs='+', type=int, default=[640, 640], help='[height, width] image sizes')\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--warmup', default=200, type=int, help='warmup time')\n    parser.add_argument('--testtime', default=1000, type=int, help='test time')\n    parser.add_argument('--half', action='store_true', default=False, help='fp16 mode.')\n    opt = parser.parse_args()\n    \n    device = select_device(opt.device, batch_size=opt.batch)\n    \n    # Model\n    weights = opt.weights\n    pretrained = weights.endswith('.pt')\n    if pretrained:\n        model = torch.load(weights, map_location=device)\n        if model['ema']:\n           model = model['ema'].float()\n        else:\n            model = model['model'].float()\n        model.fuse()\n        model.info(img_size=opt.imgs[0])\n        print(f'Loaded {weights}')  # report\n    else:\n        assert weights.endswith('.pt'), \"compress need weights.\"\n    \n    example_inputs = torch.randn((opt.batch, 3, *opt.imgs)).to(device)\n    \n    if opt.half:\n        model = model.half()\n        example_inputs = example_inputs.half()\n    \n    print('begin warmup...')\n    for i in tqdm(range(opt.warmup), desc='warmup....'):\n        model(example_inputs)\n    \n    print('begin test latency...')\n    time_arr = []\n    \n    for i in tqdm(range(opt.testtime), desc='test latency....'):\n        if device.type == 'cuda':\n            torch.cuda.synchronize()\n        start_time = time.time()\n        \n        model(example_inputs)\n        \n        if device.type == 'cuda':\n            torch.cuda.synchronize()\n        end_time = time.time()\n        time_arr.append(end_time - start_time)\n    \n    mean_time, std_time = np.mean(time_arr), np.std(time_arr)\n    \n    print(f'model weights:{opt.weights} size:{get_weight_size(opt.weights)}M Latency:{mean_time:.5f}s +- {std_time:.5f}s fps:{1 / mean_time:.1f}')", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "objectdetection-tricks/tricks_8.py", "language": "python", "complexity": "simple", "star_count": 6481, "quality_score": 0.85, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 90, "word_count": 263}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Object Detection Script\nËøô‰∏™È°πÁõÆ‰∏ªË¶ÅÊòØÊèê‰æõ‰∏Ä‰∫õÂÖ≥‰∫éÁõÆÊ†áÊ£ÄÊµãÁöÑ‰ª£Á†ÅÂíåÊîπËøõÊÄùË∑ØÂèÇËÄÉ.\n\n### [BiliBiliËßÜÈ¢ëÊåáÂçó](https://github.com/z1069614715/objectdetection_script/blob/master/bilibili-guide.md)\n\n# Project <ÈúÄË¶ÅÂÖ•ÊâãËØ∑Âä†‰ºÅÈπÖ1615905974/1069614715,Â¶ÇÊ∑ªÂä†‰∏ç‰∏äÂèØbilibiliÁßÅËÅäÁõ¥Âèë‰ºÅÈπÖÂè∑Á†Å,ÊúÄÂ•ΩÂ•ΩÂèãËØ∑Ê±Ç‰πüËÆæÁΩÆ‰∏çÈúÄË¶ÅÈ™åËØÅÂ∞±ÂèØ‰ª•Âä†‰∏ä>\n1. Âü∫‰∫éUltralyticsÁöÑyolov8„ÄÅyolov10ÊîπËøõÈ°πÁõÆ.(69.9¬•)\n    \n    [ÁõÆÂâçÂ∑≤ÊúâÁöÑÊîπËøõÊñπÊ°àÂíåÊõ¥Êñ∞ËØ¶ÁªÜÂÖ¨Âëä](https://github.com/z1069614715/objectdetection_script/blob/master/yolo-improve/yolov8v10-project.md)  \n    È°πÁõÆÁÆÄÂçï‰ªãÁªçÔºåËØ¶ÊÉÖËØ∑ÁúãÈ°πÁõÆËØ¶Ëß£.\n    1. Êèê‰æõ‰øÆÊîπÂ•ΩÁöÑ‰ª£Á†ÅÂíåÊØè‰∏™ÊîπËøõÁÇπÁöÑÈÖçÁΩÆÊñá‰ª∂,Áõ∏ÂΩì‰∫éÁßØÊú®ÈÉΩÁªôÂ§ßÂÆ∂ÂáÜÂ§áÂ•Ω,Â§ßÂÆ∂Âè™ÈúÄË¶ÅÂÅöÂÆûÈ™åÂíåÊê≠ÁßØÊú®(‰øÆÊîπyamlÈÖçÁΩÆÊñá‰ª∂ÁªÑÂêàÂàõÊñ∞ÁÇπ)Âç≥ÂèØ,Ë£ÖÂ•ΩÁéØÂ¢ÉÂç≥ÂèØ‰ΩøÁî®.\n    2. ÂêéÁª≠ÁöÑÊîπËøõÊñπÊ°àÈÉΩ‰ºöÂü∫‰∫éËøô‰∏™È°πÁõÆÊõ¥Êñ∞ËøõË°åÂèëÂ∏ÉÔºåÂú®Áæ§ÂÖ¨ÂëäËøõË°åÊõ¥Êñ∞ÁôæÂ∫¶‰∫ëÈìæÊé•.\n    3. Ë¥≠‰π∞‰∫ÜÊú¨È°πÁõÆÁöÑÈÉΩ‰ºöËµ†ÈÄÅyolov5-PAGCPÈÄöÈÅìÂâ™ÊûùÁÆóÊ≥ï‰ª£Á†ÅÂíåÁõ∏ÂÖ≥ÂÆûÈ™åÂèÇÊï∞ÂëΩ‰ª§.\n    4. Ë¥≠‰π∞ÂêéËøõYOLOV8V10‰∫§ÊµÅÁæ§(‰ª£Á†ÅËßÜÈ¢ëÂùáÂú®Áæ§ÂÖ¨Âëä),Áæ§ÈáåÂèØ‰∫§ÊµÅ‰ª£Á†ÅÂíåËÆ∫ÊñáÁõ∏ÂÖ≥,ÁõÆÂâç1Áæ§2Áæ§Â∑≤Êª°,Áé∞Âú®ËøõÁöÑÊòØ3Áæ§,Ê∞îÊ∞õÊ¥ªË∑É.\n    5. È°πÁõÆÂõ†‰∏∫(‰ª∑Ê†ºÈóÆÈ¢ò)‰∏çÈôÑÂ∏¶‰∏ÄÂØπ‰∏ÄÁßÅ‰∫∫Á≠îÁñëÊúçÂä°,Áæ§ÈáåÈôÑÂ∏¶Á≠îÁñëÊúçÂä°,Âπ≥Êó∂ÊàëÊúâÊó∂Èó¥ÈÉΩ‰ºöÂõûÂ§çÁæ§ÈáåÈÉ®ÂàÜÈóÆÈ¢ò.\n    6. ÈáåÈù¢ÈÖçÂ§á‰ΩøÁî®ËØ¥Êòé(ÈÉ®ÂàÜÊîπËøõÁÇπ‰ΩøÁî®Â§çÊùÇÂ∫¶È´ò„ÄÅ‰∫åÊ¨°ÂàõÊñ∞„ÄÅÂéüÂàõÁöÑÊ®°ÂùóÈÉΩ‰ºöÊúâÂØπÂ∫îÁöÑËßÜÈ¢ëËøõË°åËØ¥Êòé)\n\n2. Âü∫‰∫éUltralyticsÁöÑyolo11„ÄÅyolo12ÊîπËøõÈ°πÁõÆ.(69.9¬•)\n    \n    [ÁõÆÂâçÂ∑≤ÊúâÁöÑÊîπËøõÊñπÊ°àÂíåÊõ¥Êñ∞ËØ¶ÁªÜÂÖ¨Âëä](https://github.com/z1069614715/objectdetection_script/blob/master/yolo-improve/yolov11-project.md)  \n    È°πÁõÆÁÆÄÂçï‰ªãÁªçÔºåËØ¶ÊÉÖËØ∑ÁúãÈ°πÁõÆËØ¶Ëß£.\n    1. Êèê‰æõ‰øÆÊîπÂ•ΩÁöÑ‰ª£Á†ÅÂíåÊØè‰∏™ÊîπËøõÁÇπÁöÑÈÖçÁΩÆÊñá‰ª∂,Áõ∏ÂΩì‰∫éÁßØÊú®ÈÉΩÁªôÂ§ßÂÆ∂ÂáÜÂ§áÂ•Ω,Â§ßÂÆ∂Âè™ÈúÄË¶ÅÂÅöÂÆûÈ™åÂíåÊê≠ÁßØÊú®(‰øÆÊîπyamlÈÖçÁΩÆÊñá‰ª∂ÁªÑÂêàÂàõÊñ∞ÁÇπ)Âç≥ÂèØ,Ë£ÖÂ•ΩÁéØÂ¢ÉÂç≥ÂèØ‰ΩøÁî®.\n    2. ÂêéÁª≠ÁöÑÊîπËøõÊñπÊ°àÈÉΩ‰ºöÂü∫‰∫éËøô‰∏™È°πÁõÆÊõ¥Êñ∞ËøõË°åÂèëÂ∏ÉÔºåÂú®Áæ§ÂÖ¨ÂëäËøõË°åÊõ¥Êñ∞ÁôæÂ∫¶‰∫ëÈìæÊé•.\n    3. Ë¥≠‰π∞‰∫ÜÊú¨È°πÁõÆÁöÑÈÉΩ‰ºöËµ†ÈÄÅyolov5-PAGCPÈÄöÈÅìÂâ™ÊûùÁÆóÊ≥ï‰ª£Á†ÅÂíåÁõ∏ÂÖ≥ÂÆûÈ™åÂèÇÊï∞ÂëΩ‰ª§.\n    4. Ë¥≠‰π∞ÂêéËøõYOLOV11‰∫§ÊµÅÁæ§(‰ª£Á†ÅËßÜÈ¢ëÂùáÂú®Áæ§ÂÖ¨Âëä),Áæ§ÈáåÂèØ‰∫§ÊµÅ‰ª£Á†ÅÂíåËÆ∫ÊñáÁõ∏ÂÖ≥,Ê∞îÊ∞õÊ¥ªË∑É.\n    5. È°πÁõÆÂõ†‰∏∫(‰ª∑Ê†ºÈóÆÈ¢ò)‰∏çÈôÑÂ∏¶‰∏ÄÂØπ‰∏ÄÁßÅ‰∫∫Á≠îÁñëÊúçÂä°,Áæ§ÈáåÈôÑÂ∏¶Á≠îÁñëÊúçÂä°,Âπ≥Êó∂ÊàëÊúâÊó∂Èó¥ÈÉΩ‰ºöÂõûÂ§çÁæ§ÈáåÈÉ®ÂàÜÈóÆÈ¢ò.\n    6. ÈáåÈù¢ÈÖçÂ§á‰ΩøÁî®ËØ¥Êòé(ÈÉ®ÂàÜÊîπËøõÁÇπ‰ΩøÁî®Â§çÊùÇÂ∫¶È´ò„ÄÅ‰∫åÊ¨°ÂàõÊñ∞„ÄÅÂéüÂàõÁöÑÊ®°ÂùóÈÉΩ‰ºöÊúâÂØπÂ∫îÁöÑËßÜÈ¢ëËøõË°åËØ¥Êòé)„ÄÇ\n    7. ÂåÖÂê´yolo12-ÁõÆÊ†áÊ£ÄÊµã„ÄÅÂÆû‰æãÂàÜÂâ≤„ÄÅÂÖ≥ÈîÆÁÇπÊ£ÄÊµã„ÄÅÊóãËΩ¨ÁõÆÊ†áÊ£ÄÊµã„ÄÅÂàÜÁ±ªÈÖçÁΩÆÊñá‰ª∂ÔºåÂèØ‰ª•ÈÄöËøá‰ªÖ‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂ÁöÑÊñπÂºèÊîπËøõyolo12„ÄÇ\n\n3. Âü∫‰∫éYOLOV5,YOLOV7ÁöÑ(Ââ™Êûù+Áü•ËØÜËí∏È¶è)È°πÁõÆ.(129.9¬•)[È°πÁõÆËØ¶Ëß£](https://github.com/z1069614715/objectdetection_script/blob/master/yolo-improve/yolov5v7-light.md)\n\n    1. Ê®°ÂûãËΩªÈáèÂåñ,ÈÉ®ÁΩ≤ÂøÖÂ§á‰πã‰∏Ä!\n    2. È°πÁõÆÈáåÈù¢ÈÖçÂ•óÂá†‰∏™Ââ™ÊûùÂíåËí∏È¶èÁöÑÁ§∫‰æã,Âπ∂‰∏îÈÉΩÈÖçÊúâËßÜÈ¢ëËÆ≤Ëß£,‰æõÂ§ßÂÆ∂ÁêÜËß£Â¶Ç‰ΩïËøõË°åÂâ™ÊûùÂíåËí∏È¶è.\n    3. Ë¥≠‰π∞ÂêéËøõYOLOV5V7ËΩªÈáèÂåñ‰∫§ÊµÅÁæ§(‰ª£Á†ÅËßÜÈ¢ëÂùáÂú®Áæ§ÂÖ¨Âëä),ËΩªÈáèÂåñÈóÆÈ¢òÈÉΩÂèØÂú®Áæ§‰∫§ÊµÅ,Âõ†‰∏∫Ââ™ÊûùËí∏È¶èÈóÆÈ¢òÊØîËæÉÂõ∞Èöæ,ÊâÄ‰ª•Ââ™ÊûùËí∏È¶èÈóÆÈ¢òÂèØ‰ª•Áæ§ÈáåÊèêÈóÆ,ÊàëÈÉΩ‰ºöÁæ§ÈáåÂõûÂ§çÁõ∏ÂÖ≥ÈóÆÈ¢ò.\n\n4. Âü∫‰∫éUltralyticsÁöÑRT-DETR(CVPR2024)ÊîπËøõÈ°πÁõÆ.(89.9¬•)\n\n    [ÁõÆÂâçÂ∑≤ÊúâÁöÑÊîπËøõÊñπÊ°àÂíåÊõ¥Êñ∞ËØ¶ÁªÜÂÖ¨Âëä](https://github.com/z1069614715/objectdetection_script/blob/master/yolo-improve/rtdetr-project.md)  \n    È°πÁõÆÁÆÄÂçï‰ªãÁªçÔºåËØ¶ÊÉÖËØ∑ÁúãÈ°πÁõÆËØ¶Ëß£.\n    1. Êèê‰æõ‰øÆÊîπÂ•ΩÁöÑ‰ª£Á†ÅÂíåÊØè‰∏™ÊîπËøõÁÇπÁöÑÈÖçÁΩÆÊñá‰ª∂,Áõ∏ÂΩì‰∫éÁßØÊú®ÈÉΩÁªôÂ§ßÂÆ∂ÂáÜÂ§áÂ•Ω,Â§ßÂÆ∂Âè™ÈúÄË¶ÅÂÅöÂÆûÈ™åÂíåÊê≠ÁßØÊú®(‰øÆÊîπyamlÈÖçÁΩÆÊñá‰ª∂ÁªÑÂêàÂàõÊñ∞ÁÇπ)Âç≥ÂèØ,Ë£ÖÂ•ΩÁéØÂ¢ÉÂç≥ÂèØ‰ΩøÁî®.\n    2. ÂêéÁª≠ÁöÑÊîπËøõÊñπÊ°àÈÉΩ‰ºöÂü∫‰∫éËøô‰∏™È°πÁõÆÊõ¥Êñ∞ËøõË°åÂèëÂ∏É,Âú®Áæ§ÂÖ¨ÂëäËøõË°åÊõ¥Êñ∞ÁôæÂ∫¶‰∫ëÈìæÊé•.\n    3. Ë¥≠‰π∞‰∫ÜRT-DETRÈ°πÁõÆÁöÑÈÉΩ‰ºöËµ†ÈÄÅyolov5-PAGCPÈÄöÈÅìÂâ™ÊûùÁÆóÊ≥ï‰ª£Á†ÅÂíåÁõ∏ÂÖ≥ÂÆûÈ™åÂèÇÊï∞ÂëΩ‰ª§.\n    4. Ë¥≠‰π∞ÂêéËøõRT-DETR‰∫§ÊµÅÁæ§(‰ª£Á†ÅËßÜÈ¢ëÂùáÂú®Áæ§ÂÖ¨Âëä),Áæ§ÈáåÂèØ‰∫§ÊµÅ‰ª£Á†ÅÂíåËÆ∫ÊñáÁõ∏ÂÖ≥.\n    5. È°πÁõÆÂõ†‰∏∫(‰ª∑Ê†ºÈóÆÈ¢ò)‰∏çÈôÑÂ∏¶‰∏ÄÂØπ‰∏ÄÁßÅ‰∫∫Á≠îÁñëÊúçÂä°,Áæ§ÈáåÈôÑÂ∏¶Á≠îÁñëÊúçÂä°,Âπ≥Êó∂ÊàëÊúâÊó∂Èó¥ÈÉΩ‰ºöÂõûÂ§çÁæ§ÈáåÈÉ®ÂàÜÈóÆÈ¢ò.\n    6. RT-DETRÈ°πÁõÆÂåÖÂê´Â§öÁßçÂü∫ÂáÜÊ®°ÂûãÊîπËøõÊñπÊ°à(RT-DETR-R18,RT-DETR-R50,RT-DETR-L,Yolov8-Detr,Yolov5-Detr),ÂÖ∑‰ΩìÂèØÁÇπÂáª[ÁõÆÂâçÂ∑≤ÊúâÁöÑÊîπËøõÊñπÊ°àÂíåÊõ¥Êñ∞ËØ¶ÁªÜÂÖ¨Âëä](https://github.com/z1069614715/objectdetection_script/blob/master/yolo-improve/rtdetr-project.md)ÁúãËØ¶ÁªÜ.\n    7. ÈáåÈù¢ÈÖçÂ§á‰ΩøÁî®ËØ¥Êòé(ÈÉ®ÂàÜÊîπËøõÁÇπ‰ΩøÁî®Â§çÊùÇÂ∫¶È´ò„ÄÅ‰∫åÊ¨°ÂàõÊñ∞„ÄÅÂéüÂàõÁöÑÊ®°ÂùóÈÉΩ‰ºöÊúâÂØπÂ∫îÁöÑËßÜÈ¢ëËøõË°åËØ¥Êòé)\n\n5. Âü∫‰∫éYOLOV8V10V11V12ÁöÑÂâ™ÊûùËí∏È¶èÈ°πÁõÆ.  \n   \n    Ê≥®ÊÑè:\n    1. Êú¨Ê¨°È°πÁõÆÂ∞±Áõ¥Êé•Êèê‰æõÂá†‰∏™Êñá‰ª∂ÔºåÂà∞Êó∂ÂÄô‰ºöÊèê‰æõÊïôÁ®ãÔºåËá™Ë°åÂ§çÂà∂Âà∞È°πÁõÆ‰∏Ä/‰∫å‰∏äÂç≥ÂèØË∑ëÔºåÂéüÁêÜ‰∏äÂÖ∂‰ªñÁâàÊú¨Â∫îËØ•‰πüÂèØ‰ª•Ë∑ëÔºå‰ΩÜÊòØÂºÄÂèëÁöÑÊó∂ÂÄôÊàëÊòØÂü∫‰∫éÈ°πÁõÆ‰∏Ä/‰∫åÁöÑ(ultralyticsÁâàÊú¨Âè∑:v8.1.9„ÄÅv8.2.50„ÄÅv8.3.1)‰∏äÂºÄÂèëÁöÑÔºåÈôÑËøëÁöÑÁâàÊú¨ÁöÑËØùÂ∫îËØ•‰πüÂèØ‰ª•Ë∑ëÔºå‰ΩÜÊòØÊ≤°ÂäûÊ≥ï‰∏Ä‰∏ÄÈ™åËØÅÔºåÊâÄ‰ª•ÈúÄËá™Ë°åËÄÉËôë!\n    2. ÈáåÈù¢‰ºöÊèê‰æõ‰∏Ä‰∏™ÂÆòÊñπÁ∫ØÂáÄÁâàÁöÑ(ultralyticsÁâàÊú¨Âè∑:8.1.9„ÄÅ8.2.50„ÄÅ8.3.1„ÄÅ8.3.78)ÁöÑultralytics‰ª•ÂèäÂÖ∂ÂØπÂ∫îÁöÑÂâ™ÊûùËí∏È¶è‰ª£Á†ÅÔºå‰ª•‰æøÊ≤°ÊúâË¥≠‰π∞È°πÁõÆ‰∏Ä/‰∫åÁöÑÂêåÂ≠¶‰ΩøÁî®„ÄÇ\n\n    Ââ™Êûù:[È°πÁõÆËØ¶Ëß£](https://github.com/z1069614715/objectdetection_script/blob/master/yolo-improve/yolov8-compress.md)(89.9¬•)\n    1. Ê®°ÂûãËΩªÈáèÂåñ,ÈÉ®ÁΩ≤,Â§ßËÆ∫ÊñáÂ†ÜÂ∑•‰ΩúÈáèÂøÖÂ§á‰πã‰∏Ä!\n    2. È°πÁõÆÈáåÈù¢ÈÖçÂ•óÂâ™ÊûùÁ§∫‰æã(Á§∫‰æã‰∏≠ÊòØÂü∫‰∫éÈ°πÁõÆ‰∏Ä/‰∫åÁöÑÊîπËøõ‰ª£Á†ÅËøõË°åÂâ™Êûù,Â¶ÇÊ≤°ÊúâÂÖ•ÊâãÈ°πÁõÆ‰∏Ä/‰∫åÊòØ‰∏çÂåÖÂê´ËøôÈÉ®ÂàÜ‰ª£Á†ÅÁöÑ,‰ΩÜÂØπ‰Ω†ÁêÜËß£Ââ™ÊûùÊìç‰ΩúÊ≤°ÂΩ±Âìç),Âπ∂‰∏îÈÉΩÈÖçÊúâËßÜÈ¢ëËÆ≤Ëß£,‰æõÂ§ßÂÆ∂ÁêÜËß£Â¶Ç‰ΩïËøõË°åÂâ™Êûù.\n    3. Ë¥≠‰π∞ÂêéËøõYOLOV8V10V11V12Ââ™Êûù‰∫§ÊµÅÁæ§(‰ª£Á†ÅËßÜÈ¢ëÂùáÂú®Áæ§ÂÖ¨Âëä),Âõ†‰∏∫Ââ™ÊûùÊìç‰ΩúÊúâ‰∏ÄÂÆöÁöÑÈöæÂ∫¶,ÊâÄ‰ª•Ââ™ÊûùÈóÆÈ¢òÂèØ‰ª•Áæ§ÈáåÊèêÈóÆ,ÊàëÈÉΩ‰ºöÁæ§ÈáåÂõûÂ§çÁõ∏ÂÖ≥ÈóÆÈ¢ò.\n    4. ÊîØÊåÅyolov8‰∏≠ÁöÑÁõÆÊ†áÊ£ÄÊµã„ÄÅÂÆû‰æãÂàÜÂâ≤„ÄÅÂßøÊÄÅÊ£ÄÊµã„ÄÅÊóãËΩ¨ÁõÆÊ†áÊ£ÄÊµãÂâ™Êûù„ÄÅyolov10ÁõÆÊ†áÊ£ÄÊµãÂâ™Êûù„ÄÅyolo11/12(ÁõÆÊ†áÊ£ÄÊµã„ÄÅÂÆû‰æãÂàÜÂâ≤„ÄÅÂßøÊÄÅÊ£ÄÊµã„ÄÅÊóãËΩ¨ÁõÆÊ†áÊ£ÄÊµãÂâ™Êûù)„ÄÇ\n\n    Ëí∏È¶è:[È°πÁõÆËØ¶Ëß£](https://github.com/z1069614715/objectdetection_script/blob/master/yolo-improve/yolov8-distill.md)(89.9¬•)\n    1. Ê®°ÂûãËΩªÈáèÂåñ,ÈÉ®ÁΩ≤,Â§ßËÆ∫ÊñáÂ†ÜÂ∑•‰ΩúÈáèÂøÖÂ§á‰πã‰∏Ä!\n    2. È°πÁõÆÈáåÈù¢ÈÖçÂ•óËí∏È¶èÁ§∫‰æã(ÈÉ®ÂàÜÁ§∫‰æã‰∏≠ÊòØÂü∫‰∫éÈ°πÁõÆ‰∏Ä/‰∫åÁöÑÊîπËøõ‰ª£Á†ÅËøõË°åËí∏È¶è,Â¶ÇÊ≤°ÊúâÂÖ•ÊâãÈ°πÁõÆ‰∏Ä/‰∫åÊòØ‰∏çÂåÖÂê´ËøôÈÉ®ÂàÜ‰ª£Á†ÅÁöÑ,‰ΩÜÂØπ‰Ω†ÁêÜËß£Ëí∏È¶èÊìç‰ΩúÊ≤°ÂΩ±Âìç),Âπ∂‰∏îÈÉΩÈÖçÊúâËßÜÈ¢ëËÆ≤Ëß£,‰æõÂ§ßÂÆ∂ÁêÜËß£Â¶Ç‰ΩïËøõË°åËí∏È¶è.\n    3. Ë¥≠‰π∞ÂêéËøõYOLOV8V10V11V12Ëí∏È¶è‰∫§ÊµÅÁæ§(‰ª£Á†ÅËßÜÈ¢ëÂùáÂú®Áæ§ÂÖ¨Âëä),Âõ†‰∏∫Ëí∏È¶èÊìç‰ΩúÊúâ‰∏ÄÂÆöÁöÑÈöæÂ∫¶,ÊâÄ‰ª•Ëí∏È¶èÊìç‰ΩúÈóÆÈ¢òÂèØ‰ª•Áæ§ÈáåÊèêÈóÆ,ÊàëÈÉΩ‰ºöÁæ§ÈáåÂõûÂ§çÁõ∏ÂÖ≥ÈóÆÈ¢ò.\n    4. ÊîØÊåÅyolov8‰∏≠ÁöÑÁõÆÊ†áÊ£ÄÊµã„ÄÅÂÆû‰æãÂàÜÂâ≤„ÄÅÂßøÊÄÅÊ£ÄÊµã„ÄÅÊóãËΩ¨ÁõÆÊ†áÊ£ÄÊµãËí∏È¶è„ÄÅyolov10ÁõÆÊ†áÊ£ÄÊµãËí∏È¶è„ÄÅyolo11/12(ÁõÆÊ†áÊ£ÄÊµã„ÄÅÂÆû‰æãÂàÜÂâ≤„ÄÅÂßøÊÄÅÊ£ÄÊµã„ÄÅÊóãËΩ¨ÁõÆÊ†áÊ£ÄÊµãËí∏È¶è)„ÄÇ\n    5. ÂÆû‰æãÂàÜÂâ≤„ÄÅÂßøÊÄÅÊ£ÄÊµã„ÄÅÊóãËΩ¨ÁõÆÊ†áÊ£ÄÊµãÊöÇ‰∏çÊîØÊåÅBCKDËí∏È¶èÊñπÊ≥ï.\n\n6. Âü∫‰∫éUltralyticsÁöÑRT-DETR(CVPR2024)ÁöÑÂâ™ÊûùËí∏È¶èÈ°πÁõÆ.  \n   \n    Ê≥®ÊÑèÔºöÂü∫‰∫éUltralyticsÁöÑRT-DETRÁöÑÂâ™ÊûùËí∏È¶èÈ°πÁõÆÊòØÂü∫‰∫éÈ°πÁõÆÂõõ‰∏äËøõË°åÂºÄÂèëÁöÑÔºåÊâÄ‰ª•ÂÖ•ÊâãÂâ™ÊûùËí∏È¶èÈ°πÁõÆ‰πüÈúÄË¶ÅÈ°πÁõÆÂõõÊâçËÉΩ‰ΩøÁî®„ÄÇ\n\n    Ââ™ÊûùÔºö[È°πÁõÆËØ¶Ëß£](https://github.com/z1069614715/objectdetection_script/blob/master/yolo-improve/rtdetr-compress.md)(89.9¬•)\n    1. Ê®°ÂûãËΩªÈáèÂåñ,ÈÉ®ÁΩ≤,Â§ßËÆ∫ÊñáÂ†ÜÂ∑•‰ΩúÈáèÂøÖÂ§á‰πã‰∏Ä!\n    2. È°πÁõÆÈáåÈù¢ÈÖçÂ•óÂâ™ÊûùÁ§∫‰æã(ÂåÖÂê´‰∏Ä‰∫õÈ°πÁõÆÂõõ‰∏≠ÁöÑÊîπËøõÊ®°ÂûãÁöÑÂâ™ÊûùÊïôÁ®ã),Âπ∂‰∏îÈÉΩÈÖçÊúâËßÜÈ¢ëËÆ≤Ëß£,‰æõÂ§ßÂÆ∂ÁêÜËß£Â¶Ç‰ΩïËøõË°åËí∏È¶è.\n    3. Ë¥≠‰π∞ÂêéËøõRTDETRÂâ™Êûù‰∫§ÊµÅÁæ§(‰ª£Á†ÅËßÜÈ¢ëÂùáÂú®Áæ§ÂÖ¨Âëä),Âõ†‰∏∫Ââ™ÊûùÊìç‰ΩúÊúâ‰∏ÄÂÆöÁöÑÈöæÂ∫¶,ÊâÄ‰ª•Ââ™ÊûùÊìç‰ΩúÈóÆÈ¢òÂèØ‰ª•Áæ§ÈáåÊèêÈóÆ,ÊàëÈÉΩ‰ºöÁæ§ÈáåÂõûÂ§çÁõ∏ÂÖ≥ÈóÆÈ¢ò.\n    4. ÁªèËøáÊàëÁõÆÂâçÁöÑÂÆûÈ™å,rtdetrÂæàÈöæËøõË°åÁ®ÄÁñèËÆ≠ÁªÉ,Âõ†Ê≠§Êú¨È°πÁõÆÁõÆÂâç‰∏çÂåÖÂê´Á®ÄÁñèËÆ≠ÁªÉÁöÑÂâ™ÊûùÊñπÊ≥ï,Â¶ÇÊûú‰∏ÄÂÆöË¶ÅËøõË°åÁ®ÄÁñèËÆ≠ÁªÉÁöÑÂâ™ÊûùÊÖéÂÖ•,ÁõÆÂâçÈ°πÁõÆÂåÖÂê´6Áßç‰∏çÈúÄË¶ÅÁ®ÄÁñèËÆ≠ÁªÉÊñπÊ≥ïÁöÑÂâ™Êûù.\n\n    Ëí∏È¶èÔºö[È°πÁõÆËØ¶Ëß£](https://github.com/z1069614715/objectdetection_script/blob/master/yolo-improve/rtdetr-distill.md)(69.9¬•)  \n    1. Ê®°ÂûãËΩªÈáèÂåñ,ÈÉ®ÁΩ≤,Â§ßËÆ∫ÊñáÂ†ÜÂ∑•‰ΩúÈáèÂøÖÂ§á‰πã‰∏Ä!\n    2. È°πÁõÆÈáåÈù¢ÈÖçÂ•óËí∏È¶èÁ§∫‰æã,Âπ∂‰∏îÈÉΩÈÖçÊúâËßÜÈ¢ëËÆ≤Ëß£,‰æõÂ§ßÂÆ∂ÁêÜËß£Â¶Ç‰ΩïËøõË°åËí∏È¶è.\n    3. Ë¥≠‰π∞ÂêéËøõRTDETRËí∏È¶è‰∫§ÊµÅÁæ§(‰ª£Á†ÅËßÜÈ¢ëÂùáÂú®Áæ§ÂÖ¨Âëä),Âõ†‰∏∫Ëí∏È¶èÊìç‰ΩúÊúâ‰∏ÄÂÆöÁöÑÈöæÂ∫¶,ÊâÄ‰ª•Ëí∏È¶èÊìç‰ΩúÈóÆÈ¢òÂèØ‰ª•Áæ§ÈáåÊèêÈóÆ,ÊàëÈÉΩ‰ºöÁæ§ÈáåÂõûÂ§çÁõ∏ÂÖ≥ÈóÆÈ¢ò.\n    4. Áü•ËØÜËí∏È¶èÊï¥‰Ωì‰øÆÊîπÈöæÂ∫¶Â§ßÔºå‰ª£Ë°®Â∞ë‰∫∫‰ΩøÁî®ÔºåÁâ©‰ª•Á®Ä‰∏∫Ë¥µÔºåÂ¢ûÂä†ÊñáÁ´†ÁöÑÂàõÊñ∞Â∫¶ÔºÅ\n\n7. Âü∫‰∫éCVPR2025-DEIMÁöÑÊîπËøõÈ°πÁõÆ.(268¬•)\n    \n    È°πÁõÆËØ¶ÁªÜ‰ªãÁªçËØ∑Áúã[Ê≠§Â§Ñ](https://github.com/z1069614715/objectdetection_script/blob/master/cvpr2025-deim-project.md)\n\n8. Âü∫‰∫éYOLO|RTDETRÂ§öÊ®°ÊÄÅÁõÆÊ†áÊ£ÄÊµãÈ°πÁõÆ.(Âéü‰ª∑288¬•,Ëã•Â∑≤Ë¥≠‰π∞yolo8101112ÊàñrtdetrÈ°πÁõÆÁöÑÂàô‰ºòÊÉ†50¬•=238¬•)\n\n    È°πÁõÆËØ¶ÁªÜ‰ªãÁªçËØ∑Áúã[Ê≠§Â§Ñ](https://github.com/z1069614715/objectdetection_script/blob/master/mutilmodel-project.md)\n\n9. Âü∫‰∫éYOLOÂíåRT-DETRÁöÑËÆ∫ÊñáÂÖ®ÊµÅÁ®ãÊåáÂØºÈ°πÁõÆ.(Âéü‰ª∑258¬•ÔºåËã•Â∑≤Ë¥≠‰π∞yolo8101112ÊàñrtdetrÈ°πÁõÆÁöÑÂàô‰ºòÊÉ†70¬•=188¬•)[È°πÁõÆËØ¶Ëß£](https://github.com/z1069614715/objectdetection_script/blob/master/yolo-improve/paper.md)  \n    Êàë‰ª¨ÁõÆÂâçÊúâÈùûÂ∏∏Â§öÁöÑ‰ª£Á†ÅÈ°πÁõÆÔºåÂá†‰πéÊòØÂÖ®ÁΩëÊúÄÂÖ®‰ª∑Ê†ºÊúÄ‰ºòÊÉ†ÊÄßÊ†ºÊØîÊúÄÈ´òÁöÑ‰∏ÄÂÆ∂Ôºå‰ΩÜÊòØÈöæÂÖçÊúâ‰∫õÂêåÂ≠¶Âú®ÂÅöÂÆåÂÆûÈ™åÂêéËøòÊòØÂÆåÂÖ®‰∏çÊáÇÂ∫îËØ•ÊÄé‰πàÂéªÂÜôor‰∏çÊÉ≥Ëµ∞Â§™Â§öÂºØË∑ØÁöÑÊÉÖÂÜµÔºåÂõ†Ê≠§ÂºÄÂ±ïËøô‰∏™Âü∫‰∫éYOLOÂíåRT-DETRÁöÑËÆ∫ÊñáÂÖ®ÊµÅÁ®ãÊåáÂØºÈ°πÁõÆÔºåÊú¨È°πÁõÆËá¥Âäõ‰∫éÂ∏ÆÂä©ÈÇ£‰∫õÂú®ËÆ∫ÊñáÈÅìË∑Ø‰∏äÊûÅÂÖ∂Âõ∞ÈöæÁöÑÂêåÂ≠¶ÔºåÂü∫Êú¨‰∏äÈÖçÂêà‰∏äËø∞ÁöÑ‰∏Ä‰∫õÊîπËøõÈ°πÁõÆÂíåÊ≠§ËÆ∫ÊñáÂÖ®ÊµÅÁ®ãÊåáÂØºÈ°πÁõÆÂÜçÂä†‰∏äËá™Â∑±ÁöÑ‰∏ÄÁÇπÂä™ÂäõÂèØ‰ª•ÂÆåÂÖ®ÂÆûÁé∞ÊØï‰∏öÊó†Âøß,È°πÁõÆÁÆÄ‰ªãÂ¶Ç‰∏ãÔºö\n\n    1. Áõ¥Êí≠ÂÜÖÂÆπÊ∂âÂèäÂà∞ÂèëËÆ∫ÊñáÁöÑÊï¥‰∏™ËÆ∫ÊñáÊ°ÜÊû∂‰ΩìÁ≥ªÁöÑÊñπÊñπÈù¢Èù¢ÔºåÊØèÊ¨°Áõ¥Êí≠ÈÉΩ‰ºö‰ºòÂÖàËÆ≤Â§ßÂÆ∂ÊúÄÊÉ≥Âê¨ÁöÑÈÉ®ÂàÜÔºåÊ†πÊçÆËØæÁ®ãÁõÆÂΩïÊäïÁ•®ÂÜ≥ÂÆö„ÄÇ\n    2. Áõ¥Êí≠Á≠îÁñëÊØè‰∏™‰∫∫ÁöÑÈóÆÈ¢òÔºå‰∏äËØæÂâç‰ºö‰ΩøÁî®excelË°®Ê†ºÂú®Á∫øÊî∂ÈõÜÂ§ßÂÆ∂ÁöÑÈóÆÈ¢òÔºåÁõ¥Êí≠Êó∂ÈõÜ‰∏≠ËÆ≤Ëß£„ÄÇ\n    3. Áõ¥Êí≠ÁöÑÂõûÊîæËßÜÈ¢ë‰ºöÂÆûÊó∂‰∏ä‰º†Âà∞ÁôæÂ∫¶ÁΩëÁõòÔºåÂπ∂‰∏îËßÜÈ¢ëÂùá‰∏∫Âä†ÂØÜËßÜÈ¢ëÔºå‰∏Ä‰∫∫‰∏ÄÊú∫‰∏ÄÁ†ÅÔºå‰∏îËØæÁ®ãÁõÆÂΩïÁöÑÊØèÈÉ®ÂàÜÂØπÂ∫îÊ£ÄÁ¥¢Áõ¥Êí≠ÂõûÊîæËßÜÈ¢ëÈìæÊé•Êñπ‰æøÂ§ßÂÆ∂ÂêéÁª≠Êü•ÊâæÔºåÂÆûÊó∂Êõ¥Êñ∞ÁôæÂ∫¶ÁΩëÁõòÈìæÊé•ÂÜÖÂÆπÂíå‰ΩøÁî®ËØ¥ÊòéÊñáÊ°£„ÄÇ\n    4. Ë¥≠‰π∞ÂêéËøõËÆ∫ÊñáÊåáÂØº‰∫§ÊµÅÁæ§(ËßÜÈ¢ëÂùáÂú®Áæ§ÂÖ¨Âëä),Áæ§ÈáåÂèØ‰∫§ÊµÅËÆ∫ÊñáÁõ∏ÂÖ≥„ÄÇ\n    5. È°πÁõÆ‰∏çÈôÑÂ∏¶ÁßÅ‰∫∫Á≠îÁñëÊúçÂä°,Áæ§ÈáåÈôÑÂ∏¶Á≠îÁñëÊúçÂä°,Âπ≥Êó∂ÊàëÊúâÊó∂Èó¥ÈÉΩ‰ºöÂõûÂ§çÁæ§ÈáåÈÉ®ÂàÜÈóÆÈ¢ò„ÄÇ\n    6. ‰∏çÂÆöÊó∂Êî∂ÈõÜÁæ§ÂèãÂèçÈ¶àÔºåÊúâÈóÆÈ¢òÂèØ‰ª•Âú®Áæ§ÂÜÖÈöèÊó∂ÊèêÂá∫ÔºåÈÄêÊ≠•ÂÆåÂñÑËØæÁ®ã‰ΩìÁ≥ªÔºåËÆ©Â§ßÂÆ∂È´òÊïàÂø´ÈÄüÂèëÂá∫ËÆ∫Êñá„ÄÇ\n    7. È°πÁõÆÊúâÊïàÊúü‰∏∫‰∏ÄÂπ¥ÔºåÊó∂Èó¥‰ªé‰ªòË¥πËøõÁæ§ÈÇ£Â§©ÂºÄÂßãÁÆóÔºå‰æãÂ¶ÇÊàë2024Âπ¥5Êúà2Êó•ËøõÁæ§Ôºå2025Âπ¥5Êúà2Êó•Âà∞ÊúüÔºå‰∏ÄÂπ¥Êó∂Èó¥Ë∂≥‰ª•Ëß£ÂÜ≥ÊâÄÊúâËÆ∫ÊñáÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢ò„ÄÇ\n    8. È°πÁõÆÂÖ¨ÂºÄËØæËØïÂê¨BÁ´ôÈìæÊé•1Ôºö[ÈïøËææ80ÂàÜÈíüÁöÑ<ËÆ∫Êñá‰∏≠ÂØπÊØîÂÆûÈ™å+Ê∂àËûçÂÆûÈ™å+ËÆ∫ÊñáÂ∑•‰ΩúÈáèÂàõÊñ∞ÁÇπËØÑ‰º∞+Á≠îÁñë>Ëß£Á≠îÁõ¥Êí≠ÂõûÊîæÊù•Âï¶~](https://www.bilibili.com/video/BV1u5rCYmE4k/)\n    9. È°πÁõÆÂÖ¨ÂºÄËØæËØïÂê¨BÁ´ôÈìæÊé•2Ôºö[ÈïøËææ60ÂàÜÈíüÁöÑ<ÂÆûÈ™åÂêëËÆ∫ÊñáËøáÊ∏°ÊåáÂØº+ËÆ∫ÊñáÂÜô‰ΩúÈ°∫Â∫è+ÂàõÊñ∞ÊÄßËØÑ‰º∞+ÂÖ¨ÂºÄÁ≠îÁñë>Áõ¥Êí≠ÂõûÊîæÊù•Âï¶~](https://www.bilibili.com/video/BV1oJPueREfR/)\n    10. È°πÁõÆÂÖ¨ÂºÄËØæËØïÂê¨BÁ´ôÈìæÊé•3Ôºö[ÈïøËææ2Â∞èÊó∂ÁöÑËÆ∫ÊñáÈ´òÊïàÁîªÂõæ‰∏ìÈ¢òÂÖ®Èù¢ÂâñÊûêÔºöÊï∞ÊçÆÂèØËßÜÂåñ+Ê®°ÂûãÂõæÁªòÂà∂+ÂÆûÈ™åÊï∞ÊçÆÂàÜÊûêÂõæ+Á≠îÁñëÁõ¥Êí≠ÔºåÂÖ®Á®ãÈ´òËÉΩÔºÅÔºÅÔºÅÔºÅ](https://www.bilibili.com/video/BV1xEEEzZEUs)\n\n# Advertising Board\n‰∫∫Â∑•Êô∫ËÉΩ-Â∑•‰ΩúÂÆ§ÈïøÊúüÂØπÂ§ñÊé•ÂçïÔºåËåÉÂõ¥‰∏ªË¶ÅÊòØ:\n1. ÁõÆÊ†áÊ£ÄÊµã.\n2. ÂõæÂÉèÂàÜÁ±ª.\n3. ÂõæÂÉèÂàÜÂâ≤.\n4. NLPÈ¢ÜÂüü.\n5. ÂõæÂÉèË∂ÖÂàÜËæ®.\n6. ÂõæÂÉèÂéªÂô™.\n7. GAN.\n8. Ê®°ÂûãÈÉ®ÁΩ≤.\n9. Ê®°ÂûãÂàõÊñ∞. \n10. ÁõÆÊ†áË∑üË∏™.\n\nÁ≠âÁ≠â. Á°ïÂçöÂõ¢Èòü‰∏∫ÊÇ®ÊúçÂä°,ÂõõÂπ¥TBËÄÅÂ∫ó,TB‰ªòÊ¨æÂÆâÂÖ®ÂèØÈù†.  \nÊúâÈúÄË¶ÅÁöÑÂèØÂä†‰ºÅÈπÖ(1615905974/1069614715)ÂêéÁõ¥Êé•Êèê‰æõ(ÂÖ∑‰ΩìÈúÄÊ±Ç+Êó∂Èó¥+È¢ÑÁÆó,Êàë‰ª¨‰ºöÊ†πÊçÆÊÇ®ÁöÑÈúÄÊ±ÇÂÆâÊéíÊúÄÈÄÇÂêàÊÇ®ÁöÑÂ∑•Á®ãÂ∏à).  \n\n# Explanation\n- **yolo**  \n    yoloÊñá‰ª∂Â§πÊòØÈíàÂØπyolov5,yolov7,yolov8ÁöÑÊï∞ÊçÆÈõÜÂ§ÑÁêÜËÑöÊú¨ÔºåÂÖ∑‰ΩìÂèØÁúã[readme.md](https://github.com/z1069614715/objectdetection_script/blob/master/yolo/readme.md).  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1tM411a7it/).  \n\n- **damo-yolo**  \n    damo-yoloÊñá‰ª∂Â§πÊòØÈíàÂØπDAMO-YOLOÁöÑÊï∞ÊçÆÈõÜÂ§ÑÁêÜËÑöÊú¨ÔºåÂÖ∑‰ΩìÂèØÁúã[readme.md](https://github.com/z1069614715/objectdetection_script/blob/master/damo-yolo/readme.md).  \n    ÁõÆÂâçÂè™ÊîØÊåÅvocËΩ¨coco.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1M24y1v7Uf/).   \n\n- **yolo-improve**  \n    yolo-improveÊñá‰ª∂Â§πÊòØÊèê‰æõ‰∏Ä‰∫õÂÖ≥‰∫éyoloÁ≥ªÂàóÊ®°ÂûãÊîπËøõÊÄùË∑ØÁöÑÊ∫êÁ†ÅÔºåÂÖ∑‰ΩìÂèØÁúã[readme.md](https://github.com/z1069614715/objectdetection_script/blob/master/yolo-improve/readme.md).   \n\n- **yolo-gradcam**  \n    yolo-gradcamÊñá‰ª∂Â§πÊòØÊèê‰æõ‰∏Ä‰∫õÂÖ≥‰∫éÂèØËßÜÂåñyoloÊ®°ÂûãÁöÑÁÉ≠ÂäõÂõæÁöÑÊ∫êÁ†ÅÔºåÂÖ∑‰ΩìÂèØÁúã[readme.md](https://github.com/z1069614715/objectdetection_script/blob/master/yolo-gradcam/README.md).\n\n- **cv-attention**  \n    cv-attentionÊñá‰ª∂Â§πÊòØÂÖ≥‰∫éCVÁöÑ‰∏Ä‰∫õÁªèÂÖ∏Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂÖ∑‰ΩìÂèØÁúã[readme.md](https://github.com/z1069614715/objectdetection_script/blob/master/cv-attention/readme.md).\n\n- **objectdetection-tricks**  \n    objectdetection-tricksÊñá‰ª∂Â§πÊòØÂÖ≥‰∫éÁõÆÊ†áÊ£ÄÊµã‰∏≠ÂêÑÁßçÂ∞èÊäÄÂ∑ßÔºåÂÖ∑‰ΩìÂèØÁúã[readme.md](https://github.com/z1069614715/objectdetection_script/blob/master/objectdetection-tricks/readme.md).\n\n- **mmdet-course**\n    mmdet-courseÊñá‰ª∂Â§πÊòØÊèê‰æõmmdetÊïôÁ®ãÁõ∏ÂÖ≥ËµÑÊñôÔºåÂÖ∑‰ΩìÂèØÁúã[readme.md](https://github.com/z1069614715/objectdetection_script/blob/master/mmdet-course/readme.md)\n\n- **data-offline-aug**\n    data-offline-augÊñá‰ª∂Â§πÊòØÂÖ≥‰∫éÂõæÂÉè‰ªªÂä°ÁöÑÁ¶ªÁ∫øÊï∞ÊçÆÂ¢ûÂº∫ËÑöÊú¨ÔºåÂÖ∑‰ΩìÂèØÁúã[readme.md](https://github.com/z1069614715/objectdetection_script/blob/master/data-offline-aug/readme.md)\n\n[![Forkers repo roster for @z1069614715/objectdetection_script](https://reporoster.com/forks/z1069614715/objectdetection_script)](https://github.com/z1069614715/objectdetection_script/network/members)\n[![Stargazers repo roster for @z1069614715/objectdetection_script](https://reporoster.com/stars/z1069614715/objectdetection_script)](https://github.com/z1069614715/objectdetection_script/stargazers)\n\n# Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=z1069614715/objectdetection_script&type=Date)](https://star-history.com/#z1069614715/objectdetection_script&Date)\n\n<a id=\"0\"></a>", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "readme.md", "language": "markdown", "complexity": "medium", "star_count": 6481, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 156, "word_count": 221}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# yolo-gradcam\nyolo model with gradcam visual.  \nÂç≥ÊèíÂç≥Áî®,‰∏çÈúÄË¶ÅÂØπÊ∫êÁ†ÅËøõË°å‰ªª‰Ωï‰øÆÊîπ!\n\n## ÂìîÂì©ÂìîÂì©ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄ\n1. yolov5-[ÂìîÂì©ÂìîÂì©Âú∞ÂùÄ](https://www.bilibili.com/video/BV1F6421V77v/)\n2. yolov7-[ÂìîÂì©ÂìîÂì©Âú∞ÂùÄ](https://www.bilibili.com/video/BV1F6421V77v/)\n3. yolov8-[ÂìîÂì©ÂìîÂì©Âú∞ÂùÄ](https://www.bilibili.com/video/BV1T2N6eaEFD/)\n4. yolov9-[ÂìîÂì©ÂìîÂì©Âú∞ÂùÄ](https://www.bilibili.com/video/BV14H4y157MP/)\n5. yolov11-[ÂìîÂì©ÂìîÂì©Âú∞ÂùÄ](https://www.bilibili.com/video/BV1T2N6eaEFD/)\n\n## ÁéØÂ¢É\npip install grad-cam==1.4.8 -i https://pypi.tuna.tsinghua.edu.cn/simple\n\n## Ê≥®ÊÑè‰∫ãÈ°π\n1. yolov5ÊòØÂú®v7.0ËøõË°åÁºñÂÜôÂíåÊµãËØïÁöÑ„ÄÇ\n2. yolov7ÊòØÂú®2023.10.1Âè∑ÁöÑÁâàÊú¨ËøõË°åÁºñÂÜôÂíåÊµãËØïÁöÑ„ÄÇ\n3. yolov8ÊòØÂú®2024.1.31Âè∑ÁöÑÁâàÊú¨ËøõË°åÁºñÂÜôÂíåÊµãËØïÁöÑ„ÄÇ\n4. yolov9ÊòØÂú®2024.3.7Âè∑ÁöÑÁâàÊú¨ËøõË°åÁºñÂÜôÂíåÊµãËØïÁöÑ„ÄÇ\n5. Âª∫ËÆÆÂú®Êñ∞ÁâàÊú¨‰∏ãËøõË°å‰ΩøÁî®ÔºåÊóßÁâàÊú¨ÂèØËÉΩ‰ºöÊúâÊä•ÈîôÔºåÈúÄË¶ÅËá™Ë°åËß£ÂÜ≥„ÄÇ\n", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "yolo-gradcam/README.md", "language": "markdown", "complexity": "simple", "star_count": 6481, "quality_score": 0.7, "features": {"has_code_examples": false, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 21, "word_count": 39}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\nimport torch, yaml, cv2, os, shutil, sys, copy\nimport numpy as np\nnp.random.seed(0)\nimport matplotlib.pyplot as plt\nfrom tqdm import trange\nfrom PIL import Image\nfrom ultralytics import YOLO\nfrom ultralytics.nn.tasks import attempt_load_weights\nfrom ultralytics.utils.torch_utils import intersect_dicts\nfrom ultralytics.utils.ops import xywh2xyxy, non_max_suppression\nfrom pytorch_grad_cam import GradCAMPlusPlus, GradCAM, XGradCAM, EigenCAM, HiResCAM, LayerCAM, RandomCAM, EigenGradCAM, KPCA_CAM, AblationCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\nfrom pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients\n\ndef letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n    # Resize and pad image while meeting stride-multiple constraints\n    shape = im.shape[:2]  # current shape [height, width]\n    if isinstance(new_shape, int):\n        new_shape = (new_shape, new_shape)\n\n    # Scale ratio (new / old)\n    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n        r = min(r, 1.0)\n\n    # Compute padding\n    ratio = r, r  # width, height ratios\n    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n    if auto:  # minimum rectangle\n        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n    elif scaleFill:  # stretch\n        dw, dh = 0.0, 0.0\n        new_unpad = (new_shape[1], new_shape[0])\n        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n\n    dw /= 2  # divide padding into 2 sides\n    dh /= 2\n\n    if shape[::-1] != new_unpad:  # resize\n        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n    return im, ratio, (top, bottom, left, right)\n\nclass ActivationsAndGradients:\n    \"\"\" Class for extracting activations and\n    registering gradients from targetted intermediate layers \"\"\"\n\n    def __init__(self, model, target_layers, reshape_transform):\n        self.model = model\n        self.gradients = []\n        self.activations = []\n        self.reshape_transform = reshape_transform\n        self.handles = []\n        for target_layer in target_layers:\n            self.handles.append(\n                target_layer.register_forward_hook(self.save_activation))\n            # Because of https://github.com/pytorch/pytorch/issues/61519,\n            # we don't use backward hook to record gradients.\n            self.handles.append(\n                target_layer.register_forward_hook(self.save_gradient))\n\n    def save_activation(self, module, input, output):\n        activation = output\n\n        if self.reshape_transform is not None:\n            activation = self.reshape_transform(activation)\n        self.activations.append(activation.cpu().detach())\n\n    def save_gradient(self, module, input, output):\n        if not hasattr(output, \"requires_grad\") or not output.requires_grad:\n            # You can only register hooks on tensor requires grad.\n            return\n\n        # Gradients are computed in reverse order\n        def _store_grad(grad):\n            if self.reshape_transform is not None:\n                grad = self.reshape_transform(grad)\n            self.gradients = [grad.cpu().detach()] + self.gradients\n\n        output.register_hook(_store_grad)\n\n    def post_process(self, result):\n        if self.model.end2end:\n            logits_ = result[:, :, 4:]\n            boxes_ = result[:, :, :4]\n            sorted, indices = torch.sort(logits_[:, :, 0], descending=True)\n            return logits_[0][indices[0]], boxes_[0][indices[0]]\n        elif self.model.task == 'detect':\n            logits_ = result[:, 4:]\n            boxes_ = result[:, :4]\n            sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n            return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]]\n        elif self.model.task == 'segment':\n            logits_ = result[0][:, 4:4 + self.model.nc]\n            boxes_ = result[0][:, :4]\n            mask_p, mask_nm = result[1][2].squeeze(), result[1][1].squeeze().transpose(1, 0)\n            c, h, w = mask_p.size()\n            mask = (mask_nm @ mask_p.view(c, -1))\n            sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n            return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]], mask[indices[0]]\n        elif self.model.task == 'pose':\n            logits_ = result[:, 4:4 + self.model.nc]\n            boxes_ = result[:, :4]\n            poses_ = result[:, 4 + self.model.nc:]\n            sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n            return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(poses_[0], dim0=0, dim1=1)[indices[0]]\n        elif self.model.task == 'obb':\n            logits_ = result[:, 4:4 + self.model.nc]\n            boxes_ = result[:, :4]\n            angles_ = result[:, 4 + self.model.nc:]\n            sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n            return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(angles_[0], dim0=0, dim1=1)[indices[0]]\n        elif self.model.task == 'classify':\n            return result[0]\n  \n    def __call__(self, x):\n        self.gradients = []\n        self.activations = []\n        model_output = self.model(x)\n        if self.model.task == 'detect':\n            post_result, pre_post_boxes = self.post_process(model_output[0])\n            return [[post_result, pre_post_boxes]]\n        elif self.model.task == 'segment':\n            post_result, pre_post_boxes, pre_post_mask = self.post_process(model_output)\n            return [[post_result, pre_post_boxes, pre_post_mask]]\n        elif self.model.task == 'pose':\n            post_result, pre_post_boxes, pre_post_pose = self.post_process(model_output[0])\n            return [[post_result, pre_post_boxes, pre_post_pose]]\n        elif self.model.task == 'obb':\n            post_result, pre_post_boxes, pre_post_angle = self.post_process(model_output[0])\n            return [[post_result, pre_post_boxes, pre_post_angle]]\n        elif self.model.task == 'classify':\n            data = self.post_process(model_output)\n            return [data]\n\n    def release(self):\n        for handle in self.handles:\n            handle.remove()\n\nclass yolo_detect_target(torch.nn.Module):\n    def __init__(self, ouput_type, conf, ratio, end2end) -> None:\n        super().__init__()\n        self.ouput_type = ouput_type\n        self.conf = conf\n        self.ratio = ratio\n        self.end2end = end2end\n    \n    def forward(self, data):\n        post_result, pre_post_boxes = data\n        result = []\n        for i in trange(int(post_result.size(0) * self.ratio)):\n            if (self.end2end and float(post_result[i, 0]) < self.conf) or (not self.end2end and float(post_result[i].max()) < self.conf):\n                break\n            if self.ouput_type == 'class' or self.ouput_type == 'all':\n                if self.end2end:\n                    result.append(post_result[i, 0])\n                else:\n                    result.append(post_result[i].max())\n            elif self.ouput_type == 'box' or self.ouput_type == 'all':\n                for j in range(4):\n                    result.append(pre_post_boxes[i, j])\n        return sum(result)\n\nclass yolo_segment_target(yolo_detect_target):\n    def __init__(self, ouput_type, conf, ratio, end2end):\n        super().__init__(ouput_type, conf, ratio, end2end)\n    \n    def forward(self, data):\n        post_result, pre_post_boxes, pre_post_mask = data\n        result = []\n        for i in trange(int(post_result.size(0) * self.ratio)):\n            if float(post_result[i].max()) < self.conf:\n                break\n            if self.ouput_type == 'class' or self.ouput_type == 'all':\n                result.append(post_result[i].max())\n            elif self.ouput_type == 'box' or self.ouput_type == 'all':\n                for j in range(4):\n                    result.append(pre_post_boxes[i, j])\n            elif self.ouput_type == 'segment' or self.ouput_type == 'all':\n                result.append(pre_post_mask[i].mean())\n        return sum(result)\n\nclass yolo_pose_target(yolo_detect_target):\n    def __init__(self, ouput_type, conf, ratio, end2end):\n        super().__init__(ouput_type, conf, ratio, end2end)\n    \n    def forward(self, data):\n        post_result, pre_post_boxes, pre_post_pose = data\n        result = []\n        for i in trange(int(post_result.size(0) * self.ratio)):\n            if float(post_result[i].max()) < self.conf:\n                break\n            if self.ouput_type == 'class' or self.ouput_type == 'all':\n                result.append(post_result[i].max())\n            elif self.ouput_type == 'box' or self.ouput_type == 'all':\n                for j in range(4):\n                    result.append(pre_post_boxes[i, j])\n            elif self.ouput_type == 'pose' or self.ouput_type == 'all':\n                result.append(pre_post_pose[i].mean())\n        return sum(result)\n\nclass yolo_obb_target(yolo_detect_target):\n    def __init__(self, ouput_type, conf, ratio, end2end):\n        super().__init__(ouput_type, conf, ratio, end2end)\n    \n    def forward(self, data):\n        post_result, pre_post_boxes, pre_post_angle = data\n        result = []\n        for i in trange(int(post_result.size(0) * self.ratio)):\n            if float(post_result[i].max()) < self.conf:\n                break\n            if self.ouput_type == 'class' or self.ouput_type == 'all':\n                result.append(post_result[i].max())\n            elif self.ouput_type == 'box' or self.ouput_type == 'all':\n                for j in range(4):\n                    result.append(pre_post_boxes[i, j])\n            elif self.ouput_type == 'obb' or self.ouput_type == 'all':\n                result.append(pre_post_angle[i])\n        return sum(result)\n\nclass yolo_classify_target(yolo_detect_target):\n    def __init__(self, ouput_type, conf, ratio, end2end):\n        super().__init__(ouput_type, conf, ratio, end2end)\n    \n    def forward(self, data):\n        return data.max()\n\nclass yolo_heatmap:\n    def __init__(self, weight, device, method, layer, backward_type, conf_threshold, ratio, show_result, renormalize, task, img_size):\n        device = torch.device(device)\n        model_yolo = YOLO(weight)\n        model_names = model_yolo.names\n        print(f'model class info:{model_names}')\n        model = copy.deepcopy(model_yolo.model)\n        model.to(device)\n        model.info()\n        for p in model.parameters():\n            p.requires_grad_(True)\n        model.eval()\n        \n        model.task = task\n        if not hasattr(model, 'end2end'):\n            model.end2end = False\n        \n        if task == 'detect':\n            target = yolo_detect_target(backward_type, conf_threshold, ratio, model.end2end)\n        elif task == 'segment':\n            target = yolo_segment_target(backward_type, conf_threshold, ratio, model.end2end)\n        elif task == 'pose':\n            target = yolo_pose_target(backward_type, conf_threshold, ratio, model.end2end)\n        elif task == 'obb':\n            target = yolo_obb_target(backward_type, conf_threshold, ratio, model.end2end)\n        elif task == 'classify':\n            target = yolo_classify_target(backward_type, conf_threshold, ratio, model.end2end)\n        else:\n            raise Exception(f\"not support task({task}).\")\n        \n        target_layers = [model.model[l] for l in layer]\n        method = eval(method)(model, target_layers)\n        method.activations_and_grads = ActivationsAndGradients(model, target_layers, None)\n        \n        colors = np.random.uniform(0, 255, size=(len(model_names), 3)).astype(np.int32)\n        self.__dict__.update(locals())\n    \n    def post_process(self, result):\n        result = non_max_suppression(result, conf_thres=self.conf_threshold, iou_thres=0.65)[0]\n        return result\n\n    def draw_detections(self, box, color, name, img):\n        xmin, ymin, xmax, ymax = list(map(int, list(box)))\n        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), tuple(int(x) for x in color), 2) # ÁªòÂà∂Ê£ÄÊµãÊ°Ü\n        cv2.putText(img, str(name), (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, tuple(int(x) for x in color), 2, lineType=cv2.LINE_AA)  # ÁªòÂà∂Á±ªÂà´„ÄÅÁΩÆ‰ø°Â∫¶\n        return img\n\n    def renormalize_cam_in_bounding_boxes(self, boxes, image_float_np, grayscale_cam):\n        \"\"\"Normalize the CAM to be in the range [0, 1] \n        inside every bounding boxes, and zero outside of the bounding boxes. \"\"\"\n        renormalized_cam = np.zeros(grayscale_cam.shape, dtype=np.float32)\n        for x1, y1, x2, y2 in boxes:\n            x1, y1 = max(x1, 0), max(y1, 0)\n            x2, y2 = min(grayscale_cam.shape[1] - 1, x2), min(grayscale_cam.shape[0] - 1, y2)\n            renormalized_cam[y1:y2, x1:x2] = scale_cam_image(grayscale_cam[y1:y2, x1:x2].copy())    \n        renormalized_cam = scale_cam_image(renormalized_cam)\n        eigencam_image_renormalized = show_cam_on_image(image_float_np, renormalized_cam, use_rgb=True)\n        return eigencam_image_renormalized\n    \n    def process(self, img_path, save_path):\n        # img process\n        try:\n            img = cv2.imdecode(np.fromfile(img_path, np.uint8), cv2.IMREAD_COLOR)\n        except:\n            print(f\"Warning... {img_path} read failure.\")\n            return\n        img, _, (top, bottom, left, right) = letterbox(img, new_shape=(self.img_size, self.img_size), auto=True) # Â¶ÇÊûúÈúÄË¶ÅÂÆåÂÖ®Âõ∫ÂÆöÊàêÂÆΩÈ´ò‰∏ÄÊ†∑Â∞±ÊääautoËÆæÁΩÆ‰∏∫False\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = np.float32(img) / 255.0\n        tensor = torch.from_numpy(np.transpose(img, axes=[2, 0, 1])).unsqueeze(0).to(self.device)\n        print(f'tensor size:{tensor.size()}')\n        \n        try:\n            grayscale_cam = self.method(tensor, [self.target])\n        except AttributeError as e:\n            print(f\"Warning... self.method(tensor, [self.target]) failure.\")\n            return\n        \n        grayscale_cam = grayscale_cam[0, :]\n        cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n        \n        pred = self.model_yolo.predict(tensor, conf=self.conf_threshold, iou=0.7)[0]\n        if self.renormalize and self.task in ['detect', 'segment', 'pose']:\n            cam_image = self.renormalize_cam_in_bounding_boxes(pred.boxes.xyxy.cpu().detach().numpy().astype(np.int32), img, grayscale_cam)\n        if self.show_result:\n            cam_image = pred.plot(img=cam_image,\n                                  conf=True, # ÊòæÁ§∫ÁΩÆ‰ø°Â∫¶\n                                  font_size=None, # Â≠ó‰ΩìÂ§ßÂ∞èÔºåNone‰∏∫Ê†πÊçÆÂΩìÂâçimageÂ∞∫ÂØ∏ËÆ°ÁÆó\n                                  line_width=None, # Á∫øÊù°ÂÆΩÂ∫¶ÔºåNone‰∏∫Ê†πÊçÆÂΩìÂâçimageÂ∞∫ÂØ∏ËÆ°ÁÆó\n                                  labels=False, # ÊòæÁ§∫Ê†áÁ≠æ\n                                  )\n        \n        # ÂéªÊéâpaddingËæπÁïå\n        cam_image = cam_image[top:cam_image.shape[0] - bottom, left:cam_image.shape[1] - right]\n        cam_image = Image.fromarray(cam_image)\n        cam_image.save(save_path)\n    \n    def __call__(self, img_path, save_path):\n        # remove dir if exist\n        if os.path.exists(save_path):\n            shutil.rmtree(save_path)\n        # make dir if not exist\n        os.makedirs(save_path, exist_ok=True)\n\n        if os.path.isdir(img_path):\n            for img_path_ in os.listdir(img_path):\n                self.process(f'{img_path}/{img_path_}', f'{save_path}/{img_path_}')\n        else:\n            self.process(img_path, f'{save_path}/result.png')\n        \ndef get_params():\n    params = {\n        'weight': 'yolo11n.pt', # Áé∞Âú®Âè™ÈúÄË¶ÅÊåáÂÆöÊùÉÈáçÂç≥ÂèØ,‰∏çÈúÄË¶ÅÊåáÂÆöcfg\n        'device': 'cuda:0',\n        'method': 'GradCAMPlusPlus', # GradCAMPlusPlus, GradCAM, XGradCAM, EigenCAM, HiResCAM, LayerCAM, RandomCAM, EigenGradCAM, KPCA_CAM\n        'layer': [10, 12, 14, 16, 18],\n        'backward_type': 'all', # detect:<class, box, all> segment:<class, box, segment, all> pose:<box, keypoint, all> obb:<box, angle, all> classify:<all>\n        'conf_threshold': 0.2, # 0.2\n        'ratio': 0.02, # 0.02-0.1\n        'show_result': True, # ‰∏çÈúÄË¶ÅÁªòÂà∂ÁªìÊûúËØ∑ËÆæÁΩÆ‰∏∫False\n        'renormalize': False, # ÈúÄË¶ÅÊääÁÉ≠ÂäõÂõæÈôêÂà∂Âú®Ê°ÜÂÜÖËØ∑ËÆæÁΩÆ‰∏∫True(‰ªÖÂØπdetect,segment,poseÊúâÊïà)\n        'task':'detect', # ‰ªªÂä°(detect,segment,pose,obb,classify)\n        'img_size':640, # ÂõæÂÉèÂ∞∫ÂØ∏\n    }\n    return params\n\n# pip install grad-cam==1.5.4 --no-deps\nif __name__ == '__main__':\n    model = yolo_heatmap(**get_params())\n    model(r'/home/hjj/Desktop/dataset/dataset_coco/coco/images/val2017/000000361238.jpg', 'result')\n    # model(r'/home/hjj/Desktop/dataset/dataset_coco/coco/images/val2017', 'result')", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "yolo-gradcam/yolov11_heatmap.py", "language": "python", "complexity": "complex", "star_count": 6481, "quality_score": 0.9099999999999999, "features": {"has_code_examples": false, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 364, "word_count": 1431}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\nimport torch, yaml, cv2, os, shutil, sys, copy\nimport numpy as np\nnp.random.seed(0)\nimport matplotlib.pyplot as plt\nfrom tqdm import trange\nfrom PIL import Image\nfrom ultralytics import YOLO\nfrom ultralytics.nn.tasks import attempt_load_weights\nfrom ultralytics.utils.torch_utils import intersect_dicts\nfrom ultralytics.utils.ops import xywh2xyxy, non_max_suppression\nfrom pytorch_grad_cam import GradCAMPlusPlus, GradCAM, XGradCAM, EigenCAM, HiResCAM, LayerCAM, RandomCAM, EigenGradCAM, KPCA_CAM, AblationCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\nfrom pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients\n\ndef letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n    # Resize and pad image while meeting stride-multiple constraints\n    shape = im.shape[:2]  # current shape [height, width]\n    if isinstance(new_shape, int):\n        new_shape = (new_shape, new_shape)\n\n    # Scale ratio (new / old)\n    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n        r = min(r, 1.0)\n\n    # Compute padding\n    ratio = r, r  # width, height ratios\n    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n    if auto:  # minimum rectangle\n        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n    elif scaleFill:  # stretch\n        dw, dh = 0.0, 0.0\n        new_unpad = (new_shape[1], new_shape[0])\n        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n\n    dw /= 2  # divide padding into 2 sides\n    dh /= 2\n\n    if shape[::-1] != new_unpad:  # resize\n        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n    return im, ratio, (top, bottom, left, right)\n\nclass ActivationsAndGradients:\n    \"\"\" Class for extracting activations and\n    registering gradients from targetted intermediate layers \"\"\"\n\n    def __init__(self, model, target_layers, reshape_transform):\n        self.model = model\n        self.gradients = []\n        self.activations = []\n        self.reshape_transform = reshape_transform\n        self.handles = []\n        for target_layer in target_layers:\n            self.handles.append(\n                target_layer.register_forward_hook(self.save_activation))\n            # Because of https://github.com/pytorch/pytorch/issues/61519,\n            # we don't use backward hook to record gradients.\n            self.handles.append(\n                target_layer.register_forward_hook(self.save_gradient))\n\n    def save_activation(self, module, input, output):\n        activation = output\n\n        if self.reshape_transform is not None:\n            activation = self.reshape_transform(activation)\n        self.activations.append(activation.cpu().detach())\n\n    def save_gradient(self, module, input, output):\n        if not hasattr(output, \"requires_grad\") or not output.requires_grad:\n            # You can only register hooks on tensor requires grad.\n            return\n\n        # Gradients are computed in reverse order\n        def _store_grad(grad):\n            if self.reshape_transform is not None:\n                grad = self.reshape_transform(grad)\n            self.gradients = [grad.cpu().detach()] + self.gradients\n\n        output.register_hook(_store_grad)\n\n    def post_process(self, result):\n        if self.model.end2end:\n            logits_ = result[:, :, 4:]\n            boxes_ = result[:, :, :4]\n            sorted, indices = torch.sort(logits_[:, :, 0], descending=True)\n            return logits_[0][indices[0]], boxes_[0][indices[0]]\n        elif self.model.task == 'detect':\n            logits_ = result[:, 4:]\n            boxes_ = result[:, :4]\n            sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n            return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]]\n        elif self.model.task == 'segment':\n            logits_ = result[0][:, 4:4 + self.model.nc]\n            boxes_ = result[0][:, :4]\n            mask_p, mask_nm = result[1][2].squeeze(), result[1][1].squeeze().transpose(1, 0)\n            c, h, w = mask_p.size()\n            mask = (mask_nm @ mask_p.view(c, -1))\n            sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n            return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]], mask[indices[0]]\n        elif self.model.task == 'pose':\n            logits_ = result[:, 4:4 + self.model.nc]\n            boxes_ = result[:, :4]\n            poses_ = result[:, 4 + self.model.nc:]\n            sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n            return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(poses_[0], dim0=0, dim1=1)[indices[0]]\n        elif self.model.task == 'obb':\n            logits_ = result[:, 4:4 + self.model.nc]\n            boxes_ = result[:, :4]\n            angles_ = result[:, 4 + self.model.nc:]\n            sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n            return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(angles_[0], dim0=0, dim1=1)[indices[0]]\n        elif self.model.task == 'classify':\n            return result[0]\n  \n    def __call__(self, x):\n        self.gradients = []\n        self.activations = []\n        model_output = self.model(x)\n        if self.model.task == 'detect':\n            post_result, pre_post_boxes = self.post_process(model_output[0])\n            return [[post_result, pre_post_boxes]]\n        elif self.model.task == 'segment':\n            post_result, pre_post_boxes, pre_post_mask = self.post_process(model_output)\n            return [[post_result, pre_post_boxes, pre_post_mask]]\n        elif self.model.task == 'pose':\n            post_result, pre_post_boxes, pre_post_pose = self.post_process(model_output[0])\n            return [[post_result, pre_post_boxes, pre_post_pose]]\n        elif self.model.task == 'obb':\n            post_result, pre_post_boxes, pre_post_angle = self.post_process(model_output[0])\n            return [[post_result, pre_post_boxes, pre_post_angle]]\n        elif self.model.task == 'classify':\n            data = self.post_process(model_output)\n            return [data]\n\n    def release(self):\n        for handle in self.handles:\n            handle.remove()\n\nclass yolo_detect_target(torch.nn.Module):\n    def __init__(self, ouput_type, conf, ratio, end2end) -> None:\n        super().__init__()\n        self.ouput_type = ouput_type\n        self.conf = conf\n        self.ratio = ratio\n        self.end2end = end2end\n    \n    def forward(self, data):\n        post_result, pre_post_boxes = data\n        result = []\n        for i in trange(int(post_result.size(0) * self.ratio)):\n            if (self.end2end and float(post_result[i, 0]) < self.conf) or (not self.end2end and float(post_result[i].max()) < self.conf):\n                break\n            if self.ouput_type == 'class' or self.ouput_type == 'all':\n                if self.end2end:\n                    result.append(post_result[i, 0])\n                else:\n                    result.append(post_result[i].max())\n            elif self.ouput_type == 'box' or self.ouput_type == 'all':\n                for j in range(4):\n                    result.append(pre_post_boxes[i, j])\n        return sum(result)\n\nclass yolo_segment_target(yolo_detect_target):\n    def __init__(self, ouput_type, conf, ratio, end2end):\n        super().__init__(ouput_type, conf, ratio, end2end)\n    \n    def forward(self, data):\n        post_result, pre_post_boxes, pre_post_mask = data\n        result = []\n        for i in trange(int(post_result.size(0) * self.ratio)):\n            if float(post_result[i].max()) < self.conf:\n                break\n            if self.ouput_type == 'class' or self.ouput_type == 'all':\n                result.append(post_result[i].max())\n            elif self.ouput_type == 'box' or self.ouput_type == 'all':\n                for j in range(4):\n                    result.append(pre_post_boxes[i, j])\n            elif self.ouput_type == 'segment' or self.ouput_type == 'all':\n                result.append(pre_post_mask[i].mean())\n        return sum(result)\n\nclass yolo_pose_target(yolo_detect_target):\n    def __init__(self, ouput_type, conf, ratio, end2end):\n        super().__init__(ouput_type, conf, ratio, end2end)\n    \n    def forward(self, data):\n        post_result, pre_post_boxes, pre_post_pose = data\n        result = []\n        for i in trange(int(post_result.size(0) * self.ratio)):\n            if float(post_result[i].max()) < self.conf:\n                break\n            if self.ouput_type == 'class' or self.ouput_type == 'all':\n                result.append(post_result[i].max())\n            elif self.ouput_type == 'box' or self.ouput_type == 'all':\n                for j in range(4):\n                    result.append(pre_post_boxes[i, j])\n            elif self.ouput_type == 'pose' or self.ouput_type == 'all':\n                result.append(pre_post_pose[i].mean())\n        return sum(result)\n\nclass yolo_obb_target(yolo_detect_target):\n    def __init__(self, ouput_type, conf, ratio, end2end):\n        super().__init__(ouput_type, conf, ratio, end2end)\n    \n    def forward(self, data):\n        post_result, pre_post_boxes, pre_post_angle = data\n        result = []\n        for i in trange(int(post_result.size(0) * self.ratio)):\n            if float(post_result[i].max()) < self.conf:\n                break\n            if self.ouput_type == 'class' or self.ouput_type == 'all':\n                result.append(post_result[i].max())\n            elif self.ouput_type == 'box' or self.ouput_type == 'all':\n                for j in range(4):\n                    result.append(pre_post_boxes[i, j])\n            elif self.ouput_type == 'obb' or self.ouput_type == 'all':\n                result.append(pre_post_angle[i])\n        return sum(result)\n\nclass yolo_classify_target(yolo_detect_target):\n    def __init__(self, ouput_type, conf, ratio, end2end):\n        super().__init__(ouput_type, conf, ratio, end2end)\n    \n    def forward(self, data):\n        return data.max()\n\nclass yolo_heatmap:\n    def __init__(self, weight, device, method, layer, backward_type, conf_threshold, ratio, show_result, renormalize, task, img_size):\n        device = torch.device(device)\n        model_yolo = YOLO(weight)\n        model_names = model_yolo.names\n        print(f'model class info:{model_names}')\n        model = copy.deepcopy(model_yolo.model)\n        model.to(device)\n        model.info()\n        for p in model.parameters():\n            p.requires_grad_(True)\n        model.eval()\n        \n        model.task = task\n        if not hasattr(model, 'end2end'):\n            model.end2end = False\n        \n        if task == 'detect':\n            target = yolo_detect_target(backward_type, conf_threshold, ratio, model.end2end)\n        elif task == 'segment':\n            target = yolo_segment_target(backward_type, conf_threshold, ratio, model.end2end)\n        elif task == 'pose':\n            target = yolo_pose_target(backward_type, conf_threshold, ratio, model.end2end)\n        elif task == 'obb':\n            target = yolo_obb_target(backward_type, conf_threshold, ratio, model.end2end)\n        elif task == 'classify':\n            target = yolo_classify_target(backward_type, conf_threshold, ratio, model.end2end)\n        else:\n            raise Exception(f\"not support task({task}).\")\n        \n        target_layers = [model.model[l] for l in layer]\n        method = eval(method)(model, target_layers)\n        method.activations_and_grads = ActivationsAndGradients(model, target_layers, None)\n        \n        colors = np.random.uniform(0, 255, size=(len(model_names), 3)).astype(np.int32)\n        self.__dict__.update(locals())\n    \n    def post_process(self, result):\n        result = non_max_suppression(result, conf_thres=self.conf_threshold, iou_thres=0.65)[0]\n        return result\n\n    def draw_detections(self, box, color, name, img):\n        xmin, ymin, xmax, ymax = list(map(int, list(box)))\n        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), tuple(int(x) for x in color), 2) # ÁªòÂà∂Ê£ÄÊµãÊ°Ü\n        cv2.putText(img, str(name), (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, tuple(int(x) for x in color), 2, lineType=cv2.LINE_AA)  # ÁªòÂà∂Á±ªÂà´„ÄÅÁΩÆ‰ø°Â∫¶\n        return img\n\n    def renormalize_cam_in_bounding_boxes(self, boxes, image_float_np, grayscale_cam):\n        \"\"\"Normalize the CAM to be in the range [0, 1] \n        inside every bounding boxes, and zero outside of the bounding boxes. \"\"\"\n        renormalized_cam = np.zeros(grayscale_cam.shape, dtype=np.float32)\n        for x1, y1, x2, y2 in boxes:\n            x1, y1 = max(x1, 0), max(y1, 0)\n            x2, y2 = min(grayscale_cam.shape[1] - 1, x2), min(grayscale_cam.shape[0] - 1, y2)\n            renormalized_cam[y1:y2, x1:x2] = scale_cam_image(grayscale_cam[y1:y2, x1:x2].copy())    \n        renormalized_cam = scale_cam_image(renormalized_cam)\n        eigencam_image_renormalized = show_cam_on_image(image_float_np, renormalized_cam, use_rgb=True)\n        return eigencam_image_renormalized\n    \n    def process(self, img_path, save_path):\n        # img process\n        try:\n            img = cv2.imdecode(np.fromfile(img_path, np.uint8), cv2.IMREAD_COLOR)\n        except:\n            print(f\"Warning... {img_path} read failure.\")\n            return\n        img, _, (top, bottom, left, right) = letterbox(img, new_shape=(self.img_size, self.img_size), auto=True) # Â¶ÇÊûúÈúÄË¶ÅÂÆåÂÖ®Âõ∫ÂÆöÊàêÂÆΩÈ´ò‰∏ÄÊ†∑Â∞±ÊääautoËÆæÁΩÆ‰∏∫False\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = np.float32(img) / 255.0\n        tensor = torch.from_numpy(np.transpose(img, axes=[2, 0, 1])).unsqueeze(0).to(self.device)\n        print(f'tensor size:{tensor.size()}')\n        \n        try:\n            grayscale_cam = self.method(tensor, [self.target])\n        except AttributeError as e:\n            print(f\"Warning... self.method(tensor, [self.target]) failure.\")\n            return\n        \n        grayscale_cam = grayscale_cam[0, :]\n        cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n        \n        pred = self.model_yolo.predict(tensor, conf=self.conf_threshold, iou=0.7)[0]\n        if self.renormalize and self.task in ['detect', 'segment', 'pose']:\n            cam_image = self.renormalize_cam_in_bounding_boxes(pred.boxes.xyxy.cpu().detach().numpy().astype(np.int32), img, grayscale_cam)\n        if self.show_result:\n            cam_image = pred.plot(img=cam_image,\n                                  conf=True, # ÊòæÁ§∫ÁΩÆ‰ø°Â∫¶\n                                  font_size=None, # Â≠ó‰ΩìÂ§ßÂ∞èÔºåNone‰∏∫Ê†πÊçÆÂΩìÂâçimageÂ∞∫ÂØ∏ËÆ°ÁÆó\n                                  line_width=None, # Á∫øÊù°ÂÆΩÂ∫¶ÔºåNone‰∏∫Ê†πÊçÆÂΩìÂâçimageÂ∞∫ÂØ∏ËÆ°ÁÆó\n                                  labels=False, # ÊòæÁ§∫Ê†áÁ≠æ\n                                  )\n        \n        # ÂéªÊéâpaddingËæπÁïå\n        cam_image = cam_image[top:cam_image.shape[0] - bottom, left:cam_image.shape[1] - right]\n        cam_image = Image.fromarray(cam_image)\n        cam_image.save(save_path)\n    \n    def __call__(self, img_path, save_path):\n        # remove dir if exist\n        if os.path.exists(save_path):\n            shutil.rmtree(save_path)\n        # make dir if not exist\n        os.makedirs(save_path, exist_ok=True)\n\n        if os.path.isdir(img_path):\n            for img_path_ in os.listdir(img_path):\n                self.process(f'{img_path}/{img_path_}', f'{save_path}/{img_path_}')\n        else:\n            self.process(img_path, f'{save_path}/result.png')\n        \ndef get_params():\n    params = {\n        'weight': 'yolo11n.pt', # Áé∞Âú®Âè™ÈúÄË¶ÅÊåáÂÆöÊùÉÈáçÂç≥ÂèØ,‰∏çÈúÄË¶ÅÊåáÂÆöcfg\n        'device': 'cuda:0',\n        'method': 'GradCAMPlusPlus', # GradCAMPlusPlus, GradCAM, XGradCAM, EigenCAM, HiResCAM, LayerCAM, RandomCAM, EigenGradCAM, KPCA_CAM\n        'layer': [10, 12, 14, 16, 18],\n        'backward_type': 'all', # detect:<class, box, all> segment:<class, box, segment, all> pose:<box, keypoint, all> obb:<box, angle, all> classify:<all>\n        'conf_threshold': 0.2, # 0.2\n        'ratio': 0.02, # 0.02-0.1\n        'show_result': True, # ‰∏çÈúÄË¶ÅÁªòÂà∂ÁªìÊûúËØ∑ËÆæÁΩÆ‰∏∫False\n        'renormalize': False, # ÈúÄË¶ÅÊääÁÉ≠ÂäõÂõæÈôêÂà∂Âú®Ê°ÜÂÜÖËØ∑ËÆæÁΩÆ‰∏∫True(‰ªÖÂØπdetect,segment,poseÊúâÊïà)\n        'task':'detect', # ‰ªªÂä°(detect,segment,pose,obb,classify)\n        'img_size':640, # ÂõæÂÉèÂ∞∫ÂØ∏\n    }\n    return params\n\n# pip install grad-cam==1.5.4 --no-deps\nif __name__ == '__main__':\n    model = yolo_heatmap(**get_params())\n    model(r'/home/hjj/Desktop/dataset/dataset_coco/coco/images/val2017/000000361238.jpg', 'result')\n    # model(r'/home/hjj/Desktop/dataset/dataset_coco/coco/images/val2017', 'result')", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "yolo-gradcam/yolov8_heatmap.py", "language": "python", "complexity": "complex", "star_count": 6481, "quality_score": 0.9099999999999999, "features": {"has_code_examples": false, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 364, "word_count": 1431}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# YOLO-Improve\nËøô‰∏™È°πÁõÆ‰∏ªË¶ÅÊòØÊèê‰æõ‰∏Ä‰∫õÂÖ≥‰∫éyoloÁ≥ªÂàóÊ®°ÂûãÁöÑÊîπËøõÊÄùË∑ØÔºåÊïàÊûúÂõ†Êï∞ÊçÆÈõÜÂíåÂèÇÊï∞ËÄåÂºÇÔºå‰ªÖ‰ΩúÂèÇËÄÉ„ÄÇ  \n\n\n# Explanation\n- **iou**  \n    Ê∑ªÂä†EIOUÔºåSIOUÔºåALPHA-IOU, FocalEIOU, Wise-IOUÂà∞yolov5,yolov8ÁöÑbox_iou‰∏≠.  \n    1. yolov5\n        ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1KM411b7Sz/).  \n        ÂçöÂÆ¢Âú∞ÂùÄÔºö[CSDN](https://blog.csdn.net/qq_37706472/article/details/128737484?spm=1001.2014.3001.5501).\n\n        #### 2023-2-8 Êõ¥Êñ∞: Êñ∞Â¢û[Wise-IoU](https://arxiv.org/abs/2301.10051) ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1tG4y1N7Gk/). reference:[github](https://github.com/Instinct323/wiou)  \n    2. yolov8\n        ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1PY4y1o7Hm/).  \n        ÂçöÂÆ¢Âú∞ÂùÄÔºö[CSDN](https://blog.csdn.net/qq_37706472/article/details/128743012?spm=1001.2014.3001.5502).\n\n        #### 2023-2-7 Êõ¥Êñ∞: Êñ∞Â¢û[Wise-IoU](https://arxiv.org/abs/2301.10051) ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1De4y1N7Mb/). reference:[github](https://github.com/Instinct323/wiou)   \n- **yolov5-GFPN**   \n    ‰ΩøÁî®DAMO-YOLO‰∏≠ÁöÑGFPNÊõøÊç¢YOLOV5‰∏≠ÁöÑHead.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1iR4y1a7bx/).  \n- **yolov5-C2F**  \n    ‰ΩøÁî®yolov8‰∏≠ÁöÑC2FÊ®°ÂùóÊõøÊç¢yolov5‰∏≠ÁöÑC3Ê®°Âùó.(Ëøô‰∏™Êìç‰ΩúÊØîËæÉÁÆÄÂçïÔºåÂõ†Ê≠§Â∞±‰∏çÊèê‰æõ‰ª£Á†ÅÔºåÁõ¥Êé•ÁúãËßÜÈ¢ëÊìç‰Ωú‰∏Ä‰∏ãÂç≥ÂèØ)  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1rx4y1g7xt/).  \n- **yolov7-iou**  \n    Ê∑ªÂä†EIOUÔºåSIOUÔºåALPHA-IOU, FocalEIOU, Wise-IOUÂà∞yolov7ÁöÑbox_iou‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1zx4y177EF/).  \n    ÂçöÂÆ¢Âú∞ÂùÄÔºö[CSDN](https://blog.csdn.net/qq_37706472/article/details/128780275?spm=1001.2014.3001.5502).  \n    #### 2023-2-11 Êõ¥Êñ∞: Êñ∞Â¢û[Wise-IoU](https://arxiv.org/abs/2301.10051) ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1yv4y147kf/). reference:[github](https://github.com/Instinct323/wiou)  \n- **yolov5-OTA**  \n    Ê∑ªÂä†Optimal Transport AssignmentÂà∞yolov5ÁöÑLoss‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1xD4y1J76n/).  \n- **yolov5-DCN**  \n    Ê∑ªÂä†Deformable convolution V2Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1rT411Q76q/).  \n- **yolov8-DCN**  \n    Ê∑ªÂä†Deformable convolution V2Âà∞yolov8‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Fo4y1i7Mm/).  \n- **yolov7-DCN**  \n    Ê∑ªÂä†Deformable convolution V2Âà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV17R4y1q7vr/).  \n- **yolov5-AUX**\n    Ê∑ªÂä†ËæÖÂä©ËÆ≠ÁªÉÂàÜÊîØÂà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Fo4y1v7bi/).  \n    ÂéüÁêÜÂèÇËÄÉÈìæÊé•Ôºö[Áü•‰πé](https://zhuanlan.zhihu.com/p/588947172)\n- **CAM**  \n    Ê∑ªÂä†context augmentation moduleÂà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV17b411d7ef/).  \n    paperÔºö[ÈìæÊé•](https://openreview.net/pdf?id=q2ZaVU6bEsT)\n- **yolov5-SAConv**  \n    Ê∑ªÂä†SACÂà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1xD4y1u7NU/).  \n    paperÔºö[ÈìæÊé•](https://arxiv.org/pdf/2006.02334.pdf)  \n    reference: [ÈìæÊé•](https://github.com/joe-siyuan-qiao/DetectoRS)\n- **yolov7-SAConv**  \n    Ê∑ªÂä†SACÂà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1xD4y1u7NU/).  \n    paperÔºö[ÈìæÊé•](https://arxiv.org/pdf/2006.02334.pdf)  \n    reference: [ÈìæÊé•](https://github.com/joe-siyuan-qiao/DetectoRS)\n- **yolov5-CoordConv**  \n    Ê∑ªÂä†CoordConvÂà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1ng4y1E7rS/).   \n    reference: [ÈìæÊé•](https://blog.csdn.net/qq_35608277/article/details/125257225)\n- **yolov5-soft-nms**  \n    Ê∑ªÂä†soft-nms(IoU,GIoU,DIoU,CIoU,EIoU,SIoU)Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1cM41147Ry/).  \n- **yolov7-CoordConv**  \n    Ê∑ªÂä†CoordConvÂà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1K54y1g7ye/).   \n    reference: [ÈìæÊé•](https://blog.csdn.net/qq_35608277/article/details/125257225)\n- **yolov7-soft-nms**  \n    Ê∑ªÂä†soft-nms(IoU,GIoU,DIoU,CIoU,EIoU,SIoU)Âà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1ZY41167iC/). \n- **yolov5-DSConv**  \n    Ê∑ªÂä†DSConvÂà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1iT411a7Mi/).   \n    paper: [ÈìæÊé•](https://arxiv.org/abs/1901.01928)  \n    reference: [ÈìæÊé•](https://github.com/ActiveVisionLab/DSConv)\n- **yolov7-DSConv**  \n    Ê∑ªÂä†DSConvÂà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1724y1b7PD/).   \n    paper: [ÈìæÊé•](https://arxiv.org/abs/1901.01928)  \n    reference: [ÈìæÊé•](https://github.com/ActiveVisionLab/DSConv)\n- **yolov5-DCNV3**  \n    Ê∑ªÂä†DCNV3Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1LY411z7iE/).   \n    Ë°•ÂÖÖ‰∫ãÈ°π-ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Dv4y1j7ij/).   \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2211.05778)  \n    reference: [ÈìæÊé•](https://github.com/OpenGVLab/InternImage)  \n- **yolov5-NWD**  \n    Ê∑ªÂä†Normalized Gaussian Wasserstein DistanceÂà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1zY4y197UP/).   \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2110.13389)  \n    reference: [ÈìæÊé•](https://github.com/jwwangchn/NWD)  \n- **yolov7-DCNV3**  \n    Ê∑ªÂä†DCNV3Âà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1mk4y1h7us/).   \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2211.05778)  \n    reference: [ÈìæÊé•](https://github.com/OpenGVLab/InternImage) \n- **yolov5-DecoupledHead**  \n    Ê∑ªÂä†Efficient-DecoupledHeadÂà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1mk4y1h7us/).   \n    paper: [yolov6ÈìæÊé•](https://arxiv.org/pdf/2301.05586.pdf)  \n    reference: [ÈìæÊé•](https://github.com/meituan/YOLOv6/blob/main/yolov6/models/effidehead.py) \n- **yolov5-FasterBlock**  \n    Ê∑ªÂä†FasterNet‰∏≠ÁöÑFaster-BlockÂà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Bs4y1H7Ph/).   \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2303.03667)  \n    reference: [ÈìæÊé•](https://github.com/JierunChen/FasterNet) \n- **yolov7-NWD**  \n    Ê∑ªÂä†Normalized Gaussian Wasserstein DistanceÂà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1kM411H7g1/).   \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2110.13389)  \n    reference: [ÈìæÊé•](https://github.com/jwwangchn/NWD)\n- **yolov7-DecoupledHead**  \n    Ê∑ªÂä†ÂÖ∑ÊúâÈöêÂºèÁü•ËØÜÂ≠¶‰π†ÁöÑEfficient-DecoupledHeadÂà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1tg4y1x7ha/).   \n    paper: [yolov6ÈìæÊé•](https://arxiv.org/pdf/2301.05586.pdf) [yolorÈìæÊé•](https://arxiv.org/abs/2105.04206) [yolorÂèÇËÄÉÂçöÂÆ¢](https://blog.csdn.net/AaronYKing/article/details/123804988)  \n    reference: [ÈìæÊé•](https://github.com/meituan/YOLOv6/blob/main/yolov6/models/effidehead.py) \n- **yolov5-backbone**  \n    Ê∑ªÂä†TimmÊîØÊåÅÁöÑ‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ÈúÄË¶ÅÂÆâË£ÖtimmÂ∫ì. ÂëΩ‰ª§: pip install -i https://pypi.tuna.tsinghua.edu.cn/simple timm  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Mx4y1A7jy/).   \n    reference: [ÈìæÊé•](https://github.com/huggingface/pytorch-image-models#:~:text=I%20missed%20anything.-,Models,-All%20model%20architecture)\n- **yolov7-PConv**  \n    Ê∑ªÂä†FasterNet‰∏≠ÁöÑPConvÂà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Z84y137oi/).   \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2303.03667)  \n    reference: [ÈìæÊé•](https://github.com/JierunChen/FasterNet) \n- **yolov5-TSCODE**  \n    Ê∑ªÂä†Task-Specific Context DecouplingÂà∞yolov5‰∏≠.  \n    ÈúÄË¶ÅÂÆâË£ÖeinopsÂ∫ì. ÂëΩ‰ª§: pip install -i https://pypi.tuna.tsinghua.edu.cn/simple einops  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1mk4y1h7us/).   \n    paper: [yolov6ÈìæÊé•](https://arxiv.org/pdf/2303.01047v1.pdf)  \n- **yolov5-backbone/fasternet**  \n    Ê∑ªÂä†FasterNet‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1ra4y1K77u/).   \n    reference: [ÈìæÊé•](https://github.com/JierunChen/FasterNet)\n- **yolov5-backbone/ODConv**  \n    Ê∑ªÂä†Omni-Dimensional Dynamic Convolution‰∏ªÂπ≤(od_mobilenetv2,od_resnet)Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Jk4y1v7EW/).   \n    reference: [ÈìæÊé•](https://github.com/OSVAI/ODConv)  \n- **yolov5-backbone/ODConvFuse**  \n    ËûçÂêàOmni-Dimensional Dynamic Convolution‰∏ªÂπ≤(od_mobilenetv2,od_resnet)‰∏≠ÁöÑConvÂíåBN.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Rs4y1N7fp/).   \n- **yolov5-CARAFE**  \n    Ê∑ªÂä†ËΩªÈáèÁ∫ß‰∏äÈááÊ†∑ÁÆóÂ≠êCARAFEÂà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1kj411c72a/).  [yolov7‰øÆÊîπËßÜÈ¢ë-ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1yc411p7wL/).  \n    reference: [ÈìæÊé•](https://github.com/XiaLiPKU/CARAFE)  \n- **yolov5-EVC**  \n    Ê∑ªÂä†CFPNet‰∏≠ÁöÑEVC-BlockÂà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Pg4y1u7cM/).  \n    reference: [ÈìæÊé•](https://github.com/QY1994-0919/CFPNet)  \n- **yolov5-dyhead**  \n    Ê∑ªÂä†Âü∫‰∫éÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÁõÆÊ†áÊ£ÄÊµãÂ§¥(DYHEAD)Âà∞yolov5‰∏≠.  \n    yolov7ÁâàÊú¨: [ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Ph4y1s7i9/).  \n    ÂÆâË£ÖÂëΩ‰ª§:\n\n        pip install -U openmim\n        mim install mmengine\n        mim install \"mmcv>=2.0.0\"\n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1qs4y117Mx/).  \n    reference: [ÈìæÊé•](https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/necks/dyhead.py)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2106.08322)  \n- **yolov5-backbone/inceptionnext**  \n    Ê∑ªÂä†(2023Âπ¥New)InceptionNeXt‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV12v4y1H7E1/).   \n    reference: [ÈìæÊé•](https://github.com/sail-sg/inceptionnext)  \n    paper: [ÈìæÊé•](https://arxiv.org/pdf/2303.16900.pdf)  \n- **yolov5-aLRPLoss**  \n    Ê∑ªÂä†aLRPLossÂà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1YV4y1Z7rV/).     \n    reference: [ÈìæÊé•](https://github.com/kemaloksuz/aLRPLoss)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2009.13592)  \n- **yolov5-res2block**  \n    ÁªìÂêàRes2NetÊèêÂá∫ÂÖ∑ÊúâÂ§öÂ∞∫Â∫¶ÊèêÂèñËÉΩÂäõÁöÑC3Ê®°Âùó.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV13X4y167VB/).     \n    reference: [ÈìæÊé•](https://github.com/Res2Net/Res2Net-PretrainedModels)  \n    paper: [ÈìæÊé•](https://arxiv.org/pdf/1904.01169.pdf)  \n- **yolov7-odconv**  \n    Ê∑ªÂä†Omni-Dimensional Dynamic ConvolutionÂà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1vh411j71Z/).     \n    reference: [ÈìæÊé•](https://github.com/OSVAI/ODConv)  \n- **yolov5-backbone/FocalNet**  \n    Ê∑ªÂä†(2022Âπ¥)FocalNet(transformer)‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1ch411L7Dk/).   \n    reference: [ÈìæÊé•](https://github.com/microsoft/FocalNet)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2203.11926)  \n- **yolov5-backbone/EMO**  \n    Ê∑ªÂä†(2023Âπ¥)EMO(transformer)‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Dh4y1J7SV/).   \n    reference: [ÈìæÊé•](https://github.com/zhangzjn/EMO)  \n    paper: [ÈìæÊé•](https://arxiv.org/pdf/2301.01146.pdf)  \n- **yolov5-backbone/EfficientFormerV2**  \n    Ê∑ªÂä†(2022Âπ¥)EfficientFormerV2(transformer)‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1da4y1g7KT/).   \n    reference: [ÈìæÊé•](https://github.com/snap-research/EfficientFormer)  \n    paper: [ÈìæÊé•](https://arxiv.org/pdf/2212.08059.pdf)  \n    weight_download: [ÁôæÂ∫¶ÁΩëÁõòÈìæÊé•](https://pan.baidu.com/s/1I0Ygc3-6fNf2LdIJe290kw?pwd=yvc8)\n- **yolov5-backbone/PoolFormer**  \n    Ê∑ªÂä†(2022Âπ¥CVPR)PoolFormer(transformer)‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1eh411c7bz/).   \n    reference: [ÈìæÊé•](https://github.com/sail-sg/poolformer)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2111.11418)  \n- **yolov5-backbone/EfficientViT**  \n    Ê∑ªÂä†(2023Âπ¥)EfficientViT(transformer)‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1xk4y1L7Gu/).   \n    reference: [ÈìæÊé•](https://github.com/mit-han-lab/efficientvit)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2205.14756)  \n    weight_download: [ÁôæÂ∫¶ÁΩëÁõòÈìæÊé•](https://pan.baidu.com/s/1dvwuQQBnRCr7aGReY8IEVw?pwd=74ad)\n- **yolov5-ContextAggregation**  \n    Ê∑ªÂä†ContextAggregationÂà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Yk4y1s7Kx/).     \n    reference: [ÈìæÊé•](https://github.com/yeliudev/CATNet)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2111.11057)  \n- **yolov5-backbone/VanillaNet**  \n    Ê∑ªÂä†(2023Âπ¥)VanillaNet‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1os4y1v7Du/).   \n    reference: [ÈìæÊé•](https://github.com/huawei-noah/VanillaNet)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2305.12972)  \n    weight_download: [ÁôæÂ∫¶ÁΩëÁõòÈìæÊé•](https://pan.baidu.com/s/1EBAiOtDVMhvQqu2NWoFSIg?pwd=ofx9)  \n- **yolov7-EVC**  \n    Ê∑ªÂä†CFPNet‰∏≠ÁöÑEVC-BlockÂà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV12u4y1f7np/).  \n    reference: [ÈìæÊé•](https://github.com/QY1994-0919/CFPNet)  \n- **yolov7-head**  \n    P2,P6Ê£ÄÊµãÂ±ÇÂú®YOLOV7‰∏≠ÁöÑÊ∑ªÂä†.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1LX4y1a72m/).  \n- **yolov7-slimneck**  \n    ‰ΩøÁî®VOVGSCSPËΩªÈáèÂåñyolov7ÁöÑNeck.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV14m4y147PC/).  \n    reference: [ÈìæÊé•](https://github.com/AlanLi1997/slim-neck-by-gsconv)  \n- **yolov5-SwinTransformer**  \n    Ê∑ªÂä†SwinTransformer-Tiny‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1WX4y1a7ea/).  \n    reference: [ÈìæÊé•](https://github.com/microsoft/Swin-Transformer)  \n    weight_download: [SwinTransformer-TinyÁôæÂ∫¶‰∫ëÈìæÊé•](https://pan.baidu.com/s/1vct0VYwwQQ8PYkBjwSSBZQ?pwd=swin)  \n- **yolov5-NextViT**  \n    Ê∑ªÂä†(2022Âπ¥)NextViT‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1im4y1i7Ht/).  \n    reference: [ÈìæÊé•](https://github.com/bytedance/Next-ViT)  \n    weight_download: [ÁôæÂ∫¶‰∫ëÈìæÊé•](https://pan.baidu.com/s/18IHKssf9kN8Ej7zIWBKfcw?pwd=houj)  \n- **yolov5-ConvNextV2**  \n    Ê∑ªÂä†(2023Âπ¥)ConvNextV2‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1es4y1e7b9/).  \n    reference: [ÈìæÊé•](https://github.com/facebookresearch/ConvNeXt-V2)  \n- **yolov5-RIFormer**  \n    Ê∑ªÂä†(2023Âπ¥)RIFormer‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1bW4y1X7Lo/).  \n    reference: [mmpretrainÈìæÊé•](https://github.com/open-mmlab/mmpretrain/blob/main/mmpretrain/models/backbones/riformer.py)  \n    weight_download: [mmpretrainÈìæÊé•](https://github.com/open-mmlab/mmpretrain/tree/main/configs/riformer)\n- **yolov5-C3RFEM**  \n    Scale-Aware RFE‰∏éC3ÁªìÂêàËÄåÊàêÁöÑC3RFEMÊ∑ªÂä†Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Gj411D7Pf/).  \n    reference: [ÈìæÊé•](https://github.com/Krasjet-Yu/YOLO-FaceV2)  \n- **yolov7-RFEM**  \n    Scale-Aware RFEÊ∑ªÂä†Âà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1hW4y1D7gQ/).  \n    reference: [ÈìæÊé•](https://github.com/Krasjet-Yu/YOLO-FaceV2)  \n- **yolov5-DBB**  \n    ÊääÈáçÂèÇÊï∞ÁªìÊûÑDiverseBranchBlock‰∏éC3ËûçÂêàÊàêC3-DBBÊ∑ªÂä†Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1sM4y177Cn/).  \n    reference: [ÈìæÊé•](https://github.com/DingXiaoH/DiverseBranchBlock)  \n- **yolov7-DBB**  \n    ÊääÈáçÂèÇÊï∞ÁªìÊûÑDiverseBranchBlockÊ∑ªÂä†Âà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV14u411b7kL/).  \n    reference: [ÈìæÊé•](https://github.com/DingXiaoH/DiverseBranchBlock)  \n- **yolov5-backbone/CVPR2023-EfficientViT**  \n    Ê∑ªÂä†(2023CVPR)EfficientViT(transformer)‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1xk4y1L7Gu/).   \n    reference: [ÈìæÊé•](https://github.com/microsoft/Cream/tree/main/EfficientViT)  \n    paper: [ÈìæÊé•](https://arxiv.org/pdf/2305.07027.pdf)  \n    weight: [githubÈìæÊé•](https://github.com/xinyuliu-jeffrey/EfficientViT_Model_Zoo/releases/tag/v1.0)\n- **yolov5-backbone/LSKNet**  \n    Ê∑ªÂä†(2023ÊóãËΩ¨ÁõÆÊ†áÊ£ÄÊµãSOTA)LSKNet‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1xk4y1L7Gu/).   \n    reference: [ÈìæÊé•](https://github.com/zcablii/LSKNet)  \n    paper: [ÈìæÊé•](https://arxiv.org/pdf/2303.09030.pdf)  \n- **yolov5-MPDiou**  \n    Ê∑ªÂä†(2023ÊúÄÊñ∞IoUÂ∫¶ÈáèÁÆóÊ≥ï)MPDiouÂà∞yolov5‰∏≠.(ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄ‰∏≠‰∏∫ËØ¶ÁªÜ‰ªéÂ§¥ÊâãÊääÊâãÊïôÂ≠¶,Âõ†Ê≠§Êú¨È°πÊ≤°ÊúâÊèê‰æõ‰ª£Á†Å)  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV19P41147gJ/).   \n    paper: [ÈìæÊé•](https://arxiv.org/pdf/2307.07662v1.pdf)  \n- **yolov7-MPDiou**  \n    Ê∑ªÂä†(2023ÊúÄÊñ∞IoUÂ∫¶ÈáèÁÆóÊ≥ï)MPDiouÂà∞yolov7‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Qh4y1r7D3/).   \n    paper: [ÈìæÊé•](https://arxiv.org/pdf/2307.07662v1.pdf)  \n- **yolov5-SlideLoss**  \n    Ê∑ªÂä†Yolo-Face-V2‰∏≠SlideLossÁöÑÂà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1W14y1i79U/).    \n    reference: [ÈìæÊé•](https://github.com/Krasjet-Yu/YOLO-FaceV2/blob/master/utils/loss.py)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2208.02019)  \n- **yolov5-backbone/CVPR2023-RepViT**  \n    Ê∑ªÂä†RepViT(transformer)‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1PH4y1S7mf/).   \n    reference: [ÈìæÊé•](https://github.com/THU-MIG/RepViT)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2307.09283)  \n- **yolov5-GOLDYOLO**  \n    Âà©Áî®Âçé‰∏∫2023ÊúÄÊñ∞GOLD-YOLO‰∏≠ÁöÑGatherand-DistributeËøõË°åÊîπËøõYOLOV5‰∏≠ÁöÑÁâπÂæÅËûçÂêàÊ®°Âùó.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1PH4y1S7mf/).   \n    reference: [ÈìæÊé•](https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2309.11331)  \n- **yolov7-GOLDYOLO(Êñá‰ª∂Âú®yolov5-GOLDYOLOÁöÑÊñá‰ª∂Â§π‰∏≠)**  \n    Âà©Áî®Âçé‰∏∫2023ÊúÄÊñ∞GOLD-YOLO‰∏≠ÁöÑGatherand-DistributeËøõË°åÊîπËøõYOLOV7‰∏≠ÁöÑÁâπÂæÅËûçÂêàÊ®°Âùó.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV14V411c7H1/).   \n    reference: [ÈìæÊé•](https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2309.11331)  \n- **yolov5-DySnakeConv**  \n    Âà©Áî®Âä®ÊÄÅËõáÂΩ¢Âç∑ÁßØÊîπËøõYOLOV5.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Qu411K7Hw/).   \n    reference: [ÈìæÊé•](https://github.com/YaoleiQi/DSCNet)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2307.08388)  \n- **yolov7-DySnakeConv**  \n    Âà©Áî®Âä®ÊÄÅËõáÂΩ¢Âç∑ÁßØÊîπËøõYOLOV7.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Wj411x7fq/).   \n    reference: [ÈìæÊé•](https://github.com/YaoleiQi/DSCNet)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2307.08388)  \n- **yolov5-AIFI**  \n    Âà©Áî®Â∏¶Êúâ‰ΩçÁΩÆ‰ø°ÊÅØÁºñÁ†ÅÁöÑAIFIËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÊîπËøõYOLOV5.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1nu4y1h7eS/).   \n    reference: [ÈìæÊé•](https://github.com/lyuwenyu/RT-DETR)  \n    paper: [ÈìæÊé•](https://arxiv.org/pdf/2304.08069.pdf)  \n- **yolov7-AIFI**  \n    Âà©Áî®Â∏¶Êúâ‰ΩçÁΩÆ‰ø°ÊÅØÁºñÁ†ÅÁöÑAIFIËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÊîπËøõYOLOV7.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1rj411a7s4/).   \n    reference: [ÈìæÊé•](https://github.com/lyuwenyu/RT-DETR)  \n    paper: [ÈìæÊé•](https://arxiv.org/pdf/2304.08069.pdf)  \n- **yolov5-backbone/UniRepLKNet**  \n    Ê∑ªÂä†UniRepLKNet‰∏ªÂπ≤Âà∞yolov5‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1PH4y1S7mf/).   \n    reference: [ÈìæÊé•](https://github.com/AILab-CVC/UniRepLKNet)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2311.15599)  \n    weights-download: [ÁôæÂ∫¶‰∫ëÈìæÊé•](https://pan.baidu.com/s/1Gk48Xa6cWKAVJgsF5cqk1g?pwd=b55v)\n- **yolov5-asf** \n    Ê∑ªÂä†Attentional Scale Sequence FusionÂà∞yolov5‰∏≠.\n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1kN411V7VZ/).   \n    reference: [ÈìæÊé•](https://github.com/mkang315/ASF-YOLO)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2312.06458)  \n- **yolov5-ccfm**\n    Ê∑ªÂä†cross-scale feature-fusionÂà∞yolov5‰∏≠.\n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Tb4y1P7yd/).   \n    reference: [ÈìæÊé•](https://github.com/ultralytics/ultralytics)  \n    paper: [ÈìæÊé•](https://arxiv.org/pdf/2304.08069.pdf)  \n- **yolov7-asf** \n    Ê∑ªÂä†Attentional Scale Sequence FusionÂà∞yolov7‰∏≠.\n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1PH4y1S7mf/).   \n    reference: [ÈìæÊé•](https://github.com/mkang315/ASF-YOLO)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2312.06458)  \n- **yolov5-RepNCSPELAN**\n    Ê∑ªÂä†yolov9‰∏≠ÁöÑRepNCSPELANÂà∞yolov5‰∏≠.\n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV17y421z73k/).   \n    reference: [ÈìæÊé•](https://github.com/WongKinYiu/yolov9)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2402.13616)\n- **yolov7-RepNCSPELAN**\n    Ê∑ªÂä†yolov9‰∏≠ÁöÑRepNCSPELANÂà∞yolov7‰∏≠.\n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1UA4m137hz/).   \n    reference: [ÈìæÊé•](https://github.com/WongKinYiu/yolov9)  \n    paper: [ÈìæÊé•](https://arxiv.org/abs/2402.13616)\n- **yolov9-backbone**  \n    Ê∑ªÂä†ÂêÑÁßçbackboneÂà∞yolov9‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Ax4y1B7Ln/).   \n- **yolov5-backbone/CVPR2024-StarNet**  \n    Ê∑ªÂä†CVPR2024-StarNetÂà∞yolov5„ÄÅyolov7„ÄÅyolov9‰∏≠.  \n    ËßÜÈ¢ëÊïôÂ≠¶Âú∞ÂùÄÔºö[ÂìîÂì©ÂìîÂì©](https://www.bilibili.com/video/BV1Ax4y1B7Ln/).   ", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "yolo-improve/readme.md", "language": "markdown", "complexity": "complex", "star_count": 6481, "quality_score": 0.73, "features": {"has_code_examples": false, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 362, "word_count": 617}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create project README with installation, usage, and examples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n\"\"\"\nRun YOLOv5 benchmarks on all supported export formats\n\nFormat                      | `export.py --include`         | Model\n---                         | ---                           | ---\nPyTorch                     | -                             | yolov5s.pt\nTorchScript                 | `torchscript`                 | yolov5s.torchscript\nONNX                        | `onnx`                        | yolov5s.onnx\nOpenVINO                    | `openvino`                    | yolov5s_openvino_model/\nTensorRT                    | `engine`                      | yolov5s.engine\nCoreML                      | `coreml`                      | yolov5s.mlmodel\nTensorFlow SavedModel       | `saved_model`                 | yolov5s_saved_model/\nTensorFlow GraphDef         | `pb`                          | yolov5s.pb\nTensorFlow Lite             | `tflite`                      | yolov5s.tflite\nTensorFlow Edge TPU         | `edgetpu`                     | yolov5s_edgetpu.tflite\nTensorFlow.js               | `tfjs`                        | yolov5s_web_model/\n\nRequirements:\n    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime openvino-dev tensorflow-cpu  # CPU\n    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime-gpu openvino-dev tensorflow  # GPU\n    $ pip install -U nvidia-tensorrt --index-url https://pypi.ngc.nvidia.com  # TensorRT\n\nUsage:\n    $ python benchmarks.py --weights yolov5s.pt --img 640\n\"\"\"\n\nimport argparse\nimport platform\nimport sys\nimport time\nfrom pathlib import Path\n\nimport pandas as pd\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\n# ROOT = ROOT.relative_to(Path.cwd())  # relative\n\nimport export\nfrom models.experimental import attempt_load\nfrom models.yolo import SegmentationModel\nfrom segment.val import run as val_seg\nfrom utils import notebook_init\nfrom utils.general import LOGGER, check_yaml, file_size, print_args\nfrom utils.torch_utils import select_device\nfrom val import run as val_det\n\n\ndef run(\n        weights=ROOT / 'yolov5s.pt',  # weights path\n        imgsz=640,  # inference size (pixels)\n        batch_size=1,  # batch size\n        data=ROOT / 'data/coco128.yaml',  # dataset.yaml path\n        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        half=False,  # use FP16 half-precision inference\n        test=False,  # test exports only\n        pt_only=False,  # test PyTorch only\n        hard_fail=False,  # throw error on benchmark failure\n):\n    y, t = [], time.time()\n    device = select_device(device)\n    model_type = type(attempt_load(weights, fuse=False))  # DetectionModel, SegmentationModel, etc.\n    for i, (name, f, suffix, cpu, gpu) in export.export_formats().iterrows():  # index, (name, file, suffix, CPU, GPU)\n        try:\n            assert i not in (9, 10), 'inference not supported'  # Edge TPU and TF.js are unsupported\n            assert i != 5 or platform.system() == 'Darwin', 'inference only supported on macOS>=10.13'  # CoreML\n            if 'cpu' in device.type:\n                assert cpu, 'inference not supported on CPU'\n            if 'cuda' in device.type:\n                assert gpu, 'inference not supported on GPU'\n\n            # Export\n            if f == '-':\n                w = weights  # PyTorch format\n            else:\n                w = export.run(weights=weights, imgsz=[imgsz], include=[f], device=device, half=half)[-1]  # all others\n            assert suffix in str(w), 'export failed'\n\n            # Validate\n            if model_type == SegmentationModel:\n                result = val_seg(data, w, batch_size, imgsz, plots=False, device=device, task='speed', half=half)\n                metric = result[0][7]  # (box(p, r, map50, map), mask(p, r, map50, map), *loss(box, obj, cls))\n            else:  # DetectionModel:\n                result = val_det(data, w, batch_size, imgsz, plots=False, device=device, task='speed', half=half)\n                metric = result[0][3]  # (p, r, map50, map, *loss(box, obj, cls))\n            speed = result[2][1]  # times (preprocess, inference, postprocess)\n            y.append([name, round(file_size(w), 1), round(metric, 4), round(speed, 2)])  # MB, mAP, t_inference\n        except Exception as e:\n            if hard_fail:\n                assert type(e) is AssertionError, f'Benchmark --hard-fail for {name}: {e}'\n            LOGGER.warning(f'WARNING ‚ö†Ô∏è Benchmark failure for {name}: {e}')\n            y.append([name, None, None, None])  # mAP, t_inference\n        if pt_only and i == 0:\n            break  # break after PyTorch\n\n    # Print results\n    LOGGER.info('\\n')\n    parse_opt()\n    notebook_init()  # print system info\n    c = ['Format', 'Size (MB)', 'mAP50-95', 'Inference time (ms)'] if map else ['Format', 'Export', '', '']\n    py = pd.DataFrame(y, columns=c)\n    LOGGER.info(f'\\nBenchmarks complete ({time.time() - t:.2f}s)')\n    LOGGER.info(str(py if map else py.iloc[:, :2]))\n    if hard_fail and isinstance(hard_fail, str):\n        metrics = py['mAP50-95'].array  # values to compare to floor\n        floor = eval(hard_fail)  # minimum metric floor to pass, i.e. = 0.29 mAP for YOLOv5n\n        assert all(x > floor for x in metrics if pd.notna(x)), f'HARD FAIL: mAP50-95 < floor {floor}'\n    return py\n\n\ndef test(\n        weights=ROOT / 'yolov5s.pt',  # weights path\n        imgsz=640,  # inference size (pixels)\n        batch_size=1,  # batch size\n        data=ROOT / 'data/coco128.yaml',  # dataset.yaml path\n        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        half=False,  # use FP16 half-precision inference\n        test=False,  # test exports only\n        pt_only=False,  # test PyTorch only\n        hard_fail=False,  # throw error on benchmark failure\n):\n    y, t = [], time.time()\n    device = select_device(device)\n    for i, (name, f, suffix, gpu) in export.export_formats().iterrows():  # index, (name, file, suffix, gpu-capable)\n        try:\n            w = weights if f == '-' else \\\n                export.run(weights=weights, imgsz=[imgsz], include=[f], device=device, half=half)[-1]  # weights\n            assert suffix in str(w), 'export failed'\n            y.append([name, True])\n        except Exception:\n            y.append([name, False])  # mAP, t_inference\n\n    # Print results\n    LOGGER.info('\\n')\n    parse_opt()\n    notebook_init()  # print system info\n    py = pd.DataFrame(y, columns=['Format', 'Export'])\n    LOGGER.info(f'\\nExports complete ({time.time() - t:.2f}s)')\n    LOGGER.info(str(py))\n    return py\n\n\ndef parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='weights path')\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='inference size (pixels)')\n    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n    parser.add_argument('--test', action='store_true', help='test exports only')\n    parser.add_argument('--pt-only', action='store_true', help='test PyTorch only')\n    parser.add_argument('--hard-fail', nargs='?', const=True, default=False, help='Exception on error or < min metric')\n    opt = parser.parse_args()\n    opt.data = check_yaml(opt.data)  # check YAML\n    print_args(vars(opt))\n    return opt\n\n\ndef main(opt):\n    test(**vars(opt)) if opt.test else run(**vars(opt))\n\n\nif __name__ == '__main__':\n    opt = parse_opt()\n    main(opt)\n", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "yolo-improve/yolov5-AUX/benchmarks.py", "language": "python", "complexity": "medium", "star_count": 6481, "quality_score": 0.8799999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 170, "word_count": 816}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "readme_documentation", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create project README with installation, usage, and examples with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n# Argoverse-HD dataset (ring-front-center camera) http://www.cs.cmu.edu/~mengtial/proj/streaming/ by Argo AI\n# Example usage: python train.py --data Argoverse.yaml\n# parent\n# ‚îú‚îÄ‚îÄ yolov5\n# ‚îî‚îÄ‚îÄ datasets\n#     ‚îî‚îÄ‚îÄ Argoverse  ‚Üê downloads here (31.3 GB)\n\n\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/Argoverse  # dataset root dir\ntrain: Argoverse-1.1/images/train/  # train images (relative to 'path') 39384 images\nval: Argoverse-1.1/images/val/  # val images (relative to 'path') 15062 images\ntest: Argoverse-1.1/images/test/  # test images (optional) https://eval.ai/web/challenges/challenge-page/800/overview\n\n# Classes\nnames:\n  0: person\n  1: bicycle\n  2: car\n  3: motorcycle\n  4: bus\n  5: truck\n  6: traffic_light\n  7: stop_sign\n\n\n# Download script/URL (optional) ---------------------------------------------------------------------------------------\ndownload: |\n  import json\n\n  from tqdm import tqdm\n  from utils.general import download, Path\n\n\n  def argoverse2yolo(set):\n      labels = {}\n      a = json.load(open(set, \"rb\"))\n      for annot in tqdm(a['annotations'], desc=f\"Converting {set} to YOLOv5 format...\"):\n          img_id = annot['image_id']\n          img_name = a['images'][img_id]['name']\n          img_label_name = f'{img_name[:-3]}txt'\n\n          cls = annot['category_id']  # instance class id\n          x_center, y_center, width, height = annot['bbox']\n          x_center = (x_center + width / 2) / 1920.0  # offset and scale\n          y_center = (y_center + height / 2) / 1200.0  # offset and scale\n          width /= 1920.0  # scale\n          height /= 1200.0  # scale\n\n          img_dir = set.parents[2] / 'Argoverse-1.1' / 'labels' / a['seq_dirs'][a['images'][annot['image_id']]['sid']]\n          if not img_dir.exists():\n              img_dir.mkdir(parents=True, exist_ok=True)\n\n          k = str(img_dir / img_label_name)\n          if k not in labels:\n              labels[k] = []\n          labels[k].append(f\"{cls} {x_center} {y_center} {width} {height}\\n\")\n\n      for k in labels:\n          with open(k, \"w\") as f:\n              f.writelines(labels[k])\n\n\n  # Download\n  dir = Path(yaml['path'])  # dataset root dir\n  urls = ['https://argoverse-hd.s3.us-east-2.amazonaws.com/Argoverse-HD-Full.zip']\n  download(urls, dir=dir, delete=False)\n\n  # Convert\n  annotations_dir = 'Argoverse-HD/annotations/'\n  (dir / 'Argoverse-1.1' / 'tracking').rename(dir / 'Argoverse-1.1' / 'images')  # rename 'tracking' to 'images'\n  for d in \"train.json\", \"val.json\":\n      argoverse2yolo(dir / annotations_dir / d)  # convert VisDrone annotations to YOLO labels\n", "metadata": {"source": "github.com/z1069614715/objectdetection_script", "file_path": "yolo-improve/yolov5-AUX/data/Argoverse.yaml", "language": "yaml", "complexity": "medium", "star_count": 6481, "quality_score": 0.82, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 75, "word_count": 294}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Release History\n\n<!-- ‚ú® You do not need to add a pull request reference or an author, this will be added automatically by CI. ‚ú® -->\n\n## Unreleased: pdoc next\n\n\n## 2025-06-04: pdoc 15.0.4\n\n- Include included HTML headers in the ToC by default by enabling markdown2's `mixed=True` option of the `header-ids` extra\n  (#806, @mrossinek)\n- Make `pdoc .` work to document the module in the current directory.\n  ([#813](https://github.com/mitmproxy/pdoc/pull/813), @mhils)\n\n## 2025-04-21: pdoc 15.0.3\n\n- Add missing styles for Github's markdown alerts.\n  ([#796](https://github.com/mitmproxy/pdoc/pull/796), @Steve-Tech)\n\n## 2025-04-17: pdoc 15.0.2\n\n- Fix a bug where type aliases wouldn't be linked.\n  ([#798](https://github.com/mitmproxy/pdoc/pull/798), @mhils)\n- Fix a bug where invalid Numpydoc docstrings would raise an exception.\n  ([#789](https://github.com/mitmproxy/pdoc/pull/789), @tobiscode, @mhils)\n- Improve debuggability of docstring processing\n  ([#784](https://github.com/mitmproxy/pdoc/pull/784), @tobiscode)\n- Fix handling of URL-escaped module names\n  ([#787](https://github.com/mitmproxy/pdoc/pull/787), @iFreilicht)\n- Embed local images referenced in docstrings with an HTML image tag (`<img src=\"./image.png\">`) in addition to Markdown (`![image](./image.png)`) \n  ([#785](https://github.com/mitmproxy/pdoc/pull/785), @earshinov)\n\n## 2024-12-12: pdoc 15.0.1\n\n- Update Mermaid.js version.\n  ([#763](https://github.com/mitmproxy/pdoc/pull/763), @CodeMelted)\n\n## 2024-10-11: pdoc 15.0.0\n\n- Remove support for Python 3.8, which has reached end-of-life on 2024-10-07.\n  ([#747](https://github.com/mitmproxy/pdoc/pull/747), @mhils)\n- Inherited members are now hidden by default if the base class is not part of the documentation.\n  Please provide feedback in the PR if you relied on the old behavior.\n  ([#748](https://github.com/mitmproxy/pdoc/pull/748), @mhils)\n- Python 3.13: `@deprecated` decorators are now rendered with visual emphasis.\n  ([#750](https://github.com/mitmproxy/pdoc/pull/750), @mhils)\n- Improve mocking of `sys.stdin`, `sys.stdout`, and `sys.stderr` to fix runtime errors with some packages.\n  ([#751](https://github.com/mitmproxy/pdoc/pull/751), @mhils)\n\n## 2024-09-11: pdoc 14.7.0\n\n- Do not shorten `current_module.func` to `func` in docstrings when linking.\n  This prevents logical errors in code examples with imports.\n  ([#740](https://github.com/mitmproxy/pdoc/pull/740), @mhils)\n- Add support for Python 3.13.\n  ([#730](https://github.com/mitmproxy/pdoc/pull/730), @mhils)\n- pdoc now strips `collections.abc.` from type annotations to make them more concise.\n  ([#736](https://github.com/mitmproxy/pdoc/pull/736), @mhils)\n\n## 2024-08-27: pdoc 14.6.1\n\n- Fix a bug where entire modules would be excluded by `--no-include-undocumented`.\n  To exclude modules, see https://pdoc.dev/docs/pdoc.html#exclude-submodules-from-being-documented.\n  ([#728](https://github.com/mitmproxy/pdoc/pull/728), @mhils)\n- Fix a bug where pdoc would crash when importing pyi files.\n  ([#732](https://github.com/mitmproxy/pdoc/pull/732), @mhils)\n- Fix a bug where subclasses of TypedDict subclasses would not render correctly.\n  ([#729](https://github.com/mitmproxy/pdoc/pull/729), @mhils)\n\n## 2024-07-24: pdoc 14.6.0\n\n- If `example.data.Data` is also exposed as `example.Data`, pdoc now links to `example.Data` in documentation.\n  ([#670](https://github.com/mitmproxy/pdoc/pull/670), @nathanthorpe, @mhils)\n- Add support for [GitHub Markdown Alerts](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax#alerts).\n  ([#718](https://github.com/mitmproxy/pdoc/pull/718), @mhils)\n- Improve rendering of enums on Python 3.12+.\n  ([#708](https://github.com/mitmproxy/pdoc/pull/708), @mhils)\n- Fix a bug where hyperlinks would get parsed as docstring references.\n  ([#709](https://github.com/mitmproxy/pdoc/pull/709), @mhils)\n- Fix a TypeError when trying to parse modules that implement `__dir__` incorrectly.\n  ([#710](https://github.com/mitmproxy/pdoc/pull/710), @mhils)\n- Fix a bug where a combination of `@dataclass` and `ctypes.Structure` would crash pdoc.\n  ([#711](https://github.com/mitmproxy/pdoc/pull/711), @mhils)\n- Update bundled version of markdown2.\n  ([#717](https://github.com/mitmproxy/pdoc/pull/717), @mhils)\n\n## 2024-06-25: pdoc 14.5.1\n\n- **[CVE-2024-38526](https://github.com/mitmproxy/pdoc/security/advisories/GHSA-5vgj-ggm4-fg62):** Documentation generated with math mode (`pdoc --math`) does not include scripts\n  from polyfill.io anymore. Users who produce documentation with math mode should update immediately. All other users are unaffected.\n  ([#703](https://github.com/mitmproxy/pdoc/pull/703), @adhintz)\n\n## 2024-05-16: pdoc 14.5.0\n\n- The `.. include:` rST directive now supports start-line, end-line, start-after, end-before options.\n  ([#684](https://github.com/mitmproxy/pdoc/pull/684), @frankharkins)\n- Fix image embedding in included rST files.\n  ([#692](https://github.com/mitmproxy/pdoc/pull/692), @meghprkh)\n- Support type-hints from stub-only packages. E.g: `scipy-stubs`\n  ([#671](https://github.com/mitmproxy/pdoc/pull/671), @erikdesmedt)\n- Modify css styles for MathJax to remove unnessesary scroll bars\n  ([#675](https://github.com/mitmproxy/pdoc/pull/675), @thehappycheese)\n\n## 2024-01-18: pdoc 14.4.0\n\n - Private methods can now be included in the documentation by adding `@public` to the docstring.\n   This complements the existing `@private` annotation.\n   ([#655](https://github.com/mitmproxy/pdoc/pull/655), @tmeyier)\n - pdoc now correctly detects the source file for wrapped functions.\n   ([#657](https://github.com/mitmproxy/pdoc/pull/657), @tmeyier)\n - Fixed a bug where memory addresses were not removed from output.\n   ([#663](https://github.com/mitmproxy/pdoc/pull/663), @mhils)\n\n## 2023-12-22: pdoc 14.3.0\n\n - Improve rendering of `.pyi` type stubs containing `@typing.overload`.\n   ([#652](https://github.com/mitmproxy/pdoc/pull/652), @mhils)\n - `@property` and `@cached_property` attributes now have a \"View Source\" button.\n   ([#654](https://github.com/mitmproxy/pdoc/pull/654), @tmeyier)\n\n## 2023-12-13: pdoc 14.2.0\n\n - pdoc now documents PyO3 or pybind11 submodules that are not picked up by Python's builtin pkgutil module.\n   ([#633](https://github.com/mitmproxy/pdoc/issues/633), @mhils)\n - pdoc now supports Python 3.12's `type` statements and has improved `TypeAlias` rendering.\n   ([#651](https://github.com/mitmproxy/pdoc/pull/651), @mhils)\n - Imports in a TYPE_CHECKING section that reference members defined in another module's TYPE_CHECKING section now work\n   correctly.\n   ([#649](https://github.com/mitmproxy/pdoc/pull/649), @mhils)\n - Add support for `code-block` ReST directives\n   ([#624](https://github.com/mitmproxy/pdoc/pull/624), @JCGoran)\n - If a variable's value meets certain entropy criteria and matches an environment variable value,\n   pdoc will now emit a warning and display the variable's name as a placeholder instead.\n   This heuristic is meant to prevent accidental leakage of environment secrets and can be disabled by setting\n   `PDOC_DISPLAY_ENV_VARS=1`.\n   ([#622](https://github.com/mitmproxy/pdoc/pull/622), @mhils)\n\n## 2023-09-10: pdoc 14.1.0\n\n - Add compatibility with Python 3.12\n   ([#620](https://github.com/mitmproxy/pdoc/pull/620), @mhils)\n - Add support for relative links. Instead of explicitly referring to `mypackage.helpers.foo`,\n   one can now also refer to `.helpers.foo` within the `mypackage` module, or `..helpers.foo` in a submodule.\n   ([#544](https://github.com/mitmproxy/pdoc/pull/544), @Crozzers)\n - Function signatures will now display \"Foo\" instead \"demo.Foo\" if the function is in the same module.\n   ([#544](https://github.com/mitmproxy/pdoc/pull/544), @mhils)\n - pdoc now also picks up docstrings from `.pyi` stub files.\n   ([#619](https://github.com/mitmproxy/pdoc/pull/619), @mhils)\n - Fix horizontal scroll navigation z-index issue.\n   ([#616](https://github.com/mitmproxy/pdoc/pull/616), @Domi04151309)\n - Be more strict about parsing URLs in pdoc's web server.\n   ([#617](https://github.com/mitmproxy/pdoc/pull/617), @mhils)\n\n## 2023-06-19: pdoc 14.0.0\n\n - Functions, classes and variables can now be hidden from documentation by adding `@private` to their docstring.\n   ([#578](https://github.com/mitmproxy/pdoc/pull/578), @mhils)\n - pdoc can now be configured to skip classes/functions/variables without docstrings by passing\n   `--no-include-undocumented`.\n   ([#578](https://github.com/mitmproxy/pdoc/pull/578), @mhils)\n - pdoc now documents variables by default, even if they have no docstring or type annotation.\n   Please make yourself heard in [#574](https://github.com/mitmproxy/pdoc/issues/574) if that\n   introduces significant issues for your use case.\n   ([#575](https://github.com/mitmproxy/pdoc/pull/575), @mhils)\n - Add support for Python 3.12.\n   ([#570](https://github.com/mitmproxy/pdoc/pull/570), @mhils)\n - Remove support for Python 3.7, which has reached end-of-life on 2023-06-27.\n   ([#569](https://github.com/mitmproxy/pdoc/pull/569), @mhils)\n\n\n## 2023-04-24: pdoc 13.1.1\n\n - Fix rendering of dynamically modified docstrings.\n   ([#537](https://github.com/mitmproxy/pdoc/pull/537), @mhils)\n - Updated bundled markdown2 version to fix a bug with empty code blocks.\n   ([#537](https://github.com/mitmproxy/pdoc/pull/537), @mhils)\n - `pdoc.doc_ast.AstInfo` now has separate `func_docstrings` and `var_docstrings` attributes \n   instead of one combined one.\n   ([#537](https://github.com/mitmproxy/pdoc/pull/537), @mhils)\n\n## 2023-03-31: pdoc 13.1.0\n\n - Add support for rendering [Mermaid diagrams](https://mermaid.js.org/) by passing `--mermaid`.\n   ([#525](https://github.com/mitmproxy/pdoc/pull/525), @thearchitector, @mhils)\n - Add rudimentary support for `typing_extensions.Literal` on Python 3.7.\n   ([#527](https://github.com/mitmproxy/pdoc/pull/527), @mhils)\n\n## 2023-03-21: pdoc 13.0.1\n\n - Add additional Jinja2 blocks to allow a more fine-grained customization of the menu.\n   ([#521](https://github.com/mitmproxy/pdoc/pull/521), @mikkelakromann)\n - Fix a crash in pdoc 13.0.0 when `__init__.py` is passed as a file to pdoc.\n   ([#522](https://github.com/mitmproxy/pdoc/pull/522), @mhils)\n\n## 2023-02-19: pdoc 13.0.0\n\n - pdoc now skips constructors if they neither have a docstring nor any parameters. This improves display\n   of classes that are not meant to be instantiated manually, for example when using PyO3.\n   ([#510](https://github.com/mitmproxy/pdoc/pull/510), @mhils)\n - Automatically fold a variable's default value if it exceeds 100 characters.\n   Feedback on this cutoff is welcome!\n   ([#511](https://github.com/mitmproxy/pdoc/pull/511), @mhils)\n - Add a workaround to support inherited TypedDicts.\n   ([#504](https://github.com/mitmproxy/pdoc/issues/504), @mhils)\n - `Variable.default_value_str` does not include the ` = ` prefix anymore. It will now emit a warning and return\n   an empty string if `repr(value)` crashes.\n   ([#510](https://github.com/mitmproxy/pdoc/pull/510), @mhils)\n - Fix a CSS issue where the lower half of the navigation toggle would be unresponsive on mobile.\n   ([#510](https://github.com/mitmproxy/pdoc/pull/510), @mhils)\n\n## 2023-01-06: pdoc 12.3.1\n\n - Switch from `setup.py` to `pyproject.toml` for pdoc itself. Please file an issue if that causes any problems. \n   ([#474](https://github.com/mitmproxy/pdoc/issues/474), @mhils)\n - Fix broken links for inherited methods if both parent and subclass have the same name.\n   ([#493](https://github.com/mitmproxy/pdoc/pull/493), @mhils) \n - \"Parameters\", \"Params\" and \"Arguments\" are now also accepted as headings for\n   argument lists in Google-style docstrings.\n   ([#489](https://github.com/mitmproxy/pdoc/pull/489), @ntamas)\n\n## 2022-11-15: pdoc 12.3.0\n\n - Docstrings can now include local images which will be embedded into the page, e.g. `![image](./image.png)`.\n   ([#282](https://github.com/mitmproxy/pdoc/issues/282), @mhils)\n - Fix a bug in parsing Google-style docstrings with extraneous whitespace.\n   ([#459](https://github.com/mitmproxy/pdoc/pull/459), @vsajip, @mhils)\n - `pdoc.doc.Doc.members` now includes variables without type annotation and docstring.\n   They continue to not be documented in the default HTML template.\n   ([#107](https://github.com/mitmproxy/pdoc/issues/107), @mhils)\n - Improve the conversion of reStructuredText to Markdown for function and method references.\n   ([#463](https://github.com/mitmproxy/pdoc/pull/463), @vsajip)\n - Static class attributes that point to a class are now rendered as variables, not as separate classes.\n   ([#465](https://github.com/mitmproxy/pdoc/pull/465), @mhils)\n\n## 2022-11-10: pdoc 12.2.2\n\n - Fix a CSS issue for overflowing math equations.\n   ([#456](https://github.com/mitmproxy/pdoc/pull/456), @mhils)\n - Fix a regression from pdoc 12.2: Enum members are now always documented \n   even if they do not have a docstring.\n   ([#457](https://github.com/mitmproxy/pdoc/pull/457), @mhils)\n\n## 2022-11-05: pdoc 12.2.1\n\n - Fix handling of type annotations in nested classes.\n   ([#440](https://github.com/mitmproxy/pdoc/issues/440), [@mhils](https://github.com/mhils))\n - `Doc.type` is now `Doc.kind` to avoid confusion with `builtins.type`.\n - The new `PDOC_ALLOW_EXEC` environment variable provides an escape hatch for\n   modules that cannot be imported without executing subprocesses.\n   ([#450](https://github.com/mitmproxy/pdoc/issues/450), [@mhils](https://github.com/mhils))\n\n## 2022-09-20: pdoc 12.2.0\n\n - Make documentation of variables more consistent. Variables with a default value\n   and no docstring are now hidden, matching the behavior of variables with a type annotation only.\n   ([#411](https://github.com/mitmproxy/pdoc/issues/411), [@mhils](https://github.com/mhils))\n - Remove `format` argument from `pdoc.pdoc()`. For the forseeable future, pdoc will only support HTML export.\n   ([#308](https://github.com/mitmproxy/pdoc/issues/308), [@mhils](https://github.com/mhils))\n - Update vendored copy of markdown2.\n   ([#429](https://github.com/mitmproxy/pdoc/issues/429), [@mhils](https://github.com/mhils))\n - Fix \"View Source\" button when a function has the same name as the module it is in.\n   ([#431](https://github.com/mitmproxy/pdoc/issues/431), [@mhils](https://github.com/mhils))\n - Improve display of dataclasses.\n   ([#411](https://github.com/mitmproxy/pdoc/issues/411), [@mhils](https://github.com/mhils))\n - Do not execute or document `__main__.py` files. `__main__` submodules can still be documented\n   by explicitly passing them when invoking pdoc.\n   ([#438](https://github.com/mitmproxy/pdoc/issues/438), [@mhils](https://github.com/mhils))\n\n## 2022-06-08: pdoc 12.1.0\n\n - Add compatibility with Python 3.11\n   ([#394](https://github.com/mitmproxy/pdoc/issues/394), [@mhils](https://github.com/mhils))\n - Make sure that docstrings are picked up for functions that have been\n   turned into non-function objects by decorators.\n   ([#416](https://github.com/mitmproxy/pdoc/issues/416), [@jeamland](https://github.com/jeamland))\n - Update vendored copy of markdown2.\n   ([#421](https://github.com/mitmproxy/pdoc/issues/421), [@mhils](https://github.com/mhils))\n - Apply syntax highlighting to search results as well. ([@mhils](https://github.com/mhils))\n - Fix display of `@classmethod @property` instances without docstrings. ([@mhils](https://github.com/mhils))\n - Add support for `@functools.singledispatchmethod`. \n   ([#428](https://github.com/mitmproxy/pdoc/issues/428), [@mhils](https://github.com/mhils))\n - pdoc now terminates if a module cannot be imported instead of raising a warning.\n   You may need to preemptively\n   [exclude submodules](https://pdoc.dev/docs/pdoc.html#exclude-submodules-from-being-documented)\n   that fail to import anyway.\n   ([#407](https://github.com/mitmproxy/pdoc/issues/407), [@mhils](https://github.com/mhils))\n - Fix compatibility with GitPython.\n   ([#430](https://github.com/mitmproxy/pdoc/issues/430), [@mhils](https://github.com/mhils))\n\n## 2022-06-08: pdoc 12.0.2\n\n - Extend auto-linking of URLs in Markdown.\n   ([#401](https://github.com/mitmproxy/pdoc/issues/401), [@mhils](https://github.com/mhils))\n - Mention which implementation of Markdown is supported, with what extras enabled\n   ([#403](https://github.com/mitmproxy/pdoc/issues/403), [@f3ndot](https://github.com/f3ndot))\n - Fix a bug where function signatures had weird line breaks.\n   ([#404](https://github.com/mitmproxy/pdoc/issues/404), [@mhils](https://github.com/mhils))\n - Exclude line numbers from text selection.\n   ([#405](https://github.com/mitmproxy/pdoc/issues/405), [@mhils](https://github.com/mhils))\n\n## 2022-06-03: pdoc 12.0.1\n\n - Fix linking of some function return annotations.\n - Refine rendering of function signatures. Syntax errors are now handled more gracefully.\n - Gracefully handle the case when users specify objects instead of strings in `__all__`.\n\n## 2022-05-15: pdoc 12.0.0\n\n - Improve rendering of function signatures. Annotations are now syntax-highlighted! ‚ú®\n - Change the implementation of *View Source* to not use an HTML `<details>` element. Recent versions\n   of Chrome started to auto-expand source code blocks on search, which made it difficult to search in docstrings.\n - Line numbers now start at 1, not at 0.\n - The aforementioned template improvements may require minor adjustments to custom templates. \n   Users who do not use custom templates are unaffected.\n   - Users who customized the `view_source` macro: \n     This macro has been split into three smaller macros, please check \n     [`module.html.jinja2`](https://github.com/mitmproxy/pdoc/blob/main/pdoc/templates/default/module.html.jinja2).\n     This change was necessary to make sure that the button does not overflow function signatures.\n   - Users who customized the `member`, `class`, `function`, `submodule` or `variable` macros:\n     Common parts have been combined in the `member` macro, please check \n     [`module.html.jinja2`](https://github.com/mitmproxy/pdoc/blob/main/pdoc/templates/default/module.html.jinja2).\n - Fix: Hide the nav menu checkbox in Firefox.  \n\n## 2022-05-04: pdoc 11.2.0\n\n - pdoc now picks up type annotations from `.pyi` stub files (PEP-561).\n   This greatly improves support for native modules where no Python source code is available,\n   for example when using PyO3.\n   ([#390](https://github.com/mitmproxy/pdoc/issues/390), [@mhils](https://github.com/mhils))\n - Template Style Improvements: The size of headings within docstrings has been reduced.\n   Docstrings are now slightly indented on wide screens.\n   ([#383](https://github.com/mitmproxy/pdoc/issues/383), [@jacksund](https://github.com/jacksund) and [@mhils](https://github.com/mhils))\n - Improve rendering of `typing.TypedDict` subclasses.\n   ([#389](https://github.com/mitmproxy/pdoc/issues/389), [@mhils](https://github.com/mhils))\n\n## 2022-04-24: pdoc 11.1.0\n\n - Display line numbers when viewing source code.\n   ([#328](https://github.com/mitmproxy/pdoc/issues/328), [@mhils](https://github.com/mhils))\n - Fix catastrophic backtracking in a markdown2 regex that processes pyshell examples.\n   ([#376](https://github.com/mitmproxy/pdoc/issues/376), [@Andrew-Sheridan](https://github.com/Andrew-Sheridan) and [@mhils](https://github.com/mhils))\n - pdoc now uses the submodule name in the rendered sidebar, rather than the full import path. \n   ([#374](https://github.com/mitmproxy/pdoc/issues/374), [@jacksund](https://github.com/jacksund))\n - Fix a bug where explicit links were rendered incorrectly.\n   ([#382](https://github.com/mitmproxy/pdoc/issues/382), [@mhils](https://github.com/mhils))\n - Fix compatibility with Pygments 2.12.\n   ([#384](https://github.com/mitmproxy/pdoc/issues/384), [@mhils](https://github.com/mhils))\n\n## 2022-04-06: pdoc 11.0.0\n\n - pdoc now picks up reStructuredText syntax in docstrings by default. We still prefer plain Markdown, \n   but this change makes it possible to seamlessly include directives like `.. include:: README.md` or admonitions, \n   which have no Markdown equivalent. reStructuredText processing can be disabled by explicitly setting the docstring \n   format to Markdown.\n   ([#373](https://github.com/mitmproxy/pdoc/issues/373), [@mhils](https://github.com/mhils))\n - pdoc's documentation has been revised, it now also includes [a simple recipe for using pdoc with GitHub Pages\n   ](https://pdoc.dev/docs/pdoc.html#deploying-to-github-pages). \n   ([#373](https://github.com/mitmproxy/pdoc/issues/373), [@mhils](https://github.com/mhils))\n - Improve display of reStructuredText admonitions.\n   ([#372](https://github.com/mitmproxy/pdoc/issues/372), [@mhils](https://github.com/mhils))\n - Add support for reStructuredText field lists: `:param foo: text`.\n   ([#275](https://github.com/mitmproxy/pdoc/issues/275), [@mhils](https://github.com/mhils))\n\n## 2022-03-23: pdoc 10.0.4\n\n - Include `typing.TypeVar` variables in documentation if they have an explicit docstring.\n   ([#361](https://github.com/mitmproxy/pdoc/issues/361), [@ktbarrett](https://github.com/ktbarrett))\n - Make sure that new-style type aliases like `dict[str,str]` are rendered like their old-style \n   `typing.Dict[str,str]` equivalents.\n   ([#363](https://github.com/mitmproxy/pdoc/issues/363), [@hriebl](https://github.com/hriebl))\n - Fix a bug in markdown2 where code snippets interfere with latex expressions\n   ([#340](https://github.com/mitmproxy/pdoc/issues/340), [@Crozzers](https://github.com/Crozzers))\n\n## 2022-03-08: pdoc 10.0.3\n\n - Fix linking of modules.\n   ([#360](https://github.com/mitmproxy/pdoc/issues/360), [@vlad-nn](https://github.com/vlad-nn))\n\n## 2022-03-01: pdoc 10.0.2\n\n - When determining the docstring for a constructor, prefer `Class.__init__.__doc__` over `Metaclass.__call__.__doc__`\n   over `Class.__new__.__doc__`.\n   ([#352](https://github.com/mitmproxy/pdoc/issues/352), [@denised](https://github.com/denised))\n - Improve linking of classes that are re-exported in a common top-level namespace.\n - Make it more clear that Markdown ist the default docformat. ([@Dliwk](https://github.com/Dliwk))\n - Fix compatiblity with code using `ctypes.util.find_library`.\n   ([#358](https://github.com/mitmproxy/pdoc/issues/358), [@bubalis](https://github.com/bubalis))\n\n## 2022-02-14: pdoc 10.0.1\n\n - Fix a bug where pdoc would crash after executing `TYPE_CHECKING` blocks.\n   ([#351](https://github.com/mitmproxy/pdoc/issues/351), [@Dliwk](https://github.com/Dliwk))\n - Add ability to specify custom CSS rules in `custom.css`.\n   The migration instructions in the 10.0.0 changelog entry have been updated accordingly.\n\n## 2022-02-14: pdoc 10.0.0\n\n - Template improvements may require minor adjustments to custom templates. Users who do not use custom templates are\n   unaffected. ([#346](https://github.com/mitmproxy/pdoc/issues/346))\n   - Users who embed pdoc's output into other systems: The main layout (sidebar/content) is now part of\n     [`frame.html.jinja2`](https://github.com/mitmproxy/pdoc/blob/main/pdoc/templates/default/frame.html.jinja2)\n     instead of\n     [`module.html.jinja2`](https://github.com/mitmproxy/pdoc/blob/main/pdoc/templates/default/module.html.jinja2).\n     This allows\n     [`index.html.jinja2`](https://github.com/mitmproxy/pdoc/blob/main/pdoc/templates/default/index.html.jinja2)\n     to cleanly extend `frame.html.jinja2` instead of patching `module.html.jinja2`. See\n     [`examples/mkdocs`](https://github.com/mitmproxy/pdoc/tree/main/examples/mkdocs) for an updated example.\n     If you defined a custom `{% block nav %}` block, you need to remove the outermost `<nav>` element, which is \n     now part of the frame around it.\n   - Users who customized pdoc's CSS: CSS style definitions moved from `module.html.jinja2` into individual CSS files,\n     namely\n     [`theme.css`](https://github.com/mitmproxy/pdoc/blob/main/pdoc/templates/theme.css),\n     [`layout.css`](https://github.com/mitmproxy/pdoc/blob/main/pdoc/templates/layout.css), and\n     [`content.css`](https://github.com/mitmproxy/pdoc/blob/main/pdoc/templates/content.css).\n     You can now either provide replacements for these files, or\n     [specify additional CSS rules in `custom.css`](https://github.com/mitmproxy/pdoc/blob/main/examples/custom-template/).\n     The existing Jinja2 blocks `style_pdoc`, `style_theme`, `style_layout`, `style_content` are being deprecated, see\n     [`frame.html.jinja2`](https://github.com/mitmproxy/pdoc/blob/main/pdoc/templates/default/frame.html.jinja2)\n     for details.\n   - Users who customized `syntax-highlighting.css`: pdoc now consistently uses `.pdoc-code` instead of `.pdoc`\n     or `.codehilite` for syntax highlighting. `.codehilite` is being deprecated but will continue to work, giving\n     custom templates time to migrate.\n - A new `--favicon` option can be used to specify a favicon. The existing embedded default favicon has been removed\n   to reduce page size. ([#345](https://github.com/mitmproxy/pdoc/issues/345))\n - Submodules that are mentioned in `__all__` are not listed as part of the module contents anymore. Instead, they\n   are listed in the navigation. This now matches the behavior as if `__all__` were not specified.\n   If this affects you, please leave feedback in [#341](https://github.com/mitmproxy/pdoc/issues/341).\n   The old behavior can be temporarily restored by setting `PDOC_SUBMODULES=1` as an environment variable while we\n   gather feedback.\n - In line with the above change, [`pdoc.doc.Module.members`](https://pdoc.dev/docs/pdoc/doc.html#Namespace.members)\n   does not contain submodules anymore unless `PDOC_SUBMODULES=1` is set. API users are advised to use\n   [`pdoc.doc.Module.submodules`](https://pdoc.dev/docs/pdoc/doc.html#Module.submodules).\n - Add a better warning message if users use `X | Y`-style type annotations\n   ([PEP 604](https://www.python.org/dev/peps/pep-0604/)) on older Python versions which do not support them.\n - Always defuse insecure `repr()` calls to also cover customized templates.\n - Improve overly greedy linking of identifiers ([#342](https://github.com/mitmproxy/pdoc/issues/342))\n - Include `py.typed` file in wheel distributions.\n\n## 2022-01-26: pdoc 9.0.1\n\n - Emit a deprecation warning if custom templates attempt to include assets that were removed from or moved within pdoc.\n - Improve representation of default values.\n - On mobile devices, scroll the menu into view\n   when the hamburger menu button is clicked.\n - On mobile devices, restrict the width of the search bar\n   to avoid overflowing into the menu button.\n\n## 2022-01-24: pdoc 9.0.0\n\n - **Breaking:** For projects that only document a single module (and its submodules),\n   the module index has been removed. `index.html` now redirects to the top-level module instead.\n   Direct submodules continue to be accessible in the menu.\n   See [#318](https://github.com/mitmproxy/pdoc/issues/318) for details.\n - Moved template assets (SVG, CSS, JS) into a `resources/` subdirectory in the template folder.  \n   Custom templates may need to adjust their paths if they reference these files.\n - pdoc web server now picks a random port if 8080 is unavailable and no explicit port has been passed.\n - Improve search tokenization to better match\n   on function arguments.\n - The \"Edit on GitHub\" button now says \"Edit on GitLab\" if it points to\n   GitLab, or \"Edit Source\" if neither platform is used.\n - The `all_modules` variable now allows templates to access all other module objects.\n - Add `pdoc.doc.Module.from_name` to simplify module creation.\n - Do not linkify identifiers that are already manually linked.\n - Hide modules in the submodule list if the were explicitly excluded from documentation.\n - When importing local file paths, always make sure that the directory is\n   at the front of `sys.path`.\n - Improve evaluation of type annotations.\n\n## 2022-01-14: pdoc 8.3.0\n\n - The search functionality now also covers function parameters,\n   annotated types, default values, and base classes.\n - Work around a Blink renderer bug to make sure that arguments\n   are clickable and the small \"expand\" triangle is displayed\n   next to the *View Source* button.\n - Add negated module specs to exclude specific (sub)modules.\n   For example, `pdoc foo !foo.bar` documents `foo` and all submodules of `foo` except `foo.bar`.\n - Only display headings up to a depth of 2 in the table of contents for module docstrings.\n\n## 2022-01-05: pdoc 8.2.0\n\n - Improve rendering of warnings emitted by pdoc.\n - Improve search quality by disabling the word stemmer.\n - Fix a bug where the search bar on the index page did not work if only a single module was documented.\n - Add a warning when multiple modules with the same name are added from different paths.\n\n## 2021-12-28: pdoc 8.1.0\n\n - Add CSS styling for Markdown tables. (@sitic)\n - Prefer epydoc-style docstrings after variable assignments \n   over the variable's `__doc__`.\n - Improve error message on search index compilation failures.\n\n## 2021-10-29: pdoc 8.0.1\n\n - Fix an edge case where class annotations were not evaluated properly.\n - Improve error messages for invalid type hints.\n - Fix module index when using pdoc's web server.\n\n## 2021-09-19: pdoc 8.0.0\n\n - `search.json` -> `search.js`: Most of pdoc's search-related JavaScript code is now\n   only fetched on demand, which improves page size and performance.\n - pdoc's search now works from `file://` pages.\n - Improve display of (extension module) data descriptors.\n\n## 2021-08-18: pdoc 7.4.0\n\n - Display error webpage for template errors.\n - When processing type hints, detect imports in \n   [`TYPE_CHECKING`](https://docs.python.org/3/library/typing.html#typing.TYPE_CHECKING) \n   blocks.\n - Add `--no-search` to disable search functionality.\n\n## 2021-08-09: pdoc 7.3.1\n\n - Fix a bug where an empty footer was incorrectly emitted by the template.\n\n## 2021-08-09: pdoc 7.3.0\n\n - Full compatibility with Python 3.10.\n - Many template customizations are now directly available as command line switches, for example:\n    - `--math`\n    - `--logo`\n    - `--no-show-source`\n    - `--footer-text`\n\n## 2021-07-28: pdoc 7.2.0\n\n - Don't include variables/attributes that only have a type annotation but no value and no docstring.\n   If one wants to document a variable, a docstring should be added.\n - Templating: `render_docstring` is split into `to_markdown` and `to_html` to increase customizability.\n - Fix hot-reloading of included Markdown files.\n - Allow Google docstring section headers to contain spaces.\n - Fix formatting of Google docstrings that have multiple colons.\n - Fix a crash when importing a module from within its directory.\n\n## 2021-06-11: pdoc 7.1.1\n\n - Do not show constructors for abstract base classes unless they have a custom docstring.\n - Fix math example to render formulae in search results.\n\n## 2021-06-03: pdoc 7.1.0\n\n - Invoking `pdoc` without any arguments now asks the user to specify module name\n   instead of starting pdoc with all available modules. The previous implementation \n   had a poor user experience as building the search index took too long.\n - Improve documentation of `pdoc.extract`. `pdoc.extract.parse_specs` has been renamed to `walk_specs`, \n   the old API now emits a deprecation warning.\n - Add `pdoc.doc.Doc.source_lines` to access where in a file an object is defined.\n - Fix a crash when importing `asyncio` on Windows on Python 3.7. \n\n## 2021-05-30: pdoc 7.0.3\n\n - Do not show a search bar on the module page if only one module is documented.\n   If the entire documentation is contained on a single HTML page, the browser's search functionality is just as good.\n   Users can piggyback on their preexisting knowledge about the search semantics in this case.\n\n## 2021-05-27: pdoc 7.0.2\n\n - Fix section indentation in Google-style docstrings.\n\n## 2021-05-21: pdoc 7.0.1\n\n - Fix compatibility with older Jinja2 versions.\n - Fix basic compatibility with Python 3.10b1.\n\n## 2021-05-12: pdoc 7.0.0\n\n - Add search functionality.\n   pdoc now has a search bar which allows users to quickly\n   find relevant parts in the documentation.\n   See https://pdoc.dev/docs/pdoc/search.html for details.\n - Redesign module list (index.html.jinja2).\n - Update Bootstrap to v5.0.0.\n - Do not fail if `inspect.getdoc()` raises.\n - Fix compatibility with Jinja2 3.0.0.\n\n## 2021-04-30: pdoc 6.6.0\n\n - Jinja2 templates can now access system environment variables,\n   for example to pass version information.\n\n## 2021-04-29: pdoc 6.5.0\n\n - Add support for `.. include::` directives to include external Markdown files.\n - Add word break points for long module and class names. (@jstriebel)\n\n## 2021-04-21: pdoc 6.4.4\n\n - Fix a crash when `inspect.signature` returns incomplete source code.\n - Fix a crash when inspecting unhashable functions.\n\n## 2021-04-11: pdoc 6.4.3\n\n - Fix a bug when dedenting multi-line decorators.\n - Make it easier to change the logo on the module index page.\n\n## 2021-03-28: pdoc 6.4.2\n\n - Minor rendering improvements for enums and typing.NamedTuples.\n - pdoc now emits a warning when directory names conflict with modules\n   already loaded by pdoc.\n - If a class is publicly reimported in the current module, pdoc now links to\n   the reimported instance instead of the source location.\n\n## 2021-03-19: pdoc 6.4.1\n\n - Private function decorators (those starting with \"\\_\")\n   are now hidden by default. (@zmoon)\n - If pdoc is invoked with a name that is both an installed Python module \n   and a local directory, notify the user that the installed module will be documented.\n - `__doc__` is now not rendered as a variable, even if included in `__all__`.\n - Submodules are now internally assigned a qualname, which fixes broken anchor links.\n\n## 2021-03-10: pdoc 6.4.0\n\n - Functions in the current scope can now be referenced without specifying\n   the full qualified name. For example, one can use `bar()` instead of \n   `Foo.bar()` in the docstring of `Foo`.\n - Numpydoc: *See Also* sections are now parsed properly.\n - reStructuredText: Add support for footnotes and fix minor bugs.\n\n## 2021-02-24: pdoc 6.3.2\n\n - Bugfix: Docstrings for data descriptors are now captured properly.\n - Add an example for math formula rendering.\n\n## 2021-02-15: pdoc 6.3.1\n\n - Cosmetic improvements in default value rendering:\n   object and function memory addresses are now stripped.\n - Accessibility Improvements\n\n## 2021-02-14: pdoc 6.3.0\n\n - Respect `__all__` when collecting submodules.\n - Correct wrong links in module index (@fweisser)\n - Emit more detailed error messages on import failure.\n\n## 2021-02-12: pdoc 6.2.0\n\n - Improvement: Add syntax highlighting in \">>>\" code block examples.\n - Bugfix: Module-level comments are not properly live-reloaded.\n\n## 2021-02-12 pdoc 6.1.1\n\n - Bugfix: Don't eat underscores in numpy/Google-style docstrings.\n - Bugfix: Fix rendering of typing.NamedTuple.\n\n## 2021-02-07 pdoc 6.1.0\n\n - Add compatibility for Python 3.7\n\n## 2021-02-07 pdoc 6.0.0\n\n - Add dark mode theme (@Arkelis)\n   pdoc's color scheme can now be customized with CSS variables.\n   This may be a minor breaking change for users who have heavily customized their templates.\n - Docs: Add an example how to integrate pdoc with mkdocs.\n - Bugfix: pdoc now retains custom rendering configuration when it renders itself with live-reload.\n\n## 2021-02-05 pdoc 5.0.0\n\n - Make it easier to embed pdoc into other systems:\n   See <https://pdoc.dev/docs/pdoc.html#integrate-pdoc-into-other-systems> for details.\n   This change may be a minor breaking change for users using custom templates.\n - Generic class bases are now displayed fully. \n   This may be a minor breaking change for users who customized class base output.\n - Add header anchors to documentation items.\n - Define all Jinja2 macros as `{% defaultmacro %}`, which makes them easier to override.\n - Parsing is not more robust if source code is unavailable.\n - Bugfix: Functions decorated with `@classmethod` now also inherit their docstring.\n - Bugfix: The \"View Source\" marker is now properly displayed in Firefox.\n\n## 2021-02-01 pdoc 4.0.0\n\n - Improve how inherited members are detected.\n   `Doc.declared_at` is superseded by `Doc.taken_from`,\n   which is a relatively minor but breaking change in the Python API.\n - Bugfix: Don't link private members in the same module.\n - Improve error message when module live-reload fails.\n - Smaller favicon, improved CSS minification\n - Improve error message if module is not found.\n\n## 2021-01-26 pdoc 3.0.1\n\n - Fix usage of `--docformat`.\n\n## 2021-01-24 pdoc 3.0.0\n\n - Add support for alternative docstring flavors.\n   Flavors can be enabled globally using `--docformat` or on a per-module\n   basis using `__docformat__ = \"...\"`.\n - Add support for Google docstrings.\n - Add basic support for Numpydoc and reStructuredText docstrings.\n   The most common rST elements are supported, but we do not intend\n   to support the full complexity of the spec.\n - Links within the current module now don't require the full qualified path.\n - Live-reloading is now more robust.\n - Improvements to the default theme.\n\n## 2021-01-22: pdoc 2.0.0\n\n - Make it possible to selectively include private or exclude public members in templates.  \n   This comes with a breaking change: `pdoc.doc.Namespace.members` now includes private members.\n - Enhancement: Keep page position when live-reloading.\n - Enhancement: Don't show common server connection errors in the console.\n\n## 2021-01-20: pdoc 1.1.0\n\n - pdoc now respects `__all__` when listing submodules.\n\n## 2021-01-19: pdoc 1.0.1\n\n - Test CI processes by shipping a quick patch release.\n - Bugfix: Don't crash on lambdas as class attributes.\n - Bugfix: Don't crash on comments between decorators.\n - Bugfix: Don't crash pdoc if a user's custom __getattr__ implementation is crashing.\n - Bugfix: use `inspect.unwrap` instead of unwrapping manually.\n\n## 2021-01-19: pdoc 1.0.0\n\nThis release features a major rewrite of pdoc, dropping compatibility\nwith Python 2 and focusing on modern Python 3 only.\n\n - Added: First-class support for type annotations\n - Added: Simpler directory structure\n - Added: New responsive documentation theme\n - Added: New website and documentation\n - Added: 100% test coverage and CI\n - Use Jinja2 instead of mako.\n - Removed: Support for `__pdoc__`, which is rarely required\n   when following modern Python standards. This feature may return\n   depending on user feedback.\n - Removed: Markdown output. The project now focuses on HTML documentation.\n   PRs to re-add markdown support will be gladly accepted.\n\npdoc is now maintained by [@mhils](https://github.com/mhils) and the rest of the mitmproxy team.\n\n\n## pdoc 0.3.2\n\n  - Bugfix release.\n\n\n## pdoc 0.3.1\n\n  - Source code is extracted from __wrapped__ if it exists, and then\n    falls back to inspect.getsourcelines. This reverses the behavior\n    implemented in #6.\n  - Fix Python 2.6 compatibility by requiring Markdown < 2.5 (#19).\n    Markdown 2.5 dropped support for Python 2.6.\n  - Get rid of tabs that sneaked in from #17.\n  - Fix pep8 violations.\n\n## pdoc 0.3.0\n\n  - Major HTML face lift. Kudos to @knadh!\n    (PR: https://github.com/mitmproxy/pdoc/pull/17)\n\n## pdoc 0.2.4\n\n  - Fixed bug in HTTP server that was referencing a non-existent\n    variable.\n\n## pdoc 0.2.3\n\n  - Fixed #10 (the --template-dir flag now works).\n\n## pdoc 0.2.2\n\n  - Fixes #7 by ignoring module loaders that lack a 'path' attribute.\n\n## pdoc 0.2.1\n\n  - Fixes #5 by trying to find source for decorated functions.\n    (@austin1howard)\n\n## pdoc 0.2.0\n\n  - Fix issue #2 by making pdoc a package instead of a module.\n    The templates are now included as package_data, which seems\n    to be more portable (its final location is more predictable).\n\n## pdoc 0.1.8\n\n  - pdoc now interprets `__pdoc__[key] = None` as an explicit way\n    to hide `key` from the public interface of its module.\n\n## pdoc 0.1.7\n\n  - Removed __new__ from the public interface. I think __init__\n    is sufficient.\n\n## pdoc 0.1.6\n\n  - Fixed bug #1.\n\n## pdoc 0.1.5\n\n  - Fixed a bug with an improper use of getattr.\n  - Made pdoc aware of __slots__. (Every identifier in __slots__\n    is automatically interpreted as an instance variable.)\n\n## pdoc 0.1.4\n\n  - Fixed bug where getargspec wasn't being used in Python 2.x.\n\n## pdoc 0.1.3\n\n  - Avoid a FQDN lookup.\n\n## pdoc 0.1.2\n\n  - A few doco touchups.\n  - Fixed a bug in Py3K. Use getfullargspec when available.\n\n## pdoc 0.1.1\n\n  - Documentation touch ups.\n  - Removed unused command line flags.\n\n## pdoc 0.1.0\n\nFirst public release.\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "CHANGELOG.md", "language": "markdown", "complexity": "complex", "star_count": 2166, "quality_score": 0.72, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 827, "word_count": 4733}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Contributing\n\nAs an open source project, pdoc welcomes contributions of all forms. If you would like to bring the project forward,\nplease consider contributing in the following areas:\n\n- **Maintenance:** We are *incredibly* thankful for individuals who are stepping up and helping with maintenance. This\n  includes (but is not limited to) helping out other users, creating minimal examples for existing bug reports, updating\n  documentation, fixing minor bugs that have recently been reported, reviewing pull requests, or picking up stale ones.\n- **Code Contributions:** We actively mark issues that we consider are\n  [good first contributions](https://github.com/mitmproxy/pdoc/labels/help-wanted). If you intend to work on a larger\n  contribution to the project, please come talk to us first. We are very careful not to introduce any feature creep.\n\n## Development Setup\n\nTo get started hacking on pdoc, please install [uv](https://docs.astral.sh/uv/). Then, do the following:\n\n```shell\ngit clone https://github.com/mitmproxy/pdoc.git\ncd pdoc\nuv run pdoc --help\n```\n\n## Testing\n\nIf you've followed the procedure above, you already have all the development requirements installed, and you can run the\nbasic test suite with [tox](https://tox.readthedocs.io/):\n\n```shell\nuv run tox\n```\n\nPlease ensure that all patches are accompanied by matching changes in the test suite. The project strictly maintains\n100% test coverage.\n\n### Fixing Snapshot Tests\n\npdoc makes heavy use of snapshot tests, which compare the rendered output with a stored copy in the [test/testdata](test/testdata) directory.\nThese tests are very useful to catch regressions, but also have a tendency to break quickly. You can overwrite failing snapshots tests by running\n\n```shell\nuv run test/test_snapshot.py\n```\n\nAlternatively, CI will automatically update all snapshots.\n\n## Documentation\n\nPlease check [docs/README.md](./docs/README.md) for instructions.\n\n\n## Release Process\n\nIf you are the current maintainer of pdoc,\nyou can perform the following steps to ship a release:\n\n1. Invoke the release workflow from the GitHub UI: https://github.com/mitmproxy/pdoc/actions/workflows/release.yml\n2. The spawned workflow run will require manual deploy confirmation on GitHub twice: https://github.com/mitmproxy/pdoc/actions\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "medium", "star_count": 2166, "quality_score": 0.6599999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 58, "word_count": 314}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with installation and setup instructions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<p align=\"center\">\n<a href=\"https://pdoc.dev/\"><img alt=\"pdoc\" src=\"https://pdoc.dev/logo.svg\" width=\"200\" height=\"100\" /></a>\n<br><br>\n<a href=\"https://pdoc.dev/docs/pdoc.html\"><img height=\"20\" alt=\"pdoc documentation\" src=\"https://shields.mitmproxy.org/badge/docs-pdoc.dev-brightgreen.svg\"></a>\n<img height=\"20\" alt=\"CI Status\" src=\"https://shields.mitmproxy.org/github/actions/workflow/status/mitmproxy/pdoc/main.yml?label=CI&logo=github\">\n<img height=\"20\" alt=\"Code Coverage\" src=\"https://shields.mitmproxy.org/badge/coverage-100%25-brightgreen\">\n<a href=\"https://autofix.ci\"><img height=\"20\" alt=\"autofix.ci: yes\" src=\"https://shields.mitmproxy.org/badge/autofix.ci-yes-success?logo=data:image/svg+xml;base64,PHN2ZyBmaWxsPSIjZmZmIiB2aWV3Qm94PSIwIDAgMTI4IDEyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCB0cmFuc2Zvcm09InNjYWxlKDAuMDYxLC0wLjA2MSkgdHJhbnNsYXRlKC0yNTAsLTE3NTApIiBkPSJNMTMyNSAtMzQwcS0xMTUgMCAtMTY0LjUgMzIuNXQtNDkuNSAxMTQuNXEwIDMyIDUgNzAuNXQxMC41IDcyLjV0NS41IDU0djIyMHEtMzQgLTkgLTY5LjUgLTE0dC03MS41IC01cS0xMzYgMCAtMjUxLjUgNjJ0LTE5MSAxNjl0LTkyLjUgMjQxcS05MCAxMjAgLTkwIDI2NnEwIDEwOCA0OC41IDIwMC41dDEzMiAxNTUuNXQxODguNSA4MXExNSA5OSAxMDAuNSAxODAuNXQyMTcgMTMwLjV0MjgyLjUgNDlxMTM2IDAgMjU2LjUgLTQ2IHQyMDkgLTEyNy41dDEyOC41IC0xODkuNXExNDkgLTgyIDIyNyAtMjEzLjV0NzggLTI5OS41cTAgLTEzNiAtNTggLTI0NnQtMTY1LjUgLTE4NC41dC0yNTYuNSAtMTAzLjVsLTI0MyAtMzAwdi01MnEwIC0yNyAzLjUgLTU2LjV0Ni41IC01Ny41dDMgLTUycTAgLTg1IC00MS41IC0xMTguNXQtMTU3LjUgLTMzLjV6TTEzMjUgLTI2MHE3NyAwIDk4IDE0LjV0MjEgNTcuNXEwIDI5IC0zIDY4dC02LjUgNzN0LTMuNSA0OHY2NGwyMDcgMjQ5IHEtMzEgMCAtNjAgNS41dC01NCAxMi41bC0xMDQgLTEyM3EtMSAzNCAtMiA2My41dC0xIDU0LjVxMCA2OSA5IDEyM2wzMSAyMDBsLTExNSAtMjhsLTQ2IC0yNzFsLTIwNSAyMjZxLTE5IC0xNSAtNDMgLTI4LjV0LTU1IC0yNi41bDIxOSAtMjQydi0yNzZxMCAtMjAgLTUuNSAtNjB0LTEwLjUgLTc5dC01IC01OHEwIC00MCAzMCAtNTMuNXQxMDQgLTEzLjV6TTEyNjIgNjE2cS0xMTkgMCAtMjI5LjUgMzQuNXQtMTkzLjUgOTYuNWw0OCA2NCBxNzMgLTU1IDE3MC41IC04NXQyMDQuNSAtMzBxMTM3IDAgMjQ5IDQ1LjV0MTc5IDEyMXQ2NyAxNjUuNWg4MHEwIC0xMTQgLTc3LjUgLTIwNy41dC0yMDggLTE0OXQtMjg5LjUgLTU1LjV6TTgwMyA1OTVxODAgMCAxNDkgMjkuNXQxMDggNzIuNWwyMjEgLTY3bDMwOSA4NnE0NyAtMzIgMTA0LjUgLTUwdDExNy41IC0xOHE5MSAwIDE2NSAzOHQxMTguNSAxMDMuNXQ0NC41IDE0Ni41cTAgNzYgLTM0LjUgMTQ5dC05NS41IDEzNHQtMTQzIDk5IHEtMzcgMTA3IC0xMTUuNSAxODMuNXQtMTg2IDExNy41dC0yMzAuNSA0MXEtMTAzIDAgLTE5Ny41IC0yNnQtMTY5IC03Mi41dC0xMTcuNSAtMTA4dC00MyAtMTMxLjVxMCAtMzQgMTQuNSAtNjIuNXQ0MC41IC01MC41bC01NSAtNTlxLTM0IDI5IC01NCA2NS41dC0yNSA4MS41cS04MSAtMTggLTE0NSAtNzB0LTEwMSAtMTI1LjV0LTM3IC0xNTguNXEwIC0xMDIgNDguNSAtMTgwLjV0MTI5LjUgLTEyM3QxNzkgLTQ0LjV6Ii8+PC9zdmc+\"></a>\n<a href=\"https://pypi.python.org/pypi/pdoc\"><img height=\"20\" alt=\"PyPI Version\" src=\"https://shields.mitmproxy.org/pypi/v/pdoc.svg\"></a>\n<img height=\"20\" alt=\"Supported Python Versions\" src=\"https://shields.mitmproxy.org/pypi/pyversions/pdoc.svg\">\n</p>\n\nAPI Documentation for Python Projects.\n\n\n# Example\n\n`pdoc -o ./html pdoc` generates this website: [pdoc.dev/docs](https://pdoc.dev/docs/pdoc.html).\n\n# Installation\n```shell\npip install pdoc\n```\n\npdoc is compatible with Python 3.9 and newer.\n\n\n# Usage\n\n```shell\npdoc your_python_module\n# or\npdoc ./my_project.py\n```\n\nRun `pdoc pdoc` to see pdoc's own documentation, \nrun `pdoc --help` to view the command line flags, \nor check our [hosted copy of the documentation](https://pdoc.dev/docs/pdoc.html).\n\n\n# Features\n\npdoc's main feature is a focus on simplicity: pdoc aims to do one thing and do it well.  \n\n\n* Documentation is plain [Markdown](https://pdoc.dev/docs/pdoc.html#markdown-support).\n* First-class support for type annotations and all other modern Python 3 features.\n* Builtin web server with live reloading.\n* Customizable HTML templates.\n* Understands numpydoc and Google-style docstrings.\n* Standalone HTML output without additional dependencies.\n  \nUnder the hood...\n\n* `pdoc` will automatically link identifiers in your docstrings to their corresponding documentation.\n* `pdoc` respects your `__all__` variable when present.\n* `pdoc` will traverse the abstract syntax tree to extract type annotations and docstrings from constructors as well.\n* `pdoc` will automatically try to resolve type annotation string literals as forward references.\n* `pdoc` will use inheritance to resolve type annotations and docstrings for class members. \n  \nIf you have substantially more complex documentation needs, we recommend using [Sphinx](https://www.sphinx-doc.org/)!\n\n\n## Contributing\n\nAs an open source project, pdoc welcomes contributions of all forms.\n\n[![Dev Guide](https://shields.mitmproxy.org/badge/dev_docs-CONTRIBUTING.md-blue)](https://github.com/mitmproxy/pdoc/blob/main/CONTRIBUTING.md)\n\n\n## pdoc vs. pdoc3\n\nThis project is not associated with \"pdoc3\", which often falsely assumes our name.\nQuoting [@BurntSushi](https://github.com/BurntSushi), the original author of pdoc:\n\n> I'm pretty disgusted that someone has taken a project I built, relicensed it, \n> [attempted to erase its entry on the Python Wiki](https://wiki.python.org/moin/DocumentationTools?action=diff&rev1=36&rev2=37), \n> released it under effectively the same name and, worst of all, associated it with Nazi symbols.\n> \n> *Source: https://github.com/pdoc3/pdoc/issues/64*\n\nIn contrast, the pdoc project strives to uphold a healthy community where everyone is treated with respect.\nEveryone is welcome to contribute as long as they adhere to basic civility. We expressly distance ourselves from the use\nof Nazi symbols and ideology.\n\n----\n\nThe pdoc project was originally created by [Andrew Gallant](https://github.com/BurntSushi) \nand is currently maintained by [Maximilian Hils](https://github.com/mhils).\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 2166, "quality_score": 0.6599999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 89, "word_count": 397}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\nfrom pathlib import Path\nimport shutil\nimport textwrap\n\nfrom jinja2 import Environment\nfrom jinja2 import FileSystemLoader\nfrom markupsafe import Markup\nimport pygments.formatters.html\nimport pygments.lexers.python\n\nimport pdoc.render\n\nhere = Path(__file__).parent\n\nif __name__ == \"__main__\":\n    demo = here / \"..\" / \"test\" / \"testdata\" / \"demo.py\"\n    env = Environment(\n        loader=FileSystemLoader([here, here / \"..\" / \"pdoc\" / \"templates\"]),\n        autoescape=True,\n    )\n\n    lexer = pygments.lexers.python.PythonLexer()\n    formatter = pygments.formatters.html.HtmlFormatter(style=\"friendly\")\n    pygments_css = formatter.get_style_defs()\n    example_html = Markup(pygments.highlight(demo.read_text(\"utf8\"), lexer, formatter))\n\n    (here / \"index.html\").write_bytes(\n        env.get_template(\"index.html.jinja2\")\n        .render(\n            example_html=example_html,\n            pygments_css=pygments_css,\n            __version__=pdoc.__version__,\n        )\n        .encode()\n    )\n\n    if (here / \"docs\").is_dir():\n        shutil.rmtree(here / \"docs\")\n\n    # Render standalone demo (without any configuration)\n    pdoc.pdoc(\n        demo,\n        output_directory=here / \"docs\",\n    )\n    (here / \"docs\" / \"demo.html\").rename(here / \"docs\" / \"demo-standalone.html\")\n\n    # Render main docs\n    pdoc.render.configure(\n        edit_url_map={\n            \"pdoc\": \"https://github.com/mitmproxy/pdoc/blob/main/pdoc/\",\n            \"demo\": \"https://github.com/mitmproxy/pdoc/blob/main/test/testdata/demo\",\n            \"demo_long\": \"https://github.com/mitmproxy/pdoc/blob/main/test/testdata/demo_long\",\n        },\n        favicon=\"/favicon.svg\",\n        logo=\"/logo.svg\",\n        logo_link=\"https://pdoc.dev\",\n        footer_text=f\"pdoc {pdoc.__version__}\",\n    )\n\n    pdoc.pdoc(\n        \"pdoc\",\n        demo,\n        here / \"..\" / \"test\" / \"testdata\" / \"demo_long.py\",\n        output_directory=here / \"docs\",\n    )\n\n    # Render dark mode example\n    pdoc.render.configure(template_directory=here / \"..\" / \"examples\" / \"dark-mode\")\n    pdoc.pdoc(demo, output_directory=here / \"docs\" / \"dark-mode\")\n\n    # Render math example\n    pdoc.render.configure(\n        math=True,\n        logo=\"/logo.svg\",\n        logo_link=\"https://pdoc.dev\",\n        edit_url_map={\n            \"math_demo\": \"https://github.com/mitmproxy/pdoc/blob/main/test/testdata/math_demo\"\n        },\n    )\n    pdoc.pdoc(\n        here / \"..\" / \"test\" / \"testdata\" / \"math_demo.py\",\n        output_directory=here / \"docs\" / \"math\",\n    )\n\n    # Render mermaid example\n    pdoc.render.configure(\n        mermaid=True,\n        logo=\"/logo.svg\",\n        logo_link=\"https://pdoc.dev\",\n        edit_url_map={\n            \"math_demo\": \"https://github.com/mitmproxy/pdoc/blob/main/test/testdata/mermaid_demo\"\n        },\n    )\n    pdoc.pdoc(\n        here / \"..\" / \"test\" / \"testdata\" / \"mermaid_demo.py\",\n        output_directory=here / \"docs\" / \"mermaid\",\n    )\n\n    # Add sitemap.xml\n    with (here / \"sitemap.xml\").open(\"w\", newline=\"\\n\") as f:\n        f.write(\n            textwrap.dedent(\n                \"\"\"\n        <?xml version=\"1.0\" encoding=\"utf-8\"?>\n        <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n           xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n           xsi:schemaLocation=\"http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\">\n        \"\"\"\n            ).strip()\n        )\n        for file in here.glob(\"**/*.html\"):\n            if file.name.startswith(\"_\"):\n                continue\n            filename = str(file.relative_to(here).as_posix()).replace(\"index.html\", \"\")\n            f.write(f\"\"\"\\n<url><loc>https://pdoc.dev/{filename}</loc></url>\"\"\")\n        f.write(\"\"\"\\n</urlset>\"\"\")\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "docs/make.py", "language": "python", "complexity": "medium", "star_count": 2166, "quality_score": 0.72, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 118, "word_count": 261}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Examples\n\nThis directory contains some pdoc template examples:\n\n - `custom-template`: Showcasing various template configuration options.\n - `dark-mode`: Style pdoc with a dark theme.\n - `library-usage`: Using pdoc as a Python library.\n - `mkdocs`: Integrate pdoc with mkdocs.\n\n### Third-Party Examples\n\n - *Hugo Integration:*\n   - Final Result: https://docs.mitmproxy.org/stable/api/events.html\n   - Template: https://github.com/mitmproxy/mitmproxy/tree/main/docs/scripts/pdoc-template\n   - Generation: https://github.com/mitmproxy/mitmproxy/blob/main/docs/scripts/api-render.py\n\nIf you have a good example of embedding pdoc into a particular CMS/static site generator, please add it here!\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "examples/README.md", "language": "markdown", "complexity": "simple", "star_count": 2166, "quality_score": 0.63, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 18, "word_count": 73}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Using a Custom Template\n\nThis is an example showing how one can customize pdoc's default template.  \nThe individual changes are explained in the template files themselves.\n\nTo use a custom template, run pdoc with the `--template-directory` (short: `-t`) command line argument:\n\n```\npdoc -t ./examples/custom-template pdoc\n```\n\n### Additional Resources for Template Development\n\n - [pdoc's Default Templates](https://github.com/mitmproxy/pdoc/tree/main/pdoc/templates)\n - [Jinja2 Template Designer Documentation](https://jinja.palletsprojects.com/en/2.11.x/templates/)\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "examples/custom-template/README.md", "language": "markdown", "complexity": "simple", "star_count": 2166, "quality_score": 0.6, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 16, "word_count": 63}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# üåí pdoc dark mode\n\n[**Demo**](https://pdoc.dev/docs/dark-mode/demo.html)\n\nThis is an example showing how to style pdoc in dark mode using custom stylesheets.\npdoc defines all colors as CSS variables, so we only need to adjust pdoc's color scheme (`theme.css`)\nand pygments' syntax highlighting theme (`syntax-highlighting.css`).\n\nRun `cd examples/dark-mode && pdoc -t . pdoc` to see it in action!\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "examples/dark-mode/README.md", "language": "markdown", "complexity": "simple", "star_count": 2166, "quality_score": 0.6, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 10, "word_count": 57}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Library Usage\n\n\nThis is an example showing how to use pdoc as a library.\nSuch an approach may be useful to patch documentation issues\nthat cannot be fixed in the source code, for example when modules\nare generated with pybind11 or PyO3.\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "examples/library-usage/README.md", "language": "markdown", "complexity": "medium", "star_count": 2166, "quality_score": 0.39999999999999997, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 8, "word_count": 43}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nExample module. We use pure Python here, but this could also be a compiled native module.\n\"\"\"\n\nimport enum\n\n\nclass Foo(enum.Enum):\n    A = 1\n    B = 2\n    C = 3\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "examples/library-usage/foo.py", "language": "python", "complexity": "simple", "star_count": 2166, "quality_score": 0.69, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 12, "word_count": 31}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate module integration for Create detailed code documentation with docstrings and comments\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\nimport foo\n\nimport pdoc\n\nif __name__ == \"__main__\":\n    doc = pdoc.doc.Module(foo)\n\n    # We can override most pdoc doc attributes by just assigning to them.\n    doc.get(\"Foo.A\").docstring = \"I'm a docstring for Foo.A.\"\n\n    out = pdoc.render.html_module(module=doc, all_modules={\"foo\": doc})\n\n    with open(\"foo.html\", \"w\") as f:\n        f.write(out)\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "examples/library-usage/make.py", "language": "python", "complexity": "simple", "star_count": 2166, "quality_score": 0.63, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 16, "word_count": 44}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# mkdocs with pdoc\n\nThis is an example showing how to use pdoc with [mkdocs](https://www.mkdocs.org).\nRun `./make.py` to generate the API documentation and then `mkdocs serve` to view this website!\n\n## Alternatives\n\n[mkdocstrings](https://mkdocstrings.github.io/) is a great alternative to pdoc if you are in the mkdocs ecosystem\nanyways.\n\n## Implementation\n\nThe main trick is that we define a custom `frame.html.jinja2` template to\nremove pdoc's usual HTML code around the main documentation contents. \nWe then invoke pdoc normally and rename the output files to `.md` so that they are picked up by mkdocs.\nmkdocs' Markdown parser accepts the interspersed HTML just fine!\n\n## Limitations\n\n - If you want to link between different pages in your documentation,\n   pdoc requires `use_directory_urls: false` for linking to work.\n - pdoc doesn't populate mkdocs' table of contents as all raw HTML is ignored by mkdocs' Markdown parser.\n   You can hide the table of contents with some custom CSS in `frame.html.jinja2`.\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "examples/mkdocs/README.md", "language": "markdown", "complexity": "medium", "star_count": 2166, "quality_score": 0.6, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 24, "word_count": 153}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "r'''\n# What is pdoc?\n\npdoc auto-generates API documentation that follows your project's Python module hierarchy.\n\npdoc's main feature is a focus on simplicity: pdoc aims to do one thing and do it well.\n\n - Easy setup, no configuration necessary.\n - Documentation is plain [Markdown](#markdown-support).\n - First-class support for type annotations.\n - Builtin web server with live reloading.\n - Customizable HTML templates.\n - Understands numpydoc and Google-style docstrings.\n\n# Quickstart\n\nAs an example, we want to generate API documentation for `demo.py`.\nOur demo module already includes a bunch of docstrings:\n\n```python\n\"\"\"\nA small `pdoc` example.\n\"\"\"\n\nclass Dog:\n    \"\"\"üêï\"\"\"\n    name: str\n    \"\"\"The name of our dog.\"\"\"\n    friends: list[\"Dog\"]\n    \"\"\"The friends of our dog.\"\"\"\n\n    def __init__(self, name: str):\n        \"\"\"Make a Dog without any friends (yet).\"\"\"\n        self.name = name\n        self.friends = []\n\n    def bark(self, loud: bool = True):\n        \"\"\"*woof*\"\"\"\n```\n\nWe can invoke pdoc to take our docstrings and render them into a standalone HTML document:\n\n```shell\npdoc ./demo.py  # or: pdoc my_module_name\n```\n\nThis opens a browser with our module documentation. Here's a copy of what you should see:\n\n<iframe style=\"\n    width: 100%;\n    height: 250px;\n    border: solid gray 1px;\n    display: block;\n    margin: 1rem auto;\n    border-radius: 5px;\"\n    title=\"rendered demo.py documentation\"\n    src=\"https://pdoc.dev/docs/demo-standalone.html\"></iframe>\n\nIf you look closely, you'll notice that docstrings are interpreted as Markdown.\nFor example, \\`pdoc\\` is rendered as `pdocÔªø`. Additionally, identifiers such as the type annotation\nfor `Dog.friends` are automatically linked.\n\nIf we edit `demo.py` now, the page will reload automatically.\nOnce we are happy with everything, we can export the documentation to an HTML file:\n\n```shell\npdoc ./demo.py -o ./docs\n```\n\nThis will create an HTML file at `docs/demo.html` which contains our module documentation. üéâ\n\n## Customizing pdoc\n\nWe can optionally configure pdoc's output via command line flags.\nFor example, we can add a project logo to the documentation:\n\n```shell\npdoc ./demo.py --logo \"https://placedog.net/300?random\"\n```\n\nTo get a list of all available rendering options, run:\n\n```shell\npdoc --help\n```\n\nIf you need more advanced customization options, see [*How can I edit pdoc's HTML template?*](#edit-pdocs-html-template).\n\n\n## Deploying to GitHub Pages\n\n*In this example we'll deploy pdoc's documentation to GitHub Pages. Of course, you can distribute\nthe generated documentation however you want! pdoc's job is to \"just\" produce self-contained HTML files for you.*\n\nA very simple way to host your API documentation is to set up a continuous integration job which\npushes your documentation to GitHub Pages. This keeps your docs updated automatically.\n\n 1. Enable GitHub Actions and GitHub Pages for your project.\n 2. In the GitHub Pages settings, select GitHub Actions as your build and deployment source.\n 3. Copy pdoc's GitHub Actions workflow into your own repository and adjust it to how you build your docs:\n    [`.github/workflows/docs.yml`](https://github.com/mitmproxy/pdoc/blob/main/.github/workflows/docs.yml)\n\nThat's it ‚Äì no need to fiddle with any secrets or set up any `gh-pages` branches. ü•≥\n\n# How can I ... ?\n\n## ...add documentation?\n\nIn Python, objects like modules, functions and classes have\na special attribute named `__doc__` which contains that object's\n*docstring*.  The docstring comes from a special placement of a string\nin your source code.  For example, the following code shows how to\ndefine a function with a docstring and access the contents of that\ndocstring:\n\n```python\n>>> def test():\n...     \"\"\"This is a docstring.\"\"\"\n...     pass\n...\n>>> test.__doc__\n'This is a docstring.'\n```\n\nSomething similar can be done for classes and modules too. For classes,\nthe docstring should come on the line immediately following `class\n...`. For modules, the docstring should start on the first line of\nthe file. These docstrings are what you see for each module, class,\nfunction and method listed in the documentation produced by pdoc.\n\n\n## ...document variables?\n\nPython itself [does not attach docstrings to\nvariables](https://www.python.org/dev/peps/pep-0224/). For example:\n\n```python\nvariable = \"SomeValue\"\n\"\"\"Docstring for variable.\"\"\"\n```\n\nThe resulting `variable` will have no `__doc__` attribute.\nTo compensate, pdoc will read the abstract syntax tree (an abstract representation of the source code)\nand include all assignment statements immediately followed by a docstring. This approach is not formally standardized,\nbut followed by many tools, including Sphinx's autodoc extension in case you ever decide to migrate off pdoc.\nDocstring detection is limited to the current module, docstrings for variables imported from other modules are not\npicked up.\n\nSomething similar is done for instance variables, which are either type-annotated in the class\nor defined in a class's `__init__`. Here is an example showing both conventions detected by pdoc:\n\n```python\nclass GoldenRetriever(Dog):\n    name: str\n    \"\"\"Full Name\"\"\"\n\n    def __init__(self):\n        self.weight: int = 10\n        \"\"\"Weight in kilograms\"\"\"\n```\n\n\nIf you would like to distinguish an instance variable from a class variable,\nyou can use [`typing.ClassVar`](https://docs.python.org/3/library/typing.html#typing.ClassVar):\n\n```python\nclass GoldenRetriever(Dog):\n    breed_code: ClassVar[str] = \"GOLD\"\n    \"\"\"International breed code (same for all instances)\"\"\"\n    name: str\n    \"\"\"Full Name (different for each instance)\"\"\"\n```\n\n\n## ...control what is documented?\n\nThe public interface of a module is determined through one of two\nways.\n- If `__all__` is defined in the module, then all identifiers in that list will be considered public.\n   No other identifiers will be considered public.\n- If `__all__` is not defined, then pdoc will consider all items public that do not start with an\n  underscore and that are defined in the current module (i.e. they are not imported).\n\nIf you want to override the default behavior for a particular item,\nyou can do so by including an annotation in its docstring:\n\n- `@private` hides an item unconditionally.\n- <code>&#64;public</code> shows an item unconditionally.\n\nIn general, we recommend keeping the following conventions:\n\n- If you want to document a private member, consider making it public.\n- If you want to hide a public member, consider making it private.\n- If you want to document a special `__dunder__` method, the recommended way to do so is\n  to not document the dunder method specifically, but to add some usage examples in the class documentation.\n\n> [!NOTE]\n> Hiding an item only removes it from documentation.\n> It is still displayed in the source code when clicking the \"View Source\" button.\n\nAs a last resort, you can override pdoc's behavior with a custom module template (see\n[*How can I edit pdoc's HTML template?*](#edit-pdocs-html-template)).\nYou can find an example at\n[`examples/custom-template/module.html.jinja2`](https://github.com/mitmproxy/pdoc/blob/main/examples/custom-template/module.html.jinja2).\n\nHiding an item only removes it from documentation. It is still displayed in the source code when clicking the \"View Source\" button.\n\n## ...exclude submodules from being documented?\n\nIf you would like to exclude specific submodules from the documentation, the recommended way is to specify `__all__` as\nshown in the previous section. Alternatively, you can pass negative regular expression `!patterns` as part of the\nmodule specification. Each pattern removes all previously specified (sub)module names that match. For example, the following\ninvocation documents `foo` and all submodules of `foo`, but not `foo.bar`:\n\n```\npdoc foo !foo.bar\n```\n\nLikewise, `pdoc pdoc !pdoc.` would document the pdoc module itself, but none of its submodules. Patterns are always\nmatched on the final module name, even if modules are passed as file paths.\n\n\n## ...link to other identifiers?\n\nIn your documentation, you can link to other identifiers by enclosing them in backticks:\n<code>\\`pdoc\\`</code> will link to `pdoc`.\nWhen linking to identifiers in other modules, the identifier name must be fully qualified.\nFor example, <code>\\`pdoc.doc.Doc\\`</code> will be automatically linked to `pdoc.doc.Doc`,\nwhile <code>\\`Doc\\`</code> only works within the `pdoc.doc` module.\n\npdoc will link all identifiers that are rendered in the current run.\nThis means that you need to run `pdoc module_a module_b` to have interlinking between module_a and module_b.\nIf you run `pdoc module_a` followed by `pdoc module_b`, there will be no cross-linking between the two modules.\n\n\n## ...change the item order?\n\nBy default, documentation items are sorted in order of (first) appearance in the source code.\nThis means that if you want to move a particular function to the beginning of your documentation,\nyou need to move it there in your source code. This is not only useful to the readers of your documentation,\nbut also useful to the consumers of your source code.\n\n\n## ...use numpydoc or Google docstrings?\n\nWhile pdoc prefers docstrings that are plain Markdown, it also understands numpydoc and Google-style docstrings.\nIf your documentation follows one of these styles, you can:\n\n1. Run `pdoc --docformat ...` to enable a particular docstring flavor globally, or\n2. Add `__docformat__ = \"...\"` at the top-level of the module you are documenting.\n\nThe following values are supported:\n\n- `markdown`: Process Markdown syntax only.\n- `restructuredtext`: Process reStructuredText elements, then Markdown (default setting).\n- `google`: Process reStructuredText elements, then Google-style syntax, then Markdown.\n- `numpy`: Process reStructuredText elements, then Numpydoc syntax, then Markdown.\n\npdoc only interprets a subset of the reStructuredText specification.\nAdding additional syntax elements is usually easy. If you feel that pdoc doesn't parse a docstring element properly,\nplease amend `pdoc.docstrings` and send us a pull request!\n\n\n## ...render math formulas?\n\nRun `pdoc --math`, and pdoc will render formulas in your docstrings. See\n[`math_demo`](https://pdoc.dev/docs/math/math_demo.html) for details.\n\n\n## ...render Mermaid diagrams?\n\nRun `pdoc --mermaid`, and pdoc will render mermaid diagrams in your docstrings. See\n[`mermaid_demo`](https://pdoc.dev/docs/mermaid/mermaid_demo.html) for details.\n\n\n## ...add my project's logo?\n\nSee [*Customizing pdoc*](#customizing-pdoc).\n\n\n## ...include Markdown files?\n\nYou can include external Markdown files in your documentation by using reStructuredText's\n`.. include::` directive. For example, a common pattern is to include your project's README in your top-level `__init__.py` like this:\n\n```python\n\"\"\"\n.. include:: ../README.md\n\"\"\"\n```\n\nYou can also include only parts of a file with the\n[`start-line`, `end-line`, `start-after`, and `end-after` options](https://docutils.sourceforge.io/docs/ref/rst/directives.html#including-an-external-document-fragment):\n\n```python\n\"\"\"\n.. include:: ../README.md\n   :start-line: 1\n   :end-before: Changelog\n\"\"\"\n```\n\n\n## ...add a title page?\n\nThe landing page for your documentation is your project's top-level `<modulename>/__init__.py` file.\nAdding a module-level docstring here is a great way to introduce users to your project.\nFor example, the documentation you are reading right now is sourced from\n[`pdoc/__init__.py`](https://github.com/mitmproxy/pdoc/blob/main/pdoc/__init__.py).\nYou can also include your title page from a [Markdown file](#include-markdown-files).\n\nIf you have multiple top-level modules, a custom title page requires modifying the `index.html.jinja2` template.\nYou can find an example in [#410](https://github.com/mitmproxy/pdoc/issues/410).\n\n## ...edit pdoc's HTML template?\n\nFor more advanced customization, we can edit pdoc's\n[default HTML template](https://github.com/mitmproxy/pdoc/blob/main/pdoc/templates/default/module.html.jinja2),\nwhich uses the\n[Jinja2](https://jinja.palletsprojects.com/) templating language.\n\nLet's assume you want to replace the logo with a custom button. We first find the right location in the template by searching\nfor \"logo\", which shows us that the logo is defined in a Jinja2 block named `nav_title`.\nWe now extend the default template by creating a file titled `module.html.jinja2` in the current directory\n with the following contents:\n\n```html+jinja\n{% extends \"default/module.html.jinja2\" %}\n{% block nav_title %}\n<button>Donate dog food</button>\n{% endblock %}\n```\n\nWe then specify our custom template directory when invoking pdoc...\n\n```shell\npdoc -t . ./demo.py\n```\n\n...and the updated documentation ‚Äì with button ‚Äì renders! üéâ\n\nSee [`examples/`](https://github.com/mitmproxy/pdoc/tree/main/examples/)\nfor more examples.\n\n\n## ...pass arguments to the Jinja2 template?\n\nIf you need to pass additional data to pdoc's Jinja2 templates,\nyou can use system environment variables.\nFor example,\n[`examples/custom-template/module.html.jinja2`](https://github.com/mitmproxy/pdoc/blob/main/examples/custom-template/module.html.jinja2)\nshows how to include a version number in the rendered HTML.\n\n\n## ...integrate pdoc into other systems?\n\npdoc's HTML and CSS are written in a way that the default template can be easily adjusted\nto produce standalone HTML fragments that can be embedded in other systems.\nThis makes it possible to integrate pdoc with almost every CMS or static site generator.\nThe only limitation is that you need to retain pdoc's directory structure\nif you would like to link between modules.\n\nTo do so, [create a custom `frame.html.jinja2` template](#edit-pdocs-html-template) which only emits CSS and the main\npage contents instead of a full standalone HTML document:\n```html+jinja\n{% block content %}{% endblock %}\n\n{% filter minify_css %}\n    {% block style %}\n        {# The same CSS files as in pdoc's default template, except for layout.css.\n        You may leave out Bootstrap Reboot, which corrects inconsistences across browsers\n        but may conflict with you website's stylesheet. #}\n        <style>{% include \"resources/bootstrap-reboot.min.css\" %}</style>\n        <style>{% include \"syntax-highlighting.css\" %}</style>\n        <style>{% include \"theme.css\" %}</style>\n        <style>{% include \"content.css\" %}</style>\n    {% endblock %}\n{% endfilter %}\n```\n\nThis should be enough to produce HTML files that can be embedded into other pages.\nAll CSS selectors are prefixed with `.pdoc` so that pdoc's page style does not interfere with the rest of your website.\n\nYou can find a full example for mkdocs in [`examples/mkdocs`](https://github.com/mitmproxy/pdoc/tree/main/examples/mkdocs/).\n\n\n# Docstring Inheritance\n\npdoc extends the standard use of docstrings in two important ways:\nby introducing variable docstrings (see [*How can I document variables?*](#document-variables)),\nand by allowing functions and classes to inherit docstrings and type annotations.\n\nThis is useful to not unnecessarily repeat information. Consider this example:\n\n```python\nclass Dog:\n    def bark(self, loud: bool) -> None:\n        \"\"\"\n        Make the dog bark. If `loud` is True,\n        use full volume. Not supported by all breeds.\n        \"\"\"\n\nclass GoldenRetriever(Dog):\n    def bark(self, loud: bool) -> None:\n        print(\"Woof Woof\")\n```\n\nIn Python, the docstring for `GoldenRetriever.bark` is empty, even though one was\ndefined in `Dog.bark`. If pdoc generates documentation for the above\ncode, then it will automatically attach the docstring for `Dog.bark` to\n`GoldenRetriever.bark` if it does not have a docstring.\n\n\n# Limitations\n\n  - **Scope:** pdoc main use case is API documentation.\n    If you have substantially more complex documentation needs, we recommend using [Sphinx](https://www.sphinx-doc.org/)!\n  - **Dynamic analysis:** pdoc makes heavy use of dynamic analysis to extract docstrings.\n    This means your Python modules will be executed/imported when pdoc runs.\n  - **HTML Output:** pdoc only supports HTML as an output format. If you want to use pdoc with a static site\n    generator that only accepts Markdown, that may work nonetheless ‚Äì take a look at\n    [integrating pdoc into other systems](https://pdoc.dev/docs/pdoc.html#integrate-pdoc-into-other-systems).\n\n\n# Markdown Support\n\n[Markdown](https://guides.github.com/features/mastering-markdown/) is a lightweight and popular markup language for text\nformatting. There are many versions or *\"flavors\"* of Markdown.\npdoc uses the [markdown2](https://github.com/trentm/python-markdown2) library, which closely matches\nthe behavior of the original [Markdown 1.0.1 spec][].\nIn addition, the following extra syntax elements are enabled:\n\n  - **[code-friendly][]:** Disable `_` and `__` for `em` and `strong`.\n  - **[cuddled-lists][]:** Allow lists to be cuddled to the preceding\n    paragraph.\n  - **[fenced-code-blocks][]:** Allows a code block to not have to be\n    indented by fencing it with <code>```</code> on a line before and after.\n    Based on [GitHub-Flavored Markdown][] with support for syntax highlighting.\n  - **[footnotes][]:** Support footnotes as in use on daringfireball.net\n    and implemented in other Markdown processors.\n  - **[header-ids][]:** Adds \"id\" attributes to headers. The id value\n    is a slug of the header text.\n  - **[markdown-in-html][]:** Allow the use of `markdown=\"1\"` in a\n    block HTML tag to have markdown processing be done on its contents.\n    Similar to [PHP-Markdown Extra][] but with some limitations.\n  - **[mermaid][]:** Allows rendering Mermaid diagrams from included Markdown files using <code>```mermaid</code> fence blocks.\n  - **[pyshell][]:** Treats unindented Python interactive shell\n    sessions as `<code>` blocks.\n  - **strike:** Parse `~~strikethrough~~` formatting.\n  - **[tables][]:** Tables using the same format as [GitHub-Flavored Markdown][] and\n    [PHP-Markdown Extra][].\n  - **task_list:** Allows GitHub-style task lists (i.e. check boxes)\n  - **toc:** The returned HTML string gets a new \"toc_html\"\n    attribute which is a Table of Contents for the document.\n\n[Markdown 1.0.1 spec]: https://daringfireball.net/projects/markdown/\n[code-friendly]: https://github.com/trentm/python-markdown2/wiki/code-friendly\n[cuddled-lists]: https://github.com/trentm/python-markdown2/wiki/cuddled-lists\n[fenced-code-blocks]: https://github.com/trentm/python-markdown2/wiki/fenced-code-blocks\n[footnotes]: https://github.com/trentm/python-markdown2/wiki/footnotes\n[header-ids]: https://github.com/trentm/python-markdown2/wiki/header-ids\n[markdown-in-html]: https://github.com/trentm/python-markdown2/wiki/markdown-in-html\n[mermaid]: https://github.com/trentm/python-markdown2/wiki/mermaid\n[pyshell]: https://github.com/trentm/python-markdown2/wiki/pyshell\n[tables]: https://github.com/trentm/python-markdown2/wiki/tables\n[GitHub-Flavored Markdown]: https://github.github.com/gfm/\n[PHP-Markdown Extra]: https://michelf.ca/projects/php-markdown/extra/#table\n\nIt is possible (but not recommended) to use another Markdown library with pdoc.\nSee [#401](https://github.com/mitmproxy/pdoc/issues/401#issuecomment-1148661829) for details.\n\n# Using pdoc as a library\n\npdoc provides the high-level `pdoc.pdoc()` interface explained below. This makes it possible to do custom adjustments\nto your Python code before pdoc is used.\n\nIt is also possible to create `pdoc.doc.Module` objects directly and modify them before rendering.\nYou can find an example in [`examples/library-usage`](https://github.com/mitmproxy/pdoc/tree/main/examples/library-usage).\n'''\n\nfrom __future__ import annotations\n\n__docformat__ = \"markdown\"  # explicitly disable rST processing in the examples above.\n__version__ = \"15.0.4\"  # this is read from setup.py\n\nfrom pathlib import Path\nfrom typing import overload\n\nfrom pdoc import doc\nfrom pdoc import extract\nfrom pdoc import render\n\n\n@overload\ndef pdoc(\n    *modules: Path | str,\n    output_directory: None = None,\n) -> str:\n    pass\n\n\n@overload\ndef pdoc(\n    *modules: Path | str,\n    output_directory: Path,\n) -> None:\n    pass\n\n\ndef pdoc(\n    *modules: Path | str,\n    output_directory: Path | None = None,\n) -> str | None:\n    \"\"\"\n    Render the documentation for a list of modules.\n\n     - If `output_directory` is `None`, returns the rendered documentation\n       for the first module in the list.\n     - If `output_directory` is set, recursively writes the rendered output\n       for all specified modules and their submodules to the target destination.\n\n    Rendering options can be configured by calling `pdoc.render.configure` in advance.\n    \"\"\"\n    all_modules: dict[str, doc.Module] = {}\n    for module_name in extract.walk_specs(modules):\n        all_modules[module_name] = doc.Module.from_name(module_name)\n\n    for module in all_modules.values():\n        out = render.html_module(module, all_modules)\n        if not output_directory:\n            return out\n        else:\n            outfile = output_directory / f\"{module.fullname.replace('.', '/')}.html\"\n            outfile.parent.mkdir(parents=True, exist_ok=True)\n            outfile.write_bytes(out.encode())\n\n    assert output_directory\n\n    index = render.html_index(all_modules)\n    if index:\n        (output_directory / \"index.html\").write_bytes(index.encode())\n\n    search = render.search_index(all_modules)\n    if search:\n        (output_directory / \"search.js\").write_bytes(search.encode())\n\n    return None\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "pdoc/__init__.py", "language": "python", "complexity": "complex", "star_count": 2166, "quality_score": 0.87, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 548, "word_count": 2770}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport argparse\nfrom pathlib import Path\nimport platform\nimport subprocess\nimport sys\nimport warnings\n\nimport pdoc.doc\nimport pdoc.extract\nimport pdoc.render\nimport pdoc.web\n\nif sys.stdout.isatty():  # pragma: no cover\n    red = \"\\x1b[31m\"\n    yellow = \"\\x1b[33m\"\n    gray = \"\\x1b[2m\"\n    white = \"\\x1b[1m\"\n    default = \"\\x1b[0m\"\nelse:\n    red = yellow = gray = white = default = \"\"\n\nparser = argparse.ArgumentParser(\n    description=\"Automatically generate API docs for Python modules.\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    add_help=False,\n)\nmainargs = parser.add_argument_group(f\"{white}Main Arguments{default}\")\nmainargs.add_argument(\n    \"modules\",\n    type=str,\n    default=[],\n    metavar=\"module\",\n    nargs=\"*\",\n    help='Python module names. These may be importable Python module names (\"pdoc.doc\") or file paths (\"./pdoc/doc.py\")'\n    '. Exclude submodules by specifying a negative !regex pattern, e.g. \"foo !foo.bar\".',\n)\nmainargs.add_argument(\n    \"-o\",\n    \"--output-directory\",\n    metavar=\"DIR\",\n    type=Path,\n    help=\"Write rendered documentation to the specified directory, don't start a webserver.\",\n)\n\nrenderopts = parser.add_argument_group(f\"{white}Customize Rendering{default}\")\nrenderopts.add_argument(\n    \"-d\",\n    \"--docformat\",\n    type=str,\n    default=\"restructuredtext\",\n    choices=(\"markdown\", \"google\", \"numpy\", \"restructuredtext\"),\n    help=\"The default docstring format. For non-Markdown formats, pdoc will first convert matching syntax elements to \"\n    \"Markdown and then process everything as Markdown.\",\n)\nrenderopts.add_argument(\n    \"--include-undocumented\",\n    action=argparse.BooleanOptionalAction,\n    default=True,\n    help=\"Show classes/functions/variables that do not have a docstring.\",\n)\nrenderopts.add_argument(\n    \"-e\",\n    \"--edit-url\",\n    action=\"append\",\n    type=str,\n    default=[],\n    metavar=\"module=url\",\n    help=\"A mapping between module names and URL prefixes, used to display an 'Edit' button. \"\n    \"May be passed multiple times. \"\n    \"Example: pdoc=https://github.com/mitmproxy/pdoc/blob/main/pdoc/\",\n)\nrenderopts.add_argument(\n    \"--favicon\",\n    type=str,\n    metavar=\"URL\",\n    help=\"Specify a custom favicon URL.\",\n)\nrenderopts.add_argument(\n    \"--footer-text\",\n    type=str,\n    metavar=\"TEXT\",\n    help=\"Custom text for the page footer, for example the project name and current version number.\",\n)\nrenderopts.add_argument(\n    \"--logo\",\n    type=str,\n    metavar=\"URL\",\n    help=\"Add a project logo image.\",\n)\nrenderopts.add_argument(\n    \"--logo-link\",\n    type=str,\n    metavar=\"URL\",\n    help=\"Optional URL the logo should point to.\",\n)\nrenderopts.add_argument(\n    \"--math\",\n    action=argparse.BooleanOptionalAction,\n    default=False,\n    help=\"Include MathJax from a CDN to enable math formula rendering.\",\n)\nrenderopts.add_argument(\n    \"--mermaid\",\n    action=argparse.BooleanOptionalAction,\n    default=False,\n    help=\"Include Mermaid.js from a CDN to enable Mermaid diagram rendering.\",\n)\nrenderopts.add_argument(\n    \"--search\",\n    action=argparse.BooleanOptionalAction,\n    default=True,\n    help=\"Enable search functionality if multiple modules are documented.\",\n)\nrenderopts.add_argument(\n    \"--show-source\",\n    action=argparse.BooleanOptionalAction,\n    default=True,\n    help='Display \"View Source\" buttons.',\n)\nrenderopts.add_argument(\n    \"-t\",\n    \"--template-directory\",\n    metavar=\"DIR\",\n    type=Path,\n    default=None,\n    help=\"A directory containing Jinja2 templates to customize output. \"\n    \"Alternatively, put your templates in $XDG_CONFIG_HOME/pdoc and pdoc will automatically find them.\",\n)\n\nmiscargs = parser.add_argument_group(f\"{white}Miscellaneous Options{default}\")\nmiscargs.add_argument(\n    \"-h\",\n    \"--host\",\n    type=str,\n    default=\"localhost\",\n    help=\"The host on which to run the HTTP server.\",\n)\nmiscargs.add_argument(\n    \"-p\",\n    \"--port\",\n    type=int,\n    default=None,\n    help=\"The port on which to run the HTTP server.\",\n)\nmiscargs.add_argument(\n    \"-n\",\n    \"--no-browser\",\n    action=\"store_true\",\n    help=\"Don't open a browser after the web server has started.\",\n)\nmiscargs.add_argument(\"--help\", action=\"help\", help=\"Show this help message and exit.\")\nmiscargs.add_argument(\n    \"--version\",\n    action=\"store_true\",\n    default=argparse.SUPPRESS,\n    help=\"Show version information and exit.\",\n)\n\n\ndef cli(args: list[str] | None = None) -> None:\n    \"\"\"Command-line entry point\"\"\"\n    opts = parser.parse_args(args)\n    if getattr(opts, \"version\", False):\n        print(\n            f\"pdoc: {get_dev_version()}\\n\"\n            f\"Python: {platform.python_version()}\\n\"\n            f\"Platform: {platform.platform()}\"\n        )\n        return\n\n    if not opts.modules:\n        parser.print_help()\n        print(\n            f\"\\n{red}Error: Please specify which files or modules you want to document.{default}\"\n        )\n        sys.exit(1)\n\n    warnings.showwarning = _nicer_showwarning\n\n    pdoc.render.configure(\n        docformat=opts.docformat,\n        include_undocumented=opts.include_undocumented,\n        edit_url_map=dict(x.split(\"=\", 1) for x in opts.edit_url),\n        favicon=opts.favicon,\n        footer_text=opts.footer_text,\n        logo=opts.logo,\n        logo_link=opts.logo_link,\n        math=opts.math,\n        mermaid=opts.mermaid,\n        search=opts.search,\n        show_source=opts.show_source,\n        template_directory=opts.template_directory,\n    )\n\n    if opts.output_directory:\n        pdoc.pdoc(\n            *opts.modules,\n            output_directory=opts.output_directory,\n        )\n        return\n    else:\n        try:\n            try:\n                httpd = pdoc.web.DocServer((opts.host, opts.port or 8080), opts.modules)\n            except OSError:\n                # Couldn't bind, let's try again with a random port.\n                httpd = pdoc.web.DocServer((opts.host, opts.port or 0), opts.modules)\n        except OSError as e:\n            print(\n                f\"{red}Cannot start web server on {opts.host}:{opts.port}: {e}{default}\"\n            )\n            sys.exit(1)\n\n        with httpd:\n            url = f\"http://{opts.host}:{httpd.server_port}\"\n            print(f\"pdoc server ready at {url}\")\n            if not opts.no_browser:\n                pdoc.web.open_browser(url)\n            try:\n                httpd.serve_forever()\n            except KeyboardInterrupt:\n                httpd.server_close()\n                return\n\n\ndef get_dev_version() -> str:\n    \"\"\"\n    Return a detailed version string, sourced either from VERSION or obtained dynamically using git.\n    \"\"\"\n\n    pdoc_version = pdoc.__version__\n\n    here = Path(__file__).parent\n\n    try:  # pragma: no cover\n        # Check that we're in the pdoc repository:\n        # 60665024af9af2cda4229e91b4d15f2359a4a3dd is the first pdoc commit.\n        subprocess.run(\n            [\"git\", \"cat-file\", \"-e\", \"60665024af9af2cda4229e91b4d15f2359a4a3dd\"],\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n            cwd=here,\n            check=True,\n        )\n        git_describe = subprocess.check_output(\n            [\"git\", \"describe\", \"--tags\", \"--long\"],\n            stderr=subprocess.STDOUT,\n            cwd=here,\n        )\n        last_tag, tag_dist_str, commit = git_describe.decode().strip().rsplit(\"-\", 2)\n        commit = commit.lstrip(\"g\")[:7]\n        tag_dist = int(tag_dist_str)\n    except Exception:  # pragma: no cover\n        pass\n    else:\n        # Add commit info for non-tagged releases\n        if tag_dist > 0:  # pragma: no cover\n            pdoc_version += f\" (+{tag_dist}, commit {commit})\"\n\n    return pdoc_version\n\n\ndef _nicer_showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"A replacement for `warnings.showwarning` that renders warnings in a more visually pleasing way.\"\"\"\n    if category is UserWarning:\n        print(\n            f\"{yellow}Warn:{default} {message} {gray}({filename}:{lineno}){default}\",\n            file=sys.stderr,\n        )\n    elif category is RuntimeWarning:\n        print(\n            f\"{yellow}Warn:{default} {message}\",\n            file=sys.stderr,\n        )\n    else:\n        print(\n            f\"{yellow}{category.__name__}:{default} {message} {gray}({filename}:{lineno}){default}\",\n            file=sys.stderr,\n        )\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    cli()\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "pdoc/__main__.py", "language": "python", "complexity": "complex", "star_count": 2166, "quality_score": 0.78, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 286, "word_count": 709}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# fmt: off\nimport sys\n\nif sys.version_info >= (3, 12):\n    from ast import TypeAlias as ast_TypeAlias\nelse:  # pragma: no cover\n    class ast_TypeAlias:\n        pass\n\nif sys.version_info >= (3, 12):\n    from typing import TypeAliasType\nelse:  # pragma: no cover\n    class TypeAliasType:\n        \"\"\"Placeholder class for TypeAliasType\"\"\"\n\nif sys.version_info >= (3, 10):\n    from typing import TypeAlias\nelse:  # pragma: no cover\n    class TypeAlias:\n        pass\n\nif sys.version_info >= (3, 10):\n    from types import UnionType  # type: ignore\nelse:  # pragma: no cover\n    class UnionType:\n        pass\n\n\nif (3, 9) <= sys.version_info < (3, 9, 8) or (3, 10) <= sys.version_info < (3, 10, 1):  # pragma: no cover\n    import inspect\n    import types\n\n    def formatannotation(annotation) -> str:\n        \"\"\"\n        https://github.com/python/cpython/pull/29212\n        \"\"\"\n        if isinstance(annotation, types.GenericAlias):\n            return str(annotation)\n        return inspect.formatannotation(annotation)\nelse:\n    from inspect import formatannotation\n\nif sys.version_info >= (3, 10):\n    from typing import is_typeddict\nelse:  # pragma: no cover\n    def is_typeddict(tp):\n        try:\n            return tp.__orig_bases__[-1].__name__ == \"TypedDict\"\n        except Exception:\n            return False\n\n\n__all__ = [\n    \"ast_TypeAlias\",\n    \"TypeAliasType\",\n    \"TypeAlias\",\n    \"UnionType\",\n    \"formatannotation\",\n    \"is_typeddict\",\n]\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "pdoc/_compat.py", "language": "python", "complexity": "simple", "star_count": 2166, "quality_score": 0.78, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 61, "word_count": 161}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis module handles all interpretation of the *Abstract Syntax Tree (AST)* in pdoc.\n\nParsing the AST is done to extract docstrings, type annotations, and variable declarations from `__init__`.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport ast\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom dataclasses import dataclass\nfrom functools import cache\nimport inspect\nfrom itertools import tee\nfrom itertools import zip_longest\nimport types\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import TypeVar\nfrom typing import overload\nimport warnings\n\nimport pdoc\n\nfrom ._compat import ast_TypeAlias\n\nif TYPE_CHECKING:\n    import pdoc.doc_types\n\n\ndef get_source(obj: Any) -> str:\n    \"\"\"\n    Returns the source code of the Python object `obj` as a str.\n\n    If this fails, an empty string is returned.\n    \"\"\"\n    # Some objects may not be hashable, so we fall back to the non-cached version if that is the case.\n    try:\n        return _get_source(obj)\n    except TypeError:\n        return _get_source.__wrapped__(obj)\n\n\n@cache\ndef _get_source(obj: Any) -> str:\n    try:\n        return inspect.getsource(obj)\n    except Exception:\n        return \"\"\n\n\n@overload\ndef parse(obj: types.ModuleType) -> ast.Module: ...\n\n\n@overload\ndef parse(obj: types.FunctionType) -> ast.FunctionDef | ast.AsyncFunctionDef: ...\n\n\n@overload\ndef parse(obj: type) -> ast.ClassDef: ...\n\n\ndef parse(obj):\n    \"\"\"\n    Parse a module, class or function and return the (unwrapped) AST node.\n    If an object's source code cannot be found, this function returns an empty ast node stub\n    which can still be walked.\n    \"\"\"\n    src = get_source(obj)\n    if isinstance(obj, types.ModuleType):\n        return _parse_module(src)\n    elif isinstance(obj, type):\n        return _parse_class(src)\n    else:\n        return _parse_function(src)\n\n\n@cache\ndef unparse(tree: ast.AST):\n    \"\"\"`ast.unparse`, but cached.\"\"\"\n    return ast.unparse(tree)\n\n\n@dataclass\nclass AstInfo:\n    \"\"\"The information extracted from walking the syntax tree.\"\"\"\n\n    var_docstrings: dict[str, str]\n    \"\"\"A qualname -> docstring mapping.\"\"\"\n    func_docstrings: dict[str, str]\n    \"\"\"A qualname -> docstring mapping for functions.\"\"\"\n    annotations: dict[str, str | type[pdoc.doc_types.empty]]\n    \"\"\"A qualname -> annotation mapping.\n    \n    Annotations are not evaluated by this module and only returned as strings.\"\"\"\n\n\ndef walk_tree(obj: types.ModuleType | type) -> AstInfo:\n    \"\"\"\n    Walks the abstract syntax tree for `obj` and returns the extracted information.\n    \"\"\"\n    return _walk_tree(parse(obj))\n\n\n@cache\ndef _walk_tree(\n    tree: ast.Module | ast.ClassDef | ast.FunctionDef | ast.AsyncFunctionDef,\n) -> AstInfo:\n    var_docstrings = {}\n    func_docstrings = {}\n    annotations = {}\n    for a, b in _pairwise_longest(_nodes(tree)):\n        if isinstance(a, ast_TypeAlias):\n            name = a.name.id\n        elif (\n            isinstance(a, ast.AnnAssign) and isinstance(a.target, ast.Name) and a.simple\n        ):\n            name = a.target.id\n            annotations[name] = unparse(a.annotation)\n        elif (\n            isinstance(a, ast.Assign)\n            and len(a.targets) == 1\n            and isinstance(a.targets[0], ast.Name)\n        ):\n            name = a.targets[0].id\n            # Make sure that all assignments are picked up, even is there is\n            # no annotation or docstring.\n            annotations.setdefault(name, pdoc.doc_types.empty)\n        elif isinstance(a, ast.FunctionDef) and a.body:\n            first = a.body[0]\n            if (\n                isinstance(first, ast.Expr)\n                and isinstance(first.value, ast.Constant)\n                and isinstance(first.value.value, str)\n            ):\n                func_docstrings[a.name] = inspect.cleandoc(first.value.value).strip()\n            continue\n        else:\n            continue\n        if (\n            isinstance(b, ast.Expr)\n            and isinstance(b.value, ast.Constant)\n            and isinstance(b.value.value, str)\n        ):\n            var_docstrings[name] = inspect.cleandoc(b.value.value).strip()\n    return AstInfo(\n        var_docstrings,\n        func_docstrings,\n        annotations,\n    )\n\n\nT = TypeVar(\"T\")\n\n\ndef sort_by_source(\n    obj: types.ModuleType | type, sorted: dict[str, T], unsorted: dict[str, T]\n) -> tuple[dict[str, T], dict[str, T]]:\n    \"\"\"\n    Takes items from `unsorted` and inserts them into `sorted` in order of appearance in the source code of `obj`.\n    The only exception to this rule is `__init__`, which (if present) is always inserted first.\n\n    Some items may not be found, for example because they've been inherited from a superclass. They are returned as-is.\n\n    Returns a `(sorted, not found)` tuple.\n    \"\"\"\n    tree = parse(obj)\n\n    if \"__init__\" in unsorted:\n        sorted[\"__init__\"] = unsorted.pop(\"__init__\")\n\n    for a in _nodes(tree):\n        if (\n            isinstance(a, ast.Assign)\n            and len(a.targets) == 1\n            and isinstance(a.targets[0], ast.Name)\n        ):\n            name = a.targets[0].id\n        elif (\n            isinstance(a, ast.AnnAssign) and isinstance(a.target, ast.Name) and a.simple\n        ):\n            name = a.target.id\n        elif isinstance(a, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n            name = a.name\n        elif isinstance(a, ast_TypeAlias):\n            name = a.name.id\n        else:\n            continue\n\n        if name in unsorted:\n            sorted[name] = unsorted.pop(name)\n    return sorted, unsorted\n\n\ndef type_checking_sections(mod: types.ModuleType) -> ast.Module:\n    \"\"\"\n    Walks the abstract syntax tree for `mod` and returns all statements guarded by TYPE_CHECKING blocks.\n    \"\"\"\n    ret = ast.Module(body=[], type_ignores=[])\n    tree = _parse_module(get_source(mod))\n    for node in tree.body:\n        if (\n            isinstance(node, ast.If)\n            and isinstance(node.test, ast.Name)\n            and node.test.id == \"TYPE_CHECKING\"\n        ):\n            ret.body.extend(node.body)\n        if (\n            isinstance(node, ast.If)\n            and isinstance(node.test, ast.Attribute)\n            and isinstance(node.test.value, ast.Name)\n            # some folks do \"import typing as t\", the accuracy with just TYPE_CHECKING is good enough.\n            # and node.test.value.id == \"typing\"\n            and node.test.attr == \"TYPE_CHECKING\"\n        ):\n            ret.body.extend(node.body)\n    return ret\n\n\n@cache\ndef _parse_module(source: str) -> ast.Module:\n    \"\"\"\n    Parse the AST for the source code of a module and return the ast.Module.\n\n    Returns an empty ast.Module if source is empty.\n    \"\"\"\n    tree = _parse(source)\n    assert isinstance(tree, ast.Module)\n    return tree\n\n\n@cache\ndef _parse_class(source: str) -> ast.ClassDef:\n    \"\"\"\n    Parse the AST for the source code of a class and return the ast.ClassDef.\n\n    Returns an empty ast.ClassDef if source is empty.\n    \"\"\"\n    tree = _parse(source)\n    if tree.body and len(tree.body) == 1:\n        t = tree.body[0]\n        if isinstance(t, ast.ClassDef):\n            return t\n    return ast.ClassDef(name=\"PdocStub\", body=[], decorator_list=[])  # type: ignore\n\n\n@cache\ndef _parse_function(source: str) -> ast.FunctionDef | ast.AsyncFunctionDef:\n    \"\"\"\n    Parse the AST for the source code of a (async) function and return the matching AST node.\n\n    Returns an empty ast.FunctionDef if source is empty.\n    \"\"\"\n    tree = _parse(source)\n    if tree.body and len(tree.body) == 1:\n        t = tree.body[0]\n        if isinstance(t, (ast.FunctionDef, ast.AsyncFunctionDef)):\n            return t\n        else:\n            # we have a lambda function,\n            # to simplify the API return the ast.FunctionDef stub.\n            pass\n    return ast.FunctionDef(\n        name=\"pdoc_stub\", args=ast.arguments(), body=[], decorator_list=[]\n    )  # type: ignore\n\n\ndef _parse(\n    source: str,\n) -> ast.Module | ast.ClassDef | ast.FunctionDef | ast.AsyncFunctionDef:\n    try:\n        return ast.parse(_dedent(source))\n    except Exception as e:\n        warnings.warn(f\"Error parsing source code: {e}\\n===\\n{source}\\n===\")\n        return ast.parse(\"\")\n\n\n@cache\ndef _dedent(source: str) -> str:\n    \"\"\"\n    Dedent the head of a function or class definition so that it can be parsed by `ast.parse`.\n    This is an alternative to `textwrap.dedent`, which does not dedent if there are docstrings\n    without indentation. For example, this is valid Python code but would not be dedented with `textwrap.dedent`:\n\n    class Foo:\n        def bar(self):\n           '''\n    this is a docstring\n           '''\n    \"\"\"\n    if not source or source[0] not in (\" \", \"\\t\"):\n        return source\n    source = source.lstrip()\n    # we may have decorators before our function definition, in which case we need to dedent a few more lines.\n    # the following heuristic should be good enough to detect if we have reached the definition.\n    # it's easy to produce examples where this fails, but this probably is not a problem in practice.\n    if not any(source.startswith(x) for x in [\"async \", \"def \", \"class \"]):\n        first_line, rest = source.split(\"\\n\", 1)\n        return first_line + \"\\n\" + _dedent(rest)\n    else:\n        return source\n\n\n@cache\ndef _nodes(tree: ast.Module | ast.ClassDef) -> list[ast.AST]:\n    \"\"\"\n    Returns the list of all nodes in tree's body, but also inlines the body of __init__.\n\n    This is useful to detect all declared variables in a class, even if they only appear in the constructor.\n    \"\"\"\n    return list(_nodes_iter(tree))\n\n\ndef _nodes_iter(tree: ast.Module | ast.ClassDef) -> Iterator[ast.AST]:\n    for a in tree.body:\n        yield a\n        if isinstance(a, ast.FunctionDef) and a.name == \"__init__\":\n            yield from _init_nodes(a)\n\n\ndef _init_nodes(tree: ast.FunctionDef) -> Iterator[ast.AST]:\n    \"\"\"\n    Transform attribute assignments like \"self.foo = 42\" to name assignments like \"foo = 42\",\n    keep all constant expressions, and no-op everything else.\n    This essentially allows us to inline __init__ when parsing a class definition.\n    \"\"\"\n    for a in tree.body:\n        if (\n            isinstance(a, ast.AnnAssign)\n            and isinstance(a.target, ast.Attribute)\n            and isinstance(a.target.value, ast.Name)\n            and a.target.value.id == \"self\"\n        ):\n            yield ast.AnnAssign(\n                ast.Name(a.target.attr), a.annotation, a.value, simple=1\n            )\n        elif (\n            isinstance(a, ast.Assign)\n            and len(a.targets) == 1\n            and isinstance(a.targets[0], ast.Attribute)\n            and isinstance(a.targets[0].value, ast.Name)\n            and a.targets[0].value.id == \"self\"\n        ):\n            yield ast.Assign(\n                [ast.Name(a.targets[0].attr)],\n                value=a.value,\n                type_comment=a.type_comment,\n            )\n        elif (\n            isinstance(a, ast.Expr)\n            and isinstance(a.value, ast.Constant)\n            and isinstance(a.value.value, str)\n        ):\n            yield a\n        else:\n            yield ast.Pass()\n\n\ndef _pairwise_longest(iterable: Iterable[T]) -> Iterable[tuple[T, T]]:\n    \"\"\"s -> (s0,s1), (s1,s2), (s2, s3),  ..., (sN, None)\"\"\"\n    a, b = tee(iterable)\n    next(b, None)\n    return zip_longest(a, b)\n", "metadata": {"source": "github.com/mitmproxy/pdoc", "file_path": "pdoc/doc_ast.py", "language": "python", "complexity": "complex", "star_count": 2166, "quality_score": 0.84, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 366, "word_count": 1244}, "authority_level": "high", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "<!--\nNOTE: Only submit issues for Python package \"pdoc3\" on PyPI.\nPython package \"pdoc\" lives elsewhere.\n-->\n\n\n### Expected Behavior\n\n\n\n### Actual Behavior\n\n\n\n### Steps to Reproduce\n\n1.\n2.\n3.\n\n\n### Additional info\n\n- pdoc version:\n", "metadata": {"source": "github.com/pdoc3/pdoc", "file_path": ".github/issue_template.md", "language": "markdown", "complexity": "simple", "star_count": 1174, "quality_score": 0.6, "features": {"has_code_examples": false, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 25, "word_count": 36}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments with installation and setup instructions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Contributing guidelines\n=======================\n\nIssues\n------\nOnly report issues for **PyPI package _pdoc3_**.\nIf your issue pertains to the other PyPI package _pdoc_,\nreport its issue [here](https://github.com/mitmproxy/pdoc).\n\nBefore reporting an issue, please see if a similar issue is already open.\nIf applicable, also check if a similar issue has recently been closed\n‚Äî your bug might have been just fixed.\n\nTo have your issue dealt with promptly, it's best to construct a\n[minimal working example] that exposes the issue in a clear and\nreproducible manner.\n\n[minimal working example]: https://en.wikipedia.org/wiki/Minimal_working_example\n\n\nInstallation\n------------\nTo install a developmental version of the project,\nfirst [fork the project]. Then:\n\n    git clone git@github.com:YOUR_USERNAME/pdoc\n    cd pdoc\n    pip install -e .   # Mind the dot\n\n[fork the project]: https://help.github.com/articles/fork-a-repo/\n\n\nTesting\n-------\nPlease write reasonable unit tests for any new / changed functionality.\nSee _pdoc/test_ directory for existing tests.\nBefore submitting a PR, ensure the tests pass:\n\n    python -m unittest -v pdoc.test\n\nAlso ensure that idiomatic code style is respected by running:\n\n    flake8\n\n\nDocumentation\n-------------\nSee _doc/README.md_. All documentation is generated from\n[pdoc]-compatible docstrings in code.\n\n[pdoc]: https://pdoc3.github.io/pdoc\n\n\nPull requests\n-------------\nIf you're new to proposing changes on GitHub, help yourself to an\n[appropriate guide]. Additionally, please use explicit commit messages.\nSee [NumPy's development workflow] for inspiration.\n\n[appropriate guide]: https://gist.github.com/Chaser324/ce0505fbed06b947d962\n[NumPy's development workflow]: https://numpy.org/doc/stable/dev/development_workflow.html\n", "metadata": {"source": "github.com/pdoc3/pdoc", "file_path": "CONTRIBUTING.md", "language": "markdown", "complexity": "medium", "star_count": 1174, "quality_score": 0.63, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 62, "word_count": 214}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with installation and setup instructions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "                    GNU AFFERO GENERAL PUBLIC LICENSE\n                       Version 3, 19 November 2007\n\n Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The GNU Affero General Public License is a free, copyleft license for\nsoftware and other kinds of works, specifically designed to ensure\ncooperation with the community in the case of network server software.\n\n  The licenses for most software and other practical works are designed\nto take away your freedom to share and change the works.  By contrast,\nour General Public Licenses are intended to guarantee your freedom to\nshare and change all versions of a program--to make sure it remains free\nsoftware for all its users.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthem if you wish), that you receive source code or can get it if you\nwant it, that you can change the software or use pieces of it in new\nfree programs, and that you know you can do these things.\n\n  Developers that use our General Public Licenses protect your rights\nwith two steps: (1) assert copyright on the software, and (2) offer\nyou this License which gives you legal permission to copy, distribute\nand/or modify the software.\n\n  A secondary benefit of defending all users' freedom is that\nimprovements made in alternate versions of the program, if they\nreceive widespread use, become available for other developers to\nincorporate.  Many developers of free software are heartened and\nencouraged by the resulting cooperation.  However, in the case of\nsoftware used on network servers, this result may fail to come about.\nThe GNU General Public License permits making a modified version and\nletting the public access it on a server without ever releasing its\nsource code to the public.\n\n  The GNU Affero General Public License is designed specifically to\nensure that, in such cases, the modified source code becomes available\nto the community.  It requires the operator of a network server to\nprovide the source code of the modified version running there to the\nusers of that server.  Therefore, public use of a modified version, on\na publicly accessible server, gives the public access to the source\ncode of the modified version.\n\n  An older license, called the Affero General Public License and\npublished by Affero, was designed to accomplish similar goals.  This is\na different license, not a version of the Affero GPL, but Affero has\nreleased a new version of the Affero GPL which permits relicensing under\nthis license.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                       TERMS AND CONDITIONS\n\n  0. Definitions.\n\n  \"This License\" refers to version 3 of the GNU Affero General Public License.\n\n  \"Copyright\" also means copyright-like laws that apply to other kinds of\nworks, such as semiconductor masks.\n\n  \"The Program\" refers to any copyrightable work licensed under this\nLicense.  Each licensee is addressed as \"you\".  \"Licensees\" and\n\"recipients\" may be individuals or organizations.\n\n  To \"modify\" a work means to copy from or adapt all or part of the work\nin a fashion requiring copyright permission, other than the making of an\nexact copy.  The resulting work is called a \"modified version\" of the\nearlier work or a work \"based on\" the earlier work.\n\n  A \"covered work\" means either the unmodified Program or a work based\non the Program.\n\n  To \"propagate\" a work means to do anything with it that, without\npermission, would make you directly or secondarily liable for\ninfringement under applicable copyright law, except executing it on a\ncomputer or modifying a private copy.  Propagation includes copying,\ndistribution (with or without modification), making available to the\npublic, and in some countries other activities as well.\n\n  To \"convey\" a work means any kind of propagation that enables other\nparties to make or receive copies.  Mere interaction with a user through\na computer network, with no transfer of a copy, is not conveying.\n\n  An interactive user interface displays \"Appropriate Legal Notices\"\nto the extent that it includes a convenient and prominently visible\nfeature that (1) displays an appropriate copyright notice, and (2)\ntells the user that there is no warranty for the work (except to the\nextent that warranties are provided), that licensees may convey the\nwork under this License, and how to view a copy of this License.  If\nthe interface presents a list of user commands or options, such as a\nmenu, a prominent item in the list meets this criterion.\n\n  1. Source Code.\n\n  The \"source code\" for a work means the preferred form of the work\nfor making modifications to it.  \"Object code\" means any non-source\nform of a work.\n\n  A \"Standard Interface\" means an interface that either is an official\nstandard defined by a recognized standards body, or, in the case of\ninterfaces specified for a particular programming language, one that\nis widely used among developers working in that language.\n\n  The \"System Libraries\" of an executable work include anything, other\nthan the work as a whole, that (a) is included in the normal form of\npackaging a Major Component, but which is not part of that Major\nComponent, and (b) serves only to enable use of the work with that\nMajor Component, or to implement a Standard Interface for which an\nimplementation is available to the public in source code form.  A\n\"Major Component\", in this context, means a major essential component\n(kernel, window system, and so on) of the specific operating system\n(if any) on which the executable work runs, or a compiler used to\nproduce the work, or an object code interpreter used to run it.\n\n  The \"Corresponding Source\" for a work in object code form means all\nthe source code needed to generate, install, and (for an executable\nwork) run the object code and to modify the work, including scripts to\ncontrol those activities.  However, it does not include the work's\nSystem Libraries, or general-purpose tools or generally available free\nprograms which are used unmodified in performing those activities but\nwhich are not part of the work.  For example, Corresponding Source\nincludes interface definition files associated with source files for\nthe work, and the source code for shared libraries and dynamically\nlinked subprograms that the work is specifically designed to require,\nsuch as by intimate data communication or control flow between those\nsubprograms and other parts of the work.\n\n  The Corresponding Source need not include anything that users\ncan regenerate automatically from other parts of the Corresponding\nSource.\n\n  The Corresponding Source for a work in source code form is that\nsame work.\n\n  2. Basic Permissions.\n\n  All rights granted under this License are granted for the term of\ncopyright on the Program, and are irrevocable provided the stated\nconditions are met.  This License explicitly affirms your unlimited\npermission to run the unmodified Program.  The output from running a\ncovered work is covered by this License only if the output, given its\ncontent, constitutes a covered work.  This License acknowledges your\nrights of fair use or other equivalent, as provided by copyright law.\n\n  You may make, run and propagate covered works that you do not\nconvey, without conditions so long as your license otherwise remains\nin force.  You may convey covered works to others for the sole purpose\nof having them make modifications exclusively for you, or provide you\nwith facilities for running those works, provided that you comply with\nthe terms of this License in conveying all material for which you do\nnot control copyright.  Those thus making or running the covered works\nfor you must do so exclusively on your behalf, under your direction\nand control, on terms that prohibit them from making any copies of\nyour copyrighted material outside their relationship with you.\n\n  Conveying under any other circumstances is permitted solely under\nthe conditions stated below.  Sublicensing is not allowed; section 10\nmakes it unnecessary.\n\n  3. Protecting Users' Legal Rights From Anti-Circumvention Law.\n\n  No covered work shall be deemed part of an effective technological\nmeasure under any applicable law fulfilling obligations under article\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\nsimilar laws prohibiting or restricting circumvention of such\nmeasures.\n\n  When you convey a covered work, you waive any legal power to forbid\ncircumvention of technological measures to the extent such circumvention\nis effected by exercising rights under this License with respect to\nthe covered work, and you disclaim any intention to limit operation or\nmodification of the work as a means of enforcing, against the work's\nusers, your or third parties' legal rights to forbid circumvention of\ntechnological measures.\n\n  4. Conveying Verbatim Copies.\n\n  You may convey verbatim copies of the Program's source code as you\nreceive it, in any medium, provided that you conspicuously and\nappropriately publish on each copy an appropriate copyright notice;\nkeep intact all notices stating that this License and any\nnon-permissive terms added in accord with section 7 apply to the code;\nkeep intact all notices of the absence of any warranty; and give all\nrecipients a copy of this License along with the Program.\n\n  You may charge any price or no price for each copy that you convey,\nand you may offer support or warranty protection for a fee.\n\n  5. Conveying Modified Source Versions.\n\n  You may convey a work based on the Program, or the modifications to\nproduce it from the Program, in the form of source code under the\nterms of section 4, provided that you also meet all of these conditions:\n\n    a) The work must carry prominent notices stating that you modified\n    it, and giving a relevant date.\n\n    b) The work must carry prominent notices stating that it is\n    released under this License and any conditions added under section\n    7.  This requirement modifies the requirement in section 4 to\n    \"keep intact all notices\".\n\n    c) You must license the entire work, as a whole, under this\n    License to anyone who comes into possession of a copy.  This\n    License will therefore apply, along with any applicable section 7\n    additional terms, to the whole of the work, and all its parts,\n    regardless of how they are packaged.  This License gives no\n    permission to license the work in any other way, but it does not\n    invalidate such permission if you have separately received it.\n\n    d) If the work has interactive user interfaces, each must display\n    Appropriate Legal Notices; however, if the Program has interactive\n    interfaces that do not display Appropriate Legal Notices, your\n    work need not make them do so.\n\n  A compilation of a covered work with other separate and independent\nworks, which are not by their nature extensions of the covered work,\nand which are not combined with it such as to form a larger program,\nin or on a volume of a storage or distribution medium, is called an\n\"aggregate\" if the compilation and its resulting copyright are not\nused to limit the access or legal rights of the compilation's users\nbeyond what the individual works permit.  Inclusion of a covered work\nin an aggregate does not cause this License to apply to the other\nparts of the aggregate.\n\n  6. Conveying Non-Source Forms.\n\n  You may convey a covered work in object code form under the terms\nof sections 4 and 5, provided that you also convey the\nmachine-readable Corresponding Source under the terms of this License,\nin one of these ways:\n\n    a) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by the\n    Corresponding Source fixed on a durable physical medium\n    customarily used for software interchange.\n\n    b) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by a\n    written offer, valid for at least three years and valid for as\n    long as you offer spare parts or customer support for that product\n    model, to give anyone who possesses the object code either (1) a\n    copy of the Corresponding Source for all the software in the\n    product that is covered by this License, on a durable physical\n    medium customarily used for software interchange, for a price no\n    more than your reasonable cost of physically performing this\n    conveying of source, or (2) access to copy the\n    Corresponding Source from a network server at no charge.\n\n    c) Convey individual copies of the object code with a copy of the\n    written offer to provide the Corresponding Source.  This\n    alternative is allowed only occasionally and noncommercially, and\n    only if you received the object code with such an offer, in accord\n    with subsection 6b.\n\n    d) Convey the object code by offering access from a designated\n    place (gratis or for a charge), and offer equivalent access to the\n    Corresponding Source in the same way through the same place at no\n    further charge.  You need not require recipients to copy the\n    Corresponding Source along with the object code.  If the place to\n    copy the object code is a network server, the Corresponding Source\n    may be on a different server (operated by you or a third party)\n    that supports equivalent copying facilities, provided you maintain\n    clear directions next to the object code saying where to find the\n    Corresponding Source.  Regardless of what server hosts the\n    Corresponding Source, you remain obligated to ensure that it is\n    available for as long as needed to satisfy these requirements.\n\n    e) Convey the object code using peer-to-peer transmission, provided\n    you inform other peers where the object code and Corresponding\n    Source of the work are being offered to the general public at no\n    charge under subsection 6d.\n\n  A separable portion of the object code, whose source code is excluded\nfrom the Corresponding Source as a System Library, need not be\nincluded in conveying the object code work.\n\n  A \"User Product\" is either (1) a \"consumer product\", which means any\ntangible personal property which is normally used for personal, family,\nor household purposes, or (2) anything designed or sold for incorporation\ninto a dwelling.  In determining whether a product is a consumer product,\ndoubtful cases shall be resolved in favor of coverage.  For a particular\nproduct received by a particular user, \"normally used\" refers to a\ntypical or common use of that class of product, regardless of the status\nof the particular user or of the way in which the particular user\nactually uses, or expects or is expected to use, the product.  A product\nis a consumer product regardless of whether the product has substantial\ncommercial, industrial or non-consumer uses, unless such uses represent\nthe only significant mode of use of the product.\n\n  \"Installation Information\" for a User Product means any methods,\nprocedures, authorization keys, or other information required to install\nand execute modified versions of a covered work in that User Product from\na modified version of its Corresponding Source.  The information must\nsuffice to ensure that the continued functioning of the modified object\ncode is in no case prevented or interfered with solely because\nmodification has been made.\n\n  If you convey an object code work under this section in, or with, or\nspecifically for use in, a User Product, and the conveying occurs as\npart of a transaction in which the right of possession and use of the\nUser Product is transferred to the recipient in perpetuity or for a\nfixed term (regardless of how the transaction is characterized), the\nCorresponding Source conveyed under this section must be accompanied\nby the Installation Information.  But this requirement does not apply\nif neither you nor any third party retains the ability to install\nmodified object code on the User Product (for example, the work has\nbeen installed in ROM).\n\n  The requirement to provide Installation Information does not include a\nrequirement to continue to provide support service, warranty, or updates\nfor a work that has been modified or installed by the recipient, or for\nthe User Product in which it has been modified or installed.  Access to a\nnetwork may be denied when the modification itself materially and\nadversely affects the operation of the network or violates the rules and\nprotocols for communication across the network.\n\n  Corresponding Source conveyed, and Installation Information provided,\nin accord with this section must be in a format that is publicly\ndocumented (and with an implementation available to the public in\nsource code form), and must require no special password or key for\nunpacking, reading or copying.\n\n  7. Additional Terms.\n\n  \"Additional permissions\" are terms that supplement the terms of this\nLicense by making exceptions from one or more of its conditions.\nAdditional permissions that are applicable to the entire Program shall\nbe treated as though they were included in this License, to the extent\nthat they are valid under applicable law.  If additional permissions\napply only to part of the Program, that part may be used separately\nunder those permissions, but the entire Program remains governed by\nthis License without regard to the additional permissions.\n\n  When you convey a copy of a covered work, you may at your option\nremove any additional permissions from that copy, or from any part of\nit.  (Additional permissions may be written to require their own\nremoval in certain cases when you modify the work.)  You may place\nadditional permissions on material, added by you to a covered work,\nfor which you have or can give appropriate copyright permission.\n\n  Notwithstanding any other provision of this License, for material you\nadd to a covered work, you may (if authorized by the copyright holders of\nthat material) supplement the terms of this License with terms:\n\n    a) Disclaiming warranty or limiting liability differently from the\n    terms of sections 15 and 16 of this License; or\n\n    b) Requiring preservation of specified reasonable legal notices or\n    author attributions in that material or in the Appropriate Legal\n    Notices displayed by works containing it; or\n\n    c) Prohibiting misrepresentation of the origin of that material, or\n    requiring that modified versions of such material be marked in\n    reasonable ways as different from the original version; or\n\n    d) Limiting the use for publicity purposes of names of licensors or\n    authors of the material; or\n\n    e) Declining to grant rights under trademark law for use of some\n    trade names, trademarks, or service marks; or\n\n    f) Requiring indemnification of licensors and authors of that\n    material by anyone who conveys the material (or modified versions of\n    it) with contractual assumptions of liability to the recipient, for\n    any liability that these contractual assumptions directly impose on\n    those licensors and authors.\n\n  All other non-permissive additional terms are considered \"further\nrestrictions\" within the meaning of section 10.  If the Program as you\nreceived it, or any part of it, contains a notice stating that it is\ngoverned by this License along with a term that is a further\nrestriction, you may remove that term.  If a license document contains\na further restriction but permits relicensing or conveying under this\nLicense, you may add to a covered work material governed by the terms\nof that license document, provided that the further restriction does\nnot survive such relicensing or conveying.\n\n  If you add terms to a covered work in accord with this section, you\nmust place, in the relevant source files, a statement of the\nadditional terms that apply to those files, or a notice indicating\nwhere to find the applicable terms.\n\n  Additional terms, permissive or non-permissive, may be stated in the\nform of a separately written license, or stated as exceptions;\nthe above requirements apply either way.\n\n  8. Termination.\n\n  You may not propagate or modify a covered work except as expressly\nprovided under this License.  Any attempt otherwise to propagate or\nmodify it is void, and will automatically terminate your rights under\nthis License (including any patent licenses granted under the third\nparagraph of section 11).\n\n  However, if you cease all violation of this License, then your\nlicense from a particular copyright holder is reinstated (a)\nprovisionally, unless and until the copyright holder explicitly and\nfinally terminates your license, and (b) permanently, if the copyright\nholder fails to notify you of the violation by some reasonable means\nprior to 60 days after the cessation.\n\n  Moreover, your license from a particular copyright holder is\nreinstated permanently if the copyright holder notifies you of the\nviolation by some reasonable means, this is the first time you have\nreceived notice of violation of this License (for any work) from that\ncopyright holder, and you cure the violation prior to 30 days after\nyour receipt of the notice.\n\n  Termination of your rights under this section does not terminate the\nlicenses of parties who have received copies or rights from you under\nthis License.  If your rights have been terminated and not permanently\nreinstated, you do not qualify to receive new licenses for the same\nmaterial under section 10.\n\n  9. Acceptance Not Required for Having Copies.\n\n  You are not required to accept this License in order to receive or\nrun a copy of the Program.  Ancillary propagation of a covered work\noccurring solely as a consequence of using peer-to-peer transmission\nto receive a copy likewise does not require acceptance.  However,\nnothing other than this License grants you permission to propagate or\nmodify any covered work.  These actions infringe copyright if you do\nnot accept this License.  Therefore, by modifying or propagating a\ncovered work, you indicate your acceptance of this License to do so.\n\n  10. Automatic Licensing of Downstream Recipients.\n\n  Each time you convey a covered work, the recipient automatically\nreceives a license from the original licensors, to run, modify and\npropagate that work, subject to this License.  You are not responsible\nfor enforcing compliance by third parties with this License.\n\n  An \"entity transaction\" is a transaction transferring control of an\norganization, or substantially all assets of one, or subdividing an\norganization, or merging organizations.  If propagation of a covered\nwork results from an entity transaction, each party to that\ntransaction who receives a copy of the work also receives whatever\nlicenses to the work the party's predecessor in interest had or could\ngive under the previous paragraph, plus a right to possession of the\nCorresponding Source of the work from the predecessor in interest, if\nthe predecessor has it or can get it with reasonable efforts.\n\n  You may not impose any further restrictions on the exercise of the\nrights granted or affirmed under this License.  For example, you may\nnot impose a license fee, royalty, or other charge for exercise of\nrights granted under this License, and you may not initiate litigation\n(including a cross-claim or counterclaim in a lawsuit) alleging that\nany patent claim is infringed by making, using, selling, offering for\nsale, or importing the Program or any portion of it.\n\n  11. Patents.\n\n  A \"contributor\" is a copyright holder who authorizes use under this\nLicense of the Program or a work on which the Program is based.  The\nwork thus licensed is called the contributor's \"contributor version\".\n\n  A contributor's \"essential patent claims\" are all patent claims\nowned or controlled by the contributor, whether already acquired or\nhereafter acquired, that would be infringed by some manner, permitted\nby this License, of making, using, or selling its contributor version,\nbut do not include claims that would be infringed only as a\nconsequence of further modification of the contributor version.  For\npurposes of this definition, \"control\" includes the right to grant\npatent sublicenses in a manner consistent with the requirements of\nthis License.\n\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\npatent license under the contributor's essential patent claims, to\nmake, use, sell, offer for sale, import and otherwise run, modify and\npropagate the contents of its contributor version.\n\n  In the following three paragraphs, a \"patent license\" is any express\nagreement or commitment, however denominated, not to enforce a patent\n(such as an express permission to practice a patent or covenant not to\nsue for patent infringement).  To \"grant\" such a patent license to a\nparty means to make such an agreement or commitment not to enforce a\npatent against the party.\n\n  If you convey a covered work, knowingly relying on a patent license,\nand the Corresponding Source of the work is not available for anyone\nto copy, free of charge and under the terms of this License, through a\npublicly available network server or other readily accessible means,\nthen you must either (1) cause the Corresponding Source to be so\navailable, or (2) arrange to deprive yourself of the benefit of the\npatent license for this particular work, or (3) arrange, in a manner\nconsistent with the requirements of this License, to extend the patent\nlicense to downstream recipients.  \"Knowingly relying\" means you have\nactual knowledge that, but for the patent license, your conveying the\ncovered work in a country, or your recipient's use of the covered work\nin a country, would infringe one or more identifiable patents in that\ncountry that you have reason to believe are valid.\n\n  If, pursuant to or in connection with a single transaction or\narrangement, you convey, or propagate by procuring conveyance of, a\ncovered work, and grant a patent license to some of the parties\nreceiving the covered work authorizing them to use, propagate, modify\nor convey a specific copy of the covered work, then the patent license\nyou grant is automatically extended to all recipients of the covered\nwork and works based on it.\n\n  A patent license is \"discriminatory\" if it does not include within\nthe scope of its coverage, prohibits the exercise of, or is\nconditioned on the non-exercise of one or more of the rights that are\nspecifically granted under this License.  You may not convey a covered\nwork if you are a party to an arrangement with a third party that is\nin the business of distributing software, under which you make payment\nto the third party based on the extent of your activity of conveying\nthe work, and under which the third party grants, to any of the\nparties who would receive the covered work from you, a discriminatory\npatent license (a) in connection with copies of the covered work\nconveyed by you (or copies made from those copies), or (b) primarily\nfor and in connection with specific products or compilations that\ncontain the covered work, unless you entered into that arrangement,\nor that patent license was granted, prior to 28 March 2007.\n\n  Nothing in this License shall be construed as excluding or limiting\nany implied license or other defenses to infringement that may\notherwise be available to you under applicable patent law.\n\n  12. No Surrender of Others' Freedom.\n\n  If conditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot convey a\ncovered work so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you may\nnot convey it at all.  For example, if you agree to terms that obligate you\nto collect a royalty for further conveying from those to whom you convey\nthe Program, the only way you could satisfy both those terms and this\nLicense would be to refrain entirely from conveying the Program.\n\n  13. Remote Network Interaction; Use with the GNU General Public License.\n\n  Notwithstanding any other provision of this License, if you modify the\nProgram, your modified version must prominently offer all users\ninteracting with it remotely through a computer network (if your version\nsupports such interaction) an opportunity to receive the Corresponding\nSource of your version by providing access to the Corresponding Source\nfrom a network server at no charge, through some standard or customary\nmeans of facilitating copying of software.  This Corresponding Source\nshall include the Corresponding Source for any work covered by version 3\nof the GNU General Public License that is incorporated pursuant to the\nfollowing paragraph.\n\n  Notwithstanding any other provision of this License, you have\npermission to link or combine any covered work with a work licensed\nunder version 3 of the GNU General Public License into a single\ncombined work, and to convey the resulting work.  The terms of this\nLicense will continue to apply to the part which is the covered work,\nbut the work with which it is combined will remain governed by version\n3 of the GNU General Public License.\n\n  14. Revised Versions of this License.\n\n  The Free Software Foundation may publish revised and/or new versions of\nthe GNU Affero General Public License from time to time.  Such new versions\nwill be similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\n  Each version is given a distinguishing version number.  If the\nProgram specifies that a certain numbered version of the GNU Affero General\nPublic License \"or any later version\" applies to it, you have the\noption of following the terms and conditions either of that numbered\nversion or of any later version published by the Free Software\nFoundation.  If the Program does not specify a version number of the\nGNU Affero General Public License, you may choose any version ever published\nby the Free Software Foundation.\n\n  If the Program specifies that a proxy can decide which future\nversions of the GNU Affero General Public License can be used, that proxy's\npublic statement of acceptance of a version permanently authorizes you\nto choose that version for the Program.\n\n  Later license versions may give you additional or different\npermissions.  However, no additional obligations are imposed on any\nauthor or copyright holder as a result of your choosing to follow a\nlater version.\n\n  15. Disclaimer of Warranty.\n\n  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\nAPPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\nOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\nIS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\nALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n  16. Limitation of Liability.\n\n  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGES.\n\n  17. Interpretation of Sections 15 and 16.\n\n  If the disclaimer of warranty and limitation of liability provided\nabove cannot be given local legal effect according to their terms,\nreviewing courts shall apply local law that most closely approximates\nan absolute waiver of all civil liability in connection with the\nProgram, unless a warranty or assumption of liability accompanies a\ncopy of the Program in return for a fee.\n\n                     END OF TERMS AND CONDITIONS\n\n            How to Apply These Terms to Your New Programs\n\n  If you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these terms.\n\n  To do so, attach the following notices to the program.  It is safest\nto attach them to the start of each source file to most effectively\nstate the exclusion of warranty; and each file should have at least\nthe \"copyright\" line and a pointer to where the full notice is found.\n\n    <one line to give the program's name and a brief idea of what it does.>\n    Copyright (C) <year>  <name of author>\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\nAlso add information on how to contact you by electronic and paper mail.\n\n  If your software can interact with users remotely through a computer\nnetwork, you should also make sure that it provides a way for users to\nget its source.  For example, if your program is a web application, its\ninterface could display a \"Source\" link that leads users to an archive\nof the code.  There are many ways you could offer source, and different\nsolutions will be better for different programs; see section 13 for the\nspecific requirements.\n\n  You should also get your employer (if you work as a programmer) or school,\nif any, to sign a \"copyright disclaimer\" for the program, if necessary.\nFor more information on this, and how to apply and follow the GNU AGPL, see\n<https://www.gnu.org/licenses/>.\n", "metadata": {"source": "github.com/pdoc3/pdoc", "file_path": "LICENSE.txt", "language": "text", "complexity": "complex", "star_count": 1174, "quality_score": 0.72, "features": {"has_code_examples": false, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 662, "word_count": 5535}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with installation and setup instructions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "[![](https://i.imgur.com/kQOtbBk.png)](https://pdoc3.github.io/pdoc/)\n\npdoc\n====\n[![Build Status](https://img.shields.io/github/actions/workflow/status/pdoc3/pdoc/ci.yml?branch=master&style=for-the-badge)](https://github.com/pdoc3/pdoc/actions)\n[![Code Coverage](https://img.shields.io/codecov/c/gh/pdoc3/pdoc.svg?style=for-the-badge&label=Covr)](https://codecov.io/gh/pdoc3/pdoc)\n[![Source lines of code](https://img.shields.io/endpoint?url=https%3A%2F%2Fghloc.vercel.app%2Fapi%2Fpdoc3%2Fpdoc%2Fbadge?filter=.py%26format=human&style=for-the-badge&label=SLOC&color=green)](https://ghloc.vercel.app/pdoc3/pdoc)\n[![pdoc3 on PyPI](https://img.shields.io/pypi/v/pdoc3.svg?color=blue&style=for-the-badge)](https://pypi.org/project/pdoc3)\n[![package downloads](https://img.shields.io/pypi/dd/pdoc3.svg?color=skyblue&label=D/L&style=for-the-badge)](https://pypistats.org/packages/pdoc3)\n[![total downloads](https://img.shields.io/pepy/dt/pdoc3?style=for-the-badge&label=%E2%88%91&color=skyblue)](https://pypistats.org/packages/pdoc3)\n[![GitHub Sponsors](https://img.shields.io/github/sponsors/kernc?color=pink&style=for-the-badge&label=%E2%99%A5)](https://github.com/sponsors/kernc)\n\nAuto-generate API documentation for Python 3+ projects.\n\n[**Project website**](https://pdoc3.github.io/pdoc/)\n\n[Documentation]\n\n[Documentation]: https://pdoc3.github.io/pdoc/doc/pdoc/\n\n\nInstallation\n------------\n\n    $ pip install pdoc3\n\n\nUsage\n-----\nPdoc will accept a Python module file, package directory or an import path.\n\n    $ pdoc your_project\n\nSee `pdoc --help` for more command-line switches and the [documentation]\nfor more usage examples.\n\n\nFeatures\n--------\n* Simple usage. Generate sensible API (+ prose) documentation without any\n  special configuration.\n* Support for common [docstrings formats] (Markdown, numpydoc, Google-style docstrings),\n  LaTeX math, and some [reST directives].\n* Support for [PEP 484] and [PEP 526] type annotations.\n* pdoc respects [`__all__`] when present.\n* Docstrings for overridden class members are [inherited] if unspecified.\n* Support for documenting module, class, and instance [_variables_] by traversing ASTs.\n* Automatic [cross-linking] of referenced identifiers.\n* Overriding docstrings generation with special module-level [`__pdoc__` dictionary].\n* Easily extended and [customized templates].\n* Built-in [development web server] for near-instant preview of rendered docstrings.\n\nThe above features are explained in more detail in pdoc's [documentation]\n(generated with pdoc).\n\n[docstrings formats]: https://pdoc3.github.io/pdoc/doc/pdoc/#supported-docstring-formats\n[reST directives]: https://pdoc3.github.io/pdoc/doc/pdoc/#supported-rest-directives\n[PEP 484]: https://www.python.org/dev/peps/pep-0484/\n[PEP 526]: https://www.python.org/dev/peps/pep-0526/\n[`__all__`]: https://pdoc3.github.io/pdoc/doc/pdoc/#what-objects-are-documented\n[inherited]: https://pdoc3.github.io/pdoc/doc/pdoc/#docstrings-inheritance\n[_variables_]: https://pdoc3.github.io/pdoc/doc/pdoc/#docstrings-for-variables\n[cross-linking]: https://pdoc3.github.io/pdoc/doc/pdoc/#linking-to-other-identifiers\n[`__pdoc__` dictionary]: https://pdoc3.github.io/pdoc/doc/pdoc/#overriding-docstrings-with-__pdoc__\n[customized templates]: https://pdoc3.github.io/pdoc/doc/pdoc/#custom-templates\n[development web server]: https://pdoc3.github.io/pdoc/doc/pdoc/#command-line-interface\n\n\nDevelopment\n-----------\nCheck [CONTRIBUTING.md](CONTRIBUTING.md) for hacking details.\n", "metadata": {"source": "github.com/pdoc3/pdoc", "file_path": "README.md", "language": "markdown", "complexity": "medium", "star_count": 1174, "quality_score": 0.69, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": true, "has_tables": false, "has_headers": true, "line_count": 72, "word_count": 222}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\n\"\"\"pdoc's CLI interface and helper functions.\"\"\"\n\nimport argparse\nimport ast\nimport importlib\nimport inspect\nimport os\nimport os.path as path\nimport json\nimport re\nimport subprocess\nimport sys\nimport warnings\nfrom contextlib import contextmanager\nfrom functools import lru_cache\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\nfrom pathlib import Path\nfrom typing import Dict, List, Sequence\nfrom warnings import warn\n\nimport pdoc\n\nparser = argparse.ArgumentParser(\n    description=\"Automatically generate API docs for Python modules.\",\n    epilog=\"Further documentation is available at <https://pdoc3.github.io/pdoc/doc>.\",\n)\naa = parser.add_argument\nmode_aa = parser.add_mutually_exclusive_group().add_argument\n\naa(\n    '--version', action='version', version=f'%(prog)s {pdoc.__version__}')\naa(\n    \"modules\",\n    type=str,\n    metavar='MODULE',\n    nargs=\"+\",\n    help=\"The Python module name. This may be an import path resolvable in \"\n    \"the current environment, or a file path to a Python module or \"\n    \"package.\",\n)\naa(\n    \"-c\", \"--config\",\n    type=str,\n    metavar='OPTION=VALUE',\n    action='append',\n    default=[],\n    help=\"Override template options. This is an alternative to using \"\n         \"a custom config.mako file in --template-dir. This option \"\n         \"can be specified multiple times.\",\n)\naa(\n    \"--filter\",\n    type=str,\n    metavar='STRING',\n    default=None,\n    help=\"Comma-separated list of filters. When specified, \"\n         \"only identifiers containing the specified string \"\n         \"will be shown in the output. Search is case sensitive. \"\n         \"Has no effect when --http is set.\",\n)\naa(\n    \"-f\", \"--force\",\n    action=\"store_true\",\n    help=\"Overwrite any existing generated (--output-dir) files.\",\n)\nmode_aa(\n    \"--html\",\n    action=\"store_true\",\n    help=\"When set, the output will be HTML formatted.\",\n)\nmode_aa(\n    \"--pdf\",\n    action=\"store_true\",\n    help=\"When set, the specified modules will be printed to standard output, \"\n         \"formatted in Markdown-Extra, compatible with most \"\n         \"Markdown-(to-HTML-)to-PDF converters.\",\n)\naa(\n    \"--html-dir\",\n    type=str,\n    help=argparse.SUPPRESS,\n)\naa(\n    \"-o\", \"--output-dir\",\n    type=str,\n    metavar='DIR',\n    help=\"The directory to output generated HTML/markdown files to \"\n         \"(default: ./html for --html).\",\n)\naa(\n    \"--html-no-source\",\n    action=\"store_true\",\n    help=argparse.SUPPRESS,\n)\naa(\n    \"--overwrite\",\n    action=\"store_true\",\n    help=argparse.SUPPRESS,\n)\naa(\n    \"--external-links\",\n    action=\"store_true\",\n    help=argparse.SUPPRESS,\n)\naa(\n    \"--template-dir\",\n    type=str,\n    metavar='DIR',\n    default=None,\n    help=\"Specify a directory containing Mako templates \"\n         \"(html.mako, text.mako, config.mako and/or any templates they include). \"\n         \"Alternatively, put your templates in $XDG_CONFIG_HOME/pdoc and \"\n         \"pdoc will automatically find them.\",\n)\naa(\n    \"--link-prefix\",\n    type=str,\n    help=argparse.SUPPRESS,\n)\naa(\n    \"--close-stdin\",\n    action=\"store_true\",\n    help=\"When set, stdin will be closed before importing, to account for \"\n         \"ill-behaved modules that block on stdin.\"\n)\n\nDEFAULT_HOST, DEFAULT_PORT = 'localhost', 8080\n\n\ndef _check_host_port(s):\n    if s and ':' not in s:\n        raise argparse.ArgumentTypeError(\n            f\"'{s}' doesn't match '[HOST]:[PORT]'. \"\n            \"Specify `--http :` to use default hostname and port.\")\n    return s\n\n\naa(\n    \"--http\",\n    default='',\n    type=_check_host_port,\n    metavar='HOST:PORT',\n    help=\"When set, pdoc will run as an HTTP server providing documentation \"\n         \"for specified modules. If you just want to use the default hostname \"\n         f\"and port ({DEFAULT_HOST}:{DEFAULT_PORT}), set the parameter to :.\",\n)\naa(\n    \"--skip-errors\",\n    action=\"store_true\",\n    help=\"Upon unimportable modules, warn instead of raising.\"\n)\n\nargs = argparse.Namespace()\n\n\nclass _WebDoc(BaseHTTPRequestHandler):\n    args = None  # Set before server instantiated\n    template_config = None\n\n    def do_HEAD(self):\n        status = 200\n        if self.path != \"/\":\n            status = self.check_modified()\n\n        self.send_response(status)\n        self.send_header(\"Content-type\", \"text/html; charset=utf-8\")\n        self.end_headers()\n\n    def check_modified(self):\n        try:\n            module = pdoc.import_module(self.import_path_from_req_url)\n            new_etag = str(os.stat(module.__file__).st_mtime)\n        except ImportError:\n            return 404\n\n        old_etag = self.headers.get('If-None-Match', new_etag)\n        if old_etag == new_etag:\n            # Don't log repeating checks\n            self.log_request = lambda *args, **kwargs: None\n            return 304\n\n        return 205\n\n    def do_GET(self):\n        # Deny favicon shortcut early.\n        if self.path == \"/favicon.ico\":\n            return None\n\n        importlib.invalidate_caches()\n        code = 200\n        if self.path == \"/\":\n            modules = [pdoc.import_module(module, reload=True)\n                       for module in self.args.modules]\n            modules = sorted((module.__name__, inspect.getdoc(module))\n                             for module in modules)\n            out = pdoc._render_template('/html.mako',\n                                        modules=modules,\n                                        **self.template_config)\n        elif self.path.endswith(\".ext\"):\n            # External links are a bit weird. You should view them as a giant\n            # hack. Basically, the idea is to \"guess\" where something lives\n            # when documenting another module and hope that guess can actually\n            # track something down in a more global context.\n            #\n            # The idea here is to start specific by looking for HTML that\n            # exists that matches the full external path given. Then trim off\n            # one component at the end and try again.\n            #\n            # If no HTML is found, then we ask `pdoc` to do its thang on the\n            # parent module in the external path. If all goes well, that\n            # module will then be able to find the external identifier.\n\n            import_path = self.path[:-4].lstrip(\"/\")\n            resolved = self.resolve_ext(import_path)\n            if resolved is None:  # Try to generate the HTML...\n                print(f\"Generating HTML for {import_path} on the fly...\", file=sys.stderr)\n                try:\n                    out = pdoc.html(import_path.split(\".\")[0], **self.template_config)\n                except Exception as e:\n                    print(f'Error generating docs: {e}', file=sys.stderr)\n                    # All hope is lost.\n                    code = 404\n                    out = f\"External identifier <code>{import_path}</code> not found.\"\n            else:\n                return self.redirect(resolved)\n        # Redirect '/pdoc' to '/pdoc/' so that relative links work\n        # (results in '/pdoc/cli.html' instead of 'cli.html')\n        elif not self.path.endswith(('/', '.html')):\n            return self.redirect(self.path + '/')\n        # Redirect '/pdoc/index.html' to '/pdoc/' so it's more pretty\n        elif self.path.endswith(pdoc._URL_PACKAGE_SUFFIX):\n            return self.redirect(self.path[:-len(pdoc._URL_PACKAGE_SUFFIX)] + '/')\n        else:\n            try:\n                out = self.html()\n            except Exception:\n                import traceback\n                from html import escape\n                code = 404\n                out = (\n                    \"Error importing module \"\n                    f\"<code>{self.import_path_from_req_url}</code>:\\n\\n\"\n                    f\"<pre>{escape(traceback.format_exc())}</pre>\"\n                )\n                out = out.replace('\\n', '<br>')\n\n        self.send_response(code)\n        self.send_header(\"Content-type\", \"text/html; charset=utf-8\")\n        self.end_headers()\n        self.echo(out)\n\n    def redirect(self, location):\n        self.send_response(302)\n        self.send_header(\"Location\", location)\n        self.end_headers()\n\n    def echo(self, s):\n        self.wfile.write(s.encode(\"utf-8\"))\n\n    def html(self):\n        \"\"\"\n        Retrieves and sends the HTML belonging to the path given in\n        URL. This method is smart and will look for HTML files already\n        generated and account for whether they are stale compared to\n        the source code.\n        \"\"\"\n        return pdoc.html(self.import_path_from_req_url,\n                         reload=True, http_server=True, external_links=True,\n                         skip_errors=self.args.skip_errors,\n                         **self.template_config)\n\n    def resolve_ext(self, import_path):\n        def exists(p):\n            p = path.join(args.output_dir, p)\n            pkg = path.join(p, pdoc._URL_PACKAGE_SUFFIX.lstrip('/'))\n            mod = p + pdoc._URL_MODULE_SUFFIX\n\n            if path.isfile(pkg):\n                return pkg[len(args.output_dir):]\n            elif path.isfile(mod):\n                return mod[len(args.output_dir):]\n            return None\n\n        parts = import_path.split(\".\")\n        for i in range(len(parts), 0, -1):\n            p = path.join(*parts[0:i])\n            realp = exists(p)\n            if realp is not None:\n                return f\"/{realp.lstrip('/')}#{import_path}\"\n        return None\n\n    @property\n    def import_path_from_req_url(self):\n        pth = self.path.split('#')[0].lstrip('/')\n        for suffix in ('/',\n                       pdoc._URL_PACKAGE_SUFFIX,\n                       pdoc._URL_INDEX_MODULE_SUFFIX,\n                       pdoc._URL_MODULE_SUFFIX):\n            if pth.endswith(suffix):\n                pth = pth[:-len(suffix)]\n                break\n        return pth.replace('/', '.')\n\n\ndef module_path(m: pdoc.Module, ext: str):\n    return path.join(args.output_dir, *re.sub(r'\\.html$', ext, m.url()).split('/'))\n\n\ndef _quit_if_exists(m: pdoc.Module, ext: str):\n    if args.force:\n        return\n\n    paths = [module_path(m, ext)]\n    if m.is_package:  # If package, make sure the dir doesn't exist either\n        paths.append(path.dirname(paths[0]))\n\n    for pth in paths:\n        if path.lexists(pth):\n            print(f\"File '{pth}' already exists. Delete it, or run with --force\", file=sys.stderr)\n            sys.exit(1)\n\n\n@contextmanager\ndef _open_write_file(filename):\n    try:\n        with open(filename, 'w', encoding='utf-8') as f:\n            yield f\n            print(filename)  # print created file path to stdout\n    except Exception:\n        try:\n            os.unlink(filename)\n        except Exception:\n            pass\n        raise\n\n\ndef recursive_write_files(m: pdoc.Module, ext: str, **kwargs):\n    assert ext in ('.html', '.md')\n    filepath = module_path(m, ext=ext)\n\n    dirpath = path.dirname(filepath)\n    if not os.access(dirpath, os.R_OK):\n        os.makedirs(dirpath)\n\n    with _open_write_file(filepath) as f:\n        if ext == '.html':\n            f.write(m.html(**kwargs))\n        elif ext == '.md':\n            f.write(m.text(**kwargs))\n\n    for submodule in m.submodules():\n        recursive_write_files(submodule, ext=ext, **kwargs)\n\n\ndef _flatten_submodules(modules: Sequence[pdoc.Module]):\n    for module in modules:\n        yield module\n        for submodule in module.submodules():\n            yield from _flatten_submodules((submodule,))\n\n\ndef _print_pdf(modules, **kwargs):\n    modules = list(_flatten_submodules(modules))\n    print(pdoc._render_template('/pdf.mako', modules=modules, **kwargs))\n\n\ndef _warn_deprecated(option, alternative='', use_config_mako=False):\n    msg = f'Program option `{option}` is deprecated.'\n    if alternative:\n        msg += f' Use `{alternative}`'\n        if use_config_mako:\n            msg += ' or override config.mako template'\n        msg += '.'\n    warn(msg, DeprecationWarning, stacklevel=2)\n\n\ndef _generate_lunr_search(modules: List[pdoc.Module],\n                          index_docstrings: bool,\n                          template_config: dict):\n    \"\"\"Generate index.js for search\"\"\"\n\n    def trim_docstring(docstring):\n        return re.sub(r'''\n            \\s+|                   # whitespace sequences\n            \\s+[-=~]{3,}\\s+|       # title underlines\n            ^[ \\t]*[`~]{3,}\\w*$|   # code blocks\n            \\s*[`#*]+\\s*|          # common markdown chars\n            \\s*([^\\w\\d_>])\\1\\s*|   # sequences of punct of the same kind\n            \\s*</?\\w*[^>]*>\\s*     # simple HTML tags\n        ''', ' ', docstring, flags=re.VERBOSE | re.MULTILINE)\n\n    def recursive_add_to_index(dobj):\n        info = {\n            'ref': dobj.refname,\n            'url': to_url_id(dobj.module),\n        }\n        if index_docstrings:\n            info['doc'] = trim_docstring(dobj.docstring)\n        if isinstance(dobj, pdoc.Function):\n            info['func'] = 1\n        nonlocal index\n        index.append(info)\n        for member_dobj in getattr(dobj, 'doc', {}).values():\n            recursive_add_to_index(member_dobj)\n\n    @lru_cache()\n    def to_url_id(module):\n        url = module.url()\n        if url not in url_cache:\n            url_cache[url] = len(url_cache)\n        return url_cache[url]\n\n    index: List[Dict] = []\n    url_cache: Dict[str, int] = {}\n    for top_module in modules:\n        recursive_add_to_index(top_module)\n    urls = sorted(url_cache.keys(), key=url_cache.__getitem__)\n\n    cmd = ['node', str(Path(__file__).with_name('build-index.js'))]\n    proc = None\n    try:\n        proc = subprocess.Popen(cmd, text=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE)\n        stdout, stderr = proc.communicate(json.dumps(index))\n        assert proc.poll() == 0, proc.poll()\n    except Exception as ex:\n        stderr = proc and proc.stderr and proc.stderr.read()  # type: ignore\n        warn(f'Prebuilding Lunr index with command `{\" \".join(cmd)}` failed with '\n             f'\"{ex.__class__.__name__}: {ex}\". Stderr: {stderr or \"\"!r}. '\n             f'Note, the search feature will still work, '\n             f'but may be slower (with the index rebuilt just before use). '\n             f'To prebuild an index in advance, ensure `node` is executable in the '\n             f'pdoc environment.', category=RuntimeWarning)\n        stdout = ('URLS=' + json.dumps(urls, indent=0, separators=(',', ':')) +\n                  ';\\nINDEX=' + json.dumps(index, indent=0, separators=(',', ':')))\n    else:\n        stdout = (f'let [INDEX, DOCS] = {stdout}; '\n                  f'let URLS={json.dumps(urls, indent=0, separators=(\",\", \":\"))}')\n    main_path = args.output_dir\n    index_path = Path(main_path).joinpath('index.js')\n    index_path.write_text(stdout)\n    print(str(index_path))\n\n    # Generate search.html\n    with _open_write_file(path.join(main_path, 'doc-search.html')) as f:\n        rendered_template = pdoc._render_template('/search.mako', **template_config)\n        f.write(rendered_template)\n\n\ndef main(_args=None):\n    \"\"\" Command-line entry point \"\"\"\n    global args\n    args = _args or parser.parse_args()\n\n    # If warnings not externally managed, show deprecation warnings\n    if not sys.warnoptions:\n        warnings.simplefilter(\"once\", DeprecationWarning)\n\n    if args.close_stdin:\n        sys.stdin.close()\n\n    if (args.html or args.http) and not args.output_dir:\n        args.output_dir = 'html'\n\n    if args.html_dir:\n        _warn_deprecated('--html-dir', '--output-dir')\n        args.output_dir = args.html_dir\n    if args.overwrite:\n        _warn_deprecated('--overwrite', '--force')\n        args.force = args.overwrite\n\n    template_config = {}\n    for config_str in args.config:\n        try:\n            key, value = config_str.split('=', 1)\n            value = ast.literal_eval(value)\n            template_config[key] = value\n        except Exception:\n            raise ValueError(\n                f'Error evaluating --config statement \"{config_str}\". '\n                'Make sure string values are quoted?'\n            )\n\n    if args.html_no_source:\n        _warn_deprecated('--html-no-source', '-c show_source_code=False', True)\n        template_config['show_source_code'] = False\n    if args.link_prefix:\n        _warn_deprecated('--link-prefix', '-c link_prefix=\"foo\"', True)\n        template_config['link_prefix'] = args.link_prefix\n    if args.external_links:\n        _warn_deprecated('--external-links')\n        template_config['external_links'] = True\n\n    if args.template_dir is not None:\n        if not path.isdir(args.template_dir):\n            print(f'Error: Template dir {args.template_dir!r} is not a directory', file=sys.stderr)\n            sys.exit(1)\n        pdoc.tpl_lookup.directories.insert(0, args.template_dir)\n\n    # Support loading modules specified as python paths relative to cwd\n    sys.path.append(os.getcwd())\n\n    # Virtual environment handling for pdoc script run from system site\n    try:\n        os.environ['VIRTUAL_ENV']\n    except KeyError:\n        pass  # pdoc was not invoked while in a virtual environment\n    else:\n        from glob import glob\n        from sysconfig import get_path\n        libdir = get_path(\"platlib\")\n        sys.path.append(libdir)\n        # Resolve egg-links from `setup.py develop` or `pip install -e`\n        # XXX: Welcome a more canonical approach\n        for pth in glob(path.join(libdir, '*.egg-link')):\n            try:\n                with open(pth) as f:\n                    sys.path.append(path.join(libdir, f.readline().rstrip()))\n            except IOError:\n                warn(f'Invalid egg-link in venv: {pth!r}')\n\n    if args.http:\n        template_config['link_prefix'] = \"/\"\n\n        # Run the HTTP server.\n        _WebDoc.args = args  # Pass params to HTTPServer xP\n        _WebDoc.template_config = template_config\n\n        host, _, port = args.http.partition(':')\n        host = host or DEFAULT_HOST\n        port = int(port or DEFAULT_PORT)\n\n        print(f'Starting pdoc server on {host}:{port}', file=sys.stderr)\n        httpd = HTTPServer((host, port), _WebDoc)\n        print(f\"pdoc server ready at http://{host}:{port}\", file=sys.stderr)\n\n        # Allow tests to perform `pdoc.cli._httpd.shutdown()`\n        global _httpd\n        _httpd = httpd\n\n        try:\n            httpd.serve_forever()\n        finally:\n            httpd.server_close()\n            sys.exit(0)\n\n    if args.filter and args.filter.strip():\n        def docfilter(obj, _filters=args.filter.strip().split(',')):\n            return any(f in obj.refname or\n                       isinstance(obj, pdoc.Class) and f in obj.doc\n                       for f in _filters)\n    else:\n        docfilter = None\n\n    modules = [pdoc.Module(module, docfilter=docfilter,\n                           skip_errors=args.skip_errors)\n               for module in args.modules]\n    pdoc.link_inheritance()\n\n    # Loading is done. Output stage ...\n    config = pdoc._get_config(**template_config)\n\n    # Load configured global markdown extensions\n    # XXX: This is hereby enabled only for CLI usage as for\n    #  API use I couldn't figure out where reliably to put it.\n    if config.get('md_extensions'):\n        from .html_helpers import _md\n        _kwargs = {'extensions': [], 'configs': {}}\n        _kwargs.update(config.get('md_extensions', {}))\n        _md.registerExtensions(**_kwargs)\n\n    if args.pdf:\n        _print_pdf(modules, **template_config)\n        import textwrap\n        PANDOC_CMD = textwrap.indent(_PANDOC_COMMAND, '    ')\n        print(f\"\"\"\nPDF-ready markdown written to standard output.\n                              ^^^^^^^^^^^^^^^\nConvert this file to PDF using e.g. Pandoc:\n\n{PANDOC_CMD}\n\nor using Python-Markdown and Chrome/Chromium/WkHtmlToPDF:\n\n    markdown_py --extension=meta         \\\\\n                --extension=abbr         \\\\\n                --extension=attr_list    \\\\\n                --extension=def_list     \\\\\n                --extension=fenced_code  \\\\\n                --extension=footnotes    \\\\\n                --extension=tables       \\\\\n                --extension=admonition   \\\\\n                --extension=smarty       \\\\\n                --extension=toc          \\\\\n                pdf.md > pdf.html\n\n    chromium --headless --disable-gpu --print-to-pdf=pdf.pdf pdf.html\n\n    wkhtmltopdf --encoding utf8 -s A4 --print-media-type pdf.html pdf.pdf\n\nor similar, at your own discretion.\"\"\",\n              file=sys.stderr)\n        sys.exit(0)\n\n    for module in modules:\n        if args.html:\n            _quit_if_exists(module, ext='.html')\n            recursive_write_files(module, ext='.html', **template_config)\n        elif args.output_dir:  # Generate text files\n            _quit_if_exists(module, ext='.md')\n            recursive_write_files(module, ext='.md', **template_config)\n        else:\n            sys.stdout.write(module.text(**template_config))\n            # Two blank lines between two modules' texts\n            sys.stdout.write(os.linesep * (1 + 2 * int(module != modules[-1])))\n\n    if args.html:\n        lunr_config = config.get('lunr_search')\n        if lunr_config is not None:\n            _generate_lunr_search(\n                modules, lunr_config.get(\"index_docstrings\", True), template_config)\n\n\n_PANDOC_COMMAND = '''\\\npandoc --metadata=title:\"MyProject Documentation\"               \\\\\n       --from=markdown+abbreviations+tex_math_single_backslash  \\\\\n       --pdf-engine=xelatex --variable=mainfont:\"DejaVu Sans\"   \\\\\n       --toc --toc-depth=4 --output=/tmp/pdoc.pdf  pdf.md\n'''\n\n\nif __name__ == \"__main__\":\n    main(parser.parse_args())\n", "metadata": {"source": "github.com/pdoc3/pdoc", "file_path": "pdoc/cli.py", "language": "python", "complexity": "complex", "star_count": 1174, "quality_score": 0.8999999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 635, "word_count": 1903}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "`pdoc` extracts documentation of:\n\n* modules (including submodules),\n* functions (including methods, properties, coroutines ...),\n* classes, and\n* variables (including globals, class variables, and instance variables).\n\nDocumentation is extracted from live objects' [docstrings]\nusing Python's `__doc__` attribute[^execution]. Documentation for\nvariables is found by examining objects' abstract syntax trees.\n\n[docstrings]: https://docs.python.org/3/glossary.html#term-docstring\n\n[^execution]:\n    Documented modules are _executed_ in order to provide `__doc__`\n    attributes. Any [non-fenced] global code in imported modules will\n    _affect the current runtime environment_.\n\n[non-fenced]: https://stackoverflow.com/questions/19578308/what-is-the-benefit-of-using-main-method-in-python/19578335#19578335\n\n\nWhat objects are documented?\n----------------------------\n[public-private]: #what-objects-are-documented\n`pdoc` only extracts _public API_ documentation.[^public]\nCode objects (modules, variables, functions, classes, methods) are considered\npublic in the modules where they are defined (vs. imported from somewhere else)\nas long as their _identifiers don't begin with an underscore_ ( \\_ ).[^private]\nIf a module defines [`__all__`][__all__], then only the identifiers contained\nin this list are considered public, regardless of where they were defined.\n\nThis can be fine-tuned through [`__pdoc__` dict][__pdoc__].\n\n[^public]:\n    Here, public API refers to the API that is made available\n    to your project end-users, not the public API e.g. of a\n    private class that can be reasonably extended elsewhere\n    by your project developers.\n\n[^private]:\n    Prefixing private, implementation-specific objects with\n    an underscore is [a common convention].\n\n[a common convention]: https://docs.python.org/3/tutorial/classes.html#private-variables\n\n[__all__]: https://docs.python.org/3/tutorial/modules.html#importing-from-a-package\n\n\nWhere does `pdoc` get documentation from?\n-----------------------------------------\nIn Python, objects like modules, functions, classes, and methods\nhave a special attribute `__doc__` which contains that object's\ndocumentation string ([docstring][docstrings]).\nFor example, the following code defines a function with a docstring\nand shows how to access its contents:\n\n    >>> def test():\n    ...     \"\"\"This is a docstring.\"\"\"\n    ...     pass\n    ...\n    >>> test.__doc__\n    'This is a docstring.'\n\nIt's pretty much the same with classes and modules.\nSee [PEP-257] for Python docstring conventions.\n\n[PEP-257]: https://www.python.org/dev/peps/pep-0257/\n\nThese docstrings are set as descriptions for each module, class,\nfunction, and method listed in the documentation produced by `pdoc`.\n\n`pdoc` extends the standard use of docstrings in Python in two\nimportant ways: by allowing methods to inherit docstrings, and\nby introducing syntax for docstrings for variables.\n\n\n### Docstrings inheritance\n[docstrings inheritance]: #docstrings-inheritance\n\n`pdoc` considers methods' docstrings inherited from superclass methods',\nfollowing the normal class inheritance patterns.\nConsider the following code example:\n\n```python-repl\n>>> class A:\n...     def test(self):\n...         \"\"\"Docstring for A.\"\"\"\n...         pass\n...\n>>> class B(A):\n...     def test(self):\n...         pass\n...\n>>> A.test.__doc__\n'Docstring for A.'\n>>> B.test.__doc__\nNone\n```\n\nIn Python, the docstring for `B.test` doesn't exist, even though a\ndocstring was defined for `A.test`.\nContrary, when `pdoc` generates documentation for code such as above,\nit will automatically attach the docstring for `A.test` to\n`B.test` if the latter doesn't define its own.\nIn the default HTML template, such inherited docstrings are greyed out.\n\n\n### Docstrings for variables\n[variable docstrings]: #docstrings-for-variables\n\nPython by itself [doesn't allow docstrings attached to variables][PEP-224].\nHowever, `pdoc` supports documenting module (or global)\nvariables, class variables, and object instance variables via\ntwo different mechanisms: [PEP-224] and `#:` doc-comments.\n\nFor example:\n\n[PEP-224]: http://www.python.org/dev/peps/pep-0224\n\n```python\nmodule_variable = 1\n\"\"\"PEP 224 docstring for module_variable.\"\"\"\n\nclass C:\n    #: Documentation comment for class_variable\n    #: spanning over three lines.\n    class_variable = 2  #: Assignment line is included.\n\n    def __init__(self):\n        #: Instance variable's doc-comment\n        self.variable = 3\n        \"\"\"But note, PEP 224 docstrings take precedence.\"\"\"\n```\n\nWhile the resulting variables have no `__doc__` attribute,\n`pdoc` compensates by reading the source code (when available)\nand parsing the syntax tree.\n\nBy convention, variables defined in a class' `__init__` method\nand attached to `self` are considered and documented as\n_instance_ variables.\n\nClass and instance variables can also [inherit docstrings][docstrings inheritance].\n\n\nOverriding docstrings with `__pdoc__`\n-------------------------------------\n[__pdoc__]: #overriding-docstrings-with-__pdoc__\nDocstrings for objects can be disabled, overridden, or whitelisted with a special\nmodule-level dictionary `__pdoc__`. The _keys_\nshould be string identifiers within the scope of the module or,\nalternatively, fully-qualified reference names. E.g. for instance\nvariable `self.variable` of class `C`, its module-level identifier is\n`'C.variable'`, and `some_package.module.C.variable` its refname.\n\nIf `__pdoc__[key] = False`, then `key` (and its members) will be\n**excluded from the documentation** of the module.\n\nConversely, if `__pdoc__[key] = True`, then `key` (and its public members) will be\n**included in the documentation** of the module. This can be used to\ninclude documentation of [private objects][public-private],\nincluding special functions such as `__call__`, which are ignored by default.\n\nAlternatively, the _values_ of `__pdoc__` can be the **overriding docstrings**.\nThis feature is useful when there's no feasible way of\nattaching a docstring to something. A good example is a\n[namedtuple](https://docs.python.org/3/library/collections.html#collections.namedtuple):\n\n```python\n__pdoc__ = {}\n\nTable = namedtuple('Table', ['types', 'names', 'rows'])\n__pdoc__['Table.types'] = 'Types for each column in the table.'\n__pdoc__['Table.names'] = 'The names of each column in the table.'\n__pdoc__['Table.rows'] = 'Lists corresponding to each row in the table.'\n```\n\n`pdoc` will then show `Table` as a class with documentation for the\n`types`, `names` and `rows` members.\n\n.. note::\n    The assignments to `__pdoc__` need to be placed where they'll be\n    executed when the module is imported. For example, at the top level\n    of a module or in the definition of a class.\n\n\nSupported docstring formats\n---------------------------\n[docstring-formats]: #supported-docstring-formats\nCurrently, pure Markdown (with [extensions]), [numpydoc],\nand [Google-style] docstrings formats are supported,\nalong with some [reST directives].\n\nAdditionally, if `latex_math` [template config][custom templates] option is enabled,\nLaTeX math syntax is supported when placed between\n[recognized delimiters]: `\\(...\\)` for inline equations and\n`\\[...\\]` or `$$...$$` for block equations. Note, you need to escape\nyour backslashes in Python docstrings (`\\\\(`, `\\\\frac{}{}`, ...)\nor, alternatively, use [raw string literals].\n\n*[reST]: reStructuredText\n[extensions]: https://python-markdown.github.io/extensions/#officially-supported-extensions\n[numpydoc]: https://numpydoc.readthedocs.io/\n[Google-style]: http://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings\n[recognized delimiters]: https://docs.mathjax.org/en/latest/input/tex/delimiters.html\n[raw string literals]: https://www.journaldev.com/23598/python-raw-string\n\n\n### Supported reST directives\n[reST directives]: #supported-rest-directives\n\nThe following reST directives should work:\n\n* specific and generic [admonitions] (attention, caution, danger,\n  error, hint, important, note, tip, warning, admonition),\n* [`.. image::`][image] or `.. figure::` (without options),\n* [`.. include::`][include], with support for the options:\n  `:start-line:`, `:end-line:`, `:start-after:` and `:end-before:`.\n* [`.. math::`][math]\n* `.. versionadded::`\n* `.. versionchanged::`\n* `.. deprecated::`\n* `.. todo::`\n\n[admonitions]: http://docutils.sourceforge.net/docs/ref/rst/directives.html#admonitions\n[image]: http://docutils.sourceforge.net/docs/ref/rst/directives.html#images\n[include]: http://docutils.sourceforge.net/docs/ref/rst/directives.html#including-an-external-document-fragment\n[math]: http://docutils.sourceforge.net/docs/ref/rst/directives.html#math\n\n\nLinking to other identifiers\n----------------------------\n[cross-linking]: #linking-to-other-identifiers\nIn your documentation, you may refer to other identifiers in\nyour modules. When exporting to HTML, linking is automatically\ndone whenever you surround an identifier with [backticks] ( \\` ).\nUnless within the current module,\nthe identifier name must be fully qualified, for example\n<code>\\`pdoc.Doc.docstring\\`</code> is correct (and will link to\n`pdoc.Doc.docstring`) while <code>\\`Doc.docstring\\`</code>\nonly works within `pdoc` module.\n\n[backticks]: https://en.wikipedia.org/wiki/Backtick\n\n\nCommand-line interface\n----------------------\n[cmd]: #command-line-interface\n`pdoc` includes a feature-rich \"binary\" program for producing\nHTML and plain text documentation of your modules.\nFor example, to produce HTML documentation of your whole package\nin subdirectory 'build' of the current directory, using the default\nHTML template, run:\n\n```shell\n$ pdoc --html --output-dir build my_package\n```\n\nIf you want to omit the source code preview, run:\n\n```shell\n$ pdoc --html --config show_source_code=False my_package\n```\n\nFind additional template configuration tunables in [custom templates]\nsection below.\n\nTo run a local HTTP server while developing your package or writing\ndocstrings for it, run:\n\n```shell\n$ pdoc --http : my_package\n```\n\nTo re-build documentation as part of your continuous integration (CI)\nbest practice, i.e. ensuring all reference links are correct and\nup-to-date, make warnings error loudly by settings the environment\nvariable [`PYTHONWARNINGS`][PYTHONWARNINGS] before running pdoc:\n\n```shell\n$ export PYTHONWARNINGS='error::UserWarning'\n```\n\n[PYTHONWARNINGS]: https://docs.python.org/3/using/cmdline.html#envvar-PYTHONWARNINGS\n\nFor brief usage instructions, type:\n\n```shell\n$ pdoc --help\n```\n\nEven more usage examples can be found in the [FAQ].\n\n[FAQ]: https://github.com/pdoc3/pdoc/issues?q=is%3Aissue+label%3Aquestion\n\n\nProgrammatic usage\n------------------\nThe main entry point is `pdoc.Module` which wraps a module object\nand recursively imports and wraps any submodules and their members.\n\nAfter all related modules are wrapped (related modules are those that\nshare the same `pdoc.Context`), you need to call\n`pdoc.link_inheritance` with the used `Context` instance to\nestablish class inheritance links.\n\nAfterwards, you can use `pdoc.Module.html` and `pdoc.Module.text`\nmethods to output documentation in the desired format.\nFor example:\n\n```python\nimport pdoc\n\nmodules = ['a', 'b']  # Public submodules are auto-imported\ncontext = pdoc.Context()\n\nmodules = [pdoc.Module(mod, context=context)\n           for mod in modules]\npdoc.link_inheritance(context)\n\ndef recursive_htmls(mod):\n    yield mod.name, mod.html()\n    for submod in mod.submodules():\n        yield from recursive_htmls(submod)\n\nfor mod in modules:\n    for module_name, html in recursive_htmls(mod):\n        ...  # Process\n```\n\nWhen documenting a single module, you might find\nfunctions `pdoc.html` and `pdoc.text` handy.\nFor importing arbitrary modules/files, use `pdoc.import_module`.\n\nAlternatively, use the [runnable script][cmd] included with this package.\n\n\nCustom templates\n----------------\n[custom templates]: #custom-templates\nTo override the built-in HTML/CSS and plain text templates, copy\nthe relevant templates from `pdoc/templates` directory into a directory\nof your choosing and edit them. When you run [pdoc command][cmd]\nafterwards, pass the directory path as a parameter to the\n`--template-dir` switch.\n\n.. tip::\n    If you find you only need to apply minor alterations to the HTML template,\n    see if you can do so by overriding just some of the following, placeholder\n    sub-templates:\n\n    * [_config.mako_]: Basic template configuration, affects the way templates\n      are rendered.\n    * _head.mako_: Included just before `</head>`. Best for adding resources and styles.\n    * _logo.mako_: Included at the very top of the navigation sidebar. Empty by default.\n    * _credits.mako_: Included in the footer, right before pdoc version string.\n\n    See [default template files] for reference.\n\n.. tip::\n   You can also alter individual [_config.mako_] preferences using the\n   `--config` command-line switch.\n\nIf working with `pdoc` programmatically, _prepend_ the directory with\nmodified templates into the `directories` list of the\n`pdoc.tpl_lookup` object.\n\n[_config.mako_]: https://github.com/pdoc3/pdoc/blob/master/pdoc/templates/config.mako\n[default template files]: https://github.com/pdoc3/pdoc/tree/master/pdoc/templates\n\n\nCompatibility\n-------------\n`pdoc` requires Python 3.7+.\nThe last version to support Python 2.x is [pdoc3 0.3.x].\n\n[pdoc3 0.3.x]: https://pypi.org/project/pdoc3/0.3.13/\n\n\nContributing\n------------\n`pdoc` is [on GitHub]. Bug reports and pull requests are welcome.\n\n[on GitHub]: https://github.com/pdoc3/pdoc\n\n\nLicense\n-------\n`pdoc` is licensed under the terms of GNU [AGPL-3.0]{: rel=license} or later,\nmeaning you can use it for any reasonable purpose and remain in\ncomplete ownership of all the documentation you produce,\nbut you are also encouraged to make sure any upgrades to `pdoc`\nitself find their way back to the community.\n\n[AGPL-3.0]: https://www.gnu.org/licenses/agpl-3.0.html\n", "metadata": {"source": "github.com/pdoc3/pdoc", "file_path": "pdoc/documentation.md", "language": "markdown", "complexity": "complex", "star_count": 1174, "quality_score": 0.8099999999999999, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 396, "word_count": 1626}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with installation and setup instructions\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nHelper functions for HTML output.\n\"\"\"\nimport inspect\nimport os\nimport re\nimport subprocess\nimport textwrap\nimport traceback\nfrom contextlib import contextmanager\nfrom functools import partial, lru_cache\nfrom typing import Callable, Match, Optional\nfrom warnings import warn\nimport xml.etree.ElementTree as etree\n\nimport markdown\nfrom markdown.inlinepatterns import InlineProcessor\nfrom markdown.util import AtomicString\n\nimport pdoc\n\n\n@lru_cache()\ndef minify_css(css: str,\n               _whitespace=partial(re.compile(r'\\s*([,{:;}])\\s*').sub, r'\\1'),\n               _comments=partial(re.compile(r'/\\*.*?\\*/', flags=re.DOTALL).sub, ''),\n               _trailing_semicolon=partial(re.compile(r';\\s*}').sub, '}')):\n    \"\"\"\n    Minify CSS by removing extraneous whitespace, comments, and trailing semicolons.\n    \"\"\"\n    return _trailing_semicolon(_whitespace(_comments(css))).strip()\n\n\ndef minify_html(html: str,\n                _minify=partial(\n                    re.compile(r'(.*?)(<pre\\b.*?</pre\\b\\s*>)|(.*)', re.IGNORECASE | re.DOTALL).sub,\n                    lambda m, _norm_space=partial(re.compile(r'\\s\\s+').sub, '\\n'): (\n                        _norm_space(m.group(1) or '') +\n                        (m.group(2) or '') +\n                        _norm_space(m.group(3) or '')))):\n    \"\"\"\n    Minify HTML by replacing all consecutive whitespace with a single space\n    (or newline) character, except inside `<pre>` tags.\n    \"\"\"\n    return _minify(html)\n\n\ndef glimpse(text: str, max_length=153, *, paragraph=True,\n            _split_paragraph=partial(re.compile(r'\\s*\\n\\s*\\n\\s*').split, maxsplit=1),\n            _trim_last_word=partial(re.compile(r'\\S+$').sub, ''),\n            _remove_titles=partial(re.compile(r'^(#+|-{4,}|={4,})', re.MULTILINE).sub, ' ')):\n    \"\"\"\n    Returns a short excerpt (e.g. first paragraph) of text.\n    If `paragraph` is True, the first paragraph will be returned,\n    but never longer than `max_length` characters.\n    \"\"\"\n    text = text.lstrip()\n    if paragraph:\n        text, *rest = _split_paragraph(text)\n        if rest:\n            text = text.rstrip('.')\n            text += ' ‚Ä¶'\n        text = _remove_titles(text).strip()\n\n    if len(text) > max_length:\n        text = _trim_last_word(text[:max_length - 2])\n        if not text.endswith('.') or not paragraph:\n            text = text.rstrip('. ') + ' ‚Ä¶'\n    return text\n\n\n_md = markdown.Markdown(\n    output_format='html',\n    extensions=[\n        \"markdown.extensions.abbr\",\n        \"markdown.extensions.admonition\",\n        \"markdown.extensions.attr_list\",\n        \"markdown.extensions.def_list\",\n        \"markdown.extensions.fenced_code\",\n        \"markdown.extensions.footnotes\",\n        \"markdown.extensions.tables\",\n        \"markdown.extensions.smarty\",\n        \"markdown.extensions.toc\",\n    ],\n    extension_configs={\n        \"markdown.extensions.smarty\": dict(\n            smart_dashes=True,\n            smart_ellipses=True,\n            smart_quotes=False,\n            smart_angled_quotes=False,\n        ),\n    },\n)\n\n\n@contextmanager\ndef _fenced_code_blocks_hidden(text):\n    def hide(text):\n        def replace(match):\n            orig = match.group()\n            new = f'@{hash(orig)}@'\n            hidden[new] = orig\n            return new\n\n        text = re.compile(r'^(?P<fence>```+|~~~+).*\\n'\n                          r'(?:.*\\n)*?'\n                          r'^(?P=fence)[ ]*(?!.)', re.MULTILINE).sub(replace, text)\n        return text\n\n    def unhide(text):\n        for k, v in hidden.items():\n            text = text.replace(k, v)\n        return text\n\n    hidden = {}\n    # Via a manager object (a list) so modifications can pass back and forth as result[0]\n    result = [hide(text)]\n    yield result\n    result[0] = unhide(result[0])\n\n\nclass _ToMarkdown:\n    \"\"\"\n    This class serves as a namespace for methods converting common\n    documentation formats into markdown our Python-Markdown with\n    addons can ingest.\n\n    If debugging regexs (I can't imagine why that would be necessary\n    ‚Äî they are all perfect!) an insta-preview tool such as RegEx101.com\n    will come in handy.\n    \"\"\"\n    @staticmethod\n    def _deflist(name, type, desc):\n        \"\"\"\n        Returns `name`, `type`, and `desc` formatted as a\n        Python-Markdown definition list entry. See also:\n        https://python-markdown.github.io/extensions/definition_lists/\n        \"\"\"\n        # Wrap any identifiers and string literals in parameter type spec\n        # in backticks while skipping common \"stopwords\" such as 'or', 'of',\n        # 'optional' ... See ¬ß4 Parameters:\n        # https://numpydoc.readthedocs.io/en/latest/format.html#sections\n        type_parts = re.split(r'( *(?: of | or |, *default(?:=|\\b)|, *optional\\b) *)', type or '')\n        type_parts[::2] = [f'`{s}`' if s else s\n                           for s in type_parts[::2]]\n        type = ''.join(type_parts)\n\n        desc = desc or '&nbsp;'\n        assert _ToMarkdown._is_indented_4_spaces(desc)\n        assert name or type\n        ret = \"\"\n        if name:\n            # NOTE: Triple-backtick argument names so we skip linkifying them\n            ret += f\"**```{name.replace(', ', '```**, **```')}```**\"\n        if type:\n            ret += f' :&ensp;{type}' if ret else type\n        ret += f'\\n:   {desc}\\n\\n'\n        return ret\n\n    @staticmethod\n    def _numpy_params(match):\n        \"\"\" Converts NumpyDoc parameter (etc.) sections into Markdown. \"\"\"\n        name, type, desc = match.group(\"name\", \"type\", \"desc\")\n        type = type or match.groupdict().get('just_type', None)\n        desc = desc.strip()\n        return _ToMarkdown._deflist(name, type, desc)\n\n    @staticmethod\n    def _numpy_seealso(match):\n        \"\"\"\n        Converts NumpyDoc \"See Also\" section either into referenced code,\n        optionally within a definition list.\n        \"\"\"\n        spec_with_desc, simple_list = match.groups()\n        if spec_with_desc:\n            spec_desc_strings = []\n            for line in filter(None, spec_with_desc.split('\\n')):\n                spec, desc = map(str.strip, line.split(':', 1))\n                spec_desc_strings.append(f'`{spec}`\\n:   {desc}')\n            return '\\n\\n'.join(spec_desc_strings)\n        return ', '.join(f'`{i}`' for i in simple_list.split(', '))\n\n    @staticmethod\n    def _numpy_sections(match):\n        \"\"\"\n        Convert sections with parameter, return, and see also lists to Markdown\n        lists.\n        \"\"\"\n        section, body = match.groups()\n        section = section.title()\n        if section == 'See Also':\n            body = re.sub(r'\\n\\s{4}\\s*', ' ', body)  # Handle line continuation\n            body = re.sub(r'^((?:\\n?[\\w.]* ?: .*)+)|(.*\\w.*)',\n                          _ToMarkdown._numpy_seealso, body)\n        elif section in ('Returns', 'Yields', 'Raises', 'Warns'):\n            body = re.sub(r'^(?:(?P<name>\\*{0,2}\\w+(?:, \\*{0,2}\\w+)*)'\n                          r'(?: ?: (?P<type>.*))|'\n                          r'(?P<just_type>\\w[^\\n`*]*))(?<!\\.)$'\n                          r'(?P<desc>(?:\\n(?: {4}.*|$))*)',\n                          _ToMarkdown._numpy_params, body, flags=re.MULTILINE)\n        elif section in ('Parameters', 'Receives', 'Other Parameters',\n                         'Arguments', 'Args', 'Attributes'):\n            name = r'(?:\\w|\\{\\w+(?:,\\w+)+\\})+'  # Support curly brace expansion\n            body = re.sub(r'^(?P<name>\\*{0,2}' + name + r'(?:, \\*{0,2}' + name + r')*)'\n                          r'(?: ?: (?P<type>.*))?(?<!\\.)$'\n                          r'(?P<desc>(?:\\n(?: {4}.*|$))*)',\n                          _ToMarkdown._numpy_params, body, flags=re.MULTILINE)\n        return f'{section}\\n-----\\n{body}'\n\n    @staticmethod\n    def numpy(text):\n        \"\"\"\n        Convert `text` in numpydoc docstring format to Markdown\n        to be further converted later.\n        \"\"\"\n        return re.sub(r'^(\\w[\\w ]+)\\n-{3,}\\n'\n                      r'((?:(?!.+\\n-+).*$\\n?)*)',\n                      _ToMarkdown._numpy_sections, text, flags=re.MULTILINE)\n\n    @staticmethod\n    def _is_indented_4_spaces(txt, _3_spaces_or_less=re.compile(r'\\n\\s{0,3}\\S').search):\n        return '\\n' not in txt or not _3_spaces_or_less(txt)\n\n    @staticmethod\n    def _fix_indent(name, type, desc):\n        \"\"\"Maybe fix indent from 2 to 4 spaces.\"\"\"\n        if not _ToMarkdown._is_indented_4_spaces(desc):\n            desc = desc.replace('\\n', '\\n  ')\n        return name, type, desc\n\n    @staticmethod\n    def indent(indent, text, *, clean_first=False):\n        if clean_first:\n            text = inspect.cleandoc(text)\n        return re.sub(r'\\n', f'\\n{indent}', indent + text.rstrip())\n\n    @staticmethod\n    def google(text):\n        \"\"\"\n        Convert `text` in Google-style docstring format to Markdown\n        to be further converted later.\n        \"\"\"\n        def googledoc_sections(match):\n            section, body = match.groups('')\n            if not body:\n                return match.group()\n            body = textwrap.dedent(body)\n            section = section.title()\n            if section in ('Args', 'Attributes'):\n                body = re.compile(\n                    r'^([\\w*]+)(?: \\(([\\w.,=|\\[\\] -]+)\\))?: '\n                    r'((?:.*)(?:\\n(?: {2,}.*|$))*)', re.MULTILINE).sub(\n                    lambda m: _ToMarkdown._deflist(*_ToMarkdown._fix_indent(*m.groups())),\n                    inspect.cleandoc(f'\\n{body}')\n                )\n            elif section in ('Returns', 'Yields', 'Raises', 'Warns'):\n                body = re.compile(\n                    r'^()([\\w.,|\\[\\] ]+): '\n                    r'((?:.*)(?:\\n(?: {2,}.*|$))*)', re.MULTILINE).sub(\n                    lambda m: _ToMarkdown._deflist(*_ToMarkdown._fix_indent(*m.groups())),\n                    inspect.cleandoc(f'\\n{body}')\n                )\n            # Convert into markdown sections. End underlines with '='\n            # to avoid matching and re-processing as Numpy sections.\n            return f'\\n{section}\\n-----=\\n{body}'\n\n        text = re.compile(r'^([A-Z]\\w+):$\\n'\n                          r'((?:\\n?(?: {2,}.*|$))+)', re.MULTILINE).sub(googledoc_sections, text)\n        return text\n\n    @staticmethod\n    def _admonition(match, module=None, limit_types=None):\n        indent, type, value, text = match.groups()\n\n        if limit_types and type not in limit_types:\n            return match.group(0)\n\n        if text is None:\n            text = \"\"\n\n        if type == 'include' and module:\n            try:\n                return _ToMarkdown._include_file(indent, value,\n                                                 _ToMarkdown._directive_opts(text), module)\n            except Exception as e:\n                raise RuntimeError(f'`.. include:: {value}` error in module {module.name!r}: {e}')\n        if type in ('image', 'figure'):\n            alt_text = text.translate(str.maketrans({\n                '\\n': ' ',\n                '[': '\\\\[',\n                ']': '\\\\]'})).strip()\n            return f'{indent}![{alt_text}]({value}){{: loading=lazy}}\\n'\n        if type == 'math':\n            return _ToMarkdown.indent(indent,\n                                      f'\\\\[ {text.strip()} \\\\]',\n                                      clean_first=True)\n\n        if type == 'versionchanged':\n            title = f'Changed in version:&ensp;{value}'\n        elif type == 'versionadded':\n            title = f'Added in version:&ensp;{value}'\n        elif type == 'deprecated' and value:\n            title = f'Deprecated since version:&ensp;{value}'\n        elif type == 'admonition':\n            title = value\n        elif type.lower() == 'todo':\n            title = 'TODO'\n            text = f'{value} {text}'\n        else:\n            title = type.capitalize()\n            if value:\n                title += f':&ensp;{value}'\n\n        text = _ToMarkdown.indent(indent + '    ', text, clean_first=True)\n        return f'{indent}!!! {type} \"{title}\"\\n{text}\\n'\n\n    @staticmethod\n    def admonitions(text, module, limit_types=None):\n        \"\"\"\n        Process reStructuredText's block directives such as\n        `.. warning::`, `.. deprecated::`, `.. versionadded::`, etc.\n        and turn them into Python-M>arkdown admonitions.\n\n        `limit_types` is optionally a set of directives to limit processing to.\n\n        See: https://python-markdown.github.io/extensions/admonition/\n        \"\"\"\n        substitute = partial(re.compile(r'^(?P<indent> *)\\.\\. ?(\\w+)::(?: *(.*))?'\n                                        r'((?:\\n(?:(?P=indent) +.*| *$))*[^\\r\\n])*',\n                                        re.MULTILINE).sub,\n                             partial(_ToMarkdown._admonition, module=module,\n                                     limit_types=limit_types))\n        # Apply twice for nested (e.g. image inside warning)\n        return substitute(substitute(text))\n\n    @staticmethod\n    def _include_file(indent: str, path: str, options: dict, module: pdoc.Module) -> str:\n        start_line = int(options.get('start-line', 0))\n        end_line = int(options.get('end-line', 0)) or None\n        start_after = options.get('start-after')\n        end_before = options.get('end-before')\n\n        with open(os.path.normpath(os.path.join(os.path.dirname(module.obj.__file__), path)),\n                  encoding='utf-8') as f:\n            text = ''.join(list(f)[start_line:end_line])\n\n        if start_after:\n            text = text[text.index(start_after) + len(start_after):]\n        if end_before:\n            text = text[:text.index(end_before)]\n\n        return _ToMarkdown.indent(indent, text)\n\n    @staticmethod\n    def _directive_opts(text: str) -> dict:\n        return dict(re.findall(r'^ *:([^:]+): *(.*)', text, re.MULTILINE))\n\n    DOCTESTS_RE = re.compile(r'^(?:>>> .*)(?:\\n.+)*', re.MULTILINE)\n\n    @staticmethod\n    def doctests(text):\n        \"\"\"\n        Fence non-fenced (`~~~`) top-level (0-indented)\n        doctest blocks so they render as Python code.\n        \"\"\"\n        text = _ToMarkdown.DOCTESTS_RE.sub(\n            lambda match: f'```python-repl\\n{match.group()}\\n```\\n', text)\n        return text\n\n    @staticmethod\n    def raw_urls(text):\n        \"\"\"Wrap URLs in Python-Markdown-compatible <angle brackets>.\"\"\"\n        pattern = re.compile(r\"\"\"\n            (?P<code_span>                   # matches whole code span\n                (?<!`)(?P<fence>`+)(?!`)     # a string of backticks\n                .*?\n                (?<!`)(?P=fence)(?!`))\n            |\n            (?P<markdown_link>\\[.*?\\]\\(.*\\))  # matches whole inline link\n            |\n            (?<![<\\\"\\'])                     # does not start with <, \", '\n            (?P<url>(?:http|ftp)s?://        # url with protocol\n                [^>\\s()]+                    # url part before any (, )\n                (?:\\([^>\\s)]*\\))*            # optionally url part within parentheses\n                [^>\\s)]*                     # url part after any )\n            )\"\"\", re.VERBOSE)\n\n        text = pattern.sub(\n            lambda m: (f'<{m.group(\"url\")}>') if m.group('url') else m.group(), text)\n        return text\n\n\nclass _MathPattern(InlineProcessor):\n    NAME = 'pdoc-math'\n    PATTERN = r'(?<!\\S|\\\\)(?:\\\\\\((.+?)\\\\\\)|\\\\\\[(.+?)\\\\\\]|\\$\\$(.+?)\\$\\$)'\n    PRIORITY = 181  # Larger than that of 'escape' pattern\n\n    def handleMatch(self, m, data):\n        for value, is_block in zip(m.groups(), (False, True, True)):\n            if value:\n                break\n        script = etree.Element('script', type=f\"math/tex{'; mode=display' if is_block else ''}\")\n        preview = etree.Element('span', {'class': 'MathJax_Preview'})\n        preview.text = script.text = AtomicString(value)\n        wrapper = etree.Element('span')\n        wrapper.extend([preview, script])\n        return wrapper, m.start(0), m.end(0)\n\n\ndef to_html(text: str, *,\n            docformat: Optional[str] = None,\n            module: Optional[pdoc.Module] = None,\n            link: Optional[Callable[..., str]] = None,\n            latex_math: bool = False):\n    \"\"\"\n    Returns HTML of `text` interpreted as `docformat`. `__docformat__` is respected\n    if present, otherwise Numpydoc and Google-style docstrings are assumed,\n    as well as pure Markdown.\n\n    `module` should be the documented module (so the references can be\n    resolved) and `link` is the hyperlinking function like the one in the\n    example template.\n    \"\"\"\n    # Optionally register our math syntax processor\n    if not latex_math and _MathPattern.NAME in _md.inlinePatterns:\n        _md.inlinePatterns.deregister(_MathPattern.NAME)\n    elif latex_math and _MathPattern.NAME not in _md.inlinePatterns:\n        _md.inlinePatterns.register(_MathPattern(_MathPattern.PATTERN),\n                                    _MathPattern.NAME,\n                                    _MathPattern.PRIORITY)\n\n    md = to_markdown(text, docformat=docformat, module=module, link=link)\n    return _md.reset().convert(md)\n\n\ndef to_markdown(text: str, *,\n                docformat: Optional[str] = None,\n                module: Optional[pdoc.Module] = None,\n                link: Optional[Callable[..., str]] = None):\n    \"\"\"\n    Returns `text`, assumed to be a docstring in `docformat`, converted to markdown.\n    `__docformat__` is respected\n    if present, otherwise Numpydoc and Google-style docstrings are assumed,\n    as well as pure Markdown.\n\n    `module` should be the documented module (so the references can be\n    resolved) and `link` is the hyperlinking function like the one in the\n    example template.\n    \"\"\"\n    if not docformat:\n        docformat = str(getattr(getattr(module, 'obj', None), '__docformat__', 'numpy,google '))\n        docformat, *_ = docformat.lower().split()\n    if not (set(docformat.split(',')) & {'', 'numpy', 'google'}):\n        warn(f'__docformat__ value {docformat!r} in module {module!r} not supported. '\n             'Supported values are: numpy, google.')\n        docformat = 'numpy,google'\n\n    with _fenced_code_blocks_hidden(text) as result:\n        text = result[0]\n\n        text = _ToMarkdown.admonitions(text, module)\n\n        if 'google' in docformat:\n            text = _ToMarkdown.google(text)\n\n        text = _ToMarkdown.doctests(text)\n        text = _ToMarkdown.raw_urls(text)\n\n        # If doing both, do numpy after google, otherwise google-style's\n        # headings are incorrectly interpreted as numpy params\n        if 'numpy' in docformat:\n            text = _ToMarkdown.numpy(text)\n\n        if module and link:\n            # Hyperlink markdown code spans not within markdown hyperlinks.\n            # E.g. `code` yes, but not [`code`](...). RE adapted from:\n            # https://github.com/Python-Markdown/markdown/blob/ada40c66/markdown/inlinepatterns.py#L106\n            # Also avoid linking triple-backticked arg names in deflists.\n            linkify = partial(_linkify, link=link, module=module, wrap_code=True)\n            text = re.sub(r'(?P<inside_link>\\[[^\\]]*?)?'\n                          r'(?:(?<!\\\\)(?:\\\\{2})+(?=`)|(?<!\\\\)(?P<fence>`+)'\n                          r'(?P<code>.+?)(?<!`)'\n                          r'(?P=fence)(?!`))',\n                          lambda m: (m.group()\n                                     if m.group('inside_link') or len(m.group('fence')) > 2\n                                     else linkify(m)), text)\n        result[0] = text\n    text = result[0]\n\n    return text\n\n\nclass ReferenceWarning(UserWarning):\n    \"\"\"\n    This warning is raised in `to_html` when a object reference in markdown\n    doesn't match any documented objects.\n\n    Look for this warning to catch typos / references to obsolete symbols.\n    \"\"\"\n\n\ndef _linkify(match: Match, *, link: Callable[..., str], module: pdoc.Module, wrap_code=False):\n    try:\n        code_span = match.group('code')\n    except IndexError:\n        code_span = match.group()\n\n    is_type_annotation = re.match(r'^[`\\w\\s.,\\[\\]()]+$', code_span)\n    if not is_type_annotation:\n        return match.group()\n\n    def handle_refname(match):\n        nonlocal link, module\n        refname = match.group()\n        dobj = module.find_ident(refname)\n        if isinstance(dobj, pdoc.External):\n            # If this is a single-word reference,\n            # most likely an argument name. Skip linking External.\n            if '.' not in refname:\n                return refname\n            # If refname in documentation has a typo or is obsolete, warn.\n            # XXX: Assume at least the first part of refname, i.e. the package, is correct.\n            module_part = module.find_ident(refname.split('.')[0])\n            if not isinstance(module_part, pdoc.External):\n                warn(f'Code reference `{refname}` in module \"{module.refname}\" does not match any '\n                     'documented object.',\n                     ReferenceWarning, stacklevel=3)\n        return link(dobj)\n\n    if wrap_code:\n        code_span = code_span.replace('[', '\\\\[')\n    linked = re.sub(r'[a-zA-Z_]\\w*(?:\\.[a-zA-Z_]\\w*)*(?:\\(\\))?', handle_refname, code_span)\n    if wrap_code:\n        # Wrapping in HTML <code> as opposed to backticks evaluates markdown */_ markers,\n        # so let's escape them in text (but not in HTML tag attributes).\n        # Backticks also cannot be used because html returned from `link()`\n        # would then become escaped.\n        # This finds overlapping matches, https://stackoverflow.com/a/5616910/1090455\n        cleaned = re.sub(r'(_(?=[^>]*?(?:<|$)))', r'\\\\\\1', linked)\n        return f'<code>{cleaned}</code>'\n    return linked\n\n\ndef extract_toc(text: str):\n    \"\"\"\n    Returns HTML Table of Contents containing markdown titles in `text`.\n    \"\"\"\n    with _fenced_code_blocks_hidden(text) as result:\n        result[0] = _ToMarkdown.DOCTESTS_RE.sub('', result[0])\n    text = result[0]\n    toc, _ = _md.reset().convert(f'[TOC]\\n\\n@CUT@\\n\\n{text}').split('@CUT@', 1)\n    if toc.endswith('<p>'):  # CUT was put into its own paragraph\n        toc = toc[:-3].rstrip()\n    return toc\n\n\ndef format_git_link(template: str, dobj: pdoc.Doc):\n    \"\"\"\n    Interpolate `template` as a formatted string literal using values extracted\n    from `dobj` and the working environment.\n    \"\"\"\n    if not template:\n        return None\n    try:\n        if 'commit' in _str_template_fields(template):\n            commit = _git_head_commit()\n        obj = pdoc._unwrap_descriptor(dobj)\n        abs_path = inspect.getfile(inspect.unwrap(obj))\n        path = _project_relative_path(abs_path)\n\n        # Urls should always use / instead of \\\\\n        if os.name == 'nt':\n            path = path.replace('\\\\', '/')\n\n        lines, start_line = inspect.getsourcelines(obj)\n        start_line = start_line or 1  # GH-296\n        end_line = start_line + len(lines) - 1\n        url = template.format(**locals())\n        return url\n    except Exception:\n        warn(f'format_git_link for {obj} failed:\\n{traceback.format_exc()}')\n        return None\n\n\n@lru_cache()\ndef _git_head_commit():\n    \"\"\"\n    If the working directory is part of a git repository, return the\n    head git commit hash. Otherwise, raise a CalledProcessError.\n    \"\"\"\n    process_args = ['git', 'rev-parse', 'HEAD']\n    try:\n        commit = subprocess.check_output(process_args, universal_newlines=True).strip()\n        return commit\n    except OSError as error:\n        warn(f\"git executable not found on system:\\n{error}\")\n    except subprocess.CalledProcessError as error:\n        warn(\n            \"Ensure pdoc is run within a git repository.\\n\"\n            f\"`{' '.join(process_args)}` failed with output:\\n{error.output}\"\n        )\n    return None\n\n\n@lru_cache()\ndef _git_project_root():\n    \"\"\"\n    Return the path to project root directory or None if indeterminate.\n    \"\"\"\n    for cmd in (['git', 'rev-parse', '--show-superproject-working-tree'],\n                ['git', 'rev-parse', '--show-toplevel']):\n        try:\n            path = subprocess.check_output(cmd, universal_newlines=True).rstrip('\\r\\n')\n            if path:\n                return os.path.normpath(path)\n        except (subprocess.CalledProcessError, OSError):\n            pass\n    return None\n\n\n@lru_cache()\ndef _project_relative_path(absolute_path):\n    \"\"\"\n    Convert an absolute path of a python source file to a project-relative path.\n    Assumes the project's path is either the current working directory or\n    Python library installation.\n    \"\"\"\n    from sysconfig import get_path\n    paths = filter(None, (\n        _git_project_root() or os.getcwd(),\n        get_path(\"stdlib\"),\n        get_path(\"platlib\"),\n    ))\n    for prefix_path in map(os.path.realpath, paths):\n        common_path = os.path.commonpath([prefix_path, absolute_path])\n        if os.path.samefile(common_path, prefix_path):\n            # absolute_path is a descendant of prefix_path\n            return os.path.relpath(absolute_path, prefix_path)\n    raise RuntimeError(\n        f\"absolute path {absolute_path!r} is not a descendant of the current working directory \"\n        f\"or of the system's python library {paths}.\"\n    )\n\n\n@lru_cache()\ndef _str_template_fields(template):\n    \"\"\"\n    Return a list of `str.format` field names in a template string.\n    \"\"\"\n    from string import Formatter\n    return [\n        field_name\n        for _, field_name, _, _ in Formatter().parse(template)\n        if field_name is not None\n    ]\n", "metadata": {"source": "github.com/pdoc3/pdoc", "file_path": "pdoc/html_helpers.py", "language": "python", "complexity": "complex", "star_count": 1174, "quality_score": 0.84, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 656, "word_count": 2233}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Module docstring\"\"\"\nfrom collections import namedtuple\nimport subprocess\nimport os\nfrom unittest.mock import AsyncMock, Mock\n\nCONST = 'const'\n\"\"\"CONST docstring\"\"\"\n\nvar = 2\n\"\"\"var docstring\"\"\"\n\n# https://github.com/mitmproxy/pdoc/pull/44\nforeign_var = subprocess.CalledProcessError(0, '')\n\"\"\"foreign var docstring\"\"\"\n\n__pdoc__ = {}\n\n\ndef foo(env=os.environ):\n    \"\"\"Doesn't leak environ\"\"\"\n\n\ndef object_as_arg_default(*args, a=object(), **kwargs):\n    \"\"\"Html-encodes angle brackets in params\"\"\"\n\n\ndef _private_function():\n    \"\"\"Private function, should only appear if whitelisted\"\"\"\n\n\nclass A:\n    \"\"\"`A` is base class for `example_pkg.B`.\"\"\"  # Test refname link\n    def overridden(self):\n        \"\"\"A.overridden docstring\"\"\"\n\n    def overridden_same_docstring(self):\n        \"\"\"A.overridden_same_docstring docstring\"\"\"\n\n    def inherited(self):  # Inherited in B\n        \"\"\"A.inherited docstring\"\"\"\n\n    def __call__(self):\n        \"\"\"A.__call__ docstring. Only shown when whitelisted\"\"\"\n\n\nnon_callable_routine = staticmethod(lambda x: 2)  # Not interpreted as Function; skipped\n\n\nclass ReadOnlyValueDescriptor:\n    \"\"\"Read-only value descriptor\"\"\"\n\n    def __get__(self, instance, instance_type=None):\n        if instance is not None:\n            return instance.var ** 2\n        return self\n\n\nclass B(A, int):\n    \"\"\"\n    B docstring\n\n    External refs: `sys.version`, `sys`\n    \"\"\"\n\n    CONST = 2\n    \"\"\"B.CONST docstring\"\"\"\n\n    var = 3\n    \"\"\"B.var docstring\"\"\"\n\n    ro_value_descriptor = ReadOnlyValueDescriptor()\n    \"\"\"ro_value_descriptor docstring\"\"\"\n\n    ro_value_descriptor_no_doc = ReadOnlyValueDescriptor()   # no doc-string\n\n    def __init__(self, x, y, z, w):\n        \"\"\"`__init__` docstring\"\"\"\n        self.instance_var = None\n        \"\"\"instance var docstring\"\"\"\n\n        self._private_instance_var = None\n        \"\"\"This should be private (hidden) despite PEP 224 docstring\"\"\"\n\n    def f(self, a: int, b: int = 1, *args, c: str = 'c', **kwargs):\n        \"\"\"B.f docstring\"\"\"\n\n    @staticmethod\n    def static(x):\n        \"\"\"B.static docstring\"\"\"\n\n    @classmethod\n    def cls(cls):\n        \"\"\"B.cls docstring\"\"\"\n\n    def _private(self):\n        \"\"\"B._private docstring\"\"\"\n\n    @staticmethod\n    def _private_static():\n        \"\"\"B._private_static docstring\"\"\"\n\n    @classmethod\n    def _private_cls(cls):\n        \"\"\"B._private_cls docstring\"\"\"\n\n    @property\n    def p(self):\n        \"\"\"B.p docstring\"\"\"\n        return 1\n\n    class C:\n        \"\"\"B.C docstring\"\"\"\n        def f(self):\n            \"\"\"B.C.f docstring\"\"\"\n\n    class _Private:\n        \"\"\"B._Private docstring\"\"\"\n        def f(self):\n            \"\"\"B._Private.f docstring\"\"\"\n\n    def overridden(self):\n        pass\n\n    assert overridden.__doc__ is None\n    __pdoc__['B.overridden'] = 'B.overridden docstring'\n\n    def overridden_same_docstring(self):\n        pass\n\n\nclass C(B): pass  # noqa: E701, E302\nclass D(C): pass  # noqa: E701, E302\n\n\nclass Hidden:\n    __pdoc__['Hidden'] = False\n\n\nclass Docformats:\n    def numpy(self):\n        \"\"\"\n        Summary line.\n\n        **Documentation**: https://pdoc3.github.io/pdoc/doc/pdoc/\n        **Source Code**: https://github.com/pdoc3/\n\n        Parameters\n        ----------\n        x1, x2 : array_like\n            Input arrays,\n            description of `x1`, `x2`.\n\n            .. versionadded:: 1.5.0\n        x : { NoneType, 'B', 'C' }, optional\n        n : int or list of int\n            Description of num\n        *args, **kwargs\n            Passed on.\n        complex : Union[Set[pdoc.Doc, Function], pdoc]\n            The `List[Doc]`s of the new signal.\n\n        Returns\n        -------\n        output : pdoc.Doc\n            The output array\n        List[pdoc.Doc]\n            The output array\n        foo\n\n        Raises\n        ------\n        TypeError\n            When something.\n\n        Raises\n        ------\n        TypeError\n\n        Returns\n        -------\n        None.\n\n        Invalid\n        -------\n        no match\n\n        See Also\n        --------\n        fromstring, loadtxt\n\n        See Also\n        --------\n        pdoc.text : Function a with its description.\n        scipy.random.norm : Random variates, PDFs, etc.\n        pdoc.Doc : A class description that\n                   spans several lines.\n\n        Examples\n        --------\n        >>> doctest\n        ...\n\n        Notes\n        -----\n        Foo bar.\n\n        ### H3 Title\n\n        Foo bar.\n        \"\"\"\n\n    def google(self):\n        \"\"\"\n        Summary line.\n        Nomatch:\n\n        Args:\n            arg1 (str, optional): Text1\n            arg2 (List[str], optional, default=10): Text2\n            data (array-like object): foo\n\n        Args:\n          arg1 (int): Description of arg1\n          arg2 (str or int): Description of arg2\n          arg3 (str | None): Description of arg3\n          test_sequence: 2-dim numpy array of real numbers, size: N * D\n            - the test observation sequence.\n\n                test_sequence =\n                code\n\n            Continue.\n          *args: passed around\n\n        Returns:\n            issue_10: description didn't work across multiple lines\n                if only a single item was listed. `inspect.cleandoc()`\n                somehow stripped the required extra indentation.\n\n        Returns:\n            A very special number\n            which is the answer of everything.\n\n        Returns:\n            Dict[int, pdoc.Doc]: Description.\n\n        Returns:\n            int | str: Description.\n\n        Raises:\n            AttributeError: The ``Raises`` section is a list of all exceptions\n                that are relevant to the interface.\n\n                and a third line.\n            ValueError: If `arg2` is equal to `arg1`.\n\n        Test a title without a blank line before it.\n        Args:\n            A: a\n\n        Examples:\n          Examples in doctest format.\n\n          >>> a = [1,2,3]\n\n        Todos:\n            * For module TODOs\n        \"\"\"\n\n    def doctests(self):\n        \"\"\"\n        Need an intro paragrapgh.\n\n            >>> Then code is indented one level\n            line1\n            line2\n\n        Alternatively\n        ```\n        >>> doctest\n        fenced code works\n        always\n        ```\n\n        Examples:\n            >>> nbytes(100)\n            '100.0 bytes'\n            line2\n\n            some text\n\n        some text\n\n        >>> another doctest\n        line1\n        line2\n\n        Example:\n            >>> f()\n            Traceback (most recent call last):\n                ...\n            Exception: something went wrong\n        \"\"\"\n\n    def reST_directives(self):\n        \"\"\"\n        .. todo::\n\n           Create something.\n\n        .. admonition:: Example\n\n           Image shows something.\n\n           .. image:: https://www.debian.org/logos/openlogo-nd-100.png\n\n           .. note::\n              Can only nest admonitions two levels.\n\n        .. image:: https://www.debian.org/logos/openlogo-nd-100.png\n\n        Now you know.\n\n        .. warning::\n\n            Some warning\n            lines.\n\n        * Describe some func in a list\n          across multiple lines:\n\n            .. deprecated:: 3.1\n              Use `spam` instead.\n\n            .. versionadded:: 2.5\n             The *spam* parameter.\n\n        .. caution::\n            Don't touch this!\n        \"\"\"\n\n\nnumpy = Docformats.numpy\n\n\ngoogle = Docformats.google\n\n\ndoctests = Docformats.doctests\n\n\nreST_directives = Docformats.reST_directives\n\n\ndef latex_math():\n    \"\"\"\n    Inline equation: \\\\( v_t *\\\\frac{1}{2}* j_i + [a] < 3 \\\\).\n\n    Block equation: \\\\[ v_t *\\\\frac{1}{2}* j_i + [a] < 3 \\\\]\n\n    Block equation: $$ v_t *\\\\frac{1}{2}* j_i + [a] < 3 $$\n\n    $\\\\mathcal{O}(N)$\n\n    Escaping \\\\$ should work in math like $X = \\\\$3.25$ once it is implemented.\n\n    ..math::\n        v_t *\\\\frac{1}{2}* j_i + [a] < 3\n    \"\"\"\n\n\nclass Location(namedtuple('Location', 'lat lon')):\n    \"\"\"Geo-location, GPS position.\"\"\"\n\n\ndef _func_spec(value: int) -> bool: ...  # noqa: E704\n\n\nasync def _coro_spec(value: int) -> bool: ...  # noqa: E704\n\n\nclass HasMockAttributes:\n    \"\"\"\n    Test class containing instances of `unittest.mock.Mock`.\n    \"\"\"\n\n    function_mock = Mock(spec=_func_spec)\n    coroutine_mock = AsyncMock(spec=_coro_spec)\n", "metadata": {"source": "github.com/pdoc3/pdoc", "file_path": "pdoc/test/example_pkg/__init__.py", "language": "python", "complexity": "complex", "star_count": 1174, "quality_score": 0.8099999999999999, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 390, "word_count": 805}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Module docstring\"\"\"\nfrom collections import namedtuple\nimport subprocess\nimport os\nfrom unittest.mock import AsyncMock, Mock\n\nCONST = 'const'\n\"\"\"CONST docstring\"\"\"\n\nvar = 2\n\"\"\"var docstring\"\"\"\n\n# https://github.com/mitmproxy/pdoc/pull/44\nforeign_var = subprocess.CalledProcessError(0, '')\n\"\"\"foreign var docstring\"\"\"\n\n__pdoc__ = {}\n\n\ndef foo(env=os.environ):\n    \"\"\"Doesn't leak environ\"\"\"\n\n\ndef object_as_arg_default(*args, a=object(), **kwargs):\n    \"\"\"Html-encodes angle brackets in params\"\"\"\n\n\ndef _private_function():\n    \"\"\"Private function, should only appear if whitelisted\"\"\"\n\n\nclass A:\n    \"\"\"`A` is base class for `example_pkg.B`.\"\"\"  # Test refname link\n    def overridden(self):\n        \"\"\"A.overridden docstring\"\"\"\n\n    def overridden_same_docstring(self):\n        \"\"\"A.overridden_same_docstring docstring\"\"\"\n\n    def inherited(self):  # Inherited in B\n        \"\"\"A.inherited docstring\"\"\"\n\n    def __call__(self):\n        \"\"\"A.__call__ docstring. Only shown when whitelisted\"\"\"\n\n\nnon_callable_routine = staticmethod(lambda x: 2)  # Not interpreted as Function; skipped\n\n\nclass ReadOnlyValueDescriptor:\n    \"\"\"Read-only value descriptor\"\"\"\n\n    def __get__(self, instance, instance_type=None):\n        if instance is not None:\n            return instance.var ** 2\n        return self\n\n\nclass B(A, int):\n    \"\"\"\n    B docstring\n\n    External refs: `sys.version`, `sys`\n    \"\"\"\n\n    CONST = 2\n    \"\"\"B.CONST docstring\"\"\"\n\n    var = 3\n    \"\"\"B.var docstring\"\"\"\n\n    ro_value_descriptor = ReadOnlyValueDescriptor()\n    \"\"\"ro_value_descriptor docstring\"\"\"\n\n    ro_value_descriptor_no_doc = ReadOnlyValueDescriptor()   # no doc-string\n\n    def __init__(self, x, y, z, w):\n        \"\"\"`__init__` docstring\"\"\"\n        self.instance_var = None\n        \"\"\"instance var docstring\"\"\"\n\n        self._private_instance_var = None\n        \"\"\"This should be private (hidden) despite PEP 224 docstring\"\"\"\n\n    def f(self, a: int, b: int = 1, *args, c: str = 'c', **kwargs):\n        \"\"\"B.f docstring\"\"\"\n\n    @staticmethod\n    def static(x):\n        \"\"\"B.static docstring\"\"\"\n\n    @classmethod\n    def cls(cls):\n        \"\"\"B.cls docstring\"\"\"\n\n    def _private(self):\n        \"\"\"B._private docstring\"\"\"\n\n    @staticmethod\n    def _private_static():\n        \"\"\"B._private_static docstring\"\"\"\n\n    @classmethod\n    def _private_cls(cls):\n        \"\"\"B._private_cls docstring\"\"\"\n\n    @property\n    def p(self):\n        \"\"\"B.p docstring\"\"\"\n        return 1\n\n    class C:\n        \"\"\"B.C docstring\"\"\"\n        def f(self):\n            \"\"\"B.C.f docstring\"\"\"\n\n    class _Private:\n        \"\"\"B._Private docstring\"\"\"\n        def f(self):\n            \"\"\"B._Private.f docstring\"\"\"\n\n    def overridden(self):\n        pass\n\n    assert overridden.__doc__ is None\n    __pdoc__['B.overridden'] = 'B.overridden docstring'\n\n    def overridden_same_docstring(self):\n        pass\n\n\nclass C(B): pass  # noqa: E701, E302\nclass D(C): pass  # noqa: E701, E302\n\n\nclass Hidden:\n    __pdoc__['Hidden'] = False\n\n\nclass Docformats:\n    def numpy(self):\n        \"\"\"\n        Summary line.\n\n        **Documentation**: https://pdoc3.github.io/pdoc/doc/pdoc/\n        **Source Code**: https://github.com/pdoc3/\n\n        Parameters\n        ----------\n        x1, x2 : array_like\n            Input arrays,\n            description of `x1`, `x2`.\n\n            .. versionadded:: 1.5.0\n        x : { NoneType, 'B', 'C' }, optional\n        n : int or list of int\n            Description of num\n        *args, **kwargs\n            Passed on.\n        complex : Union[Set[pdoc.Doc, Function], pdoc]\n            The `List[Doc]`s of the new signal.\n\n        Returns\n        -------\n        output : pdoc.Doc\n            The output array\n        List[pdoc.Doc]\n            The output array\n        foo\n\n        Raises\n        ------\n        TypeError\n            When something.\n\n        Raises\n        ------\n        TypeError\n\n        Returns\n        -------\n        None.\n\n        Invalid\n        -------\n        no match\n\n        See Also\n        --------\n        fromstring, loadtxt\n\n        See Also\n        --------\n        pdoc.text : Function a with its description.\n        scipy.random.norm : Random variates, PDFs, etc.\n        pdoc.Doc : A class description that\n                   spans several lines.\n\n        Examples\n        --------\n        >>> doctest\n        ...\n\n        Notes\n        -----\n        Foo bar.\n\n        ### H3 Title\n\n        Foo bar.\n        \"\"\"\n\n    def google(self):\n        \"\"\"\n        Summary line.\n        Nomatch:\n\n        Args:\n            arg1 (str, optional): Text1\n            arg2 (List[str], optional, default=10): Text2\n            data (array-like object): foo\n\n        Args:\n          arg1 (int): Description of arg1\n          arg2 (str or int): Description of arg2\n          arg3 (str | None): Description of arg3\n          test_sequence: 2-dim numpy array of real numbers, size: N * D\n            - the test observation sequence.\n\n                test_sequence =\n                code\n\n            Continue.\n          *args: passed around\n\n        Returns:\n            issue_10: description didn't work across multiple lines\n                if only a single item was listed. `inspect.cleandoc()`\n                somehow stripped the required extra indentation.\n\n        Returns:\n            A very special number\n            which is the answer of everything.\n\n        Returns:\n            Dict[int, pdoc.Doc]: Description.\n\n        Returns:\n            int | str: Description.\n\n        Raises:\n            AttributeError: The ``Raises`` section is a list of all exceptions\n                that are relevant to the interface.\n\n                and a third line.\n            ValueError: If `arg2` is equal to `arg1`.\n\n        Test a title without a blank line before it.\n        Args:\n            A: a\n\n        Examples:\n          Examples in doctest format.\n\n          >>> a = [1,2,3]\n\n        Todos:\n            * For module TODOs\n        \"\"\"\n\n    def doctests(self):\n        \"\"\"\n        Need an intro paragrapgh.\n\n            >>> Then code is indented one level\n            line1\n            line2\n\n        Alternatively\n        ```\n        >>> doctest\n        fenced code works\n        always\n        ```\n\n        Examples:\n            >>> nbytes(100)\n            '100.0 bytes'\n            line2\n\n            some text\n\n        some text\n\n        >>> another doctest\n        line1\n        line2\n\n        Example:\n            >>> f()\n            Traceback (most recent call last):\n                ...\n            Exception: something went wrong\n        \"\"\"\n\n    def reST_directives(self):\n        \"\"\"\n        .. todo::\n\n           Create something.\n\n        .. admonition:: Example\n\n           Image shows something.\n\n           .. image:: https://www.debian.org/logos/openlogo-nd-100.png\n\n           .. note::\n              Can only nest admonitions two levels.\n\n        .. image:: https://www.debian.org/logos/openlogo-nd-100.png\n\n        Now you know.\n\n        .. warning::\n\n            Some warning\n            lines.\n\n        * Describe some func in a list\n          across multiple lines:\n\n            .. deprecated:: 3.1\n              Use `spam` instead.\n\n            .. versionadded:: 2.5\n             The *spam* parameter.\n\n        .. caution::\n            Don't touch this!\n        \"\"\"\n\n\nnumpy = Docformats.numpy\n\n\ngoogle = Docformats.google\n\n\ndoctests = Docformats.doctests\n\n\nreST_directives = Docformats.reST_directives\n\n\ndef latex_math():\n    \"\"\"\n    Inline equation: \\\\( v_t *\\\\frac{1}{2}* j_i + [a] < 3 \\\\).\n\n    Block equation: \\\\[ v_t *\\\\frac{1}{2}* j_i + [a] < 3 \\\\]\n\n    Block equation: $$ v_t *\\\\frac{1}{2}* j_i + [a] < 3 $$\n\n    $\\\\mathcal{O}(N)$\n\n    Escaping \\\\$ should work in math like $X = \\\\$3.25$ once it is implemented.\n\n    ..math::\n        v_t *\\\\frac{1}{2}* j_i + [a] < 3\n    \"\"\"\n\n\nclass Location(namedtuple('Location', 'lat lon')):\n    \"\"\"Geo-location, GPS position.\"\"\"\n\n\ndef _func_spec(value: int) -> bool: ...  # noqa: E704\n\n\nasync def _coro_spec(value: int) -> bool: ...  # noqa: E704\n\n\nclass HasMockAttributes:\n    \"\"\"\n    Test class containing instances of `unittest.mock.Mock`.\n    \"\"\"\n\n    function_mock = Mock(spec=_func_spec)\n    coroutine_mock = AsyncMock(spec=_coro_spec)\n", "metadata": {"source": "github.com/pdoc3/pdoc", "file_path": "pdoc/test/example_pkg/_private/__init__.py", "language": "python", "complexity": "complex", "star_count": 1174, "quality_score": 0.8099999999999999, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 390, "word_count": 805}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Module docstring\"\"\"\nfrom collections import namedtuple\nimport subprocess\nimport os\nfrom unittest.mock import AsyncMock, Mock\n\nCONST = 'const'\n\"\"\"CONST docstring\"\"\"\n\nvar = 2\n\"\"\"var docstring\"\"\"\n\n# https://github.com/mitmproxy/pdoc/pull/44\nforeign_var = subprocess.CalledProcessError(0, '')\n\"\"\"foreign var docstring\"\"\"\n\n__pdoc__ = {}\n\n\ndef foo(env=os.environ):\n    \"\"\"Doesn't leak environ\"\"\"\n\n\ndef object_as_arg_default(*args, a=object(), **kwargs):\n    \"\"\"Html-encodes angle brackets in params\"\"\"\n\n\ndef _private_function():\n    \"\"\"Private function, should only appear if whitelisted\"\"\"\n\n\nclass A:\n    \"\"\"`A` is base class for `example_pkg.B`.\"\"\"  # Test refname link\n    def overridden(self):\n        \"\"\"A.overridden docstring\"\"\"\n\n    def overridden_same_docstring(self):\n        \"\"\"A.overridden_same_docstring docstring\"\"\"\n\n    def inherited(self):  # Inherited in B\n        \"\"\"A.inherited docstring\"\"\"\n\n    def __call__(self):\n        \"\"\"A.__call__ docstring. Only shown when whitelisted\"\"\"\n\n\nnon_callable_routine = staticmethod(lambda x: 2)  # Not interpreted as Function; skipped\n\n\nclass ReadOnlyValueDescriptor:\n    \"\"\"Read-only value descriptor\"\"\"\n\n    def __get__(self, instance, instance_type=None):\n        if instance is not None:\n            return instance.var ** 2\n        return self\n\n\nclass B(A, int):\n    \"\"\"\n    B docstring\n\n    External refs: `sys.version`, `sys`\n    \"\"\"\n\n    CONST = 2\n    \"\"\"B.CONST docstring\"\"\"\n\n    var = 3\n    \"\"\"B.var docstring\"\"\"\n\n    ro_value_descriptor = ReadOnlyValueDescriptor()\n    \"\"\"ro_value_descriptor docstring\"\"\"\n\n    ro_value_descriptor_no_doc = ReadOnlyValueDescriptor()   # no doc-string\n\n    def __init__(self, x, y, z, w):\n        \"\"\"`__init__` docstring\"\"\"\n        self.instance_var = None\n        \"\"\"instance var docstring\"\"\"\n\n        self._private_instance_var = None\n        \"\"\"This should be private (hidden) despite PEP 224 docstring\"\"\"\n\n    def f(self, a: int, b: int = 1, *args, c: str = 'c', **kwargs):\n        \"\"\"B.f docstring\"\"\"\n\n    @staticmethod\n    def static(x):\n        \"\"\"B.static docstring\"\"\"\n\n    @classmethod\n    def cls(cls):\n        \"\"\"B.cls docstring\"\"\"\n\n    def _private(self):\n        \"\"\"B._private docstring\"\"\"\n\n    @staticmethod\n    def _private_static():\n        \"\"\"B._private_static docstring\"\"\"\n\n    @classmethod\n    def _private_cls(cls):\n        \"\"\"B._private_cls docstring\"\"\"\n\n    @property\n    def p(self):\n        \"\"\"B.p docstring\"\"\"\n        return 1\n\n    class C:\n        \"\"\"B.C docstring\"\"\"\n        def f(self):\n            \"\"\"B.C.f docstring\"\"\"\n\n    class _Private:\n        \"\"\"B._Private docstring\"\"\"\n        def f(self):\n            \"\"\"B._Private.f docstring\"\"\"\n\n    def overridden(self):\n        pass\n\n    assert overridden.__doc__ is None\n    __pdoc__['B.overridden'] = 'B.overridden docstring'\n\n    def overridden_same_docstring(self):\n        pass\n\n\nclass C(B): pass  # noqa: E701, E302\nclass D(C): pass  # noqa: E701, E302\n\n\nclass Hidden:\n    __pdoc__['Hidden'] = False\n\n\nclass Docformats:\n    def numpy(self):\n        \"\"\"\n        Summary line.\n\n        **Documentation**: https://pdoc3.github.io/pdoc/doc/pdoc/\n        **Source Code**: https://github.com/pdoc3/\n\n        Parameters\n        ----------\n        x1, x2 : array_like\n            Input arrays,\n            description of `x1`, `x2`.\n\n            .. versionadded:: 1.5.0\n        x : { NoneType, 'B', 'C' }, optional\n        n : int or list of int\n            Description of num\n        *args, **kwargs\n            Passed on.\n        complex : Union[Set[pdoc.Doc, Function], pdoc]\n            The `List[Doc]`s of the new signal.\n\n        Returns\n        -------\n        output : pdoc.Doc\n            The output array\n        List[pdoc.Doc]\n            The output array\n        foo\n\n        Raises\n        ------\n        TypeError\n            When something.\n\n        Raises\n        ------\n        TypeError\n\n        Returns\n        -------\n        None.\n\n        Invalid\n        -------\n        no match\n\n        See Also\n        --------\n        fromstring, loadtxt\n\n        See Also\n        --------\n        pdoc.text : Function a with its description.\n        scipy.random.norm : Random variates, PDFs, etc.\n        pdoc.Doc : A class description that\n                   spans several lines.\n\n        Examples\n        --------\n        >>> doctest\n        ...\n\n        Notes\n        -----\n        Foo bar.\n\n        ### H3 Title\n\n        Foo bar.\n        \"\"\"\n\n    def google(self):\n        \"\"\"\n        Summary line.\n        Nomatch:\n\n        Args:\n            arg1 (str, optional): Text1\n            arg2 (List[str], optional, default=10): Text2\n            data (array-like object): foo\n\n        Args:\n          arg1 (int): Description of arg1\n          arg2 (str or int): Description of arg2\n          arg3 (str | None): Description of arg3\n          test_sequence: 2-dim numpy array of real numbers, size: N * D\n            - the test observation sequence.\n\n                test_sequence =\n                code\n\n            Continue.\n          *args: passed around\n\n        Returns:\n            issue_10: description didn't work across multiple lines\n                if only a single item was listed. `inspect.cleandoc()`\n                somehow stripped the required extra indentation.\n\n        Returns:\n            A very special number\n            which is the answer of everything.\n\n        Returns:\n            Dict[int, pdoc.Doc]: Description.\n\n        Returns:\n            int | str: Description.\n\n        Raises:\n            AttributeError: The ``Raises`` section is a list of all exceptions\n                that are relevant to the interface.\n\n                and a third line.\n            ValueError: If `arg2` is equal to `arg1`.\n\n        Test a title without a blank line before it.\n        Args:\n            A: a\n\n        Examples:\n          Examples in doctest format.\n\n          >>> a = [1,2,3]\n\n        Todos:\n            * For module TODOs\n        \"\"\"\n\n    def doctests(self):\n        \"\"\"\n        Need an intro paragrapgh.\n\n            >>> Then code is indented one level\n            line1\n            line2\n\n        Alternatively\n        ```\n        >>> doctest\n        fenced code works\n        always\n        ```\n\n        Examples:\n            >>> nbytes(100)\n            '100.0 bytes'\n            line2\n\n            some text\n\n        some text\n\n        >>> another doctest\n        line1\n        line2\n\n        Example:\n            >>> f()\n            Traceback (most recent call last):\n                ...\n            Exception: something went wrong\n        \"\"\"\n\n    def reST_directives(self):\n        \"\"\"\n        .. todo::\n\n           Create something.\n\n        .. admonition:: Example\n\n           Image shows something.\n\n           .. image:: https://www.debian.org/logos/openlogo-nd-100.png\n\n           .. note::\n              Can only nest admonitions two levels.\n\n        .. image:: https://www.debian.org/logos/openlogo-nd-100.png\n\n        Now you know.\n\n        .. warning::\n\n            Some warning\n            lines.\n\n        * Describe some func in a list\n          across multiple lines:\n\n            .. deprecated:: 3.1\n              Use `spam` instead.\n\n            .. versionadded:: 2.5\n             The *spam* parameter.\n\n        .. caution::\n            Don't touch this!\n        \"\"\"\n\n\nnumpy = Docformats.numpy\n\n\ngoogle = Docformats.google\n\n\ndoctests = Docformats.doctests\n\n\nreST_directives = Docformats.reST_directives\n\n\ndef latex_math():\n    \"\"\"\n    Inline equation: \\\\( v_t *\\\\frac{1}{2}* j_i + [a] < 3 \\\\).\n\n    Block equation: \\\\[ v_t *\\\\frac{1}{2}* j_i + [a] < 3 \\\\]\n\n    Block equation: $$ v_t *\\\\frac{1}{2}* j_i + [a] < 3 $$\n\n    $\\\\mathcal{O}(N)$\n\n    Escaping \\\\$ should work in math like $X = \\\\$3.25$ once it is implemented.\n\n    ..math::\n        v_t *\\\\frac{1}{2}* j_i + [a] < 3\n    \"\"\"\n\n\nclass Location(namedtuple('Location', 'lat lon')):\n    \"\"\"Geo-location, GPS position.\"\"\"\n\n\ndef _func_spec(value: int) -> bool: ...  # noqa: E704\n\n\nasync def _coro_spec(value: int) -> bool: ...  # noqa: E704\n\n\nclass HasMockAttributes:\n    \"\"\"\n    Test class containing instances of `unittest.mock.Mock`.\n    \"\"\"\n\n    function_mock = Mock(spec=_func_spec)\n    coroutine_mock = AsyncMock(spec=_coro_spec)\n", "metadata": {"source": "github.com/pdoc3/pdoc", "file_path": "pdoc/test/example_pkg/_private/_private.py", "language": "python", "complexity": "complex", "star_count": 1174, "quality_score": 0.8099999999999999, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 390, "word_count": 805}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Module docstring\"\"\"\nfrom collections import namedtuple\nimport subprocess\nimport os\nfrom unittest.mock import AsyncMock, Mock\n\nCONST = 'const'\n\"\"\"CONST docstring\"\"\"\n\nvar = 2\n\"\"\"var docstring\"\"\"\n\n# https://github.com/mitmproxy/pdoc/pull/44\nforeign_var = subprocess.CalledProcessError(0, '')\n\"\"\"foreign var docstring\"\"\"\n\n__pdoc__ = {}\n\n\ndef foo(env=os.environ):\n    \"\"\"Doesn't leak environ\"\"\"\n\n\ndef object_as_arg_default(*args, a=object(), **kwargs):\n    \"\"\"Html-encodes angle brackets in params\"\"\"\n\n\ndef _private_function():\n    \"\"\"Private function, should only appear if whitelisted\"\"\"\n\n\nclass A:\n    \"\"\"`A` is base class for `example_pkg.B`.\"\"\"  # Test refname link\n    def overridden(self):\n        \"\"\"A.overridden docstring\"\"\"\n\n    def overridden_same_docstring(self):\n        \"\"\"A.overridden_same_docstring docstring\"\"\"\n\n    def inherited(self):  # Inherited in B\n        \"\"\"A.inherited docstring\"\"\"\n\n    def __call__(self):\n        \"\"\"A.__call__ docstring. Only shown when whitelisted\"\"\"\n\n\nnon_callable_routine = staticmethod(lambda x: 2)  # Not interpreted as Function; skipped\n\n\nclass ReadOnlyValueDescriptor:\n    \"\"\"Read-only value descriptor\"\"\"\n\n    def __get__(self, instance, instance_type=None):\n        if instance is not None:\n            return instance.var ** 2\n        return self\n\n\nclass B(A, int):\n    \"\"\"\n    B docstring\n\n    External refs: `sys.version`, `sys`\n    \"\"\"\n\n    CONST = 2\n    \"\"\"B.CONST docstring\"\"\"\n\n    var = 3\n    \"\"\"B.var docstring\"\"\"\n\n    ro_value_descriptor = ReadOnlyValueDescriptor()\n    \"\"\"ro_value_descriptor docstring\"\"\"\n\n    ro_value_descriptor_no_doc = ReadOnlyValueDescriptor()   # no doc-string\n\n    def __init__(self, x, y, z, w):\n        \"\"\"`__init__` docstring\"\"\"\n        self.instance_var = None\n        \"\"\"instance var docstring\"\"\"\n\n        self._private_instance_var = None\n        \"\"\"This should be private (hidden) despite PEP 224 docstring\"\"\"\n\n    def f(self, a: int, b: int = 1, *args, c: str = 'c', **kwargs):\n        \"\"\"B.f docstring\"\"\"\n\n    @staticmethod\n    def static(x):\n        \"\"\"B.static docstring\"\"\"\n\n    @classmethod\n    def cls(cls):\n        \"\"\"B.cls docstring\"\"\"\n\n    def _private(self):\n        \"\"\"B._private docstring\"\"\"\n\n    @staticmethod\n    def _private_static():\n        \"\"\"B._private_static docstring\"\"\"\n\n    @classmethod\n    def _private_cls(cls):\n        \"\"\"B._private_cls docstring\"\"\"\n\n    @property\n    def p(self):\n        \"\"\"B.p docstring\"\"\"\n        return 1\n\n    class C:\n        \"\"\"B.C docstring\"\"\"\n        def f(self):\n            \"\"\"B.C.f docstring\"\"\"\n\n    class _Private:\n        \"\"\"B._Private docstring\"\"\"\n        def f(self):\n            \"\"\"B._Private.f docstring\"\"\"\n\n    def overridden(self):\n        pass\n\n    assert overridden.__doc__ is None\n    __pdoc__['B.overridden'] = 'B.overridden docstring'\n\n    def overridden_same_docstring(self):\n        pass\n\n\nclass C(B): pass  # noqa: E701, E302\nclass D(C): pass  # noqa: E701, E302\n\n\nclass Hidden:\n    __pdoc__['Hidden'] = False\n\n\nclass Docformats:\n    def numpy(self):\n        \"\"\"\n        Summary line.\n\n        **Documentation**: https://pdoc3.github.io/pdoc/doc/pdoc/\n        **Source Code**: https://github.com/pdoc3/\n\n        Parameters\n        ----------\n        x1, x2 : array_like\n            Input arrays,\n            description of `x1`, `x2`.\n\n            .. versionadded:: 1.5.0\n        x : { NoneType, 'B', 'C' }, optional\n        n : int or list of int\n            Description of num\n        *args, **kwargs\n            Passed on.\n        complex : Union[Set[pdoc.Doc, Function], pdoc]\n            The `List[Doc]`s of the new signal.\n\n        Returns\n        -------\n        output : pdoc.Doc\n            The output array\n        List[pdoc.Doc]\n            The output array\n        foo\n\n        Raises\n        ------\n        TypeError\n            When something.\n\n        Raises\n        ------\n        TypeError\n\n        Returns\n        -------\n        None.\n\n        Invalid\n        -------\n        no match\n\n        See Also\n        --------\n        fromstring, loadtxt\n\n        See Also\n        --------\n        pdoc.text : Function a with its description.\n        scipy.random.norm : Random variates, PDFs, etc.\n        pdoc.Doc : A class description that\n                   spans several lines.\n\n        Examples\n        --------\n        >>> doctest\n        ...\n\n        Notes\n        -----\n        Foo bar.\n\n        ### H3 Title\n\n        Foo bar.\n        \"\"\"\n\n    def google(self):\n        \"\"\"\n        Summary line.\n        Nomatch:\n\n        Args:\n            arg1 (str, optional): Text1\n            arg2 (List[str], optional, default=10): Text2\n            data (array-like object): foo\n\n        Args:\n          arg1 (int): Description of arg1\n          arg2 (str or int): Description of arg2\n          arg3 (str | None): Description of arg3\n          test_sequence: 2-dim numpy array of real numbers, size: N * D\n            - the test observation sequence.\n\n                test_sequence =\n                code\n\n            Continue.\n          *args: passed around\n\n        Returns:\n            issue_10: description didn't work across multiple lines\n                if only a single item was listed. `inspect.cleandoc()`\n                somehow stripped the required extra indentation.\n\n        Returns:\n            A very special number\n            which is the answer of everything.\n\n        Returns:\n            Dict[int, pdoc.Doc]: Description.\n\n        Returns:\n            int | str: Description.\n\n        Raises:\n            AttributeError: The ``Raises`` section is a list of all exceptions\n                that are relevant to the interface.\n\n                and a third line.\n            ValueError: If `arg2` is equal to `arg1`.\n\n        Test a title without a blank line before it.\n        Args:\n            A: a\n\n        Examples:\n          Examples in doctest format.\n\n          >>> a = [1,2,3]\n\n        Todos:\n            * For module TODOs\n        \"\"\"\n\n    def doctests(self):\n        \"\"\"\n        Need an intro paragrapgh.\n\n            >>> Then code is indented one level\n            line1\n            line2\n\n        Alternatively\n        ```\n        >>> doctest\n        fenced code works\n        always\n        ```\n\n        Examples:\n            >>> nbytes(100)\n            '100.0 bytes'\n            line2\n\n            some text\n\n        some text\n\n        >>> another doctest\n        line1\n        line2\n\n        Example:\n            >>> f()\n            Traceback (most recent call last):\n                ...\n            Exception: something went wrong\n        \"\"\"\n\n    def reST_directives(self):\n        \"\"\"\n        .. todo::\n\n           Create something.\n\n        .. admonition:: Example\n\n           Image shows something.\n\n           .. image:: https://www.debian.org/logos/openlogo-nd-100.png\n\n           .. note::\n              Can only nest admonitions two levels.\n\n        .. image:: https://www.debian.org/logos/openlogo-nd-100.png\n\n        Now you know.\n\n        .. warning::\n\n            Some warning\n            lines.\n\n        * Describe some func in a list\n          across multiple lines:\n\n            .. deprecated:: 3.1\n              Use `spam` instead.\n\n            .. versionadded:: 2.5\n             The *spam* parameter.\n\n        .. caution::\n            Don't touch this!\n        \"\"\"\n\n\nnumpy = Docformats.numpy\n\n\ngoogle = Docformats.google\n\n\ndoctests = Docformats.doctests\n\n\nreST_directives = Docformats.reST_directives\n\n\ndef latex_math():\n    \"\"\"\n    Inline equation: \\\\( v_t *\\\\frac{1}{2}* j_i + [a] < 3 \\\\).\n\n    Block equation: \\\\[ v_t *\\\\frac{1}{2}* j_i + [a] < 3 \\\\]\n\n    Block equation: $$ v_t *\\\\frac{1}{2}* j_i + [a] < 3 $$\n\n    $\\\\mathcal{O}(N)$\n\n    Escaping \\\\$ should work in math like $X = \\\\$3.25$ once it is implemented.\n\n    ..math::\n        v_t *\\\\frac{1}{2}* j_i + [a] < 3\n    \"\"\"\n\n\nclass Location(namedtuple('Location', 'lat lon')):\n    \"\"\"Geo-location, GPS position.\"\"\"\n\n\ndef _func_spec(value: int) -> bool: ...  # noqa: E704\n\n\nasync def _coro_spec(value: int) -> bool: ...  # noqa: E704\n\n\nclass HasMockAttributes:\n    \"\"\"\n    Test class containing instances of `unittest.mock.Mock`.\n    \"\"\"\n\n    function_mock = Mock(spec=_func_spec)\n    coroutine_mock = AsyncMock(spec=_coro_spec)\n", "metadata": {"source": "github.com/pdoc3/pdoc", "file_path": "pdoc/test/example_pkg/_private/module.py", "language": "python", "complexity": "complex", "star_count": 1174, "quality_score": 0.8099999999999999, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": true, "has_tables": true, "has_headers": true, "line_count": 390, "word_count": 805}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n```python\n    .. include:: _include_me.py\n        :start-line: 1\n        :end-line: 2\n```\n\n.. include:: foo/../_include_me.py\n    :start-after: =\n    :end-before: 4\n\n.. include:: table.md\n\nRemaining.\n\"\"\"\n", "metadata": {"source": "github.com/pdoc3/pdoc", "file_path": "pdoc/test/example_pkg/_reST_include/test.py", "language": "python", "complexity": "simple", "star_count": 1174, "quality_score": 0.63, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 16, "word_count": 22}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "pass  # Avoids OSError on `inspect.getsource(doc_obj.obj)` in `pdoc._var_docstrings`\n", "metadata": {"source": "github.com/pdoc3/pdoc", "file_path": "pdoc/test/example_pkg/_test_linking/__init__.py", "language": "python", "complexity": "simple", "star_count": 1174, "quality_score": 0.39999999999999997, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": false, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 2, "word_count": 8}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "-   id: pydocstyle\n    name: pydocstyle\n    description: pydocstyle is a static analysis tool for checking compliance with Python docstring conventions.\n    entry: pydocstyle\n    language: python\n    types: [python]\n", "metadata": {"source": "github.com/PyCQA/pydocstyle", "file_path": ".pre-commit-hooks.yaml", "language": "yaml", "complexity": "simple", "star_count": 1117, "quality_score": 0.43, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 7, "word_count": 25}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Thanks for submitting a PR!\n\nPlease make sure to check for the following items:\n- [ ] Add unit tests and integration tests where applicable.  \n      If you've added an error code or changed an error code behavior,\n      you should probably add or change a test case file under `tests/test_cases/` and add \n      it to the list under `tests/test_definitions.py`.  \n      If you've added or changed a command line option,\n      you should probably add or change a test in `tests/test_integration.py`.\n- [ ] Add a line to the release notes (docs/release_notes.rst) under \"Current Development Version\".  \n      Make sure to include the PR number after you open and get one.\n   \nPlease don't get discouraged as it may take a while to get a review.\n", "metadata": {"source": "github.com/PyCQA/pydocstyle", "file_path": "PULL_REQUEST_TEMPLATE.md", "language": "markdown", "complexity": "simple", "star_count": 1117, "quality_score": 0.63, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 14, "word_count": 118}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "pydocstyle - docstring style checker\n====================================\n\n======================================\nDeprecation Notice: Pydocstyle\n======================================\n\n.. IMPORTANT: Deprecation Notice\n   :style: alert\n\n.. image:: https://img.shields.io/badge/deprecated-red\n   :target: https://github.com/PyCQA/pydocstyle\n   :alt: Deprecated\n\nThe Pydocstyle project is officially deprecated, and it is no longer actively maintained. \n\n\n\nRuff: A Modern Alternative\n--------------------------\n\n- **GitHub Repository:** `ruff <https://github.com/astral-sh/ruff>`_\n\nRuff offers full parity with pydocstyle along with advanced features, better support for the latest Python versions, and ongoing development to ensure a top-notch linting experience. We highly recommend pydocstyle users to switch over to ruff for a seamless transition.\n\n\nA Heartfelt Thank You\n-----------------------\n\nWe want to express our heartfelt gratitude to the pydocstyle community, maintainers, and contributors for their support and dedication over the years. Your contributions have been invaluable, and we appreciate the time and effort you've invested in making pydocstyle a valuable tool for the Python community.\n\nThank you for your support of pydocstyle in the past.\n\n\n\n---------\n\n\n\n.. image:: https://github.com/PyCQA/pydocstyle/workflows/Run%20tests/badge.svg\n    :target: https://github.com/PyCQA/pydocstyle/actions?query=workflow%3A%22Run+tests%22+branch%3Amaster\n\n.. image:: https://readthedocs.org/projects/pydocstyle/badge/?version=latest\n    :target: https://readthedocs.org/projects/pydocstyle/?badge=latest\n    :alt: Documentation Status\n\n.. image:: https://img.shields.io/pypi/pyversions/pydocstyle.svg\n    :target: https://pypi.org/project/pydocstyle\n\n.. image:: https://pepy.tech/badge/pydocstyle\n    :target: https://pepy.tech/project/pydocstyle\n\n.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n    :target: https://github.com/psf/black\n\n.. image:: https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336\n    :target: https://pycqa.github.io/isort/\n\n.. image:: https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod\n    :target: https://gitpod.io/#https://github.com/PyCQA/pydocstyle\n    :alt: Gitpod ready-to-code\n\n**pydocstyle** is a static analysis tool for checking compliance with Python\ndocstring conventions.\n\n**pydocstyle** supports most of\n`PEP 257 <http://www.python.org/dev/peps/pep-0257/>`_ out of the box, but it\nshould not be considered a reference implementation.\n\n**pydocstyle** supports Python 3.6+.\n\n\nQuick Start\n-----------\n\nInstall\n^^^^^^^\n\n.. code::\n\n    pip install pydocstyle\n\n\nRun\n^^^\n\n.. code::\n\n    $ pydocstyle test.py\n    test.py:18 in private nested class `meta`:\n            D101: Docstring missing\n    test.py:27 in public function `get_user`:\n        D300: Use \"\"\"triple double quotes\"\"\" (found '''-quotes)\n    test:75 in public function `init_database`:\n        D201: No blank lines allowed before function docstring (found 1)\n    ...\n\nDevelop\n^^^^^^^\n\nYou can use Gitpod to run pre-configured dev environment in the cloud right from your browser -\n\n.. image:: https://gitpod.io/button/open-in-gitpod.svg\n    :target: https://gitpod.io/#https://github.com/PyCQA/pydocstyle\n    :alt: Open in Gitpod\n    \nBefore submitting a PR make sure that you run `make all`.\n\nLinks\n-----\n\n* `Read the full documentation here <https://pydocstyle.org/en/stable/>`_.\n\n* `Fork pydocstyle on GitHub <https://github.com/PyCQA/pydocstyle>`_.\n\n* `PyPI project page <https://pypi.python.org/pypi/pydocstyle>`_.\n", "metadata": {"source": "github.com/PyCQA/pydocstyle", "file_path": "README.rst", "language": "restructuredtext", "complexity": "medium", "star_count": 1117, "quality_score": 0.75, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 117, "word_count": 340}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# pydocstyle documentation build configuration file, created by\n# sphinx-quickstart on Fri Jan 30 20:30:42 2015.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\nimport pydocstyle\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.coverage',\n    'sphinx.ext.viewcode',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix of source filenames.\nsource_suffix = '.rst'\n\n# The encoding of source files.\n#source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = 'pydocstyle'\ncopyright = '2020, Amir Rachum, Sambhav Kothari'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = pydocstyle.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = pydocstyle.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = ''\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = ['_build']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built documents.\n#keep_warnings = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\ntry:\n    import sphinx_rtd_theme\nexcept ImportError:\n    html_theme = 'default'\nelse:\n    html_theme = 'sphinx_rtd_theme'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n#html_static_path = ['_static']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'pydocstyledoc'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size ('letterpaper' or 'a4paper').\n#'papersize': 'letterpaper',\n\n# The font size ('10pt', '11pt' or '12pt').\n#'pointsize': '10pt',\n\n# Additional stuff for the LaTeX preamble.\n#'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n  ('index', 'pydocstyle.tex', 'pydocstyle Documentation',\n   'Amir Rachum', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    ('index', 'pydocstyle', 'pydocstyle Documentation',\n     ['Amir Rachum'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  ('index', 'pydocstyle', 'pydocstyle Documentation',\n   'Amir Rachum', 'pydocstyle', 'Docstring style checker',\n   'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n#texinfo_show_urls = 'footnote'\n\n# If true, do not generate a @detailmenu in the \"Top\" node's menu.\n#texinfo_no_detailmenu = False\n\n\ndef generate_error_code_table():\n    from pydocstyle.violations import ErrorRegistry\n    with open(os.path.join('snippets', 'error_code_table.rst'), 'wt') as outf:\n        outf.write(ErrorRegistry.to_rst())\n\ngenerate_error_code_table()\n", "metadata": {"source": "github.com/PyCQA/pydocstyle", "file_path": "docs/conf.py", "language": "python", "complexity": "complex", "star_count": 1117, "quality_score": 0.78, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": true, "line_count": 274, "word_count": 1310}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Error Codes\n===========\n\nGrouping\n--------\n\n.. include:: snippets/error_code_table.rst\n\nDefault conventions\n-------------------\n\nNot all error codes are checked for by default.  There are three conventions\nthat may be used by pydocstyle: ``pep257``, ``numpy`` and ``google``.\n\nThe ``pep257`` convention (specified in `PEP257\n<http://www.python.org/dev/peps/pep-0257/>`_), which is enabled by default in\npydocstyle, checks for all of the above errors except for D203, D212, D213,\nD214, D215, D404, D405, D406, D407, D408, D409, D410, D411, D413, D415, D416\nand D417.\n\nThe ``numpy`` convention added in v2.0.0 supports the `numpydoc docstring\n<https://github.com/numpy/numpydoc>`_ standard. This checks all of the\nerrors except for D107, D203, D212, D213, D402, D413, D415, D416, and D417.\n\nThe ``google`` convention added in v4.0.0 supports the `Google Python Style\nGuide <https://google.github.io/styleguide/pyguide.html>`_. This checks for\nall the errors except D203, D204, D213, D215, D400, D401, D404, D406, D407,\nD408, D409 and D413.\n\nThese conventions may be specified using ``--convention=<name>`` when\nrunning pydocstyle from the command line or by specifying the\nconvention in a configuration file.  See the :ref:`cli_usage` section\nfor more details.\n\n.. note::\n\n  It makes no sense to check the same docstring for both ``numpy`` and ``google``\n  conventions. Therefore, if we successfully detect that a docstring is in the\n  ``numpy`` style, we don't check it for ``google``.\n\n  The reason ``numpy`` style takes precedence over ``google`` is that the\n  heuristics of detecting it are better, and we don't want to enforce users to\n  provide external hints to `pydocstyle` in order to let it know which style\n  docstrings are written in.\n\nPublicity\n---------\n\n.. include:: snippets/publicity.rst\n", "metadata": {"source": "github.com/PyCQA/pydocstyle", "file_path": "docs/error_codes.rst", "language": "restructuredtext", "complexity": "medium", "star_count": 1117, "quality_score": 0.6599999999999999, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 50, "word_count": 251}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "pydocstyle's documentation\n==========================\n\n**pydocstyle** is a static analysis tool for checking compliance with Python\ndocstring conventions.\n\n**pydocstyle** supports most of\n`PEP 257 <http://www.python.org/dev/peps/pep-0257/>`_ out of the box, but it\nshould not be considered a reference implementation.\n\n**pydocstyle** supports Python 3.7 through 3.11.\n\nAlthough pydocstyle is tries to be compatible with Python 3.6, it is not tested.\n\n\n.. include:: quickstart.rst\n\n\nContents:\n\n.. toctree::\n   :maxdepth: 2\n\n   usage\n   error_codes\n   release_notes\n   license\n\n\nCredits\n=======\n\npydocstyle is a rename and continuation of pep257, a project created by\nVladimir Keleshev.\n\nMaintained by Amir Rachum and Sambhav Kothari.\n", "metadata": {"source": "github.com/PyCQA/pydocstyle", "file_path": "docs/index.rst", "language": "restructuredtext", "complexity": "simple", "star_count": 1117, "quality_score": 0.6, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 37, "word_count": 91}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Quick Start\n===========\n\n1. Install\n\n    .. code::\n\n        pip install pydocstyle\n\n2. Run\n\n    .. code::\n\n        $ pydocstyle test.py\n        test.py:18 in private nested class `meta`:\n                D101: Docstring missing\n        test.py:27 in public function `get_user`:\n            D300: Use \"\"\"triple double quotes\"\"\" (found '''-quotes)\n        test:75 in public function `init_database`:\n            D201: No blank lines allowed before function docstring (found 1)\n        ...\n\n3. Fix your code :)\n\n", "metadata": {"source": "github.com/PyCQA/pydocstyle", "file_path": "docs/quickstart.rst", "language": "restructuredtext", "complexity": "simple", "star_count": 1117, "quality_score": 0.69, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": false, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 25, "word_count": 59}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments for API endpoints and usage\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "Release Notes\n=============\n\n**pydocstyle** version numbers follow the\n`Semantic Versioning <http://semver.org/>`_ specification.\n\n6.3.0 - January 17th, 2023\n--------------------------\n\nNew Features\n\n* Add `ignore-self-only-init` config (#560).\n\nBug Fixes\n\n* Revert - Obey match rules in pre-commit usage (#610).\n\n\n6.2.3 - January 8th, 2023\n---------------------------\n\nBug Fixes\n\n* Fix decorator parsing for async function. Resolves some false positives\n  with async functions and ``overload``. (#577)\n* Obey match rules in pre-commit usage (#610).\n\n6.2.2 - January 3rd, 2023\n---------------------------\n\nBug Fixes\n\n* Fix false positives of D417 in google convention docstrings (#619).\n\n6.2.1 - January 3rd, 2023\n---------------------------\n\nBug Fixes\n\n* Use tomllib/tomli to correctly read .toml files (#599, #600).\n\n6.2.0 - January 2nd, 2023\n---------------------------\n\nNew Features\n\n* Allow for hanging indent when documenting args in Google style. (#449)\n* Add support for `property_decorators` config to ignore D401.\n* Add support for Python 3.10 (#554).\n* Replace D10X errors with D419 if docstring exists but is empty (#559).\n* Add support for Sphinx-style parameter descriptions to D417.\n\nBug Fixes\n\n* Fix ``--match`` option to only consider filename when matching full paths (#550).\n\n6.1.1 - May 17th, 2021\n---------------------------\n\nBug Fixes\n\n* Split ``--source`` by lines instead of by characters (#536).\n\n6.1.0 - May 17th, 2021\n---------------------------\n\nNew Features\n\n* Enable full toml configuration and pyproject.toml (#534).\n\n6.0.0 - March 18th, 2021\n---------------------------\n\nMajor Updates\n\n* Support for Python 3.5 has been dropped (#510).\n\nNew Features\n\n* Add flag to disable `# noqa` comment processing in API (#485).\n* Methods, Functions and Nested functions that have a docstring now throw D418 (#511).\n* Methods decorated with @overload no longer reported as D102 (#511).\n* Functions and nested functions decorated with @overload no longer reported as D103 (#511).\n\nBug Fixes\n\n* Treat \"package\" as an imperative verb for D401 (#356).\n* Fix the parsing of decorated one line functions (#499).\n\n5.1.2 - September 13th, 2020\n----------------------------\n\nNew Features\n\n* Methods, Functions and Nested functions that have a docstring now throw D418 (#511).\n* Methods decorated with @overload no longer reported as D102.\n* Functions and nested functions decorated with @overload no longer reported as D103.\n\n\n5.1.1 - August 29th, 2020\n---------------------------\n\nBug Fixes\n\n* Fix ``IndexError`` crash on one-line backslashed docstrings (#506).\n\n5.1.0 - August 22nd, 2020\n---------------------------\n\nNew Features\n\n* Skip function arguments prefixed with `_` in D417 check (#440).\n\nBug Fixes\n\n* Update convention support documentation (#386, #393)\n* Detect inner asynchronous functions for D202 (#467)\n* Fix indentation error while parsing class methods (#441).\n* Fix a bug in parsing Google-style argument description.\n  The bug caused some argument names to go unreported in D417 (#448).\n* Fixed an issue where skipping errors on module level docstring via #noqa\n  failed when there where more prior comments (#446).\n* Support backslash-continued descriptions in docstrings (#472).\n* Correctly detect publicity of modules inside directories (#470, #494).\n\n5.0.2 - January 8th, 2020\n---------------------------\n\nBug Fixes\n\n* Fix ``DeprecationWarning`` / ``SyntaxError`` \"invalid escape sequence\" with\n  Python 3.6+ (#445).\n\n5.0.1 - December 9th, 2019\n--------------------------\n\nBug Fixes\n\n* Fixed an issue where AttributeError was raised when parsing the parameter\n  section of a class docstring (#434, #436).\n\n5.0.0 - December 9th, 2019\n--------------------------\n\nMajor Updates\n\n* Support for Python 3.4 has been dropped (#402).\n\nNew Features\n\n* Extend support for detecting missing arguments in Google style\n  docstrings to method calls (#384).\n* Extend support for detecting missing argument description in Numpy style\n  docstrings (#407).\n* Added support for Python 3.8 (#423).\n* Allow skipping errors on module level docstring via #noqa (#427).\n* Whitespace is ignored with set options split across multiple lines (#221).\n\nBug Fixes\n\n* Remove D413 from the google convention (#430).\n* Remove D413 from the pep257 convention (#404).\n* Replace `semicolon` with `colon` in D416 messages. (#409)\n* D301 (Use r\"\"\" if any backslashes in a docstring) does not trigger on\n  backslashes for line continuation or unicode literals ``\\u...`` and\n  ``\\N...`` anymore. These are considered intended elements of the docstring\n  and thus should not be escaped by using a raw docstring (#365).\n* Fix decorator parsing (#411).\n* Google-style sections no longer cause false errors when used with\n  Numpy-style sections (#388, #424).\n* D202: Allow a blank line after function docstring when followed by\n  declaration of an inner function or class (#395, #426).\n* Fix D401 and D404 checks not working for docstrings containing only one word and ending with non-alpha character (#421)\n\n4.0.1 - August 14th, 2019\n-------------------------\n\nBug Fixes\n\n* D401: Fixed a false positive where one stem had multiple imperative forms,\n  e.g., init and initialize / initiate (#382).\n* Fix parser hanging when there's a comment directly after ``__all__``\n  (#391, #366).\n* Fixed RST error in table which resulted in the online documentation missing\n  the violation code table (#396).\n* Fixed IndentationError when parsing function arguments (#392).\n\n4.0.0 - July 6th, 2019\n----------------------\n\nMajor Updates\n\n* Support for Python 2.x and PyPy has been dropped (#340).\n* Added initial support for Google convention (#357).\n\nNew Features\n\n* Added pre-commit hook (#346)\n\nBug Fixes\n\n* Fix parsing tuple syntax ``__all__`` (#355, #352).\n\n3.0.0 - October 14th, 2018\n--------------------------\n\nMajor Updates\n\n* Support for Python 3.3 has been dropped (#315, #316).\n* Added support for Python 3.7 (#324).\n\nNew features\n\n* Violations are now reported on the line where the docstring starts, not the\n  line of the ``def``/``class`` it corresponds to (#238, #83).\n* Updated description of pep257 and numpy conventions (#300).\n* ``__all__`` parsing is now done on a best-effort basis - if ``__all__`` can't\n  be statically determined, it will be ignored (#320, #313).\n\nBug Fixes\n\n* Fixed a false-positive recognition of section names causing D405 to be\n  reported (#311, #317).\n* Fixed a bug where functions that don't end with a newline will sometimes\n  raise an exception (#321, #336).\n\n\n2.1.1 - October 9th, 2017\n-------------------------\n\nBug Fixes\n\n* Changed wheel configuration to be NOT universal, as #281 added\n  ``configparser`` as a dependency for Python 2.7.\n* Updated usage documentation.\n\n\n2.1.0 - October 8th, 2017\n-------------------------\n\nNew Features\n\n* Public nested classes missing a docstring are now reported as D106 instead\n  of D101 (#198, #261).\n* ``__init__`` methods missing a docstring are now reported as D107 instead of\n  D102 (#273, #277).\n* Added support for Python 3.6 (#270).\n* Specifying an invalid error code prefix (e.g., ``--select=D9``) will print\n  a warning message to ``stderr`` (#253, #279).\n* Configuration files now support multiple-lined entries (#250, #281).\n* Improved description of how error selection works in the help section\n  (#231, #283).\n\nBug Fixes\n\n* Fixed an issue where the ``--source`` flag would result in improperly\n  spaced output (#256, #257, #260).\n* Fixed an issue where if a first word in a docstring had Unicode characters\n  and the docstring was not a unicode string, an exception would be raised\n  (#258, #264).\n* Configuration files that were specified by CLI and don't contain a valid\n  section name will now issue a warning to ``stderr`` (#276, #280).\n* Removed D107 from the numpy convention (#288).\n\n\n2.0.0 - April 18th, 2017\n------------------------\n\nMajor Updates\n\n* Support for ``numpy`` conventions verification has been added (#129, #226).\n* Support for Python 2.6 has been dropped (#206, #217).\n* Support for PyPy3 has been temporarily dropped, until it will be\n  equivalent to CPython 3.3+ and supported by ``pip`` (#223).\n* Support for the ``pep257`` console script has been dropped. Only the\n  ``pydocstyle`` console script should be used (#216, #218).\n* Errors are now printed to ``stdout`` instead of ``stderr`` (#201, #210).\n\nNew Features\n\n* Decorator-based skipping via ``--ignore-decorators`` has been added (#204).\n* Support for using pycodestyle style wildcards has been added (#72, #209).\n* Superfluous opening quotes are now reported as part of D300 (#166, #225).\n* Fixed a false-positive recognition of `D410` and added `D412` (#230, #233).\n* Added ``--config=<path>`` flag to override the normal config file discovery\n  and choose a specific config file (#117, #247).\n* Support for specifying error codes with partial prefix has been added, e.g.,\n  ``--select=D101,D2`` (#72, #209).\n* All configuration file can now have the ``.ini`` extension (#237).\n* Added better imperative mood checks using third party stemmer (#235, #68).\n\nBug Fixes\n\n* Made parser more robust to bad source files (#168, #214)\n* Modules are now considered private if their name starts with a single\n  underscore. This is a bugfix where \"public module\" (D100) was reported\n  regardless of module name (#199, #222).\n* Removed error when ``__all__`` is a list (#62, #227).\n* Fixed a bug where the ``@`` sign was used as a matrix multiplication operator\n  in Python 3.5, but was considered a decorator by the parser (#246, #191).\n\n\n1.1.1 - October 4th, 2016\n-------------------------\n\nBug Fixes\n\n* Fixed an issue where the ``flake8-docstrings`` failed when accessing some\n  public API from ``pydocstyle``.\n\n\n1.1.0 - September 29th, 2016\n----------------------------\n\nMajor Updates\n\n* ``pydocstyle`` is no longer a single file. This might make it difficult for\n  some users to just add it to their project, but the project has reached\n  certain complexity where splitting it into modules was necessary (#200).\n\nNew Features\n\n* Added the optional error codes D212 and D213, for checking whether\n  the summary of a multi-line docstring starts at the first line,\n  respectively at the second line (#174).\n\n* Added D404 - First word of the docstring should not be \"This\". It is turned\n  off by default (#183).\n\n* Added the ability to ignore specific function and method docstrings with\n  inline comments:\n\n    1. \"# noqa\" skips all checks.\n\n    2. \"# noqa: D102,D203\" can be used to skip specific checks.\n\nBug Fixes\n\n* Fixed an issue where file paths were printed in lower case (#179, #181).\n\n* The error code D300 is now also being reported if a docstring has\n  uppercase literals (``R`` or ``U``) as prefix (#176).\n\n* Fixed a bug where an ``__all__`` error was reported when ``__all__`` was\n  imported from another module with a different name (#182, #187).\n\n* Fixed a bug where ``raise X from Y`` syntax caused ``pydocstyle`` to crash\n  (#196, #200).\n\n1.0.0 - January 30th, 2016\n--------------------------\n\nMajor Updates\n\n* The project was renamed to **pydocstyle** and the new release will be 1.0.0!\n\nNew Features\n\n* Added support for Python 3.5 (#145).\n\n* Classes nested inside classes are no longer considered private. Nested\n  classes are considered public if their names are not prepended with an\n  underscore and if their parent class is public, recursively (#13, #146).\n\n* Added the D403 error code - \"First word of the first line should be\n  properly capitalized\". This new error is turned on by default (#164, #165,\n  #170).\n\n* Added support for ``.pydocstylerc`` and as configuration file name\n  (#140, #173).\n\nBug Fixes\n\n* Fixed an issue where a ``NameError`` was raised when parsing complex\n  definitions of ``__all__`` (#142, #143).\n\n* Fixed a bug where D202 was falsely reported when a function with just a\n  docstring and no content was followed by a comment (#165).\n\n* Fixed wrong ``__all__`` definition in main module (#150, #156).\n\n* Fixed a bug where an ``AssertionError`` could occur when parsing\n  ``__future__`` imports (#154).\n\n\nOlder Versions\n==============\n\n.. note::\n\n    Versions documented below are before renaming the project from **pep257**\n    to **pydocstyle**.\n\n\n0.7.0 - October 9th, 2015\n-------------------------\n\nNew Features\n\n* Added the D104 error code - \"Missing docstring in public package\". This new\n  error is turned on by default. Missing docstring in ``__init__.py`` files which\n  previously resulted in D100 errors (\"Missing docstring in public module\")\n  will now result in D104 (#105, #127).\n\n* Added the D105 error code - \"Missing docstring in magic method'. This new\n  error is turned on by default. Missing docstrings in magic method which\n  previously resulted in D102 error (\"Missing docstring in public method\")\n  will now result in D105. Note that exceptions to this rule are variadic\n  magic methods - specifically ``__init__``, ``__call__`` and ``__new__``, which\n  will be considered non-magic and missing docstrings in them will result\n  in D102 (#60, #139).\n\n* Support the option to exclude all error codes. Running pep257 with\n  ``--select=`` (or ``select=`` in the configuration file) will exclude all errors\n  which could then be added one by one using ``add-select``. Useful for projects\n  new to pep257 (#132, #135).\n\n* Added check D211: No blank lines allowed before class docstring. This change\n  is a result of a change to the official PEP257 convention. Therefore, D211\n  will now be checked by default instead of D203, which required a single\n  blank line before a class docstring (#137).\n\n* Configuration files are now handled correctly. The closer a configuration file\n  is to a checked file the more it matters.\n  Configuration files no longer support ``explain``, ``source``, ``debug``,\n  ``verbose`` or ``count`` (#133).\n\nBug Fixes\n\n* On Python 2.x, D302 (\"Use u\"\"\" for Unicode docstrings\") is not reported\n  if `unicode_literals` is imported from `__future__` (#113, #134).\n\n* Fixed a bug where there was no executable for `pep257` on Windows (#73,\n  #136).\n\n\n0.6.0 - July 20th, 2015\n-----------------------\n\nNew Features\n\n* Added support for more flexible error selections using ``--ignore``,\n  ``--select``, ``--convention``, ``--add-ignore`` and ``--add-select``\n  (#96, #123).\n\nBug Fixes\n\n* Property setter and deleter methods are now treated as private and do not\n  require docstrings separate from the main property method (#69, #107).\n\n* Fixed an issue where pep257 did not accept docstrings that are both\n  unicode and raw in Python 2.x (#116, #119).\n\n* Fixed an issue where Python 3.x files with Unicode encodings were\n  not read correctly (#118).\n\n\n0.5.0 - March 14th, 2015\n------------------------\n\nNew Features\n\n* Added check D210: No whitespaces allowed surrounding docstring text (#95).\n\n* Added real documentation rendering using Sphinx (#100, #101).\n\nBug Fixes\n\n* Removed log level configuration from module level (#98).\n\n* D205 used to check that there was *a* blank line between the one line summary\n  and the description. It now checks that there is *exactly* one blank line\n  between them (#79).\n\n* Fixed a bug where ``--match-dir`` was not properly respected (#108, #109).\n\n0.4.1 - January 10th, 2015\n--------------------------\n\nBug Fixes\n\n* Getting ``ImportError`` when trying to run pep257 as the installed script\n  (#92, #93).\n\n\n0.4.0 - January 4th, 2015\n-------------------------\n\n.. warning::\n\n    A fatal bug was discovered in this version (#92). Please use a newer\n    version.\n\nNew Features\n\n* Added configuration file support (#58, #87).\n\n* Added a ``--count`` flag that prints the number of violations found (#86,\n  #89).\n\n* Added support for Python 3.4, PyPy and PyPy3 (#81).\n\nBug Fixes\n\n* Fixed broken tests (#74).\n\n* Fixed parsing various colon and parenthesis combinations in definitions\n  (#82).\n\n* Allow for greater flexibility in parsing ``__all__`` (#67).\n\n* Fixed handling of one-liner definitions (#77).\n\n\n0.3.2 - March 11th, 2014\n------------------------\n\nFirst documented release!\n", "metadata": {"source": "github.com/PyCQA/pydocstyle", "file_path": "docs/release_notes.rst", "language": "restructuredtext", "complexity": "complex", "star_count": 1117, "quality_score": 0.72, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": true, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 519, "word_count": 2418}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": ".. _cli_usage:\n\nUsage\n^^^^^\n\n.. code::\n\n    Usage: pydocstyle [options] [<file|dir>...]\n\n    Options:\n      --version             show program's version number and exit\n      -h, --help            show this help message and exit\n      -e, --explain         show explanation of each error\n      -s, --source          show source for each error\n      -d, --debug           print debug information\n      -v, --verbose         print status information\n      --count               print total number of errors to stdout\n      --config=<path>       use given config file and disable config discovery\n      --match=<pattern>     check only files that exactly match <pattern> regular\n                            expression; default is --match='(?!test_).*\\.py' which\n                            matches files that don't start with 'test_' but end\n                            with '.py'\n      --match-dir=<pattern>\n                            search only dirs that exactly match <pattern> regular\n                            expression; default is --match-dir='[^\\.].*', which\n                            matches all dirs that don't start with a dot\n      --ignore-decorators=<decorators>\n                            ignore any functions or methods that are decorated by\n                            a function with a name fitting the <decorators>\n                            regular expression; default is --ignore-decorators=''\n                            which does not ignore any decorated functions.\n\n      Note:\n        When using --match, --match-dir or --ignore-decorators consider\n        whether you should use a single quote (') or a double quote (\"),\n        depending on your OS, Shell, etc.\n\n      Error Check Options:\n        Only one of --select, --ignore or --convention can be specified. If\n        none is specified, defaults to `--convention=pep257`. These three\n        options select the \"basic list\" of error codes to check. If you wish\n        to change that list (for example, if you selected a known convention\n        but wish to ignore a specific error from it or add a new one) you can\n        use `--add-[ignore/select]` in order to do so.\n\n        --select=<codes>    choose the basic list of checked errors by specifying\n                            which errors to check for (with a list of comma-\n                            separated error codes or prefixes). for example:\n                            --select=D101,D2\n        --ignore=<codes>    choose the basic list of checked errors by specifying\n                            which errors to ignore out of all of the available\n                            error codes (with a list of comma-separated error\n                            codes or prefixes). for example: --ignore=D101,D2\n        --convention=<name>\n                            choose the basic list of checked errors by specifying\n                            an existing convention. Possible conventions: pep257,\n                            numpy, google.\n        --add-select=<codes>\n                            add extra error codes to check to the basic list of\n                            errors previously set by --select, --ignore or\n                            --convention.\n        --add-ignore=<codes>\n                            ignore extra error codes by removing them from the\n                            basic list previously set by --select, --ignore or\n                            --convention.\n\n\n.. note::\n\n    When using any of the ``--select``, ``--ignore``, ``--add-select``, or\n    ``--add-ignore`` command line flags, it is possible to pass a prefix for an\n    error code. It will be expanded so that any code beginning with that prefix\n    will match. For example, running the command ``pydocstyle --ignore=D4``\n    will ignore all docstring content issues as their error codes beginning with\n    \"D4\" (i.e. D400, D401, D402, D403, and D404).\n\nReturn Code\n^^^^^^^^^^^\n\n+--------------+--------------------------------------------------------------+\n| 0            | Success - no violations                                      |\n+--------------+--------------------------------------------------------------+\n| 1            | Some code violations were found                              |\n+--------------+--------------------------------------------------------------+\n| 2            | Illegal usage - see error message                            |\n+--------------+--------------------------------------------------------------+\n", "metadata": {"source": "github.com/PyCQA/pydocstyle", "file_path": "docs/snippets/cli.rst", "language": "restructuredtext", "complexity": "medium", "star_count": 1117, "quality_score": 0.69, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": true, "has_headers": false, "line_count": 87, "word_count": 462}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate configuration for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "``pydocstyle`` supports *ini*-like and *toml* configuration files.\nIn order for ``pydocstyle`` to use a configuration file automatically, it must\nbe named one of the following options.\n\n* ``setup.cfg``\n* ``tox.ini``\n* ``.pydocstyle``\n* ``.pydocstyle.ini``\n* ``.pydocstylerc``\n* ``.pydocstylerc.ini``\n* ``pyproject.toml``\n\nWhen searching for a configuration file, ``pydocstyle`` looks for one of the\nfile specified above *in that exact order*. *ini*-like configuration files must\nhave a ``[pydocstyle]`` section while *toml* configuration files must have a\n``[tool.pydocstyle]`` section. If a configuration file was not found,\n``pydocstyle`` keeps looking for one up the directory tree until one is found\nor uses the default configuration.\n\n.. note::\n\n    *toml* configuration file support is only enabled if the ``toml`` python\n    package is installed. You can ensure that this is the case by installing\n    the ``pydocstyle[toml]`` optional dependency.\n\n.. note::\n\n    For backwards compatibility purposes, **pydocstyle** supports configuration\n    files named ``.pep257``, as well as section header ``[pep257]``. However,\n    these are considered deprecated and support will be removed in the next\n    major version.\n\nAvailable Options\n#################\n\nNot all configuration options are available in the configuration files.\nAvailable options are:\n\n* ``convention``\n* ``select``\n* ``ignore``\n* ``add_select``\n* ``add_ignore``\n* ``match``\n* ``match_dir``\n* ``ignore_decorators``\n* ``property_decorators``\n* ``ignore_self_only_init``\n\nSee the :ref:`cli_usage` section for more information.\n\nInheritance\n###########\n\nBy default, when finding a configuration file, ``pydocstyle`` tries to inherit\nthe parent directory's configuration and merge them to the local ones.\n\nThe merge process is as follows:\n\n* If one of ``select``, ``ignore`` or ``convention`` was specified in the child\n  configuration - Ignores the parent configuration and set the new error codes\n  to check. Otherwise, simply copies the parent checked error codes.\n* If ``add-ignore`` or ``add-select`` were specified, adds or removes the\n  specified error codes from the checked error codes list.\n* If ``match`` or ``match-dir`` were specified - use them. Otherwise, use the\n  parent's.\n\nIn order to disable this (useful for configuration files located in your repo's\nroot), simply add ``inherit=false`` to your configuration file.\n\n\n.. note::\n\n  If any of ``select``, ``ignore`` or ``convention`` were specified in\n  the CLI, the configuration files will take no part in choosing which error\n  codes will be checked. ``match`` and ``match-dir`` will still take effect.\n\nExample\n#######\n\n.. code::\n\n    [pydocstyle]\n    inherit = false\n    ignore = D100,D203,D405\n    match = .*\\.py\n\n", "metadata": {"source": "github.com/PyCQA/pydocstyle", "file_path": "docs/snippets/config.rst", "language": "restructuredtext", "complexity": "medium", "star_count": 1117, "quality_score": 0.6599999999999999, "features": {"has_code_examples": true, "has_installation_guide": true, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": true, "has_troubleshooting": true, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 88, "word_count": 377}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate function-based implementation for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "``pydocstyle`` supports inline commenting to skip specific checks on\nspecific functions or methods. The supported comments that can be added are:\n\n1. ``\"# noqa\"`` skips all checks.\n\n2. ``\"# noqa: D102,D203\"`` can be used to skip specific checks. Note that\n   this is compatible with skips from `flake8 <http://flake8.pycqa.org/>`_,\n   e.g. ``# noqa: D102,E501,D203``.\n\nFor example, this will skip the check for a period at the end of a function\ndocstring::\n\n    >>> def bad_function():  # noqa: D400\n    ...     \"\"\"Omit a period in the docstring as an exception\"\"\"\n    ...     pass\n", "metadata": {"source": "github.com/PyCQA/pydocstyle", "file_path": "docs/snippets/in_file.rst", "language": "restructuredtext", "complexity": "medium", "star_count": 1117, "quality_score": 0.69, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": false, "has_links": true, "has_images": false, "has_tables": false, "has_headers": true, "line_count": 16, "word_count": 87}, "authority_level": "medium", "authentic": true}}
{"domain": "technical_documentation", "category": "code_comments", "prompt": "### Technical_Documentation Request (CLAUDE.md):\nCreate class-based implementation for Create detailed code documentation with docstrings and comments with practical examples and code samples\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "The D1xx group of errors deals with missing docstring in public constructs:\nmodules, classes, methods, etc. It is important to note how publicity is\ndetermined and what its effects are.\n\n\nHow publicity is determined\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nPublicity for all constructs is determined as follows: a construct is\nconsidered *public* if -\n\n1. Its immediate parent is public *and*\n2. Its name does *not* start with a single or double underscore.\n\n    a. Note, names that start and end with a double underscore are *public* (e.g. ``__init__.py``).\n\nA construct's immediate parent is the construct that contains it. For example,\na method's parent is a class object. A class' parent is usually a module, but\nmight also be a function, method, etc. A module can either have no parent, or\nit can have a parent that is a package.\n\nIn order for a construct to be considered public, its immediate parent must\nalso be public. Since this definition is recursive, it means that *all* of its\nparents need to be public. The corollary is that if a construct is considered\nprivate, then all of its descendants are also considered private. For example,\na class called ``_Foo`` is considered private. A method ``bar`` in ``_Foo`` is\nalso considered private since its parent is a private class, even though its\nname does not begin with a single underscore.\n\nNote, a module's parent is recursively checked upward until we reach a directory\nin ``sys.path`` to avoid considering the complete filepath of a module.\nFor example, consider the module ``/_foo/bar/baz.py``.\nIf ``PYTHONPATH`` is set to ``/``, then ``baz.py`` is *private*.\nIf ``PYTHONPATH`` is set to ``/_foo/``, then ``baz.py`` is *public*.\n\nModules are parsed to look if ``__all__`` is defined. If so, only those top\nlevel constructs are considered public. The parser looks for ``__all__``\ndefined as a literal list or tuple. As the parser doesn't execute the module,\nany mutation of ``__all__`` will not be considered.\n\n\nHow publicity affects error reports\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe immediate effect of a construct being determined as private is that no\nD1xx errors will be reported for it (or its children, as the previous section\nexplains). A private method, for instance, will not generate a D102 error, even\nif it has no docstring.\n\nHowever, it is important to note that while docstring are optional for private\nconstruct, they are still required to adhere to your style guide. So if a\nprivate module `_foo.py` does not have a docstring, it will not generate a\nD100 error, but if it *does* have a docstring, that docstring might generate\nother errors.\n", "metadata": {"source": "github.com/PyCQA/pydocstyle", "file_path": "docs/snippets/publicity.rst", "language": "restructuredtext", "complexity": "medium", "star_count": 1117, "quality_score": 0.63, "features": {"has_code_examples": true, "has_installation_guide": false, "has_usage_examples": true, "has_api_documentation": false, "has_configuration": false, "has_troubleshooting": true, "has_links": false, "has_images": false, "has_tables": false, "has_headers": false, "line_count": 55, "word_count": 424}, "authority_level": "medium", "authentic": true}}
