{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Sole-tenancy overview\n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Home Compute Engine Documentation Guides Send feedback Sole-tenancy overview Stay organized with collections Save and categorize content based on your preferences. Linux Windows This document describes sole-tenant nodes. For information about how to provision VMs on sole-tenant nodes, see Provisioning VMs on sole-tenant nodes. Sole-tenancy lets you have exclusive access to a sole-tenant node, which is a physical Compute Engine server that is dedicated to hosting only your project's VMs. Use sole-tenant nodes to keep your VMs physically separated from VMs in other projects, or to group your VMs together on the same host hardware as shown in the following diagram. You can also create a sole-tenant node group and specify whether you want to share it with other projects or with the entire organization. Figure 1: A multi-tenant host versus a sole-tenant node. VMs running on sole-tenant nodes can use the same Compute Engine features as other VMs, including transparent scheduling and block storage, but with an added layer of hardware isolation. To give you full control over the VMs on the physical server, each sole-tenant node maintains a one-to-one mapping to the physical server that is backing the node. Within a sole-tenant node, you can provision multiple VMs on machine types of various sizes, which lets you efficiently use the underlying resources of the dedicated host hardware. Also, if you choose not to share the host hardware with other projects, you can meet security or compliance requirements with workloads that require physical isolation from other workloads or VMs. If your workload requires sole tenancy only temporarily, you can modify VM tenancy as necessary. Sole-tenant nodes can help you meet dedicated hardware requirements for bring your own license (BYOL) scenarios that require per-core or per-processor licenses. When you use sole-tenant nodes, you have some visibility into the underlying hardware, which lets you track core and processor usage. To track this usage, Compute Engine reports the ID of the physical server on which a VM is scheduled. Then, by using Cloud Logging, you can view the historical server usage of a VM. To optimize the use of the host hardware, you can do the following: Overcommit CPUs on sole-tenant VMs Share sole-tenant node groups Manually live migrate sole-tenant VMs Through a configurable host maintenance policy, you can control the behavior of sole-tenant VMs while their host is undergoing maintenance. You can specify when maintenance occurs, and whether the VMs maintain affinity with a specific physical server or are moved to other sole-tenant nodes within a node group. Workload considerations The following types of workloads might benefit from using sole-tenant nodes: Gaming workloads with performance requirements Finance or healthcare workloads with security and compliance requirements Windows workloads with licensing requirements Machine learning, data processing, or image rendering workloads. For these workloads, consider reserving GPUs. Workloads requiring increased I/O operations per second (IOPS) and decreased latency, or workloads that use temporary storage in the form of caches, processing space, or low-value data. For these workloads, consider reserving Local SSD disks. Node templates A node template is a regional resource that defines the properties of each node in a node group. When you create a node group from a node template, the properties of the node template are immutably copied to each node in the node group. When you create a node template you must specify a node type. You can optionally specify node affinity labels when you create a node template. You can only specify node affinity labels on a node template. You can't specify node affinity labels on a node group. Node types When configuring a node template, specify a node type to apply to all nodes within a node group created based on the node template. The sole-tenant node type, referenced by the node template, specifies the total amount of vCPU cores and memory for nodes created in node groups that use that template. For example, the n2-node-80-640 node type has 80 vCPUs and 640 GB of memory. The VMs that you add to a sole-tenant node must have the same machine type as the node type that you specify in the node template. For example, n2 sole-tenant node types are only compatible with VMs created with the n2 machine type. You can add VMs to a sole-tenant node until the total amount of vCPUs or memory exceeds the capacity of the node. When you create a node group using a node template, each node in the node group inherits the node template's node type specifications. A node type applies to each individual node within a node group, not to all of the nodes in the group uniformly. So, if you create a node group with two nodes that are both of the n2-node-80-640 node type, each node is allocated 80 vCPUs and 640 GB of memory. Depending on your workload requirements, you might fill the node with multiple smaller VMs running on machine types of various sizes, including predefined machine types, custom machine types, and machine types with extended memory. When a node is full, you cannot schedule additional VMs on that node. The following table displays the available node types. To see a list of the node types available for your project, run the gcloud compute sole-tenancy node-types list command or create a nodeTypes.list REST request. Node type Processor vCPU GB vCPU:GB Sockets Cores:Socket Total cores Max VMs allowed c2-node-60-240 Cascade Lake 60 240 1:4 2 18 36 15 c3-node-176-352 Sapphire Rapids 176 352 1:2 2 48 96 44 c3-node-176-704 Sapphire Rapids 176 704 1:4 2 48 96 44 c3-node-176-1408 Sapphire Rapids 176 1408 1:8 2 48 96 44 c3d-node-360-708 AMD EPYC Genoa 360 708 1:2 2 96 192 34 c3d-node-360-1440 AMD EPYC Genoa 360 1440 1:4 2 96 192 40 c3d-node-360-2880 AMD EPYC Genoa 360 2880 1:8 2 96 192 40 c4-node-192-384 Emerald Rapids 192 384 1:2 2 60 120 40 c4-node-192-720 Emerald Rapids 192 720 1:3.75 2 60 120 30 c4-node-192-1488 Emerald Rapids 192 1,488 1:7.75 2 60 120 30 c4a-node-72-144 Google Axion 72 144 1:2 1 80 80 22 c4a-node-72-288 Google Axion 72 288 1:4 1 80 80 22 c4a-node-72-576 Google Axion 72 576 1:8 1 80 80 36 c4d-node-384-720 AMD EPYC Turin 384 744 1:2 2 96 192 24 c4d-node-384-1488 AMD EPYC Turin 384 1488 1:4 2 96 192 25 c4d-node-384-3024 AMD EPYC Turin 384 3024 1:8 2 96 192 25 g2-node-96-384 Cascade Lake 96 384 1:4 2 28 56 8 g2-node-96-432 Cascade Lake 96 432 1:4.5 2 28 56 8 h3-node-88-352 Sapphire Rapids 88 352 1:4 2 48 96 1 m1-node-96-1433 Skylake 96 1433 1:14.93 2 28 56 1 m1-node-160-3844 Broadwell E7 160 3844 1:24 4 22 88 4 m2-node-416-8832 Cascade Lake 416 8832 1:21.23 8 28 224 1 m2-node-416-11776 Cascade Lake 416 11776 1:28.31 8 28 224 2 m3-node-128-1952 Ice Lake 128 1952 1:15.25 2 36 72 2 m3-node-128-3904 Ice Lake 128 3904 1:30.5 2 36 72 2 m4-node-224-2976 Emerald Rapids 224 2976 1:13.3 2 112 224 1 m4-node-224-5952 Emerald Rapids 224 5952 1:26.7 2 112 224 1 n1-node-96-624 Skylake 96 624 1:6.5 2 28 56 96 n2-node-80-640 Cascade Lake 80 640 1:8 2 24 48 80 n2-node-128-864 Ice Lake 128 864 1:6.75 2 36 72 128 n2d-node-224-896 AMD EPYC Rome 224 896 1:4 2 64 128 112 n2d-node-224-1792 AMD EPYC Milan 224 1792 1:8 2 64 128 112 n4-node-224-1372 Emerald Rapids 224 1372 1:6 2 60 120 90 For information about the prices of these node types, see sole-tenant node pricing. All nodes let you schedule VMs of different shapes. Node n type are general-purpose nodes, on which you can schedule custom machine type instances. For recommendations about which node type to choose, see Recommendations for machine types. For information about performance, see CPU platforms. Node groups and VM provisioning Sole-tenant node templates define the properties of a node group, and you must create a node template before creating a node group in a Google Cloud zone. When you create a group, specify the host maintenance policy for VMs on the node group, the number of nodes for the node group, and whether to share it with other projects or with the entire organization. A node group can have zero or more nodes; for example, you can reduce the number of nodes in a node group to zero when you don't need to run any VMs on nodes in the group, or you can enable the node group autoscaler to manage the size of the node group automatically. Before provisioning VMs on sole-tenant nodes, you must create a sole-tenant node group. A node group is a homogeneous set of sole-tenant nodes in a specific zone. Node groups can contain multiple VMs from the same machine series running on machine types of various sizes, as long as the machine type has 2 or more vCPUs. When you create a node group, enable autoscaling so that the size of the group adjusts automatically to meet the requirements of your workload. If your workload requirements are static, you can manually specify the size of the node group. After creating a node group, you can provision VMs on the group or on a specific node within the group. For further control, use node affinity labels to schedule VMs on any node with matching affinity labels. After you've provisioned VMs on node groups, and optionally assigned affinity labels to provision VMs on specific node groups or nodes, consider labeling your resources to help manage your VMs. Labels are key-value pairs that can help you categorize your VMs so that you can view them in aggregate for reasons such as billing. For example, you can use labels to mark the role of a VM, its tenancy, the license type, or its location. Host maintenance policy Depending on your licensing scenarios and workloads, you might want to limit the number of physical cores used by your VMs. The host maintenance policy you choose might depend on, for example, your licensing or compliance requirements, or, you might want to choose a policy that lets you limit usage of physical servers. With all of these policies, your VMs remain on dedicated hardware. When you schedule VMs on sole-tenant nodes, you can choose from the following three different host maintenance policy options, which let you determine how and whether Compute Engine live migrates VMs during host events, which occur approximately every 4 to 6 weeks. During maintenance, Compute Engine live migrates, as a group, all of the VMs on the host to a different sole-tenant node, but, in some cases, Compute Engine might break up the VMs into smaller groups and live migrate each smaller group of VMs to separate sole-tenant nodes. Default host maintenance policy This is the default host maintenance policy, and VMs on nodes groups configured with this policy follow traditional maintenance behavior for non-sole-tenant VMs. That is, depending on the on-host maintenance setting of the VM's host, VMs live migrate to a new sole-tenant node in the node group before a host maintenance event, and this new sole-tenant node only runs the customer's VMs. This policy is most suitable for per-user or per-device licenses that require live migration during host events. This setting doesn't restrict migration of VMs to within a fixed pool of physical servers, and is recommended for general workloads without physical server requirements and that don't require existing licenses. Because VMs live migrate to any server without considering existing server affinity with this policy, this policy is not suitable for scenarios requiring minimization of the use of physical cores during host events. The following figure shows an animation of the Default host maintenance policy. Figure 2: Animation of the Default host maintenance policy. Restart in place host maintenance policy When you use this host maintenance policy, Compute Engine stops VMs during host events, and then restarts the VMs on the same physical server after the host event. You must set the VM's on host maintenance setting to TERMINATE when using this policy. This policy is most suitable for workloads that are fault-tolerant and can experience approximately one hour of downtime during host events, workloads that must remain on the same physical server, workloads that don't require live migration, or if you have licenses that are based on the number of physical cores or processors. With this policy, the instance can be assigned to the node group using node-name, node-group-name, or node affinity label. The following figure shows an animation of the Restart in place maintenance policy. Figure 3: Animation of the Restart in place host maintenance policy. Migrate within node group host maintenance policy When using this host maintenance policy, Compute Engine live migrates VMs within a fixed-sized group of physical servers during host events, which helps limit the number of unique physical servers used by the VM. This policy is most suitable for high-availability workloads with licenses that are based on the number of physical cores or processors, because with this host maintenance policy, each sole-tenant node in the group is pinned to a fixed set of physical servers, which is different than the default policy that lets VMs migrate to any server. To confirm the capacity for live migration, Compute Engine reserves 1 holdback node for every 20 nodes that you reserve. The following figure shows an animation of the Migrate within node group host maintenance policy. Figure 4: Animation of the Migrate within node group host maintenance policy. The following table shows how many holdback nodes Compute Engine reserves depending on how many nodes you reserve for your node group. Total nodes in group Holdback nodes reserved for live migration 1 Not applicable. Must reserve at least 2 nodes. 2 to 20 1 21 to 40 2 41 to 60 3 61 to 80 4 81 to 100 5 Pin an instance to multiple node groups You can pin an instance to multiple node groups using the node-group-name affinity label under the following conditions: The instance that you want to pin is using a default host maintenance policy (Migrate VM instance). The host maintenance policy of all the node groups that you want to pin the instance to is migrate within node group. If you try to pin an instance to node groups with different host maintenance policies, the operation fails with an error. For example, if you want to pin an instance test-node to two node groups node-group1 and node-group2, verify the following: The host maintenance policy of test-node is Migrate VM instance. The host maintenance policy of node-group1 and node-group2 is migrate within node group. Note: If the instance's on host maintenance policy is Terminate, you can pin the instance to only a single node group. You cannot assign an instance to any specific node with the affinity label node-name. You can use any custom node affinity labels for your instances as long as they are assigned the node-group-name and not the node-name. Maintenance windows If you are managing workloads—for example—finely tuned databases, that might be sensitive to the performance impact of live migration, then you can determine when maintenance begins on a sole-tenant node group by specifying a maintenance window when you create the node group. You can't modify the maintenance window after you create the node group. Maintenance windows are 4-hour blocks of time that you can use to specify when Google performs maintenance on your sole-tenant VMs. Maintenance events occur approximately once every 4 to 6 weeks. The maintenance window applies to all VMs in the sole-tenant node group, and it only specifies when the maintenance begins. Maintenance is not guaranteed to finish during the maintenance window, and there is no guarantee on how frequently maintenance occurs. Maintenance windows are not supported on node groups with the Migrate within node group host maintenance policy. Simulate a host maintenance event You can simulate a host maintenance event to test how your workloads that are running on sole-tenant nodes behave during a host maintenance event. This lets you see the effects of the sole-tenant VM's host maintenance policy on the applications running on the VMs. Host errors When there is a rare critical hardware failure on the host—sole-tenant or multi-tenant—Compute Engine does the following: Retires the physical server and its unique identifier. Revokes your project's access to the physical server. Replaces the failed hardware with a new physical server that has a new unique identifier. Moves the VMs from the failed hardware to the replacement node. Restarts the affected VMs if you configured them to automatically restart. Node affinity and anti-affinity Sole-tenant nodes make sure that your VMs don't share host with VMs from other projects unless you use shared sole-tenant node groups. With shared sole-tenant node groups, other projects within the organization can provision VMs on the same host. However, you still might want to group several workloads together on the same sole-tenant node or isolate your workloads from one another on different nodes. For example, to help meet some compliance requirements, you might need to use affinity labels to separate sensitive workloads from non-sensitive workloads. When you create a VM, you request sole-tenancy by specifying node affinity or anti-affinity, referencing one or more node affinity labels. You specify custom node affinity labels when you create a node template, and Compute Engine automatically includes some default affinity labels on each node. By specifying affinity when you create a VM, you can schedule VMs together on a specific node or nodes in a node group. By specifying anti-affinity when you create a VM, you can make sure that certain VMs are not scheduled together on the same node or nodes in a node group. Node affinity labels are key-value pairs assigned to nodes, and are inherited from a node template. Affinity labels let you: Control how individual VM instances are assigned to nodes. Control how VM instances created from a template, such as those created by a managed instance group, are assigned to nodes. Group sensitive VM instances on specific nodes or node groups, separate from other VMs. Default affinity labels Compute Engine assigns the following default affinity labels to each node: A label for the node group name: Key: compute.googleapis.com/node-group-name Value: Name of the node group. A label for the node name: Key: compute.googleapis.com/node-name Value: Name of the individual node. A label for the projects the node group is shared with: Key: compute.googleapis.com/projects Value: Project ID of the project containing the node group. Custom affinity labels You can create custom node affinity labels when you create a node template. These affinity labels are assigned to all nodes in node groups created from the node template. You can't add more custom affinity labels to nodes in a node group after the node group has been created. For information about how to use affinity labels, see Configuring node affinity. Pricing To help you to minimize the cost of your sole-tenant nodes, Compute Engine provides committed use discounts (CUDs) and sustained use discounts (SUDs). Note that for sole-tenancy premium charges, you can receive only flexible CUDs and SUDs, but not resource-based CUDs. Because you are already billed for the vCPU and memory of your sole-tenant nodes, you don't pay extra for the VMs that you create on those nodes. If you provision sole-tenant nodes with GPUs or Local SSD disks, you are billed for all of the GPUs or Local SSD disks on each node that you provision. The sole-tenancy premium is based only on the vCPUs and memory that you use for the sole-tenant node, and doesn't include GPUs or Local SSD disks. Availability Sole-tenant nodes are available in select zones. To verify high-availability, schedule VMs on sole-tenant nodes in different zones. Before using GPUs or Local SSD disks on sole-tenant nodes, make sure you have enough GPU or Local SSD quota in the zone where you are reserving the resource. Compute Engine supports GPUs on n1 and g2 sole-tenant node types that are in zones with GPU support. The following table shows the types of GPUs that you can attach to n1 and g2 nodes and how many GPUs you must attach when you create the node template. GPU type GPU quantity Sole-tenant node type NVIDIA L4 8 g2 NVIDIA P100 4 n1 NVIDIA P4 4 n1 NVIDIA T4 4 n1 NVIDIA V100 8 n1 Compute Engine supports Local SSD disks on n1, n2, n2d, and g2 sole-tenant node types that are used in zones that support those machine series. Restrictions You can't use sole-tenant VMs with the follow machine series and types: T2D, T2A, E2, C2D, A2, A3, A4, A4X, or bare metal instances. Sole-tenant VMs can't specify a minimum CPU platform. You can't migrate a VM to a sole-tenant node if that VM specifies a minimum CPU platform. To migrate a VM to a sole-tenant node, remove the minimum CPU platform specification by setting it to automatic before updating the VM's node affinity labels. Sole-tenant nodes don't support preemptible VM instances. For information about the limitations of using Local SSD disks on sole-tenant nodes, see Local SSD data persistence. For information about how using GPUs affects live migration, see the limitations of live migration. Sole-tenant nodes with GPUs don't support VMs without GPUs. Only N1, N2, N2D, and N4 sole-tenant nodes support overcommitting CPUs. C3 and C4 sole-tenant nodes require that VMs have the same vCPU-to-memory ratio as the node type—for example—you can't place a c3-standard VM on a -highmem node type. You can't update the maintenance policy on a live node group. What's next Learn how to create, configure, and consume your sole-tenant nodes. Learn how to overcommit CPUs on sole-tenant VMs. Learn how to bring your own licenses. Review our best practices for using sole-tenant nodes to run VM workloads. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.  2025-07-23 UTC.", "metadata": {"source": "https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 3656, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9700000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for What is Active Assist\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Home Recommender Documentation Send feedback Stay organized with collections Save and categorize content based on your preferences. What is Active Assist Active Assist refers to the portfolio of tools used in Google Cloud to generate recommendations and insights to help you optimize your Google Cloud projects. This includes recommenders that generate recommendations and insights and analysis tools. Recommenders and Recommendations Recommenders generate recommendations that fall into six value categories that can help you optimize your cloud in a variety of ways. See Recommendations and Recommenders for a detailed explanation of these concepts. Grant permissions to view and update recommendations and insights Each recommender and insight type has specific roles and permissions to control access to its recommendations and insights. In order to enable users to review and assess these recommendations and insights, they will include some metadata about resources. Granting these permissions provides users with a partial view of the resource's metadata. This partial view of data is particularly important to consider if you are using custom roles to grant permissions. For example, the Identity and Access Management recommender provides recommendations about permissions. Members that have the recommender.iamPolicyRecommendations.get and recommender.iamPolicyRecommendations.list permissions can also see information about your IAM policy bindings. Ensure that you have the necessary roles and permissions to view recommendations. Find recommendations and insights Each recommender lets you view and manage its recommendations and insights using one or more of the following clients: Recommendation Hub In context using the service's user interface (UI) in the Google Cloud console REST API or Google Cloud CLI For details about recommenders and supported clients, see Recommenders. View recommendations in Recommendation Hub Preview — Recommendation Hub This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the Service Specific Terms. Pre-GA features are available \"as is\" and might have limited support. For more information, see the launch stage descriptions. View your recommendations Certain recommendations such as the IAM Recommendation require different levels of permissions to view. For more information, see the guide for the specific Recommendation that you are looking for. Manage recommendations and insights In Recommendation Hub, you can choose to apply or dismiss the recommendations. Some recommendations can be applied automatically, while other recommendations require additional steps to complete. Any additional steps will appear in the details panel. Apply recommendations Before applying recommendations, ensure that they are reviewed by someone who can properly assess the impacts of changes. Recommender provides information on direct impacts in areas such as cost, performance, or security. Recommendation reviewers should have a holistic understanding of your infrastructure and processes so that other business-specific impacts are considered. Warning: Before applying recommendations in the Google Cloud console or using the API, ensure that impacts are assessed by a reviewer. The reviewer should have the knowledge to assess impacts identified in recommendations, as well as impacts specific to your infrastructure and business. Applying recommendations without proper assessment could result in unexpected changes, such as issues with system performance, poor reliability, or loss of required permissions. If you choose to apply recommendations without human review, ensure that you have set up a rollback process before making any changes. Get started with recommendations Recommendations are an easy way to optimize your cloud to help you maintain a secure and cost effective workspace. Use the Recommendation Hub quickstart guide to get started with the Recommendation Hub. Alternatively, you can follow the In Context quickstart guide to find recommendations directly in your service pages. Recommendations can also be reviewed and applied or dismissed using the gCloud CL or REST API. Intelligence Centers and Tools Active Assist offers intelligent tools that helps you proactively monitor and manage your cloud. We offer tools such as the Network Intelligence Center and Policy Simulator to simplify and automate your management experience on Google Cloud. Read the following documentation pages for more information on how our products can help you optimize your cloud experience. Network Intelligence Centers Policy Simulator Policy Analyzer Policy Troubleshooter Firewall Insights Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.  2025-07-18 UTC.", "metadata": {"source": "https://cloud.google.com/recommender/docs/whatis-activeassist", "language": "markdown", "complexity": "medium", "type": "official_site", "word_count": 714, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9300000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Knative is an Open-Source Enterprise-level\n            solution to build Serverless and Event Driven Applications\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Knative is an Open-Source Enterprise-level solution to build Serverless and Event Driven Applications Try the tutorial Install Knative on your cluster Trusted by Need to know more? Learn about how Knative works with Kubernetes and the underlying ideas behind containers and serverless infrastructure. Explore Knative Why serverless containers? Serverless refers to running back-end programs and processes in the cloud. Serverless works on an as-used basis, meaning that companies only pay for what they use. Knative is a platform-agnostic solution for running serverless deployments. Simpler Abstractions Simplify your YAML with our custom CRDs Autoscaling Scale down to zero and up from zero Progressive Rollouts Choose your rollout strategy depending on your needs Event Integrations Handle events from many sources Handle Events Trigger handlers from the event broker Plugable Kubernetes native to be integrated and extended Knative Components Knative has two main components that empower teams working with Kubernetes. Serving and Eventing work together to automate and manage tasks and applications. Run serverless containers in Kubernetes with ease. Knative takes care of the details of networking, autoscaling (even to zero), and revision tracking. Teams can focus on core logic using any programming language. Universal subscription, delivery and management of events. Build modern apps by attaching compute to a data stream with declarative event connectivity and developer friendly object models. Run serverless containers in Kubernetes with ease. Knative takes care of the details of networking, autoscaling (even to zero), and revision tracking. Universal subscription, delivery and management of events. Build modern apps by attaching compute to a data stream with declarative event connectivity and developer-friendly object models. Trying it all together Developers focus on code From container to URL with less YAML config Scale up and down Scale down to zero based on traffic patterns Cloud agnostic Control traffic shifting for smoother rollouts Deploy Knative Today Case Studies \"Knative allowed the team to centralize their system, scale, audit, and even select events while enforcing policies and simplifying the architecture\" \"The introduction of the ML training infrastructure using Knative Eventing has enabled us to establish a well-defined operational boundary for the service teams.\" \"The power of Knative’s eventing and serverless features allows PNC to bridge processes between Apache Kafka and CI/CD toolchain events and achieve this automated state.\" Knative is a Cloud Native Computing Foundation incubation project Back to top", "metadata": {"source": "https://knative.dev/docs/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 386, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8625, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.5}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Config Connector overview\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Home Config Connector Documentation Guides Send feedback Stay organized with collections Save and categorize content based on your preferences. Config Connector overview Standard Config Connector is an open source Kubernetes add-on that lets you manage Google Cloud resources through Kubernetes. Many cloud-native development teams work with a mix of configuration systems, APIs, and tools to manage their infrastructure. This mix is often difficult to understand, leading to reduced velocity and expensive mistakes. Config Connector provides a method to configure many Google Cloud services and resources using Kubernetes tooling and APIs. With Config Connector, your environments can use Kubernetes-managed Resources including: RBAC for access control. Events for visibility. Single source of configuration and desired state management for reduced complexity. Eventual consistency for loosely coupling dependencies. You can manage your Google Cloud infrastructure the same way you manage your Kubernetes applications, reducing the complexity and cognitive load for developers. How Config Connector works Config Connector provides a collection of Kubernetes Custom Resource Definitions (CRDs) and controllers. The Config Connector CRDs allow Kubernetes to create and manage Google Cloud resources when you configure and apply Objects to your cluster. For Config Connector CRDs to function correctly, Config Connector deploys Pods to your nodes that have elevated RBAC permissions, such as the ability to create, delete, get, and list CustomResourceDefinitions (CRDs). These permissions are required for Config Connector to create and reconcile Kubernetes resources. To get started, install Config Connector and create your first resource. Config Connector's controllers eventually reconcile your environment with your desired state. Customizing Config Connector's behavior Config Connector provides additional features beyond creating resources. For example, you can manage existing Google Cloud resources, and use Kubernetes Secrets to provide sensitive data, such as passwords, to your resources. For more information, see the list of how-to guides. In addition, you can learn more about how Config Connector uses Kubernetes constructs to manage Resources and see the Google Cloud resources Config Connector can manage. What's next Install Config Connector. Get started by creating your first resource. Explore Config Connector source code. Config Connector is fully open sourced on GitHub. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.  2025-07-18 UTC.", "metadata": {"source": "https://cloud.google.com/config-connector/docs/overview", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 397, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9175000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for OpenCue getting started guide\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "OpenCue getting started guide Guides for system admins deploying OpenCue components and installing dependencies The guides in this section provide detailed instructions for installing OpenCue in a production environment. If you’d like to quickly try out OpenCue to evaluate the project for your needs, see the Quick starts. OpenCue relies on a lot of components and project-specific terminology. If you’re new to OpenCue, you might find it useful to learn how some of the components work together. For more information, see the OpenCue overview and Glossary. Setting up the database Set up the OpenCue database Deploying Cuebot Deploy Cuebot to a server from source or a Docker image Deploying RQD Deploy RQD to all OpenCue render hosts Checking out the source code Check out the OpenCue source code Installing PyCue and PyOutline Install the OpenCue Python API and related Python library Installing CueGUI Install CueGUI to monitor, manage, and troubleshoot jobs Installing CueSubmit Install CueSubmit to submit render jobs Installing CueAdmin Install the CueAdmin command-line client Feedback Was this page helpful? Yes No Glad to hear it! Please tell us how we can improve. Sorry to hear that. Please tell us how we can improve. Last modified Tuesday, March 31, 2020: Add an introduction and relevant links to Getting Started. (#172) (a6fbde0)", "metadata": {"source": "https://www.opencue.io/docs/getting-started/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 212, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8574999999999999, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.5}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Terraform blueprints and modules for Google Cloud\n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Home Documentation Terraform on Google Cloud Guides Send feedback Terraform blueprints and modules for Google Cloud Stay organized with collections Save and categorize content based on your preferences. Blueprints and modules help you automate provisioning and managing Google Cloud resources at scale. A module is a reusable set of Terraform configuration files that creates a logical abstraction of Terraform resources. A blueprint is a package of deployable, reusable modules and policy that implements and documents a specific opinionated solution. Deployable configuration for all Terraform blueprints are packaged as Terraform modules. Select a category Compute Containers Data analytics Databases Developer tools End-to-end Healthcare and life sciences Networking Operations Security and identity Serverless computing Storage Workspace Category Blueprints and modules Description End-to-end, Data analytics ai-notebook Demonstrates how to protect confidential data in Vertex AI Workbench notebooks Data analytics, End-to-end crmint Deploy the marketing analytics application, CRMint End-to-end, Operations enterprise-application Deploy an enterprise developer platform on Google Cloud End-to-end, Operations example-foundation Shows how the CFT modules can be composed to build a secure cloud foundation End-to-end fabric Provides advanced examples designed for prototyping Developer tools, End-to-end, Security and identity secure-cicd Builds a secure CI/CD pipeline on Google Cloud End-to-end, Data analytics secured-data-warehouse Deploys a secured BigQuery data warehouse Data analytics, End-to-end, Security and identity secured-data-warehouse-onprem-ingest Deploys a secured data warehouse variant for ingesting encrypted data from on-prem sources End-to-end vertex-mlops Create a Vertex AI environment needed for MLOps Networking address Manages Google Cloud IP addresses Databases alloy-db Creates an AlloyDB for PostgreSQL instance Data analytics analytics-lakehouse Deploys a Lakehouse Architecture Solution Compute anthos-vm Creates VMs on Google Distributed Cloud clusters Developer tools apphub Creates and manages App Hub resources Containers, Developer tools artifact-registry Create and manage Artifact Registry repositories Developer tools, Operations, Security and identity bastion-host Generates a bastion host VM compatible with OS Login and IAP tunneling that can be used to access internal VMs Compute, Operations backup-dr Deploy Backup and DR appliances Data analytics bigquery Creates opinionated BigQuery datasets and tables Data analytics bigtable Create and manage Google Bigtable resources Developer tools, Operations bootstrap Bootstraps Terraform usage and related CI/CD in a new Google Cloud organization Compute, Networking cloud-armor Deploy Google Cloud Armor security policy Databases cloud-datastore Manages Datastore Developer tools cloud-deploy Create Cloud Deploy pipelines and targets Networking cloud-dns Creates and manages Cloud DNS public or private zones and their records Serverless computing cloud-functions Deploys Cloud Run functions (Gen 2) Networking, Security and identity cloud-ids Deploys a Cloud IDS instance and associated resources Networking cloud-nat Creates and configures Cloud NAT Operations cloud-operations Manages Cloud Logging and Cloud Monitoring Networking cloud-router Manages a Cloud Router on Google Cloud Serverless computing cloud-run Deploys apps to Cloud Run, along with option to map custom domain Databases cloud-spanner Deploys Spanner instances Storage cloud-storage Creates one or more Cloud Storage buckets and assigns basic permissions on them to arbitrary users Developer tools, Serverless computing cloud-workflows Manage Workflows with optional Cloud Scheduler or Eventarc triggers End-to-end, Data analytics, Operations composer Manages Cloud Composer v1 and v2 along with option to manage networking Compute, Containers container-vm Deploys containers on Compute Engine instances Data analytics data-fusion Manages Cloud Data Fusion Data analytics dataflow Handles opinionated Dataflow job configuration and deployments Data analytics datalab Creates DataLab instances with support for GPU instances Data analytics dataplex-auto-data-quality Deploys data quality rules on BigQuery tables across development and production environments using Cloud Build Serverless computing event-function Responds to logging events with a Cloud Run functions Developer tools folders Creates several Google Cloud folders under the same parent Developer tools gcloud Executes Google Cloud CLI commands within Terraform Developer tools github-actions-runners Creates self-hosted GitHub Actions Runners on Google Cloud Developer tools gke-gitlab Installs GitLab on Kubernetes Engine Workspace group Manages Google Groups Operations, Workspace gsuite-export Creates a Compute Engine VM instance and sets up a cronjob to export Google Workspace Admin SDK data to Cloud Logging on a schedule Healthcare and life sciences healthcare Handles opinionated Google Cloud Healthcare datasets and stores Security and identity iam Manages multiple IAM roles for resources on Google Cloud Developer tools jenkins Creates a Compute Engine instance running Jenkins Security and identity kms Allows managing a keyring, zero or more keys in the keyring, and IAM role bindings on individual keys Compute, Containers kubernetes-engine Configures opinionated GKE clusters Networking lb Creates a regional TCP proxy load balancer for Compute Engine by using target pools and forwarding rules Networking lb-http Creates a global HTTP load balancer for Compute Engine by using forwarding rules Networking lb-internal Creates an internal load balancer for Compute Engine by using forwarding rules Networking load-balanced-vms Creates a managed instance group with a load balancer Data analytics log-analysis Stores and analyzes log data Operations log-export Creates log exports at the project, folder, or organization level Operations media-cdn-vod Deploys Media CDN video-on-demand Databases memorystore Creates a fully functional Google Memorystore (redis) instance Compute, Networking netapp-volumes Deploy Google Cloud NetApp Volumes Networking network Sets up a new VPC network on Google Cloud Networking network-forensics Deploys Zeek on Google Cloud Security and identity org-policy Manages Google Cloud organization policies Networking out-of-band-security-3P Creates a 3P out-of-band security appliance deployment Security and identity pam Deploy Privileged Access Manager Operations project-factory Creates an opinionated Google Cloud project by using Shared VPC, IAM, and Google Cloud APIs Data analytics Pub/Sub Creates Pub/Sub topic and subscriptions associated with the topic Compute sap Deploys SAP products Serverless computing scheduled-function Sets up a scheduled job to trigger events and run functions Security and identity secret-manager Creates one or more Google Secret Manager secrets and manages basic permissions for them Networking, Security and identity secure-web-proxy Create and manage Secure Web Proxy on Google Cloud for secured egress web traffic Security and identity service-accounts Creates one or more service accounts and grants them basic roles Operations slo Creates SLOs on Google Cloud from custom Stackdriver metrics capability to export SLOs to Google Cloud services and other systems Databases sql-db Creates a Cloud SQL database instance Compute startup-scripts Provides a library of useful startup scripts to embed in VMs Operations, Security and identity tags Create and manage Google Cloud Tags Developer tools, Operations, Security and identity tf-cloud-agents Creates self-hosted Terraform Cloud Agent on Google Cloud Databases, Serverless computing three-tier-web-app Deploys a three-tier web application using Cloud Run and Cloud SQL Operations utils Gets the short names for a given Google Cloud region Developer tools, Operations, Security and identity vault Deploys Vault on Compute Engine Compute vertex-ai Deploy Vertex AI resources Compute vm Provisions VMs in Google Cloud Networking vpc-service-controls Handles opinionated VPC Service Controls and Access Context Manager configuration and deployments Networking vpn Sets up a Cloud VPN gateway Operations waap Deploys the WAAP solution on Google Cloud Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.  2025-07-18 UTC.", "metadata": {"source": "https://cloud.google.com/docs/terraform/blueprints/terraform-blueprints", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 1160, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9225000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for BigQuery overview\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Home BigQuery Documentation Guides Send feedback Stay organized with collections Save and categorize content based on your preferences. BigQuery overview BigQuery is a fully managed, AI-ready data platform that helps you manage and analyze your data with built-in features like machine learning, search, geospatial analysis, and business intelligence. BigQuery's serverless architecture lets you use languages like SQL and Python to answer your organization's biggest questions with zero infrastructure management. BigQuery provides a uniform way to work with both structured and unstructured data and supports open table formats like Apache Iceberg, Delta, and Hudi. BigQuery streaming supports continuous data ingestion and analysis while BigQuery's scalable, distributed analysis engine lets you query terabytes in seconds and petabytes in minutes. BigQuery offers built-in governance capabilities that let you discover and curate data, and manage metadata and data quality. Through features like semantic search and data lineage, you can find and validate relevant data for analysis. You can share data and AI assets across your organization with the benefits of access control. These features are powered by Dataplex Universal Catalog, which is a unified, intelligent governance solution for data and AI assets in Google Cloud. BigQuery's architecture consists of two parts: a storage layer that ingests, stores, and optimizes data and a compute layer that provides analytics capabilities. These compute and storage layers efficiently operate independently of each other thanks to Google's petabit-scale network that enables the necessary communication between them. Legacy databases usually have to share resources between read and write operations and analytical operations. This can result in resource conflicts and can slow queries while data is written to or read from storage. Shared resource pools can become further strained when resources are required for database management tasks such as assigning or revoking permissions. BigQuery's separation of compute and storage layers lets each layer dynamically allocate resources without impacting the performance or availability of the other. This separation principle lets BigQuery innovate faster because storage and compute improvements can be deployed independently, without downtime or negative impact on system performance. It is also essential to offering a fully managed serverless data warehouse in which the BigQuery engineering team handles updates and maintenance. The result is that you don't need to provision or manually scale resources, leaving you free to focus on delivering value instead of traditional database management tasks. BigQuery interfaces include Google Cloud console interface and the BigQuery command-line tool. Developers and data scientists can use client libraries with familiar programming including Python, Java, JavaScript, and Go, as well as BigQuery's REST API and RPC API to transform and manage data. ODBC and JDBC drivers provide interaction with existing applications including third-party tools and utilities. As a data analyst, data engineer, data warehouse administrator, or data scientist, BigQuery helps you load, process, and analyze data to inform critical business decisions. Get started with BigQuery You can start exploring BigQuery in minutes. Take advantage of BigQuery's free usage tier or no-cost sandbox to start loading and querying data. BigQuery's sandbox: Get started in the BigQuery sandbox, risk-free and at no cost. Google Cloud console quickstart: Familiarize yourself with the power of the BigQuery Console. Public datasets: Experience BigQuery's performance by exploring large, real-world data from the Public Datasets Program. Explore BigQuery BigQuery's serverless infrastructure lets you focus on your data instead of resource management. BigQuery combines a cloud-based data warehouse and powerful analytic tools. BigQuery storage BigQuery stores data using a columnar storage format that is optimized for analytical queries. BigQuery presents data in tables, rows, and columns and provides full support for database transaction semantics (ACID). BigQuery storage is automatically replicated across multiple locations to provide high availability. Learn about common patterns to organize BigQuery resources in the data warehouse and data marts. Learn about datasets, BigQuery's top-level container of tables and views. Load data into BigQuery using: Stream data with the Storage Write API. Batch-load data from local files or Cloud Storage using formats that include: Avro, Parquet, ORC, CSV, JSON, Datastore, and Firestore formats. BigQuery Data Transfer Service automates data ingestion. For more information, see Overview of BigQuery storage. BigQuery analytics Descriptive and prescriptive analysis uses include business intelligence, ad hoc analysis, geospatial analytics, and machine learning. You can query data stored in BigQuery or run queries on data where it lives using external tables or federated queries including Cloud Storage, Bigtable, Spanner, or Google Sheets stored in Google Drive. ANSI-standard SQL queries (SQL:2011 support) including support for joins, nested and repeated fields, analytic and aggregation functions, multi-statement queries, and a variety of spatial functions with geospatial analytics - Geographic Information Systems. Create views to share your analysis. Business intelligence tool support including BI Engine with Looker Studio, Looker, Google Sheets, and 3rd party tools like Tableau and Power BI. BigQuery ML provides machine learning and predictive analytics. BigQuery Studio offers features such as Python notebooks, and version control for both notebooks and saved queries. These features make it easier for you to complete your data analysis and machine learning (ML) workflows in BigQuery. Query data outside of BigQuery with external tables and federated queries. For more information, see Overview of BigQuery analytics. BigQuery administration BigQuery provides centralized management of data and compute resources while Identity and Access Management (IAM) helps you secure those resources with the access model that's used throughout Google Cloud. Google Cloud security best practices provide a solid yet flexible approach that can include traditional perimeter security or more complex and granular defense-in-depth approach. Intro to data security and governance helps you understand data governance, and what controls you might need to secure BigQuery resources. Jobs are actions that BigQuery runs on your behalf to load, export, query, or copy data. Reservations let you switch between on-demand pricing and capacity-based pricing. For more information, see Introduction to BigQuery administration. BigQuery resources Explore BigQuery resources: Release notes provide change logs of features, changes, and deprecations. Pricing for analysis and storage. See also: BigQuery ML, BI Engine, and Data Transfer Service pricing. Locations define where you create and store datasets (regional and multi-region locations). Stack Overflow hosts an engaged community of developers and analysts working with BigQuery. BigQuery Support provides help with BigQuery. Google BigQuery: The Definitive Guide: Data Warehousing, Analytics, and Machine Learning at Scale by Valliappa Lakshmanan and Jordan Tigani, explains how BigQuery works and provides an end-to-end walkthrough on how to use the service. APIs, tools, and references Reference materials for BigQuery developers and analysts: SQL query syntax for details about using GoogleSQL. BigQuery API and client libraries present overviews of BigQuery's features and their use. BigQuery code samples provide hundreds of snippets for client libraries in C#, Go, Java, Node.js, Python, Ruby. Or view the sample browser. DML, DDL, and user-defined functions (UDF) syntax lets you manage and transform your BigQuery data. bq command-line tool reference documents the syntax, commands, flags, and arguments for the bq CLI interface. ODBC / JDBC integration connect BigQuery to your existing tooling and infrastructure. Gemini in BigQuery features Gemini in BigQuery is part of the Gemini for Google Cloud product suite which provides AI-powered assistance to help you work with your data. Gemini in BigQuery provides AI assistance to help you do the following: Explore and understand your data with data insights. Data insights offers an automated, intuitive way to uncover patterns and perform statistical analysis by using insightful queries that are generated from the metadata of your tables. This feature is especially helpful in addressing the cold-start challenges of early data exploration. For more information, see Generate data insights in BigQuery. Discover, transform, query, and visualize data with BigQuery data canvas. You can use natural language with Gemini in BigQuery, to find, join, and query table assets, visualize results, and seamlessly collaborate with others throughout the entire process. For more information, see Analyze with data canvas. Get assisted SQL and Python data analysis. You can use Gemini in BigQuery to generate or suggest code in either SQL or Python, and to explain an existing SQL query. You can also use natural language queries to begin data analysis. To learn how to generate, complete, and summarize code, see the following documentation: SQL code assist Use the SQL generation tool Prompt to generate SQL queries Generate SQL queries with Gemini Cloud Assist (Preview) Complete a SQL query (Preview) Explain a SQL query Python code assist Generate Python code with the code generation tool Generate Python code with Gemini Cloud Assist (Preview) Python code completion Generate BigQuery DataFrames Python code (Preview) Prepare data for analysis. Data preparation in BigQuery gives you context aware, AI-generated transformation recommendations to cleanse data for analysis. For more information, see Prepare data with Gemini. Customize your SQL translations with translation rules. (Preview) Create Gemini-enhanced translation rules to customize your SQL translations when using the interactive SQL translator. You can describe changes to the SQL translation output using natural language prompts or specify SQL patterns to find and replace. For more information, see Create a translation rule. To learn how to set up Gemini in BigQuery, see Set up Gemini in BigQuery. BigQuery roles and resources BigQuery addresses the needs of data professionals across the following roles and responsibilities. Data Analyst Task guidance to help if you need to do the following: Query BigQuery data using interactive or batch queries using SQL query syntax Reference SQL functions, operators, and conditional expressions to query data Use tools to analyze and visualize BigQuery data including: Looker, Looker Studio, and Google Sheets. Use geospatial analytics to analyze and visualize geospatial data with BigQuery's Geographic Information Systems Optimize query performance using: Partitioned tables: Prune large tables based on time or integer ranges. Materialized views: Define cached views to optimize queries or provide persistent results. BI Engine: BigQuery's fast, in-memory analysis service. To take a tour of BigQuery's data analytics features directly in the Google Cloud console, click Take the tour. Take the tour Data Administrator Task guidance to help if you need to do the following: Manage costs with reservations to balance on-demand and capacity-based pricing. Understand data security and governance to help secure data by dataset, table, column, row, or view Backup data with table snapshots to preserve the contents of a table at a particular time. View BigQuery INFORMATION_SCHEMA to understand the metadata of datasets, jobs, access control, reservations, tables and more. Use Jobs to have BigQuery load, export, query, or copy data are actions on your behalf. Monitor logs and resources to understand BigQuery and workloads. For more information, see Introduction to BigQuery administration. To take a tour of BigQuery data administration features directly in the Google Cloud console, click Take the tour. Take the tour Data Scientist Task guidance to help if you need to use BigQuery ML's machine learning to do the following: Understand the end-to-end user journey for machine learning models Manage access control for BigQuery ML Create and train a BigQuery ML models including: Linear regression forecasting Binary logistic and multiclass logistic regression classifications K-means clustering for data segmentation Time series forecasting with Arima+ models Data Developer Task guidance to help if you need to do the following: Load data into BigQuery with: batch-load data for Avro, Parquet, ORC, CSV, JSON, Datastore, and Firestore formats BigQuery Data Transfer Service BigQuery Storage Write API Use code sample library including: Connection samples Reservation sample Storage code samples Google Cloud sample browser (scoped for BigQuery) APIs and Libraries Overview ODBC / JDBC integration BigQuery video tutorials The following series of video tutorials get you started with BigQuery: Title Description How to get started with BigQuery (17:18) An overview that summarizes what is BigQuery and how to use it. Segments include: ETL pipelines, pricing and optimization, BigQuery ML and BI Engine, and wrapping up with a demo of BigQuery in Google Cloud console. What is BigQuery? (4:39) An overview of BigQuery of how BigQuery is designed to ingest and store large amounts of data to help analysts and developers alike Using the BigQuery sandbox (3:05) How to set up a BigQuery sandbox, letting you run queries without needing a credit card Asking questions, running queries (5:11) How to write and run SQL queries in the BigQuery UI - plus picking a winning jersey number Loading data into BigQuery (5:31) How to ingest and analyze data in real time, or just a one-time batch analysis of data - plus cats v. dogs Visualizing query results (5:38) How data visualization is useful for making complex datasets easier to understand and internalize Managing access with IAM (5:23) How to allow other users to query your datasets in BigQuery with IAM permissions and access control Saving and sharing queries (6:17) How to save and share your queries in BigQuery hassle-free Protecting sensitive data with authorized views (7:12) How to easily share datasets with different users by setting customized access controls Querying external data with BigQuery (5:49) How to set up an external data source in BigQuery and query data from Cloud Storage, Cloud SQL, Google Drive, and more What are user-defined functions? (4:59) How to create user-defined functions (UDFs) for analyzing datasets in BigQuery What's next For an overview of BigQuery storage, see Overview of BigQuery storage. For an overview of BigQuery queries, see Overview of BigQuery analytics. For an overview of BigQuery administration, see Introduction to BigQuery administration. For an overview of BigQuery security, see Overview of data security and governance. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.  2025-07-18 UTC.", "metadata": {"source": "https://cloud.google.com/bigquery-transfer/docs/introduction", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 2272, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9550000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Overview\n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Home Documentation Transfer Appliance Guides Send feedback Overview Stay organized with collections Save and categorize content based on your preferences. Transfer Appliance is a high-capacity storage device that enables you to transfer and securely ship your data to a Google upload facility, where we upload your data to Cloud Storage. For Transfer Appliance capacities and requirements, refer to the Specifications page. How it works Request an appliance: We'll work with you to select the appropriate appliance for your requirements. Upload your data: Linux and Apple macOS systems mount the NFS share exposed by the appliance. Windows systems use SCP or SSH to upload data to the appliance. Ship the appliance back: Complete the transfer and seal the appliance. Google uploads the data: We upload the data to your Cloud Storage bucket, then wipe the appliance when we're done. Transfer is complete! You can now access your data in Google Cloud. Security features Your data and network security is important. Transfer Appliance helps ensure that you're connecting a trusted device to your equipment and network, and secures your data from end to end so that it is read by people you trust. To ensure Transfer Appliance is trusted and safe to connect to your devices, Transfer Appliance offers the following features: Tamper resistant: Bad actors cannot easily open Transfer Appliance's physical case. We also apply tamper-evident tags to the shipping case, so that you can visually inspect each appliance's integrity prior to opening the package. Ruggedized: Transfer Appliance's shipping container is ruggedized, ensuring your data arrives safely. Trusted Platform Module (TPM) chip: We validate the TPM's Platform Configuration Registers to ensure that the immutable root filesystem and software components haven't been tampered with. Hardware attestation: We use a remote attestation process to validate the appliance before you can connect it to your device and copy data to it. If anything is amiss, we work with you to quickly send you a new appliance. To ensure your data is safe during and after transit, Transfer Appliance uses the following features to protect you: AES 256 encryption: Your data is encrypted with industry-standard encryption to keep it safe. Customer-managed encryption keys: We use encryption keys that you manage using Cloud Key Management Service (Cloud KMS), enabling you to control and secure your data prior to shipping an appliance back to us. NIST 800-88 compliant data erasure: We securely erase your data from Transfer Appliance after uploading your data to Cloud Storage. You can request a wipe certificate to verify that we've wiped your data. For more information, refer to Security and encryption. Performance To enable you move data quickly and efficiently, Transfer Appliance has the following performance features: All SSD drives: Increased reliability over hard disk drives to ensure your transfer is smooth. Multiple network connectivity options: Quickly move data from your devices to Transfer Appliance, using either a 10Gbps RJ45 interface or a 40Gbps QSFP+ interface. Scalability with multiple appliances: You can scale your transfers by ordering multiple appliance to increase your transfer speed. Globally distributed processing: Reduced shipping times to and from Google ensures your data transfer to Cloud Storage is quick. Minimal software: For Linux and Apple macOS systems, copy directly to Transfer Appliance by mounting the exposed NFS share on the appliance to your workstation, using common software already installed on the system. For Microsoft Windows systems, copy directly to Transfer Appliance from your workstation using SCP. Online capabilities Enabling online mode allows you to perform online transfers by streaming data directly to your Cloud Storage bucket after copying it to your appliance. Online transfers offer the following benefits: Quickly transfer data to Cloud Storage with low latency: Online transfers are an accelerated method of transferring your data to Cloud Storage, omitting the need to wait for your appliance to be shipped back to Google before the data is copied to your destination bucket. Connect to multiple appliances: Online mode allows parallel connectivity to multiple appliances. Cost-effective: Online capability is offered as a low-cost, fully-managed method for transferring your data. Secure connection: Your data is encrypted during online transfers, ensuring end-to-end security. After the transfer is complete, your data is removed from the appliance. Easy to enable or disable: You can toggle between online and offline mode using simple commands. For more information on how to enable or disable online mode, refer to the Online/offline transfer page. Is Transfer Appliance suitable for me? Transfer Appliance is a good fit for your data transfer needs if: You are an existing Google Cloud customer. Your data resides in locations that Transfer Appliance is available. It would take more than one week to upload your data over the network. Other transfer options Other Google Cloud transfer options include: Storage Transfer Service to move data to a Cloud Storage bucket from other cloud storage providers or from your on-premises storage. BigQuery Data Transfer Service to move data from software as a service (SaaS) applications to BigQuery. Transfer service for on-premises data to move data from your on-premises machines to Cloud Storage. Where is Transfer Appliance available? Transfer Appliance is available in the following locations: Location TA7 TA40(Rackable) TA300(Rackable) TA40(Freestanding) TA300(Freestanding) United States European Union member states United Kingdom Singapore Japan Canada Australia India For a complete list of countries where Transfer Appliance is available, refer to the Order Appliance page on the Google Cloud console. If you don't find your country listed, reach out to Support at data-support@google.com. Data transfer speeds With a typical network bandwidth of 100 Mbps, 300 terabytes of data takes about 9 months to upload. However, with Transfer Appliance, you can receive the appliance and capture 300 terabytes of data in under 25 days. Your data can be accessed in Cloud Storage within another 25 days, all without consuming any outbound network bandwidth. Example use cases Data collection If you need to transfer data from researchers, vendors, or other sites to Google Cloud, Transfer Appliance can move that data for you. Once transferred to Cloud Storage or BigQuery, your data is accessible via our Dataflow processing service for machine learning projects. Google Cloud Machine Learning Engine is a managed service that enables you to easily build machine learning models, that work on any type of data, of any size. Data replication Transfer Appliance can assist you in taking advantage of hybrid architectures, supporting current operations with existing on-premises infrastructure while experimenting with the cloud. By transferring a copy of your data to Google Cloud, you can decommission duplicate datasets, test cloud infrastructure, and expose your data to machine learning and analysis. Data migration Offline data transfer is suited for moving large amounts of existing backup images and archives to Cloud Storage, which can be stored in ultra low-cost, highly-durable, and highly available storage classes such as Archive Storage. For structured and unstructured data sets, whether they are small and frequently accessed or huge and rarely referenced, Google offers solutions like Cloud Storage, BigQuery, and Dataproc to store and analyze that data. Data handling for the European Union For customers in the EU, appliances are shipped from Belgium. When data capture is complete, you ship the appliance to Belgium for data upload. Your data is then uploaded to a Cloud Storage location in a region that you have specified. If you choose a destination region within the EU, your data never leaves the boundaries of the European Union during any part of the data transfer process. What's next? Request Transfer Appliance. Learn more about Transfer Appliance pricing. Review the procedure for using Transfer Appliance. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.  2025-07-18 UTC.", "metadata": {"source": "https://cloud.google.com/transfer-appliance/docs/4.0/overview", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 1304, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9450000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Error Reporting overview\n\n\n      \n      Stay organized with collections\n    \n\n      \n      Save and categorize content based on your preferences.\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Home Google Cloud Observability Error Reporting Documentation Guides Send feedback Error Reporting overview Stay organized with collections Save and categorize content based on your preferences. Error Reporting aggregates errors produced in your running cloud services. These errors are either reported by the Error Reporting API or are inferred to be errors when Error Reporting inspects log entries for common text patterns such as stack traces. Error Reporting groups errors which are considered to have the same root cause. Error Reporting is automatically enabled. Error Reporting samples up to 1,000 errors per hour. When this limit is reached, the displayed counts are estimated. If too many events are received, then Error Reporting samples up to 100 errors per hour and continue to extrapolate the counts. When Error Reporting analyzes log entries Error Reporting is a global service built on Cloud Logging and can analyze log entries when all of the following are true: Assured workloads are disabled. For more information, see Overview of Assured Workloads. Customer-managed encryption keys (CMEK) is disabled on all log buckets that store the log entry. For information about how to determine the CMEK configuration for a log bucket, see Verify key enablement. The log bucket satisfies one of the following: The log bucket is stored in the same project where the log entries originated. The log entries were routed to a project, and then that project stored those log entries in a log bucket that it owns. If you are storing your log entries in log buckets with CMEK enabled, then you can still use Error Reporting. However, you must use the Error Reporting client libraries or the Error Reporting API. For more information, see the Error Reporting API overview and Error Reporting client libraries. How errors are grouped When Error Reporting evaluates log entries, it ignores log entries with the following conditions: On App Engine standard environment, errors logged with a severity lower than ERROR are ignored. Stack frames which are not owned by the user are ignored (for instance, those that belong to public libraries). Any repeating sequence of one or more stack frames is replaced by a single occurrence of that sequence. Compiler-introduced methods and symbols are removed. Next, Error Reporting follows these rules to group errors: Exceptions are grouped together if they have the same exception type and similar stacks. The stack trace is ignored for exceptions that are typically unrelated to the source location where they occur. Errors without an exception stack are grouped together if they were created by the same log entry, approximated by the source location it was reported from (reportLocation). Specifically, the following grouping rules are applied in this order: Error type Grouped by Errors caused by a general problem in the environment. For example, App Engine specific problems: com.google.apphosting.runtime.HardDeadlineExceededError com.google.appengine.api.datastore.DatastoreTimeoutException Java problems: java.util.concurrent.CancellationException Grouped by exception type. Errors with a stack trace. In the case of nested exceptions, the innermost exception is considered. For example: runtime error: index out of range package1.func1() file1:20 package2.func2() file2:33 Grouped by exception type and the 5 top-most frames. Errors without a stack trace, but with a message. For example: runtime error: index out of range func1() Grouped by message and (if present) function name. Only the first 3 literal tokens of the message are considered. In the example to the left, these are runtime, error, and index. Data regionality If you set up Assured Workloads for data-residency or Impact Level 4 (IL4) requirements, then Google Cloud automatically disables Error Reporting. In Cloud Logging, you can regionalize your logs by routing them to a specific location. On the Error Groups page, Error Reporting organizes and shows error groups based on the region of the log bucket that contains the log entries. For example, an error group listed under us-central-1 contains only error logs that are part of a log bucket in us-central-1. Global error groups contain only error logs that are part of a log bucket in the global region. To filter the region of the error groups displayed on the Error Groups page, select a value from the Region menu. This menu has a default value of global. Note: Because Error Reporting is a global service, error groups can be accessed from any region. This behavior isn't configurable. What's next Collect error data by using Error Reporting View errors Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.  2025-07-23 UTC.\n\n## Code Examples\n\n### Example 1\n\n```\nruntime error: index out of range\npackage1.func1()\n      file1:20\npackage2.func2()\n      file2:33\n```\n\n### Example 2\n\n```\nruntime error: index out of range\n    func1()\n```\n\n", "metadata": {"source": "https://cloud.google.com/error-reporting/docs/grouping-errors", "language": "markdown", "complexity": "medium", "type": "official_site", "word_count": 769, "code_examples": 2, "authentic": true}, "validation": {"overall_quality": 0.9325000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Cloud Quotas documentation\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Home Documentation Cloud Quotas Cloud Quotas documentation Read product documentation Cloud Quotas enables customers to manage quotas for all of their Google Cloud services. With Cloud Quotas, users are able to easily monitor quota usage, create and modify quota alerts, and request limit adjustments for quotas. Quotas are managed through the Cloud Quotas dashboard or the Cloud Quotas API. Learn more. Get started for free Start your proof of concept with $300 in free credit Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment View free product offers Keep exploring with 20+ always-free products Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Documentation resources Find quickstarts and guides, review key references, and get help with common issues. format_list_numbered Guides Overview Understand quotas Predefined roles and permissions find_in_page Reference REST API Client libraries info Resources Release notes Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.  2025-07-18 UTC.", "metadata": {"source": "https://cloud.google.com/docs/quotas", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 206, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9400000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Google Cloud quickstarts and tutorials\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Home Documentation Send feedback Stay organized with collections Save and categorize content based on your preferences. Google Cloud quickstarts and tutorials Get started using Google Cloud by trying one of our product quickstarts, tutorials, checklists, or interactive walkthroughs. Application development Big data and analytics Compute Containers Databases DevOps Healthcare and life sciences High performance computing (HPC) Hybrid and multicloud Identity Internet of Things (IoT) Logging and monitoring Machine learning and artificial intelligence (ML/AI) Migrations Networking Security and compliance Serverless Storage Google Cloud console Google Cloud CLI Cloud Client Libraries Quickstart Tutorial Checklist Interactive walkthrough 03:38 Get started with Google Cloud quickstarts What are Google Cloud quickstarts? Whether you're looking to deploy a web app, set up a database, or run big data workloads, it can be challenging to get started. Luckily, Google Cloud quickstarts offer step-by-step tutorials that cover basic use cases, operating the Google Cloud console, and how to use the Google command-line tools. Send feedback", "metadata": {"source": "https://cloud.google.com/docs/tutorials?doctype=quickstart", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 157, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9075000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Get started with Google Cloud  |  Documentation\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Home Documentation Send feedback Stay organized with collections Save and categorize content based on your preferences. Get started with Google Cloud Start by creating a Google Cloud account. Plus, you get $300 in free credits and free usage of 20+ products on signup to run, test, and deploy workloads. Create an account Google Cloud overview Get started with Google Cloud Get hands-on experience with free usage of 20+ products, including popular products like AI APIs, Compute Engine, BigQuery, and more. Go to console Google Cloud overview Start your platform setup Click to show or hide setup steps by job function: IT administration FinOps administration DevOps engineering Application development Data analysis IT administration Establish administrators, billing accounts, and other settings in your Google Cloud environment. 1 Establish your organization, administrators, and billing 2 Create an initial architecture 3 Deploy or download configuration Resources for later: Cloud Quotas overview Google Cloud deployment archetypes (Architecture Center) FinOps administration Set up billing, spending notifications, and resource structure to facilitate cost monitoring and optimization. 1 Learn about Cloud Billing 2 Create a billing account 3 Set up spending notifications 4 Enable billing data export for custom reporting Resources for later: Monitor costs using billing reports Optimize costs with FinOps hub Resource hierarchy options for cost tracking Implement cost optimization strategies (Architecture Center) DevOps engineering Start automating infrastructure and secure collaboration with teammates using Google Cloud tools and best practices. 1 Set up API access 2 Install the gcloud CLI 3 Enable teammates using IAM 4 Choose and install a Cloud Client Library 5 Learn about authentication and authorization Resources for later: Observability in Google Cloud Terraform and Infrastructure Manager CI/CD pipeline for containerized apps (Architecture Center) Application development Get basic API access and set up a development environment that can interact with Google Cloud services. 1 Set up API access 2 Install the gcloud CLI 3 Choose and install a Cloud Client Library 4 Set up IDE extensions 5 Learn about authentication and authorization Resources for later: Build a generative AI application Data analysis Analyze sample data using Google Cloud products with minimal setup. 1 Load and query sample data 2 Explore, analyze, and share data 3 Learn about programmatic analysis tools 4 Introduction to AI and ML in BigQuery Resources for later: Set up the bq command-line tool Gemini in BigQuery overview Data analytics design patterns (Architecture Center) Begin building wb_iridescent Jump Start Solutions Learn and experiment with pre-built solution templates. home_repair_service Cloud SDK, languages, frameworks, and tools Discover tools, resources, and products that enable interaction with Google Cloud using code. hub Google Cloud Well-Architected Framework Follow recommendations and best practices to design and operate a well-architected cloud topology. Explore products dashboards Google Cloud products at a glance A quick reference for Google Cloud products. cloud Google Cloud for AWS and Azure users Compare AWS and Azure services to Google Cloud. Grow skills school Training & certification Explore our curriculum and grow your skills with Google Cloud Training. Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.  2025-07-24 UTC.", "metadata": {"source": "https://cloud.google.com/docs/get-started/", "language": "markdown", "complexity": "medium", "type": "official_site", "word_count": 542, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9175000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Google Cloud Documentation\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Home Documentation Send feedback Stay organized with collections Save and categorize content based on your preferences. Google Cloud Documentation Find user guides, quickstarts, tutorials, use cases, code samples, and more. Get started for free Start your proof of concept with $300 in free credit Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment View free product offers Keep exploring with 20+ always-free products Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Discover Learn about Google Cloud, find prescriptive guidance, and explore migration resources. Google Cloud overview Learn about Google Cloud, including popular features with pointers to documentation and tutorials. Cloud Architecture Center Plan and design your infrastructure with architectural guidance and best practices. Start free Start building in the console with $300 in free credits for new customers. All customers get free usage of over 20 products. Common developer use cases Find documentation and tutorials to help you solve common developer problems and use cases. Migrate to Google Cloud Plan how to migrate your workloads, data, and processes. Google Cloud for AWS and Azure customers Compare AWS and Azure services to Google Cloud. Get started Find tools, resources, and training to begin building on Google Cloud. Get started with Google Cloud Explore product quickstarts, tutorials, and interactive walkthroughs. Boost productivity with Generative AI Get generative-AI powered assistance with an always-on collaborator. Enterprise setup checklist Prepare your organization for scalable, production-ready, enterprise workloads. Grow your skills and get certified Study emerging cloud technologies and get Google Cloud certified. Browse code samples Review samples to demonstrate the use of Google Cloud products. Jump Start Solutions Plan and quickly deploy pre-built solutions to solve your business challenges. Documentation spotlight New guide Build a generative AI application on Google Cloud Learn the stages of building a generative AI application, choose the best products and tools for your use case, and access the documentation you need to get started. Popular guide Install the Google Cloud CLI Install and initialize the Google Cloud CLI and run core gcloud CLI commands. SDKs, languages, frameworks, and tools gcloud CLI Python Java Go JavaScript and Node.js C# and .NET PHP C++ Ruby Spring Terraform Prometheus Kubernetes Explore technology areas AI and ML Leverage the power of AI/ML solutions to transform your organization and solve real-world problems. Application development Create applications with a comprehensive set of tools and services. Application hosting Run and manage applications on a secure platform. Compute Run your workloads on virtual machines with specialized offerings for ML, high-performance computing, and other workloads to match your needs. Data analytics and pipelines Load, transform, and analyze data to provide business intelligence insights. Databases Migrate and manage enterprise data with security, reliability, high availability, and fully-managed data services. Distributed, hybrid, and multicloud Extend your Google Cloud topology to the edge, on-premises, and other cloud platforms. Generative AI Build generative AI applications using a range of products and tools across the entire development life cycle. Industry solutions Transform your business with Google Cloud solutions for specific industries like retail, healthcare, and financial services. Networking Connect your networks and workloads, load balance traffic, and secure your network. Observability and monitoring Documentation, guides, and resources for observability and monitoring across Google Cloud products and services. Security Google Cloud security products help organizations secure their cloud environment, protect their data, and comply with industry regulations. Storage Data storage, backup, and disaster recovery. Explore cross-product tools Access and resource management Organize, analyze, and manage access to your Google Cloud resources and services. Costs and usage management Manage costs and usage across Google Cloud products and services. Google Cloud SDK, languages, frameworks, and tools Use Google Cloud SDK, languages, frameworks, and tools effectively in cloud development. Infrastructure as code Configure your infrastructure using code instead of graphical interfaces or command-line scripts. Migration Use tools and information to help you on your journey to Google Cloud. Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.  2025-07-18 UTC.", "metadata": {"source": "https://cloud.google.com/docs/", "language": "markdown", "complexity": "medium", "type": "official_site", "word_count": 710, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9550000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Google Cloud samples\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Home Documentation Send feedback Stay organized with collections Save and categorize content based on your preferences. Google Cloud samples Search for samples demonstrating the usage of Google Cloud products. For Terraform samples, see Resource samples and Blueprints. Go Java Node.js C++ C# JSON PHP Python Ruby TypeScript Terraform YAML Sample Notebook Apache Kafka for BigQuery Batch for Google Cloud BigQuery BigQuery Data Transfer Service Bigtable Certificate Authority Service Cloud Build Cloud Composer Cloud Data Loss Prevention Cloud Functions Cloud Key Management Service Cloud Life Sciences Cloud Logging Cloud Monitoring Cloud Natural Language API Cloud Profiler Cloud Pub/Sub Cloud Run Cloud SQL for MySQL Cloud SQL for PostgreSQL Cloud SQL for SQL Server Cloud Storage Cloud Tasks Cloud Translation Cloud Vision Compute Engine Data Catalog Dataflow Dataplex Universal Catalog Dataproc Datastore Document AI Enterprise Knowledge Graph Eventarc Firestore Generative AI on Vertex AI Google Cloud VMware Engine Google Gen AI SDK Google Kubernetes Engine (GKE) Identity and Access Management Identity-Aware Proxy Live Stream API PubSub Lite Secret Manager Security Command Center Service Directory Spanner Speech-to-Text Talent Solution Text-to-Speech Transcoder API Vertex AI Vertex AI Agent Builder Video Intelligence API Video Stitcher API Web Risk Workflows reCAPTCHA Enterprise", "metadata": {"source": "https://cloud.google.com/docs/samples", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 196, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8975000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Architecture styles\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Table of contents Exit focus mode Ask Learn Ask Learn Table of contents Read in English Add Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories. Access to this page requires authorization. You can try changing directories. Architecture styles 2025-07-11 Feedback An architecture style is a family of architectures that share specific characteristics. For example, N-tier is a common architecture style. More recently, microservice architectures are starting to gain favor. Architecture styles don't require the use of specific technologies, but some technologies are better suited for certain architectures. For example, containers are well-suited for microservices. We have identified a set of architecture styles that are commonly found in cloud applications. The article for each style includes: A description and logical diagram of the style. Recommendations for when to choose this style. Benefits, challenges, and best practices. A recommended deployment using relevant Azure services. A quick tour of the styles This section gives a quick tour of the architecture styles that we've identified, along with some high-level considerations for their use. This list isn't exhaustive. Read more details in the linked topics. N-tier An arrow points from the client to the WAF, then from WAF to the Web tier. Two flows point from the Web tier. The first flow shows an arrow that points from the Web tier to the Middle Tier 1. Two arrows point from the Middle Tier 1 to the Remote service and to the Data tier. Two arrows point from the Data tier through the Cache section and to Middle Tier 1 and Middle Tier 2. The second flow shows an arrow that points from Web tier to Messaging and then from Messaging to Middle Tier 2. An arrow points from Middle Tier 2 to Data tier. N-tier is a traditional architecture for enterprise applications. Dependencies are managed by dividing the application into layers that perform logical functions, such as presentation, business logic, and data access. A layer can only call into layers that sit below it. However, this horizontal layering can be a liability. It can be hard to introduce changes in one part of the application without touching the rest of the application. That makes frequent updates a challenge, limiting how quickly new features can be added. N-tier is well-suited for migrating existing applications that already use a layered architecture. For that reason, N-tier is most often seen in infrastructure as a service (IaaS) solutions or applications that use a combination of IaaS and managed services. Web-Queue-Worker An arrow points from the client to the Identity provider. An arrow points from the Identity provider to the Web front end. Three arrows point from the Web front end in separate flows. In the top flow, an arrow points to the Remote service. In the middle flow, an arrow points to the Database. In the bottom flow, an arrow points to the Queue, to the Worker, and then to the Database. An arrow points from the Database to the Web front end. An arrow points from the Database through the Cache to the Worker. Another arrow points from the Static content to the CDN and then from the CDN to the client. For a purely PaaS solution, consider a Web-Queue-Worker architecture. In this style, the application has a web front end that handles HTTP requests and a back-end worker that performs CPU-intensive tasks or long-running operations. The front end communicates to the worker through an asynchronous message queue. Web-Queue-Worker is suitable for relatively simple domains with some resource-intensive tasks. Like N-tier, the architecture is easy to understand. Managed services simplify deployment and operations. But with complex domains, it can be hard to manage dependencies. The front end and the worker can easily become large, monolithic components that are hard to maintain and update. As with N-tier, Web-Queue-Worker can reduce the frequency of updates and limit innovation. Microservices The diagram illustrates a microservices architecture deployed on Microsoft Azure. It's organized into eight labeled sections that each represent a key architectural component. The layout flows from left to right and top to bottom. On the far left, icons labeled clients represent users or external systems that initiate requests to the application. An arrow points from the clients to API Gateway. Requests flow through API Gateway and on to the appropriate microservices. Arrows point from API Gateway to a box labeled microservices. This box contains two icons for domain services: one icon for composition services and one icon for service. Arrows point from the microservices to another box that's labeled data persistence. This box contains icons that represent SQL DB, NoSQL DB, and SQL DB. Arrows also point from microservices to a box that represents event streaming and messaging services. The diagram also includes boxes that represent observability, management and orchestration, and DevOps. If your application has a more complex domain, consider moving to a Microservices architecture. A microservices application is composed of many small, independent services. Each service implements a single business capability. Services are loosely coupled, communicating through API contracts. Each service can be developed by a small, focused team. Individual services can be deployed with minimal coordination across teams, which supports frequent updates. Compared to N-tier or Web-Queue-Worker architectures, a microservice architecture is more complex to build and operate. It requires a mature development and DevOps culture. However, with the right practices in place, this approach can result in higher release velocity, faster innovation, and a more resilient architecture. Event-driven architecture An arrow points from the Event producers section to the Event ingestion section. Three arrows point from the Event ingestion section to three individual sections that are all labeled Event consumers. Event-Driven Architectures use a publish-subscribe (pub-sub) model, where producers publish events, and consumers subscribe to them. The producers are independent from the consumers, and consumers are independent from each other. Consider an event-driven architecture for applications that ingest and process a large volume of data with low latency, such as Internet of Things (IoT) solutions. The style is also useful when different subsystems must perform different types of processing on the same event data. Big data, big compute Two parallel arrows point from the Data sources section in separate flows. The first flow shows an arrow that points from Data sources to Data storage. An arrow points from Batch processing to Analytics and reporting. Another arrow points from Batch processing to Analytical data store and then from Analytical data store to Analytics and reporting. The second flow has an arrow point from Data sources to Real-time message ingestion. An arrow points from the adjacent Stream processing section to Analytics and reporting. An Orchestration section is at the bottom of the diagram. Big data and big compute are specialized architecture styles for workloads that match specific profiles. Big data splits a large dataset into chunks and performs parallel processing across the entire set for analysis and reporting. Big compute, also known as high-performance computing, performs parallel computations across thousands of cores. Common domains include simulations, modeling, and 3D rendering. Architecture styles as constraints An architecture style places constraints on the design, including the set of elements that can appear and the allowed relationships between those elements. Constraints guide the \"shape\" of an architecture by restricting the universe of choices. When an architecture conforms to the constraints of a particular style, certain desirable properties emerge. For example, the constraints in microservices include: A service represents a single responsibility. Every service is independent of the others. Data is private to the service that owns it. Services don't share data. By adhering to these constraints, what emerges is a system where services can be deployed independently, faults are isolated, frequent updates are possible, and it's easy to introduce new technologies into the application. Each architecture style has its own trade-offs. Before you choose an architectural style, it's essential to understand the underlying principles and constraints. Without that understanding, you risk creating a design that superficially conforms to the style without realizing its full benefits. Focus more on why you're selecting a specific style than on how to implement it. Be practical. Sometimes it's better to relax a constraint than to chase architectural purity. Ideally, the choice of architectural style should be made with input from informed workload stakeholders. The workload team should start by identifying the nature of the problem that they're solving. They should then define the key business drivers and the corresponding architecture characteristics, also known as nonfunctional requirements, and prioritize them. For example, if time to market is critical, the team might prioritize maintainability, testability, and reliability to enable rapid deployment. If the team has tight budget constraints, feasibility and simplicity might take precedence. Selecting and sustaining an architectural style isn't a one-time task. It requires ongoing measurement, validation, and refinement. Because changing architectural direction later can be costly, it's often worthwhile to invest more effort upfront to support long-term efficiency and reduce risks. The following table summarizes how each style manages dependencies, and the types of domain that are best suited for each. Architecture style Dependency management Domain type N-tier Horizontal tiers divided by subnet Traditional business domain. Frequency of updates is low. Web-Queue-Worker Front-end and back-end jobs, decoupled by asynchronous messaging. Relatively simple domain with some resource intensive tasks. Microservices Vertically (functionally) decomposed services that call each other through APIs. Complicated domain. Frequent updates. Event-driven architecture Producer or consumer. Independent view for each subsystem. IoT and real-time systems. Big data Divide a huge dataset into small chunks. Parallel processing on local datasets. Batch and real-time data analysis. Predictive analysis using ML. Big compute Data allocation to thousands of cores. Compute intensive domains such as simulation. Consider challenges and benefits Constraints also create challenges, so it's important to understand the trade-offs when adopting any of these styles. Do the benefits of the architecture style outweigh the challenges, for this subdomain and bounded context. Here are some of the types of challenges to consider when selecting an architecture style: Complexity. The architecture's complexity must match the domain. If it's too simplistic, it can result in a big ball of mud, where dependencies aren't well managed and the structure breaks down. Asynchronous messaging and eventual consistency. Asynchronous messaging is used to decouple services and improve reliability because messages can be retried. It also enhances scalability. However, asynchronous messaging also creates challenges in handling eventual consistency and the possibility of duplicate messages. Interservice communication. Decomposing an application into separate services might increase communication overhead. In microservices architectures, this overhead often results in latency problems or network congestion. Manageability. Managing the application includes tasks such as monitoring, deploying updates, and maintaining operational health. Related resources Ten design principles for Azure applications Build applications on the Microsoft Cloud Best practices in cloud applications Cloud Design Patterns Performance testing and antipatterns for cloud applications Architect multitenant solutions on Azure Mission critical workload architecture on Azure Architecture for startups Feedback Was this page helpful? Yes No Additional resources", "metadata": {"source": "https://docs.microsoft.com/en-us/azure/architecture/guide/architecture-styles/", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 1833, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9550000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Choose an Azure container service\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Table of contents Exit focus mode Ask Learn Ask Learn Table of contents Read in English Add Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories. Access to this page requires authorization. You can try changing directories. Choose an Azure container service 2025-06-27 Feedback Azure provides a range of container hosting services that are designed to accommodate various workloads, architectures, and business requirements. This container service selection guide can help your workload team understand which Azure container service is best suited to your workload scenarios and requirements. Note In this guide, workload refers to a collection of application resources that support a business goal or the implementation of a business process. A workload uses multiple services, like APIs and data stores, that work together to deliver specific end-to-end functionality. Overview This guide includes this introduction article and another article about considerations that are shared across all workload types. Note If you aren't committed to containerization, choose a different compute option to host your workload. This introductory article outlines the Azure container services covered in this guide and compares their service models based on configurability and predefined solutions, such as customer-managed versus Microsoft-managed approaches. After you identify candidate services based on your service model preferences, the next step is to assess the options against your workload requirements by reviewing the article on shared considerations for networking, security, operations, and reliability. This guide helps you evaluate trade-offs based on your workload's technical requirements, size, and complexity. It also considers the expertise of your team to ensure informed decision-making. Azure container services in scope for this guide This guide focuses on a subset of the container services that Azure provides. This subset provides a mature feature set for web applications and APIs, networking, observability, developer tools, and operations. The following container services are compared: Azure Container Apps is a fully managed platform that allows you to run containerized applications without worrying about orchestration or infrastructure. For more information, see Container Apps documentation. Azure Kubernetes Service (AKS) is a managed Kubernetes service for running containerized applications. With AKS, you can take advantage of managed add-ons and extensions for extra capabilities while preserving the broadest level of configurability. For more information, see AKS documentation. Web App for Containers is a feature of Azure App Service. App Service is a fully managed service for hosting HTTP-based web apps that have built-in infrastructure maintenance, security patching, scaling, and diagnostic tooling. For more information, see App Service documentation. For a complete list of Azure container services, see Containers on Azure. Service model considerations A service model helps you understand how much flexibility and control each Azure container service provides. Complex services provide more control, while simpler services make management easier but limit customization. For more information about service model terminology and concepts, including infrastructure as a service (IaaS) and platform as a service (PaaS), see Shared responsibility in the cloud. Compare the service models of Azure container solutions Azure Kubernetes Service (AKS) AKS is a mix of IaaS and PaaS that focuses more on control than simplicity. It uses Kubernetes, which is the standard system for orchestrating containers. AKS streamlines the management of the underlying core infrastructure. However, this virtual machine (VM)-based platform is exposed to your applications and requires appropriate guardrails and processes, like patching, to ensure security and business continuity. The compute infrastructure is supported by extra Azure resources that are hosted directly in your subscription, like Azure load balancers, container registries, or application gateways. AKS provides access to the Kubernetes API server, which enables you to customize container orchestration and deploy auxiliary applications from the Cloud Native Computing Foundation. As a result, workload teams that are new to Kubernetes face a significant learning curve. If you're unfamiliar with containerized solutions, you must consider this learning curve. The following PaaS solutions provide a lower barrier to entry. You can transition to Kubernetes when your requirements demand it. AKS Automatic AKS Automatic makes it easier to adopt Kubernetes by automating complex cluster management tasks. This automation reduces the need for advanced Kubernetes expertise. It provides a more streamlined, PaaS-like experience while maintaining Kubernetes' flexibility and extensibility. Azure manages cluster setup, node provisioning, scaling, security patching, and applies best-practice configurations by default. This automation reduces operational effort but limits the available topology options. Note This guide differentiates between AKS Standard and AKS Automatic where applicable. Otherwise, you can assume that the described functionality is consistent across both offerings. Container Apps Container Apps is an abstraction layer on top of Kubernetes that allows your apps to run and scale without requiring direct management of the underlying infrastructure. Container Apps provides both serverless and dedicated compute options. These options give you full control over the type and amount of compute resources available to your applications. Container Apps abstracts container orchestration APIs while still providing built-in access to key features like Layer 7 ingress, traffic splitting, A/B testing, and application life cycle management. Web App for Containers Web App for Containers is a PaaS offering that prioritizes simplicity over control compared to Container Apps. It abstracts container orchestration while still supporting scaling, application life cycle management, traffic splitting, network integration, and observability. Hosting model considerations You can use Azure resources, like AKS clusters, to host multiple workloads. This approach can help you streamline operations, which reduces overall cost. If you choose this option, consider the following features: AKS is commonly used to host multiple workloads or disparate workload components. You can isolate these workloads and components by using Kubernetes' native functionality, such as namespaces, access controls, and network controls, to meet security requirements. You can also use AKS in single-workload scenarios if you need the extra functionality that the Kubernetes API provides and your workload team has sufficient experience operating a Kubernetes cluster. Teams with less Kubernetes experience can still effectively manage their own clusters by using Azure-managed add-ons and features like cluster automatic-upgrade to reduce operational effort. Container Apps should be used to host a single workload with a shared security boundary. Container Apps has a single top-level logical boundary called a Container Apps environment, which also serves as an enhanced security boundary. There are no mechanisms for more granular access control. For example, intra-environment communication is unrestricted and all applications share a single Log Analytics workspace. If the workload has multiple components and security boundaries, deploy multiple Container Apps environments or consider AKS instead. Web App for Containers is a feature of App Service. App Service groups applications into a logical compute boundary called an App Service plan. Because you can scope role-based access control at the application level, you might want to host multiple workloads in a single plan. However, it's best to host a single workload for each plan to avoid the noisy neighbor problem. All apps in a single App Service plan share the same allocated compute, memory, and storage. When you consider hardware isolation, keep in mind that App Service plans generally run on infrastructure shared with other Azure customers. You can choose Dedicated tiers for dedicated VMs or Isolated tiers for dedicated VMs in a dedicated virtual network. In general, all Azure container services can host multiple applications that have multiple components. However, Container Apps and Web App for Containers work best for a single workload component or multiple closely related workload components that share a similar life cycle and a single team owns and runs the application. If you need to host disparate, potentially unrelated application components or workloads on one host, consider AKS. The trade-off between control and ease of use AKS provides the most configurability, but that configurability requires more operational effort compared to the other services. Container Apps and Web App for Containers are both PaaS services that have similar levels of Microsoft-managed features. Web App for Containers focuses on simplicity to serve its target audience, which is existing App Service customers who are familiar with the interface. Best practices Services that provide more simplicity usually suit customers who focus on feature development instead of infrastructure management. Services that provide more control usually suit customers who need configurability and have the skills, resources, and business justification to manage their own infrastructure. Shared considerations across all workloads A workload team might prefer a specific service model, but that model might not meet the organization's overall requirements. For example, developers might want less operational effort, but security teams might consider this overhead necessary for compliance. Teams need to collaborate to make the right trade-offs. Shared considerations cover a wide range of factors. Only a subset of considerations might apply to you based on the workload type. Your role within the organization also affects which considerations are relevant. The following table provides a high-level overview of considerations, including service feature comparisons. Review the considerations in each category and compare them against your workload's requirements. Category Overview Networking considerations Networking in Azure container services depends on your preference for simplicity or configurability. AKS provides extensive control over network flow but requires more operational effort. Container Apps has Azure-managed networking features and sits between AKS and Web App for Containers, which serves customers who already use App Service. Network design decisions have long-term consequences because changing them often requires you to redeploy workloads. Several factors, such as IP address planning, load balancing, service discovery, and private networking, vary across these services. You should carefully review how each service meets specific networking requirements. Security considerations Container Apps, AKS, and Web App for Containers integrate with key Azure security offerings like Azure Key Vault and managed identities. AKS provides extra features like runtime threat protection and network policies. PaaS services like Container Apps might seem to have fewer security features, but that's partly because Azure manages more of the underlying infrastructure components. Because these components aren't exposed to customers, the risk is lower. Operational considerations AKS provides the most customization, but it requires more operational input. PaaS solutions like Container Apps and Web App for Containers let Azure handle tasks like OS updates. Scalability and hardware SKU flexibility are important. AKS provides flexible hardware options, but Container Apps and Web App for Containers have fewer choices. In AKS, application scalability is your responsibility, so you can apply any Kubernetes-compatible solution. AKS Automatic, Container Apps, and Web App for Containers focus on simpler approaches. Reliability considerations Web App for Containers and Container Apps have limited health probe configurations compared to AKS. However, they're simpler to set up because they use the familiar Azure Resource Manager API. AKS requires the Kubernetes API and also requires you to manage Kubernetes node pool scalability and availability to properly schedule application instances. These requirements increase operational effort for AKS. Service-level agreements (SLAs) for Container Apps and Web App for Containers are simpler to calculate than AKS SLAs. The AKS control plane and node pools each have their own SLAs, which must be compounded. All services provide zone redundancy in datacenters that support it. After you review the preceding considerations, you might still not find the perfect fit, which is typical. Evaluate trade-offs Cloud computing is complex. It involves collaboration across many teams and must account for constraints in people, budgets, and time. These factors make cloud service selection difficult and full of trade-offs. For any workload, some requirements might be more critical than others. For example, an application team might prefer a PaaS solution like Container Apps but choose AKS because their security team requires deny-by-default network controls between colocated workload components. This AKS-only feature uses Kubernetes network policies. The preceding shared considerations cover the most common requirements but aren't comprehensive. You must evaluate every requirement against your preferred service's feature set before you make a decision. Conclusion This guide covers the most common considerations when you choose an Azure container service. It's designed to help your workload team make informed decisions. The process begins with selecting a cloud service model, which involves determining the desired level of control. More control comes at the expense of simplicity. In other words, the goal is to find the right balance between a self-managed infrastructure and a Microsoft-managed infrastructure. Many workload teams choose an Azure container service based solely on whether they prefer either PaaS or IaaS. Other teams need to investigate further to determine how service-specific features address workload or organizational requirements. Use this guide to carefully evaluate your options and avoid making decisions that are difficult to reverse. However, no decision is final until developers try the service and evaluate it based on hands-on experience instead of theory. Contributors Microsoft maintains this article. The following contributors wrote this article. Principal authors: Andre Dewes | Senior Customer Engineer Marcos Martinez | Senior Service Engineer Julie Ng | Senior Engineer Other contributors: Mick Alberts | Technical Writer Martin Gjoshevski | Senior Customer Engineer Don High | Principal Customer Engineer Nelly Kiboi | Service Engineer Xuhong Liu | Senior Service Engineer Faisal Mustafa | Senior Customer Engineer Walter Myers | Principal Customer Engineering Manager Sonalika Roy | Senior Customer Engineer Paolo Salvatori | Principal Customer Engineer Victor Santana | Principal Customer Engineer Carlos Mestre del Pino | Cloud Solution Architect To see nonpublic LinkedIn profiles, sign in to LinkedIn. Related resource Shared architectural considerations Feedback Was this page helpful? Yes No Additional resources", "metadata": {"source": "https://docs.microsoft.com/en-us/azure/architecture/guide/choose-azure-container-service", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 2224, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9650000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for SaaS and multitenant solution architecture\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Table of contents Exit focus mode Ask Learn Ask Learn Table of contents Read in English Add Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories. Access to this page requires authorization. You can try changing directories. SaaS and multitenant solution architecture 2025-05-21 Feedback An organization can use software as a service (SaaS) to efficiently deliver software to its customers. Typically, a SaaS vendor operates and manages the software for their customers. Many independent software vendors (ISVs) are moving away from providing software that customers must install and manage by themselves to using a SaaS model because it improves customer experience and reduces overhead. This article is an introduction to a series of articles that provide guidance and resources for organizations, including startups, that build SaaS solutions. It also provides extensive guidance about how to architect multitenant solutions on Azure. Key concepts The key concepts in this article are SaaS, startups, and multitenancy. These terms are related, so they're often mistakenly used interchangeably. It's important to understand how these terms are different. SaaS and startups are business concepts, and multitenancy is an architecture concept. SaaS is a business model. An organization can choose to provide its software product as a service to its customers. SaaS products are sold either to businesses in a business-to-business (B2B) model or directly to consumers in a business-to-consumer (B2C) model. SaaS products are different from products that customers install and manage by themselves because the solution vendor hosts and maintains SaaS products. Many SaaS solutions use a multitenant architecture. SaaS solutions might also use different multitenancy models or approaches. Startups are businesses in an early stage of their life cycle. Many software startups build SaaS solutions, but some might provide software in other ways. Startups often have specific concerns, including rapid innovation, finding a product and market fit, and anticipating scale and growth. Multitenancy is a way of architecting a solution to share components between multiple tenants, which usually correspond to customers. You usually use multitenant architectures in SaaS solutions. You can also use multitenant architectures outside of SaaS, such as in organizations that build a platform for multiple business units to share. Multitenancy doesn't imply that every component in a solution is shared. Instead, it implies that at least some components of a solution are reused across multiple tenants. How you define a tenant and choose a tenancy model depends on whether your business model is B2C SaaS or B2B SaaS or you're a large organization. Note This series uses the term tenant to refer to your tenants, which might be your customers or groups of users. The guidance can help you build your own multitenant software solutions on top of the Azure platform. In Microsoft Entra ID, a tenant refers to individual directories, and multitenancy refers to interactions between multiple Microsoft Entra tenants. The terms are the same, but the concepts aren't. For clarity, this series uses the full term, Microsoft Entra tenant, when referring to the Microsoft Entra ID concept of a tenant. The following diagram illustrates the differences between these concepts for an organization that uses a SaaS business model: The diagram shows a business model section and a technical architecture section. The business model section includes a SaaS section that has B2C and B2B SaaS types. Arrows point from these types to the tenants in the multitenant application section. The technical architecture section includes tenants, user groups, and employees. The following diagram illustrates how you can use a multitenant architecture in a business model that isn't a SaaS: The diagram shows a business model section and a technical architecture section. The business model section includes an enterprise section which contains a business units section. The business units section includes accounting, HR, and sales. The technical architecture section includes a multitenant application section which contains a tenant section that has multiple tenants. The key difference between the two diagrams is the business model, which affects how you define a tenant in the context of your organization. Your business model also affects your design choices for the underlying multitenant architecture, but the principles of multitenancy always remain the same. Related resources Understand how startups architect their solutions Learn about multitenant architectural approaches Software as a service (SaaS) Workload Documentation - Azure Well-Architected Framework Feedback Was this page helpful? Yes No Additional resources", "metadata": {"source": "https://docs.microsoft.com/en-us/azure/architecture/guide/saas-multitenant-solution-architecture/", "language": "markdown", "complexity": "medium", "type": "official_site", "word_count": 737, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9125000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Design and develop a RAG solution\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Table of contents Exit focus mode Ask Learn Ask Learn Table of contents Read in English Add Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories. Access to this page requires authorization. You can try changing directories. Design and develop a RAG solution 2025-01-09 Feedback The Retrieval-Augmented Generation (RAG) pattern is an industry-standard approach to building applications that use language models to process specific or proprietary data that the model doesn't already know. The architecture is straightforward, but designing, experimenting with, and evaluating RAG solutions that fit into this architecture involve many complex considerations that benefit from a rigorous, scientific approach. This article is the introduction of a series. Each article in the series covers a specific phase in RAG solution design. The other articles in this series cover the following considerations: How to determine which test documents and queries to use during evaluation How to choose a chunking strategy How to determine which chunks you should enrich and how to enrich them How to choose the right embedding model How to configure the search index How to determine which searches, such as vector, full text, hybrid, and manual multiple searches, you should perform How to evaluate each step RAG architecture RAG application flow The following workflow describes a high-level flow for a RAG application. The user issues a query in an intelligent application user interface. The intelligent application makes an API call to an orchestrator. You can implement the orchestrator with tools or platforms like Semantic Kernel, Azure AI Agent service, or LangChain. The orchestrator determines which search to perform on Azure AI Search and issues the query. The orchestrator packages the top N results from the query. It packages the top results and the query as context within a prompt and sends the prompt to the language model. The orchestrator returns the response to the intelligent application for the user to read. RAG data pipeline flow The following workflow describes a high-level flow for a data pipeline that supplies grounding data for a RAG application. Documents are either pushed or pulled into a data pipeline. The data pipeline processes each document individually by completing the following steps: Chunk document: Breaks down the document into semantically relevant parts that ideally have a single idea or concept. Enrich chunks: Adds metadata fields that the pipeline creates based on the content in the chunks. The data pipeline categorizes the metadata into discrete fields, such as title, summary, and keywords. Embed chunks: Uses an embedding model to vectorize the chunk and any other metadata fields that are used for vector searches. Persist chunks: Stores the chunks in the search index. RAG design and evaluation considerations You must make various implementation decisions as you design your RAG solution. The following diagram illustrates some of the questions you should ask when you make those decisions. The following list provides a brief description of what you should do during each phase of RAG solution development. During the preparation phase, you should: Determine the solution domain. Clearly define the business requirements for the RAG solution. Gather representative test documents. Gather test documents for your RAG solution that are representative of your document collection. Gather test queries. Gather information and test queries and generate synthetic queries and queries that your documents don't cover. During the chunking phase, you should: Understand chunking economics. Understand which factors to consider as you evaluate the overall cost of your chunking solution for your text collection. Perform document analysis. Ask the following questions to help you make decisions when you analyze a document type: What content in the document do you want to ignore or exclude? What content do you want to capture in chunks? How do you want to chunk that content? Understand chunking approaches. Understand the different approaches to chunking, including sentence-based, fixed-size, and custom approaches or by using language model augmentation, document layout analysis, and machine learning models. Understand how document structure affects chunking. Choose a chunking approach based on the degree of structure that the document has. During the chunk enrichment phase, you should: Clean chunks. Implement cleaning approaches to eliminate differences that don't affect the meaning of the text. This method supports closeness matches. Augment chunks. Consider augmenting your chunk data with common metadata fields and understand their potential uses in search. Learn about commonly used tools or techniques for generating metadata content. During the embedding phase, you should: Understand the importance of the embedding model. An embedding model can significantly affect the relevancy of your vector search results. Choose the right embedding model for your use case. Evaluate embedding models. Evaluate embedding models by visualizing embeddings and calculating embedding distances. During the information retrieval phase, you should: Create a search index. Apply the appropriate vector search configurations to your vector fields. Understand search options. Consider the different types of searches, including vector, full-text, hybrid, and manual multiple searches. Learn about how to split a query into subqueries and filter queries. Evaluate searches. Use retrieval evaluation methods to evaluate your search solution. During the language model end-to-end evaluation phase, you should: Understand language model evaluation metrics. There are several metrics, including groundedness, completeness, utilization, and relevancy, that you can use to evaluate the language model's response. Understand similarity and evaluation metrics. You can use similarity and evaluation metrics to evaluate your RAG solution. Understand the importance of documentation, reporting, and aggregation. Document the hyperparameters and the evaluation results. Aggregate the results from multiple queries and visualize the results. Use the RAG experiment accelerator. Use the RAG experiment accelerator GitHub repository to help your team find the best strategies for RAG implementation by running multiple experiments, persisting, and evaluating the results. Structured approach Because of the number of steps and variables, it's important that you follow a structured evaluation process for your RAG solution. Evaluate the results of each step and make changes based on your requirements. You should evaluate each step independently for optimization, but remember that the end result is what your customers experience. Make sure that you understand all of the steps in this process before you determine your own acceptance criteria for each step. Contributors This article is maintained by Microsoft. It was originally written by the following contributors. Principal authors: Raouf Aliouat | Software Engineer II Rob Bagby | Principal Content Developer - Azure Patterns & Practices Prabal Deb | Principal Software Engineer Chad Kittel | Principal Software Engineer - Azure Patterns & Practices Ritesh Modi | Principal Engineer Ryan Pfalz | Senior Technical Program Manager Randy Thurman | Principal AI Cloud Solution Architect To see non-public LinkedIn profiles, sign in to LinkedIn. Next steps Retrieval Augmented Generation (RAG) in Azure AI Search Retrieval augmented generation and indexes Related resources Preparation phase Feedback Was this page helpful? Yes No Additional resources", "metadata": {"source": "https://docs.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-solution-design-and-evaluation-guide", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 1145, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9275000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Access Azure OpenAI and other language models through a gateway\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Table of contents Exit focus mode Ask Learn Ask Learn Table of contents Read in English Add Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories. Access to this page requires authorization. You can try changing directories. Access Azure OpenAI and other language models through a gateway Azure AI services Azure OpenAI Service Azure API Management This article describes the key challenges across the five pillars of the Azure Well-Architected Framework that you encounter if your workload design includes direct access from your consumers to the Azure OpenAI Service data plane APIs. Learn how introducing a gateway into your architecture can help resolve these direct access challenges, while introducing new challenges. This article describes the architectural pattern but not how to implement the gateway. Azure OpenAI exposes HTTP APIs that let your applications perform embeddings or completions by using OpenAI's language models. Intelligent applications call these HTTP APIs directly from clients or orchestrators. Examples of clients include chat UI code and custom data processing pipelines. Examples of orchestrators include LangChain, Semantic Kernel, and Azure AI Foundry Agent Service. When your workload connects to one or more Azure OpenAI instances, you must decide whether these consumers connect directly or through a reverse proxy API gateway. Because a gateway can be used to solve specific scenarios that might not be present in every workload, see be sure to see Specific scenario guidance, which looks at that specific use case of a gateway in more depth. Key challenges Without an API gateway or the ability to add logic into the Azure OpenAI HTTP APIs, the client has to handle the API client logic, which includes retry mechanisms or circuit breakers. This situation can be challenging in scenarios in which you don't directly control the client code, or when the code is restricted to specific SDK usage. Multiple clients or multiple Azure OpenAI instances and deployments present further challenges, such as coordination of safe deployments and observability. This section provides examples of specific key architectural challenges that you might face if your architecture only supports direct access to Azure OpenAI from consumers. The challenges are organized by using the Azure Well-Architected Framework pillars. Reliability challenges The reliability of the workload depends on several factors, including its capacity for self-preservation and self-recovery, which are often implemented through replication and failover mechanisms. Without a gateway, all reliability concerns must be addressed exclusively by using client logic and Azure OpenAI Service features. Workload reliability is compromised when there isn't enough reliability control available in either of those two surfaces. Load balancing or Redundancy: Failing over between multiple Azure OpenAI instances based on service availability is a client responsibility that you need to control through configuration and custom logic. Global, standard or provisioned, and data zone, standard or provisioned, do not impact the availability of the Azure OpenAI service from a regional endpoint availability perspective. You still have the responsibility to implement failover logic yourself. Scale out to handle spikes: Failing over to Azure OpenAI instances with capacity when throttled is another client responsibility that you need to control through configuration and custom logic. Updating multiple client configurations for new Azure OpenAI instances presents greater risk and has timeliness concerns. The same is true for updating client code to implement changes in logic, such as directing low priority requests to a queue during high demand periods. Throttling: Azure OpenAI APIs throttle requests by returning an HTTP 429 error response code to requests that exceed the Token-Per-Minute (TPM) or Requests-Per-Minute (RPM) in the standard model. Azure OpenAI APIs also throttle requests that exceed provisioned capacity for the pre-provisioned billing model. Handling appropriate back-off and retry logic is left exclusively to client implementations. Most workloads should solve this specific issue by using global and data zone deployments of Azure OpenAI. Those deployments to use model capacity from data centers with the enough capacity for each request. Using global and data zone deployments will significantly decrease service throttling without added complexity of custom gateways. The global and data zone deployments are themselves a gateway implementation. Security challenges Security controls must help protect workload confidentiality, integrity, and availability. Without a gateway, all security concerns must be addressed exclusively in client logic and Azure OpenAI Service features. Workload requirements might demand more than what's available for client segmentation, client control, or service security features for direct communication. Identity management - authentication scope: The data plane APIs exposed by Azure OpenAI can be secured in one of two ways: API key or Azure role-based access control (RBAC). In both cases, authentication happens at the Azure OpenAI instance level, not the individual deployment level, which introduces complexity for providing least privileged access and identity segmentation for specific deployment models. Identity management - identity providers: Clients that can't use identities located in the Microsoft Entra tenant that's backing the Azure OpenAI instance must share a single full-access API key. API keys have security usefulness limitations and are problematic when multiple clients are involved and all share the same identity. Network security: Depending on client location relative to your Azure OpenAI instances, public internet access to language models might be necessary. Data sovereignty: Data sovereignty in the context of Azure OpenAI refers to the legal and regulatory requirements related to the storage and processing of data within the geographic boundaries of a specific country or region. Your workload needs to ensure regional affinity so that clients can comply with data residency and sovereignty laws. This process involves multiple Azure OpenAI deployments. You should be aware that when you are using global or data zone deployments of Azure OpenAI, data at rest remains in the designated Azure geography, but data may be transmitted and processed for inferencing in any Azure OpenAI location. Cost optimization challenges Workloads benefit when architectures minimize waste and maximize utility. Strong cost modeling and monitoring are an important requirement for any workload. Without a gateway, utilization of provisioned or per-client cost tracking can be authoritatively achieved exclusively from aggregating Azure OpenAI instance usage telemetry. Cost tracking: Being able to provide a financial perspective on Azure OpenAI usage is limited to data aggregated from Azure OpenAI instance usage telemetry. When required to do chargeback or showback, you need to attribute that usage telemetry with various clients across different departments or even customers for multitenant scenarios. Provisioned throughput utilization: Your workload wants to avoid waste by fully utilizing the provisioned throughput that you paid for. This means that clients must be trusted and coordinated to use provisioned model deployments before spilling over into any standard model deployments. Operational excellence challenges Without a gateway, observability, change control, and development processes are limited to what is provided by direct client-to-server communication. Quota control: Clients receive 429 response codes directly from Azure OpenAI when the HTTP APIs are throttled. Workload operators are responsible for ensuring that enough quota is available for legitimate usage and that misbehaving clients don't consume in excess. When your workload consists of multiple model deployments or multiple data zones, understanding quota usage and quota availability can be difficult to visualize. Monitoring and observability: Azure OpenAI default metrics are available through Azure Monitor. However, there's latency with the availability of the data and it doesn't provide real-time monitoring. Safe deployment practices: Your GenAIOps process requires coordination between clients and the models that are deployed in Azure OpenAI. For advanced deployment approaches, such as blue-green or canary, logic needs to be handled on the client side. Performance efficiency challenges Without a gateway, your workload puts responsibility on clients to be individually well-behaved and to behave fairly with other clients against limited capacity. Performance optimization - priority traffic: Prioritizing client requests so that high priority clients have preferential access over low priority clients would require extensive, and likely unreasonable, client-to-client coordination. Some workloads might benefit from having low priority requests queued to run when model utilization is low. Performance optimization - client compliance: To share capacity, clients need to be well-behaved. An example of this is when clients ensure that max_tokens and best_of are set to approved values. Without a gateway, you must trust clients to act in the best interest of preserving capacity of your Azure OpenAI instance. Minimize latency: While network latency is usually a small component of the overall prompt and completion request flow, ensuring that clients are routed to a network endpoint and model close to them might be beneficial. Without a gateway, clients would need to self-select which model deployment endpoints to use and what credentials are necessary for that specific Azure OpenAI data plane API. Solution The diagram shows an intelligent application icon with an arrow pointing into a dashed line box labeled gateway. The arrow goes through a line that is labeled 'Federated Authentication,' pointing to a 'rate limiter' icon. The 'rate limiter' has an arrow that points to a 'router' icon. The 'router' has four arrows pointing to different icons. The first arrow points to a 'load balancer,' which points to 'OpenAI deployment' or 'LLM' icons in two regions and on-premises. The second arrow points to a 'monitoring' icon that later points to a 'cost' and a 'usage' icon. The third arrow points to a 'compute' icon. The fourth points to a 'message queue' icon, which then points to the 'Load balancer.' Figure 1: Conceptual architecture of accessing Azure OpenAI through a gateway To address the many challenges listed in Key challenges, you can inject a reverse proxy gateway to decouple the intelligent application from Azure OpenAI. This gateway offloading lets you shift responsibility, complexity, and observability away from clients and gives you an opportunity to augment Azure OpenAI by providing other capabilities that aren't built in. Some examples are: Potential to implement federated authentication. Ability to control pressure on models through rate limiting. Cross-cutting and cross-model monitoring. Ability to introduce gateway aggregation and advanced routing to multiple services, such as routing low priority messages to a queue for queue-based load leveling or to compute to perform tasks. Load balancing that uses health endpoint monitoring to route only to healthy endpoints by circuit breaking on unavailable or overloaded model deployments. Some specific scenarios have more guidance available that directly addresses an API gateway and Azure OpenAI instances. Those scenarios are listed in the Next steps section. Considerations The decision to add a gateway and what technology to use is made as part of the Application design described in the Azure Well-Architected Framework's AI workloads on Azure guidance. As an architect, you'll need to make the decision to include or exclude this component. When you introduce a new component into your architecture, you need to evaluate the newly introduced tradeoffs. When you inject an API gateway between your clients and the Azure OpenAI data plane to address any of key challenges, you introduce new considerations into your architecture. Carefully evaluate whether the workload impact across these architectural considerations justifies the added value or utility of the gateway. Reliability Reliability ensures that your application meets the commitments you make to your customers. For more information, see Design review checklist for Reliability. The gateway solution can introduce a single point of failure. This failure could have its origin in service availability of the gateway platform, interruptions due to code or configuration deployments, or even misconfigured critical API endpoints in your gateway. Ensure that you design your implementation to meet your workload's availability requirements. Consider resiliency and fault tolerance capabilities in the implementation by including the gateway in the failure mode analysis of the workload. Your solution might require global routing capabilities if your architecture requires Azure OpenAI instances in multiple regions to increase the availability of your Azure OpenAI endpoints, such as the ability to continue to serve requests in the event of a regional outage. This situation can further complicate the topology through management of extra fully qualified domain names, TLS certificates, and more global routing components. Important Don't implement a gateway if doing so would jeopardize your workload's ability to meet agreed upon service-level objectives (SLOs). Security When considering how an API gateway benefits your architecture, use the Design review checklist for Security to evaluate your design. You need to address security considerations, such as the following: The surface area of the workload is increased with the addition of the gateway. That surface area brings extra identity and access management (IAM) considerations of the Azure resources, increased hardening efforts, and more. The gateway can act as a network boundary transition between client network space and private Azure OpenAI network space. Even though the gateway makes a previously internet-facing Azure OpenAI endpoint private through the use of Azure Private Link, it now becomes the new point of entry and must be adequately secured. A gateway is in a unique position to see raw request data and formulated responses from the language model, which could include confidential data from either source. Data compliance and regulatory scope is now extended to this other component. A gateway can extend the scope of client authorization and authentication beyond Microsoft Entra ID and API key authentication, and potentially across multiple identity providers (IdP). Data sovereignty must be factored in your implementation in multi-region implementations. Ensure that your gateway compute and routing logic adheres to sovereignty requirements placed on your workload. Important Don't implement a gateway if doing so would leave your workload unable to protect the confidentiality, integrity, or availability of itself or its users' data. Cost Optimization Cost optimization is about looking at ways to reduce unnecessary expenses and improve operational efficiencies. For more information, see Design review checklist for Cost Optimization. All implemented API gateways have runtime costs that need to be budgeted and accounted for. Those costs usually increase with added features to address the reliability, security, and performance of the gateway itself along with operational costs introduced with added APIOps management. These added costs need be measured against the new value delivered from the system with the gateway. You want to reach a point where the new capabilities introduced by using a gateway outweigh the cost to implement and maintain the gateway. Depending on your workload's relationship to its users, you might be able to chargeback usage. To help manage costs when developing and testing a gateway, consider using a simulated endpoint for Azure OpenAI. For example, use the solution in the Azure OpenAI API simulator GitHub repository. Operational Excellence When considering how an API gateway benefits your architecture, use the Design review checklist for Operational Excellence to evaluate your design. You need to address operational excellence considerations such as the following: The gateway itself needs to be monitored by your workload's monitoring solution and potentially by clients. This means that gateway compute and operations need to be included in the workload's health modeling. Your safe deployment practices now need to address the deployment of the API gateway infrastructure and the code or configuration of the gateway routing. Your infrastructure automation and infrastructure as code (IaC) solution needs to consider how to treat your gateway as a long-lived resource in the workload. You need to build or extend your APIOps approach to cover the APIs exposed in the gateway. You duplicate capabilities that are available through solutions such as the Azure AI Service resource or Azure OpenAI data zone load distribution functionality. Performance Efficiency When considering how an API gateway benefits your architecture, use the Design review checklist for Performance Efficiency to evaluate your design. You need to address performance efficiency considerations such as the following: The gateway service can introduce a throughput bottleneck. Ensure the gateway has adequate performance to handle full concurrent load and can easily scale in line with your growth expectations. Ensure elasticity in the solution so that the gateway can reduce supply, or scale down, when demand is low, such as with business day usage. The gateway service has processing it must perform per request, and introduces added latency per API invocation. You should optimize your routing logic to keep requests performing well. In most cases, the gateway should be geographically near both the users and the Azure OpenAI instances to reduce latency. While network latency is usually a small percentage of time in overall API calls to language models, it might be a competitive factor for your workload. Evaluate the impact of the gateway on Azure OpenAI features, such as streaming responses or instance pinning for stateful interactions, such as the Assistants API. Important Don't implement a gateway if doing so makes achieving negotiated performance targets impossible or too compromising on other tradeoffs. Implementation options Azure doesn't offer a turn-key solution designed specifically to proxy Azure OpenAI's HTTP API or other custom language model inferencing APIs to cover all of these scenarios. But there are still several options for your workload team to implement, such as a gateway in Azure. Use Azure API Management Azure API Management is a platform-managed service designed to offload cross-cutting concerns for HTTP-based APIs. It's configuration driven and supports customization through its inbound and outbound request processing policy system. It supports highly available, zone-redundant, and even multi-region replicas by using a single control plane. Most of the gateway routing and request handling logic must be implemented in the policy system of API Management. You can combine built-in policies specific to Azure OpenAI, such as Limit Azure OpenAI API token usage or Emit metrics for consumption of Azure OpenAI tokens, and your own custom policies. The GenAI gateway toolkit GitHub repository contains a number of custom API Management policies along with a load-testing setup for testing the behavior of the policies. Use the Well-Architected Framework service guide for API Management when designing a solution that involves Azure API Management. If your workload exists as part of an application landing zone, review the guidance available in the Cloud Adoption Framework for Azure on implementing an Azure API Management landing zone. Using Azure API Management for your gateway implementation is generally the preferred approach to building and operating an Azure OpenAI gateway. It's preferred because the service is a platform as a service (PaaS) offering with rich built-in capabilities, high availability, and networking options. It also has robust APIOps approaches to managing your completion APIs. Use custom code The custom code approach requires a software development team to create a custom coded solution and to deploy that solution to an Azure application platform of their choice. Building a self-managed solution to handle the gateway logic can be a good fit for workload teams proficient at managing network and routing code. The workload can usually use compute that they're familiar with, such as hosting the gateway code on Azure App Service, Azure Container Apps, or Azure Kubernetes Service. Custom code deployments can also be fronted with API Management when API Management is used exclusively for its core HTTP API gateway capabilities between your clients and your custom code. This way your custom code interfaces exclusively with your Azure OpenAI HTTP APIs based on the necessary business logic. The use of non-Microsoft gateway technology, which is a product or service that isn't natively provided by Azure, can be considered as part of this approach. Example architecture The diagram shows an intelligent application icon with an arrow that points to two Azure API Management icons, one of which is gateway only. The icons have arrows that point to two Azure OpenAI icons in one region and one in another. One of the Azure API Management icons has an arrow labeled Batch request that points to Azure Event Hubs. The same icon has an arrow that points to an Azure Function with the arrow labeled Compute service. The Azure function has an arrow that points to API Management with a label Replay batched request and an arrow that points to Azure Event Hubs labeled Batched request. A Microsoft Entra ID icon, an Azure Traffic Manager icon, and an Application Insights and Azure Monitor icon are at the bottom of the diagram. Figure 2: Example architecture of accessing Azure OpenAI through an Azure API Management-based gateway Next steps Learn about a specific scenario where deploying a gateway between an intelligent application and Azure OpenAI deployments is used to address workload requirements: Load balancing or failover between multiple backend instances Custom authentication and authorization for client applications Implement logging and monitoring for Azure OpenAI models Related resources Azure OpenAI Service API gateway in Azure API Management API Management landing zone GitHub repository covering generative AI scenarios API Management gateway toolkit OpenAI API Simulator Feedback Was this page helpful? Yes No", "metadata": {"source": "https://docs.microsoft.com/en-us/azure/architecture/ai-ml/guide/azure-openai-gateway-guide", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 3426, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9700000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Machine learning operations\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Table of contents Exit focus mode Ask Learn Ask Learn Table of contents Read in English Add Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories. Access to this page requires authorization. You can try changing directories. Machine learning operations 2024-07-12 Feedback This article describes three Azure architectures for machine learning operations that have end-to-end continuous integration and continuous delivery (CI/CD) pipelines and retraining pipelines. The architectures are for these AI applications: Classical machine learning Computer vision (CV) Natural language processing These architectures are the product of the MLOps v2 project. They incorporate best practices that solution architects identified in the process of developing various machine learning solutions. The result is deployable, repeatable, and maintainable patterns. All three architectures use the Azure Machine Learning service. For an implementation with sample deployment templates for MLOps v2, see Azure MLOps v2 GitHub repository. Potential use cases Classical machine learning: Time-series forecasting, regression, and classification on tabular structured data are the most common use cases in this category. Examples include: Binary and multi-label classification. Linear, polynomial, ridge, lasso, quantile, and Bayesian regression. ARIMA, autoregressive, SARIMA, VAR, SES, LSTM. CV: The MLOps framework in this article focuses mostly on the CV use cases of segmentation and image classification. Natural language processing: You can use this MLOps framework to implement: Named entity recognition Text classification Text generation Sentiment analysis Translation Question answering Summarization Sentence detection Language detection Part-of-speech tagging AI simulations, deep reinforcement learning, and other forms of AI aren't described in this article. MLOps as a key design area for AI workloads The planning and implementation of a MLOps and GenAIOps are a core design area in AI workloads on Azure. To get a background on why these machine learning workloads need specialized operations, see MLOps and GenAIOps for AI workloads on Azure in the Azure Well-Architected Framework. Architecture The MLOps v2 architectural pattern has four main modular components, or phases, of the MLOps lifecycle: Data estate Administration and setup Model development, or the inner loop phase Model deployment, or the outer loop phase The preceding components, the connections between them, and the typical personas involved are standard across all MLOps v2 scenario architectures. Variations in the details of each component depend on the scenario. The base architecture for MLOps v2 for Machine Learning is the classical machine learning scenario for tabular data. The CV and NLP architectures build on and modify this base architecture. MLOps v2 covers the following architectures that are described in this article: Classical machine learning architecture Machine Learning CV architecture Machine Learning natural language processing architecture Classical machine learning architecture Download a Visio file of this architecture. Workflow for the classical machine learning architecture Data estate This component illustrates the data estate of the organization and potential data sources and targets for a data science project. Data engineers are the primary owners of this component of the MLOps v2 lifecycle. The Azure data platforms in this diagram aren't exhaustive or prescriptive. A green check mark indicates the data sources and targets that represent recommended best practices that are based on the customer use case. Administration and setup This component is the first step in the MLOps v2 solution deployment. It consists of all tasks related to the creation and management of resources and roles that are associated with the project. For example, the infrastructure team might: Create project source code repositories. Use Bicep or Terraform to create Machine Learning workspaces. Create or modify datasets and compute resources for model development and deployment. Define project team users, their roles, and access controls to other resources. Create CI/CD pipelines. Create monitoring components to collect and create alerts for model and infrastructure metrics. The primary persona associated with this phase is the infrastructure team, but an organization might also have data engineers, machine learning engineers, or data scientists. Model development (inner loop phase) The inner loop phase consists of an iterative data science workflow that acts within a dedicated and secure Machine Learning workspace. The preceding diagram shows a typical workflow. The process starts with data ingestion, moves through exploratory data analysis, experimentation, model development and evaluation, and then registers a model for production use. This modular component is agnostic and adaptable to the process that your data science team uses to develop models. Personas associated with this phase include data scientists and machine learning engineers. Machine Learning registries After the data science team develops a model that they can deploy to production, they register the model in the Machine Learning workspace registry. CI pipelines that are triggered, either automatically by model registration or by gated human-in-the-loop approval, promote the model and any other model dependencies to the model deployment phase. Personas associated with this stage are typically machine learning engineers. Model deployment (outer loop phase) The model deployment, or outer loop phase, consists of preproduction staging and testing, production deployment, and monitoring of the model, data, and infrastructure. When the model meets the criteria of the organization and use case, CD pipelines promote the model and related assets through production, monitoring, and potential retraining. Personas associated with this phase are primarily machine learning engineers. Staging and test The staging and test phase varies according to customer practices. This phase typically includes operations such as retraining and testing the model candidate on production data, test deployments for endpoint performance, data quality checks, unit testing, and responsible AI checks for model and data bias. This phase takes place in one or more dedicated and secure Machine Learning workspaces. Production deployment After a model passes the staging and test phase, machine learning engineers can use human-in-the-loop gated approval to promote it to production. Model deployment options include a managed batch endpoint for batch scenarios or either a managed online endpoint or Kubernetes deployment that uses Azure Arc for online, near real-time scenarios. Production typically takes place in one or more dedicated and secure Machine Learning workspaces. Monitoring Machine learning engineers monitor components in staging, testing, and production to collect metrics related to changes in performance of the model, data, and infrastructure. They can use those metrics to take action. Model and data monitoring can include checking for model and data drift, model performance on new data, and responsible AI problems. Infrastructure monitoring might identify slow endpoint response, inadequate compute capacity, or network problems. Data and model monitoring: events and actions Based on model and data criteria, such as metric thresholds or schedules, automated triggers and notifications can implement appropriate actions to take. For example, a trigger might retrain a model to use new production data and then loopback the model to staging and testing for a preproduction evaluation. Or a model or data problem might trigger an action that requires a loopback to the model development phase where data scientists can investigate the problem and potentially develop a new model. Infrastructure monitoring: events and actions Automated triggers and notifications can implement appropriate actions to take based on infrastructure criteria, such as an endpoint response lag or insufficient compute for the deployment. Automatic triggers and notifications might trigger a loopback to the setup and administration phase where the infrastructure team can investigate the problem and potentially reconfigure the compute and network resources. Machine Learning CV architecture Download a Visio file of this architecture. Workflow for the CV architecture The Machine Learning CV architecture is based on the classical machine learning architecture, but it has modifications that are specific to supervised CV scenarios. Data estate This component demonstrates the data estate of the organization and potential data sources and targets for a data science project. Data engineers are the primary owners of this component in the MLOps v2 lifecycle. The Azure data platforms in this diagram aren't exhaustive or prescriptive. Images for CV scenarios can come from various data sources. For efficiency when developing and deploying CV models with Machine Learning, we recommend Azure Blob Storage and Azure Data Lake Storage. Administration and setup This component is the first step in the MLOps v2 deployment. It consists of all tasks related to the creation and management of resources and roles associated with the project. For CV scenarios, administration and setup of the MLOps v2 environment is largely the same as for classical machine learning but includes an extra step. The infrastructure team uses the labeling feature of Machine Learning or another tool to create image labeling and annotation projects. Model development (inner loop phase) The inner loop phase consists of an iterative data science workflow performed within a dedicated and secure Machine Learning workspace. The primary difference between this workflow and the classical machine learning scenario is that image labeling and annotation is a key component of this development loop. Machine Learning registries After the data science team develops a model that they can deploy to production, they register the model in the Machine Learning workspace registry. CI pipelines that are triggered automatically by model registration or by gated human-in-the-loop approval promote the model and any other model dependencies to the model deployment phase. Model deployment (outer loop phase) The model deployment or outer loop phase consists of preproduction staging and testing, production deployment, and monitoring of the model, data, and infrastructure. When the model meets the criteria of the organization and use case, CD pipelines promote the model and related assets through production, monitoring, and potential retraining. Staging and test The staging and test phase varies according to customer practices. This phase typically includes operations such as test deployments for endpoint performance, data quality checks, unit testing, and responsible AI checks for model and data bias. For CV scenarios, machine learning engineers don't need to retrain the model candidate on production data because of resource and time constraints. The data science team can instead use production data for model development. The candidate model registered from the development loop is evaluated for production. This phase takes place in one or more dedicated and secure Machine Learning workspaces. Production deployment After a model passes the staging and test phase, machine learning engineers can use human-in-the-loop gated approval to promote it to production. Model deployment options include a managed batch endpoint for batch scenarios or either a managed online endpoint or Kubernetes deployment that uses Azure Arc for online, near real-time scenarios. Production typically takes place in one or more dedicated and secure Machine Learning workspaces. Monitoring Machine learning engineers monitor components in staging, testing, and production to collect metrics related to changes in performance of the model, data, and infrastructure. They can use those metrics to take action. Model and data monitoring can include checking for model performance on new images. Infrastructure monitoring might identify slow endpoint response, inadequate compute capacity, or network problems. Data and model monitoring: events and actions The data and model monitoring and event and action phases of MLOps for natural language processing are the key differences from classical machine learning. Automated retraining is typically not done in CV scenarios when model performance degradation on new images is detected. In this case, a human-in-the-loop process is necessary to review and annotate new images for the model that performs poorly. The next action often goes back to the model development loop to update the model with the new images. Infrastructure monitoring: events and actions Automated triggers and notifications can implement appropriate actions to take based on infrastructure criteria, such as an endpoint response lag or insufficient compute for the deployment. Automatic triggers and notifications might trigger a loopback to the setup and administration phase where the infrastructure team can investigate the problem and potentially reconfigure environment, compute, and network resources. Machine Learning natural language processing architecture Download a Visio file of this architecture. Workflow for the natural language processing architecture The Machine Learning natural language processing architecture is based on the classical machine learning architecture, but it has some modifications that are specific to NLP scenarios. Data estate This component demonstrates the organization data estate and potential data sources and targets for a data science project. Data engineers are the primary owners of this component in the MLOps v2 lifecycle. The Azure data platforms in this diagram aren't exhaustive or prescriptive. A green check mark indicates sources and targets that represent recommended best practices that are based on the customer use case. Administration and setup This component is the first step in the MLOps v2 deployment. It consists of all tasks related to the creation and management of resources and roles associated with the project. For natural language processing scenarios, administration and setup of the MLOps v2 environment is largely the same as for classical machine learning, but with an extra step: create text labeling and annotation projects by using the labeling feature of Machine Learning or another tool. Model development (inner loop phase) The inner loop phase consists of an iterative data science workflow performed within a dedicated and secure Machine Learning workspace. The typical NLP model development loop differs from the classical machine learning scenario in that the typical development steps for this scenario include annotators for sentences and tokenization, normalization, and embeddings for text data. Machine Learning registries After the data science team develops a model that they can deploy to production, they register the model in the Machine Learning workspace registry. CI pipelines that are triggered automatically by model registration or by gated human-in-the-loop approval promote the model and any other model dependencies to the model deployment phase. Model deployment (outer loop phase) The model deployment or outer loop phase consists of preproduction staging and testing, production deployment, and monitoring of the model, data, and infrastructure. When the model meets the criteria of the organization and use case, CD pipelines promote the model and related assets through production, monitoring, and potential retraining. Staging and test The staging and test phase varies according to customer practices. This phase typically includes operations such as retraining and testing the model candidate on production data, test deployments for endpoint performance, data quality checks, unit testing, and responsible AI checks for model and data bias. This phase takes place in one or more dedicated and secure Machine Learning workspaces. Production deployment After a model passes the staging and test phase, machine learning engineers can use human-in-the-loop gated approval to promote it to production. Model deployment options include a managed batch endpoint for batch scenarios or either a managed online endpoint or Kubernetes deployment that uses Azure Arc for online, near real-time scenarios. Production typically takes place in one or more dedicated and secure Machine Learning workspaces. Monitoring Machine learning engineers monitor components in staging, testing, and production to collect metrics related to changes in performance of the model, data, and infrastructure. They can use those metrics to take action. Model and data monitoring can include checking for model and data drift, model performance on new text data, and responsible AI problems. Infrastructure monitoring might identify problems, such as slow endpoint response, inadequate compute capacity, and network problems. Data and model monitoring: events and actions As with the CV architecture, the data and model monitoring and event and action phases of MLOps for natural language processing are the key differences from classical machine learning. Automated retraining isn't typically done in natural language processing scenarios when model performance degradation on new text is detected. In this case, a human-in-the-loop process is necessary to review and annotate new text data for the model that performs poorly. Often the next action is to go back to the model development loop to update the model with the new text data. Infrastructure monitoring: events and actions Automated triggers and notifications can implement appropriate actions to take based on infrastructure criteria, such as an endpoint response lag or insufficient compute for the deployment. Automatic triggers and notifications might trigger a loopback to the setup and administration phase where the infrastructure team can investigate the problem and potentially reconfigure compute and network resources. Components Machine Learning is a cloud service that you can use to train, score, deploy, and manage machine learning models at scale. Azure Pipelines is a build-and-test system that's based on Azure DevOps and is used for build and release pipelines. Azure Pipelines splits these pipelines into logical steps called tasks. GitHub is a code-hosting platform for version control, collaboration, and CI/CD workflows. Azure Arc is a platform that uses Azure Resource Manager to manage Azure resources and on-premises resources. The resources can include virtual machines, Kubernetes clusters, and databases. Kubernetes is an open-source system that you can use to automate the deployment, scaling, and management of containerized applications. Azure Data Lake Storage is a Hadoop-compatible file system. It has an integrated hierarchical namespace and the massive scale and economy of Blob Storage. Azure Synapse Analytics is a limitless analytics service that brings together data integration, enterprise data warehousing, and big data analytics. Azure Event Hubs is a service that ingests data streams that client applications generate. It then ingests and stores streaming data, which preserves the sequence of events received. Customers can connect to the hub endpoints to retrieve messages for processing. This architecture uses Data Lake Storage integration. Other considerations The preceding MLOps v2 architectural pattern has several critical components, including role-based access control (RBAC) that aligns with business stakeholders, efficient package management, and robust monitoring mechanisms. These components collectively contribute to the successful implementation and management of machine learning workflows. Persona-based RBAC It's crucial that you manage access to machine learning data and resources. RBAC provides a robust framework to help you manage who can perform specific actions and access specific areas within your solution. Design your identity segmentation strategy to align with the lifecycle of machine learning models in Machine Learning and the personas included in the process. Each persona has a specific set of responsibilities that are reflected in their RBAC roles and group membership. Example personas To support appropriate segmentation in a machine learning workload, consider the following common personas that inform the identity-based RBAC group design. Data scientist and machine learning engineer Data scientists and machine learning engineers perform various machine learning and data science activities across the software development life cycle of a project. Their duties include exploratory data analysis and data preprocessing. Data scientists and machine learning engineers are responsible for training, evaluating, and deploying models. These roles' responsibilities also include break-fix activities for machine learning models, packages, and data. These duties are out of scope for the platform's technical support team. Type: Person Project specific: Yes Data analyst Data analysts provide the necessary input for data science activities, such as running SQL queries for business intelligence. This role's responsibilities include working with data, performing data analysis, and supporting model development and model deployment. Type: Person Project specific: Yes Model tester Model testers conduct tests in testing and staging environments. This role provides functional segregation from the CI/CD processes. Type: Person Project specific: Yes Business stakeholders Business stakeholders are associated with the project, such as a marketing manager. Type: Person Project specific: Yes Project lead or data science lead The data science lead is a project administration role for the Machine Learning workspace. This role also does break-fix activities for the machine learning models and packages. Type: Person Project specific: Yes Project or product owner (Business owner) Business stakeholders are responsible for the Machine Learning workspace according to data ownership. Type: Person Project specific: Yes Platform technical support Platform technical support is the technical support staff responsible for break-fix activities across the platform. This role covers infrastructure or service but not the machine learning models, packages, or data. These components remain under the data scientist or machine learning engineer role and are the project lead's responsibility. Type: Person Project specific: No Model end user Model end users are the end consumers of the machine learning model. Type: Person or Process Project specific: Yes CI/CD processes CI/CD processes release or roll back changes across platform environments. Type: Process Project specific: No Machine Learning workspace Machine Learning workspaces use managed identities to interact with other parts of Azure. This persona represents the various services that make up a Machine Learning implementation. These services interact with other parts of the platform, such as the development workspace that connects with the development data store. Type: Process Project specific: No Monitoring processes Monitoring processes are compute processes that monitor and alert based on platform activities. Type: Process Project specific: No Data governance processes Data governance processes scan the machine learning project and data stores for data governance. Type: Process Project specific: No Microsoft Entra group membership When you implement RBAC, Microsoft Entra groups provide a flexible and scalable way to manage access permissions across different personas. You can use Microsoft Entra groups to manage users that need the same access and permissions to resources, such as potentially restricted apps and services. Instead of adding special permissions to individual users, you create a group that applies the special permissions to every member of that group. In this architectural pattern, you can couple these groups with a Machine Learning workspace setup, such as a project, team, or department. You can associate users with specific groups to define fine-grained access policies. The policies grant or restrict permissions to various Machine Learning workspaces based on job functions, project requirements, or other criteria. For example, you can have a group that grants all data scientists access to a development workspace for a specific use case. Identity RBAC Consider how you can use the following built-in Azure RBAC roles to apply RBAC to production and preproduction environments. For the architecture in this article, the production environments include staging, testing, and production environments. The preproduction environments include development environments. The following RBAC roles are based on the personas described earlier in this article. Standard roles R = Reader C = Contributor O = Owner Component specific roles ADS = Machine Learning Data Scientist ACO = Machine Learning Compute Operator AcrPush = Azure Container Registry Push DOPA = DevOps Project Administrators DOPCA = DevOps Project Collection Administrators LAR = Log Analytics Reader LAC = Log Analytics Contributor MR = Monitoring Reader MC = Monitoring Contributor KVA = Key Vault Administrator KVR = Key Vault Reader These Azure RBAC role abbreviations correspond with the following tables. Production environment Persona Machine Learning workspace Azure Key Vault Container Registry Azure Storage account Azure DevOps Azure Artifacts Log Analytics workspace Azure Monitor Data scientist R LAR MR Data analyst Model tester Business stakeholders MR Project lead (Data science lead) R R, KVR R LAR MR Project/product owner MR Platform technical support O O, KVA DOPCA O O O Model end user CI/CD processes O O, KVA AcrPush DOPCA O O O Machine Learning workspace R C C Monitoring processes R LAR MR Data governance processes R R R R R Preproduction environment Persona Machine Learning workspace Key Vault Container Registry Storage account Azure DevOps Azure Artifacts Log Analytics workspace Azure Monitor Data scientist ADS R, KVA C C C C LAC MC Data analyst R C LAR MC Model tester R R, KVR R R R R LAR MR Business stakeholders R R R R R Project lead (Data science lead) C C, KVA C C C C LAC MC Project/product owner R R MR Platform technical support O O, KVA O O DOPCA O O O Model end user CI/CD processes O O, KVA AcrPush O DOPCA O O O Machine Learning workspace R, KVR C C Monitoring processes R R R R R R LAC Data governance processes R R R Note Every persona retains access for the project's duration except platform technical support, which has temporary or just-in-time Microsoft Entra Privileged Identity Management (PIM) access. RBAC plays a vital role in securing and streamlining MLOps workflows. RBAC restricts access based on assigned roles and prevents unauthorized users from accessing sensitive data, which mitigates security risks. Sensitive data includes training data or models and critical infrastructure, such as production pipelines. You can use RBAC to ensure compliance with data privacy regulations. RBAC also provides a clear record of access and permissions, which simplifies auditing, makes it easy to identify security gaps, and tracks user activity. Package management Dependencies on various packages, libraries, and binaries are common throughout the MLOps lifecycle. These dependencies, often community-developed and rapidly evolving, necessitate subject matter expert knowledge for proper use and understanding. You must ensure that the appropriate people have secure access to diverse assets, such as packages and libraries, but you must also prevent vulnerabilities. Data scientists encounter this problem when they assemble specialized building blocks for machine learning solutions. Traditional software management approaches are costly and inefficient. Other approaches provide more value. To manage these dependencies, you can use a secure, self-serve, package management process based on the Quarantine pattern. You can design this process to allow data scientists to self-serve from a curated list of packages and ensure that the packages are secure and compliant with organizational standards. This approach includes safe-listing three industry standard machine learning package repositories: Microsoft Artifact Registry, Python Package Index (PyPI), and Conda. Safe-listing enables self-serve from individual Machine Learning workspaces. Then use an automated testing process during the deployment to scan the resulting solution containers. Failures elegantly exit the deployment process and remove the container. The following diagram and process flow demonstrates this process: Process flow Data scientists that work in a Machine Learning workspace that has a network configuration can self-serve machine learning packages on-demand from the machine learning package repositories. An exception process is required for everything else by using the private storage pattern, which is seeded and maintained by using a centralized function. Machine Learning delivers machine learning solutions as Docker containers. As these solutions are developed, they're uploaded to Container Registry. Microsoft Defender for Containers generates vulnerability assessments for the container image. Solution deployment occurs through a CI/CD process. Microsoft Defender for DevOps is used across the stack to provide security posture management and threat protection. The solution container is deployed only if it passes each of the security processes. If the solution container fails a security process, the deployment fails with error notifications and full audit trails. The solution container is discarded. The previous process flow provides a secure, self-serve, package management process for data scientists and ensures that the packages are secure and compliant with organizational standards. To balance innovation and security, you can grant data scientists self-service access to common machine learning packages, libraries, and binaries in preproduction environments. Require exceptions for less common packages. This strategy ensures that data scientists can remain productive during development, which prevents a major bottleneck during delivery. To streamline your release processes, containerize environments for use in production environments. Containerized environments reduce toil and ensure continued security through vulnerability scanning. This process flow provides a repeatable approach that you can use across use cases to the time of delivery. It reduces the overall cost to build and deploy machine learning solutions within your enterprise. Monitoring In MLOps, monitoring is crucial for maintaining the health and performance of machine learning systems and ensuring that models remain effective and aligned with business goals. Monitoring supports governance, security, and cost controls during the inner loop phase. And it provides observability into the performance, model degradation, and usage when deploying solutions during the outer loop phase. Monitoring activities are relevant for personas such as Data Scientists, Business Stakeholders, Project Leads, Project Owners, Platform Technical Support, CI/CD processes, and Monitoring Processes. Choose your monitoring and verification platform depending on your Machine Learning workspace setup, such as a project, team, or department. Model performance Monitor model performance to detect model problems and performance degradation early. Track performance to ensure that models remain accurate, reliable, and aligned with business objectives. Data drift Data drift tracks changes in the distribution of a model's input data by comparing it to the model's training data or recent past production data. These changes are a result of changes in market dynamics, feature transformation changes, or upstream data changes. Such changes can degrade model performance, so it's important to monitor for drift to ensure timely remediation. To perform a comparison, data drift refactoring requires recent production datasets and outputs. Environment: Production Azure facilitation: Machine Learning – Model monitoring Prediction drift Prediction drift tracks changes in the distribution of a model's prediction outputs by comparing it to validation, test-labeled, or recent production data. To perform a comparison, data drift refactoring requires recent production datasets and outputs. Environment: Production Azure facilitation: Machine Learning – Model monitoring Resource Use several model serving endpoint metrics to indicate quality and performance, such as CPU or memory usage. This approach helps you learn from production to help drive future investments or changes. Environment: All Azure facilitation: Monitor - Online endpoints metrics Usage metrics Monitor the usage of endpoints to ensure that you meet organization-specific or workload-specific key performance indicators, track usage patterns, and diagnose and remediate problems that your users experience. Client requests Track the number of client requests to the model endpoint to understand the active usage profile of the endpoints, which can affect scaling or cost optimization efforts. Environment: Production Azure facilitation: Monitor - Online endpoints metrics, such as RequestsPerMinute. Notes: You can align acceptable thresholds to t-shirt sizing or anomalies that are tailored to your workload's needs. Retire models that are no longer in use from production. Throttling delays Throttling delays are slowdowns in the request and response of data transfers. Throttling happens at the Resource Manager level and the service level. Track metrics at both levels. Environment: Production Azure facilitation: Monitor - Resource Manager, sum of RequestThrottlingDelayMs, ResponseThrottlingDelayMs. Machine Learning - To check information about your endpoints' requests, you can enable online endpoint traffic logs. You can use a Log Analytics workspace to process logs. Notes: Align acceptable thresholds to your workload's service-level objectives (SLOs) or service-level agreements (SLAs) and the solution's nonfunctional requirements (NFRs). Errors generated Track response code errors to help measure service reliability and ensure early detection of service problems. For example, a sudden increase in 500 server error responses could indicate a critical problem that needs immediate attention. Environment: Production Azure facilitation: Machine Learning - Enable online endpoint traffic logs to check information about your request. For example, you can check the count of XRequestId by using ModelStatusCode or ModelStatusReason. You can use a Log Analytics workspace to process logs. Notes: All HTTP responses codes in the 400 and 500 range are classified as an error. Cost optimization Cost management and optimization in a cloud environment are crucial because they help workloads control expenses, allocate resources efficiently, and maximize value from their cloud services. Workspace compute When monthly operating expenses reach or exceed a predefined amount, generate alerts to notify relevant stakeholders, such as project leads or project owners, based on the workspace setup boundaries. You can determine your workspace setup based on project, team, or department-related boundaries. Environment: All Azure facilitation: Microsoft Cost Management - Budget alerts Notes: Set budget thresholds based on the initial NFRs and cost estimates. Use multiple threshold tiers. Multiple threshold tiers ensure that stakeholders get appropriate warning before the budget is exceeded. These stakeholders might include business leads, project owners, or project Leads depending on the organization or workload. Consistent budget alerts could also be a trigger for refactoring to support greater demand. Workspace staleness If a Machine Learning workspace shows no signs of active use based on the associated compute usage for the intended use case, a project owner might decommission the workspace if it's no longer needed for a given project. Environment: Preproduction Azure facilitation: Monitor - Machine Learning metrics Machine Learning - Workspace metrics, such as the count of active cores over a period of time Notes: Active cores should equal zero with aggregation of count. Align date thresholds to the project schedule. Security Monitor to detect deviations from appropriate security controls and baselines to ensure that Machine Learning workspaces are compliant with your organization's security policies. You can use a combination of predefined and custom-defined policies. Environment: All Azure facilitation: Azure Policy for Machine Learning Endpoint security To gain visibility into business-critical APIs, implement targeted security monitoring of all Machine Learning endpoints. You can investigate and improve your API security posture, prioritize vulnerability fixes, and quickly detect active real-time threats. Environment: Production Azure facilitation: Microsoft Defender for APIs offers broad lifecycle protection, detection, and response coverage for APIs. Notes: Defender for APIs provides security for APIs that are published in Azure API Management. You can onboard Defender for APIs in the Microsoft Defender for Cloud portal or within the API Management instance in the Azure portal. You must integrate Machine Learning online endpoints with API Management. Deployment monitoring Deployment monitoring ensures that any endpoints you create adhere to your workload or organization policies and are free from vulnerabilities. This process requires that you enforce compliance policies on your Azure resources before and after deployment, provide continued security through vulnerability scanning, and ensure that the service meets SLOs while in operation. Standards and governance Monitor to detect deviations from appropriate standards and ensure that your workload adheres to guardrails. Environment: All Azure facilitation: Managed policy assignment and lifecycle through Azure Pipelines to treat policy as code. PSRule for Azure provides a testing framework for Azure infrastructure as code. You can use Enterprise Azure policy as code in CI/CD-based system deploy policies, policy sets, assignments, policy exemptions, and role assignments. Notes: For more information, see Azure guidance for Machine Learning regulatory compliance. Security scanning Implement automated security scans as part of the automated integration and deployment processes. Environment: All Azure facilitation: Defender For DevOps Notes: You can use apps in Azure Marketplace to extend this process for non-Microsoft security testing modules. Ongoing service Monitor the ongoing service of an API for performance optimization, security, and resource usage. Ensure timely error detection, efficient troubleshooting, and compliance with standards. Environment: Production Azure facilitation: Monitor - Machine Learning metrics Machine Learning - You can enable online endpoint traffic logs to check information about your service. Contributors This article is maintained by Microsoft. It was originally written by the following contributors. Principal author: Setu Chokshi | Senior Technical Specialist Other contributors: Scott Mckinnon | Cloud Solution Architect Darren Turchiarelli | Cloud Solution Architect Leo Kozhushnik | Cloud Solution Architect To see non-public LinkedIn profiles, sign in to LinkedIn. Next steps What is Azure Pipelines? Azure Arc overview What is Machine Learning? Data in Machine Learning Azure MLOps v2 GitHub repository End-to-end machine learning operations (MLOps) with Machine Learning Introduction to Azure Data Lake Storage Gen2 Azure DevOps documentation GitHub Docs Synapse Analytics documentation Event Hubs documentation How Machine Learning works: resources and assets (v2) What are Machine Learning pipelines? Related resources Choose a Microsoft Azure AI services technology Natural language processing technology Compare the machine learning products and technologies from Microsoft What is the Team Data Science Process? Feedback Was this page helpful? Yes No Additional resources", "metadata": {"source": "https://docs.microsoft.com/en-us/azure/architecture/ai-ml/guide/machine-learning-operations-v2", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 5819, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9700000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Generative AI operations for organizations with MLOps investments\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Table of contents Exit focus mode Ask Learn Ask Learn Table of contents Read in English Add Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories. Access to this page requires authorization. You can try changing directories. Generative AI operations for organizations with MLOps investments 2025-06-04 Feedback This article provides guidance to workload teams that have existing machine learning operations (MLOps) investments and want to extend those investments to include generative AI technology and patterns in their workload. To operationalize generative AI workload features, you need to extend your MLOps investments with generative AI operations (GenAIOps), sometimes known as LLMOps. This article outlines technical patterns that are common to both traditional machine learning and generative AI workloads, and patterns unique to generative AI. Understand where you can apply existing investments in operationalization and where you need to extend those investments. The planning and implementation of MLOps and GenAIOps are part of a core design area in AI workloads on Azure. For more information about why these workloads need specialized operations, see MLOps and GenAIOps for AI workloads on Azure. Generative AI technical patterns Generative AI workloads differ from traditional machine learning workloads in several ways: Focus on generative models. Traditional machine learning workloads focus on training new models for specific tasks. Generative AI workloads consume and sometimes fine-tune generative models that can address a broader range of use cases. Some of these models are multimodal. Focus on extending the models. The key asset in traditional machine learning is the trained and deployed model. Access to the model is provided to client code in one or more workloads, but the workload typically isn't part of the MLOps process. With generative AI solutions, a key aspect of the solution is the prompt provided to the generative model. The prompt must be composed of instructions and often contains context data from one or more data stores. The system that orchestrates the logic, calls to the various back ends or agents, generates the prompt, and calls to the generative model is part of the generative AI system that you govern with GenAIOps. Some generative AI solutions use traditional machine learning practices like model training and fine-tuning. However, these solutions introduce new patterns that you should standardize. There are three broad categories of technical patterns for generative AI solutions: Pretraining and fine-tuning Prompt engineering Retrieval-augmented generation (RAG) Training and fine-tuning language models Many generative AI solutions use existing foundation language models that don't require fine-tuning before use. However, some use cases can benefit from fine-tuning a foundation model or training a new generative AI model, like a small language model (SLM). Training a new SLM and fine-tuning a generative foundation model follow the same logical processes as training traditional machine learning models. These processes should use your existing MLOps investments. Prompt engineering Prompt engineering includes all the processes included in designing an effective prompt that's sent as input to a generative model. There's generally an orchestrator that controls a workflow that generates the prompt. The orchestrator can call into multiple data stores directly or indirectly through agents to gather information, including grounding data. It then applies the required logic to generate the most effective prompt. The orchestrator is then deployed as an API endpoint that the client code accesses in an intelligent application. The following diagram shows an architecture for prompt engineering. The diagram illustrates a flow. An intelligent application collects input from a user. The intelligent application and a headless intelligent application send the input to an orchestrator. The orchestrator calls the data stores. And then the orchestrator sends a prompt to Azure OpenAI Service. This category of technical patterns can address many use cases: Classification Translation Summarization RAG RAG RAG is an architectural pattern that uses prompt engineering to incorporate domain-specific data as the grounding data for a language model. The language model is trained against a specific set of data. Your workload might require reasoning over data that's specific to your company, customers, or domain. In RAG solutions, your data is queried, and the most relevant results are provided to the language model as part of the prompt, usually through an orchestration layer. A typical RAG implementation is to break up your source data into chunks and store them in a vector store along with metadata. Vector stores, such as Azure AI Search, allow you to perform both textual and vector similarity searches to return contextually relevant results. RAG solutions can also use other data stores to return grounding data. The following diagram illustrates a RAG architecture that includes data from documents. The diagram illustrates two flows. The first flow starts with a user and then flows to an intelligent application. From there, the flow leads to an orchestrator. From the orchestrator, the flow leads to Azure OpenAI Service and to Azure AI Search, which is the last item in the second flow. The second flow starts with documents and then flows to four stages: chunk documents, enrich chunks, embed chunks, and index chunks. From there, the flow leads to the same Azure AI Search instance that connects to the first flow. Extend MLOps for generative AI technical patterns Your MLOps process addresses both inner loop and outer loop processes. Generative AI technical patterns also have many of the same activities. In some cases, you apply your existing MLOps investments. In other cases, you need to extend them: Inner loop DataOps Experimentation Evaluation Outer loop Deployment Inferencing and monitoring Feedback loop DataOps Both MLOps and GenAIOps apply the fundamentals of data operations (DataOps) to create extensible and reproducible workflows. These workflows ensure that data is cleaned, transformed, and formatted correctly for experimentation and evaluation. Workflow reproducibility and data versioning are important features of DataOps for all technical patterns. The sources, types, and intent of the data depend on the pattern. Training and fine-tuning This technical pattern should fully take advantage of the existing DataOps investments from your MLOps implementation. Reproducibility and data versioning allow you to experiment with different feature engineering data, compare the performance of the different models, and reproduce results. RAG and prompt engineering The intent for the data in RAG solutions is to provide grounding data (or context) that's presented to the language model as part of a prompt. RAG solutions often require the processing of large documents or data sets into a collection of right-sized, semantically relevant chunks, and persisting those chunks in a vector store. For more information, see Design and develop a RAG solution. Reproducibility and data versioning for RAG solutions allows you to experiment with different chunking and embedding strategies, compare performance, and roll back to previous versions. Data pipelines for chunking documents aren't part of DataOps in traditional MLOps, so you have to extend your architecture and operations. The data pipelines can read data from disparate sources that include both structured and unstructured data. They can also write the transformed data to different targets. You must extend your pipelines to include the data stores that you use for grounding data. Typical data stores for these patterns are vector stores like AI Search. Just like training and fine-tuning, Azure Machine Learning pipelines or other data pipelining tools can be used to orchestrate the stages of chunking. Search index maintenance You also must extend your operations to maintain the freshness and validity of the search indexes in your data stores. You might need to periodically rebuild these indexes if you can't incrementally add, remove, or update data in place. Index updates must meet the business requirements for data freshness, the nonfunctional requirements, such as performance and availability, and the compliance requirements, such as right to be forgotten requests. You need to extend your existing MLOps process to account for maintaining and updating search indexes to ensure accuracy, compliance, and optimal performance. Experimentation Experimentation, a part of the inner loop, is the iterative process of creating, evaluating, and refining your solution. The following sections describe experimentation for the typical generative AI technical patterns. Training and fine-tuning When you fine-tune an existing language model or train an SLM, you can take advantage of your current MLOps investments. For instance, Machine Learning pipelines provide a toolkit for conducting experiments efficiently and effectively. These pipelines enable you to manage the entire fine-tuning process, from data preprocessing to model training and evaluation. RAG and prompt engineering Experimentation with prompt engineering and RAG workloads requires you to extend your MLOps investments. For these technical patterns, the workload doesn't end with the model. The workload requires an orchestrator, which is a system that can run logic, call data stores or agents for required information like grounding data, generate prompts, and call language models. The data stores and indexes in the stores are also part of the workload. You need to extend your operations to govern these aspects of the workload. You can experiment on multiple dimensions for prompt engineering solutions, including different instructions, personas, examples, constraints, and advanced techniques like prompt chaining. When you experiment with RAG solutions, you can also experiment with other areas: Chunking strategies Methods for enriching chunks Embedding model selection Configuration of the search index Types of searches to perform, such as vector, full-text, and hybrid As described in DataOps, reproducibility and data versioning are key to experimentation. A good experimentation framework enables you to store inputs, such as changes to hyperparameters or prompts, along with outputs to be used when you evaluate the experiment. Just like in your existing MLOps environment, you can take advantage of frameworks such as Machine Learning pipelines. Machine Learning pipelines have features that support indexing by integrating with vector stores like AI Search. Your GenAIOps environment can take advantage of these pipeline features. Evaluation and experimentation Evaluation is key in the iterative experimentation process of building, evaluating, and refining your solution. The evaluation of your changes provides the feedback that you need to make your refinements or validate that the current iteration meets your requirements. The following sections describe evaluation in the experimentation phase for the typical generative AI technical patterns. Training and fine-tuning For the evaluation of fine-tuned or trained generative AI models, you should take advantage of your existing MLOps investments. For example, if you use Machine Learning pipelines to orchestrate your machine learning model training, you can use the same evaluation features to fine-tune foundation language models or train new SLMs. These features include the Evaluate Model component, which computes industry-standard evaluation metrics for specific model types and compares results across models. If your workload uses Azure AI Foundry, you could instead extend your MLOps process to include its evaluation capabilities found in the Evaluation SDK. RAG and prompt engineering You need to extend your existing MLOps investments to evaluate generative AI solutions. You can use tools like prompt flow, which provides a framework for evaluation. Prompt flow enables teams to define custom evaluation logic by specifying criteria and metrics to assess the performance of various prompt variants and large language models (LLMs). This structured approach allows you to compare different configurations side by side, like hyperparameter or architectural variations, to identify the optimal setup for specific tasks. Jobs in prompt flow automatically capture both input and output data throughout the experimentation process to create a comprehensive trial record. You can gain insights and identify promising configurations that can inform future iterations by analyzing this data. You can accelerate the development of your generative AI solutions by using prompt flows to conduct efficient and systematic experimentation. The experimentation process remains consistent, regardless of the use case for your generative AI solution. These use cases include classification, summarization, translation, and RAG. The important difference is the metrics that you use to evaluate the different use cases. Consider the following metrics based on use case: Translation: BLEU Summarization: ROUGE, BLEU, BERTScore, METEOR Classification: Precision, Recall, Accuracy, Cross-entropy RAG: Groundedness, Relevancy Note For more information about how to evaluate language models and RAG solutions, see LLM end-to-end evaluation. Generative AI solutions generally extend the responsibilities of the machine learning team from training models to prompt engineering and managing grounding data. Because prompt engineering and RAG experimentation and evaluation don't necessarily require data scientists, you might be tempted to use other roles, like software engineers and data engineers, to perform these functions. You might encounter challenges if you omit data scientists from the process of experimenting with prompt engineering and RAG solutions. Other roles often lack the specialized training needed to scientifically evaluate results as effectively as data scientists. For more information, see Design and develop a RAG solution. Investing in generative AI solutions helps alleviate some of the workload on your data science resources. The role of software engineers expands in these solutions. For example, software engineers are great resources for managing the orchestration responsibility in generative AI solutions, and they're adept at setting up the evaluation metrics. It's important to have data scientists review this work. They have the training and experience to understand how to properly evaluate the experiments. Deployment Some generative AI solutions include deploying custom-trained models or fine-tuning existing models. For generative AI solutions, you need to include the extra tasks of deploying the orchestrators and any data stores. The following sections describe deployment for typical generative AI technical patterns. Training and fine-tuning You should use your existing MLOps investments, with some possible adjustments, to deploy generative AI models and fine-tune foundation models. For example, to fine-tune an LLM in Azure OpenAI Service, you need to ensure that your training and validation datasets are in JSONL format, and you need to upload the data via a REST API. You also need to create a fine-tuning job. To deploy a trained SLM, you can take advantage of your existing MLOps investments. RAG and prompt engineering For RAG and prompt engineering, other considerations include orchestration logic, modifications to data stores such as indexes and schemas, and adjustments to data pipeline logic. Orchestration logic is typically encapsulated in a framework like Semantic Kernel. You can deploy the orchestrator to different compute resources, including resources that you might currently deploy custom models to. Also, agent orchestrators can be low code solutions, such as the Azure AI Foundry Agent Service. For more information about how to deploy a chat agent, see Baseline AI Foundry chat reference architecture. Deployments of changes to database resources, like changes to data models or indexes, are new tasks that need to be handled in GenAIOps. A common practice when working with LLMs is to use a gateway in front of the LLM. Many generative AI architectures that consume platform-hosted language models, like those served from Azure OpenAI, include a gateway like Azure API Management. The gateway use cases include load balancing, authentication, and monitoring. The gateway can play a role in deployment of newly trained or fine-tuned models, which allows you to progressively roll out new models. The use of a gateway, along with model versioning, enables you to minimize risk when you deploy changes and to roll back to previous versions when problems occur. Deployments of elements that are specific to generative AI, such as the orchestrator, should follow proper operational procedures: Rigorous testing, including unit tests Integration tests A/B tests End-to-end tests Roll-out strategies, like canary deployments or blue-green deployments Because the deployment responsibilities for generative AI applications extend beyond model deployment, you might need extra job roles to manage the deployment and monitoring of components like the user interface, the orchestrator, and the data stores. These roles are often aligned to DevOps engineer skill sets. Inferencing and monitoring Inferencing is the process of passing input to a trained and deployed model, which then generates a response. You should monitor both traditional machine learning and generative AI solutions from the perspectives of operational monitoring, learning from production, and resource management. Operational monitoring Operational monitoring is the process of observing the ongoing operations of the system, including DataOps and model training. This type of monitoring looks for deviations, including errors, changes to error rates, and changes to processing times. For model training and fine-tuning, you generally observe the DataOps for processing feature data, model training, and fine-tuning. The monitoring of these inner-loop processes should take advantage of your existing MLOps and DataOps investments. For prompt engineering in generative AI solutions, you have extra monitoring concerns. You must monitor the data pipelines that process the grounding data or other data that's used to generate prompts. This processing might include data store operations like building or rebuilding indexes. In a multi-agent system, you need to monitor the availability, performance characteristics, and response quality and consistency of the agents that your orchestrator interfaces with. Learn from production A crucial aspect of monitoring during the inferencing stage is learning from production. Monitoring for traditional machine learning models tracks metrics like accuracy, precision, and recall. A key goal is to avoid prediction drift. Solutions that use generative models for predictions, such as a GPT model for classification, should take advantage of existing MLOps monitoring investments. Solutions that use generative models to reason over grounding data use metrics like groundedness, completeness, usage, and relevancy. The goal is to ensure that the model fully answers the query and bases the response on its context. In this solution, you need to try to prevent problems like data drift. You want to ensure that the grounding data and the prompt that you provide to the model are maximally relevant to the user query. Solutions that use generative models for nonpredictive tasks, like RAG solutions, often benefit from human feedback from end users to evaluate usefulness sentiments. User interfaces can capture feedback like thumbs up or down. You can use this data to periodically evaluate the responses. A typical pattern for generative AI solutions is to deploy a gateway in front of the generative models. One of the use cases for the gateway is to monitor the foundation models. You can use the gateway to log input prompts and model output. Another key area to monitor for generative solutions is content safety. The goal is to moderate responses and detect harmful or undesirable content. Microsoft Azure AI Content Safety Studio is a tool that you can use to moderate content. Resource management Generative solutions that use models exposed as a service, like Azure OpenAI, have different resource management concerns than models that you deploy yourself. For models that are exposed as a service, infrastructure management isn't a concern. Instead, the focus is on service throughput, quota, and throttling. Azure OpenAI uses tokens for billing, throttling, and quotas. You should monitor quota usage for cost management and performance efficiency. Azure OpenAI also provides logging capabilities to track token usage. Tooling Many MLOps practitioners use a standardized toolkit to organize activities such as automation, tracking, deployment, and experimentation. This approach abstracts common concerns and implementation details, which makes these processes more efficient and manageable. A popular unified platform is MLflow. Before you look for new tools to support GenAIOps patterns, you should review your existing MLOps tooling to evaluate its support for generative AI. For example, MLflow supports a wide range of features for language models. You can also explore the benefits and trade-offs of introducing new tools into your flow. For example, the Azure AI Evaluation SDK for Python could be a feasible option because it has native support in the Azure AI Foundry portal. MLOps and GenAIOps maturity models You might have used the MLOps maturity model to evaluate the maturity of your current MLOps and environment. As you extend your MLOps investments for generative AI workloads, you should use the Generative AI Operations (GenAIOps) maturity model to evaluate those operations. You might want to combine the two maturity models, but we recommend that you measure each model independently because MLOps and GenAIOps evolve separately. For example, you might be at level four in the MLOps maturity model but only at level one in the GenAIOps maturity model. Use the GenAIOps Maturity Model assessment. This assessment helps you understand how your investments in GenAIOps are progressing. Summary As you start to extend your MLOps investments to include generative AI, it's important to understand that you don't need to start over. You can use your existing MLOps investments for several of the generative AI technical patterns. Fine-tuning generative models is a great example. Some processes in generative AI solutions, such as prompt engineering and RAG, are new. Because they're not part of traditional AI workflows, you need to extend your existing operations investments and gain new skills to effectively use them. Contributors Microsoft maintains this article. The following contributors wrote this article. Luiz Braz | Senior Technical Specialist Marco Aurelio Cardoso | Senior Software Engineer Paulo Lacerda | Cloud Solution Architect Ritesh Modi | Principal Software Engineer To see nonpublic LinkedIn profiles, sign in to LinkedIn. Next steps Machine Learning Azure OpenAI Related resources Design and develop a RAG solution Baseline AI Foundry chat reference architecture MLOps Feedback Was this page helpful? Yes No Additional resources", "metadata": {"source": "https://docs.microsoft.com/en-us/azure/architecture/ai-ml/guide/genaiops-for-mlops", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 3504, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9700000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Choose an Azure compute service\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Table of contents Exit focus mode Ask Learn Ask Learn Table of contents Read in English Add Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories. Access to this page requires authorization. You can try changing directories. Choose an Azure compute service Azure App Service Azure Kubernetes Service (AKS) Azure Virtual Machines Azure Container Apps Azure Functions Azure offers many ways to host your application code. The term compute refers to the hosting model for the resources that your application runs on. This article helps choose a compute service for your application. Choose a candidate service Use the following flowchart to select a candidate compute service. Download a Visio file of this decision tree. This diagram refers to two migration strategies: Lift and shift: A strategy for migrating a workload to the cloud without redesigning the application or making code changes. It's also called rehosting. For more information, see Azure migration and modernization center. Cloud optimized: A strategy for migrating to the cloud by refactoring an application to take advantage of cloud-native features and capabilities. The output from this flowchart is your starting point. Next, evaluate the service to see if it meets your needs. This article includes several tables that can help you choose a service. The initial candidate from the flowchart might be unsuitable for your application or workload. In that case, expand your analysis to include other compute services. If your application consists of multiple workloads, evaluate each workload separately. A complete solution can incorporate two or more compute services. Understand the basic features If you're not familiar with the Azure service selected in the previous section, see this overview documentation: Azure Virtual Machines: A service where you deploy and manage virtual machines (VMs) inside an Azure virtual network. Azure App Service: A managed service for hosting web apps, mobile app back ends, RESTful APIs, or automated business processes. Azure Functions: A service that provides managed functions that run based on a variety of trigger types for event-driven applications. Azure Kubernetes Service (AKS): A managed Kubernetes service for running containerized applications. Azure Container Apps: A managed service built on Kubernetes, which simplifies the deployment of containerized applications in a serverless environment. Azure Container Instances: This service is a fast and simple way to run a single container or group of containers in Azure. Azure Container Instances doesn't provide full container orchestration, but you can implement them without having to provision any VMs or adopt a higher-level service. Azure Red Hat OpenShift: A fully managed OpenShift cluster for running containers in production with Kubernetes. Azure Service Fabric: A distributed systems platform that can run in many environments, including Azure or on-premises. Azure Batch: A managed service for running large-scale parallel and high-performance computing (HPC) applications. Azure VMware Solution: A managed service for running VMware workloads natively on Azure. Understand the hosting models For hosting models, cloud services fall into three categories: Infrastructure as a service (IaaS): Lets you provision VMs along with the associated networking and storage components. Then you can deploy whatever software and applications you want onto those VMs. This model is the closest to a traditional on-premises environment. Microsoft manages the infrastructure. You still manage the VMs. Platform as a service (PaaS): Provides a managed hosting environment where you can deploy your application without needing to manage VMs or networking resources. Azure App Service and Azure Container Apps are PaaS services. Functions as a service (FaaS): Lets you deploy your code to the service, which automatically runs it. Azure Functions is a FaaS service. Note Azure Functions is an Azure serverless compute offering. To see how this service compares with other Azure serverless offerings, such as Logic Apps, which provides serverless workflows, see Choose the right integration and automation services in Azure. There's a spectrum from IaaS to pure PaaS. For example, Azure VMs can automatically scale by using virtual machine scale sets. This capability isn't strictly a PaaS, but it's the type of management feature found in PaaS. There's a tradeoff between control and ease of management. IaaS gives the most control, flexibility, and portability, but you have to provision, configure, and manage the VMs and network components you create. FaaS services automatically manage nearly all aspects of running an application. PaaS falls somewhere in between. Service Application composition Density Minimum number of nodes State management Web hosting Azure Virtual Machines Agnostic Agnostic 1 2 Stateless or stateful Agnostic Azure App Service Applications, containers Multiple apps per instance by using App Service plan 1 Stateless Built in Azure Functions Functions, containers Serverless 1 Serverless 1 Stateless or stateful 6 Not applicable Azure Kubernetes Service Containers Multiple containers per node 3 3 Stateless or stateful Agnostic Azure Container Apps Containers Serverless Serverless Stateless or stateful Agnostic Azure Container Instances Containers No dedicated instances No dedicated nodes Stateless Agnostic Azure Red Hat OpenShift Containers Multiple containers per node 6 5 Stateless or stateful Agnostic Azure Service Fabric Services, guest executables, containers Multiple services per VM 5 3 Stateless or stateful Agnostic Azure Batch Scheduled jobs Multiple apps per VM 1 4 Stateless No Azure VMware Solution Agnostic Agnostic 3 7 Stateless or stateful Agnostic Notes If you're using a Consumption plan. For an App Service plan, functions run on the VMs allocated for your App Service plan. See Choose the correct service plan for Azure Functions. Higher service-level agreement (SLA) with two or more instances. Recommended for production environments. Can scale down to zero after job completes. Three for primary nodes and three for worker nodes. When using Durable Functions. Require minimum number of three nodes. Networking Service Virtual network integration Hybrid connectivity Azure Virtual Machines Supported Supported Azure App Service Supported 1 Supported 2 Azure Functions Supported 1 Supported 3 Azure Kubernetes Service Supported Supported Azure Container Apps Supported Supported Azure Container Instances Supported Supported Azure Red Hat OpenShift Supported Supported Azure Service Fabric Supported Supported Azure Batch Supported Supported Azure VMware Solution Supported Supported Notes Requires App Service Environment or a dedicated compute pricing tier. Use Azure App Service Hybrid Connections. Requires App Service plan or Azure Functions Premium plan. DevOps Service Local debugging Programming model Application update Azure Virtual Machines Agnostic Agnostic No built-in support Azure App Service IIS Express, others 1 Web and API applications, WebJobs for background tasks Deployment slots Azure Functions Visual Studio or Azure Functions CLI Serverless, event-driven Deployment slots Azure Kubernetes Service Minikube, Docker, others Agnostic Rolling update Azure Container Apps Local container runtime Agnostic Revision management Azure Container Instances Local container runtime Agnostic Not applicable Azure Red Hat OpenShift Minikube, Docker, others Agnostic Rolling update Azure Service Fabric Local node cluster Guest executable, Service model, Actor model, Containers Rolling upgrade (per service) Azure Batch Not supported Command-line application Not applicable Azure VMware Solution Agnostic Agnostic No built-in support Notes Options include IIS Express for ASP.NET or node.js (iisnode), PHP web server, Azure Toolkit for IntelliJ, and Azure Toolkit for Eclipse. App Service also supports remote debugging of deployed web app. Scalability Service Autoscaling Load balancer Scale limit3 Azure Virtual Machines Virtual machine scale sets Azure Load Balancer Platform image: 1,000 nodes per scale set, Custom image: 600 nodes per scale set Azure App Service Built-in service Integrated 30 instances, 100 with App Service Environment Azure Functions Built-in service Integrated 200 instances per function app Azure Kubernetes Service Pod autoscaling1, cluster autoscaling2 Azure Load Balancer or Azure Application Gateway 5,000 nodes when using Uptime SLA Azure Container Apps Scaling rules4 Integrated 15 environments per region (default limit), unlimited container apps per environment and replicas per container app (depending on available cores) Azure Container Instances Not supported No built-in support 100 container groups per subscription (default limit) Azure Red Hat OpenShift Pod autoscaling, cluster autoscaling Azure Load Balancer or Azure Application Gateway 250 nodes per cluster (default limit) Azure Service Fabric Virtual machine scale sets Azure Load Balancer 100 nodes per virtual machine scale set Azure Batch Not applicable Azure Load Balancer Core limit of 900 dedicated and 100 low-priority (default limit) Azure VMware Solution Built-in service5 Integrated6 Per VMware vCenter can manage between 3 ~ 16 VMware ESXi hosts Notes See Autoscale pods. See Automatically scale a cluster to meet application demands on Azure Kubernetes Service. See Azure subscription and service limits, quotas, and constraints. See Set scaling rules in Azure Container Apps. See Scale a Azure VMware Solution. See VMware NSX. Availability Service Multiregion failover option Azure Virtual Machines Azure Traffic Manager, Azure Front Door, and cross-region Azure Load Balancer Azure App Service Azure Traffic Manager and Azure Front Door Azure Functions Azure Traffic Manager and Azure Front Door Azure Kubernetes Service (AKS) Azure Traffic Manager, Azure Front Door, and Multiregion Cluster Azure Container Apps Azure Traffic Manager and Azure Front Door Azure Container Instances Azure Traffic Manager and Azure Front Door Azure Red Hat OpenShift Azure Traffic Manager and Azure Front Door Azure Service Fabric Azure Traffic Manager, Azure Front Door, and cross-region Azure Load Balancer Azure Batch Not applicable Azure VMware Solution Not applicable For guided learning on service guarantees, see Core Cloud Services - Azure architecture and service guarantees. Security Review and understand the available security controls and visibility for each service: Azure Windows virtual machine Azure Linux virtual machine Azure App Service Azure Functions Azure Kubernetes Service Azure Container Instances Azure Container Apps Azure Service Fabric Azure Batch Azure VMware Solution Other criteria Service TLS Cost Suitable architecture styles Azure Virtual Machines Configured in VM Windows, Linux N-tier, big compute (HPC) Azure App Service Supported App Service pricing Web-queue-worker Azure Functions Supported Functions pricing Microservices, event-driven architecture Azure Kubernetes Service (AKS) Ingress controller AKS pricing Microservices, event-driven architecture Azure Container Apps Ingress controller Container Apps pricing Microservices, event-driven architecture Azure Container Instances Use sidecar container Container Instances pricing Microservices, task automation, batch jobs Azure Red Hat OpenShift Supported Azure Red Hat OpenShift pricing Microservices, event-driven architecture Azure Service Fabric Supported Service Fabric pricing Microservices, event-driven architecture Azure Batch Supported Batch pricing Big compute (HPC) Azure VMware Solution Configured in VM Azure VMware Solution pricing VM workload based on VMware format Consider limits and cost Along with the previous comparison tables, do a more detailed evaluation of the following aspects of the candidate service: Service limits Cost SLA Regional availability Contributors This article is maintained by Microsoft. It was originally written by the following contributors: Ayobami Ayodeji | Senior Program Manager Jelle Druyts | Principal Service Engineer Martin Gjoshevski | Senior Service Engineer Phil Huang | Senior Cloud Solution Architect Julie Ng | Senior Service Engineer Paolo Salvatori | Principal Service Engineer To see nonpublic LinkedIn profiles, sign in to LinkedIn. Next steps Core Cloud Services - Azure compute options. This Learn module explores how compute services can solve common business needs. Related resources Choose an Azure compute option for microservices Technology choices for Azure solutions Feedback Was this page helpful? Yes No", "metadata": {"source": "https://docs.microsoft.com/en-us/azure/architecture/guide/technology-choices/compute-decision-tree", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 1826, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8700000000000001, "syntax_valid": true, "completeness_score": 0.65, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Build workloads on spot virtual machines\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Table of contents Exit focus mode Ask Learn Ask Learn Table of contents Read in English Add Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories. Access to this page requires authorization. You can try changing directories. Build workloads on spot virtual machines Azure Virtual Machines This article describes best practices for how to build on Azure Spot Virtual Machines. It includes a deployable example scenario. Spot virtual machines (spot VMs) provide access to compute capacity at lower prices than regular VMs. This discount makes them a good option for organizations that want to optimize costs. But the savings come with a trade-off. Spot VMs can be evicted at any time, which means that they lose access to compute resources. Workloads that run on spot VMs must be able to handle these interruptions in compute. The right workload and a flexible orchestration mechanism are the keys to success. The following recommendations describe how to build on spot VMs. Understand spot VMs On a technical level, spot VMs are the same as regular VMs. They use the same images, hardware, and disks that translate to the same performance. The key difference between spot VMs and regular VMs is their priority and availability. Spot VMs have no priority to access compute capacity, and they have no availability guarantees after they access that compute capacity. No priority access. Regular VMs have priority access to compute capacity. They access compute capacity when they request it. However, spot VMs only deploy when there's spare compute capacity. And they only continue to run when a regular VM doesn't need the underlying hardware. No availability guarantee. Spot VMs don't have any availability guarantees or service-level agreements (SLAs). Spot VMs can lose access to compute capacity immediately, or anytime after deployment or eviction. Spot VMs are cheaper because they can be evicted. When Azure needs the compute capacity back, an eviction notice is sent and evicts the spot VM. Azure provides a minimum of 30-seconds advance notice before the actual eviction occurs. For more information, see Continuously monitor for eviction. Understand spot VM pricing Spot VMs can be up to 90% cheaper than regular pay-as-you-go VMs. The discount varies based on demand, VM size, deployment region, and the operating system. To get a cost savings estimate, see Azure Spot Virtual Machines pricing tool and Spot Virtual Machines pricing overview. You can also query the Azure retail prices API to programmatically obtain the spot pricing for any SKU. Understand interruptible workloads Spot VMs are ideal for interruptible workloads, which share several common characteristics. Interruptible workloads have minimal to no time constraints, low organizational priority, and short processing times. They run processes that can stop suddenly and resume later without harming essential organizational processes. Examples of interruptible workloads are batch processing applications, data analytics, and workloads that create a continuous integration and continuous deployment agent for a nonproduction environment. These features compare to regular or mission-critical workloads that have SLAs, sticky sessions, and stateful data. You can use spot VMs in non-interruptible workloads, but they shouldn't be the only source of compute capacity. Use as many regular VMs as you need to meet your uptime requirements. Understand eviction Spot VMs have no SLAs after creation, and they can lose access to compute at any time. We call this compute loss an eviction. Compute supply and demand drive evictions. When the demand for a specific VM size exceeds a specific level, Azure evicts spot VMs to make compute available to regular VMs. Demand is location specific. For example, an increase in demand in region A doesn't affect spot VMs in region B. Spot VMs have two configuration options that affect eviction. These configurations are the eviction type and eviction policy of the spot VM. You set these configurations when you create the spot VM. The eviction type defines the conditions of an eviction. The eviction policy determines what eviction does to your spot VM. Eviction type Capacity changes or price changes cause evictions. The way capacity and price changes affect spot VMs depends on the eviction type that you choose when the VM is created. The type of eviction defines the conditions of an eviction. The eviction types are capacity-only eviction and price or capacity eviction. Capacity-only eviction: This eviction type triggers an eviction when excess compute capacity is no longer available. By default, the price is capped at the pay-as-you-go rate. Use this eviction type when you don't want to pay more than the pay-as-you-go VM price. Price or capacity eviction: This eviction type has two triggers. Azure evicts a spot VM when excess compute capacity is no longer available or the cost of the VM exceeds the maximum price that you set. This eviction type allows you to set a maximum price far below the pay-as-you-go price. Use this eviction type to set your own price cap. Eviction policy The eviction policy that you choose for a spot VM affects its orchestration. Orchestration is the process of handling an eviction and is discussed later in this article. The eviction policies are the Stop/Deallocate policy and the Delete policy. Stop/Deallocate policy: The Stop/Deallocate policy is ideal when the workload can wait for release capacity within the same location and VM type. The Stop/Deallocate policy stops the VM and ends its lease with the underlying hardware. Stopping and deallocating a spot VM is the same as stopping and deallocating a regular VM. The VM remains accessible in Azure, and you can restart the same VM later. The VM loses compute capacity and nonstatic IP addresses with the Stop/Deallocate policy. However, the VM data disks remain and continue to incur charges. The VM also occupies cores in the subscription. VMs can't be moved from their region or zone even when they're stopped or deallocated. For more information, see Power states and billing. Delete policy: Use the Delete policy if the workload can change location or VM size. Changing the location or VM size allows the VM to redeploy faster. The Delete policy deletes the VM and any data disk. The VM doesn't occupy cores in subscriptions. For more information, see Eviction policy. Design for flexible orchestration Orchestration is the process of replacing a spot VM after an eviction. It's the foundation for building a reliably interruptible workload. A good orchestration system has built-in flexibility. Flexibility means designing your orchestration to have options, use multiple VM sizes, deploy to different regions, have eviction awareness, and account for different eviction scenarios to improve workload reliability and speed. Design for speed For a workload that runs on spot VMs, compute capacity is crucial. Because of the potential for eviction, ensure that you understand the allocated compute time so that you can make informed design decisions that prioritize workload speed. Generally, you should optimize the compute time that you have. Build a VM image that has all the required software preinstalled. Preinstalled software helps minimize the time between eviction and a fully operational application. Avoid using compute time on processes that don't contribute to the workload purpose. For example, a workload for data analytics should focus most of its compute time on data processing and as little time as possible on gathering eviction metadata. Eliminate nonessential processes from your application. Use multiple VM sizes and locations To increase flexibility, build an orchestration to use multiple types and sizes of VMs. The goal is to give your orchestration options to replace an evicted VM. Azure has different types and sizes of VMs that provide similar capabilities for about the same price. Filter for the minimum vCPUs or cores, the minimum RAM for VMs, and the maximum price. This process helps you find multiple VMs that fit in your budget and have enough power to run your workload. Each type of VM has an eviction rate that's expressed as a percentage range, such as 0%-5%, 5%-10%, 10%-15%, 15%-20%, or 20+%. Eviction rates can vary across regions. You might find a better eviction rate for the same type of VM in a different region. You can find the eviction rates for each type of VM in the portal under the Basics tab. Next to Size, select View pricing history or See all sizes. You can also programmatically get spot VM data by using Azure Resource Graph. In your orchestration system, consider using the spot placement score feature to evaluate the likelihood of success for individual spot deployments. For more information, see the following resources: Eviction rates Resource Graph Spot placement score Use the most flexible eviction policy The eviction policy of the evicted spot VM affects the replacement process. For example, a Delete policy is more flexible than a Stop/Deallocate policy. Consider the Delete policy first: Use a Delete policy if your workload can handle it. Deletion allows the orchestration to deploy replacement spot VMs to new zones and regions. This deployment flexibility could help your workload find spare compute capacity faster than a stopped or deallocated VM. Stopped or deallocated VMs have to wait for spare compute capacity in the same zone that they were created in. For the Delete policy, you need an external process to monitor for evictions and orchestrate deployments to various regions, use different VM SKUs, or both. Understand the Stop/Deallocate policy: The Stop/Deallocate policy has less flexibility than the Delete policy. The spot VMs must stay in the same region and zone. You can't move a stopped or deallocated VM to another location. Because the VMs have a fixed location, you need something in place to reallocate the VM when compute capacity becomes available. There's no way to predict the availability of compute capacity. So you should use an automated schedule pipeline to attempt a redeployment after an eviction. An eviction should trigger the schedule pipeline, and the redeployment attempts should continuously check for compute capacity until it becomes available. Policy When to use the policy Delete policy - Ephemeral compute and data - Don't want to pay for data disks - Minimal budget Stop/Deallocate policy - Need a specific VM size - Can't change location - Long application installation process - Indefinite wait time - Not driven by cost savings alone Continuously monitor for eviction Monitoring is the key to workload reliability on spot VMs. Spot VMs have no SLA after creation and can be evicted at any time. The best way to improve workload reliability on spot VMs is to anticipate when they're going to be evicted. If you have this information, you can attempt a workload graceful shutdown and trigger automation to orchestrate the replacement. Use Scheduled Events: Use the Scheduled Events service for each VM. Azure sends signals to VMs when infrastructure maintenance is going to affect them. Evictions qualify as infrastructure maintenance. Azure sends out the Preempt signal to all VMs at a minimum of 30 seconds before they're evicted. The Scheduled Events service allows you to capture this Preempt signal by querying an endpoint at the static, nonroutable IP address 169.254.169.254. Use frequent queries: Query the Scheduled Events endpoint often enough to orchestrate a graceful shutdown. You can query the Scheduled Events endpoint up to every second, but a one-second frequency might not be necessary for all use cases. These queries must come from an application that runs on the spot VM. The query can't come from an external source. As a result, the queries consume VM compute capacity and steal processing power from the main workload. You need to balance those competing priorities to meet your specific situation. Automate orchestration: After you collect the Preempt signal, your orchestration should act on that signal. Given the time constraints, the Preempt signal should attempt a graceful shutdown of your workload and start an automated process that replaces the spot VM. For more information, see the following resources: Scheduled Events Scheduled event types Endpoint querying frequency Build a deployment system Your orchestration needs an automated pipeline to deploy new spot VMs after eviction. The pipeline should run outside the interruptible workload to help ensure permanence. The deployment pipeline should work according to the eviction policy that you choose for your spot VMs. For a Delete policy, we recommend that you build a pipeline that uses different VM sizes and deploys to different regions. For a Stop/Deallocate policy, the deployment pipeline needs two distinct actions. For the initial creation of a VM, the pipeline needs to deploy the right size VMs to the right location. For an evicted VM, the pipeline needs to try to restart the VM until it works. A combination of Azure Monitor alerts and Azure functions is one way to automate a deployment system. The pipeline could use bicep templates. They're declarative and idempotent and represent a best practice for infrastructure deployment. Prepare for immediate eviction It's possible for Azure to evict a spot VM as soon as you create it and before your workload runs. In some cases, there might be enough capacity to create a spot VM, but it won't last. Spot VMs have no availability guarantees, or SLAs, after creation. Your orchestration needs to account for immediate evictions. The Preempt signal provides a minimum of 30-seconds advance notice of the eviction. Incorporate VM health checks into your orchestration to prepare for immediate evictions. Orchestration for immediate evictions can't depend on the Scheduled Events Preempt signal. Only the VM itself can query the Preempt signal, and there's not enough time to start an application, query the Scheduled Events endpoint, and gracefully shut down. So the health check needs to reside outside the workload environment. The health checks need to monitor the status of the spot VM and start the deployment pipeline to replace the spot VM when the status changes to deallocating or stopping. Plan for multiple simultaneous evictions If you run a cluster of spot VMs, architect the workload to withstand multiple simultaneous evictions. Multiple spot VMs in the workload can be evicted at the same time. A simultaneous eviction of multiple VMs could affect the throughput of the application. To prevent this situation, your deployment pipeline should be able to gather signals from multiple VMs and deploy multiple replacement VMs simultaneously. Design for a graceful shutdown The VM shutdown process should be less than 30 seconds and allow your VM to shut down before an eviction. The amount of time the shutdown should take depends on how frequently your workload queries the Scheduled Events endpoint. The more often you query the endpoint, the longer the shutdown process can take. The shutdown process should release resources, drain connections, and flush event logs. You should regularly create and save checkpoints to retain the context and build a more efficient recovery strategy. The checkpoint is just information about what processes or transactions the next VM needs to start on. They should indicate if the VM should resume where the previous VM left off or if the new VM should revert the changes and start the entire process again. Store the checkpoints outside the spot VM environment, like in a storage account. Test the orchestration Simulate eviction events to test orchestration in dev/test environments. For more information, see Simulate eviction. Design an idempotent workload We recommend that you design an idempotent workload. The outcome of processing an event more than one time should be the same as processing it once. Evictions can result in forced shutdowns, despite efforts to ensure graceful shutdowns. Forced shutdowns can terminate processes before completion. Idempotent workloads can receive the same message more than one time without changing the outcome. For more information, see Idempotency. Use an application warmup period Most interruptible workloads run applications. Applications need time to install and to start up. They also need time to connect to external storage and gather information from checkpoints. Have an application warmup period before you allow it to start processing. During the warmup period, the application should start, establish connections, and prepare to contribute. Only allow an application to start processing data after you validate the health of the application. Configure user-assigned managed identities Assign user-assigned managed identities to streamline the authentication and authorization process. User-assigned managed identities let you avoid putting credentials in code and aren't tied to a single resource like system-assigned managed identities. The user-assigned managed identities contain permissions and access tokens from Microsoft Entra ID that can be reused and assigned to spot VMs during orchestration. Token consistency across spot VMs helps streamline orchestration and simplifies the access to workload resources that the spot VMs have. If you use system-assigned managed identities, a new spot VM might get a different access token from Microsoft Entra ID. If you need to use system-assigned managed identities, make the workloads resilient to 403 Forbidden Error responses. Your orchestration needs to get tokens from Microsoft Entra ID with the right permissions. For more information, see Managed identities. Example scenario The example scenario deploys a queue processing application that qualifies as an interruptible workload. The scripts in the scenario serve as examples. The scenario guides you through a one-time, manual push to deploy resources. This implementation doesn't have a deployment pipeline. However, a deployment pipeline is essential to automate the orchestrating process. The following diagram shows the architecture of the example scenario. Download a Visio file of this architecture. The following workflow corresponds to the previous diagram: VM application definition: The VM application definition is created in the Azure compute gallery. It defines the application name, location, operating system, and metadata. The application version is a numbered version of the VM application definition. The application version represents the VM application. It needs to be in the same region as the spot VM. The application version links to the source application package in the storage account. Storage account: The storage account stores the source application package. In this architecture, it's a compressed tar file named worker-0.1.0.tar.gz. It contains two files. One file is the orchestrate.sh bash script that installs the .NET worker application. Spot VM: The spot VM deploys. It must be in the same region as the application version. It downloads worker-0.1.0.tar.gz to the VM after deployment. The bicep template deploys an Ubuntu image on a standard family VM. These configurations meet the needs of this application and aren't general recommendations for your applications. Storage queue: The other service that runs in the .NET worker contains message queue logic. Microsoft Entra ID grants the spot VM access to the storage queue in Azure Queue Storage with a user-assigned identity by using role-based access control. .NET worker application: The orchestrate.sh script installs a .NET worker application that runs two background services. The first service queries the Scheduled Events endpoint, looks for the Preempt signal, and sends this signal to the second service. The second service processes messages from the storage queue and listens for the Preempt signal from the first service. When the second service receives the signal, it interrupts storage queue processing and begins to shut down. Query Scheduled Events endpoint: An API request is sent to a static nonroutable IP address 169.254.169.254. The API request queries the Scheduled Events endpoint for infrastructure maintenance signals. Application Insights: The architecture uses Application Insights only for learning purposes. It's not an essential component of interruptible workload orchestration, but allows you to validate the telemetry from the .NET worker application. The .NET worker application sends telemetry to Application Insights. For more information, see Enable live metrics from the .NET application. Deploy this scenario There's a GitHub repository called interruptible workload on spot that has templates, scripts, and step-by-step instructions to deploy this architecture. Next step Spot Virtual Machines Related resources Architectural approaches for cost management and allocation in a multitenant solution Run a Windows VM on Azure Run a Linux VM on Azure Feedback Was this page helpful? Yes No", "metadata": {"source": "https://docs.microsoft.com/en-us/azure/architecture/guide/spot/spot-eviction", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 3301, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9325000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Azure hybrid options\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Table of contents Exit focus mode Ask Learn Ask Learn Table of contents Read in English Add Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories. Access to this page requires authorization. You can try changing directories. Azure hybrid options Azure Arc Azure IoT Edge Azure Stack Edge Azure Local Azure Stack Hub Azure offers several hybrid solutions that can host applications and workloads, extend Azure services, and provide security and operational tooling for hybrid environments. Azure hybrid services range from virtualized hardware that hosts traditional IT apps and databases to integrated platform as a service (PaaS) solutions for on-premises, edge, and multicloud scenarios. This guide helps you choose a hybrid solution that meets your business requirements. Hybrid concepts Hybrid environments include the following types of hosting locations and infrastructure: Hybrid cloud: These environments combine public cloud services with on-premises infrastructure. This hybrid strategy is common for organizations that have strict data sovereignty regulations, low latency requirements, or crucial resiliency and business continuity needs. Edge: These environments host devices that provide on-premises computing and data storage. This approach is common for organizations and applications that need to remain close to the data, reduce latency, or compute data in near real time. Multicloud: These environments use multiple cloud computing services and providers. This strategy provides flexibility, can reduce risk, and lets organizations investigate and use different providers for specific applications. But this approach often requires cloud-specific knowledge and adds complexity to management, operations, and security. Hybrid solutions encompass a system's control plane and data plane. Control plane: This plane refers to resource management operations, such as creating Azure virtual machines (VMs). Azure uses Azure Resource Manager to handle the control plane. Data plane: This plane uses the capabilities of resource instances that the control plane creates, such as accessing Azure VMs over remote desktop protocol (RDP). Azure hybrid solutions can extend Azure control plane operations outside of Azure datacenters, or run dedicated control plane instances, to provide data plane capabilities. Hybrid considerations To make a hybrid solution decision, you must consider hardware, hosting and deployment, and application or workload requirements and constraints. Hybrid solutions must also support developer operations (DevOps) and comply with organizational and industry standards and regulations. Hardware Depending on workload type, you might need traditional datacenter hardware that can run VMs, containers, and databases. For other scenarios, like IoT deployments, restricted hardware devices are a better fit and can run on rack, portable, or ruggedized servers. Consider whether to refresh, repurpose, or replace existing hardware. Brownfield scenarios use existing hardware in modern hybrid workload approaches. Greenfield scenarios acquire new hardware or use hardware as a service with a monthly fee. Hosting and deployment Consider whether to use on-premises datacenter, edge, Azure cloud, or multicloud hosting with a consistent cloud-native technology approach. Business, compliance, cost, or security requirements might determine the hosting location. A large-scale application deployment is different from smaller-scale implementations. A traditional IT deployment to VMs and databases is different from deployments to containers or distributed devices. Distributed, complex, large-scale deployments must be able to massively scale service implementation, and might address concerns like business continuity differently than traditional IT. Application or workload Consider whether applications or workloads are distributed, containerized, or traditional IT hosted on VMs or databases. Azure IoT Hub, Azure Kubernetes Service (AKS) clusters, or PaaS solutions outside Azure datacenters can host hybrid workloads. Traditional applications that run on VMs benefit from hyperconverged infrastructure (HCI) and Azure operational, security, and management tooling for day-two operations. Cloud-native applications are better suited to run on container orchestrators like AKS and use Azure PaaS solutions. If you need to deploy models built and trained in the cloud and run them on-premises, monitor IoT devices at scale, or provide Azure data transfer options, consider edge deployments and solutions. Choose a hybrid solution All the preceding factors are important for the final solution, but depending on requirements, background, and expertise, organizations might approach solution evaluation from different perspectives. Organizations might start with their hardware and hosting requirements and constraints, or by investigating Azure services from an application and workload perspective. DevOps teams might focus on mass deployments and restricted or purpose-built hardware, while systems administrators might emphasize hosting location or hardware and hypervisor usage. The following sections present a hybrid solution decision tree based on deployment model and an Azure hybrid service matrix describing supported workloads, hardware types, and deployment models. Work through these illustrations to choose a candidate solution. Then, carry out a detailed evaluation of the candidate services to see if they meet your needs. Hybrid solution decision tree The following decision tree starts with choosing an existing or custom, multicloud, or Azure-specified hybrid solution. The tree proceeds through decision points to select an appropriate Azure hybrid service. Download a PowerPoint file of all diagrams in this article. For existing or custom deployments: Decide whether the hardware is restricted or deployed in a datacenter. For restricted hardware, decide whether the deployment is mass or low scale. For datacenter and multicloud deployments, determine whether the workload type uses containers or traditional IT deployment in VMs or SQL databases. Existing and custom IoT workloads can use Azure IoT Edge. Existing and custom traditional, database, and cloud-native deployments can use Azure Arc-enabled servers and services. Container-based deployments can use Azure Arc-enabled Kubernetes. VM-based deployments can use Azure Arc-enabled servers, Azure Arc-enabled VMware vSphere, or Azure Arc-enabled System Center Virtual Machine Manager. SQL database deployments can use Azure Arc-enabled data services. For Azure-specified deployments: Decide whether you want hardware as a service or Azure datacenter-like deployments. Azure datacenter-like deployments can use Azure Stack Hub. For hardware as a service, decide whether your workload type uses data transfer and compute or a hyperconverged infrastructure (HCI). For a hyperconverged solution, you can use Azure Local. Data transfer and compute workloads can use Azure Stack Edge. Datacenter deployments can use Azure Stack Edge Pro 2 and Azure Stack Edge Pro GPU. Portable deployments can use Azure Stack Edge Mini R. Ruggedized deployments can use Azure Stack Edge Pro R. Azure hybrid services matrix The following decision matrix presents supported workloads, hardware capabilities, and deployment models for several Azure hybrid services. All Azure services include the Azure portal and other Azure operations and management tools. Download a PowerPoint file of all diagrams in this article. The Azure cloud provides cloud-based software as a service (SaaS), infrastructure as a service (IaaS), and PaaS compute, storage, and network services. The services run on Microsoft hardware in Azure datacenters. Azure VMware Solution (AVS) enables running VMware workloads natively on Azure by providing private clouds that contain VMware vSphere clusters built from dedicated bare-metal Azure infrastructure. This solution provides a way to extend or migrate existing on-premises VMware environments to Azure without the need to rearchitect applications or retool operations. AVS integrates with Azure services, enabling you to use Azure's scalability, security, and global reach while using familiar VMware tools and processes. Azure Stack is a family of products and solutions that extend Azure to the edge or to on-premises datacenters. Azure Stack provides several solutions for various use cases. Azure Stack Hub extends Azure to run apps in on-premises environments. Azure Stack Hub provides SaaS, IaaS, and PaaS hyperconverged compute, storage, and network services and runs on industry-standard hardware on-premises or in multicloud datacenters. Azure Stack Hub delivers Azure services to datacenters with integrated systems and can run on connected or disconnected environments. Azure Local is a hyperconverged solution that uses validated hardware to run virtualized and containerized workloads on-premises. Azure Local provides VM-based and AKS-based hyperconverged compute, storage, and network services and runs on industry-standard hardware on-premises or in multicloud datacenters. Azure Local connects workloads to Azure for cloud services and management. Azure Stack Edge delivers Azure capabilities such as compute, storage, networking, and hardware-accelerated machine learning to edge locations. Azure Stack Edge provides VM-based, AKS-based, machine learning, and data transfer services on industry-standard hardware as a service and runs on-premises or in multicloud datacenters. Azure IoT Edge and IoT Hub deploy custom functionality to mass devices. IoT Edge natively integrates with IoT Hub to provide DevOps, PaaS, and containerized services on custom and industry-standard hardware and runs on-premises or in multicloud datacenters. Azure Arc provides application delivery and management by using Azure Arc-enabled services for VMs, SQL databases, and Kubernetes. Azure Arc projects existing bare metal, VM, and Kubernetes infrastructure resources into Azure to handle operations with Azure management and security tools. Azure Arc simplifies governance and management by delivering a consistent multicloud and on-premises management platform for Azure services. Azure Arc runs on existing industry-standard hardware, hypervisors, Azure Local, or Azure Stack Edge, on-premises or in multicloud datacenters. Azure Arc includes the following capabilities: Azure Arc-enabled servers SQL Server on Azure Arc-enabled servers Azure Arc-enabled Kubernetes Azure Arc-enabled VMware vSphere Azure Arc-enabled System Center Virtual Machine Manager Azure Arc-enabled VMs on Azure Local Azure Arc-enabled services let you create on-premises and multicloud applications with Azure PaaS and data services such as Azure App Service, Azure Functions, Azure Logic Apps, Azure SQL Managed Instance, PostgreSQL Hyperscale, and Azure Machine Learning. You can run these services anywhere and use existing infrastructure. Contributors This article is maintained by Microsoft. It was originally written by the following contributors: Robert Eichenseer | Senior Service Engineer Laura Nicolas | Senior Software Engineer To see nonpublic LinkedIn profiles, sign in to LinkedIn. Next steps Azure hybrid and multicloud patterns and solutions documentation Introduction to hybrid and multicloud Introduction to Azure hybrid cloud services (Learn module) Related resources Hybrid architecture design Implement a secure hybrid network Feedback Was this page helpful? Yes No", "metadata": {"source": "https://docs.microsoft.com/en-us/azure/architecture/guide/technology-choices/hybrid-considerations", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 1617, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9225000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Architectural considerations for identity in a multitenant solution\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Table of contents Exit focus mode Ask Learn Ask Learn Table of contents Read in English Add Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories. Access to this page requires authorization. You can try changing directories. Architectural considerations for identity in a multitenant solution 2025-07-04 Feedback Identity is an important aspect of any multitenant solution. The identity components of your application are responsible for the following tasks: Verifying a user's identity, known as authentication Enforcing a user's permissions within the scope of a tenant, known as authorization Your customers might also wish to authorize external applications to access their data or integrate with your solution. A user's identity determines what information a user or service can access. It's important that you consider your identity requirements to isolate your application and data between tenants. Caution Authentication and authorization services within multitenant and software as a service (SaaS) applications are typically provided by an external identity provider (IdP). An IdP is usually an integral part of a managed identity platform. Building your own IdP is complex, expensive, and challenging to secure. It's considered an antipattern, and we don't recommend it. Before you define a multitenant identity strategy, first consider the following high-level identity requirements for your service: Determine whether users or workload identities access a single application or multiple applications within a product family. Some product families might include distinct applications that share the same identity infrastructure, such as point-of-sale systems and inventory management platforms. Consider whether your solution implements modern authentication and authorization standards, such as OAuth2 and OpenID Connect. Evaluate whether authentication is limited to UI-based applications or if API access also applies to tenants and third-party systems. Determine whether tenants need to federate with their own IdPs and if multiple IdPs must be supported for each tenant. For example, you might have tenants with Microsoft Entra ID, Auth0, and Active Directory Federation Services where each tenant federates with your solution. Identify which federation protocols their IdPs use because those protocols determine what your IdP must support. Review any applicable compliance requirements that they need to meet, such as the General Data Protection Regulation (GDPR), that shape your identity strategy. Determine whether tenants require identity data to be stored in specific geographic regions to meet legal, compliance, or operational needs. Assess whether users access data from multiple tenants within the application. If they do, you might need to support seamless tenant switching or provide consolidated views across tenants for specific users. For example, users who sign into the Azure portal can easily switch between different Microsoft Entra ID directories and subscriptions that they have access to. When you establish your high-level requirements, you can start to plan more specific details and requirements, such as user directory sources and sign-up and sign-in flows. Identity directory For a multitenant solution to authenticate and authorize a user or service, it needs a place to store identity information. A directory can include authoritative records for each identity. Or it might contain references to external identities that are stored in the directory of another IdP. When you design an identity system for your multitenant solution, you need to consider which of the following types of IdP that your tenants and customers might need: Local IdP: A local IdP allows users to sign themselves up for the service. Users provide a username, an email address, or an identifier, such as a rewards card number. They also provide a credential, like a password. The IdP stores both the user identifier and the credentials. Social IdP: A social IdP allows users to use an identity that they have on a social network or other public IdP, such as Facebook, Google, or a personal Microsoft account. Social IdPs are commonly used in business-to-consumer (B2C) SaaS solutions. The tenant's Microsoft Entra ID directory: In many business-to-business (B2B) SaaS solutions, tenants might already have their own Microsoft Entra ID directory, and they might want your solution to use their directory as the IdP for accessing your solution. This approach is possible when your solution is built as a multitenant Microsoft Entra application. Federation with a tenant's IdP: In some B2B SaaS solutions, tenants might have their own IdP, other than Microsoft Entra ID, and they might want your solution to federate with it. Federation enables single sign-on (SSO). It also enables tenants to manage the life cycle and security policies of their users independently of your solution. You should consider if your tenants need to support multiple IdPs. For example, a single tenant might need to support local identities, social identities, and federated identities. This requirement is typical when companies use a solution intended for both their employees and contractors. They might use federation to grant employees access, while also allowing access for contractors or users who don't have accounts in the federated IdP. Store authentication and tenant authorization information In a multitenant solution, you need to consider where to store several types of identity information, including the following types: Details about user and service accounts, including their names and other information that your tenants require. Information that's required to securely authenticate your users, including information for multifactor authentication (MFA). Tenant-specific information, such as tenant roles and permissions, for authorization. Caution We don't recommend building authentication processes yourself. Modern IdPs provide these authentication services to your application, and they also include other important features, such as MFA and conditional access. Building your own identity provider is an antipattern. We don't recommend it. Consider the following options for storing identity information: Store all identity and authorization information in the IdP directory, and share it between multiple tenants. Store user credentials in the IdP directory. Store authorization data in the application tier, alongside tenant information. Determine the number of identities for a user Multitenant solutions often allow a user or workload identity to access application resources and data across multiple tenants. When this approach is required, consider the following factors: Decide whether to create a single user identity for each person or to create separate identities for each tenant-user combination. Use a single identity for each person when possible. It simplifies account management for both the solution provider and users. Also, many of the intelligent threat protections that modern IdPs provide rely on each person having a single user account. Use multiple distinct identities in some scenarios. For example, if people use your system both for work and personal purposes, they might want to separate their user accounts. Or if your tenants have strict regulatory or geographical data storage requirements, they might require a person to have multiple identities so that they can comply with regulations or laws. Avoid storing credentials multiple times if you use per-tenant identities. Users should have their credentials stored against a single identity, and you should use features like guest identities to refer to the same user credentials from multiple tenants' identity records. Grant users access to tenant data Consider how you plan to map users to a tenant. For example, during the sign-up process, you might provide a unique invitation code for users to enter when they access a tenant for the first time. In some solutions, the domain name of the user's sign-up email address can identify their associated tenant. Alternatively, you might use another attribute from the user's identity record to determine the tenant mapping. This association should then be stored based on immutable, unique identifiers for both the tenant and the user. If your solution limits each user to accessing data for a single tenant, consider the following decisions: Determine how the IdP detects which tenant a user accesses. Explain how the IdP communicates the tenant identifier to the application. Typically, a tenant identifier claim is added to the token. If a single user needs to be granted access to multiple tenants, consider the following decisions: The solution must support logic for identifying users and granting appropriate permissions across tenants. For example, a user might have administrative rights in one tenant but limited access in another tenant. For example, suppose a user signed up with a social identity and was then granted access to two tenants. Tenant A enriched the user's identity with more information. Should tenant B have access to the enriched information? A clear mechanism should allow users to switch between tenants. This approach ensures a smooth user experience and prevents accidental cross-tenant access. Workload identities, if you use them, must specify which tenant they're trying to access. This requirement might include embedding tenant-specific context in authentication requests. Consider whether tenant-specific information stored in a user's identity record could unintentionally leak between tenants. For example, if a user signs up with a social identity and gains access to two tenants, and Tenant A enriches the user profile, determine whether Tenant B should have access to that enriched information. User sign-up process for local identities or social identities Some tenants might need to allow users to sign themselves up for an identity in your solution. Self-service sign-up might be required if you don't require federation with a tenant's IdP. If a self-sign up process is needed, then you should consider the following factors: Define which identity sources users are allowed to sign up from. For example, determine whether users can create a local identity and also use a social IdP. Specify whether your solution allows only specific email domains if local identities are used. For example, determine whether a tenant can restrict sign-ups to users with an @contoso.com email address. The user principal name (UPN) used for identifying local identities must be clearly established. Common UPNs include email addresses, usernames, phone numbers, or membership identifiers. Because UPNs can change, it's advisable to reference the underlying immutable unique identifier for authorization and auditing. An example is the object ID (OID) in Microsoft Entra ID. Verification of UPNs might be required to ensure their accuracy. This process could include validating ownership of an email address or phone number before granting access. Some solutions might require tenant administrators to approve user sign-ups. This approval process allows for administrative control over who joins a tenant. Decide whether tenants require a tenant-specific sign-up experience or URL. For example, determine if your tenants require a branded sign-up experience when users sign up, or the ability to intercept a sign-up request and perform extra validation before it proceeds. Tenant access for self sign-up users If users can sign themselves up for an identity, define a process to grant them access to a tenant. The sign-up flow might include a manual access grant process or an automated process based on the information about the users, such as their email addresses. It's important to plan for this process and consider the following factors: Define how the sign-up flow determines that a user is granted access to a specific tenant. Define whether your solution automatically revokes user access to a tenant when appropriate. For example, when users leave an organization, there should be a manual or automated process in place to remove their access. Provide a user audit capability so that tenants can review which users have access to their environment and understand their assigned permissions. Automated account life cycle management A common requirement for corporate or enterprise customers of a solution is a set of features that allows them to automate account onboarding and offboarding. Open protocols, such as System for Cross-Domain Identity Management (SCIM), provide an industry-standard approach to automation. This automated process usually includes the creation and removal of identity records and the management of tenant permissions. Consider the following factors when you implement automated account life cycle management in a multitenant solution: Determine whether your customers need to configure and manage an automated life cycle process for each tenant. For example, when a user is onboarded, you might need to create the identity within multiple tenants in your application, where each tenant has a different set of permissions. Determine whether you need to implement SCIM or offer federation. Federation allows tenants to retain control over user management by keeping the source of truth within their own systems instead of managing local users in your solution. User authentication process When a user signs into a multitenant application, your identity system authenticates the user. Consider the following factors when you plan your authentication process: Some tenants might require the ability to configure their own MFA policies. For example, if your tenants are in the financial services industry, they need to implement strict MFA policies, while a small online retailer might not have the same requirements. The option to define custom conditional access rules might be important for tenants. For example, different tenants might need to block sign-in attempts from specific geographic regions. Determine whether tenants need to customize the sign-in process individually. For example, your solution might need to show a customer's logo. Or it might need to retrieve user information, such as a rewards number, from another system and return it to the IdP to enrich the user profile. Some users might need to impersonate other users. For example, a support team member might wish to investigate a problem that another user has by impersonating their user account without having to authenticate as the user. API access might be required for some users or external applications. These scenarios might include calling the solution's APIs directly, which bypasses standard user authentication flows. Workload identities In most solutions, an identity often represents a user. Some multitenant systems also allow workload identities to be used by services and applications to gain access to your application resources. For example, your tenants might need to access an API that your solution provides so that they can automate their management tasks. Microsoft Entra supports workload identities, and other IdPs also commonly support them too. Workload identities are similar to user identities, but usually they require different authentication methods, such as keys or certificates. Workload identities don't use MFA. Instead, workload identities usually require other security controls, such as regular key-rolling and certificate expiration. If your tenants can enable workload identity access to your multitenant solution, then you should consider the following factors: Determine how workload identities are created and managed in each tenant. Decide how workload identity requests are scoped to the tenant. Evaluate if you need to limit the number of workload identities that each tenant creates. Determine if conditional access controls are required for workload identities in each tenant. For example, a tenant might want to limit a workload identity from being authenticated from outside a specific region. Identify which security controls you provide to tenants to ensure that workload identities remain secure. For example, automated key rolling, key expiration, certificate expiration, and sign-in risk monitoring help reduce the risk of workload identity misuse. Federate with an IdP for SSO Tenants who already have their own user directories might want your solution to federate to their directories. Federation allows your solution to use the identities in their directory instead of managing another directory with distinct identities. Federation is especially important when some tenants want to specify their own identity policies or to enable SSO experiences. If you expect tenants to federate with your solution, consider the following factors: Consider the process for configuring the federation for a tenant. Determine if tenants configure federation themselves or if the process requires manual configuration and maintenance by your team. Define which federation protocols your solution supports. Establish processes that prevent federation misconfigurations from granting access to unintended tenants. Plan for whether a single tenant's IdP needs to be federated to more than one tenant in your solution. For example, if customers have both a training and a production tenant, they might need to federate the same IdP with each tenant. Authorization models Decide on the authorization model that makes the most sense for your solution. Consider the following common authorization approaches: Role-based authorization: Users are assigned to roles. Some features of the application are restricted to specific roles. For example, a user in the administrator role can perform any action, while a user in a lower role might have a subset of permissions throughout the system. Resource-based authorization: Your solution provides a set of distinct resources, each of which has its own set of permissions. A specific user might be an administrator of one resource and have no access to another resource. These models are distinct, and the approach that you select affects your implementation and the complexity of the authorization policies that you can implement. Entitlements and licensing In some solutions, you might use per-user licensing as part of your commercial pricing model. In this scenario, you provide different tiers of user licenses that have different capabilities. For example, users with one license might be permitted to use a subset of the features of the application. The capabilities that specific users are allowed to access, based on their licenses, is sometimes called an entitlement. The application code or a dedicated entitlements system typically tracks and enforces entitlements instead of the identity system. This process is similar to authorization but occurs outside the identity management layer. Identity scale and authentication volume As multitenant solutions grow, the number of users and sign-in requests that the solution needs to process increases. Consider the following factors: Assess whether the user directory scales to support the required number of users. Evaluate whether the authentication process handles the expected number of sign-ins and sign-ups. Determine whether there are spikes that the authentication system can't handle. For example, at 9 AM Pacific Time, everyone in the western United States might start work and sign in to your solution, which creates a spike in sign-in requests. These scenarios are sometimes called login storms. Determine whether high load in parts of your solution affects the performance of the authentication process. For example, if authentication requires calling into an application-tier API, a surge in authentication requests could affect overall system performance. Define how your solution behaves if the IdP becomes unavailable. Consider whether a backup authentication service should be included to maintain business continuity. Contributors Microsoft maintains this article. The following contributors wrote this article. Principal authors: John Downs | Principal Software Engineer, Azure Patterns & Practices Daniel Scott-Raynsford | Partner Technology Strategist Arsen Vladimirskiy | Principal Customer Engineer, FastTrack for Azure Other contributors: Jelle Druyts | Principal Customer Engineer, FastTrack for Azure Landon Pierce | Senior Customer Engineer Sander van den Hoven | Senior Partner Technology Strategist Nick Ward | Senior Cloud Solution Architect To see nonpublic LinkedIn profiles, sign in to LinkedIn. Related resource Architectural approaches for identity in multitenant solutions Feedback Was this page helpful? Yes No Additional resources", "metadata": {"source": "https://docs.microsoft.com/en-us/azure/architecture/guide/multitenant/considerations/identity", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 3114, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9600000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Security architecture design\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Table of contents Exit focus mode Ask Learn Ask Learn Table of contents Read in English Add Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories. Access to this page requires authorization. You can try changing directories. Security architecture design Microsoft Entra ID Azure Firewall Azure Front Door Azure Key Vault Azure Private Link Information security has always been a complex subject, and it evolves quickly with the creative ideas and implementations of attackers and security researchers. Security is one of the most important aspects of any architecture. Good security provides confidentiality, integrity, and availability assurances against deliberate attacks and abuse of your valuable data and systems. Losing these assurances can harm your business operations and revenue, and your organization's reputation. Note Learn how cloud security is an ongoing journey of incremental progress and maturity, in Security in the Microsoft Cloud Adoption Framework for Azure. Learn how to build security into your solution, in the Azure Well-Architected Framework Overview of the security pillar. Here are some broad categories to consider when you design a security system: Azure provides a wide range of security tools and capabilities. These are just some of the key security services available in Azure: Microsoft Defender for Cloud. A unified infrastructure security management system that strengthens the security posture of your datacenters. It also provides advanced threat protection across your hybrid workloads in the cloud and on-premises. Microsoft Entra ID. The Microsoft cloud-based identity and access management service. Azure Front Door. A global, scalable entry-point that uses the Microsoft global edge network to create fast, highly secure, and widely scalable web applications. Azure Firewall. A cloud-native, intelligent network firewall security service that provides threat protection for your cloud workloads that run in Azure. Azure Key Vault. A high-security secret store for tokens, passwords, certificates, API keys, and other secrets. You can also use Key Vault to create and control the encryption keys used to encrypt your data. Azure Private Link. A service that enables you to access Azure PaaS services, Azure-hosted services that you own, or partner services over a private endpoint in your virtual network. Azure Application Gateway. An advanced web traffic load balancer that enables you to manage traffic to your web applications. Azure Policy. A service that helps you enforce organizational standards and assess compliance. For a more comprehensive description of Azure security tools and capabilities, see End-to-end security in Azure. Introduction to security on Azure If you're new to security on Azure, the best way to learn more is with Microsoft Learn training. This free online platform provides interactive training for Microsoft products and more. Here are two learning paths to get you started: Microsoft Azure Fundamentals: Describe general security and network security features Microsoft Security, Compliance, and Identity Fundamentals: Describe the capabilities of Microsoft security solutions Path to production To secure Azure application workloads, you use protective measures like authentication and encryption in the applications themselves. You can also add security layers to the virtual machine (VM) networks that host the applications. See Firewall and Application Gateway for virtual networks for an overview. Zero Trust is a proactive, integrated approach to security across all layers of the digital estate. It explicitly and continuously verifies every transaction, asserts least privilege, and relies on intelligence, advanced detection, and real-time response to threats. For an implementation strategy for web apps, see Zero Trust network for web applications with Azure Firewall and Application Gateway. Azure governance establishes the tooling needed to support cloud governance, compliance auditing, and automated guardrails. See Azure governance design area guidance for information about governing your Azure environment. Best practices The Azure Well-Architected Framework is a set of guiding tenets, based on five pillars, that you can use to improve the quality of your architectures. For information, see Overview of the security pillar and Security design principles in Azure. The Well-Architected Framework also provides these checklists: Azure identity and access management considerations Network security Data protection considerations Governance, risk, and compliance Security architectures Identity and access management Microsoft Entra identity management and access management for AWS Threat protection Multilayered protection for Azure virtual machine access Stay current with security Get the latest updates on Azure security services and features. Additional resources Example solutions Improved-security access to multitenant web apps from an on-premises network Restrict interservice communications Securely managed web applications Web app private connectivity to Azure SQL database Browse all our security architectures. AWS or Google Cloud professionals Identity with Azure and AWS AWS to Azure services comparison - Security Google Cloud to Azure services comparison - Security Next steps Security architecture is part of a comprehensive set of security guidance that also includes: Security in the Microsoft Cloud Adoption Framework for Azure: A high-level overview of a cloud security end state. Azure Well-Architected Framework: Guidance on securing your workloads on Azure. Azure security benchmarks: Prescriptive best practices and controls for Azure security. End-to-end security in Azure: Documentation that introduces you to the security services in Azure. Top 10 security best practices for Azure: Top Azure security best practices that Microsoft recommends based on lessons learned across customers and our own environments. Microsoft Cybersecurity Architectures: The diagrams describe how Microsoft security capabilities integrate with Microsoft platforms and 3rd-party platforms. Feedback Was this page helpful? Yes No", "metadata": {"source": "https://docs.microsoft.com/en-us/azure/architecture/guide/security/security-start-here", "language": "markdown", "complexity": "medium", "type": "official_site", "word_count": 892, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9275000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Microsoft security solutions for AWS\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Table of contents Exit focus mode Ask Learn Ask Learn Table of contents Read in English Add Add to plan Edit Share via Facebook x.com LinkedIn Email Print Note Access to this page requires authorization. You can try signing in or changing directories. Access to this page requires authorization. You can try changing directories. Microsoft security solutions for AWS Azure Microsoft Sentinel Microsoft Defender for Cloud Apps Microsoft Defender for Cloud This guide describes how Microsoft security solutions can help secure and protect Amazon Web Services (AWS) account access and environments. This diagram summarizes how AWS installations can benefit from key Microsoft security components: Download a PowerPoint file of this diagram. Microsoft Entra Centralized identity and access management Microsoft Entra ID is a comprehensive, cloud-based centralized identity and access management solution that can help secure and protect AWS accounts and environments. Microsoft Entra ID provides strong SSO authentication to almost any app or platform that follows common web authentication standards, including AWS. AWS accounts that support critical workloads and highly sensitive information need strong identity protection and access control. AWS identity management is enhanced when you combine it with Microsoft Entra ID. AWS organizations that use Microsoft Entra ID for Microsoft 365 or hybrid cloud identity and access protection can quickly and easily deploy Microsoft Entra ID for AWS accounts, often without incurring additional costs. Microsoft Entra ID provides several capabilities for direct integration with AWS: Integration with AWS IAM Identity Center for enhanced security, improved user experience, centralized access control, and SSO across legacy, traditional, and modern authentication solutions. Microsoft Entra multifactor authentication, including integration with several third-party solutions from Microsoft Intelligent Security Association partners. Powerful Conditional Access features for strong authentication and strict governance. Microsoft Entra ID uses Conditional Access policies and risk-based assessments to authenticate and authorize user access to the AWS Management Console and AWS resources. Improved protection against identity-based attacks via real-time detection and remediation of risky sign-ins and unusual user behavior. Privileged Identity Management (PIM) to enable just-in-time provisioning of specific resources. You can expand PIM to any delegated permission by controlling access to custom groups, like groups that you create for access to AWS roles. For more information and detailed instructions, see Microsoft Entra identity and access management for AWS. Microsoft Defender for Cloud Apps When several users or roles make administrative changes, configuration drift away from intended security architecture and standards can occur. Security standards can also change over time. Security personnel must constantly and consistently detect new risks, evaluate mitigation options, and update security architecture to help prevent potential breaches. Security management across multiple public cloud and private infrastructure environments can become burdensome. Defender for Cloud Apps provides enhanced protection for software as a service (SaaS) applications. It provides the following features to help you monitor and protect your cloud app data: Fundamental Cloud Access Security Broker functionality, including shadow IT discovery, visibility into cloud app usage, enhanced protection against app-based threats from anywhere in the cloud, and information protection and compliance assessments. SaaS Security Posture Management features that enable security teams to improve the organization's security posture. Advanced threat protection, as part of the Microsoft extended detection and response solution, which enables powerful correlation of signal and visibility across the full cyberattack chain of advanced attacks. App-to-app protection, which extends the core threat scenarios to OAuth-enabled apps that have permissions and privileges to critical data and resources. Connecting AWS to Defender for Cloud Apps helps you secure your assets and detect potential threats by monitoring administrative and sign-in activities. You get notifications of possible brute force attacks, malicious use of privileged user accounts, unusual deletions of VMs, and publicly exposed storage buckets. Defender for Cloud Apps helps protect AWS environments from abuse of cloud resources, compromised accounts and insider threats, data leakage, and resource misconfiguration and insufficient access control. The following Defender for Cloud Apps capabilities are especially useful when you work with AWS environments. Detect cloud threats, compromised accounts, malicious insiders, and ransomware. Defender for Cloud Apps anomaly detection policies are triggered when there are unusual activities performed by users in AWS. Defender for Cloud Apps continually monitors your users' activities and uses UEBA and machine learning to learn and understand the typical behavior of your users and trigger alerts on any deviations. Limit exposure of shared data and enforce collaboration policies. Automate governance controls via actions like notifying users about alerts, requiring re-authentication or suspending users, making an S3 bucket private, or removing collaborators from an S3 bucket. Audit activities. Connect AWS auditing to Defender for Cloud apps to get visibility into user, admin, and sign-in activities. Get enhanced real-time protection for AWS. Use Defender for Cloud Apps Conditional Access app control to block and help protect downloads of sensitive AWS data by risky users. For more information on how to connect AWS environments to Defender for Cloud Apps, see Protect your Amazon Web Services environment. Microsoft Defender for Cloud Defender for Cloud is a Cloud-Native Application Protection Platform that's made up of security measures and practices that are designed to protect cloud-based applications from various cyberthreats and vulnerabilities. Defender for Cloud provides the following capabilities: A development security operations solution that unifies security management at the code level across multicloud and multiple-pipeline environments A cloud security posture management (CSPM) solution that surfaces actions that you can take to help prevent breaches A cloud workload protection platform (CWPP) that provides protection for servers, containers, storage, databases, and other workloads Defender for Cloud native AWS support provides several benefits: Foundational CSPM for AWS resources Defender CSPM for AWS resources CWPP support for Amazon EKS clusters CWPP support for AWS EC2 instances CWPP support for SQL servers running on AWS EC2 and RDS Custom for SQL Server The foundational CPSM and Defender CSPM are both completely agentless. Foundational CSPM provides recommendations on how to best harden your AWS resources and remediate misconfigurations. Defender for Cloud offers foundational multicloud CSPM capabilities for free. Defender CSPM provides advanced posture management capabilities like attack path analysis, cloud security explorer, advanced threat hunting, and security governance capabilities. It also provides tools to assess your security compliance with a wide range of benchmarks, regulatory standards, and any custom security policies required in your organization, industry, or region. The CWPP support for AWS EC2 instances provides capabilities like automatic provisioning of prerequisites on existing and new machines, vulnerability assessment, an integrated license for Microsoft Defender for Endpoint, file integrity monitoring, and more. The CWPP support for Amazon EKS clusters provides capabilities like discovery of unprotected clusters, advanced threat detection for the control plane and workload level, Kubernetes data plane recommendations (via the Azure Policy extension), and more. The CWPP support for SQL servers running on AWS EC2 and AWS RDS Custom for SQL Server provides capabilities like advanced threat protection, vulnerability assessment scanning, and more. Security standards provide support for assessing resources and workloads in AWS against regulatory compliance standards like Center for Internet Security (CIS) and Payment Card Industry (PCI) standards, and for the AWS Foundational Security Best Practices standard. For more information about protecting workloads in AWS, see Connect your AWS account and Assign regulatory compliance standards in Microsoft Defender for Cloud. Microsoft Sentinel Microsoft Sentinel is a scalable cloud-native security information and event management (SIEM) system that provides an intelligent and comprehensive solution for SIEM and security orchestration, automation, and response. Microsoft Sentinel provides cyberthreat detection, investigation, response, and proactive hunting. It gives you a bird's-eye view across your enterprise. You can use the AWS connectors to pull AWS service logs into Microsoft Sentinel. These connectors work by granting Microsoft Sentinel access to your AWS resource logs. Setting up the connector establishes a trust relationship between AWS and Microsoft Sentinel. You create this relationship on AWS by creating a role that gives permission to Microsoft Sentinel to access your AWS logs. The connector can ingest logs from the following AWS services by pulling them from an S3 bucket: Service Data source Amazon Virtual Private Cloud (VPC) VPC Flow Logs Amazon GuardDuty GuardDuty findings AWS CloudTrail Management and data events AWS CloudWatch CloudWatch Logs For more information on how to install and configure the AWS connector in Microsoft Sentinel, see Connect Microsoft Sentinel to Amazon Web Services to ingest AWS service log data. Recommendations Use the Microsoft security solutions and basic AWS security recommendations to protect AWS accounts. Basic AWS account security For information about basic security hygiene for AWS accounts and resources, review the AWS security guidance at Best practices for securing AWS accounts and resources. Reduce the risk of uploading and downloading malware and other malicious content by actively inspecting all data transfers via the AWS Management Console. Content that you upload or download directly to resources within the AWS platform, such as web servers or databases, might need additional protection. Provide security for access keys by rotating the keys periodically. Avoid embedding them in code. Use IAM roles instead of long-term access keys wherever possible. Use security groups and network ACLs to control inbound and outbound traffic to your resources. Implement VPC to isolate resources. Encrypt sensitive data at rest and in transit by using AWS Key Management Services. Protect devices that administrators and developers use to access the AWS Management Console. Contributors This article is maintained by Microsoft. It was originally written by the following contributor. Principal author: Lavanya Murthy | Principal Cloud Solution Architect To see non-public LinkedIn profiles, sign in to LinkedIn. Next steps Monitor and protect AWS administrative and sign-in activities Protect workloads in AWS Connect Microsoft Sentinel to Amazon Web Services to ingest AWS service log data Related resources Secure AWS identities Feedback Was this page helpful? Yes No", "metadata": {"source": "https://docs.microsoft.com/en-us/azure/architecture/guide/aws/aws-azure-security-solutions", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 1613, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9450000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Kubernetes Documentation\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Kubernetes is an open source container orchestration engine for automating deployment, scaling, and management of containerized applications. The open source project is hosted by the Cloud Native Computing Foundation (CNCF).Understand KubernetesLearn about Kubernetes and its fundamental concepts.Why Kubernetes?Components of a clusterThe Kubernetes APIObjects In KubernetesContainersWorkloads and PodsView ConceptsTry KubernetesFollow tutorials to learn how to deploy applications in Kubernetes.Hello MinikubeWalkthrough the basicsStateless Example: PHP Guestbook with RedisStateful Example: Wordpress with Persistent VolumesView TutorialsSet up a K8s clusterGet Kubernetes running based on your resources and needs.Learning environmentProduction environmentInstall the kubeadm setup toolSecuring a clusterkubeadm command referenceSet up KubernetesLearn how to use KubernetesLook up common tasks and how to perform them using a short sequence of steps.kubectl Quick ReferenceInstall kubectlConfigure access to clustersUse the Web UI DashboardConfigure a Pod to Use a ConfigMapGetting helpView TasksLook up reference informationBrowse terminology, command line syntax, API resource types, and setup tool documentation.Glossarykubectl command line toolLabels, annotations and taintsKubernetes API referenceOverview of APIFeature GatesView ReferenceContribute to KubernetesFind out how you can help make Kubernetes better.Contribute to KubernetesContribute to documentationSuggest content improvementsOpening a pull requestDocumenting a feature for a releaseLocalizing the docsParticipating in SIG DocsViewing Site AnalyticsSee Ways to ContributeTrainingGet certified in Kubernetes and make your cloud native projects successful!View trainingDownload KubernetesInstall Kubernetes or upgrade to the newest version.Download KubernetesAbout the documentationThis website contains documentation for the current and previous 4 versions of Kubernetes.See available versionsLast modified April 20, 2024 at 8:53 PM PST: Ready docs landing page for vanilla Docsy (4da04a797b)", "metadata": {"source": "https://kubernetes.io/docs/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 244, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9125000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Concepts\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "The Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your cluster, and helps you obtain a deeper understanding of how Kubernetes works.OverviewKubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.Cluster ArchitectureThe architectural concepts behind Kubernetes.ContainersTechnology for packaging an application along with its runtime dependencies.WorkloadsUnderstand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them.Services, Load Balancing, and NetworkingConcepts and resources behind networking in Kubernetes.StorageWays to provide both long-term and temporary storage to Pods in your cluster.ConfigurationResources that Kubernetes provides for configuring Pods.SecurityConcepts for keeping your cloud-native workload secure.PoliciesManage security and best-practices with policies.Scheduling, Preemption and EvictionCluster AdministrationLower-level detail relevant to creating or administering a Kubernetes cluster.Windows in KubernetesKubernetes supports nodes that run Microsoft Windows.Extending KubernetesDifferent ways to change the behavior of your Kubernetes cluster.FeedbackWas this page helpful?Yes NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub Repository if you want to report a problem or suggest an improvement.Last modified June 22, 2020 at 11:01 PM PST: Add descriptions to Concept sections (3ff7312cff)", "metadata": {"source": "https://kubernetes.io/docs/concepts/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 226, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9025000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Concepts\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "You are viewing documentation for Kubernetes version: v1.32Kubernetes v1.32 documentation is no longer actively maintained. The version you are currently viewing is a static snapshot. For up-to-date information, see the latest version.The Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your cluster, and helps you obtain a deeper understanding of how Kubernetes works.OverviewKubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.Cluster ArchitectureThe architectural concepts behind Kubernetes.ContainersTechnology for packaging an application along with its runtime dependencies.WorkloadsUnderstand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them.Services, Load Balancing, and NetworkingConcepts and resources behind networking in Kubernetes.StorageWays to provide both long-term and temporary storage to Pods in your cluster.ConfigurationResources that Kubernetes provides for configuring Pods.SecurityConcepts for keeping your cloud-native workload secure.PoliciesManage security and best-practices with policies.Scheduling, Preemption and EvictionCluster AdministrationLower-level detail relevant to creating or administering a Kubernetes cluster.Windows in KubernetesKubernetes supports nodes that run Microsoft Windows.Extending KubernetesDifferent ways to change the behavior of your Kubernetes cluster.FeedbackWas this page helpful?Yes NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub Repository if you want to report a problem or suggest an improvement.Last modified June 22, 2020 at 11:01 PM PST: Add descriptions to Concept sections (3ff7312cff)", "metadata": {"source": "https://v1-32.docs.kubernetes.io/docs/concepts/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 257, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9025000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Concepts\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "You are viewing documentation for Kubernetes version: v1.31 Kubernetes v1.31 documentation is no longer actively maintained. The version you are currently viewing is a static snapshot. For up-to-date information, see the latest version. The Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your cluster, and helps you obtain a deeper understanding of how Kubernetes works. Overview Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. Cluster Architecture The architectural concepts behind Kubernetes. Containers Technology for packaging an application along with its runtime dependencies. Workloads Understand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them. Services, Load Balancing, and Networking Concepts and resources behind networking in Kubernetes. Storage Ways to provide both long-term and temporary storage to Pods in your cluster. Configuration Resources that Kubernetes provides for configuring Pods. Security Concepts for keeping your cloud-native workload secure. Policies Manage security and best-practices with policies. Scheduling, Preemption and Eviction Cluster Administration Lower-level detail relevant to creating or administering a Kubernetes cluster. Windows in Kubernetes Kubernetes supports nodes that run Microsoft Windows. Extending Kubernetes Different ways to change the behavior of your Kubernetes cluster. Feedback Was this page helpful? Yes No Thanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub Repository if you want to report a problem or suggest an improvement. Last modified June 22, 2020 at 11:01 PM PST: Add descriptions to Concept sections (3ff7312cff)", "metadata": {"source": "https://v1-31.docs.kubernetes.io/docs/concepts/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 289, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9025000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for ধারণা\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "ধারণা বিভাগটি আপনাকে কুবারনেটিস সিস্টেমের অংশগুলো এবং কুবারনেটিস আপনার ক্লাস্টারের প্রতিনিধিত্ব করার জন্য যে অ্যাবস্ট্রাকশনগুলো ব্যবহার করে সেগুলো সম্পর্কে শিখতে সাহায্য করে এবং কুবারনেটিস কীভাবে কাজ করে সে সম্পর্কে আপনাকে গভীরভাবে বুঝতে সাহায্য করে ।ওভারভিউকুবারনেটিস হল একটি পোর্টেবল, এক্সটেনসিবল, ওপেন সোর্স প্ল্যাটফর্ম যা কন্টেইনারাইজড ওয়ার্কলোড এবং সার্ভিসগুলি পরিচালনা করার জন্য, ঘোষণামূলক কনফিগারেশন এবং অটোমেশন উভয়কেই সহজতর করে। এটির একটি বড়, দ্রুত বর্ধনশীল ইকোসিস্টেম রয়েছে। কুবারনেটিস সার্ভিসগুলি, সাপোর্ট, এবং টুলস ব্যাপকভাবে সহজলভ্য।ক্লাস্টার আর্কিটেকচারকুবারনেটিসের পিছনে আর্কিটেকচারের ধারণা ।কন্টেইনাররানটাইম নির্ভরতা সহ একটি অ্যাপ্লিকেশন প্যাকেজ করার প্রযুক্তি।ওয়ার্কলোডপড বুঝুন, কুবারনেটিসের সবচেয়ে ছোট ডেপ্লয়বল কম্পিউট অবজেক্ট এবং উচ্চ-লেভেল অবস্ট্রাক্শন যা আপনাকে সেগুলো চালাতে সাহায্য করে ।সার্ভিস, লোড ব্যালেন্সিং এবং নেটওয়ার্কিংকুবারনেটিসে নেটওয়ার্কিংয়ের পিছনে থাকা ধারণা এবং রিসোর্স।স্টোরেজআপনার ক্লাস্টারে পডগুলোতে দীর্ঘমেয়াদী এবং অস্থায়ী উভয় স্টোরেজ সরবরাহ করার উপায়।কনফিগারেশনপডস কনফিগার করার জন্য কুবারনেটিস যে রিসোর্সগুলো প্রদান করে ।নিরাপত্তাক্লাউড-নেটিভ ওয়ার্কলোডকে নিরাপত্তা রক্ষা করার প্রস্তুতির জন্য ধারণাগুলি।নীতিমালানীতিগুলির সাথে সুরক্ষা এবং সর্বোত্তম-অনুশীলনগুলি পরিচালনা করুনশিডিউলিং, প্রিএম্পশন এবং ইভিকশন (Scheduling, Preemption and Eviction)ক্লাস্টার অ্যাডমিনিস্ট্রেশনএকটি কুবারনেটিস ক্লাস্টার তৈরি বা পরিচালনার জন্য প্রাসঙ্গিক নিম্ন-স্তরের ডিটেইল।কুবারনেটিসে উইন্ডোজকুবারনেটিস নোড সমর্থন করে যা মাইক্রোসফ্ট উইন্ডোজ চালায়।কুবারনেটিস প্রসারিত করাআপনার কুবারনেটিস ক্লাস্টারের আচরণ পরিবর্তন করার বিভিন্ন উপায়।ফিডব্যাকএই পেজটি কি সহায়ক ছিল?হ্যাঁ নাসাহায্য করার জন্য ধন্যবাদ. কুবারনেটিস কীভাবে ব্যবহার করবেন সে সম্পর্কে আপনার যদি একটি নির্দিষ্ট, উত্তরযোগ্য প্রশ্ন থাকে তবে এটি জিজ্ঞাসা করুন Stack Overflow. আপনি যদি চান তবে গিটহাব রিপোতে এ একটি ইস্যু খুলুন একটি সমস্যা রিপোর্ট করুন অথবা উন্নতির পরামর্শ দিন.সর্বশেষ পরিবর্তিত May 05, 2024 at 11:49 AM PST: Update _index.md (48349c67d2)", "metadata": {"source": "https://kubernetes.io/bn/docs/concepts/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 232, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8975000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for 概念\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "概念部分帮助你了解 Kubernetes 系统的各个部分以及 Kubernetes 用来表示集群的抽象概念， 并帮助你更深入地理解 Kubernetes 是如何工作的。概述Kubernetes 是一个可移植、可扩展的开源平台，用于管理容器化的工作负载和服务，方便进行声明式配置和自动化。Kubernetes 拥有一个庞大且快速增长的生态系统，其服务、支持和工具的使用范围广泛。Kubernetes 架构Kubernetes 背后的架构概念。容器打包应用及其运行依赖环境的技术。工作负载理解 Kubernetes 中可部署的最小计算对象 Pod 以及辅助 Pod 运行的上层抽象。服务、负载均衡和联网Kubernetes 网络背后的概念和资源。存储为集群中的 Pods 提供长期和临时存储的方法。配置Kubernetes 为配置 Pod 所提供的资源。安全确保云原生工作负载安全的一组概念。策略通过策略管理安全性和最佳实践。调度、抢占和驱逐集群管理关于创建和管理 Kubernetes 集群的底层细节。Kubernetes 中的 WindowsKubernetes 支持运行 Microsoft Windows 节点。扩展 Kubernetes改变你的 Kubernetes 集群的行为的若干方法。反馈此页是否对你有帮助？是 否感谢反馈。如果你有一个关于如何使用 Kubernetes 的具体问题需要答案，可以访问 Stack Overflow. 如果需要，请在 GitHub 仓库 上登记新的问题 报告问题 或者 提出改进建议.最后修改 August 16, 2022 at 3:51 PM PST: [zh] Update content\\zh-cn\\docs\\concepts\\_index.md (da40f4ad83)", "metadata": {"source": "https://kubernetes.io/zh-cn/docs/concepts/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 58, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8875000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Concepts\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "La section Concepts vous aide à mieux comprendre les composants du système Kubernetes et les abstractions que Kubernetes utilise pour représenter votre cluster. Elle vous aide également à mieux comprendre le fonctionnement de Kubernetes en général.Vue d'ensemblePour utiliser Kubernetes, vous utilisez les objets de l'API Kubernetes pour décrire l'état souhaité de votre cluster: quelles applications ou autres processus que vous souhaitez exécuter, quelles images de conteneur elles utilisent, le nombre de réplicas, les ressources réseau et disque que vous mettez à disposition, et plus encore. Vous définissez l'état souhaité en créant des objets à l'aide de l'API Kubernetes, généralement via l'interface en ligne de commande, kubectl. Vous pouvez également utiliser l'API Kubernetes directement pour interagir avec le cluster et définir ou modifier l'état souhaité.Une fois que vous avez défini l'état souhaité, le plan de contrôle Kubernetes (control plane en anglais) permet de faire en sorte que l'état actuel du cluster corresponde à l'état souhaité. Pour ce faire, Kubernetes effectue automatiquement diverses tâches, telles que le démarrage ou le redémarrage de conteneurs, la mise à jour du nombre de réplicas d'une application donnée, etc. Le control plane Kubernetes comprend un ensemble de processus en cours d'exécution sur votre cluster:Le maître Kubernetes (Kubernetes master en anglais) qui est un ensemble de trois processus qui s'exécutent sur un seul nœud de votre cluster, désigné comme nœud maître (master node en anglais). Ces processus sont: kube-apiserver, kube-controller-manager et kube-scheduler.Chaque nœud non maître de votre cluster exécute deux processus:kubelet, qui communique avec le Kubernetes master.kube-proxy, un proxy réseau reflétant les services réseau Kubernetes sur chaque nœud.Objets KubernetesKubernetes contient un certain nombre d'abstractions représentant l'état de votre système: applications et processus conteneurisés déployés, leurs ressources réseau et disque associées, ainsi que d'autres informations sur les activités de votre cluster. Ces abstractions sont représentées par des objets de l'API Kubernetes; consultez Vue d'ensemble des objets Kubernetes pour plus d'informations.Les objets de base de Kubernetes incluent:PodServiceVolumeNamespaceEn outre, Kubernetes contient un certain nombre d'abstractions de niveau supérieur appelées Contrôleurs. Les contrôleurs s'appuient sur les objets de base et fournissent des fonctionnalités supplémentaires.Voici quelques exemples:ReplicaSetDeploymentStatefulSetDaemonSetJobKubernetes control planeLes différentes parties du control plane Kubernetes, telles que les processus Kubernetes master et kubelet, déterminent la manière dont Kubernetes communique avec votre cluster. Le control plane conserve un enregistrement de tous les objets Kubernetes du système et exécute des boucles de contrôle continues pour gérer l'état de ces objets. À tout moment, les boucles de contrôle du control plane répondent aux modifications du cluster et permettent de faire en sorte que l'état réel de tous les objets du système corresponde à l'état souhaité que vous avez fourni.Par exemple, lorsque vous utilisez l'API Kubernetes pour créer un objet Deployment, vous fournissez un nouvel état souhaité pour le système. Le control plane Kubernetes enregistre la création de cet objet et exécute vos instructions en lançant les applications requises et en les planifiant vers des nœuds de cluster, afin que l'état actuel du cluster corresponde à l'état souhaité.Kubernetes masterLe Kubernetes master est responsable du maintien de l'état souhaité pour votre cluster. Lorsque vous interagissez avec Kubernetes, par exemple en utilisant l'interface en ligne de commande kubectl, vous communiquez avec le master Kubernetes de votre cluster.Le \"master\" fait référence à un ensemble de processus gérant l'état du cluster. En règle générale, tous les processus sont exécutés sur un seul nœud du cluster. Ce nœud est également appelé master. Le master peut également être répliqué pour la disponibilité et la redondance.Noeuds KubernetesLes nœuds d’un cluster sont les machines (serveurs physiques, machines virtuelles, etc.) qui exécutent vos applications et vos workflows. Le master node Kubernetes contrôle chaque noeud; vous interagirez rarement directement avec les nœuds.Metadonnées des objets KubernetesAnnotationsA suivreSi vous souhaitez écrire une page de concept, consultez Utilisation de modèles de page pour plus d'informations sur le type de page pour la documentation d'un concept.Vue d'ensembleKubernetes est une plateforme open source portable et extensible pour gérer les charges de travail et les services conteneurisés, qui facilite à la fois la configuration déclarative et l'automatisation. Il dispose d'un écosystème vaste et en pleine croissance. Les services, le support et les outils Kubernetes sont largement disponibles.Architecture du clusterLes concepts architecturaux derrière Kubernetes.Les conteneursConteneurs KubernetesWorkloadsComprendre les Pods, le plus petit objet déployable sur Kubernetes, et les abstractions de haut niveaux vous permettant de les lancer.Services, Equilibreur de charge, et RéseauService Reseau KubernetesStockageConfigurationSécuritéPolitiquesAdministration d'un clusterAdministration cluster KubernetesExtensions KubernetesFeedbackCette page est elle utile ?Oui NonThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub Repository if you want to report a problem or suggest an improvement.Dernière modification September 02, 2024 at 9:30 PM PST: update fr docs concepts overview (db1e142f84)", "metadata": {"source": "https://kubernetes.io/fr/docs/concepts/", "language": "markdown", "complexity": "medium", "type": "official_site", "word_count": 783, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.9025000000000001, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Konzepte\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Im Abschnitt Konzepte erfahren Sie mehr über die Bestandteile des Kubernetes-Systems und die Abstraktionen, die Kubernetes zur Verwaltung Ihres Clusters zur Verfügung stellt. Sie erhalten zudem ein tieferes Verständnis der Funktionsweise von Kubernetes.ÜberblickKubernetes ist eine portable, erweiterbare und quelloffene Plattform, um containerisierte Arbeitslasten und Dienste zu verwalten. Dies wird mithilfe von Automatisierungen und deklarativen Konfigurationen erreicht. Kubernetes hat ein großes, schnell wachsendes Ökosystem. Dienstleistungen, Hilfestellungen und Tools für Kubernetes sind weit verbreitet.Kubernetes ArchitekturHier werden die architektonischen Konzepte von Kubernetes beschrieben.ContainerMethoden, um Anwendungen und ihre Abhängigkeiten zusammenzufassen.WorkloadsInformationen über Pods, die kleinsten Einheiten, die in Kubernetes bereitgestellt werden können und über Abstraktionen, die hierbei behilflich sind.Dienste, Lastverteilung und NetzwerkfunktionenKonzepte und Resourcen bezüglich Netzwerktechnik in KubernetesSpeicherMethoden, um volatilen oder persistenten Speicher für Pods im Cluster zur Verfügung zu stellen.KonfigurationResourcen, die bei der Konfiguration von Pods in Kubernetes nützlich sind.RichtlinienSie können Richtlinien erstellen, die Resource-Gruppen zugewiesen werden können.Cluster AdministrationTiefergreifende Details, die für das Erstellen und Administrieren eines Kubernetes Clusters relevant sind.Kubernetes erweiternVerschiedene Wege, um die Funktionalität von Kubernetes zu erweitern.Konzept Dokumentations-VorlageWenn Sie eine Konzeptseite schreiben möchten, lesen Sie Seitenvorlagen verwenden für Informationen zum Konzeptseitentyp und zur Dokumentations-Vorlage.FeedbackWar diese Seite hilfreich?Ja NeinDanke für die Rückmeldung. Wenn Sie eine spezifische, beantwortbare Frage zur Verwendung von Kubernetes haben, stellen Sie diese unter Stack Overflow. Öffnen Sie ein Problem im GitHub-Repo, wenn Sie möchten Ein Problem melden oder Eine Verbesserung vorschlagen.Zuletzt geändert May 21, 2023 at 2:52 PM PST: Change style of German Concepts index page (d524178a60)", "metadata": {"source": "https://kubernetes.io/de/docs/concepts/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 239, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8975000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for अवधारणाएँ\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "अवधारणा अनुभाग आपको कुबेरनेट्स प्रणाली के हिस्सों के बारे में जानने में मदद करता है जिसका उपयोग कुबेरनेट्स आपके क्लस्टर का प्रतिनिधित्व करने के लिए करता है, और कुबेरनेट्स कार्यप्रणाली की गहरी समझ प्राप्त करने में आपकी मदद करता है।अवलोकनकुबेरनेट्स कंटेनरीकृत वर्कलोड और सेवाओं के प्रबंधन के लिए एक पोर्टेबल, एक्स्टेंसिबल, ओपन-सोर्स प्लेटफॉर्म है, जो घोषणात्मक कॉन्फ़िगरेशन और स्वचालन दोनों की सुविधा प्रदान करता है। इसका एक बड़ा, तेजी से बढ़ता हुआ पारिस्थितिकी तंत्र है। कुबेरनेट्स सेवाएँ, समर्थन और उपकरण व्यापक रूप से उपलब्ध हैं।सर्विसेज, लोड बैलेंसिंग और नेटवर्किंगकुबेर्नेट्स में नेटवर्किंग की मूल अवधारणाएँ और संसाधन।नीतियाँनीतियों के साथ सुरक्षा और सर्वोत्तम प्रथाओं को प्रबंधित करें।प्रतिक्रियाक्या यह पृष्ठ उपयोगी था?हाँ नहींप्रतिक्रिया के लिए धन्यवाद। यदि आपके पास कुबेरनेट्स का उपयोग करने के तरीके के बारे में कोई विशिष्ट, जवाबदेह प्रश्न है, तो इस पर पूछें Stack Overflow. यदि आप चाहते हैं तो GitHub रेपो में एक issue खोलें समस्या के बारे में बताएं या सुधार का सुझाव दें.Last modified July 01, 2023 at 10:08 AM PST: [hi] added content/hi/docs/concepts/_index.md (#39569) (590e87120e)", "metadata": {"source": "https://kubernetes.io/hi/docs/concepts/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 170, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8975000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Konsep\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Bagian konsep ini membantu kamu belajar tentang bagian-bagian sistem serta abstraksi yang digunakan Kubernetes untuk merepresentasikan klaster kamu, serta membantu kamu belajar lebih dalam bagaimana cara kerja Kubernetes.IkhtisarUntuk menggunakan Kubernetes, kamu menggunakan objek-objek Kubernetes API untuk merepresentasikan state yang diinginkan: apa yang aplikasi atau workload lain yang ingin kamu jalankan, image kontainer yang digunakan, jaringan atau resource disk apa yang ingin kamu sediakan, dan lain sebagainya. Kamu membuat state yang diinginkan dengan cara membuat objek dengan menggunakan API Kubernetes, dan biasanya menggunakan command-line interface, yaitu kubectl. Kamu juga dapat secara langsung berinteraksi dengan klaster untuk membuat atau mengubah state yang kamu inginkan.Setelah kamu membuat state yang kamu inginkan, Control Plane Kubernetes menggunakan Pod Lifecycle Event Generator (PLEG) untuk mengubah state yang ada saat ini supaya sama dengan state yang diinginkan. Untuk melakukan hal tersebut, Kubernetes melakukan berbagai task secara otomatis, misalnya dengan mekanisme start atau stop kontainer, melakukan scale replika dari suatu aplikasi, dan lain sebagainya. Control Plane Kubernetes terdiri dari sekumpulan process yang dijalankan di klaster:Kubernetes Master terdiri dari tiga buah process yang dijalankan pada sebuah node di klaster kamu, node ini disebut sebagai master, yang terdiri kube-apiserver, kube-controller-manager dan kube-scheduler.Setiap node non-master pada klaster kamu menjalankan dua buah process:kubelet, yang menjadi perantara komunikasi dengan master.kube-proxy, sebuah proxy yang merupakan representasi jaringan yang ada pada setiap node.Objek KubernetesKubernetes memiliki beberapa abstraksi yang merepresentasikan state dari sistem kamu: apa yang aplikasi atau workload lain yang ingin kamu jalankan, jaringan atau resource disk apa yang ingin kamu sediakan, serta beberapa informasi lain terkait apa yang sedang klaster kamu lakukan. Abstraksi ini direpresentasikan oleh objek yang tersedia di API Kubernetes; lihat ikhtisar objek-objek Kubernetes untuk penjelasan yang lebih mendetail.Objek mendasar Kubernetes termasuk:PodServiceVolumeNamespaceSebagai tambahan, Kubernetes memiliki beberapa abstraksi yang lebih tinggi yang disebut kontroler. Kontroler merupakan objek mendasar dengan fungsi tambahan, contoh dari kontroler ini adalah:ReplicaSetDeploymentStatefulSetDaemonSetJobControl Plane KubernetesBerbagai bagian Control Plane Kubernetes, seperti master dan process-process kubelet, mengatur bagaimana Kubernetes berkomunikasi dengan klaster kamu. Control Plane menjaga seluruh record dari objek Kubernetes serta terus menjalankan iterasi untuk melakukan manajemen state objek. Control Plane akan memberikan respon apabila terdapat perubahan pada klaster kamu dan mengubah state saat ini agar sesuai dengan state yang diinginkan.Contohnya, ketika kamu menggunakan API Kubernetes untuk membuat sebuah Deployment, kamu memberikan sebuah state baru yang harus dipenuhi oleh sistem. Control Plane kemudian akan mencatat objek apa saja yang dibuat, serta menjalankan instruksi yang kamu berikan dengan cara melakukan start aplikasi dan melakukan scheduling aplikasi tersebut pada node, dengan kata lain mengubah state saat ini agar sesuai dengan state yang diinginkan.MasterMaster Kubernetes bertanggung jawab untuk memelihara state yang diinginkan pada klaster kamu. Ketika kamu berinteraksi dengan Kubernetes, misalnya saja menggunakan perangkat kubectl, kamu berkomunikasi dengan master klaster Kubernetes kamu.\"master\" merujuk pada tiga buah process yang dijalankan pada sebuah node pada klaster kamu, node ini disebut sebagai master, yang terdiri kube-apiserver, kube-controller-manager dan kube-scheduler.NodeNode di dalam klaster Kubernetes adalah mesin (mesin virtual maupun fisik) yang menjalankan aplikasi kamu. Master mengontrol setiap node; kamu akan jarang berinteraksi dengan node secara langsung.Metadata objekAnotasiSelanjutnyaJika kamu ingin menulis halaman konsep, perhatikan cara penggunaan template pada laman untuk informasi mengenai konsep tipe halaman dan template konsep.IkhtisarArsitektur KubernetesKontainerWorkloadsServices, Load Balancing, dan JaringanStorageKonfigurasiKeamananKonsep-konsep untuk menjaga cloud-native workload tetap aman.Penjadwalan dan PengusiranPoliciesAdministrasi KlasterMemperluas KubernetesMasukanApakah halaman ini membantu?Ya TidakTerima kasih atas masukannya. Jika kamu mempunyai pertanyaan yang spesifik terkait bagaimana menggunakan Kubernetes, tanyakanlah di Stack Overflow. Buat isu di repositori GitHub jika kamu ingin laporkan problem atau beri saran perbaikan.Last modified February 26, 2024 at 12:57 AM PST: fix broken link indonesian docs (470efefe50)", "metadata": {"source": "https://kubernetes.io/id/docs/concepts/", "language": "markdown", "complexity": "medium", "type": "official_site", "word_count": 583, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8975000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Concetti\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "La sezione Concetti ti aiuta a conoscere le parti del sistema Kubernetes e le astrazioni utilizzate da Kubernetes per rappresentare il tuo cluster e ti aiuta ad ottenere una comprensione più profonda di come funziona Kubernetes.OverviewPer lavorare con Kubernetes, usi gli oggetti API Kubernetes per descrivere lo stato desiderato del tuo cluster: quali applicazioni o altri carichi di lavoro vuoi eseguire, quali immagini del contenitore usano, numero di repliche, quali risorse di rete e disco vuoi rendere disponibile e altro ancora. Puoi impostare lo stato desiderato creando oggetti usando l'API di Kubernetes, in genere tramite l'interfaccia della riga di comando, kubectl. Puoi anche utilizzare direttamente l'API di Kubernetes per interagire con il cluster e impostare o modificare lo stato desiderato.Una volta impostato lo stato desiderato, il Kubernetes Control Plane funziona per fare in modo che lo stato corrente del cluster corrisponda allo stato desiderato. Per fare ciò, Kubernetes esegue automaticamente una serie di attività, come l'avvio o il riavvio dei contenitori, il ridimensionamento del numero di repliche di una determinata applicazione e altro ancora. Il piano di controllo di Kubernetes è costituito da una raccolta di processi in esecuzione sul cluster:Il Kubernetes Master è una raccolta di tre processi che vengono eseguiti su un singolo nodo nel cluster, che è designato come nodo principale. Questi processi sono: kube-apiserver, kube-controller-manager e kube-scheduler.Ogni singolo nodo non principale nel cluster esegue due processi: * kubelet, che comunica con il master di Kubernetes. * kube-proxy, un proxy di rete che riflette i servizi di rete di Kubernetes su ciascun nodo.Kubernetes Objectskubernetes contiene una serie di astrazioni che rappresentano lo stato del tuo sistema: applicazioni e carichi di lavoro distribuiti in container, le loro risorse di rete e disco associate e altre informazioni su ciò che sta facendo il tuo cluster. Queste astrazioni sono rappresentate da oggetti nell'API di Kubernetes; guarda la Panoramica degli oggetti di Kubernetes per maggiori dettagli.Gli oggetti di base di Kubernetes includono:PodServiceVolumeNamespace209/5000 Inoltre, Kubernetes contiene una serie di astrazioni di livello superiore denominate Controllori. I controller si basano sugli oggetti di base e forniscono funzionalità aggiuntive e funzionalità di convenienza. Loro includono:ReplicaSetDeploymentStatefulSetDaemonSetJobKubernetes Control PlaneLe varie parti del Piano di controllo di Kubernetes, come i master Kubernetes e i processi di Kubelet, regolano il modo in cui Kubernetes comunica con il cluster. Il Piano di controllo mantiene un registro di tutti gli oggetti Kubernetes nel sistema e esegue cicli di controllo continui per gestire lo stato di tali oggetti. In qualsiasi momento, i loop di controllo di Control Plane risponderanno ai cambiamenti nel cluster e lavoreranno per fare in modo che lo stato effettivo di tutti gli oggetti nel sistema corrisponda allo stato desiderato che hai fornito.Ad esempio, quando si utilizza l'API di Kubernetes per creare un oggetto di distribuzione, si fornisce un nuovo stato desiderato per il sistema. Il piano di controllo di Kubernetes registra la creazione dell'oggetto e svolge le tue istruzioni avviando le applicazioni richieste e pianificandole sui nodi del cluster, in modo che lo stato effettivo del cluster corrisponda allo stato desiderato.Kubernetes MasterIl master Kubernetes è responsabile della gestione dello stato desiderato per il tuo cluster. Quando interagisci con Kubernetes, ad esempio utilizzando l'interfaccia della riga di comando kubectl, stai comunicando con il master di Kubernetes del cluster.Il \"master\" si riferisce a una raccolta di processi che gestiscono lo stato del cluster. In genere questi processi vengono eseguiti tutti su un singolo nodo nel cluster e questo nodo viene anche definito master. Il master può anche essere replicato per disponibilità e ridondanza.Kubernetes NodesI nodi di un cluster sono le macchine (VM, server fisici, ecc.) che eseguono i flussi di lavoro delle applicazioni e del cloud. Il master Kubernetes controlla ciascun nodo; raramente interagirai direttamente con i nodi.Object MetadataAnnotationsVoci correlateSe vuoi scrivere una pagina concettuale, vedi Uso dei modelli di pagina per informazioni sul tipo di pagina di concetto e il modello di concetto.OverviewArchitettura di KubernetesContainersLa tecnologia per distribuire un'applicazione insieme con le dipendenze necessarie per la sua esecuzione.ConfigurazioneRisorse che fornisce Kubernetes per configurare i Pods.Amministrazione del ClusterEsempio di modello di concettoFeedbackQuesta pagina è stata di aiuto?Sì NoGrazie per il feedback. Se hai una domanda specifica su Kubernetes, chiedi su Stack Overflow. Apri un issue sul repository GitHub se vuoi riportare un problema o suggerire un miglioramento.Ultima modifica July 02, 2020 at 12:07 PM PST: docs(it/docs/concepts/index): fix wrong uppercase (c82e192033)", "metadata": {"source": "https://kubernetes.io/it/docs/concepts/", "language": "markdown", "complexity": "medium", "type": "official_site", "word_count": 721, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8975000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for コンセプト\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "本セクションは、Kubernetesシステムの各パートと、クラスターを表現するためにKubernetesが使用する抽象概念について学習し、Kubernetesの仕組みをより深く理解するのに役立ちます。概要Kubernetesを機能させるには、Kubernetes API オブジェクト を使用して、実行したいアプリケーションやその他のワークロード、使用するコンテナイメージ、レプリカ(複製)の数、どんなネットワークやディスクリソースを利用可能にするかなど、クラスターの desired state (望ましい状態)を記述します。desired state (望ましい状態)をセットするには、Kubernetes APIを使用してオブジェクトを作成します。通常はコマンドラインインターフェース kubectl を用いてKubernetes APIを操作しますが、Kubernetes APIを直接使用してクラスターと対話し、desired state (望ましい状態)を設定、または変更することもできます。一旦desired state (望ましい状態)を設定すると、Pod Lifecycle Event Generator(PLEG)を使用したKubernetes コントロールプレーンが機能し、クラスターの現在の状態をdesired state (望ましい状態)に一致させます。そのためにKubernetesはさまざまなタスク(たとえば、コンテナの起動または再起動、特定アプリケーションのレプリカ数のスケーリング等)を自動的に実行します。Kubernetesコントロールプレーンは、クラスターで実行されている以下のプロセスで構成されています。Kubernetes Master: kube-apiserver、kube-controller-manager、kube-scheduler の3プロセスの集合です。これらのプロセスはクラスター内の一つのノード上で実行されます。実行ノードはマスターノードとして指定します。クラスター内の個々の非マスターノードは、それぞれ2つのプロセスを実行します。kubelet: Kubernetes Masterと通信します。kube-proxy: 各ノードのKubernetesネットワークサービスを反映するネットワークプロキシです。KubernetesオブジェクトKubernetesには、デプロイ済みのコンテナ化されたアプリケーションやワークロード、関連するネットワークとディスクリソース、クラスターが何をしているかに関するその他の情報といった、システムの状態を表現する抽象が含まれています。これらの抽象は、Kubernetes APIのオブジェクトによって表現されます。詳細については、Kubernetesオブジェクトについて知るをご覧ください。基本的なKubernetesのオブジェクトは次のとおりです。PodServiceVolumeNamespaceKubernetesには、コントローラーに依存して基本オブジェクトを構築し、追加の機能と便利な機能を提供する高レベルの抽象化も含まれています。これらには以下のものを含みます:DeploymentDaemonSetStatefulSetReplicaSetJobKubernetesコントロールプレーンKubernetesマスターや kubeletプロセスといったKubernetesコントロールプレーンのさまざまなパーツは、Kubernetesがクラスターとどのように通信するかを統制します。コントロールプレーンはシステム内のすべてのKubernetesオブジェクトの記録を保持し、それらのオブジェクトの状態を管理するために継続的制御ループを実行します。コントロールプレーンの制御ループは常にクラスターの変更に反応し、システム内のすべてのオブジェクトの実際の状態が、指定した状態に一致するように動作します。たとえば、Kubernetes APIを使用してDeploymentを作成する場合、システムには新しいdesired state (望ましい状態)が提供されます。Kubernetesコントロールプレーンは、そのオブジェクトの作成を記録します。そして、要求されたアプリケーションの開始、およびクラスターノードへのスケジューリングにより指示を完遂します。このようにしてクラスターの実際の状態を望ましい状態に一致させます。KubernetesマスターKubernetesのマスターは、クラスターの望ましい状態を維持する責務を持ちます。kubectl コマンドラインインターフェースを使用するなどしてKubernetesとやり取りするとき、ユーザーは実際にはクラスターにあるKubernetesのマスターと通信しています。「マスター」とは、クラスター状態を管理するプロセスの集合を指します。通常これらのプロセスは、すべてクラスター内の単一ノードで実行されます。このノードはマスターとも呼ばれます。マスターは、可用性と冗長性のために複製することもできます。Kubernetesノードクラスターのノードは、アプリケーションとクラウドワークフローを実行するマシン(VM、物理サーバーなど)です。Kubernetesのマスターは各ノードを制御します。運用者自身がノードと直接対話することはほとんどありません。次の項目コンセプトページを追加したい場合は、 ページテンプレートの使用 のコンセプトページタイプとコンセプトテンプレートに関する情報を確認してください。概要Kubernetesは、宣言的な構成管理と自動化を促進し、コンテナ化されたワークロードやサービスを管理するための、ポータブルで拡張性のあるオープンソースのプラットフォームです。Kubernetesは巨大で急速に成長しているエコシステムを備えており、それらのサービス、サポート、ツールは幅広い形で利用可能です。クラスターのアーキテクチャKubernetesの背後にあるアーキテクチャのコンセプト。コンテナアプリケーションとランタイムの依存関係を一緒にパッケージ化するための技術ワークロードKubernetesにおけるデプロイ可能な最小のオブジェクトであるPodと、高レベルな抽象化がPodの実行を助けることを理解します。Service、負荷分散とネットワーキングKubernetesにおけるネットワーキングの概念とリソース。ストレージ設定セキュリティクラウドネイティブなワークロードをセキュアに維持するためのコンセプトポリシースケジューリング、プリエンプションと退避Kubernetesにおいてスケジューリングとは、稼働させたいPodをノードにマッチさせ、kubeletが実行できるようにすることを指します。 プリエンプションは、優先度の低いPodを終了させて、より優先度の高いPodがノード上でスケジュールできるようにするプロセスです。 退避(eviction)とは、リソース不足のノードで1つ以上のPodを積極的に終了させるプロセスです。クラスターの管理Kubernetesを拡張するKubernetesクラスターの挙動を変えるいろいろな方法フィードバックこのページは役に立ちましたか？はい いいえThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub repo if you want to 問題を報告する or 改善を提案.最終更新 June 15, 2025 at 2:27 PM PST: [jp] Remove the /ja prefix from text links on concepts index. (069681f90f)", "metadata": {"source": "https://kubernetes.io/ja/docs/concepts/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 94, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8925000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for 개념\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "개념 섹션을 통해 쿠버네티스 시스템을 구성하는 요소와 클러스터를 표현하는데 사용되는 추상 개념에 대해 배우고 쿠버네티스가 작동하는 방식에 대해 보다 깊이 이해할 수 있다.쿠버네티스란 무엇인가?쿠버네티스는 컨테이너화된 워크로드와 서비스를 관리하기 위한 이식할 수 있고, 확장 가능한 오픈소스 플랫폼으로, 선언적 구성과 자동화를 모두 지원한다. 쿠버네티스는 크고 빠르게 성장하는 생태계를 가지고 있다. 쿠버네티스 서비스, 지원 그리고 도구들은 광범위하게 제공된다.클러스터 아키텍처쿠버네티스 뒤편의 구조와 설계 개념들컨테이너런타임 의존성과 함께 애플리케이션을 패키징하는 기술쿠버네티스에서의 윈도우워크로드쿠버네티스에서 배포할 수 있는 가장 작은 컴퓨트 오브젝트인 파드와, 이를 실행하는 데 도움이 되는 하이-레벨(higher-level) 추상화서비스, 로드밸런싱, 네트워킹쿠버네티스의 네트워킹에 대한 개념과 리소스에 대해 설명한다.스토리지클러스터의 파드에 장기(long-term) 및 임시 스토리지를 모두 제공하는 방법구성쿠버네티스가 파드 구성을 위해 제공하는 리소스보안클라우드 네이티브 워크로드를 안전하게 유지하기 위한 개념정책리소스의 그룹에 적용되도록 구성할 수 있는 정책스케줄링, 선점(Preemption), 축출(Eviction)쿠버네티스에서, 스케줄링은 kubelet이 파드를 실행할 수 있도록 파드를 노드에 할당하는 것을 말한다. 선점은 우선순위가 높은 파드가 노드에 스케줄될 수 있도록 우선순위가 낮은 파드를 종료시키는 과정을 말한다. 축출은 리소스가 부족한 노드에서 하나 이상의 파드를 사전에 종료시키는 프로세스이다.클러스터 관리쿠버네티스 클러스터 생성 또는 관리에 관련된 로우-레벨(lower-level)의 세부 정보를 설명한다.쿠버네티스 확장쿠버네티스 클러스터의 동작을 변경하는 다양한 방법피드백이 페이지가 도움이 되었나요?네 아니요피드백 감사합니다. 쿠버네티스 사용 방법에 대해서 구체적이고 답변 가능한 질문이 있다면, 다음 링크에서 질문하십시오. Stack Overflow. 원한다면 GitHub 리포지터리에 이슈를 열어서 문제 리포트 또는 개선 제안이 가능합니다..최종 수정 July 15, 2020 at 10:59 AM PST: Eighth Korean l10n work for release 1.18 (fe296af187)", "metadata": {"source": "https://kubernetes.io/ko/docs/concepts/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 213, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8975000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Pojęcia\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Rozdział dotyczący pojęć ma za zadanie pomóc w zrozumieniu poszczególnych składowych systemu oraz obiektów abstrakcyjnych, których Kubernetes używa do reprezentacji klastra, a także posłużyć do lepszego poznania działania całego systemu.PrzeglądKubernetes to przenośna, rozszerzalna platforma oprogramowania open-source służąca do zarządzania zadaniami i serwisami uruchamianymi w kontenerach. Umożliwia ich deklaratywną konfigurację i automatyzację. Kubernetes posiada duży i dynamicznie rozwijający się ekosystem. Szeroko dostępne są usługi, wsparcie i dodatkowe narzędzia.Architektura klastraPodstawowe założenia architektury Kubernetesa.KontenerySystem \"pakowania\" aplikacji i jej zależności w spójne środowisko uruchomieniowe.WorkloadPoznaj Pody – podstawowy element obliczeniowy w Kubernetes – oraz mechanizmy ułatwiające ich wdrażanie.Usługi, równoważenie obciążenia i sieci w KubernetesiePojęcia i zasoby związane z siecią w Kubernetesie.Przechowywanie danychTrwałe i tymczasowe mechanizmy przechowywania danych dla Podów w klastrze.KonfiguracjaZasoby Kubernetesa wykorzystywane do konfiguracji Podów.BezpieczeństwoZasady ochrony aplikacji cloud-native.PolitykiStosuj polityki do zarządzania bezpieczeństwem i wdrażania najlepszych praktyk.Harmonogramowanie, pierszeństwo i eksmisjaAdministracja klastremNiskopoziomowe szczegóły istotne dla tworzenia i administracji klastrem Kubernetesa.Windows w KubernetesieKubernetes obsługuje węzły działające na systemie Microsoft Windows.Rozszerzanie KubernetesaRóżne sposoby na modyfikację działania klastra Kubernetesa.Twoja opiniaCzy ta strona była przydatna?Tak NieDziękujemy za informację zwrotną. Jeśli masz konkretne pytanie dotyczące użycia Kubernetesa, odwiedź Stack Overflow. Zgłoś problem w repozytorium GitHuba, jeśli chcesz zgłosić problem lub Zaproponuj ulepszenie.Ostatnia modyfikacja August 12, 2020 at 10:38 AM PST: Polish translation update 2020-08-12 (402d9459e7)", "metadata": {"source": "https://kubernetes.io/pl/docs/concepts/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 204, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8975000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Conceitos\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "A seção de Conceitos irá te ajudar a aprender mais sobre as partes do ecossistema Kubernetes e as abstrações que o Kubernetes usa para representar seu cluster.Ela irá lhe ajudar a obter um entendimento mais profundo sobre como o Kubernetes funciona.ControladoresVisão GeralKubernetes é um plataforma de código aberto, portável e extensiva para o gerenciamento de cargas de trabalho e serviços distribuídos em contêineres, que facilita tanto a configuração declarativa quanto a automação. Ele possui um ecossistema grande, e de rápido crescimento. Serviços, suporte, e ferramentas para Kubernetes estão amplamente disponíveis.Arquitetura do ClusterOs conceitos arquiteturais por trás do Kubernetes.ContêineresTecnologia para empacotar aplicações com suas dependências em tempo de execuçãoServiços, balanceamento de carga e conectividadeConceitos e recursos por trás da conectividade no Kubernetes.ArmazenamentoFormas de fornecer armazenamento temporário e de longa duração a Pods em seu cluster.ConfiguraçãoSegurançaPolíticasPolíticas que você pode configurar e que afetam grupos de recursos.Escalonamento, preempção e remoçãoNo Kubernetes, escalonamento refere-se à certeza de que os Pods correspondam aos nós para que o kubelet possa executá-los. Preempção é o processo de finalizar Pods com menor prioridade, para que os Pods com maior prioridade possam ser escalonados nos nós. Remoção é o processo de finalização proativa de um ou mais Pods em nós com poucos recursos.Administração de ClusterDetalhes de baixo nível relevantes para criar ou administrar um cluster Kubernetes.Extendendo o KubernetesWindows no KubernetesO Kubernetes oferece suporte a nós que executam Microsoft Windows.ComentáriosEsta página foi útil?Sim NãoObrigado pelo feedback. Se você tiver uma pergunta específica sobre como utilizar o Kubernetes, faça em Stack Overflow. Abra um bug no repositório do GitHub se você deseja relatar um problema ou sugerir uma melhoria.Última modificação April 19, 2021 at 9:10 PM PST: Move portuguese content to pt-br to correct shortcode problems and add redirection (#27413) (ae197690de)", "metadata": {"source": "https://kubernetes.io/pt-br/docs/concepts/", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 290, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8575000000000002, "syntax_valid": true, "completeness_score": 0.8, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Концепции\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Раздел \"Концепции\" поможет вам узнать о частях системы Kubernetes и об абстракциях, которые Kubernetes использует для представления вашего кластера, и помогает вам глубже понять, как работает Kubernetes.Краткий обзорЧтобы работать с Kubernetes, вы используете объекты API Kubernetes для описания желаемого состояния вашего кластера: какие приложения или другие рабочие нагрузки вы хотите запустить, какие образы контейнеров они используют, количество реплик, какие сетевые и дисковые ресурсы вы хотите использовать и сделать доступными и многое другое. Вы устанавливаете желаемое состояние, создавая объекты с помощью API Kubernetes, обычно через интерфейс командной строки kubectl. Вы также можете напрямую использовать API Kubernetes для взаимодействия с кластером и установки или изменения желаемого состояния.После того, как вы установили желаемое состояние, Управляющий слой Kubernetes (control plane) заставляет текущее состояние кластера соответствовать желаемому состоянию с помощью генератора событий жизненного цикла подов (Pod Lifecycle Event Generator, PLEG). Для этого Kubernetes автоматически выполняет множество задач, таких как запуск или перезапуск контейнеров, масштабирование количества реплик данного приложения и многое другое. Управляющий слой Kubernetes состоит из набора процессов, запущенных в вашем кластере:Мастер Kubernetes — это коллекция из трех процессов, которые выполняются на одном узле в вашем кластере, который обозначен как главный узел. Это процессы: kube-apiserver, kube-controller-manager и kube-scheduler.Каждый отдельный неосновной узел в вашем кластере выполняет два процесса:kubelet, который взаимодействует с мастером Kubernetes.kube-proxy, сетевой прокси, который обрабатывает сетевые сервисы Kubernetes на каждом узле.Объекты KubernetesKubernetes содержит ряд абстракций, которые представляют состояние вашей системы: развернутые контейнеризованные приложения и рабочие нагрузки, связанные с ними сетевые и дисковые ресурсы и другую информацию о том, что делает ваш кластер. Эти абстракции представлены объектами в API Kubernetes. См. Понимание объектов Kubernetes для получения более подробной информации.Основные объекты Kubernetes включают в себя:PodServiceТомNamespaceKubernetes также содержит абстракции более высокого уровня, которые опираются на Контроллеры для создания базовых объектов и предоставляют дополнительные функциональные и удобные функции. Они включают:DeploymentDaemonSetStatefulSetReplicaSetJobУправляющий слой KubernetesРазличные части управляющего слоя Kubernetes (control plane), такие как мастер Kubernetes и процессы kubelet, определяют, как Kubernetes взаимодействует с кластером. Управляющий слой поддерживает запись всех объектов Kubernetes в системе и запускает непрерывные циклы управления для обработки состояния этих объектов. В любое время циклы управления управляющего слоя будут реагировать на изменения в кластере и работать, чтобы фактическое состояние всех объектов в системе соответствовало желаемому состоянию, которое вы указали.Например, когда вы используете API Kubernetes для создания развертывания, вы предоставляете новое желаемое состояние для системы. Управляющий слой Kubernetes записывает создание этого объекта и выполняет ваши инструкции, запуская необходимые приложения и планируя их на узлы кластера, чтобы фактическое состояние кластера соответствовало желаемому состоянию.Мастер KubernetesМастер Kubernetes отвечает за поддержание желаемого состояния для вашего кластера. Когда вы взаимодействуете с Kubernetes, например, используя интерфейс командной строки kubectl, вы работаете с мастером Kubernetes вашего кластера.Под \"мастером\" понимается совокупность процессов, которые управляют состоянием кластера. Обычно все эти процессы выполняются на одном узле кластера, и поэтому этот узел называется главным (master). Мастер также может быть реплицирован для доступности и резервирования.Узлы KubernetesУзлы в кластере - это машины (виртуальные машины, физические серверы и т.д.), на которых работают ваши приложения и облачные рабочие процессы. Мастер Kubernetes контролирует каждый узел; вы редко будете взаимодействовать с узлами напрямую.Что дальшеЕсли вы хотите описать концепт, обратитесь к странице Использование шаблонов страниц для получения информации о типе страницы и шаблоне концепции.ОбзорКластерная АрхитектураАрхитектурные концепции, лежащие в основе Kubernetes.КонтейнерыТехнология упаковки приложения вместе с его runtime-зависимостями.Рабочие нагрузкиПоймите под, самый маленький развертываемый вычислительный объект в Kubernetes, и абстракции более высокого уровня, которые помогут вам их запускать.Планирование, приоритизация и вытеснениеВ Kubernetes под планированием понимается поиск подходящих узлов, на которых kubelet сможет запустить Pod'ы. Приоритизация — процесс завершения работы Pod'ов с более низким приоритетом и высвобождения места для Pod'ов с более высоким приоритетом. Вытеснение — это проактивное завершение работы одного или нескольких Pod'ов на узлах с дефицитом ресурсов.Администрирование кластераLower-level detail relevant to creating or administering a Kubernetes cluster.Обратная связьЭта страница была полезна?Да НетСпасибо за обратную связь! Если у вас есть конкретный вопрос об использовании Kubernetes, спросите на Stack Overflow. Создайте issue в репозитории GitHub, если вы хотите сообщить о проблеме или предложить улучшение.Изменено March 26, 2024 at 1:34 PM PST: Rename control plane in Russian (b49f023c29)", "metadata": {"source": "https://kubernetes.io/ru/docs/concepts/", "language": "markdown", "complexity": "medium", "type": "official_site", "word_count": 655, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8975000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Conceptos\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "La sección de conceptos te ayudará a conocer los componentes de Kubernetes así como las abstracciones que utiliza para representar tu cluster. Además, te ayudará a obtener un conocimiento más profundo sobre cómo funciona Kubernetes.IntroducciónEn Kubernetes se utilizan los objetos de la API de Kubernetes para describir el estado deseado del clúster: qué aplicaciones u otras cargas de trabajo se quieren ejecutar, qué imagenes de contenedores usan, el número de replicas, qué red y qué recursos de almacenamiento quieres que tengan disponibles, etc. Se especifica el estado deseado del clúster mediante la creación de objetos usando la API de Kubernetes, típicamente mediante la interfaz de línea de comandos, kubectl. También se puede usar la API de Kubernetes directamente para interactuar con el clúster y especificar o modificar tu estado deseado.Una vez que se especifica el estado deseado, el Plano de Control de Kubernetes realizará las acciones necesarias para que el estado actual del clúster coincida con el estado deseado. Para ello, Kubernetes realiza diferentes tareas de forma automática, como pueden ser: parar o arrancar contenedores, escalar el número de réplicas de una aplicación dada, etc. El Plano de Control de Kubernetes consiste en un grupo de daemons que corren en tu clúster:El Master de Kubernetes es un conjunto de tres daemons que se ejecutan en un único nodo del clúster, que se denomina nodo master. Estos daemons son: kube-apiserver, kube-controller-manager y kube-scheduler.Los restantes nodos no master contenidos en tu clúster, ejecutan los siguientes dos daemons:kubelet, el cual se comunica con el Master de Kubernetes.kube-proxy, un proxy de red que implementa los servicios de red de Kubernetes en cada nodo.Objetos de KubernetesKubernetes tiene diferentes abstracciones que representan el estado de tu sistema: aplicaciones contenerizadas desplegadas y cargas de trabajo, sus recursos de red y almacenamiento asociados e información adicional acerca de lo que el clúster está haciendo en un momento dado. Estas abstracciones están representadas por objetos de la API de Kubernetes. Puedes revisar [Entendiendo los Objetos de Kubernetes] (/docs/concepts/overview/working-with-objects/kubernetes-objects/) para obtener más detalles.Los objetos básicos de Kubernetes incluyen:PodServiceVolumeNamespaceAdemás, Kubernetes contiene abstracciónes de nivel superior llamadas Controladores. Los Controladores se basan en los objetos básicos y proporcionan funcionalidades adicionales sobre ellos. Incluyen:ReplicaSetDeploymentStatefulSetDaemonSetJobPlano de Control de KubernetesLos distintos componentes del Plano de Control de Kubernetes, tales como el Master de Kubernetes y el proceso kubelet, gobiernan cómo Kubernetes se comunica con el clúster. El Plano de Control mantiene un registro de todos los Objetos de Kubernetes presentes en el sistema y ejecuta continuos bucles de control para gestionar el estado de los mismos. En un momento dado, los bucles del Plano de Control responderán a los cambios que se realicen en el clúster y ejecutarán las acciones necesarias para hacer que el estado actual de todos los objetos del sistema converjan hacia el estado deseado que has proporcionado.Por ejemplo, cuando usas la API de Kubernetes para crear un Deployment, estás proporcionando un nuevo estado deseado para el sistema. El Plano de Control de Kubernetes registra la creación del objeto y lleva a cabo tus instrucciones ejecutando las aplicaciones requeridas en los nodos del clúster, haciendo de esta manera que el estado actual coincida con el estado deseado.El Master de KubernetesEl Master de Kubernetes es el responsable de mantener el estado deseado de tu clúster. Cuando interactuas con Kubernetes, como por ejemplo cuando utilizas la interfaz de línea de comandos kubectl, te estás comunicando con el master de tu clúster de Kubernetes.Por \"master\" entendemos la colección de daemons que gestionan el estado del clúster. Típicamente, estos daemons se ejecutan todos en un único nodo del clúster, y este nodo recibe por tanto la denominación de master. El master puede estar replicado por motivos de disponibilidad y redundancia.Kubernetes NodesEn un clúster de Kubernetes, los nodos son las máquinas (máquinas virtuales, servidores físicos, etc) que ejecutan tus aplicaciones y flujos de trabajo en la nube. El master de Kubernetes controla cada nodo, por lo que en raras ocasiones interactuarás con los nodos directamente.Metadatos de los ObjectosAnnotationsSiguientes pasosSi quieres empezar a contribuir a la documentación de Kubernetes accede a la página Empieza a contribuir.IntroducciónArquitectura de KubernetesContenedoresCargas de trabajoServicios, balanceo de carga y redesAlmacenamientoConfiguraciónSeguridadPolíticasPolíticas configurables que se aplican a grupos de recursos.Administración del ClústerExtendiendo KubernetesComentarios¿Esta página le ha sido de ayuda?Sí NoMuchas gracias por el feedback. Si tienes alguna pregunta específica sobre como usar Kubernetes, puedes preguntar en Stack Overflow. Abre un issue en el repositorio de GitHub si quieres reportar un problema o sugerir alguna mejora.Última modificación May 06, 2021 at 3:04 PM PST: añadir la preposición (377d27d0ff)", "metadata": {"source": "https://kubernetes.io/es/docs/concepts/", "language": "markdown", "complexity": "medium", "type": "official_site", "word_count": 752, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.7975000000000001, "syntax_valid": true, "completeness_score": 0.5, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite system architecture documentation for Концепції\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "В розділі \"Концепції\" описані складові системи Kubernetes і абстракції, за допомогою яких Kubernetes реалізовує ваш кластер. Цей розділ допоможе вам краще зрозуміти, як працює Kubernetes.Загальна інформаціяДля роботи з Kubernetes ви використовуєте об'єкти API Kubernetes для того, щоб описати бажаний стан вашого кластера: які застосунки або інші робочі навантаження ви плануєте запускати, які образи контейнерів вони використовують, кількість реплік, скільки ресурсів мережі та диску ви хочете виділити тощо. Ви задаєте бажаний стан, створюючи об'єкти в Kubernetes API, зазвичай через інтерфейс командного рядка kubectl. Ви також можете взаємодіяти із кластером, задавати або змінювати його бажаний стан безпосередньо через Kubernetes API.Після того, як ви задали бажаний стан, площина управління Kubernetes приводить поточний стан кластера до бажаного за допомогою Pod Lifecycle Event Generator (PLEG). Для цього Kubernetes автоматично виконує ряд задач: запускає або перезапускає контейнери, масштабує кількість реплік у певному застосунку тощо. Площина управління Kubernetes складається із набору процесів, що виконуються у вашому кластері:Kubernetes master становить собою набір із трьох процесів, запущених на одному вузлі вашого кластера, що визначений як керівний (master). До цих процесів належать: kube-apiserver, kube-controller-manager і kube-scheduler.На кожному не-мастер вузлі вашого кластера виконуються два процеси:kubelet, що обмінюється даними з Kubernetes master.kube-proxy, мережевий проксі, що відображає мережеві сервіси Kubernetes на кожному вузлі.Об'єкти KubernetesKubernetes оперує певною кількістю абстракцій, що відображають стан вашої системи: розгорнуті у контейнерах застосунки та робочі навантаження, пов'язані з ними ресурси мережі та диску, інша інформація щодо функціонування вашого кластера. Ці абстракції представлені як об'єкти Kubernetes API. Для більш детальної інформації ознайомтесь з Об'єктами Kubernetes.До базових об'єктів Kubernetes належать:PodServiceVolumeNamespaceВ Kubernetes є також абстракції вищого рівня, які надбудовуються над базовими об'єктами за допомогою контролерів і забезпечують додаткову функціональність і зручність. До них належать:DeploymentDaemonSetStatefulSetReplicaSetJobПлощина управління Kubernetes (Kubernetes Control Plane)Різні частини площини управління Kubernetes, такі як Kubernetes Master і kubelet, регулюють, як Kubernetes спілкується з вашим кластером. Площина управління веде облік усіх об'єктів Kubernetes в системі та безперервно, в циклі перевіряє стан цих об'єктів. У будь-який момент часу контрольні цикли, запущені площиною управління, реагуватимуть на зміни у кластері і намагатимуться привести поточний стан об'єктів до бажаного, що заданий у конфігурації.Наприклад, коли за допомогою API Kubernetes ви створюєте Deployment, ви задаєте новий бажаний стан для системи. Площина управління Kubernetes фіксує створення цього об'єкта і виконує ваші інструкції шляхом запуску потрібних застосунків та їх розподілу між вузлами кластера. В такий спосіб досягається відповідність поточного стану бажаному.Kubernetes MasterKubernetes Master відповідає за підтримку бажаного стану вашого кластера. Щоразу, як ви взаємодієте з Kubernetes, наприклад при використанні інтерфейсу командного рядка kubectl, ви обмінюєтесь даними із Kubernetes master вашого кластера.Слово \"master\" стосується набору процесів, які управляють станом кластера. Переважно всі ці процеси виконуються на одному вузлі кластера, який також називається master. Master-вузол можна реплікувати для забезпечення високої доступності кластера.Вузли KubernetesВузлами кластера називають машини (ВМ, фізичні сервери тощо), на яких запущені ваші застосунки та хмарні робочі навантаження. Кожен вузол керується Kubernetes master; ви лише зрідка взаємодіятимете безпосередньо із вузлами.Що даліЯкщо ви хочете створити нову сторінку у розділі Концепції, у статті Використання шаблонів сторінок ви знайдете інформацію щодо типу і шаблона сторінки.ОглядРобочі навантаженняСервіси, балансування навантаження та мережаСховища інформаціїКонфігураціяРозширення можливостей KubernetesРізні способи зміни поведінки вашого кластера Kubernetes.Ваша думкаЧи була ця сторінка корисною?Так НіДякуємо за ваш відгук. Якщо ви маєте конкретне запитання щодо використання Kubernetes, ви можете поставити його Stack Overflow. Створіть issue в GitHub репозиторії, якщо ви хочете повідомити про проблему або запропонувати покращення.Змінено January 16, 2023 at 5:22 PM PST: Update design doc reference link of PLEG (ce54e0797c)", "metadata": {"source": "https://kubernetes.io/uk/docs/concepts/", "language": "markdown", "complexity": "medium", "type": "official_site", "word_count": 557, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8975000000000002, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "architecture_documentation", "prompt": "### Documentation Request (CLAUDE.md):\nWrite technical architecture documentation based on: Graph Systems blog posts\n\n### EXPLORE Phase:\nAnalyze system requirements and constraints.\n\n### PLAN Phase:\nStructure documentation for engineering teams.\n\n### CODE Phase:", "completion": "Graph Systems blog posts Economic Graph From the Economic Graph to Economic Insights: Building the Inf... Jun 2, 2023 Graph Systems How LIquid Connects Everything So Our Members Can Do Anything May 2, 2023 Culture Career stories: Spotlighting Technical Program Management Mar 2, 2023 Knowledge Completing a member knowledge graph with Graph Neural Networks Jaewon Yang Dec 1, 2021 Graph Systems LIquid: The soul of a new graph database, Part 2 Scott Meyer Sep 16, 2020 Graph Systems LIquid: The soul of a new graph database, Part 1 Scott Meyer Jul 23, 2020 Graph Systems InGraphs: Monitoring and Unexpected Artwork Cliff Snyder Aug 3, 2017 Knowledge Building The LinkedIn Knowledge Graph Qi He Oct 6, 2016 Getting to Know Jim Zhang and Alice Zhou Karan Parikh Apr 29, 2016 Getting To Know Karan Parikh Josh Clemm Apr 22, 2015 Distributed Systems Real-time distributed tracing for website performance and effi... Feb 3, 2015 Graph Systems Using set cover algorithm to optimize query latency for a larg... Rui Wang Aug 23, 2013", "metadata": {"source": "https://www.linkedin.com/blog/engineering/graph-systems", "language": "markdown", "complexity": "simple", "type": "engineering_blog", "word_count": 170, "authentic": true}, "validation": {"overall_quality": 0.7525, "syntax_valid": true, "completeness_score": 0.5, "authenticity_score": 0.5}}
{"domain": "technical_documentation", "category": "operational_guides", "prompt": "### Documentation Request (CLAUDE.md):\nCreate operational guide documentation for Deployments\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "DeploymentsA Deployment manages a set of Pods to run an application workload, usually one that doesn't maintain state.A Deployment provides declarative updates for Pods and ReplicaSets.You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.Note:Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.Use CaseThe following are typical use cases for Deployments:Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.Rollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.Scale up the Deployment to facilitate more load.Pause the rollout of a Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.Use the status of the Deployment as an indicator that a rollout has stuck.Clean up older ReplicaSets that you don't need anymore.Creating a DeploymentThe following is an example of a Deployment. It creates a ReplicaSet to bring up three nginx Pods:controllers/nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 In this example:A Deployment named nginx-deployment is created, indicated by the .metadata.name field. This name will become the basis for the ReplicaSets and Pods which are created later. See Writing a Deployment Spec for more details.The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the .spec.replicas field.The .spec.selector field defines how the created ReplicaSet finds which Pods to manage. In this case, you select a label that is defined in the Pod template (app: nginx). However, more sophisticated selection rules are possible, as long as the Pod template itself satisfies the rule.Note:The .spec.selector.matchLabels field is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". All of the requirements, from both matchLabels and matchExpressions, must be satisfied in order to match.The .spec.template field contains the following sub-fields:The Pods are labeled app: nginxusing the .metadata.labels field.The Pod template's specification, or .spec field, indicates that the Pods run one container, nginx, which runs the nginx Docker Hub image at version 1.14.2.Create one container and name it nginx using the .spec.containers[0].name field.Before you begin, make sure your Kubernetes cluster is up and running. Follow the steps given below to create the above Deployment:Create the Deployment by running the following command:kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml Run kubectl get deployments to check if the Deployment was created.If the Deployment is still being created, the output is similar to the following:NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 0/3 0 0 1s When you inspect the Deployments in your cluster, the following fields are displayed:NAME lists the names of the Deployments in the namespace.READY displays how many replicas of the application are available to your users. It follows the pattern ready/desired.UP-TO-DATE displays the number of replicas that have been updated to achieve the desired state.AVAILABLE displays how many replicas of the application are available to your users.AGE displays the amount of time that the application has been running.Notice how the number of desired replicas is 3 according to .spec.replicas field.To see the Deployment rollout status, run kubectl rollout status deployment/nginx-deployment.The output is similar to:Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment \"nginx-deployment\" successfully rolled out Run the kubectl get deployments again a few seconds later. The output is similar to this:NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 18s Notice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.To see the ReplicaSet (rs) created by the Deployment, run kubectl get rs. The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-deployment-75675f5897 3 3 3 18s ReplicaSet output shows the following fields:NAME lists the names of the ReplicaSets in the namespace.DESIRED displays the desired number of replicas of the application, which you define when you create the Deployment. This is the desired state.CURRENT displays how many replicas are currently running.READY displays how many replicas of the application are available to your users.AGE displays the amount of time that the application has been running.Notice that the name of the ReplicaSet is always formatted as [DEPLOYMENT-NAME]-[HASH]. This name will become the basis for the Pods which are created.The HASH string is the same as the pod-template-hash label on the ReplicaSet.To see the labels automatically generated for each Pod, run kubectl get pods --show-labels. The output is similar to:NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-75675f5897-7ci7o 1/1 Running 0 18s app=nginx,pod-template-hash=75675f5897 nginx-deployment-75675f5897-kzszj 1/1 Running 0 18s app=nginx,pod-template-hash=75675f5897 nginx-deployment-75675f5897-qqcnn 1/1 Running 0 18s app=nginx,pod-template-hash=75675f5897 The created ReplicaSet ensures that there are three nginx Pods.Note:You must specify an appropriate selector and Pod template labels in a Deployment (in this case, app: nginx).Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.Pod-template-hash labelCaution:Do not change this label.The pod-template-hash label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the PodTemplate of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels, and in any existing Pods that the ReplicaSet might have.Updating a DeploymentNote:A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, .spec.template) is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.Follow the steps given below to update your Deployment:Let's update the nginx Pods to use the nginx:1.16.1 image instead of the nginx:1.14.2 image.kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 or use the following command:kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 where deployment/nginx-deployment indicates the Deployment, nginx indicates the Container the update will take place and nginx:1.16.1 indicates the new image and its tag.The output is similar to:deployment.apps/nginx-deployment image updated Alternatively, you can edit the Deployment and change .spec.template.spec.containers[0].image from nginx:1.14.2 to nginx:1.16.1:kubectl edit deployment/nginx-deployment The output is similar to:deployment.apps/nginx-deployment edited To see the rollout status, run:kubectl rollout status deployment/nginx-deployment The output is similar to this:Waiting for rollout to finish: 2 out of 3 new replicas have been updated... ordeployment \"nginx-deployment\" successfully rolled out Get more details on your updated Deployment:After the rollout succeeds, you can view the Deployment by running kubectl get deployments. The output is similar to this:NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 36s Run kubectl get rs to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it up to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 3 6s nginx-deployment-2035384211 0 0 0 36s Running get pods should now show only the new Pods:kubectl get pods The output is similar to this:NAME READY STATUS RESTARTS AGE nginx-deployment-1564180365-khku8 1/1 Running 0 14s nginx-deployment-1564180365-nacti 1/1 Running 0 14s nginx-deployment-1564180365-z9gth 1/1 Running 0 14s Next time you want to update these Pods, you only need to update the Deployment's Pod template again.Deployment ensures that only a certain number of Pods are down while they are being updated. By default, it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).Deployment also ensures that only a certain number of Pods are created above the desired number of Pods. By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).For example, if you look at the above Deployment closely, you will see that it first creates a new Pod, then deletes an old Pod, and creates another new one. It does not kill old Pods until a sufficient number of new Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed. It makes sure that at least 3 Pods are available and that at max 4 Pods in total are available. In case of a Deployment with 4 replicas, the number of Pods would be between 3 and 5.Get details of your Deployment:kubectl describe deployments The output is similar to this:Name: nginx-deployment Namespace: default CreationTimestamp: Thu, 30 Nov 2017 10:56:25 +0000 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=2 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-1564180365 (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 2m deployment-controller Scaled up replica set nginx-deployment-2035384211 to 3 Normal ScalingReplicaSet 24s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 1 Normal ScalingReplicaSet 22s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 2 Normal ScalingReplicaSet 22s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 2 Normal ScalingReplicaSet 19s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 1 Normal ScalingReplicaSet 19s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 3 Normal ScalingReplicaSet 14s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 0 Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211) and scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet (nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet to 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times. It then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy. Finally, you'll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.Note:Kubernetes doesn't count terminating Pods when calculating the number of availableReplicas, which must be between replicas - maxUnavailable and replicas + maxSurge. As a result, you might notice that there are more Pods than expected during a rollout, and that the total resources consumed by the Deployment is more than replicas + maxSurge until the terminationGracePeriodSeconds of the terminating Pods expires.Rollover (aka multiple updates in-flight)Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels match .spec.selector but whose template does not match .spec.template are scaled down. Eventually, the new ReplicaSet is scaled to .spec.replicas and all old ReplicaSets is scaled to 0.If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet as per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously -- it will add it to its list of old ReplicaSets and start scaling it down.For example, suppose you create a Deployment to create 5 replicas of nginx:1.14.2, but then update the Deployment to create 5 replicas of nginx:1.16.1, when only 3 replicas of nginx:1.14.2 had been created. In that case, the Deployment immediately starts killing the 3 nginx:1.14.2 Pods that it had created, and starts creating nginx:1.16.1 Pods. It does not wait for the 5 replicas of nginx:1.14.2 to be created before changing course.Label selector updatesIt is generally discouraged to make label selector updates and it is suggested to plan your selectors up front. In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped all of the implications.Note:In API version apps/v1, a Deployment's label selector is immutable after it gets created.Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too, otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and creating a new ReplicaSet.Selector updates changes the existing value in a selector key -- result in the same behavior as additions.Selector removals removes an existing key from the Deployment selector -- do not require any changes in the Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the removed label still exists in any existing Pods and ReplicaSets.Rolling Back a DeploymentSometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping. By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want (you can change that by modifying revision history limit).Note:A Deployment's revision is created when a Deployment's rollout is triggered. This means that the new revision is created if and only if the Deployment's Pod template (.spec.template) is changed, for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment, do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling. This means that when you roll back to an earlier revision, only the Deployment's Pod template part is rolled back.Suppose that you made a typo while updating the Deployment, by putting the image name as nginx:1.161 instead of nginx:1.16.1:kubectl set image deployment/nginx-deployment nginx=nginx:1.161 The output is similar to this:deployment.apps/nginx-deployment image updated The rollout gets stuck. You can verify it by checking the rollout status:kubectl rollout status deployment/nginx-deployment The output is similar to this:Waiting for rollout to finish: 1 out of 3 new replicas have been updated... Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts, read more here.You see that the number of old replicas (adding the replica count from nginx-deployment-1564180365 and nginx-deployment-2035384211) is 3, and the number of new replicas (from nginx-deployment-3066724191) is 1.kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 3 25s nginx-deployment-2035384211 0 0 0 36s nginx-deployment-3066724191 1 1 0 6s Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.kubectl get pods The output is similar to this:NAME READY STATUS RESTARTS AGE nginx-deployment-1564180365-70iae 1/1 Running 0 25s nginx-deployment-1564180365-jbqqo 1/1 Running 0 25s nginx-deployment-1564180365-hysrc 1/1 Running 0 25s nginx-deployment-3066724191-08mng 0/1 ImagePullBackOff 0 6s Note:The Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (maxUnavailable specifically) that you have specified. Kubernetes by default sets the value to 25%.Get the description of the Deployment:kubectl describe deployment The output is similar to this:Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 15 Mar 2016 14:48:04 -0700 Labels: app=nginx Selector: app=nginx Replicas: 3 desired | 1 updated | 4 total | 3 available | 1 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.161 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True ReplicaSetUpdated OldReplicaSets: nginx-deployment-1564180365 (3/3 replicas created) NewReplicaSet: nginx-deployment-3066724191 (1/1 replicas created) Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-2035384211 to 3 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 1 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 2 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 2 21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 1 21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 3 13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 0 13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-3066724191 to 1 To fix this, you need to rollback to a previous revision of Deployment that is stable.Checking Rollout History of a DeploymentFollow the steps given below to check the rollout history:First, check the revisions of this Deployment:kubectl rollout history deployment/nginx-deployment The output is similar to this:deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml 2 kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 3 kubectl set image deployment/nginx-deployment nginx=nginx:1.161 CHANGE-CAUSE is copied from the Deployment annotation kubernetes.io/change-cause to its revisions upon creation. You can specify theCHANGE-CAUSE message by:Annotating the Deployment with kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"image updated to 1.16.1\"Manually editing the manifest of the resource.To see the details of each revision, run:kubectl rollout history deployment/nginx-deployment --revision=2 The output is similar to this:deployments \"nginx-deployment\" revision 2 Labels: app=nginx pod-template-hash=1159050644 Annotations: kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP QoS Tier: cpu: BestEffort memory: BestEffort Environment Variables: <none> No volumes. Rolling Back to a Previous RevisionFollow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.Now you've decided to undo the current rollout and rollback to the previous revision:kubectl rollout undo deployment/nginx-deployment The output is similar to this:deployment.apps/nginx-deployment rolled back Alternatively, you can rollback to a specific revision by specifying it with --to-revision:kubectl rollout undo deployment/nginx-deployment --to-revision=2 The output is similar to this:deployment.apps/nginx-deployment rolled back For more details about rollout related commands, read kubectl rollout.The Deployment is now rolled back to a previous stable revision. As you can see, a DeploymentRollback event for rolling back to revision 2 is generated from Deployment controller.Check if the rollback was successful and the Deployment is running as expected, run:kubectl get deployment nginx-deployment The output is similar to this:NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 30m Get the description of the Deployment:kubectl describe deployment nginx-deployment The output is similar to this:Name: nginx-deployment Namespace: default CreationTimestamp: Sun, 02 Sep 2018 18:17:55 -0500 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=4 kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-c4747d96c (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 12m deployment-controller Scaled up replica set nginx-deployment-75675f5897 to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 0 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-595696685f to 1 Normal DeploymentRollback 15s deployment-controller Rolled back deployment \"nginx-deployment\" to revision 2 Normal ScalingReplicaSet 15s deployment-controller Scaled down replica set nginx-deployment-595696685f to 0 Scaling a DeploymentYou can scale a Deployment by using the following command:kubectl scale deployment/nginx-deployment --replicas=10 The output is similar to this:deployment.apps/nginx-deployment scaled Assuming horizontal Pod autoscaling is enabled in your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of Pods you want to run based on the CPU utilization of your existing Pods.kubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80 The output is similar to this:deployment.apps/nginx-deployment scaled Proportional scalingRollingUpdate Deployments support running multiple versions of an application at the same time. When you or an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress or paused), the Deployment controller balances the additional replicas in the existing active ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called proportional scaling.For example, you are running a Deployment with 10 replicas, maxSurge=3, and maxUnavailable=2.Ensure that the 10 replicas in your Deployment are running.kubectl get deploy The output is similar to this:NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 10 10 10 10 50s You update to a new image which happens to be unresolvable from inside the cluster.kubectl set image deployment/nginx-deployment nginx=nginx:sometag The output is similar to this:deployment.apps/nginx-deployment image updated The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the maxUnavailable requirement that you mentioned above. Check out the rollout status:kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-deployment-1989198191 5 5 0 9s nginx-deployment-618515232 8 8 8 1m Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas to 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.In our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the new ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming the new replicas become healthy. To confirm this, run:kubectl get deploy The output is similar to this:NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 15 18 7 8 7m The rollout status confirms how the replicas were added to each ReplicaSet.kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-deployment-1989198191 7 7 0 7m nginx-deployment-618515232 11 11 11 7m Pausing and Resuming a rollout of a DeploymentWhen you update a Deployment, or plan to, you can pause rollouts for that Deployment before you trigger one or more updates. When you're ready to apply those changes, you resume rollouts for the Deployment. This approach allows you to apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.For example, with a Deployment that was created:Get the Deployment details:kubectl get deploy The output is similar to this:NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 3 3 3 3 1m Get the rollout status:kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 1m Pause by running the following command:kubectl rollout pause deployment/nginx-deployment The output is similar to this:deployment.apps/nginx-deployment paused Then update the image of the Deployment:kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 The output is similar to this:deployment.apps/nginx-deployment image updated Notice that no new rollout started:kubectl rollout history deployment/nginx-deployment The output is similar to this:deployments \"nginx\" REVISION CHANGE-CAUSE 1 <none> Get the rollout status to verify that the existing ReplicaSet has not changed:kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 2m You can make as many updates as you wish, for example, update the resources that will be used:kubectl set resources deployment/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi The output is similar to this:deployment.apps/nginx-deployment resource requirements updated The initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to the Deployment will not have any effect as long as the Deployment rollout is paused.Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:kubectl rollout resume deployment/nginx-deployment The output is similar to this:deployment.apps/nginx-deployment resumed Watch the status of the rollout until it's done.kubectl get rs --watch The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-2142116321 2 2 2 2m nginx-3926361531 2 2 0 6s nginx-3926361531 2 2 1 18s nginx-2142116321 1 2 2 2m nginx-2142116321 1 2 2 2m nginx-3926361531 3 2 1 18s nginx-3926361531 3 2 1 18s nginx-2142116321 1 1 1 2m nginx-3926361531 3 3 1 18s nginx-3926361531 3 3 2 19s nginx-2142116321 0 1 1 2m nginx-2142116321 0 1 1 2m nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 20s Get the status of the latest rollout:kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 28s Note:You cannot rollback a paused Deployment until you resume it.Deployment statusA Deployment enters various states during its lifecycle. It can be progressing while rolling out a new ReplicaSet, it can be complete, or it can fail to progress.Progressing DeploymentKubernetes marks a Deployment as progressing when one of the following tasks is performed:The Deployment creates a new ReplicaSet.The Deployment is scaling up its newest ReplicaSet.The Deployment is scaling down its older ReplicaSet(s).New Pods become ready or available (ready for at least MinReadySeconds).When the rollout becomes “progressing”, the Deployment controller adds a condition with the following attributes to the Deployment's .status.conditions:type: Progressingstatus: \"True\"reason: NewReplicaSetCreated | reason: FoundNewReplicaSet | reason: ReplicaSetUpdatedYou can monitor the progress for a Deployment by using kubectl rollout status.Complete DeploymentKubernetes marks a Deployment as complete when it has the following characteristics:All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any updates you've requested have been completed.All of the replicas associated with the Deployment are available.No old replicas for the Deployment are running.When the rollout becomes “complete”, the Deployment controller sets a condition with the following attributes to the Deployment's .status.conditions:type: Progressingstatus: \"True\"reason: NewReplicaSetAvailableThis Progressing condition will retain a status value of \"True\" until a new rollout is initiated. The condition holds even when availability of replicas changes (which does instead affect the Available condition).You can check if a Deployment has completed by using kubectl rollout status. If the rollout completed successfully, kubectl rollout status returns a zero exit code.kubectl rollout status deployment/nginx-deployment The output is similar to this:Waiting for rollout to finish: 2 of 3 updated replicas are available... deployment \"nginx-deployment\" successfully rolled out and the exit status from kubectl rollout is 0 (success):echo $? 0 Failed DeploymentYour Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur due to some of the following factors:Insufficient quotaReadiness probe failuresImage pull errorsInsufficient permissionsLimit rangesApplication runtime misconfigurationOne way you can detect this condition is to specify a deadline parameter in your Deployment spec: (.spec.progressDeadlineSeconds). .spec.progressDeadlineSeconds denotes the number of seconds the Deployment controller waits before indicating (in the Deployment status) that the Deployment progress has stalled.The following kubectl command sets the spec with progressDeadlineSeconds to make the controller report lack of progress of a rollout for a Deployment after 10 minutes:kubectl patch deployment/nginx-deployment -p '{\"spec\":{\"progressDeadlineSeconds\":600}}' The output is similar to this:deployment.apps/nginx-deployment patched Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following attributes to the Deployment's .status.conditions:type: Progressingstatus: \"False\"reason: ProgressDeadlineExceededThis condition can also fail early and is then set to status value of \"False\" due to reasons as ReplicaSetCreateError. Also, the deadline is not taken into account anymore once the Deployment rollout completes.See the Kubernetes API conventions for more information on status conditions.Note:Kubernetes takes no action on a stalled Deployment other than to report a status condition with reason: ProgressDeadlineExceeded. Higher level orchestrators can take advantage of it and act accordingly, for example, rollback the Deployment to its previous version.Note:If you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline. You can safely pause a Deployment rollout in the middle of a rollout and resume without triggering the condition for exceeding the deadline.You may experience transient errors with your Deployments, either due to a low timeout that you have set or due to any other kind of error that can be treated as transient. For example, let's suppose you have insufficient quota. If you describe the Deployment you will notice the following section:kubectl describe deployment nginx-deployment The output is similar to this:<...> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True ReplicaSetUpdated ReplicaFailure True FailedCreate <...> If you run kubectl get deployment nginx-deployment -o yaml, the Deployment status is similar to this:status: availableReplicas: 2 conditions: - lastTransitionTime: 2016-10-04T12:25:39Z lastUpdateTime: 2016-10-04T12:25:39Z message: Replica set \"nginx-deployment-4262182780\" is progressing. reason: ReplicaSetUpdated status: \"True\" type: Progressing - lastTransitionTime: 2016-10-04T12:25:42Z lastUpdateTime: 2016-10-04T12:25:42Z message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \"True\" type: Available - lastTransitionTime: 2016-10-04T12:25:39Z lastUpdateTime: 2016-10-04T12:25:39Z message: 'Error creating: pods \"nginx-deployment-4262182780-\" is forbidden: exceeded quota: object-counts, requested: pods=1, used: pods=3, limited: pods=2' reason: FailedCreate status: \"True\" type: ReplicaFailure observedGeneration: 3 replicas: 2 unavailableReplicas: 2 Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the reason for the Progressing condition:Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing False ProgressDeadlineExceeded ReplicaFailure True FailedCreate You can address an issue of insufficient quota by scaling down your Deployment, by scaling down other controllers you may be running, or by increasing quota in your namespace. If you satisfy the quota conditions and the Deployment controller then completes the Deployment rollout, you'll see the Deployment's status update with a successful condition (status: \"True\" and reason: NewReplicaSetAvailable).Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable type: Available with status: \"True\" means that your Deployment has minimum availability. Minimum availability is dictated by the parameters specified in the deployment strategy. type: Progressing with status: \"True\" means that your Deployment is either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum required new replicas are available (see the Reason of the condition for the particulars - in our case reason: NewReplicaSetAvailable means that the Deployment is complete).You can check if a Deployment has failed to progress by using kubectl rollout status. kubectl rollout status returns a non-zero exit code if the Deployment has exceeded the progression deadline.kubectl rollout status deployment/nginx-deployment The output is similar to this:Waiting for rollout to finish: 2 out of 3 new replicas have been updated... error: deployment \"nginx\" exceeded its progress deadline and the exit status from kubectl rollout is 1 (indicating an error):echo $? 1 Operating on a failed deploymentAll actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.Clean up PolicyYou can set .spec.revisionHistoryLimit field in a Deployment to specify how many old ReplicaSets for this Deployment you want to retain. The rest will be garbage-collected in the background. By default, it is 10.Note:Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment thus that Deployment will not be able to roll back.The cleanup only starts after a Deployment reaches a complete state. If you set .spec.revisionHistoryLimit to 0, any rollout nonetheless triggers creation of a new ReplicaSet before Kubernetes removes the old one.Even with a non-zero revision history limit, you can have more ReplicaSets than the limit you configure. For example, if pods are crash looping, and there are multiple rolling updates events triggered over time, you might end up with more ReplicaSets than the .spec.revisionHistoryLimit because the Deployment never reaches a complete state.Canary DeploymentIf you want to roll out releases to a subset of users or servers using the Deployment, you can create multiple Deployments, one for each release, following the canary pattern described in managing resources.Writing a Deployment SpecAs with all other Kubernetes configs, a Deployment needs .apiVersion, .kind, and .metadata fields. For general information about working with config files, see deploying applications, configuring containers, and using kubectl to manage resources documents.When the control plane creates new Pods for a Deployment, the .metadata.name of the Deployment is part of the basis for naming those Pods. The name of a Deployment must be a valid DNS subdomain value, but this can produce unexpected results for the Pod hostnames. For best compatibility, the name should follow the more restrictive rules for a DNS label.A Deployment also needs a .spec section.Pod TemplateThe .spec.template and .spec.selector are the only required fields of the .spec.The .spec.template is a Pod template. It has exactly the same schema as a Pod, except it is nested and does not have an apiVersion or kind.In addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See selector.Only a .spec.template.spec.restartPolicy equal to Always is allowed, which is the default if not specified.Replicas.spec.replicas is an optional field that specifies the number of desired Pods. It defaults to 1.Should you manually scale a Deployment, example via kubectl scale deployment deployment --replicas=X, and then you update that Deployment based on a manifest (for example: by running kubectl apply -f deployment.yaml), then applying that manifest overwrites the manual scaling that you previously did.If a HorizontalPodAutoscaler (or any similar API for horizontal scaling) is managing scaling for a Deployment, don't set .spec.replicas.Instead, allow the Kubernetes control plane to manage the .spec.replicas field automatically.Selector.spec.selector is a required field that specifies a label selector for the Pods targeted by this Deployment..spec.selector must match .spec.template.metadata.labels, or it will be rejected by the API.In API version apps/v1, .spec.selector and .metadata.labels do not default to .spec.template.metadata.labels if not set. So they must be set explicitly. Also note that .spec.selector is immutable after creation of the Deployment in apps/v1.A Deployment may terminate Pods whose labels match the selector if their template is different from .spec.template or if the total number of such Pods exceeds .spec.replicas. It brings up new Pods with .spec.template if the number of Pods is less than the desired number.Note:You should not create other Pods whose labels match this selector, either directly, by creating another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you do so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.If you have multiple controllers that have overlapping selectors, the controllers will fight with each other and won't behave correctly.Strategy.spec.strategy specifies the strategy used to replace old Pods by new ones. .spec.strategy.type can be \"Recreate\" or \"RollingUpdate\". \"RollingUpdate\" is the default value.Recreate DeploymentAll existing Pods are killed before new ones are created when .spec.strategy.type==Recreate.Note:This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods of the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new revision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the replacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an \"at most\" guarantee for your Pods, you should consider using a StatefulSet.Rolling Update DeploymentThe Deployment updates Pods in a rolling update fashion (gradually scale down the old ReplicaSets and scale up the new one) when .spec.strategy.type==RollingUpdate. You can specify maxUnavailable and maxSurge to control the rolling update process.Max Unavailable.spec.strategy.rollingUpdate.maxUnavailable is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5) or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by rounding down. The value cannot be 0 if .spec.strategy.rollingUpdate.maxSurge is 0. The default value is 25%.For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available at all times during the update is at least 70% of the desired Pods.Max Surge.spec.strategy.rollingUpdate.maxSurge is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a percentage of desired Pods (for example, 10%). The value cannot be 0 if maxUnavailable is 0. The absolute number is calculated from the percentage by rounding up. The default value is 25%.For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the total number of Pods running at any time during the update is at most 130% of desired Pods.Here are some Rolling Update Deployment examples that use the maxUnavailable and maxSurge:apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 Progress Deadline Seconds.spec.progressDeadlineSeconds is an optional field that specifies the number of seconds you want to wait for your Deployment to progress before the system reports back that the Deployment has failed progressing - surfaced as a condition with type: Progressing, status: \"False\". and reason: ProgressDeadlineExceeded in the status of the resource. The Deployment controller will keep retrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment controller will roll back a Deployment as soon as it observes such a condition.If specified, this field needs to be greater than .spec.minReadySeconds.Min Ready Seconds.spec.minReadySeconds is an optional field that specifies the minimum number of seconds for which a newly created Pod should be ready without any of its containers crashing, for it to be considered available. This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when a Pod is considered ready, see Container Probes.Terminating PodsFEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)You can enable this feature by setting the DeploymentReplicaSetTerminatingReplicas feature gate on the API server and on the kube-controller-managerPods that become terminating due to deletion or scale down may take a long time to terminate, and may consume additional resources during that period. As a result, the total number of all pods can temporarily exceed .spec.replicas. Terminating pods can be tracked using the .status.terminatingReplicas field of the Deployment.Revision History LimitA Deployment's revision history is stored in the ReplicaSets it controls..spec.revisionHistoryLimit is an optional field that specifies the number of old ReplicaSets to retain to allow rollback. These old ReplicaSets consume resources in etcd and crowd the output of kubectl get rs. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.More specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up. In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.Paused.spec.paused is an optional boolean field for pausing and resuming a Deployment. The only difference between a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused Deployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when it is created.What's nextLearn more about Pods.Run a stateless application using a Deployment.Read the Deployment to understand the Deployment API.Read about PodDisruptionBudget and how you can use it to manage application availability during disruptions.Use kubectl to create a Deployment.FeedbackWas this page helpful?Yes NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub Repository if you want to report a problem or suggest an improvement.Last modified June 18, 2025 at 9:08 PM PST: feat: make rollingUpdate description clearer in deployment.md (be155f3fda)\n\n## Code Examples\n\n### Example 1\n\n```\nName:                   nginx-deployment\nNamespace:              default\nCreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000\nLabels:                 app=nginx\nAnnotations:            deployment.kubernetes.io/revision=2\nSelector:               app=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  app=nginx\n   Containers:\n    nginx:\n      Image:        nginx:1.16.1\n      Port:         80/TCP\n      Environment:  <none>\n      Mounts:       <none>\n    Volumes:        <none>\n  Conditions:\n    Type           Status  Reason\n    ----           ------  ------\n    Available      True    MinimumReplicasAvailable\n    Progressing    True    NewReplicaSetAvailable\n  OldReplicaSets:  <none>\n  NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)\n  Events:\n    Type    Reason             Age   From                   Message\n    ----    ------             ----  ----                   -------\n    Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3\n    Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1\n    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2\n    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2\n    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1\n    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3\n    Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0\n```\n\n### Example 2\n\n```\nName:           nginx-deployment\nNamespace:      default\nCreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700\nLabels:         app=nginx\nSelector:       app=nginx\nReplicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable\nStrategyType:       RollingUpdate\nMinReadySeconds:    0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  app=nginx\n  Containers:\n   nginx:\n    Image:        nginx:1.161\n    Port:         80/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    ReplicaSetUpdated\nOldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)\nNewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)\nEvents:\n  FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message\n  --------- --------    -----   ----                    -------------   --------    ------              -------\n  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3\n  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1\n  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2\n  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2\n  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1\n  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3\n  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0\n  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1\n```\n\n### Example 3\n\n```\ndeployments \"nginx-deployment\"\nREVISION    CHANGE-CAUSE\n1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml\n2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n3           kubectl set image deployment/nginx-deployment nginx=nginx:1.161\n```\n\n", "metadata": {"source": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/", "language": "markdown", "complexity": "complex", "type": "official_site", "word_count": 6871, "code_examples": 37, "authentic": true}, "validation": {"overall_quality": 0.865, "syntax_valid": true, "completeness_score": 0.55, "authenticity_score": 0.8}}
{"domain": "technical_documentation", "category": "operational_guides", "prompt": "### Documentation Request (CLAUDE.md):\nCreate operational guide documentation for Observability and monitoring\njust got easier\n\n### EXPLORE Phase:\nAnalyze documentation requirements and audience needs.\n\n### PLAN Phase:\nStructure information for clarity and usability.\n\n### CODE Phase:", "completion": "Grafana solutionsObservability and monitoring just got easierGrafana Solutions Monitoring AerospikeMonitorApache ActiveMQMonitorApache AirflowMonitorApache CassandraMonitorApache CouchDBMonitorApache HadoopMonitorApache HBaseMonitorApache HTTP ServerMonitorApache MesosMonitorApache SolrMonitorApache SparkMonitorApache TomcatMonitorApollo ServerMonitorAsteriskMonitorAWXMonitorCaddyMonitorCatchpointMonitor VisualizeCephMonitorcert-managerMonitorCilium EnterpriseMonitorClickHouseMonitorCloudflareMonitor VisualizeCockroachDBMonitor VisualizeConfluent CloudMonitorConsulMonitorCoreDNSMonitorCouchbaseMonitorDiscourseMonitorDnsmasqMonitorDockerMonitorDocker DesktopMonitorElasticsearchMonitorEnvoyMonitoretcdMonitorF5 BIG-IPMonitorGiteaMonitorGitHubMonitorGitLabMonitor VisualizeGoMonitorGrafana AgentMonitorGrafana AlloyMonitorGrafana Loki (self-hosted)MonitorGrafana Mimir (self-hosted)MonitorHAProxyMonitorHashiCorp VaultMonitorHashicorp vault (agentless)MonitorHome AssistantMonitorIBM DB2MonitorIBM MQMonitorInfluxDBMonitorIstioMonitorJava virtual machine (JVM)MonitorJenkinsMonitorJiraMonitor VisualizeKafkaMonitorLinuxMonitormacOSMonitorMemcachedMonitorMetrics Endpoint (Prometheus)MonitorMicrosoft IISMonitorMicrosoft SQL ServerMonitorMinIOMonitorMongoDBMonitor VisualizeMongoDB AtlasMonitorMySQLMonitor VisualizeNGINXMonitorNode.jsMonitorNomadMonitorNSQMonitorOpenAIMonitorOpenLDAPMonitorOpenSearchMonitorOpenStackMonitorOracle DatabaseMonitor VisualizePgBouncerMonitorPostgreSQLMonitorPrestoMonitorRabbitMQMonitorRaspberry PiMonitorRcloneMonitorRedisMonitor VisualizeRedis EnterpriseMonitorRuby RackMonitorSAP HANA®Monitor VisualizeSNMPMonitorSnowflakeMonitor VisualizeSpring BootMonitorSquidMonitorSupabaseMonitorTensorFlow ServingMonitorTraefikMonitorUbiquiti EdgeRouterMonitorVarnish CacheMonitorVeleroMonitorVMware vSphereMonitorWildflyMonitorWindowsMonitorWindows Active DirectoryMonitorWSO2 Enterprise IntegratorMonitorWSO2 Streaming IntegratorMonitor", "metadata": {"source": "https://grafana.com/solutions/monitoring/?plcmt=nav-solutions-cta2", "language": "markdown", "complexity": "simple", "type": "official_site", "word_count": 75, "code_examples": 0, "authentic": true}, "validation": {"overall_quality": 0.8475, "syntax_valid": true, "completeness_score": 1.0, "authenticity_score": 0.5}}
